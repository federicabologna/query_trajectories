{
    "query": "What are the comparative effects of FastText-based versus BERT-based classifiers on the quality and efficiency of pre-training data filtering for large language models?",
    "user_id": "lib_user",
    "task_id": "19ff337c-cbab-4359-8db3-bc6fc82041ee",
    "timestamp": "2025-06-23T21:06:58.205492",
    "n_retrieval": 256,
    "n_retrieved": 271,
    "n_candidates": 37,
    "n_rerank": 50,
    "opt_in": true,
    "total_cost": 0.37251300000000004,
    "decomposed_query": {
        "rewritten_query": "Comparative effects of FastText-based versus BERT-based classifiers on the quality and efficiency of pre-training data filtering for large language models.",
        "keyword_query": "FastText BERT classifiers quality efficiency pre-training data filtering large language models",
        "search_filters": {
            "fieldsOfStudy": "Computer Science"
        },
        "cost": 0.009945,
        "model": "claude-3-7-sonnet-20250219"
    },
    "candidates": [
        {
            "title": "Go Simple and Pre-Train on Domain-Specific Corpora: On the Role of Training Data for Text Classification",
            "venue": "International Conference on Computational Linguistics",
            "year": 2020,
            "reference_count": 39,
            "citation_count": 25,
            "influential_citation_count": 1,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://doi.org/10.18653/v1/2020.coling-main.481",
                "status": "HYBRID",
                "license": "CCBY",
                "disclaimer": "Notice: Paper or abstract available at https://aclanthology.org/2020.coling-main.481, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "1383074767",
                    "name": "A. Edwards"
                },
                {
                    "authorId": "1387447871",
                    "name": "Jos\u00e9 Camacho-Collados"
                },
                {
                    "authorId": "2750681",
                    "name": "H\u00e9l\u00e8ne de Ribaupierre"
                },
                {
                    "authorId": "1762890",
                    "name": "A. Preece"
                }
            ],
            "abstract": "Pre-trained language models provide the foundations for state-of-the-art performance across a wide range of natural language processing tasks, including text classification. However, most classification datasets assume a large amount labeled data, which is commonly not the case in practical settings. In particular, in this paper we compare the performance of a light-weight linear classifier based on word embeddings, i.e., fastText (Joulin et al., 2017), versus a pre-trained language model, i.e., BERT (Devlin et al., 2019), across a wide range of datasets and classification tasks. In general, results show the importance of domain-specific unlabeled data, both in the form of word embeddings or language models. As for the comparison, BERT outperforms all baselines in standard datasets with large training sets. However, in settings with small training datasets a simple method like fastText coupled with domain-specific word embeddings performs equally well or better than BERT, even when pre-trained on domain-specific data.",
            "corpus_id": 227231089,
            "sentences": [
                {
                    "corpus_id": "227231089",
                    "title": "Go Simple and Pre-Train on Domain-Specific Corpora: On the Role of Training Data for Text Classification",
                    "text": "Pre-trained language models provide the foundations for state-of-the-art performance across a wide range of natural language processing tasks, including text classification. However, most classification datasets assume a large amount labeled data, which is commonly not the case in practical settings. In particular, in this paper we compare the performance of a light-weight linear classifier based on word embeddings, i.e., fastText (Joulin et al., 2017), versus a pre-trained language model, i.e., BERT (Devlin et al., 2019), across a wide range of datasets and classification tasks. In general, results show the importance of domain-specific unlabeled data, both in the form of word embeddings or language models. As for the comparison, BERT outperforms all baselines in standard datasets with large training sets. However, in settings with small training datasets a simple method like fastText coupled with domain-specific word embeddings performs equally well or better than BERT, even when pre-trained on domain-specific data.",
                    "score": 0.6852936110644444,
                    "section_title": "abstract",
                    "char_start_offset": 0,
                    "sentence_offsets": [],
                    "ref_mentions": [],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.9541015625
                },
                {
                    "corpus_id": "227231089",
                    "title": "Go Simple and Pre-Train on Domain-Specific Corpora: On the Role of Training Data for Text Classification",
                    "text": "Table 2 shows the results with different sizes of training data randomly extracted from the training set. Surprisingly, classification models based on corpus-trained embeddings achieve higher performance with less labelled data compared to the classifier based on pre-trained contextualised models. However, for cases with more than 5,000 training samples, the performance of fine-tuned BERT significantly outperforms fastText corpus-based classifier, especially when domain-trained BERT model (i.e., BERT (Twitter)) is used. Further to that, the fine-tuned model performance improves at a higher rate than the classifier based on corpus-trained embeddings for training sets with more than 2,000 instances. For instance, for the SE-18 dataset, fastText with domain embeddings improves 0.112 micro-F1 points when the entire dataset is used with respect to using only 200 instances, while BERT-Twitter provides a 0.360 absolute improvement. In contrast, fastText with pre-trained embeddings performs similarly to the baseline. This shows the advantage for pre-trained models to be fine-tuned to the given domain and task.",
                    "score": 0.5641025241806263,
                    "section_title": "Effect of training size",
                    "char_start_offset": 9068,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 105
                        },
                        {
                            "start": 106,
                            "end": 298
                        },
                        {
                            "start": 299,
                            "end": 525
                        },
                        {
                            "start": 526,
                            "end": 706
                        },
                        {
                            "start": 707,
                            "end": 938
                        },
                        {
                            "start": 939,
                            "end": 1024
                        },
                        {
                            "start": 1025,
                            "end": 1119
                        }
                    ],
                    "ref_mentions": [],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.9248046875
                },
                {
                    "corpus_id": "227231089",
                    "title": "Go Simple and Pre-Train on Domain-Specific Corpora: On the Role of Training Data for Text Classification",
                    "text": "However, to the best of our knowledge there has not been an extensive comparison between such methods and newer pre-trained language models. To this end, we compare the light-weight linear classification model fastText (Joulin et al., 2017), coupled with generic and corpus-specific word embeddings, and the pre-trained language model BERT (Devlin et al., 2019), trained on generic data and domain-specific data. Specifically, we analyze the effect of training size over the performance of the classifiers in settings where such training data is limited, both in few-shot scenarios with a balanced set and keeping the original distributions. In both cases, our results show that a large pre-trained language model may not provide significant gains over a linear model that leverage word embeddings, especially when these belong to the given domain.",
                    "score": 0.4227106823263234,
                    "section_title": "Introduction",
                    "char_start_offset": 1754,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 140
                        },
                        {
                            "start": 141,
                            "end": 412
                        },
                        {
                            "start": 413,
                            "end": 641
                        },
                        {
                            "start": 642,
                            "end": 848
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 219,
                            "end": 240,
                            "matchedPaperCorpusId": "1210515"
                        },
                        {
                            "start": 340,
                            "end": 361,
                            "matchedPaperCorpusId": "52967399"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.90966796875
                },
                {
                    "corpus_id": "227231089",
                    "title": "Go Simple and Pre-Train on Domain-Specific Corpora: On the Role of Training Data for Text Classification",
                    "text": "In this paper, we analyzed the role of training and unlabeled domain-specific data in supervised text classification. We compared both linear and neural models based on transformer-based language models. In settings with small training data, a simple method such as fastText coupled with domain-specific word embeddings appear to be more robust than a more data-consuming model such as BERT, even when BERT is pre-trained on domain-relevant data. However, the same classifier with generic pretrained word embeddings does not perform consistently better than a traditional frequency-based linear baseline. 4 BERT, pre-trained on domain-specific data (i.e., Twitter) leads to improvements over generic BERT, especially for few-shot experiments. For future work it would be interesting to further delve into the role of unlabeled data in text classification, both in terms of word embeddings (e.g., by making use of meta-embeddings (Yin and Sch\u00fctze, 2016)) and the data used to train language models (Gururangan et al., 2020). Moreover, this quantitative analysis could be extended to more classification tasks and different models, e.g., larger language models such as RoBERTa (Liu et al., 2019) and GPT-3 (Brown et al., 2020), which appear to be more suited to few-shot experiments. However, the generic domain embeddings tend to fail to represent the meaning of more domainspecific words, which may explain their lower performance. This is confirmed by the nearest neighbour analysis (see Table 5) which showed that the generic domain embeddings do not provide accurate representations of more technical words such as 'Windows' and 'Sun'. In the IMDB reviews, words such as 'Toothless', used within a very specific context are also not correctly represented by the generic model. Moreover, tweets are rich with abbreviations which have domain-specific meaning such as 'SF' referring to 'San Francisco'.",
                    "score": 0.4218332958693194,
                    "section_title": "Conclusion and Future Work",
                    "char_start_offset": 14243,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 117
                        },
                        {
                            "start": 118,
                            "end": 203
                        },
                        {
                            "start": 204,
                            "end": 446
                        },
                        {
                            "start": 447,
                            "end": 606
                        },
                        {
                            "start": 607,
                            "end": 742
                        },
                        {
                            "start": 743,
                            "end": 1023
                        },
                        {
                            "start": 1024,
                            "end": 1281
                        },
                        {
                            "start": 1282,
                            "end": 1431
                        },
                        {
                            "start": 1432,
                            "end": 1638
                        },
                        {
                            "start": 1639,
                            "end": 1779
                        },
                        {
                            "start": 1780,
                            "end": 1902
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 929,
                            "end": 952,
                            "matchedPaperCorpusId": "2820782"
                        },
                        {
                            "start": 997,
                            "end": 1022,
                            "matchedPaperCorpusId": "216080466"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.87060546875
                },
                {
                    "corpus_id": "227231089",
                    "title": "Go Simple and Pre-Train on Domain-Specific Corpora: On the Role of Training Data for Text Classification",
                    "text": "5 instances per label 10 instances per label FT(gen) FT(dom) BERT(gen) BERT(T) FT(gen) FT(dom) BERT(gen) BERT(T) FT(gen) FT(dom) BERT(gen) BERT(T)  . Similarly to the experiments with randomized data samples, fastText based on corpus-trained embeddings is the best performing classification model for very small amounts of balanced labeled data (see Figure 2). However, as the amount of training data increases, BERT model outperforms fastText on average by 0.0442%. As in the previous experiment, the classification model based on pre-trained embeddings perform poorly compared to the corpus-trained embeddings and models fine-tuned to the task. Further, BERT (Twitter) leads to significant improvements over BERT when only 10 instances per label are used (i.e., for SE-16, BERT (Twitter) has macro-F1 = 0.370, similar to domain-based fastText with macro-F1 = 0.384 versus base BERT with macro-F1 = 0.200).",
                    "score": 0.5613743190071048,
                    "section_title": "instances per label",
                    "char_start_offset": 13305,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 149
                        },
                        {
                            "start": 150,
                            "end": 360
                        },
                        {
                            "start": 361,
                            "end": 466
                        },
                        {
                            "start": 467,
                            "end": 646
                        },
                        {
                            "start": 647,
                            "end": 907
                        }
                    ],
                    "ref_mentions": [],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.806640625
                },
                {
                    "corpus_id": "227231089",
                    "title": "Go Simple and Pre-Train on Domain-Specific Corpora: On the Role of Training Data for Text Classification",
                    "text": "Linear models. Linear models such as SVMs or logistic regression coupled with frequency-based handcrafted features have been traditionally used for text classification. Despite their simplicity, they are considered a strong baseline for many text classification tasks (Joachims, 1998;McCallum et al., 1998;Fan et al., 2008), even more recently on noisy corpora such as social media text (C \u00b8\u00f6ltekin and Rama, 2018;Mohammad et al., 2018). In general, however, these methods tend to struggle with OOV (Out-Of-Vocabulary) words, fine-grained distinctions and unbalanced datasets. FastText (Joulin et al., 2017), which is the model evaluated in this paper, partially addresses these issues by integrating a linear model with a rank constraint, allowing sharing parameters among features and classes, and integrates word embeddings that are then averaged into a text representation. \n\nNeural models. Neural models can learn non-linear and complex relationships which makes them a preferable method for many NLP tasks such as sentiment analysis or question answering (Sun et al., 2019). In particular, LSTMs, sometimes in combination with CNNs for text classification (Xiao and Cho, 2016;Pilehvar et al., 2017), enable capturing long-range dependencies in a sequential manner where data is read from only one direction (referred to as the 'unidirectionality constraint'). Recent state-of-the-art language models, such as BERT (Devlin et al., 2019), overcome the unidirectionality constraint by using transformer-based masked language models to learn pre-trained deep bidirectional representations. These pre-trained models leverage generic knowledge on large unlabeled corpora that can then be fine-tuned on the specific task by using the pre-trained parameters. BERT, which is the pretrained language model tested in this paper, has been proved to provide state-of-the-art results in most standard NLP benchmarks (Wang et al., 2019b), including text classification.",
                    "score": 0.4331175836356697,
                    "section_title": "Supervised machine learning models",
                    "char_start_offset": 3077,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 14
                        },
                        {
                            "start": 15,
                            "end": 168
                        },
                        {
                            "start": 169,
                            "end": 437
                        },
                        {
                            "start": 438,
                            "end": 576
                        },
                        {
                            "start": 577,
                            "end": 877
                        },
                        {
                            "start": 880,
                            "end": 894
                        },
                        {
                            "start": 895,
                            "end": 1080
                        },
                        {
                            "start": 1081,
                            "end": 1365
                        },
                        {
                            "start": 1366,
                            "end": 1591
                        },
                        {
                            "start": 1592,
                            "end": 1756
                        },
                        {
                            "start": 1757,
                            "end": 1960
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 268,
                            "end": 284,
                            "matchedPaperCorpusId": "2427083"
                        },
                        {
                            "start": 284,
                            "end": 306,
                            "matchedPaperCorpusId": "7311285"
                        },
                        {
                            "start": 306,
                            "end": 323,
                            "matchedPaperCorpusId": "3116168"
                        },
                        {
                            "start": 387,
                            "end": 414,
                            "matchedPaperCorpusId": "43958367"
                        },
                        {
                            "start": 414,
                            "end": 436,
                            "matchedPaperCorpusId": "4941467"
                        },
                        {
                            "start": 586,
                            "end": 607,
                            "matchedPaperCorpusId": "1210515"
                        },
                        {
                            "start": 1061,
                            "end": 1079,
                            "matchedPaperCorpusId": "153312532"
                        },
                        {
                            "start": 1182,
                            "end": 1204,
                            "matchedPaperCorpusId": "22604822"
                        },
                        {
                            "start": 1420,
                            "end": 1441,
                            "matchedPaperCorpusId": "52967399"
                        },
                        {
                            "start": 1908,
                            "end": 1928,
                            "matchedPaperCorpusId": "5034059"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.71630859375
                },
                {
                    "corpus_id": "227231089",
                    "title": "Go Simple and Pre-Train on Domain-Specific Corpora: On the Role of Training Data for Text Classification",
                    "text": "Datasets. For our experiments we selected a suite of datasets with different domains and nature. These are: SemEval 2016 task on sentiment analysis (Nakov et al., 2019), SemEval 2018 task on emoji predic-tion (Barbieri et al., 2018), AG News (Zhang et al., 2015), Newsgroups (Lang, 1995) and IMDB (Maas et al., 2011). The main features and statistics of each dataset are summarized in Table 1 Comparison models. As mentioned in Section 2, our evaluation is focused on fastText (Joulin et al., 2017, FT) and BERT (Devlin et al., 2019). For completeness we include a simple baseline based on frequency-based features and a suite of classification algorithms available in the Scikit-Learn library (Pedregosa et al., 2011), namely Gaussian Naive Bayes (GNB), Logistic Regression and Support Vector Machines (SVM). Of the three, the best results were achieved using Logistic Regression, which is the model we include in this paper as a baseline for our experiments. \n\nTraining. As pre-trained word embeddings we downloaded 300-dimensional fastText embeddings trained on Common Crawl (Bojanowski et al., 2017). In order to learn domain-specific word embedding models we used the corresponding training sets for each dataset, except for the Twitter datasets where we leveraged an existing collection of unlabeled tweets from October 2015 to July 2018 to train 300-dimensional fastText embeddings (Camacho Collados et al., 2020). Word embeddings are then fed as input to a fastText classifier where we used default parameters and softmax as the loss function. As for BERT, we fine-tune it for the classification task using a sequence classifier, a learning rate of 2e-5 and 4 epochs. In particular, we made use of BERT's Hugging Face default transformers implementation for classifying sentences (Wolf et al., 2019) and the hierarchical principles described in Pappagari et al. (2019) for pre-processing long texts before feeding them to BERT.",
                    "score": 0.5180798294610637,
                    "section_title": "Experimental Setting",
                    "char_start_offset": 6392,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 9
                        },
                        {
                            "start": 10,
                            "end": 96
                        },
                        {
                            "start": 97,
                            "end": 317
                        },
                        {
                            "start": 318,
                            "end": 411
                        },
                        {
                            "start": 412,
                            "end": 534
                        },
                        {
                            "start": 535,
                            "end": 809
                        },
                        {
                            "start": 810,
                            "end": 960
                        },
                        {
                            "start": 963,
                            "end": 972
                        },
                        {
                            "start": 973,
                            "end": 1104
                        },
                        {
                            "start": 1105,
                            "end": 1421
                        },
                        {
                            "start": 1422,
                            "end": 1551
                        },
                        {
                            "start": 1552,
                            "end": 1675
                        },
                        {
                            "start": 1676,
                            "end": 1935
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 209,
                            "end": 232,
                            "matchedPaperCorpusId": "44145836"
                        },
                        {
                            "start": 242,
                            "end": 262,
                            "matchedPaperCorpusId": "368182"
                        },
                        {
                            "start": 275,
                            "end": 287,
                            "matchedPaperCorpusId": "1921714"
                        },
                        {
                            "start": 297,
                            "end": 316,
                            "matchedPaperCorpusId": "1428702"
                        },
                        {
                            "start": 512,
                            "end": 533,
                            "matchedPaperCorpusId": "52967399"
                        },
                        {
                            "start": 694,
                            "end": 718,
                            "matchedPaperCorpusId": "10659969"
                        },
                        {
                            "start": 1078,
                            "end": 1103,
                            "matchedPaperCorpusId": "207556454"
                        },
                        {
                            "start": 1389,
                            "end": 1420,
                            "matchedPaperCorpusId": "213824149"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.6728515625
                }
            ],
            "relevance_judgement": 0.9541015625,
            "relevance_judgment_input_expanded": "# Title: Go Simple and Pre-Train on Domain-Specific Corpora: On the Role of Training Data for Text Classification\n# Venue: International Conference on Computational Linguistics\n# Authors: A. Edwards, Jos\u00e9 Camacho-Collados, H\u00e9l\u00e8ne de Ribaupierre, A. Preece\n## Abstract\nPre-trained language models provide the foundations for state-of-the-art performance across a wide range of natural language processing tasks, including text classification. However, most classification datasets assume a large amount labeled data, which is commonly not the case in practical settings. In particular, in this paper we compare the performance of a light-weight linear classifier based on word embeddings, i.e., fastText (Joulin et al., 2017), versus a pre-trained language model, i.e., BERT (Devlin et al., 2019), across a wide range of datasets and classification tasks. In general, results show the importance of domain-specific unlabeled data, both in the form of word embeddings or language models. As for the comparison, BERT outperforms all baselines in standard datasets with large training sets. However, in settings with small training datasets a simple method like fastText coupled with domain-specific word embeddings performs equally well or better than BERT, even when pre-trained on domain-specific data.\n## Introduction\nHowever, to the best of our knowledge there has not been an extensive comparison between such methods and newer pre-trained language models. To this end, we compare the light-weight linear classification model fastText (Joulin et al., 2017), coupled with generic and corpus-specific word embeddings, and the pre-trained language model BERT (Devlin et al., 2019), trained on generic data and domain-specific data. Specifically, we analyze the effect of training size over the performance of the classifiers in settings where such training data is limited, both in few-shot scenarios with a balanced set and keeping the original distributions. In both cases, our results show that a large pre-trained language model may not provide significant gains over a linear model that leverage word embeddings, especially when these belong to the given domain.\n\n## Supervised machine learning models\nLinear models. Linear models such as SVMs or logistic regression coupled with frequency-based handcrafted features have been traditionally used for text classification. Despite their simplicity, they are considered a strong baseline for many text classification tasks (Joachims, 1998;McCallum et al., 1998;Fan et al., 2008), even more recently on noisy corpora such as social media text (C \u00b8\u00f6ltekin and Rama, 2018;Mohammad et al., 2018). In general, however, these methods tend to struggle with OOV (Out-Of-Vocabulary) words, fine-grained distinctions and unbalanced datasets. FastText (Joulin et al., 2017), which is the model evaluated in this paper, partially addresses these issues by integrating a linear model with a rank constraint, allowing sharing parameters among features and classes, and integrates word embeddings that are then averaged into a text representation. \n\nNeural models. Neural models can learn non-linear and complex relationships which makes them a preferable method for many NLP tasks such as sentiment analysis or question answering (Sun et al., 2019). In particular, LSTMs, sometimes in combination with CNNs for text classification (Xiao and Cho, 2016;Pilehvar et al., 2017), enable capturing long-range dependencies in a sequential manner where data is read from only one direction (referred to as the 'unidirectionality constraint'). Recent state-of-the-art language models, such as BERT (Devlin et al., 2019), overcome the unidirectionality constraint by using transformer-based masked language models to learn pre-trained deep bidirectional representations. These pre-trained models leverage generic knowledge on large unlabeled corpora that can then be fine-tuned on the specific task by using the pre-trained parameters. BERT, which is the pretrained language model tested in this paper, has been proved to provide state-of-the-art results in most standard NLP benchmarks (Wang et al., 2019b), including text classification.\n\n## Experimental Setting\nDatasets. For our experiments we selected a suite of datasets with different domains and nature. These are: SemEval 2016 task on sentiment analysis (Nakov et al., 2019), SemEval 2018 task on emoji predic-tion (Barbieri et al., 2018), AG News (Zhang et al., 2015), Newsgroups (Lang, 1995) and IMDB (Maas et al., 2011). The main features and statistics of each dataset are summarized in Table 1 Comparison models. As mentioned in Section 2, our evaluation is focused on fastText (Joulin et al., 2017, FT) and BERT (Devlin et al., 2019). For completeness we include a simple baseline based on frequency-based features and a suite of classification algorithms available in the Scikit-Learn library (Pedregosa et al., 2011), namely Gaussian Naive Bayes (GNB), Logistic Regression and Support Vector Machines (SVM). Of the three, the best results were achieved using Logistic Regression, which is the model we include in this paper as a baseline for our experiments. \n\nTraining. As pre-trained word embeddings we downloaded 300-dimensional fastText embeddings trained on Common Crawl (Bojanowski et al., 2017). In order to learn domain-specific word embedding models we used the corresponding training sets for each dataset, except for the Twitter datasets where we leveraged an existing collection of unlabeled tweets from October 2015 to July 2018 to train 300-dimensional fastText embeddings (Camacho Collados et al., 2020). Word embeddings are then fed as input to a fastText classifier where we used default parameters and softmax as the loss function. As for BERT, we fine-tune it for the classification task using a sequence classifier, a learning rate of 2e-5 and 4 epochs. In particular, we made use of BERT's Hugging Face default transformers implementation for classifying sentences (Wolf et al., 2019) and the hierarchical principles described in Pappagari et al. (2019) for pre-processing long texts before feeding them to BERT.\n\n## Effect of training size\nTable 2 shows the results with different sizes of training data randomly extracted from the training set. Surprisingly, classification models based on corpus-trained embeddings achieve higher performance with less labelled data compared to the classifier based on pre-trained contextualised models. However, for cases with more than 5,000 training samples, the performance of fine-tuned BERT significantly outperforms fastText corpus-based classifier, especially when domain-trained BERT model (i.e., BERT (Twitter)) is used. Further to that, the fine-tuned model performance improves at a higher rate than the classifier based on corpus-trained embeddings for training sets with more than 2,000 instances. For instance, for the SE-18 dataset, fastText with domain embeddings improves 0.112 micro-F1 points when the entire dataset is used with respect to using only 200 instances, while BERT-Twitter provides a 0.360 absolute improvement. In contrast, fastText with pre-trained embeddings performs similarly to the baseline. This shows the advantage for pre-trained models to be fine-tuned to the given domain and task.\n\n## instances per label\n5 instances per label 10 instances per label FT(gen) FT(dom) BERT(gen) BERT(T) FT(gen) FT(dom) BERT(gen) BERT(T) FT(gen) FT(dom) BERT(gen) BERT(T)  . Similarly to the experiments with randomized data samples, fastText based on corpus-trained embeddings is the best performing classification model for very small amounts of balanced labeled data (see Figure 2). However, as the amount of training data increases, BERT model outperforms fastText on average by 0.0442%. As in the previous experiment, the classification model based on pre-trained embeddings perform poorly compared to the corpus-trained embeddings and models fine-tuned to the task. Further, BERT (Twitter) leads to significant improvements over BERT when only 10 instances per label are used (i.e., for SE-16, BERT (Twitter) has macro-F1 = 0.370, similar to domain-based fastText with macro-F1 = 0.384 versus base BERT with macro-F1 = 0.200).\n\n## Conclusion and Future Work\nIn this paper, we analyzed the role of training and unlabeled domain-specific data in supervised text classification. We compared both linear and neural models based on transformer-based language models. In settings with small training data, a simple method such as fastText coupled with domain-specific word embeddings appear to be more robust than a more data-consuming model such as BERT, even when BERT is pre-trained on domain-relevant data. However, the same classifier with generic pretrained word embeddings does not perform consistently better than a traditional frequency-based linear baseline. 4 BERT, pre-trained on domain-specific data (i.e., Twitter) leads to improvements over generic BERT, especially for few-shot experiments. For future work it would be interesting to further delve into the role of unlabeled data in text classification, both in terms of word embeddings (e.g., by making use of meta-embeddings (Yin and Sch\u00fctze, 2016)) and the data used to train language models (Gururangan et al., 2020). Moreover, this quantitative analysis could be extended to more classification tasks and different models, e.g., larger language models such as RoBERTa (Liu et al., 2019) and GPT-3 (Brown et al., 2020), which appear to be more suited to few-shot experiments. However, the generic domain embeddings tend to fail to represent the meaning of more domainspecific words, which may explain their lower performance. This is confirmed by the nearest neighbour analysis (see Table 5) which showed that the generic domain embeddings do not provide accurate representations of more technical words such as 'Windows' and 'Sun'. In the IMDB reviews, words such as 'Toothless', used within a very specific context are also not correctly represented by the generic model. Moreover, tweets are rich with abbreviations which have domain-specific meaning such as 'SF' referring to 'San Francisco'.",
            "reference_string": "[227231089 | Edwards et al. | 2020 | Citations: 25]"
        },
        {
            "title": "Non-Contextual BERT or FastText? A Comparative Analysis",
            "venue": "",
            "year": 2024,
            "reference_count": 23,
            "citation_count": 0,
            "influential_citation_count": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2411.17661, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2308480949",
                    "name": "Abhay Gajanan Shanbhag"
                },
                {
                    "authorId": "2308481744",
                    "name": "Suramya Jadhav"
                },
                {
                    "authorId": "2329035171",
                    "name": "Amogh Thakurdesai"
                },
                {
                    "authorId": "2296717726",
                    "name": "Ridhima Sinare"
                },
                {
                    "authorId": "2332349659",
                    "name": "Raviraj Joshi"
                }
            ],
            "abstract": "Natural Language Processing (NLP) for low-resource languages, which lack large annotated datasets, faces significant challenges due to limited high-quality data and linguistic resources. The selection of embeddings plays a critical role in achieving strong performance in NLP tasks. While contextual BERT embeddings require a full forward pass, non-contextual BERT embeddings rely only on table lookup. Existing research has primarily focused on contextual BERT embeddings, leaving non-contextual embeddings largely unexplored. In this study, we analyze the effectiveness of non-contextual embeddings from BERT models (MuRIL and MahaBERT) and FastText models (IndicFT and MahaFT) for tasks such as news classification, sentiment analysis, and hate speech detection in one such low-resource language Marathi. We compare these embeddings with their contextual and compressed variants. Our findings indicate that non-contextual BERT embeddings extracted from the model's first embedding layer outperform FastText embeddings, presenting a promising alternative for low-resource NLP.",
            "corpus_id": 274280560,
            "sentences": [
                {
                    "corpus_id": "274280560",
                    "title": "Non-Contextual BERT or FastText? A Comparative Analysis",
                    "text": "To enhance the comparison, we replicate the experiments using widely utilized contextual BERT embeddings. We also evaluate the impact of compression on both contextual and non-contextual BERT-based embeddings. Our analysis shows that non-contextual BERT embeddings generally perform better than FastText in most tasks. Furthermore, contextual BERT embeddings consistently outperform Fast-Text across all evaluated tasks. However, compressing non-contextual embeddings reduces their performance, making FastText more effective than compressed non-contextual BERT. \n\nThe key contributions of this work are as follows: \n\n\u2022 We conduct a detailed study comparing noncontextual BERT embeddings and FastText embeddings for Marathi, a low-resource language. The evaluation covers multiple classification tasks, including sentiment analysis, news classification, and hate speech detection. \n\n\u2022 To ensure a fair comparison, we compress BERT embeddings from 768 to 300 dimensions using Singular Value Decomposition (SVD). This allows us to analyze how dimensionality reduction impacts BERT's performance compared to its uncompressed version and FastText. \n\n\u2022 We explore the differences between contextual and non-contextual BERT embeddings, examining their impact on classification performance in low-resource settings. \n\nThe paper is organized as follows: Section 2 provides a concise review of prior research on Fast-Text and BERT. Section 3 includes the datasets and model embeddings that are utilized for the experiments. Section 4 presents the methodology used. Section 5 presents the results and key insights drawn from the findings along with a comparative analysis of FastText embeddings and BERT. In Section 6, we analyze our results and explain the reasons behind them. In Section 7, we conclude our discussion.",
                    "score": 0.5615528190707377,
                    "section_title": "Introduction",
                    "char_start_offset": 3764,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 105
                        },
                        {
                            "start": 106,
                            "end": 209
                        },
                        {
                            "start": 210,
                            "end": 318
                        },
                        {
                            "start": 319,
                            "end": 420
                        },
                        {
                            "start": 421,
                            "end": 562
                        },
                        {
                            "start": 565,
                            "end": 615
                        },
                        {
                            "start": 618,
                            "end": 749
                        },
                        {
                            "start": 750,
                            "end": 880
                        },
                        {
                            "start": 883,
                            "end": 1010
                        },
                        {
                            "start": 1011,
                            "end": 1143
                        },
                        {
                            "start": 1146,
                            "end": 1308
                        },
                        {
                            "start": 1311,
                            "end": 1422
                        },
                        {
                            "start": 1423,
                            "end": 1514
                        },
                        {
                            "start": 1515,
                            "end": 1555
                        },
                        {
                            "start": 1556,
                            "end": 1694
                        },
                        {
                            "start": 1695,
                            "end": 1768
                        },
                        {
                            "start": 1769,
                            "end": 1810
                        }
                    ],
                    "ref_mentions": [],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.93896484375
                },
                {
                    "corpus_id": "274280560",
                    "title": "Non-Contextual BERT or FastText? A Comparative Analysis",
                    "text": "Word embedding is a way of representing words into dense vectors in a continuous space such that the vectors capture the semantic relationship between the words for the models to understand the context and meaning of the text. FastText, a context-independent method, basically captures the subword information, enabling it to learn rare words, misspelled words, and out-of-vocabulary words. It is recognized in the NLP community for its efficient performance in tasks like text classification and sentiment analysis. Despite being relatively old, it still remains one of the most effective alternatives when performing tasks on large datasets across various languages due to its subword-based approach. \n\nBERT (Bidirectional Encoder Representations from Transformers) (Devlin et al., 2018) word embeddings understand the meaning of a word based on its context in a sentence. The embeddings extracted just before the first embedding layer of the BERT architecture are referred to as non-contextual embeddings, while those obtained from the last hidden layer of BERT are known as contextual embeddings (Refer Fig 1). Numerous variations of BERT like IndicBERT (Kakwani et al., 2020), MuRIL (Khanuja et al., 2021), AfriBERT (Ralethe, 2020), and mBERT (Devlin et al., 2018) to name a few, are available for experiments. \n\nRecent studies have experimented with both Fast-Text and BERT for various tasks; however, most of them focus on exploring contextual BERT embeddings. Experiments of D 'Sa et al. (2020) demonstrated that BERT embeddings outperformed Fast-Text for classifying English text into toxic and nontoxic. Findings of Ahmed et al. (2024) suggested that BERT embeddings outperformed those of Fast-Text with an F1 score of 84% when evaluated for depressive post-detection in Bangla. \n\nWhile BERT consistently outperforms other word embeddings in various tasks for high-resource languages (HRLs) like English (Malik et al. (2021)), its effectiveness in low-resource languages (LRLs) remains relatively underexplored.",
                    "score": 0.4436409057270061,
                    "section_title": "Introduction",
                    "char_start_offset": 15,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 226
                        },
                        {
                            "start": 227,
                            "end": 390
                        },
                        {
                            "start": 391,
                            "end": 516
                        },
                        {
                            "start": 517,
                            "end": 702
                        },
                        {
                            "start": 705,
                            "end": 874
                        },
                        {
                            "start": 875,
                            "end": 1114
                        },
                        {
                            "start": 1115,
                            "end": 1315
                        },
                        {
                            "start": 1318,
                            "end": 1467
                        },
                        {
                            "start": 1468,
                            "end": 1613
                        },
                        {
                            "start": 1614,
                            "end": 1788
                        },
                        {
                            "start": 1791,
                            "end": 2021
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 1158,
                            "end": 1180,
                            "matchedPaperCorpusId": "267935553"
                        },
                        {
                            "start": 1221,
                            "end": 1236,
                            "matchedPaperCorpusId": "218973721"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.8251953125
                },
                {
                    "corpus_id": "274280560",
                    "title": "Non-Contextual BERT or FastText? A Comparative Analysis",
                    "text": "While BERT consistently outperforms other word embeddings in various tasks for high-resource languages (HRLs) like English (Malik et al. (2021)), its effectiveness in low-resource languages (LRLs) remains relatively underexplored. This gap is particularly pronounced when balancing model performance with computational efficiency, which becomes a critical factor in low-resource settings. \n\nPrevious studies (D'Sa et al. (2020)) have focused on contextual BERT embeddings, which outperform FastText due to their ability to capture contextual information. However, the use of non-contextual BERT embeddings for classification tasks in low-resource languages like Marathi remains unexplored. Unlike contextual embeddings, which require a full forward pass through the model, non-contextual embeddings can be obtained through a simple table lookup. To our knowledge, no prior work has examined the effectiveness of non-contextual BERT embeddings. We investigate how these embeddings, extracted from the model's first layer, compare to FastText embeddings for tasks such as news classification, sentiment analysis, and hate speech detection in Marathi. \n\nAdditionally, past comparisons often used BERT's 768-dimensional embeddings against Fast-Text's 300-dimensional ones, which is unfair since higher dimensions naturally provide better feature extraction. To address this, we ensure a fair comparison by reducing the BERT embeddings to 300 dimensions. \n\nThis paper focuses on utilizing FastText and non-contextual BERT for the Marathi language for the following tasks: Sentiment Classification, 2-Class and 4-Class Hate Speech Detection, and News Article Classification for headlines, long paragraphs, and long documents. We construct a comprehensive analysis of FastText embeddings, IndicFT (Kakwani et al., 2020) and MahaFT (Joshi, 2022a) embeddings, and BERT embeddings, including muril-base-cased (Khanuja et al., 2021) and marathi-bert-v2 (Joshi, 2022a). To enhance the comparison, we replicate the experiments using widely utilized contextual BERT embeddings.",
                    "score": 0.4705380476992397,
                    "section_title": "Introduction",
                    "char_start_offset": 1806,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 230
                        },
                        {
                            "start": 231,
                            "end": 388
                        },
                        {
                            "start": 391,
                            "end": 554
                        },
                        {
                            "start": 555,
                            "end": 689
                        },
                        {
                            "start": 690,
                            "end": 845
                        },
                        {
                            "start": 846,
                            "end": 943
                        },
                        {
                            "start": 944,
                            "end": 1148
                        },
                        {
                            "start": 1151,
                            "end": 1353
                        },
                        {
                            "start": 1354,
                            "end": 1449
                        },
                        {
                            "start": 1452,
                            "end": 1719
                        },
                        {
                            "start": 1720,
                            "end": 1957
                        },
                        {
                            "start": 1958,
                            "end": 2063
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 123,
                            "end": 143,
                            "matchedPaperCorpusId": "233990897"
                        },
                        {
                            "start": 408,
                            "end": 427,
                            "matchedPaperCorpusId": "213529509"
                        },
                        {
                            "start": 1790,
                            "end": 1812,
                            "matchedPaperCorpusId": "267935553"
                        },
                        {
                            "start": 1824,
                            "end": 1838,
                            "matchedPaperCorpusId": "246473294"
                        },
                        {
                            "start": 1942,
                            "end": 1956,
                            "matchedPaperCorpusId": "246473294"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.7138671875
                }
            ],
            "relevance_judgement": 0.93896484375,
            "relevance_judgment_input_expanded": "# Title: Non-Contextual BERT or FastText? A Comparative Analysis\n# Venue: \n# Authors: Abhay Gajanan Shanbhag, Suramya Jadhav, Amogh Thakurdesai, Ridhima Sinare, Raviraj Joshi\n## Abstract\nNatural Language Processing (NLP) for low-resource languages, which lack large annotated datasets, faces significant challenges due to limited high-quality data and linguistic resources. The selection of embeddings plays a critical role in achieving strong performance in NLP tasks. While contextual BERT embeddings require a full forward pass, non-contextual BERT embeddings rely only on table lookup. Existing research has primarily focused on contextual BERT embeddings, leaving non-contextual embeddings largely unexplored. In this study, we analyze the effectiveness of non-contextual embeddings from BERT models (MuRIL and MahaBERT) and FastText models (IndicFT and MahaFT) for tasks such as news classification, sentiment analysis, and hate speech detection in one such low-resource language Marathi. We compare these embeddings with their contextual and compressed variants. Our findings indicate that non-contextual BERT embeddings extracted from the model's first embedding layer outperform FastText embeddings, presenting a promising alternative for low-resource NLP.\n## Introduction\nWord embedding is a way of representing words into dense vectors in a continuous space such that the vectors capture the semantic relationship between the words for the models to understand the context and meaning of the text. FastText, a context-independent method, basically captures the subword information, enabling it to learn rare words, misspelled words, and out-of-vocabulary words. It is recognized in the NLP community for its efficient performance in tasks like text classification and sentiment analysis. Despite being relatively old, it still remains one of the most effective alternatives when performing tasks on large datasets across various languages due to its subword-based approach. \n\nBERT (Bidirectional Encoder Representations from Transformers) (Devlin et al., 2018) word embeddings understand the meaning of a word based on its context in a sentence. The embeddings extracted just before the first embedding layer of the BERT architecture are referred to as non-contextual embeddings, while those obtained from the last hidden layer of BERT are known as contextual embeddings (Refer Fig 1). Numerous variations of BERT like IndicBERT (Kakwani et al., 2020), MuRIL (Khanuja et al., 2021), AfriBERT (Ralethe, 2020), and mBERT (Devlin et al., 2018) to name a few, are available for experiments. \n\nRecent studies have experimented with both Fast-Text and BERT for various tasks; however, most of them focus on exploring contextual BERT embeddings. Experiments of D 'Sa et al. (2020) demonstrated that BERT embeddings outperformed Fast-Text for classifying English text into toxic and nontoxic. Findings of Ahmed et al. (2024) suggested that BERT embeddings outperformed those of Fast-Text with an F1 score of 84% when evaluated for depressive post-detection in Bangla. \n\nWhile BERT consistently outperforms other word embeddings in various tasks for high-resource languages (HRLs) like English (Malik et al. (2021)), its effectiveness in low-resource languages (LRLs) remains relatively underexplored.\n...\nWhile BERT consistently outperforms other word embeddings in various tasks for high-resource languages (HRLs) like English (Malik et al. (2021)), its effectiveness in low-resource languages (LRLs) remains relatively underexplored. This gap is particularly pronounced when balancing model performance with computational efficiency, which becomes a critical factor in low-resource settings. \n\nPrevious studies (D'Sa et al. (2020)) have focused on contextual BERT embeddings, which outperform FastText due to their ability to capture contextual information. However, the use of non-contextual BERT embeddings for classification tasks in low-resource languages like Marathi remains unexplored. Unlike contextual embeddings, which require a full forward pass through the model, non-contextual embeddings can be obtained through a simple table lookup. To our knowledge, no prior work has examined the effectiveness of non-contextual BERT embeddings. We investigate how these embeddings, extracted from the model's first layer, compare to FastText embeddings for tasks such as news classification, sentiment analysis, and hate speech detection in Marathi. \n\nAdditionally, past comparisons often used BERT's 768-dimensional embeddings against Fast-Text's 300-dimensional ones, which is unfair since higher dimensions naturally provide better feature extraction. To address this, we ensure a fair comparison by reducing the BERT embeddings to 300 dimensions. \n\nThis paper focuses on utilizing FastText and non-contextual BERT for the Marathi language for the following tasks: Sentiment Classification, 2-Class and 4-Class Hate Speech Detection, and News Article Classification for headlines, long paragraphs, and long documents. We construct a comprehensive analysis of FastText embeddings, IndicFT (Kakwani et al., 2020) and MahaFT (Joshi, 2022a) embeddings, and BERT embeddings, including muril-base-cased (Khanuja et al., 2021) and marathi-bert-v2 (Joshi, 2022a). To enhance the comparison, we replicate the experiments using widely utilized contextual BERT embeddings.\n...\nTo enhance the comparison, we replicate the experiments using widely utilized contextual BERT embeddings. We also evaluate the impact of compression on both contextual and non-contextual BERT-based embeddings. Our analysis shows that non-contextual BERT embeddings generally perform better than FastText in most tasks. Furthermore, contextual BERT embeddings consistently outperform Fast-Text across all evaluated tasks. However, compressing non-contextual embeddings reduces their performance, making FastText more effective than compressed non-contextual BERT. \n\nThe key contributions of this work are as follows: \n\n\u2022 We conduct a detailed study comparing noncontextual BERT embeddings and FastText embeddings for Marathi, a low-resource language. The evaluation covers multiple classification tasks, including sentiment analysis, news classification, and hate speech detection. \n\n\u2022 To ensure a fair comparison, we compress BERT embeddings from 768 to 300 dimensions using Singular Value Decomposition (SVD). This allows us to analyze how dimensionality reduction impacts BERT's performance compared to its uncompressed version and FastText. \n\n\u2022 We explore the differences between contextual and non-contextual BERT embeddings, examining their impact on classification performance in low-resource settings. \n\nThe paper is organized as follows: Section 2 provides a concise review of prior research on Fast-Text and BERT. Section 3 includes the datasets and model embeddings that are utilized for the experiments. Section 4 presents the methodology used. Section 5 presents the results and key insights drawn from the findings along with a comparative analysis of FastText embeddings and BERT. In Section 6, we analyze our results and explain the reasons behind them. In Section 7, we conclude our discussion.",
            "reference_string": "[274280560 | Shanbhag et al. | 2024 | Citations: 0]"
        },
        {
            "title": "Towards Better Query Classification with Multi-Expert Knowledge Condensation in JD Ads Search",
            "venue": "arXiv.org",
            "year": 2023,
            "reference_count": 49,
            "citation_count": 0,
            "influential_citation_count": 0,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://arxiv.org/pdf/2308.01098",
                "status": "CLOSED",
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2308.01098, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "66266326",
                    "name": "Kun-Peng Ning"
                },
                {
                    "authorId": "2053434618",
                    "name": "Ming Pang"
                },
                {
                    "authorId": "2072874946",
                    "name": "Zheng Fang"
                },
                {
                    "authorId": "2226458237",
                    "name": "Xue Jiang"
                },
                {
                    "authorId": "2226511841",
                    "name": "Xi-Wei Zhao"
                },
                {
                    "authorId": "1949218014",
                    "name": "Changping Peng"
                },
                {
                    "authorId": "2146396439",
                    "name": "Zhangang Lin"
                },
                {
                    "authorId": "22528137",
                    "name": "Jinghe Hu"
                },
                {
                    "authorId": "2118926502",
                    "name": "Jingping Shao"
                }
            ],
            "abstract": "Search query classification, as an effective way to understand user intents, is of great importance in real-world online ads systems. To ensure a lower latency, a shallow model (e.g. FastText) is widely used for efficient online inference. However, the representation ability of the FastText model is insufficient, resulting in poor classification performance, especially on some low-frequency queries and tailed categories. Using a deeper and more complex model (e.g. BERT) is an effective solution, but it will cause a higher online inference latency and more expensive computing costs. Thus, how to juggle both inference efficiency and classification performance is obviously of great practical importance. To overcome this challenge, in this paper, we propose knowledge condensation (KC), a simple yet effective knowledge distillation framework to boost the classification performance of the online FastText model under strict low latency constraints. Specifically, we propose to train an offline BERT model to retrieve more potentially relevant data. Benefiting from its powerful semantic representation, more relevant labels not exposed in the historical data will be added into the training set for better FastText model training. Moreover, a novel distribution-diverse multi-expert learning strategy is proposed to further improve the mining ability of relevant data. By training multiple BERT models from different data distributions, it can respectively perform better at high, middle, and low-frequency search queries. The model ensemble from multi-distribution makes its retrieval ability more powerful. We have deployed two versions of this framework in JD search, and both offline experiments and online A/B testing from multiple datasets have validated the effectiveness of the proposed approach.",
            "corpus_id": 260379057,
            "sentences": [
                {
                    "corpus_id": "260379057",
                    "title": "Towards Better Query Classification with Multi-Expert Knowledge Condensation in JD Ads Search",
                    "text": "FastText has been widely used in various text classification tasks. \n\nEspecially in E-commerce search tasks, benefited from its high inference efficiency, the FastText with low online latency and stable performance become one of the most popular classification models in the industry. On the other hand, the BERT model has also achieved great success in various natural language processing (NLP) tasks based on its powerful semantic representation. However, there are limited works to investigate the property of both models. Thus we conduct toy experiments on the JD search ads dataset to explore the property of the FastText and the BERT models. Firstly, we collected the training set with nearly 160 million user queries from historical click-through data. Then, we respectively train the FastText and BERT models on this training data. Next, we generate two testing sets from T+0 and T+1 days respectively, and each testing set has nearly 2 million user queries. Importantly, in the testing set of T+0 and T+1 days, 99.89% and 57.47% of test queries have appeared in the training set respectively. Finally, we respectively test both models and record their performance on the T+0 and T+1 days. \n\nTable 1 demonstrates the performance comparison of the Fast-Text and BERT models on various evaluation datasets. We find that the FastText model can do better on the training set and testing set (T+0 day), while the BERT model performs better on the testing set (T+1 day). As 99.89% of test queries on T+0 day have appeared in the training set, it means that the FastText model can be remembered better. On the other hand, as only 57.47% of test queries on T+1 day has exposed in the training set, it means that the BERT model can generalize better on some unseen or low-frequency search query. In other words, in the E-commerce search query classification task, the FastText is better at remembering while the BERT is better at generalization. This empirical finding motivates us to combine the advantages of both models to further improve the online performance. To achieve this goal, we introduce knowledge condensation, a Teacher-Student learning framework illustrated in the following subsection.",
                    "score": 0.648502663941603,
                    "section_title": "A Closer Look on FastText and BERT Models",
                    "char_start_offset": 10344,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 67
                        },
                        {
                            "start": 70,
                            "end": 284
                        },
                        {
                            "start": 285,
                            "end": 448
                        },
                        {
                            "start": 449,
                            "end": 525
                        },
                        {
                            "start": 526,
                            "end": 647
                        },
                        {
                            "start": 648,
                            "end": 759
                        },
                        {
                            "start": 760,
                            "end": 839
                        },
                        {
                            "start": 840,
                            "end": 966
                        },
                        {
                            "start": 967,
                            "end": 1101
                        },
                        {
                            "start": 1102,
                            "end": 1197
                        },
                        {
                            "start": 1200,
                            "end": 1312
                        },
                        {
                            "start": 1313,
                            "end": 1472
                        },
                        {
                            "start": 1473,
                            "end": 1603
                        },
                        {
                            "start": 1604,
                            "end": 1794
                        },
                        {
                            "start": 1795,
                            "end": 1944
                        },
                        {
                            "start": 1945,
                            "end": 2064
                        },
                        {
                            "start": 2065,
                            "end": 2201
                        }
                    ],
                    "ref_mentions": [],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.91552734375
                }
            ],
            "relevance_judgement": 0.91552734375,
            "relevance_judgment_input_expanded": "# Title: Towards Better Query Classification with Multi-Expert Knowledge Condensation in JD Ads Search\n# Venue: arXiv.org\n# Authors: Kun-Peng Ning, Ming Pang, Zheng Fang, Xue Jiang, Xi-Wei Zhao, Changping Peng, Zhangang Lin, Jinghe Hu, Jingping Shao\n## Abstract\nSearch query classification, as an effective way to understand user intents, is of great importance in real-world online ads systems. To ensure a lower latency, a shallow model (e.g. FastText) is widely used for efficient online inference. However, the representation ability of the FastText model is insufficient, resulting in poor classification performance, especially on some low-frequency queries and tailed categories. Using a deeper and more complex model (e.g. BERT) is an effective solution, but it will cause a higher online inference latency and more expensive computing costs. Thus, how to juggle both inference efficiency and classification performance is obviously of great practical importance. To overcome this challenge, in this paper, we propose knowledge condensation (KC), a simple yet effective knowledge distillation framework to boost the classification performance of the online FastText model under strict low latency constraints. Specifically, we propose to train an offline BERT model to retrieve more potentially relevant data. Benefiting from its powerful semantic representation, more relevant labels not exposed in the historical data will be added into the training set for better FastText model training. Moreover, a novel distribution-diverse multi-expert learning strategy is proposed to further improve the mining ability of relevant data. By training multiple BERT models from different data distributions, it can respectively perform better at high, middle, and low-frequency search queries. The model ensemble from multi-distribution makes its retrieval ability more powerful. We have deployed two versions of this framework in JD search, and both offline experiments and online A/B testing from multiple datasets have validated the effectiveness of the proposed approach.\n## A Closer Look on FastText and BERT Models\nFastText has been widely used in various text classification tasks. \n\nEspecially in E-commerce search tasks, benefited from its high inference efficiency, the FastText with low online latency and stable performance become one of the most popular classification models in the industry. On the other hand, the BERT model has also achieved great success in various natural language processing (NLP) tasks based on its powerful semantic representation. However, there are limited works to investigate the property of both models. Thus we conduct toy experiments on the JD search ads dataset to explore the property of the FastText and the BERT models. Firstly, we collected the training set with nearly 160 million user queries from historical click-through data. Then, we respectively train the FastText and BERT models on this training data. Next, we generate two testing sets from T+0 and T+1 days respectively, and each testing set has nearly 2 million user queries. Importantly, in the testing set of T+0 and T+1 days, 99.89% and 57.47% of test queries have appeared in the training set respectively. Finally, we respectively test both models and record their performance on the T+0 and T+1 days. \n\nTable 1 demonstrates the performance comparison of the Fast-Text and BERT models on various evaluation datasets. We find that the FastText model can do better on the training set and testing set (T+0 day), while the BERT model performs better on the testing set (T+1 day). As 99.89% of test queries on T+0 day have appeared in the training set, it means that the FastText model can be remembered better. On the other hand, as only 57.47% of test queries on T+1 day has exposed in the training set, it means that the BERT model can generalize better on some unseen or low-frequency search query. In other words, in the E-commerce search query classification task, the FastText is better at remembering while the BERT is better at generalization. This empirical finding motivates us to combine the advantages of both models to further improve the online performance. To achieve this goal, we introduce knowledge condensation, a Teacher-Student learning framework illustrated in the following subsection.",
            "reference_string": "[260379057 | Ning et al. | 2023 | Citations: 0]"
        },
        {
            "title": "Deep Neural Network and Boosting Based Hybrid Quality Ranking for e-Commerce Product Search",
            "venue": "Big Data and Cognitive Computing",
            "year": 2021,
            "reference_count": 52,
            "citation_count": 6,
            "influential_citation_count": 0,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://www.mdpi.com/2504-2289/5/3/35/pdf?version=1632459839",
                "status": "GOLD",
                "license": "CCBY",
                "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.3390/bdcc5030035?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.3390/bdcc5030035, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "1471736056",
                    "name": "Mourad Jbene"
                },
                {
                    "authorId": "3377410",
                    "name": "Smail Tigani"
                },
                {
                    "authorId": "1916966",
                    "name": "Rachid Saadane"
                },
                {
                    "authorId": "2782896",
                    "name": "Abdellah Chehri"
                }
            ],
            "abstract": "In the age of information overload, customers are overwhelmed with the number of products available for sale. Search engines try to overcome this issue by filtering relevant items to the users\u2019 queries. Traditional search engines rely on the exact match of terms in the query and product meta-data. Recently, deep learning-based approaches grabbed more attention by outperforming traditional methods in many circumstances. In this work, we involve the power of embeddings to solve the challenging task of optimizing product search engines in e-commerce. This work proposes an e-commerce product search engine based on a similarity metric that works on top of query and product embeddings. Two pre-trained word embedding models were tested, the first representing a category of models that generate fixed embeddings and a second representing a newer category of models that generate context-aware embeddings. Furthermore, a re-ranking step was performed by incorporating a list of quality indicators that reflects the utility of the product to the customer as inputs to well-known ranking methods. To prove the reliability of the approach, the Amazon reviews dataset was used for experimentation. The results demonstrated the effectiveness of context-aware embeddings in retrieving relevant products and the quality indicators in ranking high-quality products.",
            "corpus_id": 238648863,
            "sentences": [
                {
                    "corpus_id": "238648863",
                    "title": "Deep Neural Network and Boosting Based Hybrid Quality Ranking for e-Commerce Product Search",
                    "text": "The performance of the product search model is reported in Table 2 in terms of precision, recall, and the F1-score at a cut-off K = 100 results. The model was tested on four datasets, using two types of word embedding representations: FastText and BERT. It can be seen from Table 2 that the retrieval performance of our product search model differs from a dataset to another. The performance is best in terms of Precision@K for the appliances dataset with 0.27, and worst for office products with 0.13, using the model with BERT embeddings. In terms of Recall@K and F1-Score@K, the appliances dataset obtained the best results using both the model with BERT and with FastText embeddings. On the other hand, toys and games achieved the worst Recall@K, and office products the worst F1-Score@K. \n\nComparing the results of the model using the different kinds of embeddings, the version with BERT embeddings generates results higher than FastText ones for all the datasets, which can be explained by the difference in dataset size on which the two models were pre-trained on, and most importantly, the capability of BERT in capturing and generating context-dependent word embeddings. \n\nTo analyze the performance of the sentiment analysis model in detecting the sentiment in reviews, we divided the reviews into two sets 80% for training the model and 20% for validation. The model was trained for 80 epochs, with a batch size of 512 and the Adam optimizer with a learning rate of 0.01, and the optimized AUC (Area Under the Curve) metric, which is commonly used to measure the quality of classification models. These parameter settings were obtained based on a grid-search approach for different values of the parameters. Following, we present plots of the loss and ROC (Receiver Operating Characteristic) curve for both the training and validation sets in Figures 2 and 3. In Figures 2 and 3, the loss and AUC metrics are depicted for both training and validation sets during the training phase of the sentiment analysis model. The loss decreases significantly during the first 20 epochs, while the accuracy increases remarkably during the first 10 epochs.",
                    "score": 0.4226715577254444,
                    "section_title": "Results and Discussions",
                    "char_start_offset": 25359,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 144
                        },
                        {
                            "start": 145,
                            "end": 253
                        },
                        {
                            "start": 254,
                            "end": 375
                        },
                        {
                            "start": 376,
                            "end": 540
                        },
                        {
                            "start": 541,
                            "end": 687
                        },
                        {
                            "start": 688,
                            "end": 792
                        },
                        {
                            "start": 795,
                            "end": 1179
                        },
                        {
                            "start": 1182,
                            "end": 1367
                        },
                        {
                            "start": 1368,
                            "end": 1607
                        },
                        {
                            "start": 1608,
                            "end": 1718
                        },
                        {
                            "start": 1719,
                            "end": 1870
                        },
                        {
                            "start": 1871,
                            "end": 2025
                        },
                        {
                            "start": 2026,
                            "end": 2154
                        }
                    ],
                    "ref_mentions": [],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.84765625
                }
            ],
            "relevance_judgement": 0.84765625,
            "relevance_judgment_input_expanded": "# Title: Deep Neural Network and Boosting Based Hybrid Quality Ranking for e-Commerce Product Search\n# Venue: Big Data and Cognitive Computing\n# Authors: Mourad Jbene, Smail Tigani, Rachid Saadane, Abdellah Chehri\n## Abstract\nIn the age of information overload, customers are overwhelmed with the number of products available for sale. Search engines try to overcome this issue by filtering relevant items to the users\u2019 queries. Traditional search engines rely on the exact match of terms in the query and product meta-data. Recently, deep learning-based approaches grabbed more attention by outperforming traditional methods in many circumstances. In this work, we involve the power of embeddings to solve the challenging task of optimizing product search engines in e-commerce. This work proposes an e-commerce product search engine based on a similarity metric that works on top of query and product embeddings. Two pre-trained word embedding models were tested, the first representing a category of models that generate fixed embeddings and a second representing a newer category of models that generate context-aware embeddings. Furthermore, a re-ranking step was performed by incorporating a list of quality indicators that reflects the utility of the product to the customer as inputs to well-known ranking methods. To prove the reliability of the approach, the Amazon reviews dataset was used for experimentation. The results demonstrated the effectiveness of context-aware embeddings in retrieving relevant products and the quality indicators in ranking high-quality products.\n## Results and Discussions\nThe performance of the product search model is reported in Table 2 in terms of precision, recall, and the F1-score at a cut-off K = 100 results. The model was tested on four datasets, using two types of word embedding representations: FastText and BERT. It can be seen from Table 2 that the retrieval performance of our product search model differs from a dataset to another. The performance is best in terms of Precision@K for the appliances dataset with 0.27, and worst for office products with 0.13, using the model with BERT embeddings. In terms of Recall@K and F1-Score@K, the appliances dataset obtained the best results using both the model with BERT and with FastText embeddings. On the other hand, toys and games achieved the worst Recall@K, and office products the worst F1-Score@K. \n\nComparing the results of the model using the different kinds of embeddings, the version with BERT embeddings generates results higher than FastText ones for all the datasets, which can be explained by the difference in dataset size on which the two models were pre-trained on, and most importantly, the capability of BERT in capturing and generating context-dependent word embeddings. \n\nTo analyze the performance of the sentiment analysis model in detecting the sentiment in reviews, we divided the reviews into two sets 80% for training the model and 20% for validation. The model was trained for 80 epochs, with a batch size of 512 and the Adam optimizer with a learning rate of 0.01, and the optimized AUC (Area Under the Curve) metric, which is commonly used to measure the quality of classification models. These parameter settings were obtained based on a grid-search approach for different values of the parameters. Following, we present plots of the loss and ROC (Receiver Operating Characteristic) curve for both the training and validation sets in Figures 2 and 3. In Figures 2 and 3, the loss and AUC metrics are depicted for both training and validation sets during the training phase of the sentiment analysis model. The loss decreases significantly during the first 20 epochs, while the accuracy increases remarkably during the first 10 epochs.",
            "reference_string": "[238648863 | Jbene et al. | 2021 | Citations: 6]"
        },
        {
            "title": "Empowering Medical Data Analysis: An Advanced Deep Fusion Model for Sorting Medicine Document",
            "venue": "IEEE Access",
            "year": 2024,
            "reference_count": 36,
            "citation_count": 2,
            "influential_citation_count": 0,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://ieeexplore.ieee.org/ielx7/6287639/6514899/10373839.pdf",
                "status": "GOLD",
                "license": "CCBYNCND",
                "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.1109/ACCESS.2023.3347029?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1109/ACCESS.2023.3347029, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2276550856",
                    "name": "Guan Bo"
                },
                {
                    "authorId": "2276574822",
                    "name": "Shanshan Wang"
                },
                {
                    "authorId": "2276553236",
                    "name": "Zhang Qing"
                },
                {
                    "authorId": "2276554085",
                    "name": "Pang Bo"
                },
                {
                    "authorId": "2276746559",
                    "name": "Zuo Yan"
                }
            ],
            "abstract": "To enhance the accuracy of medical document classification, we propose an advanced deep fusion model for sorting medicine document. Specifically, we enhance text representation using the bidirectional encoder representation from transformers (BERT). BERT is a bidirectional model that considers context information in input sequences. This capability is particularly valuable for medical document, as medical information often requires understanding in a global context, such as diagnoses, medical history, and treatment plans. Furthermore, BERT can learn the semantics of words and phrases, comprehending the different meanings of the same word in distinct contexts, which is crucial for representing medical document. For example, In the context of cardiology, stroke often refers to a cerebrovascular accident, which is a condition where blood flow to the brain is disrupted, leading to neurological impairment. This type of stroke is related to the brain and is a significant concern in the field of cardiology due to its impact on the circulatory system. In dermatology, stroke might be used to refer to a type of skin condition, such as stroking the skin. However, this context is less common and not related to the cerebrovascular meaning. Subsequently, we employ both Convolutional Neural Network (ConvNet) and Bidirectional Long Short Term Memory (Bi-LSTM) to extract local features and global long-term dependencies, respectively. Their outputs are then fused to extract useful document features at multiple levels, effectively capturing the documental structure. The proposed deep fusion model leverages the complementary strengths of these components, enhancing the model\u2019s generalization ability and mitigating the risk of over-fitting. Ultimately, by comparing our approach with state-of-the-art methods in medical document classification, we demonstrate the effectiveness of the proposed methodology.",
            "corpus_id": 266566817,
            "sentences": [
                {
                    "corpus_id": "266566817",
                    "title": "Empowering Medical Data Analysis: An Advanced Deep Fusion Model for Sorting Medicine Document",
                    "text": "The fundamental yet essential component of document classification is the transformation of text into numeric vectors, enabling computers to conduct subsequent calculations and processing. This process is known as text representation [13]. Word embedding is a common method for text representation, which embeds words from the text into a space and expresses them in the form of vectors. One-hot, Bag-Of-Words models, TF-IDF, etc., are frequently used text representation methods. However, the aforementioned feature representations disregard the context relationships within the text, treating each word as independent, which fails to capture semantic information. Moreover, these representations can lead to issues of sparsity and dimensionality disaster in the feature matrix. Consequently, the focus of subsequent research has been on constructing low-dimensional, distributed, dense word vectors. Word2Vec [14] is a neural network language model that considers both contextual semantics and avoids dimensionality issues, yielding superior performance compared to earlier models. Additionally, FastText [15], released by Facebook in 2016, is a tool for word vector computation and document classification. In classification tasks, FastText often achieves accuracy comparable to deep neural networks but with faster training times. However, both Word2Vec and FastText are static models since they have a one-to-one correspondence between words and vectors, making them incapable of dynamically adjusting and optimizing for specific tasks, thus unable to address the issue of polysemy. \n\nIn contrast, BERT is a pre-trained language model trained on a large-scale corpus, based on the multi-layer Transformer encoder architecture, utilizing attention mechanisms to directly encode word meanings, effectively addressing the polysemy issue based on contextual information [16]. BERT places greater emphasis on pre-training word meanings, allowing downstream NLP tasks to perform fine-tuning operations based on the specific task's requirements.",
                    "score": 0.4399548832339821,
                    "section_title": "A. WORD EMBEDDING",
                    "char_start_offset": 7853,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 188
                        },
                        {
                            "start": 189,
                            "end": 239
                        },
                        {
                            "start": 240,
                            "end": 387
                        },
                        {
                            "start": 388,
                            "end": 480
                        },
                        {
                            "start": 481,
                            "end": 665
                        },
                        {
                            "start": 666,
                            "end": 779
                        },
                        {
                            "start": 780,
                            "end": 901
                        },
                        {
                            "start": 902,
                            "end": 1083
                        },
                        {
                            "start": 1084,
                            "end": 1209
                        },
                        {
                            "start": 1210,
                            "end": 1334
                        },
                        {
                            "start": 1335,
                            "end": 1587
                        },
                        {
                            "start": 1590,
                            "end": 1876
                        },
                        {
                            "start": 1877,
                            "end": 2043
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 911,
                            "end": 915,
                            "matchedPaperCorpusId": "253828309"
                        },
                        {
                            "start": 1107,
                            "end": 1111,
                            "matchedPaperCorpusId": "259376879"
                        },
                        {
                            "start": 1871,
                            "end": 1875,
                            "matchedPaperCorpusId": "239459434"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.82958984375
                }
            ],
            "relevance_judgement": 0.82958984375,
            "relevance_judgment_input_expanded": "# Title: Empowering Medical Data Analysis: An Advanced Deep Fusion Model for Sorting Medicine Document\n# Venue: IEEE Access\n# Authors: Guan Bo, Shanshan Wang, Zhang Qing, Pang Bo, Zuo Yan\n## Abstract\nTo enhance the accuracy of medical document classification, we propose an advanced deep fusion model for sorting medicine document. Specifically, we enhance text representation using the bidirectional encoder representation from transformers (BERT). BERT is a bidirectional model that considers context information in input sequences. This capability is particularly valuable for medical document, as medical information often requires understanding in a global context, such as diagnoses, medical history, and treatment plans. Furthermore, BERT can learn the semantics of words and phrases, comprehending the different meanings of the same word in distinct contexts, which is crucial for representing medical document. For example, In the context of cardiology, stroke often refers to a cerebrovascular accident, which is a condition where blood flow to the brain is disrupted, leading to neurological impairment. This type of stroke is related to the brain and is a significant concern in the field of cardiology due to its impact on the circulatory system. In dermatology, stroke might be used to refer to a type of skin condition, such as stroking the skin. However, this context is less common and not related to the cerebrovascular meaning. Subsequently, we employ both Convolutional Neural Network (ConvNet) and Bidirectional Long Short Term Memory (Bi-LSTM) to extract local features and global long-term dependencies, respectively. Their outputs are then fused to extract useful document features at multiple levels, effectively capturing the documental structure. The proposed deep fusion model leverages the complementary strengths of these components, enhancing the model\u2019s generalization ability and mitigating the risk of over-fitting. Ultimately, by comparing our approach with state-of-the-art methods in medical document classification, we demonstrate the effectiveness of the proposed methodology.\n## A. WORD EMBEDDING\nThe fundamental yet essential component of document classification is the transformation of text into numeric vectors, enabling computers to conduct subsequent calculations and processing. This process is known as text representation [13]. Word embedding is a common method for text representation, which embeds words from the text into a space and expresses them in the form of vectors. One-hot, Bag-Of-Words models, TF-IDF, etc., are frequently used text representation methods. However, the aforementioned feature representations disregard the context relationships within the text, treating each word as independent, which fails to capture semantic information. Moreover, these representations can lead to issues of sparsity and dimensionality disaster in the feature matrix. Consequently, the focus of subsequent research has been on constructing low-dimensional, distributed, dense word vectors. Word2Vec [14] is a neural network language model that considers both contextual semantics and avoids dimensionality issues, yielding superior performance compared to earlier models. Additionally, FastText [15], released by Facebook in 2016, is a tool for word vector computation and document classification. In classification tasks, FastText often achieves accuracy comparable to deep neural networks but with faster training times. However, both Word2Vec and FastText are static models since they have a one-to-one correspondence between words and vectors, making them incapable of dynamically adjusting and optimizing for specific tasks, thus unable to address the issue of polysemy. \n\nIn contrast, BERT is a pre-trained language model trained on a large-scale corpus, based on the multi-layer Transformer encoder architecture, utilizing attention mechanisms to directly encode word meanings, effectively addressing the polysemy issue based on contextual information [16]. BERT places greater emphasis on pre-training word meanings, allowing downstream NLP tasks to perform fine-tuning operations based on the specific task's requirements.",
            "reference_string": "[266566817 | Bo et al. | 2024 | Citations: 2]"
        },
        {
            "title": "Measuring Bias of Web-filtered Text Datasets and Bias Propagation Through Training",
            "venue": "arXiv.org",
            "year": 2024,
            "reference_count": 49,
            "citation_count": 2,
            "influential_citation_count": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2412.02857, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2294362620",
                    "name": "Youssef Mansour"
                },
                {
                    "authorId": "2294361310",
                    "name": "Reinhard Heckel"
                }
            ],
            "abstract": "We investigate biases in pretraining datasets for large language models (LLMs) through dataset classification experiments. Building on prior work demonstrating the existence of biases in popular computer vision datasets, we analyze popular open-source pretraining datasets for LLMs derived from CommonCrawl including C4, RefinedWeb, DolmaCC, RedPajama-V2, FineWeb, and DCLM-Baseline. Despite those datasets being obtained with similar curation steps, neural networks can classify surprisingly well which dataset a single text sequence belongs to, significantly better than a human can. This indicates that small differences in filtering and processing pipelines induce fingerprints evident in formatting, vocabulary, and content distributions. Those biases remain even when the text is rewritten with LLMs. Moreover, these biases propagate through training: Random sequences generated by models trained on those datasets can be classified well by a classifier trained on the original datasets. This can be leveraged to estimate the pretraining mixture proportions of the data sources.",
            "corpus_id": 274464991,
            "sentences": [
                {
                    "corpus_id": "274464991",
                    "title": "Measuring Bias of Web-filtered Text Datasets and Bias Propagation Through Training",
                    "text": "BERT initially achieves slightly lower accuracy but eventually reaches a marginally higher accuracy. The observation that BERT requires more training sequences is somewhat expected, as the autoregressive transformer has a loss associated with each subsequence, while BERT processes each sequence only once. \n\nFastText classifier: FastText [Jou+16] is an efficient text classification library designed to provide fast and scalable text classification tasks, particularly suitable for classification of largescale datasets. FastText relies on a simple shallow neural network architecture that enables rapid training and inference. Similar to BERT, FastText processes each sequence as a whole. \n\nWe plot FastText's performance as a function of the number of training sequences in Figure 9. The transformer-based classifier and BERT significantly outperform FastText, but are significantly slower, and require significantly more compute. \n\nMajority vote at test time: We classify a given sequence as a whole at test time throughout the paper. In this ablation study, we classify all subsequences of one test sequence, and then determine the final prediction as the majority vote. For instance, a sequence of length n tokens will yield n predictions. The final predicted class is the most frequent on of the individual predictions. Using majority voting reduces accuracy to 67.37%, which is a 7.39% decrease compared to the default whole sequence classification. \n\nAggregating sequences: Throughout the paper, we classify individual sequences. In this ablation study we combine the sequences of the same dataset together to form sequences of length 2048 tokens, aligning with the context length of our transformer model. This creates a uniform test set with sequences of equal length, were each sequence utilizes the entire attention span of the transformer. \n\nThe aggregation of sequences yields an impressive 95.18% classification accuracy, approximately 10% higher than the default sequence based testing with sequences of length 1800-2000 tokens as seen in Figure 8. This suggests that providing the classifier with multiple concatenated sequences simplifies the classification task, making it easier than classifying a single sequence of similar combined length. \n\nLinear probing: Linear probing refers to training a linear classifier on fixed pretrained representations.",
                    "score": 0.4268795056642635,
                    "section_title": "FineWeb-Edu",
                    "char_start_offset": 45277,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 100
                        },
                        {
                            "start": 101,
                            "end": 306
                        },
                        {
                            "start": 309,
                            "end": 521
                        },
                        {
                            "start": 522,
                            "end": 628
                        },
                        {
                            "start": 629,
                            "end": 690
                        },
                        {
                            "start": 693,
                            "end": 786
                        },
                        {
                            "start": 787,
                            "end": 933
                        },
                        {
                            "start": 936,
                            "end": 1038
                        },
                        {
                            "start": 1039,
                            "end": 1175
                        },
                        {
                            "start": 1176,
                            "end": 1245
                        },
                        {
                            "start": 1246,
                            "end": 1326
                        },
                        {
                            "start": 1327,
                            "end": 1457
                        },
                        {
                            "start": 1460,
                            "end": 1538
                        },
                        {
                            "start": 1539,
                            "end": 1715
                        },
                        {
                            "start": 1716,
                            "end": 1853
                        },
                        {
                            "start": 1856,
                            "end": 2065
                        },
                        {
                            "start": 2066,
                            "end": 2262
                        },
                        {
                            "start": 2265,
                            "end": 2371
                        }
                    ],
                    "ref_mentions": [],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.8291015625
                }
            ],
            "relevance_judgement": 0.8291015625,
            "relevance_judgment_input_expanded": "# Title: Measuring Bias of Web-filtered Text Datasets and Bias Propagation Through Training\n# Venue: arXiv.org\n# Authors: Youssef Mansour, Reinhard Heckel\n## Abstract\nWe investigate biases in pretraining datasets for large language models (LLMs) through dataset classification experiments. Building on prior work demonstrating the existence of biases in popular computer vision datasets, we analyze popular open-source pretraining datasets for LLMs derived from CommonCrawl including C4, RefinedWeb, DolmaCC, RedPajama-V2, FineWeb, and DCLM-Baseline. Despite those datasets being obtained with similar curation steps, neural networks can classify surprisingly well which dataset a single text sequence belongs to, significantly better than a human can. This indicates that small differences in filtering and processing pipelines induce fingerprints evident in formatting, vocabulary, and content distributions. Those biases remain even when the text is rewritten with LLMs. Moreover, these biases propagate through training: Random sequences generated by models trained on those datasets can be classified well by a classifier trained on the original datasets. This can be leveraged to estimate the pretraining mixture proportions of the data sources.\n## FineWeb-Edu\nBERT initially achieves slightly lower accuracy but eventually reaches a marginally higher accuracy. The observation that BERT requires more training sequences is somewhat expected, as the autoregressive transformer has a loss associated with each subsequence, while BERT processes each sequence only once. \n\nFastText classifier: FastText [Jou+16] is an efficient text classification library designed to provide fast and scalable text classification tasks, particularly suitable for classification of largescale datasets. FastText relies on a simple shallow neural network architecture that enables rapid training and inference. Similar to BERT, FastText processes each sequence as a whole. \n\nWe plot FastText's performance as a function of the number of training sequences in Figure 9. The transformer-based classifier and BERT significantly outperform FastText, but are significantly slower, and require significantly more compute. \n\nMajority vote at test time: We classify a given sequence as a whole at test time throughout the paper. In this ablation study, we classify all subsequences of one test sequence, and then determine the final prediction as the majority vote. For instance, a sequence of length n tokens will yield n predictions. The final predicted class is the most frequent on of the individual predictions. Using majority voting reduces accuracy to 67.37%, which is a 7.39% decrease compared to the default whole sequence classification. \n\nAggregating sequences: Throughout the paper, we classify individual sequences. In this ablation study we combine the sequences of the same dataset together to form sequences of length 2048 tokens, aligning with the context length of our transformer model. This creates a uniform test set with sequences of equal length, were each sequence utilizes the entire attention span of the transformer. \n\nThe aggregation of sequences yields an impressive 95.18% classification accuracy, approximately 10% higher than the default sequence based testing with sequences of length 1800-2000 tokens as seen in Figure 8. This suggests that providing the classifier with multiple concatenated sequences simplifies the classification task, making it easier than classifying a single sequence of similar combined length. \n\nLinear probing: Linear probing refers to training a linear classifier on fixed pretrained representations.",
            "reference_string": "[274464991 | Mansour et al. | 2024 | Citations: 2]"
        },
        {
            "title": "Extracting Semantic Topics About Development in Africa From Social Media",
            "venue": "IEEE Access",
            "year": 2024,
            "reference_count": 88,
            "citation_count": 2,
            "influential_citation_count": 0,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://doi.org/10.1109/access.2024.3466834",
                "status": "GOLD",
                "license": "CCBYNCND",
                "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.1109/ACCESS.2024.3466834?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1109/ACCESS.2024.3466834, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2309097417",
                    "name": "Harriet Sibitenda"
                },
                {
                    "authorId": "2819839",
                    "name": "Awa Diattara"
                },
                {
                    "authorId": "2062458540",
                    "name": "A. Traor\u00e9"
                },
                {
                    "authorId": "2150192474",
                    "name": "Ruofan Hu"
                },
                {
                    "authorId": "2140415398",
                    "name": "Dongyu Zhang"
                },
                {
                    "authorId": "2260740400",
                    "name": "E. Rundensteiner"
                },
                {
                    "authorId": "2264055757",
                    "name": "Cheikh Ba"
                }
            ],
            "abstract": "The extraction of knowledge about prevalent issues discussed on social media in Africa using Artificial Intelligence techniques is vital for informing public governance. The objectives of our study are twofold: (a) to develop machine learning-based models to identify common topics of social concern related to Africa on social media, and (b) to design a classifier capable of inferring the most relevant topic associated with a given social media post. We designed a three-step framework to achieve the first goal of topic identification. The first step applies text-based representation learning methods to generate text embeddings for feature representation. The second step utilizes state-of-the-art Natural Language Processing models, commonly referred to as topic modeling, to group the representations into categories. The third step generates topics from each group, leveraging large language models to create meaningful short-sentence labels from the associated bag-of-tokens. Additionally, we used Llama2 to refine the token words into concise single-word themes that describe each topic in relation to social concerns about development. To address the second goal of classification, we trained classifiers using ensemble voting and stacking methods to determine which of the identified topics best characterizes a given social media post. For our experimental study, we collected a corpus called Social Media for Africa (SMA), consisting of 22,036 records extracted from comments on Twitter (X) and YouTube. The clustering-based model BERTopic produced 304 topics with a topic coherence score of 0.81 (C-v). After merging the topics into broader classes, the BERTopic+ model yielded 11 common topic classes with a coherence score of 0.76 (C-v). For theme extraction, we further refined the leading token words using Llama2, resulting in 98 unique themes labeled by BERTopic_theme, with a coherence score of 0.75 and an IRBO score of 0.50. We used the identified topics, based on the groupings, as labels for training a topic classifier. These labels were generated using Llama2 on our SMA corpus. Our comparative study of topic classifiers employing stacking and voting schemes demonstrated that the BERTopic model achieved an accuracy of 0.83 and an F1 score of 0.82 with ensemble voting for training on topics. Furthermore, when training on topic classes, BERTopic+ with ensemble voting achieved the highest accuracy (0.95) and F1 score (0.95) compared to other methods. Additionally, BERTopic_theme achieved superior performance with an ensemble voting classifier, attaining an F1 score of 0.93 and an accuracy of 0.93. The overall performance of classifiers using ensemble stacking was slightly better than that of voting methods for short-sentence topic labeling. For Africa, policymakers should focus on the most pressing social issues: the impact of COVID-19 restrictions on public health and economic recovery, promoting entrepreneurial innovation in energy and environmental sustainability to combat climate change, and responding strategically to China\u2019s rise in global politics to maintain geopolitical stability and foster international cooperation.",
            "corpus_id": 270862694,
            "sentences": [
                {
                    "corpus_id": "270862694",
                    "title": "Extracting Semantic Topics About Development in Africa From Social Media",
                    "text": "FastText is based on Bojanowski's SG model, which treats each word as a bag of character n-grams [54]. It has improved the representation of out-of-vocabulary words and works well with large corpora. The limitations of FastText include ignoring the grammatical sense of words. Our study compared the embeddings of Word2Vec-SMA, pretrained Word2Vec, and FastText. \n\nContextualized Word Embeddings are words based on the meanings derived from the sentences in which they appear. Language model-driven embeddings are contextual; they consider the terms surrounding the target words to generate linguistically-based representations [55]. Contextualized embeddings are widely used in various NLP applications, including classification, question-answering, and summarization. The most popular models for generating dynamic word embeddings are from the Bidirectional Encoder Representation Transformer (BERT) family. BERT uses self-supervised learning to learn the contextual connections between words without labeled data. In the pre-training phase, BERT utilizes two main strategies: the Masked Language Model (MLM) and Next Sentence Prediction (NSP) to learn the grammatical semantics of sentences. In our study, we used the pretrained sentence BERT embeddings called \"paraphrase-MiniLM-L6-v2,\" which supports paraphrasing text for NLP tasks with efficiency and high computation power [56].",
                    "score": 0.4625721616208924,
                    "section_title": "Wording Embeddings",
                    "char_start_offset": 23486,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 102
                        },
                        {
                            "start": 103,
                            "end": 199
                        },
                        {
                            "start": 200,
                            "end": 276
                        },
                        {
                            "start": 277,
                            "end": 362
                        },
                        {
                            "start": 365,
                            "end": 476
                        },
                        {
                            "start": 477,
                            "end": 633
                        },
                        {
                            "start": 634,
                            "end": 769
                        },
                        {
                            "start": 770,
                            "end": 909
                        },
                        {
                            "start": 910,
                            "end": 1016
                        },
                        {
                            "start": 1017,
                            "end": 1194
                        },
                        {
                            "start": 1195,
                            "end": 1386
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 97,
                            "end": 101,
                            "matchedPaperCorpusId": "244833681"
                        },
                        {
                            "start": 1381,
                            "end": 1385,
                            "matchedPaperCorpusId": "52967399"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.82421875
                }
            ],
            "relevance_judgement": 0.82421875,
            "relevance_judgment_input_expanded": "# Title: Extracting Semantic Topics About Development in Africa From Social Media\n# Venue: IEEE Access\n# Authors: Harriet Sibitenda, Awa Diattara, A. Traor\u00e9, Ruofan Hu, Dongyu Zhang, E. Rundensteiner, Cheikh Ba\n## Abstract\nThe extraction of knowledge about prevalent issues discussed on social media in Africa using Artificial Intelligence techniques is vital for informing public governance. The objectives of our study are twofold: (a) to develop machine learning-based models to identify common topics of social concern related to Africa on social media, and (b) to design a classifier capable of inferring the most relevant topic associated with a given social media post. We designed a three-step framework to achieve the first goal of topic identification. The first step applies text-based representation learning methods to generate text embeddings for feature representation. The second step utilizes state-of-the-art Natural Language Processing models, commonly referred to as topic modeling, to group the representations into categories. The third step generates topics from each group, leveraging large language models to create meaningful short-sentence labels from the associated bag-of-tokens. Additionally, we used Llama2 to refine the token words into concise single-word themes that describe each topic in relation to social concerns about development. To address the second goal of classification, we trained classifiers using ensemble voting and stacking methods to determine which of the identified topics best characterizes a given social media post. For our experimental study, we collected a corpus called Social Media for Africa (SMA), consisting of 22,036 records extracted from comments on Twitter (X) and YouTube. The clustering-based model BERTopic produced 304 topics with a topic coherence score of 0.81 (C-v). After merging the topics into broader classes, the BERTopic+ model yielded 11 common topic classes with a coherence score of 0.76 (C-v). For theme extraction, we further refined the leading token words using Llama2, resulting in 98 unique themes labeled by BERTopic_theme, with a coherence score of 0.75 and an IRBO score of 0.50. We used the identified topics, based on the groupings, as labels for training a topic classifier. These labels were generated using Llama2 on our SMA corpus. Our comparative study of topic classifiers employing stacking and voting schemes demonstrated that the BERTopic model achieved an accuracy of 0.83 and an F1 score of 0.82 with ensemble voting for training on topics. Furthermore, when training on topic classes, BERTopic+ with ensemble voting achieved the highest accuracy (0.95) and F1 score (0.95) compared to other methods. Additionally, BERTopic_theme achieved superior performance with an ensemble voting classifier, attaining an F1 score of 0.93 and an accuracy of 0.93. The overall performance of classifiers using ensemble stacking was slightly better than that of voting methods for short-sentence topic labeling. For Africa, policymakers should focus on the most pressing social issues: the impact of COVID-19 restrictions on public health and economic recovery, promoting entrepreneurial innovation in energy and environmental sustainability to combat climate change, and responding strategically to China\u2019s rise in global politics to maintain geopolitical stability and foster international cooperation.\n## Wording Embeddings\nFastText is based on Bojanowski's SG model, which treats each word as a bag of character n-grams [54]. It has improved the representation of out-of-vocabulary words and works well with large corpora. The limitations of FastText include ignoring the grammatical sense of words. Our study compared the embeddings of Word2Vec-SMA, pretrained Word2Vec, and FastText. \n\nContextualized Word Embeddings are words based on the meanings derived from the sentences in which they appear. Language model-driven embeddings are contextual; they consider the terms surrounding the target words to generate linguistically-based representations [55]. Contextualized embeddings are widely used in various NLP applications, including classification, question-answering, and summarization. The most popular models for generating dynamic word embeddings are from the Bidirectional Encoder Representation Transformer (BERT) family. BERT uses self-supervised learning to learn the contextual connections between words without labeled data. In the pre-training phase, BERT utilizes two main strategies: the Masked Language Model (MLM) and Next Sentence Prediction (NSP) to learn the grammatical semantics of sentences. In our study, we used the pretrained sentence BERT embeddings called \"paraphrase-MiniLM-L6-v2,\" which supports paraphrasing text for NLP tasks with efficiency and high computation power [56].",
            "reference_string": "[270862694 | Sibitenda et al. | 2024 | Citations: 2]"
        },
        {
            "title": "Sentiment Analysis: Predicting Product Reviews for E-Commerce Recommendations Using Deep Learning and Transformers",
            "venue": "Mathematics",
            "year": 2024,
            "reference_count": 46,
            "citation_count": 7,
            "influential_citation_count": 1,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://www.mdpi.com/2227-7390/12/15/2403/pdf?version=1722572320",
                "status": "GOLD",
                "license": "CCBY",
                "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.3390/math12152403?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.3390/math12152403, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2214766554",
                    "name": "Oumaima Bellar"
                },
                {
                    "authorId": "9281050",
                    "name": "A. Baina"
                },
                {
                    "authorId": "2314661959",
                    "name": "Mostafa Ballafkih"
                }
            ],
            "abstract": "The abundance of publicly available data on the internet within the e-marketing domain is consistently expanding. A significant portion of this data revolve around consumers\u2019 perceptions and opinions regarding the goods or services of organizations, making it valuable for market intelligence collectors in marketing, customer relationship management, and customer retention. Sentiment analysis serves as a tool for examining customer sentiment, marketing initiatives, and product appraisals. This valuable information can inform decisions related to future product and service development, marketing campaigns, and customer service enhancements. In social media, predicting ratings is commonly employed to anticipate product ratings based on user reviews. Our study provides an extensive benchmark comparison of different deep learning models, including convolutional neural networks (CNN), recurrent neural networks (RNN), and bi-directional long short-term memory (Bi-LSTM). These models are evaluated using various word embedding techniques, such as bi-directional encoder representations from transformers (BERT) and its derivatives, FastText, and Word2Vec. The evaluation encompasses two setups: 5-class versus 3-class. This paper focuses on sentiment analysis using neural network-based models for consumer sentiment prediction by evaluating and contrasting their performance indicators on a dataset of reviews of different products from customers of an online women\u2019s clothes retailer.",
            "corpus_id": 271691319,
            "sentences": [
                {
                    "corpus_id": "271691319",
                    "title": "Sentiment Analysis: Predicting Product Reviews for E-Commerce Recommendations Using Deep Learning and Transformers",
                    "text": "FastText: a Word2Vec plugin that breaks down words into n-grams, or smaller units, like \"application\" into \"app,\" with the goal of teaching word morphology. Every word in the text is converted by the model into a bag of embedded vectors. \n\nPolysemous words might provide difficulties when using Word2Vec and FastText since they always assign the same embedding vector, regardless of context. Researchers have been using transformer-based embeddings, such as BERT and its derivatives, to address this problem. Word contexts from BooksCorpus and Wikipedia were used to pre-train models like BERT, and the resulting embeddings were then used in classifiers to make predictions. These models have demonstrated state of the art performance in natural language processing tasks by providing contextualized word embeddings. \n\nThe bidirectional transformer used in the BERT-based model has been pre-trained on large amounts of unlabeled textual data to provide a language representation that can be tailored to different classification tasks. One noteworthy variation is RoBERTa, which Facebook unveiled. It is an improved version of BERT with increased processing power and expanded prediction capabilities that can handle larger amounts of data. Furthermore, ALBERT, a condensed and effective BERT variant that is far smaller than BERT, was created by Google and Toyota. In particular, two variants of a BERT-based model, RoBERTa and ALBERT, were studied in this work.",
                    "score": 0.47606143898843756,
                    "section_title": "\u2022",
                    "char_start_offset": 30142,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 156
                        },
                        {
                            "start": 157,
                            "end": 237
                        },
                        {
                            "start": 240,
                            "end": 391
                        },
                        {
                            "start": 392,
                            "end": 508
                        },
                        {
                            "start": 509,
                            "end": 674
                        },
                        {
                            "start": 675,
                            "end": 816
                        },
                        {
                            "start": 819,
                            "end": 1034
                        },
                        {
                            "start": 1035,
                            "end": 1096
                        },
                        {
                            "start": 1097,
                            "end": 1239
                        },
                        {
                            "start": 1240,
                            "end": 1364
                        },
                        {
                            "start": 1365,
                            "end": 1462
                        }
                    ],
                    "ref_mentions": [],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.80615234375
                },
                {
                    "corpus_id": "271691319",
                    "title": "Sentiment Analysis: Predicting Product Reviews for E-Commerce Recommendations Using Deep Learning and Transformers",
                    "text": "FastText: a Word2Vec plugin that breaks down words into n-grams, or smaller units, like \"application\" into \"app,\" with the goal of teaching word morphology. Every word in the text is converted by the model into a bag of embedded vectors. \n\nPolysemous words might provide difficulties when using Word2Vec and FastText since they always assign the same embedding vector, regardless of context. Researchers have been using transformer-based embeddings, such as BERT and its derivatives, to address this problem. Word contexts from BooksCorpus and Wikipedia were used to pretrain models like BERT, and the resulting embeddings were then used in classifiers to make predictions. These models have demonstrated state of the art performance in natural language processing tasks by providing contextualized word embeddings. \n\nThe bidirectional transformer used in the BERT-based model has been pre-trained on large amounts of unlabeled textual data to provide a language representation that can be tailored to different classification tasks. One noteworthy variation is RoBERTa, which Facebook unveiled. It is an improved version of BERT with increased processing power and expanded prediction capabilities that can handle larger amounts of data. Furthermore, ALBERT, a condensed and effective BERT variant that is far smaller than BERT, was created by Google and Toyota. In particular, two variants of a BERT-based model, RoBERTa and ALBERT, were studied in this work.",
                    "score": 0.4702444435125115,
                    "section_title": "\u2022",
                    "char_start_offset": 31744,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 156
                        },
                        {
                            "start": 157,
                            "end": 237
                        },
                        {
                            "start": 240,
                            "end": 391
                        },
                        {
                            "start": 392,
                            "end": 508
                        },
                        {
                            "start": 509,
                            "end": 673
                        },
                        {
                            "start": 674,
                            "end": 815
                        },
                        {
                            "start": 818,
                            "end": 1033
                        },
                        {
                            "start": 1034,
                            "end": 1095
                        },
                        {
                            "start": 1096,
                            "end": 1238
                        },
                        {
                            "start": 1239,
                            "end": 1363
                        },
                        {
                            "start": 1364,
                            "end": 1461
                        }
                    ],
                    "ref_mentions": [],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.80224609375
                }
            ],
            "relevance_judgement": 0.80615234375,
            "relevance_judgment_input_expanded": "# Title: Sentiment Analysis: Predicting Product Reviews for E-Commerce Recommendations Using Deep Learning and Transformers\n# Venue: Mathematics\n# Authors: Oumaima Bellar, A. Baina, Mostafa Ballafkih\n## Abstract\nThe abundance of publicly available data on the internet within the e-marketing domain is consistently expanding. A significant portion of this data revolve around consumers\u2019 perceptions and opinions regarding the goods or services of organizations, making it valuable for market intelligence collectors in marketing, customer relationship management, and customer retention. Sentiment analysis serves as a tool for examining customer sentiment, marketing initiatives, and product appraisals. This valuable information can inform decisions related to future product and service development, marketing campaigns, and customer service enhancements. In social media, predicting ratings is commonly employed to anticipate product ratings based on user reviews. Our study provides an extensive benchmark comparison of different deep learning models, including convolutional neural networks (CNN), recurrent neural networks (RNN), and bi-directional long short-term memory (Bi-LSTM). These models are evaluated using various word embedding techniques, such as bi-directional encoder representations from transformers (BERT) and its derivatives, FastText, and Word2Vec. The evaluation encompasses two setups: 5-class versus 3-class. This paper focuses on sentiment analysis using neural network-based models for consumer sentiment prediction by evaluating and contrasting their performance indicators on a dataset of reviews of different products from customers of an online women\u2019s clothes retailer.\n## \u2022\nFastText: a Word2Vec plugin that breaks down words into n-grams, or smaller units, like \"application\" into \"app,\" with the goal of teaching word morphology. Every word in the text is converted by the model into a bag of embedded vectors. \n\nPolysemous words might provide difficulties when using Word2Vec and FastText since they always assign the same embedding vector, regardless of context. Researchers have been using transformer-based embeddings, such as BERT and its derivatives, to address this problem. Word contexts from BooksCorpus and Wikipedia were used to pre-train models like BERT, and the resulting embeddings were then used in classifiers to make predictions. These models have demonstrated state of the art performance in natural language processing tasks by providing contextualized word embeddings. \n\nThe bidirectional transformer used in the BERT-based model has been pre-trained on large amounts of unlabeled textual data to provide a language representation that can be tailored to different classification tasks. One noteworthy variation is RoBERTa, which Facebook unveiled. It is an improved version of BERT with increased processing power and expanded prediction capabilities that can handle larger amounts of data. Furthermore, ALBERT, a condensed and effective BERT variant that is far smaller than BERT, was created by Google and Toyota. In particular, two variants of a BERT-based model, RoBERTa and ALBERT, were studied in this work.\n...\nFastText: a Word2Vec plugin that breaks down words into n-grams, or smaller units, like \"application\" into \"app,\" with the goal of teaching word morphology. Every word in the text is converted by the model into a bag of embedded vectors. \n\nPolysemous words might provide difficulties when using Word2Vec and FastText since they always assign the same embedding vector, regardless of context. Researchers have been using transformer-based embeddings, such as BERT and its derivatives, to address this problem. Word contexts from BooksCorpus and Wikipedia were used to pretrain models like BERT, and the resulting embeddings were then used in classifiers to make predictions. These models have demonstrated state of the art performance in natural language processing tasks by providing contextualized word embeddings. \n\nThe bidirectional transformer used in the BERT-based model has been pre-trained on large amounts of unlabeled textual data to provide a language representation that can be tailored to different classification tasks. One noteworthy variation is RoBERTa, which Facebook unveiled. It is an improved version of BERT with increased processing power and expanded prediction capabilities that can handle larger amounts of data. Furthermore, ALBERT, a condensed and effective BERT variant that is far smaller than BERT, was created by Google and Toyota. In particular, two variants of a BERT-based model, RoBERTa and ALBERT, were studied in this work.",
            "reference_string": "[271691319 | Bellar et al. | 2024 | Citations: 7]"
        },
        {
            "title": "Language Models and Word Sense Disambiguation: An Overview and Analysis",
            "venue": "arXiv.org",
            "year": 2020,
            "reference_count": 114,
            "citation_count": 14,
            "influential_citation_count": 3,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2008.11608, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2058866904",
                    "name": "Daniel Loureiro"
                },
                {
                    "authorId": "1667035673",
                    "name": "Kiamehr Rezaee"
                },
                {
                    "authorId": "1717641",
                    "name": "Mohammad Taher Pilehvar"
                },
                {
                    "authorId": "1387447871",
                    "name": "Jos\u00e9 Camacho-Collados"
                }
            ],
            "abstract": "Transformer-based language models have taken many fields in NLP by storm. BERT and its derivatives dominate most of the existing evaluation benchmarks, including those for Word Sense Disambiguation (WSD), thanks to their ability in capturing context-sensitive semantic nuances. However, there is still little knowledge about their capabilities and potential limitations for encoding and recovering word senses. In this article, we provide an in-depth quantitative and qualitative analysis of the celebrated BERT model with respect to lexical ambiguity. One of the main conclusions of our analysis is that BERT performs a decent job in capturing high-level sense distinctions, even when a limited number of examples is available for each word sense. Our analysis also reveals that in some cases language models come close to solving coarse-grained noun disambiguation under ideal conditions in terms of availability of training data and computing resources. However, this scenario rarely occurs in real-world settings and, hence, many practical challenges remain even in the coarse-grained setting. We also perform an in-depth comparison of the two main language model based WSD strategies, i.e., fine-tuning and feature extraction, finding that the latter approach is more robust with respect to sense bias and it can better exploit limited available training data.",
            "corpus_id": 221319787,
            "sentences": [
                {
                    "corpus_id": "221319787",
                    "title": "Language Models and Word Sense Disambiguation: An Overview and Analysis",
                    "text": "Word-specific results for different configurations of BERT and the FastText baseline are shown in Table 5. In general, results are high for all BERT-based models, over 90% in most cases. This reinforces the potential of language models for WSD, both in its lightweight 1NN and in the fine-tuning settings. in all BERT configurations. Also, higher polysemy is not a strong indicator of lower performance (see Table 4.2 for statistics of the twenty words, including polysemy), as one would expect from a classification task with higher number of classes (near zero average correlation across settings). In the following we also discuss other relevant points with respect to Most Frequent Sense (MFS) bias and fine-tuning. \n\nMFS Bias. As expected, macro F1 results degrade for the purely supervised classification models (FastText and fine-tuned BERT), indicating the inherent sense biases captured by the model which lead to lowered performance for the obscure senses (see the work by Postma et al. (2016) for a more thorough analysis on this issue). However, BERT proves to be much more robust with this respect whereas FastText suffers heavily (highlighted in the macro setting). \n\nImpact of fine-tuning. By average, fine-tuning improves the performance for BERT-Large by 1.6 points in terms of micro-F1 (from 95.8% to 97.5%) but decreases on macro-F1 (from 96.4% to 95.1%). While BERT-Base significantly correlates with BERT-Large in the 1NN setting (Pearson correlation above 0.9 for both micro and macro), it has a relatively low correlation with the fine-tuned BERT-Base (0.60 on Micro-F1 and 0.75 on macro-F1). The same trend is observed for BERT-Large, where the correlation between fine-tuning and 1NN is 0.71 and 0.63 on micro-F1 and macro-F1, respectively. The operating principles behind both approaches are significantly different, which may explain this relatively low correlation. While fine-tuning is optimizing a loss function during training, the 1NN approach is simply memorizing states. By optimizing losses, fine-tuning is more susceptible to overfit on the MFS.",
                    "score": 0.5084060089200342,
                    "section_title": "Results",
                    "char_start_offset": 43862,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 106
                        },
                        {
                            "start": 107,
                            "end": 186
                        },
                        {
                            "start": 187,
                            "end": 305
                        },
                        {
                            "start": 306,
                            "end": 333
                        },
                        {
                            "start": 334,
                            "end": 600
                        },
                        {
                            "start": 601,
                            "end": 719
                        },
                        {
                            "start": 722,
                            "end": 731
                        },
                        {
                            "start": 732,
                            "end": 1048
                        },
                        {
                            "start": 1049,
                            "end": 1179
                        },
                        {
                            "start": 1182,
                            "end": 1204
                        },
                        {
                            "start": 1205,
                            "end": 1374
                        },
                        {
                            "start": 1375,
                            "end": 1615
                        },
                        {
                            "start": 1616,
                            "end": 1765
                        },
                        {
                            "start": 1766,
                            "end": 1893
                        },
                        {
                            "start": 1894,
                            "end": 2004
                        },
                        {
                            "start": 2005,
                            "end": 2081
                        }
                    ],
                    "ref_mentions": [],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.796875
                },
                {
                    "corpus_id": "221319787",
                    "title": "Language Models and Word Sense Disambiguation: An Overview and Analysis",
                    "text": "CoarseWSD-20 consists of 20 separate sets, each containing sentences for different senses of the corresponding target word. Therefore, the evaluation can be framed as a standard classification task for each word. \n\nGiven the classification nature of the CoarseWSD-20 datasets, we can perform experiments with our 1NN BERT system and compare it with a standard fine-tuned BERT model (see Section 3.3 for more details on the LM-based WSD approaches). Note that fine-tuning for individual target words results in many models (one per word). Therefore, this setup would not be computationally feasible in a general WSD setting, as the number of models would approach the vocabulary size. However, in our experiments we are interested in verifying the limits of BERT, without any other confounds or modelspecific restrictions. We also include two FastText linear classifiers (Joulin et al. 2017) as baselines: FTX-B (base model without pre-trained embeddings) and FTX-C (using pre-trained embeddings from Common Crawl). We chose FastText as baseline given its efficiency and competitive results for sentence classification.",
                    "score": 0.5232972853013131,
                    "section_title": "Experimental setting",
                    "char_start_offset": 39186,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 123
                        },
                        {
                            "start": 124,
                            "end": 212
                        },
                        {
                            "start": 215,
                            "end": 448
                        },
                        {
                            "start": 449,
                            "end": 537
                        },
                        {
                            "start": 538,
                            "end": 683
                        },
                        {
                            "start": 684,
                            "end": 821
                        },
                        {
                            "start": 822,
                            "end": 1014
                        },
                        {
                            "start": 1015,
                            "end": 1118
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 870,
                            "end": 890,
                            "matchedPaperCorpusId": "195477534"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.68994140625
                },
                {
                    "corpus_id": "221319787",
                    "title": "Language Models and Word Sense Disambiguation: An Overview and Analysis",
                    "text": "Our experiments with BERT used the Transformers framework (v2.5.1) developed by Wolf et al. (2019), and we used the uncased pre-trained base and large models released by Devlin et al. (2019). Following previous feature extraction works (including our experiment in Section 3.4.1), with CoarseWSD-20 we also average subword representations and use the sum of the last four layers when extracting contextual embeddings. For fine-tuning experiments, we used a concatenation of the average embedding of target word's sub-words with the embedding of the [CLS] token, and fed them to a classifier. We used the same default hyper-parameter configuration for all the experiments. Given the fluctuation of results with fine-tuning, all the experiments are based on the average of three independent runs. Our experiments with FastText used the official package 18 (v0.9.1), with FastText-Base corresponding to the default supervised classification pipeline using randomly-initialized vectors, and FastText-Crawl corresponding to the same pipeline but starting with pre-trained 300-dimensional vectors based on Common Crawl. Following Joulin et al. ( 2017), classification with FastText is performed using multinomial logistic regression and averaged sub-word representations. \n\nEvaluation measures. In a classification setting, the performance of a model is measured by various metrics, among which precision, recall and F-score are the most popular. Let T P i (true-positive) and F P i (false-positive) be the number of instances correctly / incorrectly classified as class c i respectively. Also, let T N i (true-negative) and F N i (false-negative) be the number of instances correctly / incorrectly classified as class c j for any j = i. Therefore, for class c i , precision P i and recall R i are defined as follows: \n\nIn other words, precision is the fraction of relevant instances among the retrieved instances, while recall is the fraction of the total number of relevant instances that were actually retrieved. The F-score F i for class c i is then defined as the harmonic mean of its precision and recall values:",
                    "score": 0.47545443975092055,
                    "section_title": "Configuration.",
                    "char_start_offset": 40323,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 191
                        },
                        {
                            "start": 192,
                            "end": 280
                        },
                        {
                            "start": 281,
                            "end": 417
                        },
                        {
                            "start": 418,
                            "end": 591
                        },
                        {
                            "start": 592,
                            "end": 671
                        },
                        {
                            "start": 672,
                            "end": 794
                        },
                        {
                            "start": 795,
                            "end": 1113
                        },
                        {
                            "start": 1114,
                            "end": 1265
                        },
                        {
                            "start": 1268,
                            "end": 1288
                        },
                        {
                            "start": 1289,
                            "end": 1440
                        },
                        {
                            "start": 1441,
                            "end": 1582
                        },
                        {
                            "start": 1583,
                            "end": 1731
                        },
                        {
                            "start": 1732,
                            "end": 1811
                        },
                        {
                            "start": 1814,
                            "end": 2009
                        },
                        {
                            "start": 2010,
                            "end": 2112
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 170,
                            "end": 190,
                            "matchedPaperCorpusId": "52967399"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.6630859375
                }
            ],
            "relevance_judgement": 0.796875,
            "relevance_judgment_input_expanded": "# Title: Language Models and Word Sense Disambiguation: An Overview and Analysis\n# Venue: arXiv.org\n# Authors: Daniel Loureiro, Kiamehr Rezaee, Mohammad Taher Pilehvar, Jos\u00e9 Camacho-Collados\n## Abstract\nTransformer-based language models have taken many fields in NLP by storm. BERT and its derivatives dominate most of the existing evaluation benchmarks, including those for Word Sense Disambiguation (WSD), thanks to their ability in capturing context-sensitive semantic nuances. However, there is still little knowledge about their capabilities and potential limitations for encoding and recovering word senses. In this article, we provide an in-depth quantitative and qualitative analysis of the celebrated BERT model with respect to lexical ambiguity. One of the main conclusions of our analysis is that BERT performs a decent job in capturing high-level sense distinctions, even when a limited number of examples is available for each word sense. Our analysis also reveals that in some cases language models come close to solving coarse-grained noun disambiguation under ideal conditions in terms of availability of training data and computing resources. However, this scenario rarely occurs in real-world settings and, hence, many practical challenges remain even in the coarse-grained setting. We also perform an in-depth comparison of the two main language model based WSD strategies, i.e., fine-tuning and feature extraction, finding that the latter approach is more robust with respect to sense bias and it can better exploit limited available training data.\n## Experimental setting\nCoarseWSD-20 consists of 20 separate sets, each containing sentences for different senses of the corresponding target word. Therefore, the evaluation can be framed as a standard classification task for each word. \n\nGiven the classification nature of the CoarseWSD-20 datasets, we can perform experiments with our 1NN BERT system and compare it with a standard fine-tuned BERT model (see Section 3.3 for more details on the LM-based WSD approaches). Note that fine-tuning for individual target words results in many models (one per word). Therefore, this setup would not be computationally feasible in a general WSD setting, as the number of models would approach the vocabulary size. However, in our experiments we are interested in verifying the limits of BERT, without any other confounds or modelspecific restrictions. We also include two FastText linear classifiers (Joulin et al. 2017) as baselines: FTX-B (base model without pre-trained embeddings) and FTX-C (using pre-trained embeddings from Common Crawl). We chose FastText as baseline given its efficiency and competitive results for sentence classification.\n\n## Configuration.\nOur experiments with BERT used the Transformers framework (v2.5.1) developed by Wolf et al. (2019), and we used the uncased pre-trained base and large models released by Devlin et al. (2019). Following previous feature extraction works (including our experiment in Section 3.4.1), with CoarseWSD-20 we also average subword representations and use the sum of the last four layers when extracting contextual embeddings. For fine-tuning experiments, we used a concatenation of the average embedding of target word's sub-words with the embedding of the [CLS] token, and fed them to a classifier. We used the same default hyper-parameter configuration for all the experiments. Given the fluctuation of results with fine-tuning, all the experiments are based on the average of three independent runs. Our experiments with FastText used the official package 18 (v0.9.1), with FastText-Base corresponding to the default supervised classification pipeline using randomly-initialized vectors, and FastText-Crawl corresponding to the same pipeline but starting with pre-trained 300-dimensional vectors based on Common Crawl. Following Joulin et al. ( 2017), classification with FastText is performed using multinomial logistic regression and averaged sub-word representations. \n\nEvaluation measures. In a classification setting, the performance of a model is measured by various metrics, among which precision, recall and F-score are the most popular. Let T P i (true-positive) and F P i (false-positive) be the number of instances correctly / incorrectly classified as class c i respectively. Also, let T N i (true-negative) and F N i (false-negative) be the number of instances correctly / incorrectly classified as class c j for any j = i. Therefore, for class c i , precision P i and recall R i are defined as follows: \n\nIn other words, precision is the fraction of relevant instances among the retrieved instances, while recall is the fraction of the total number of relevant instances that were actually retrieved. The F-score F i for class c i is then defined as the harmonic mean of its precision and recall values:\n\n## Results\nWord-specific results for different configurations of BERT and the FastText baseline are shown in Table 5. In general, results are high for all BERT-based models, over 90% in most cases. This reinforces the potential of language models for WSD, both in its lightweight 1NN and in the fine-tuning settings. in all BERT configurations. Also, higher polysemy is not a strong indicator of lower performance (see Table 4.2 for statistics of the twenty words, including polysemy), as one would expect from a classification task with higher number of classes (near zero average correlation across settings). In the following we also discuss other relevant points with respect to Most Frequent Sense (MFS) bias and fine-tuning. \n\nMFS Bias. As expected, macro F1 results degrade for the purely supervised classification models (FastText and fine-tuned BERT), indicating the inherent sense biases captured by the model which lead to lowered performance for the obscure senses (see the work by Postma et al. (2016) for a more thorough analysis on this issue). However, BERT proves to be much more robust with this respect whereas FastText suffers heavily (highlighted in the macro setting). \n\nImpact of fine-tuning. By average, fine-tuning improves the performance for BERT-Large by 1.6 points in terms of micro-F1 (from 95.8% to 97.5%) but decreases on macro-F1 (from 96.4% to 95.1%). While BERT-Base significantly correlates with BERT-Large in the 1NN setting (Pearson correlation above 0.9 for both micro and macro), it has a relatively low correlation with the fine-tuned BERT-Base (0.60 on Micro-F1 and 0.75 on macro-F1). The same trend is observed for BERT-Large, where the correlation between fine-tuning and 1NN is 0.71 and 0.63 on micro-F1 and macro-F1, respectively. The operating principles behind both approaches are significantly different, which may explain this relatively low correlation. While fine-tuning is optimizing a loss function during training, the 1NN approach is simply memorizing states. By optimizing losses, fine-tuning is more susceptible to overfit on the MFS.",
            "reference_string": "[221319787 | Loureiro et al. | 2020 | Citations: 14]"
        },
        {
            "title": "Sentiment Analysis and Stance Detection on German YouTube Comments on Gender Diversity",
            "venue": "Journal of Computer-Assisted Linguistic Research",
            "year": 2022,
            "reference_count": 38,
            "citation_count": 1,
            "influential_citation_count": 0,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://polipapers.upv.es/index.php/jclr/article/download/18224/15516",
                "status": "GOLD",
                "license": "CCBYNCND",
                "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.4995/jclr.2022.18224?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.4995/jclr.2022.18224, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "78466463",
                    "name": "Lidiia Melnyk"
                },
                {
                    "authorId": "2192124742",
                    "name": "Linda Feld"
                }
            ],
            "abstract": "This paper explores different options of detecting the stance of German YouTube comments regarding the topic of gender diversity and compares the respective results with those of sentiment analysis, showing that these are two very different NLP tasks focusing on distinct characteristics of the discourse. While an already existing model was used to analyze the comments\u2019 sentiment (BERT), the comments\u2019 stance was first annotated and then used to train different models \u2013 SVM with TF-IDF, DistilBERT, LSTM and CNN \u2013 for predicting the stance of unseen comments. The best results were achieved by the CNN, reaching 78.3% accuracy (92% after dataset normalization) on the test set. Whereas the most common stance identified in the comments is a neutral one (neither completely in favor nor completely against gender diversity), the overall sentiment of the discourse turns out to be negative. This shows that the discourse revolving around the topic of gender diversity in YouTube comments is filled with strong opinions, on the one hand, but also opens up a space for anonymously inquiring and learning about the topic and its implications, on the other. Our research thereby (1) contributes to the understanding and application of different NLP tasks used to predict the sentiment and stance of unstructured textual data, and (2) provides relevant insights into society\u2019s attitudes towards a changing system of values and beliefs.",
            "corpus_id": 253860591,
            "sentences": [
                {
                    "corpus_id": "253860591",
                    "title": "Sentiment Analysis and Stance Detection on German YouTube Comments on Gender Diversity",
                    "text": "The two sentiment classification models Guhr et al. (2020) trained and tested on this corpus were FastText and BERT. BERT, Bidirectional Encoder Representations from Transformers, is a language representation model \"designed to pre-train deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers\" (Devlin et al. 2019, 4171). It can be fine-tuned by adding just one more output layer, and it can be applied to solving a variety of NLP tasks without task-specific modifications of the model architecture. BERT relies on two core processes: pretraining and fine-tuning. During the pretraining stage, the model is trained on unlabeled data with a variety of tasks, and after initializing the model with the pretrained parameters, these will be fine-tuned with the use of labeled data. The major advantage of BERT is the existence of a wide spectrum of large pretrained models, which minimizes the need for a big amount of training data for the model to provide representative results. Guhr et al. (2020) used a model pretrained by the developers on the German BERT small model, making use of \"bidirectional training of a deep transformer-based network architecture\" (Guhr et al. 2020(Guhr et al. , 1630)), and trained their German BERT model for SA using the implementation provided by the HuggingFace repository. While the authors recognize certain advantages of FastText, being a traditional word embedding model, and the model's performance was relatively high for both data from known domains as well as from an unknown domain, the BERT model scored better in all of these settings and in classifying unseen data, reaching an F1 score of 0.80 and outperforming FastText by 9.4%. \n\nWe used a Python script to apply the pretrained model to our data. We sought to predict the sentiment of each comment in our dataset separately and to receive a clear result as to whether the polarity is positive, negative, or neutral.",
                    "score": 0.5804318853337339,
                    "section_title": "SENTIMENT ANALYSIS WITH BERT",
                    "char_start_offset": 10302,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 116
                        },
                        {
                            "start": 117,
                            "end": 390
                        },
                        {
                            "start": 391,
                            "end": 568
                        },
                        {
                            "start": 569,
                            "end": 632
                        },
                        {
                            "start": 633,
                            "end": 846
                        },
                        {
                            "start": 847,
                            "end": 1046
                        },
                        {
                            "start": 1047,
                            "end": 1375
                        },
                        {
                            "start": 1376,
                            "end": 1744
                        },
                        {
                            "start": 1747,
                            "end": 1813
                        },
                        {
                            "start": 1814,
                            "end": 1982
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 40,
                            "end": 58,
                            "matchedPaperCorpusId": "218974283"
                        },
                        {
                            "start": 1047,
                            "end": 1065,
                            "matchedPaperCorpusId": "218974283"
                        },
                        {
                            "start": 1228,
                            "end": 1245,
                            "matchedPaperCorpusId": "218974283"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.78955078125
                }
            ],
            "relevance_judgement": 0.78955078125,
            "relevance_judgment_input_expanded": "# Title: Sentiment Analysis and Stance Detection on German YouTube Comments on Gender Diversity\n# Venue: Journal of Computer-Assisted Linguistic Research\n# Authors: Lidiia Melnyk, Linda Feld\n## Abstract\nThis paper explores different options of detecting the stance of German YouTube comments regarding the topic of gender diversity and compares the respective results with those of sentiment analysis, showing that these are two very different NLP tasks focusing on distinct characteristics of the discourse. While an already existing model was used to analyze the comments\u2019 sentiment (BERT), the comments\u2019 stance was first annotated and then used to train different models \u2013 SVM with TF-IDF, DistilBERT, LSTM and CNN \u2013 for predicting the stance of unseen comments. The best results were achieved by the CNN, reaching 78.3% accuracy (92% after dataset normalization) on the test set. Whereas the most common stance identified in the comments is a neutral one (neither completely in favor nor completely against gender diversity), the overall sentiment of the discourse turns out to be negative. This shows that the discourse revolving around the topic of gender diversity in YouTube comments is filled with strong opinions, on the one hand, but also opens up a space for anonymously inquiring and learning about the topic and its implications, on the other. Our research thereby (1) contributes to the understanding and application of different NLP tasks used to predict the sentiment and stance of unstructured textual data, and (2) provides relevant insights into society\u2019s attitudes towards a changing system of values and beliefs.\n## SENTIMENT ANALYSIS WITH BERT\nThe two sentiment classification models Guhr et al. (2020) trained and tested on this corpus were FastText and BERT. BERT, Bidirectional Encoder Representations from Transformers, is a language representation model \"designed to pre-train deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers\" (Devlin et al. 2019, 4171). It can be fine-tuned by adding just one more output layer, and it can be applied to solving a variety of NLP tasks without task-specific modifications of the model architecture. BERT relies on two core processes: pretraining and fine-tuning. During the pretraining stage, the model is trained on unlabeled data with a variety of tasks, and after initializing the model with the pretrained parameters, these will be fine-tuned with the use of labeled data. The major advantage of BERT is the existence of a wide spectrum of large pretrained models, which minimizes the need for a big amount of training data for the model to provide representative results. Guhr et al. (2020) used a model pretrained by the developers on the German BERT small model, making use of \"bidirectional training of a deep transformer-based network architecture\" (Guhr et al. 2020(Guhr et al. , 1630)), and trained their German BERT model for SA using the implementation provided by the HuggingFace repository. While the authors recognize certain advantages of FastText, being a traditional word embedding model, and the model's performance was relatively high for both data from known domains as well as from an unknown domain, the BERT model scored better in all of these settings and in classifying unseen data, reaching an F1 score of 0.80 and outperforming FastText by 9.4%. \n\nWe used a Python script to apply the pretrained model to our data. We sought to predict the sentiment of each comment in our dataset separately and to receive a clear result as to whether the polarity is positive, negative, or neutral.",
            "reference_string": "[253860591 | Melnyk et al. | 2022 | Citations: 1]"
        },
        {
            "title": "Extracting Software Requirements from Unstructured Documents",
            "venue": "International Joint Conference on the Analysis of Images, Social Networks and Texts",
            "year": 2022,
            "reference_count": 27,
            "citation_count": 11,
            "influential_citation_count": 1,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": "CLOSED",
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2202.02135, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2072422334",
                    "name": "V. Ivanov"
                },
                {
                    "authorId": "2883654",
                    "name": "Andrey Sadovykh"
                },
                {
                    "authorId": "3361049",
                    "name": "Alexandr Naumchev"
                },
                {
                    "authorId": "36000045",
                    "name": "A. Bagnato"
                },
                {
                    "authorId": "2117465706",
                    "name": "K. Yakovlev"
                }
            ],
            "abstract": "Requirements identification in textual documents or extraction is a tedious and error prone task that many researchers suggest automating. We manually annotated the PURE dataset and thus created a new one containing both requirements and non-requirements. Using this dataset, we fine-tuned the BERT model and compare the results with several baselines such as fastText and ELMo. In order to evaluate the model on semantically more complex documents we compare the PURE dataset results with experiments on Request For Information (RFI) documents. The RFIs often include software requirements, but in a less standardized way. The fine-tuned BERT showed promising results on PURE dataset on the binary sentence classification task. Comparing with previous and recent studies dealing with constrained inputs, our approach demonstrates high performance in terms of precision and recall metrics, while being agnostic to the unstructured textual input.",
            "corpus_id": 246608158,
            "sentences": [
                {
                    "corpus_id": "246608158",
                    "title": "Extracting Software Requirements from Unstructured Documents",
                    "text": "The main purpose of this study was about assessing three major classification metrics: Precision, Recall, and F1-score. Those metrics are perceived as the golden standard in Machine Learning and Deep Learning areas. \n\nResult of the first phase of evaluation is presented in Table 1. As it was expected, more advanced model (BERT) showed better results in terms of F1-score. BERT-based model showed high precision (0.92) and lower recall (0.8). Results of the BERT model is available at https://bit.ly/3oPElMm. However, the values of precision and recall metrics behave differently for the fastText and ELMobased baselines. fastText-based classifier showed better Recall (0.93) comparing with other architectures. This property might be useful in some cases when it is necessary to extract more relevant sentences and text patterns associated with requirements.",
                    "score": 0.5125530142348842,
                    "section_title": "Experiments with PURE documents",
                    "char_start_offset": 15016,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 119
                        },
                        {
                            "start": 120,
                            "end": 215
                        },
                        {
                            "start": 218,
                            "end": 282
                        },
                        {
                            "start": 283,
                            "end": 373
                        },
                        {
                            "start": 374,
                            "end": 509
                        },
                        {
                            "start": 510,
                            "end": 622
                        },
                        {
                            "start": 623,
                            "end": 712
                        },
                        {
                            "start": 713,
                            "end": 860
                        }
                    ],
                    "ref_mentions": [],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.775390625
                }
            ],
            "relevance_judgement": 0.775390625,
            "relevance_judgment_input_expanded": "# Title: Extracting Software Requirements from Unstructured Documents\n# Venue: International Joint Conference on the Analysis of Images, Social Networks and Texts\n# Authors: V. Ivanov, Andrey Sadovykh, Alexandr Naumchev, A. Bagnato, K. Yakovlev\n## Abstract\nRequirements identification in textual documents or extraction is a tedious and error prone task that many researchers suggest automating. We manually annotated the PURE dataset and thus created a new one containing both requirements and non-requirements. Using this dataset, we fine-tuned the BERT model and compare the results with several baselines such as fastText and ELMo. In order to evaluate the model on semantically more complex documents we compare the PURE dataset results with experiments on Request For Information (RFI) documents. The RFIs often include software requirements, but in a less standardized way. The fine-tuned BERT showed promising results on PURE dataset on the binary sentence classification task. Comparing with previous and recent studies dealing with constrained inputs, our approach demonstrates high performance in terms of precision and recall metrics, while being agnostic to the unstructured textual input.\n## Experiments with PURE documents\nThe main purpose of this study was about assessing three major classification metrics: Precision, Recall, and F1-score. Those metrics are perceived as the golden standard in Machine Learning and Deep Learning areas. \n\nResult of the first phase of evaluation is presented in Table 1. As it was expected, more advanced model (BERT) showed better results in terms of F1-score. BERT-based model showed high precision (0.92) and lower recall (0.8). Results of the BERT model is available at https://bit.ly/3oPElMm. However, the values of precision and recall metrics behave differently for the fastText and ELMobased baselines. fastText-based classifier showed better Recall (0.93) comparing with other architectures. This property might be useful in some cases when it is necessary to extract more relevant sentences and text patterns associated with requirements.",
            "reference_string": "[246608158 | Ivanov et al. | 2022 | Citations: 11]"
        },
        {
            "title": "Automation of Book Categorisation Based on Network Centric Quality Management System",
            "venue": "International Journal of Advanced Computer Science and Applications",
            "year": 2024,
            "reference_count": 20,
            "citation_count": 1,
            "influential_citation_count": 0,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://doi.org/10.14569/ijacsa.2024.0150827",
                "status": "GOLD",
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.14569/ijacsa.2024.0150827?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.14569/ijacsa.2024.0150827, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2320145184",
                    "name": "Tingting Liu"
                },
                {
                    "authorId": "2320123052",
                    "name": "Qiyuan Liu"
                },
                {
                    "authorId": "2319980631",
                    "name": "Linya Fu"
                }
            ],
            "abstract": "org",
            "corpus_id": 272449378,
            "sentences": [
                {
                    "corpus_id": "272449378",
                    "title": "Automation of Book Categorisation Based on Network Centric Quality Management System",
                    "text": "The Fasttext model performs better, with high speed and second only to the TextRNN-Att model. The loss values for the Fasttext model and the TextRNN-Att model has a loss value of 0.62 and 0.59 respectively, while the TextRNN model with embedded attention mechanism has the best result in all comparisons, which highlights the feature extraction advantage of the attention algorithm, and verifies the feasibility of studying the classification research with the pretrained model based on the attention mechanism. The LERT model retains the original architecture of the BERT, but optimises the training tasks and methods, which provides a good opportunity to study the BERT series of models. The LERT model retains the original BERT architecture but optimises the training tasks and methods, providing guidance for the study of the BERT series of models. The learning rate is set to 1e-5, the batch size is 8, and 10 rounds of training are performed, taking the maximum sequence length of the input text as 16,32,64,128,192,256, and 512 for the experiments, respectively. The experimental results corresponding to Fig. 8 are obtained. As can be seen in Fig. 7, the performance of the LERT model when dealing with different maximum sequence lengths, where the best performance is achieved when 256 is the sequence length. A sequence that is too short (e.g., 16) will lose more information because the important content is truncated, while setting it too long results in filling too many invalid 0-values, which affects feature extraction and computational efficiency. Therefore, setting 256 as the most suitable text length for the model to handle on the task of secondary subject classification of Chinese books is a significantly preferred solution. Fig. 9 shows the experimental results of the LERT model under different batch sizes. Fig. 9 shows that the performance of the model typically improves with increasing batch size, although the effect decreases after a certain point. The minimum batch is theoretically good for optimisation, but too low may cause unstable convergence and prolong training time. On the contrary, too large batches tend to trigger memory overruns and impair accuracy. Considering the experimental results and computational efficiency, 16 is chosen as the preferred batch size setting for the BERT pre-training model on the Chinese book secondary subject classification dataset.",
                    "score": 0.4237403013929248,
                    "section_title": "IV. ANALYSIS OF EXPERIMENTAL RESULTS",
                    "char_start_offset": 28206,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 93
                        },
                        {
                            "start": 94,
                            "end": 511
                        },
                        {
                            "start": 512,
                            "end": 689
                        },
                        {
                            "start": 690,
                            "end": 852
                        },
                        {
                            "start": 853,
                            "end": 1069
                        },
                        {
                            "start": 1070,
                            "end": 1132
                        },
                        {
                            "start": 1133,
                            "end": 1318
                        },
                        {
                            "start": 1319,
                            "end": 1564
                        },
                        {
                            "start": 1565,
                            "end": 1748
                        },
                        {
                            "start": 1749,
                            "end": 1833
                        },
                        {
                            "start": 1834,
                            "end": 1980
                        },
                        {
                            "start": 1981,
                            "end": 2108
                        },
                        {
                            "start": 2109,
                            "end": 2196
                        },
                        {
                            "start": 2197,
                            "end": 2406
                        }
                    ],
                    "ref_mentions": [],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.7724609375
                }
            ],
            "relevance_judgement": 0.7724609375,
            "relevance_judgment_input_expanded": "# Title: Automation of Book Categorisation Based on Network Centric Quality Management System\n# Venue: International Journal of Advanced Computer Science and Applications\n# Authors: Tingting Liu, Qiyuan Liu, Linya Fu\n## Abstract\norg\n## IV. ANALYSIS OF EXPERIMENTAL RESULTS\nThe Fasttext model performs better, with high speed and second only to the TextRNN-Att model. The loss values for the Fasttext model and the TextRNN-Att model has a loss value of 0.62 and 0.59 respectively, while the TextRNN model with embedded attention mechanism has the best result in all comparisons, which highlights the feature extraction advantage of the attention algorithm, and verifies the feasibility of studying the classification research with the pretrained model based on the attention mechanism. The LERT model retains the original architecture of the BERT, but optimises the training tasks and methods, which provides a good opportunity to study the BERT series of models. The LERT model retains the original BERT architecture but optimises the training tasks and methods, providing guidance for the study of the BERT series of models. The learning rate is set to 1e-5, the batch size is 8, and 10 rounds of training are performed, taking the maximum sequence length of the input text as 16,32,64,128,192,256, and 512 for the experiments, respectively. The experimental results corresponding to Fig. 8 are obtained. As can be seen in Fig. 7, the performance of the LERT model when dealing with different maximum sequence lengths, where the best performance is achieved when 256 is the sequence length. A sequence that is too short (e.g., 16) will lose more information because the important content is truncated, while setting it too long results in filling too many invalid 0-values, which affects feature extraction and computational efficiency. Therefore, setting 256 as the most suitable text length for the model to handle on the task of secondary subject classification of Chinese books is a significantly preferred solution. Fig. 9 shows the experimental results of the LERT model under different batch sizes. Fig. 9 shows that the performance of the model typically improves with increasing batch size, although the effect decreases after a certain point. The minimum batch is theoretically good for optimisation, but too low may cause unstable convergence and prolong training time. On the contrary, too large batches tend to trigger memory overruns and impair accuracy. Considering the experimental results and computational efficiency, 16 is chosen as the preferred batch size setting for the BERT pre-training model on the Chinese book secondary subject classification dataset.",
            "reference_string": "[272449378 | Liu et al. | 2024 | Citations: 1]"
        },
        {
            "title": "A machine learning approach to recognize bias and discrimination in job advertisements",
            "venue": "Ai & Society",
            "year": 2022,
            "reference_count": 32,
            "citation_count": 19,
            "influential_citation_count": 1,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://link.springer.com/content/pdf/10.1007/s00146-022-01574-0.pdf",
                "status": "HYBRID",
                "license": "CCBY",
                "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.1007/s00146-022-01574-0?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1007/s00146-022-01574-0, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2188338310",
                    "name": "Richard Frissen"
                },
                {
                    "authorId": "2848544",
                    "name": "K. Adebayo"
                },
                {
                    "authorId": "39418246",
                    "name": "Rohan Nanda"
                }
            ],
            "abstract": "In recent years, the work of organizations in the area of digitization has intensified significantly. This trend is also evident in the field of recruitment where job application tracking systems (ATS) have been developed to allow job advertisements to be published online. However, recent studies have shown that recruiting in most organizations is not inclusive, being subject to human biases and prejudices. Most discrimination activities appear early but subtly in the hiring process, for instance, exclusive phrasing in job advertisement discourages qualified applicants from minority groups from applying. The existing works are limited to analyzing, categorizing and highlighting the occurrence of bias in the recruitment process. In this paper, we go beyond this and develop machine learning models for identifying and classifying biased and discriminatory language in job descriptions. We develop and evaluate a machine learning system for identifying five major categories of biased and discriminatory language in job advertisements, i.e., masculine-coded, feminine-coded, exclusive, LGBTQ-coded, demographic and racial language. We utilized the combination of linguistic features with recent state-of-the-art word embeddings representations as input features for various machine learning classifiers. Our results show that the machine learning classifiers were able to identify all the five categories of biased and discriminatory language with a decent accuracy. The Random Forest classifier with FastText word embeddings achieved the best performance with tenfolds cross-validation. Our system directly addresses the bias in the attraction phase of hiring by identifying and classifying biased and discriminatory language and thus encouraging recruiters to write more inclusive job advertisements.",
            "corpus_id": 253015767,
            "sentences": [
                {
                    "corpus_id": "253015767",
                    "title": "A machine learning approach to recognize bias and discrimination in job advertisements",
                    "text": "In this section, we present the results of various machine learning models on the EMSCAD dataset. The dataset was divided into 80% training and 20% testing set. The evaluation metrics: accuracy, precision, recall and F1-score were computed for each model. Figure 1 presents the evaluation metrics for various classifiers with different feature sets. The results indicate that the RF classifier with BERT word embeddings as textual feature achieved the best performance. This illustrates that contextual word embedding representations such as BERT had a superior performance over the non-contextual word embeddings such as FastText and Word-2vec. We also observe that tree-based (Random Forest and Decision Tree) classifiers had a better performance in classifying biased and discriminatory language as compared to the remaining classifiers. Among the textual features, word embedding representations BERT, FastText and ELMo in combination with the RF classifier had the best performance. This was followed by FastText, ELMo and Flair word embeddings in combination with the DT classifier. \n\nWe further evaluate the various machine learning classifiers with different word embedding representations as features using tenfolds cross-validation. Figure 2 presents the macro-averages of the precision, recall and F1-score Figures 7 and 8 present the confusion matrices of the two best performing models: (1) RF classifier with FastText word embeddings and (2) RF classifier with BERT word embeddings. The results in Figs. 7 and 8 indicate that all the five classes of biased and discriminatory language were distinguishable from each other. \n\nWe observe a linear improvement in the performance of a sample of our models as training size increases in our primary experiment. We wanted to see if the behavior was by any chance related to these particular models or statistically grounded across all our implemented models. To validate this improvement, we included additional data (3000 additional job descriptions) and ran a new experiment with for the lightweight classifiers (DT, LR and NB). \n\nThe results obtained from the new experiment are shown in Fig. 9 for the regular models (80% training set and 20% test set), and Fig. 10 for the tenfold cross-validated models.",
                    "score": 0.48252122184862584,
                    "section_title": "Results and analysis",
                    "char_start_offset": 21097,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 97
                        },
                        {
                            "start": 98,
                            "end": 160
                        },
                        {
                            "start": 161,
                            "end": 255
                        },
                        {
                            "start": 256,
                            "end": 349
                        },
                        {
                            "start": 350,
                            "end": 469
                        },
                        {
                            "start": 470,
                            "end": 645
                        },
                        {
                            "start": 646,
                            "end": 840
                        },
                        {
                            "start": 841,
                            "end": 987
                        },
                        {
                            "start": 988,
                            "end": 1088
                        },
                        {
                            "start": 1091,
                            "end": 1242
                        },
                        {
                            "start": 1243,
                            "end": 1496
                        },
                        {
                            "start": 1497,
                            "end": 1517
                        },
                        {
                            "start": 1518,
                            "end": 1636
                        },
                        {
                            "start": 1639,
                            "end": 1769
                        },
                        {
                            "start": 1770,
                            "end": 1916
                        },
                        {
                            "start": 1917,
                            "end": 2088
                        },
                        {
                            "start": 2091,
                            "end": 2267
                        }
                    ],
                    "ref_mentions": [],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.77197265625
                }
            ],
            "relevance_judgement": 0.77197265625,
            "relevance_judgment_input_expanded": "# Title: A machine learning approach to recognize bias and discrimination in job advertisements\n# Venue: Ai & Society\n# Authors: Richard Frissen, K. Adebayo, Rohan Nanda\n## Abstract\nIn recent years, the work of organizations in the area of digitization has intensified significantly. This trend is also evident in the field of recruitment where job application tracking systems (ATS) have been developed to allow job advertisements to be published online. However, recent studies have shown that recruiting in most organizations is not inclusive, being subject to human biases and prejudices. Most discrimination activities appear early but subtly in the hiring process, for instance, exclusive phrasing in job advertisement discourages qualified applicants from minority groups from applying. The existing works are limited to analyzing, categorizing and highlighting the occurrence of bias in the recruitment process. In this paper, we go beyond this and develop machine learning models for identifying and classifying biased and discriminatory language in job descriptions. We develop and evaluate a machine learning system for identifying five major categories of biased and discriminatory language in job advertisements, i.e., masculine-coded, feminine-coded, exclusive, LGBTQ-coded, demographic and racial language. We utilized the combination of linguistic features with recent state-of-the-art word embeddings representations as input features for various machine learning classifiers. Our results show that the machine learning classifiers were able to identify all the five categories of biased and discriminatory language with a decent accuracy. The Random Forest classifier with FastText word embeddings achieved the best performance with tenfolds cross-validation. Our system directly addresses the bias in the attraction phase of hiring by identifying and classifying biased and discriminatory language and thus encouraging recruiters to write more inclusive job advertisements.\n## Results and analysis\nIn this section, we present the results of various machine learning models on the EMSCAD dataset. The dataset was divided into 80% training and 20% testing set. The evaluation metrics: accuracy, precision, recall and F1-score were computed for each model. Figure 1 presents the evaluation metrics for various classifiers with different feature sets. The results indicate that the RF classifier with BERT word embeddings as textual feature achieved the best performance. This illustrates that contextual word embedding representations such as BERT had a superior performance over the non-contextual word embeddings such as FastText and Word-2vec. We also observe that tree-based (Random Forest and Decision Tree) classifiers had a better performance in classifying biased and discriminatory language as compared to the remaining classifiers. Among the textual features, word embedding representations BERT, FastText and ELMo in combination with the RF classifier had the best performance. This was followed by FastText, ELMo and Flair word embeddings in combination with the DT classifier. \n\nWe further evaluate the various machine learning classifiers with different word embedding representations as features using tenfolds cross-validation. Figure 2 presents the macro-averages of the precision, recall and F1-score Figures 7 and 8 present the confusion matrices of the two best performing models: (1) RF classifier with FastText word embeddings and (2) RF classifier with BERT word embeddings. The results in Figs. 7 and 8 indicate that all the five classes of biased and discriminatory language were distinguishable from each other. \n\nWe observe a linear improvement in the performance of a sample of our models as training size increases in our primary experiment. We wanted to see if the behavior was by any chance related to these particular models or statistically grounded across all our implemented models. To validate this improvement, we included additional data (3000 additional job descriptions) and ran a new experiment with for the lightweight classifiers (DT, LR and NB). \n\nThe results obtained from the new experiment are shown in Fig. 9 for the regular models (80% training set and 20% test set), and Fig. 10 for the tenfold cross-validated models.",
            "reference_string": "[253015767 | Frissen et al. | 2022 | Citations: 19]"
        },
        {
            "title": "A Comparison of Different Source Code Representation Methods for Vulnerability Prediction in Python",
            "venue": "Quality of Information and Communications Technology",
            "year": 2021,
            "reference_count": 28,
            "citation_count": 17,
            "influential_citation_count": 1,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://arxiv.org/pdf/2108.02044",
                "status": "GREEN",
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2108.02044, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2059362210",
                    "name": "Amir Bagheri"
                },
                {
                    "authorId": "2076092",
                    "name": "P\u00e9ter Heged\u00fcs"
                }
            ],
            "abstract": "In the age of big data and machine learning, at a time when the techniques and methods of software development are evolving rapidly, a problem has arisen: programmers can no longer detect all the security flaws and vulnerabilities in their code manually. To overcome this problem, developers can now rely on automatic techniques, like machine learning based prediction models, to detect such issues. An inherent property of such approaches is that they work with numeric vectors (i.e., feature vectors) as inputs. Therefore, one needs to transform the source code into such feature vectors, often referred to as code embedding. A popular approach for code embedding is to adapt natural language processing techniques, like text representation, to automatically derive the necessary features from the source code. However, the suitability and comparison of different text representation techniques for solving Software Engineering (SE) problems is rarely studied systematically. In this paper, we present a comparative study on three popular text representation methods, word2vec, fastText, and BERT applied to the SE task of detecting vulnerabilities in Python code. Using a data mining approach, we collected a large volume of Python source code in both vulnerable and fixed forms that we embedded with word2vec, fastText, and BERT to vectors and used a Long Short-Term Memory network to train on them. Using the same LSTM architecture, we could compare the efficiency of the different embeddings in deriving meaningful feature vectors. Our findings show that all the text representation methods are suitable for code representation in this particular task, but the BERT model is the most promising as it is the least time consuming and the LSTM model based on it achieved the best overall accuracy(93.8%) in predicting Python source code vulnerabilities.",
            "corpus_id": 236912522,
            "sentences": [
                {
                    "corpus_id": "236912522",
                    "title": "A Comparison of Different Source Code Representation Methods for Vulnerability Prediction in Python",
                    "text": "FastText: We use the exact same process for calculating fastText [15] embeddings as for word2vec. This means, we apply the same data analysis and tokenizer scripts and train the embedding model with the same Python code corpus. We chose fastText as a study subject because word2vec only learns vectors for words that are complete in the training corpus. FastText, on the other hand, learns vectors for both the n-grams and the full words contained inside each word. Fast-Text uses the mean of the target word vector and its component n-gram vectors for training at each step. The change derived from the error is then applied uniformly to all of the vectors that were combined to form the target. This adds a significant amount of extra computation to the training step. A word must sum and average its n-gram component parts at each point. BERT: Bidirectional Encoder Representations from Transformers (BERT) is a Google-developed Transformer-based machine learning technique for natural language processing (NLP) pre-training [11]. Jacob Devlin and his Google colleagues developed and released BERT in 2018. We selected this embedding method for comparison due to its recent successes within the NLP field. As the BERT model is pre-trained on natural language texts, to adopt it to source code, we used its Microsoft's variant, called CodeBERT [12]. CodeBERT is a pre-trained BERT model for programming languages. In the context of this paper, we used BERT only as an embedding method and we feed all output tokens to an LSTM model. The biggest difference between BERT and the other two embedding methods is that the training part of the embedding model is done in advance using a huge corpus in case of BERT, while for word2vec and fastText, we need to do the training locally. This means that CodeBERT can be used out of the box, without having to train a model for token embeddings.",
                    "score": 0.5429055436771162,
                    "section_title": "Fig. 3. Transforming code into vectors",
                    "char_start_offset": 14449,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 97
                        },
                        {
                            "start": 98,
                            "end": 227
                        },
                        {
                            "start": 228,
                            "end": 353
                        },
                        {
                            "start": 354,
                            "end": 465
                        },
                        {
                            "start": 466,
                            "end": 575
                        },
                        {
                            "start": 576,
                            "end": 696
                        },
                        {
                            "start": 697,
                            "end": 770
                        },
                        {
                            "start": 771,
                            "end": 840
                        },
                        {
                            "start": 841,
                            "end": 1033
                        },
                        {
                            "start": 1034,
                            "end": 1109
                        },
                        {
                            "start": 1110,
                            "end": 1208
                        },
                        {
                            "start": 1209,
                            "end": 1351
                        },
                        {
                            "start": 1352,
                            "end": 1415
                        },
                        {
                            "start": 1416,
                            "end": 1534
                        },
                        {
                            "start": 1535,
                            "end": 1780
                        },
                        {
                            "start": 1781,
                            "end": 1887
                        }
                    ],
                    "ref_mentions": [],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.76953125
                }
            ],
            "relevance_judgement": 0.76953125,
            "relevance_judgment_input_expanded": "# Title: A Comparison of Different Source Code Representation Methods for Vulnerability Prediction in Python\n# Venue: Quality of Information and Communications Technology\n# Authors: Amir Bagheri, P\u00e9ter Heged\u00fcs\n## Abstract\nIn the age of big data and machine learning, at a time when the techniques and methods of software development are evolving rapidly, a problem has arisen: programmers can no longer detect all the security flaws and vulnerabilities in their code manually. To overcome this problem, developers can now rely on automatic techniques, like machine learning based prediction models, to detect such issues. An inherent property of such approaches is that they work with numeric vectors (i.e., feature vectors) as inputs. Therefore, one needs to transform the source code into such feature vectors, often referred to as code embedding. A popular approach for code embedding is to adapt natural language processing techniques, like text representation, to automatically derive the necessary features from the source code. However, the suitability and comparison of different text representation techniques for solving Software Engineering (SE) problems is rarely studied systematically. In this paper, we present a comparative study on three popular text representation methods, word2vec, fastText, and BERT applied to the SE task of detecting vulnerabilities in Python code. Using a data mining approach, we collected a large volume of Python source code in both vulnerable and fixed forms that we embedded with word2vec, fastText, and BERT to vectors and used a Long Short-Term Memory network to train on them. Using the same LSTM architecture, we could compare the efficiency of the different embeddings in deriving meaningful feature vectors. Our findings show that all the text representation methods are suitable for code representation in this particular task, but the BERT model is the most promising as it is the least time consuming and the LSTM model based on it achieved the best overall accuracy(93.8%) in predicting Python source code vulnerabilities.\n## Fig. 3. Transforming code into vectors\nFastText: We use the exact same process for calculating fastText [15] embeddings as for word2vec. This means, we apply the same data analysis and tokenizer scripts and train the embedding model with the same Python code corpus. We chose fastText as a study subject because word2vec only learns vectors for words that are complete in the training corpus. FastText, on the other hand, learns vectors for both the n-grams and the full words contained inside each word. Fast-Text uses the mean of the target word vector and its component n-gram vectors for training at each step. The change derived from the error is then applied uniformly to all of the vectors that were combined to form the target. This adds a significant amount of extra computation to the training step. A word must sum and average its n-gram component parts at each point. BERT: Bidirectional Encoder Representations from Transformers (BERT) is a Google-developed Transformer-based machine learning technique for natural language processing (NLP) pre-training [11]. Jacob Devlin and his Google colleagues developed and released BERT in 2018. We selected this embedding method for comparison due to its recent successes within the NLP field. As the BERT model is pre-trained on natural language texts, to adopt it to source code, we used its Microsoft's variant, called CodeBERT [12]. CodeBERT is a pre-trained BERT model for programming languages. In the context of this paper, we used BERT only as an embedding method and we feed all output tokens to an LSTM model. The biggest difference between BERT and the other two embedding methods is that the training part of the embedding model is done in advance using a huge corpus in case of BERT, while for word2vec and fastText, we need to do the training locally. This means that CodeBERT can be used out of the box, without having to train a model for token embeddings.",
            "reference_string": "[236912522 | Bagheri et al. | 2021 | Citations: 17]"
        },
        {
            "title": "Assessing Guest Nationality Composition from Hotel Reviews",
            "venue": "Swiss Conference on Data Science",
            "year": 2022,
            "reference_count": 2,
            "citation_count": 0,
            "influential_citation_count": 0,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://arxiv.org/pdf/2308.06175",
                "status": "GREEN",
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2308.06175, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2026999646",
                    "name": "Fabian Gr\u00f6ger"
                },
                {
                    "authorId": "1715783",
                    "name": "M. Pouly"
                },
                {
                    "authorId": "2187827840",
                    "name": "Flavia Tinner"
                },
                {
                    "authorId": "39589520",
                    "name": "Leif Brandes"
                }
            ],
            "abstract": "Many hotels target guest acquisition efforts to specific markets in order to best anticipate individual preferences and needs of their guests. Likewise, such strategic positioning is a prerequisite for efficient marketing budget allocation. Official statistics report on the number of visitors from different countries, but no fine-grained information on the guest composition of individual businesses exists. There is, however, growing interest in such data from competitors, suppliers, researchers and the general public. We demonstrate how machine learning can be leveraged to extract references to guest nationalities from unstructured text reviews in order to dynamically assess and monitor the dynamics of guest composition of individual businesses. In particular, we show that a rather simple architecture of pre-trained embeddings and stacked LSTM layers provides a better performance-runtime tradeoff than more complex state-of-the-art language models.",
            "corpus_id": 252901000,
            "sentences": [
                {
                    "corpus_id": "252901000",
                    "title": "Assessing Guest Nationality Composition from Hotel Reviews",
                    "text": "Multiple machine learning models of varying complexity were trained on the training set. The results from the validation set are displayed in Table I. The best performing model consists of pre-trained FastText embeddings followed by a stack of bidirectional LSTM layers. This rather simple architecture performs more that 20% better in F 1 compared to a support vector machine baseline with TF-IDF feature engineering and also beats transfer learning with a pretrained BERT model 3 by a small margin. On the one hand, this indicates that such large language models as BERT can generalize pretty well even in the presence of small data. On the other hand, BERT comes with an increase of 160% in the number of parameters and thus significantly higher inference time compared to the winning model. Hence, even with more data and an expected increase in the BERT model performance, the LSTM architecture will show the better performance-runtime tradeoff. Table II shows an excerpt of a qualitative evaluation of the best performing model by displaying its predictions in specific edge cases. On the hidden test set prepared with independent annotators, the winning model achieved an F 1 score of 93.2%, convincingly demonstrating its capability to generalize to reviews from other sources. For comparison, the keyword-based approach without supervised learning would only achieve an F 1 score as low as 31.3%.",
                    "score": 0.4877318385480758,
                    "section_title": "IV. RESULTS",
                    "char_start_offset": 5875,
                    "sentence_offsets": [],
                    "ref_mentions": [],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.76904296875
                }
            ],
            "relevance_judgement": 0.76904296875,
            "relevance_judgment_input_expanded": "# Title: Assessing Guest Nationality Composition from Hotel Reviews\n# Venue: Swiss Conference on Data Science\n# Authors: Fabian Gr\u00f6ger, M. Pouly, Flavia Tinner, Leif Brandes\n## Abstract\nMany hotels target guest acquisition efforts to specific markets in order to best anticipate individual preferences and needs of their guests. Likewise, such strategic positioning is a prerequisite for efficient marketing budget allocation. Official statistics report on the number of visitors from different countries, but no fine-grained information on the guest composition of individual businesses exists. There is, however, growing interest in such data from competitors, suppliers, researchers and the general public. We demonstrate how machine learning can be leveraged to extract references to guest nationalities from unstructured text reviews in order to dynamically assess and monitor the dynamics of guest composition of individual businesses. In particular, we show that a rather simple architecture of pre-trained embeddings and stacked LSTM layers provides a better performance-runtime tradeoff than more complex state-of-the-art language models.\n## IV. RESULTS\nMultiple machine learning models of varying complexity were trained on the training set. The results from the validation set are displayed in Table I. The best performing model consists of pre-trained FastText embeddings followed by a stack of bidirectional LSTM layers. This rather simple architecture performs more that 20% better in F 1 compared to a support vector machine baseline with TF-IDF feature engineering and also beats transfer learning with a pretrained BERT model 3 by a small margin. On the one hand, this indicates that such large language models as BERT can generalize pretty well even in the presence of small data. On the other hand, BERT comes with an increase of 160% in the number of parameters and thus significantly higher inference time compared to the winning model. Hence, even with more data and an expected increase in the BERT model performance, the LSTM architecture will show the better performance-runtime tradeoff. Table II shows an excerpt of a qualitative evaluation of the best performing model by displaying its predictions in specific edge cases. On the hidden test set prepared with independent annotators, the winning model achieved an F 1 score of 93.2%, convincingly demonstrating its capability to generalize to reviews from other sources. For comparison, the keyword-based approach without supervised learning would only achieve an F 1 score as low as 31.3%.",
            "reference_string": "[252901000 | Groger et al. | 2022 | Citations: 0]"
        },
        {
            "title": "Transformer based Contextual Model for Sentiment Analysis of Customer Reviews: A Fine-tuned BERT",
            "venue": "International Journal of Advanced Computer Science and Applications",
            "year": 2021,
            "reference_count": 27,
            "citation_count": 29,
            "influential_citation_count": 1,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "http://thesai.org/Downloads/Volume12No11/Paper_53-Transformer_based_Contextual_Model_for_Sentiment_Analysis.pdf",
                "status": "GOLD",
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.14569/ijacsa.2021.0121153?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.14569/ijacsa.2021.0121153, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "90290875",
                    "name": "A. Durairaj"
                },
                {
                    "authorId": "2048021830",
                    "name": "Anandan Chinnalagu"
                }
            ],
            "abstract": "\u2014The Bidirectional Encoder Representations from Transformers (BERT) is a state-of-the-art language model used for multiple natural language processing tasks and sequential modeling applications. The accuracy of predictions from context-based sentiment and analysis of customer review data from various social media platforms are challenging and time-consuming tasks due to the high volumes of unstructured data. In recent years, more research has been conducted based on the recurrent neural network algorithm, Long Short-Term Memory (LSTM), Bidirectional LSTM (BiLSTM) as well as hybrid, neutral, and traditional text classification algorithms. This paper presents our experimental research work to overcome these known challenges of the sentiment analysis models, such as its performance, accuracy, and context-based predictions. We\u2019ve proposed a fine-tuned BERT model to predict customer sentiments through the utilization of customer reviews from Twitter, IMDB Movie Reviews, Yelp, Amazon. In addition, we compared the results of the proposed model with our custom Linear Support Vector Machine (LSVM), fastText, BiLSTM and hybrid fastText-BiLSTM models, as well as presented a comparative analysis dashboard report. This experiment result shows that the proposed model performs better than other models with respect to various performance measures.",
            "corpus_id": 244895506,
            "sentences": [
                {
                    "corpus_id": "244895506",
                    "title": "Transformer based Contextual Model for Sentiment Analysis of Customer Reviews: A Fine-tuned BERT",
                    "text": "The proposed BERT model outperforms in terms of accuracy and model performance compare to other models. The results of the fastText model showed low accuracy when unigram and bigram methods were used for training the model. The overall model training and data preparation tasks took less time for BERT model in comparison to others. This experiment reveals that the BERT model required more computational resources to train compared with other traditional models. The fastText model performed well with a standard server environment with minimal computational resources compare to other models. The fine-tuned BERT model simplifies the sentiment analysis tasks on large datasets. \n\nA proposal for future research can be made to build transformer models for various domains, and framework for a continuous model trained to streamline data processing methods. In the future, more work will be done to conduct pre-training a BERT-base model on datasets of customer reviews for sentiment analysis and detecting emotions.",
                    "score": 0.6015251771203731,
                    "section_title": "V. CONCLUSION AND FUTURE WORK",
                    "char_start_offset": 14497,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 103
                        },
                        {
                            "start": 104,
                            "end": 223
                        },
                        {
                            "start": 224,
                            "end": 332
                        },
                        {
                            "start": 333,
                            "end": 463
                        },
                        {
                            "start": 464,
                            "end": 594
                        },
                        {
                            "start": 595,
                            "end": 679
                        },
                        {
                            "start": 682,
                            "end": 857
                        },
                        {
                            "start": 858,
                            "end": 1016
                        }
                    ],
                    "ref_mentions": [],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.7607421875
                }
            ],
            "relevance_judgement": 0.7607421875,
            "relevance_judgment_input_expanded": "# Title: Transformer based Contextual Model for Sentiment Analysis of Customer Reviews: A Fine-tuned BERT\n# Venue: International Journal of Advanced Computer Science and Applications\n# Authors: A. Durairaj, Anandan Chinnalagu\n## Abstract\n\u2014The Bidirectional Encoder Representations from Transformers (BERT) is a state-of-the-art language model used for multiple natural language processing tasks and sequential modeling applications. The accuracy of predictions from context-based sentiment and analysis of customer review data from various social media platforms are challenging and time-consuming tasks due to the high volumes of unstructured data. In recent years, more research has been conducted based on the recurrent neural network algorithm, Long Short-Term Memory (LSTM), Bidirectional LSTM (BiLSTM) as well as hybrid, neutral, and traditional text classification algorithms. This paper presents our experimental research work to overcome these known challenges of the sentiment analysis models, such as its performance, accuracy, and context-based predictions. We\u2019ve proposed a fine-tuned BERT model to predict customer sentiments through the utilization of customer reviews from Twitter, IMDB Movie Reviews, Yelp, Amazon. In addition, we compared the results of the proposed model with our custom Linear Support Vector Machine (LSVM), fastText, BiLSTM and hybrid fastText-BiLSTM models, as well as presented a comparative analysis dashboard report. This experiment result shows that the proposed model performs better than other models with respect to various performance measures.\n## V. CONCLUSION AND FUTURE WORK\nThe proposed BERT model outperforms in terms of accuracy and model performance compare to other models. The results of the fastText model showed low accuracy when unigram and bigram methods were used for training the model. The overall model training and data preparation tasks took less time for BERT model in comparison to others. This experiment reveals that the BERT model required more computational resources to train compared with other traditional models. The fastText model performed well with a standard server environment with minimal computational resources compare to other models. The fine-tuned BERT model simplifies the sentiment analysis tasks on large datasets. \n\nA proposal for future research can be made to build transformer models for various domains, and framework for a continuous model trained to streamline data processing methods. In the future, more work will be done to conduct pre-training a BERT-base model on datasets of customer reviews for sentiment analysis and detecting emotions.",
            "reference_string": "[244895506 | Durairaj et al. | 2021 | Citations: 29]"
        },
        {
            "title": "A Comprehensive Study on NLP Data Augmentation for Hate Speech Detection: Legacy Methods, BERT, and LLMs",
            "venue": "arXiv.org",
            "year": 2024,
            "reference_count": 50,
            "citation_count": 6,
            "influential_citation_count": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2404.00303, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "89607889",
                    "name": "Md Saroar Jahan"
                },
                {
                    "authorId": "2065873917",
                    "name": "M. Oussalah"
                },
                {
                    "authorId": "26940993",
                    "name": "D. Beddiar"
                },
                {
                    "authorId": "2137750963",
                    "name": "Jhuma Kabir Mim"
                },
                {
                    "authorId": "1644031647",
                    "name": "Nabil Arhab"
                }
            ],
            "abstract": "The surge of interest in data augmentation within the realm of NLP has been driven by the need to address challenges posed by hate speech domains, the dynamic nature of social media vocabulary, and the demands for large-scale neural networks requiring extensive training data. However, the prevalent use of lexical substitution in data augmentation has raised concerns, as it may inadvertently alter the intended meaning, thereby impacting the efficacy of supervised machine learning models. In pursuit of suitable data augmentation methods, this study explores both established legacy approaches and contemporary practices such as Large Language Models (LLM), including GPT in Hate Speech detection. Additionally, we propose an optimized utilization of BERT-based encoder models with contextual cosine similarity filtration, exposing significant limitations in prior synonym substitution methods. Our comparative analysis encompasses five popular augmentation techniques: WordNet and Fast-Text synonym replacement, Back-translation, BERT-mask contextual augmentation, and LLM. Our analysis across five benchmarked datasets revealed that while traditional methods like back-translation show low label alteration rates (0.3-1.5%), and BERT-based contextual synonym replacement offers sentence diversity but at the cost of higher label alteration rates (over 6%). Our proposed BERT-based contextual cosine similarity filtration markedly reduced label alteration to just 0.05%, demonstrating its efficacy in 0.7% higher F1 performance. However, augmenting data with GPT-3 not only avoided overfitting with up to sevenfold data increase but also improved embedding space coverage by 15% and classification F1 score by 1.4% over traditional methods, and by 0.8% over our method.",
            "corpus_id": 268819482,
            "sentences": [
                {
                    "corpus_id": "268819482",
                    "title": "A Comprehensive Study on NLP Data Augmentation for Hate Speech Detection: Legacy Methods, BERT, and LLMs",
                    "text": "The experiment was conducted in three stages.Initially, we evaluated a dataset without any augmentation.Subsequently, we applied four distinct augmentation techniques: WordNet, FastText, Back-translation, and BERT masking.In the final stage, we applied BERT-base cosine similarity filtering to each augmented dataset.Additionally, we assessed the augmentation performance of GPT-3.\n\nTable 14 shows results of not using the data augmentation.In this analysis, we chose the optimal classifier architecture.The comparison reveals that, in the absence of data augmentation, BERT marginally surpasses CNN, showing a slight improvement of 0.9% in accuracy and 1.0% in F1 score.Consequently, for experiments involving data augmentation, our focus has been solely on BERT, the state-of-the-art (SOTA) model demonstrating the best performance.\n\nTable 15 exhibits the experiment results of non-augmented and augmented datasets with and without BERT-cosine for four different datasets: AskFm, HASOC, TREC, and SST1.The examination of BERT-based classifier performance across these four distinct datasets-AskFm, HASOC, TREC, and SST1-reveals insights into the effects of data augmentation and the role of BERT-cosine similarity filtration.Initially, the impact of data augmentation methods such as WordNet, FastText, and BERT, when applied without BERT-cosine similarity, shows minimal to no improvements in accuracy and F1 scores.For example, BERT-mask data augmentation for AskFm has resulted in a drop of 0.3% in accuracy and 0.2% in F1.This suggests that the mere addition of augmented sentences does not guarantee enhanced model performance, emphasizing the necessity for quality over quantity in augmentation strategies.\n\nAmong the various data augmentation methods evaluated, back-translation demonstrated the best performance, showing a 0.5% improvement despite generating a smaller number of augmented sentences compared to WordNet, FastText, and BERT-masking methods.One possible explanation for this is that, although back-translation produces fewer sentences, it results in less label alteration compared to other methods, as observed in the previous table 12.",
                    "score": 0.43835803083347114,
                    "section_title": "Classification Results",
                    "char_start_offset": 46644,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 45
                        },
                        {
                            "start": 45,
                            "end": 104
                        },
                        {
                            "start": 104,
                            "end": 222
                        },
                        {
                            "start": 222,
                            "end": 317
                        },
                        {
                            "start": 317,
                            "end": 381
                        },
                        {
                            "start": 383,
                            "end": 441
                        },
                        {
                            "start": 441,
                            "end": 504
                        },
                        {
                            "start": 504,
                            "end": 671
                        },
                        {
                            "start": 671,
                            "end": 834
                        },
                        {
                            "start": 836,
                            "end": 1004
                        },
                        {
                            "start": 1004,
                            "end": 1227
                        },
                        {
                            "start": 1227,
                            "end": 1419
                        },
                        {
                            "start": 1419,
                            "end": 1528
                        },
                        {
                            "start": 1528,
                            "end": 1714
                        },
                        {
                            "start": 1716,
                            "end": 1965
                        },
                        {
                            "start": 1965,
                            "end": 2160
                        }
                    ],
                    "ref_mentions": [],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.75341796875
                }
            ],
            "relevance_judgement": 0.75341796875,
            "relevance_judgment_input_expanded": "# Title: A Comprehensive Study on NLP Data Augmentation for Hate Speech Detection: Legacy Methods, BERT, and LLMs\n# Venue: arXiv.org\n# Authors: Md Saroar Jahan, M. Oussalah, D. Beddiar, Jhuma Kabir Mim, Nabil Arhab\n## Abstract\nThe surge of interest in data augmentation within the realm of NLP has been driven by the need to address challenges posed by hate speech domains, the dynamic nature of social media vocabulary, and the demands for large-scale neural networks requiring extensive training data. However, the prevalent use of lexical substitution in data augmentation has raised concerns, as it may inadvertently alter the intended meaning, thereby impacting the efficacy of supervised machine learning models. In pursuit of suitable data augmentation methods, this study explores both established legacy approaches and contemporary practices such as Large Language Models (LLM), including GPT in Hate Speech detection. Additionally, we propose an optimized utilization of BERT-based encoder models with contextual cosine similarity filtration, exposing significant limitations in prior synonym substitution methods. Our comparative analysis encompasses five popular augmentation techniques: WordNet and Fast-Text synonym replacement, Back-translation, BERT-mask contextual augmentation, and LLM. Our analysis across five benchmarked datasets revealed that while traditional methods like back-translation show low label alteration rates (0.3-1.5%), and BERT-based contextual synonym replacement offers sentence diversity but at the cost of higher label alteration rates (over 6%). Our proposed BERT-based contextual cosine similarity filtration markedly reduced label alteration to just 0.05%, demonstrating its efficacy in 0.7% higher F1 performance. However, augmenting data with GPT-3 not only avoided overfitting with up to sevenfold data increase but also improved embedding space coverage by 15% and classification F1 score by 1.4% over traditional methods, and by 0.8% over our method.\n## Classification Results\nThe experiment was conducted in three stages.Initially, we evaluated a dataset without any augmentation.Subsequently, we applied four distinct augmentation techniques: WordNet, FastText, Back-translation, and BERT masking.In the final stage, we applied BERT-base cosine similarity filtering to each augmented dataset.Additionally, we assessed the augmentation performance of GPT-3.\n\nTable 14 shows results of not using the data augmentation.In this analysis, we chose the optimal classifier architecture.The comparison reveals that, in the absence of data augmentation, BERT marginally surpasses CNN, showing a slight improvement of 0.9% in accuracy and 1.0% in F1 score.Consequently, for experiments involving data augmentation, our focus has been solely on BERT, the state-of-the-art (SOTA) model demonstrating the best performance.\n\nTable 15 exhibits the experiment results of non-augmented and augmented datasets with and without BERT-cosine for four different datasets: AskFm, HASOC, TREC, and SST1.The examination of BERT-based classifier performance across these four distinct datasets-AskFm, HASOC, TREC, and SST1-reveals insights into the effects of data augmentation and the role of BERT-cosine similarity filtration.Initially, the impact of data augmentation methods such as WordNet, FastText, and BERT, when applied without BERT-cosine similarity, shows minimal to no improvements in accuracy and F1 scores.For example, BERT-mask data augmentation for AskFm has resulted in a drop of 0.3% in accuracy and 0.2% in F1.This suggests that the mere addition of augmented sentences does not guarantee enhanced model performance, emphasizing the necessity for quality over quantity in augmentation strategies.\n\nAmong the various data augmentation methods evaluated, back-translation demonstrated the best performance, showing a 0.5% improvement despite generating a smaller number of augmented sentences compared to WordNet, FastText, and BERT-masking methods.One possible explanation for this is that, although back-translation produces fewer sentences, it results in less label alteration compared to other methods, as observed in the previous table 12.",
            "reference_string": "[268819482 | Jahan et al. | 2024 | Citations: 6]"
        },
        {
            "title": "Research on Violent Text Detection System Based on BERT-fasttext Model",
            "venue": "arXiv.org",
            "year": 2024,
            "reference_count": 13,
            "citation_count": 0,
            "influential_citation_count": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2412.16455, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2337013078",
                    "name": "Yongsheng Yang"
                },
                {
                    "authorId": "2337081796",
                    "name": "Xiaoying Wang"
                }
            ],
            "abstract": "In the digital age of today, the internet has become an indispensable platform for people's lives, work, and information exchange. However, the problem of violent text proliferation in the network environment has arisen, which has brought about many negative effects. In view of this situation, it is particularly important to build an effective system for cutting off violent text. The study of violent text cutting off based on the BERT-fasttext model has significant meaning. BERT is a pre-trained language model with strong natural language understanding ability, which can deeply mine and analyze text semantic information; Fasttext itself is an efficient text classification tool with low complexity and good effect, which can quickly provide basic judgments for text processing. By combining the two and applying them to the system for cutting off violent text, on the one hand, it can accurately identify violent text, and on the other hand, it can efficiently and reasonably cut off the content, preventing harmful information from spreading freely on the network. Compared with the single BERT model and fasttext, the accuracy was improved by 0.7% and 0.8%, respectively. Through this model, it is helpful to purify the network environment, maintain the health of network information, and create a positive, civilized, and harmonious online communication space for netizens, driving the development of social networking, information dissemination, and other aspects in a more benign direction.",
            "corpus_id": 274982292,
            "sentences": [
                {
                    "corpus_id": "274982292",
                    "title": "Research on Violent Text Detection System Based on BERT-fasttext Model",
                    "text": "This study proposed a text detection method based on the fusion of BERT and fasttext models, explored the detection ability of violent text, and aimed to combine the text understanding ability of BERT with the efficiency of fasttext to improve the accuracy and robustness of text detection. Experimental results show that this model successfully achieved the best text detection effect on standard datasets. Specifically, the BERT model provides accurate text representation for sentiment analysis through its powerful language understanding ability, while using FastText for fast text classification has advantages in computational efficiency and reasoning speed. The contribution of this study is to combine BERT with fasttext models for violent text detection tasks, which provides new ideas for research and application in the field of text detection, and injects new vitality into the field of violent text. Future work can further explore more complex deep learning technologies and their application effects in practical scenarios.",
                    "score": 0.5149005580769268,
                    "section_title": "Conclusion",
                    "char_start_offset": 14639,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 290
                        },
                        {
                            "start": 291,
                            "end": 407
                        },
                        {
                            "start": 408,
                            "end": 664
                        },
                        {
                            "start": 665,
                            "end": 912
                        },
                        {
                            "start": 913,
                            "end": 1038
                        }
                    ],
                    "ref_mentions": [],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.74560546875
                }
            ],
            "relevance_judgement": 0.74560546875,
            "relevance_judgment_input_expanded": "# Title: Research on Violent Text Detection System Based on BERT-fasttext Model\n# Venue: arXiv.org\n# Authors: Yongsheng Yang, Xiaoying Wang\n## Abstract\nIn the digital age of today, the internet has become an indispensable platform for people's lives, work, and information exchange. However, the problem of violent text proliferation in the network environment has arisen, which has brought about many negative effects. In view of this situation, it is particularly important to build an effective system for cutting off violent text. The study of violent text cutting off based on the BERT-fasttext model has significant meaning. BERT is a pre-trained language model with strong natural language understanding ability, which can deeply mine and analyze text semantic information; Fasttext itself is an efficient text classification tool with low complexity and good effect, which can quickly provide basic judgments for text processing. By combining the two and applying them to the system for cutting off violent text, on the one hand, it can accurately identify violent text, and on the other hand, it can efficiently and reasonably cut off the content, preventing harmful information from spreading freely on the network. Compared with the single BERT model and fasttext, the accuracy was improved by 0.7% and 0.8%, respectively. Through this model, it is helpful to purify the network environment, maintain the health of network information, and create a positive, civilized, and harmonious online communication space for netizens, driving the development of social networking, information dissemination, and other aspects in a more benign direction.\n## Conclusion\nThis study proposed a text detection method based on the fusion of BERT and fasttext models, explored the detection ability of violent text, and aimed to combine the text understanding ability of BERT with the efficiency of fasttext to improve the accuracy and robustness of text detection. Experimental results show that this model successfully achieved the best text detection effect on standard datasets. Specifically, the BERT model provides accurate text representation for sentiment analysis through its powerful language understanding ability, while using FastText for fast text classification has advantages in computational efficiency and reasoning speed. The contribution of this study is to combine BERT with fasttext models for violent text detection tasks, which provides new ideas for research and application in the field of text detection, and injects new vitality into the field of violent text. Future work can further explore more complex deep learning technologies and their application effects in practical scenarios.",
            "reference_string": "[274982292 | Yang et al. | 2024 | Citations: 0]"
        },
        {
            "title": "Embed2Detect: temporally clustered embedded words for event detection in social media",
            "venue": "Machine-mediated learning",
            "year": 2020,
            "reference_count": 71,
            "citation_count": 33,
            "influential_citation_count": 1,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://link.springer.com/content/pdf/10.1007/s10994-021-05988-7.pdf",
                "status": "HYBRID",
                "license": "CCBY",
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2006.05908, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "1453653937",
                    "name": "Hansi Hettiarachchi"
                },
                {
                    "authorId": "1403921831",
                    "name": "Mariam Adedoyin-Olowe"
                },
                {
                    "authorId": "3181018",
                    "name": "Jagdev Bhogal"
                },
                {
                    "authorId": "1698684",
                    "name": "M. Gaber"
                }
            ],
            "abstract": "Social media is becoming a primary medium to discuss what is happening around the world. Therefore, the data generated by social media platforms contain rich information which describes the ongoing events. Further, the timeliness associated with these data is capable of facilitating immediate insights. However, considering the dynamic nature and high volume of data production in social media data streams, it is impractical to filter the events manually and therefore, automated event detection mechanisms are invaluable to the community. Apart from a few notable exceptions, most previous research on automated event detection have focused only on statistical and syntactical features in data and lacked the involvement of underlying semantics which are important for effective information retrieval from text since they represent the connections between words and their meanings. In this paper, we propose a novel method termed Embed2Detect for event detection in social media by combining the characteristics in word embeddings and hierarchical agglomerative clustering. The adoption of word embeddings gives Embed2Detect the capability to incorporate powerful semantical features into event detection and overcome a major limitation inherent in previous approaches. We experimented our method on two recent real social media data sets which represent the sports and political domain and also compared the results to several state-of-the-art methods. The obtained results show that Embed2Detect is capable of effective and efficient event detection and it outperforms the recent event detection methods. For the sports data set, Embed2Detect achieved 27% higher F-measure than the best-performed baseline and for the political data set, it was an increase of 29%.",
            "corpus_id": 219558245,
            "sentences": [
                {
                    "corpus_id": "219558245",
                    "title": "Embed2Detect: temporally clustered embedded words for event detection in social media",
                    "text": "According to the obtained results, classic word embedding models (e.g. Skip-gram and fastText) learn the representations faster than transformer-based models (e.g. BERT and DistilBERT). \n\nComparing fastText and Skip-gram, fastText took more time because it processes subword information. But, incorporation of subwords allows this model to capture connections between modified words. For example, consider the goal-related words found within the top 20 words with high cluster change during a goal score: Skip-gramgoal, goalll, rashyyy, scores fastTextgoalll, goooaaalll, rashford, rashyyy, @marcusrashford, scored, scores fastText captures more modified words than Skip-gram. We could not run a complete evaluation using fastText embeddings, because it requires a manual process since GT keywords only contain the words in actual form. \n\nTransformer-based models took more time than both Skip-gram and fastText due to their complex architecture to learn contextualised word embeddings. DistilBERT is found to be faster than BERT, however, the learning time of DistilBERT is not fast enough for real-time processing because it exceeds the tweet generation time. For example to learn from tweets posted during a 2-minute time window, it took approximately 7.2 minutes. If this model can be further distilled, there is a possibility to achieve the required efficiency to become suitable for real-time processing. However, further distillation can reduce the language understanding capability of the model as there is a 3% reduction in DistilBERT compared to BERT (Sanh et al. 2019). \n\nAccording to recent literature, transformer-based models performed well on many NLPrelated tasks, because of the ability to capture the contextual sense of words. BERT is capable of generating different embeddings for the same word depending on its surrounding context. In other words, the main idea behind BERT is capturing spacial changes of words. From the perspective of processing formally written natural language, this is a very useful feature. But, in social media, language is mostly informal and for event detection using social media text, temporal changes of words need to be more focused.",
                    "score": 0.519166803175414,
                    "section_title": "Extension to other word embedding models",
                    "char_start_offset": 80185,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 70
                        },
                        {
                            "start": 71,
                            "end": 163
                        },
                        {
                            "start": 164,
                            "end": 185
                        },
                        {
                            "start": 188,
                            "end": 287
                        },
                        {
                            "start": 288,
                            "end": 383
                        },
                        {
                            "start": 384,
                            "end": 676
                        },
                        {
                            "start": 677,
                            "end": 836
                        },
                        {
                            "start": 839,
                            "end": 986
                        },
                        {
                            "start": 987,
                            "end": 1161
                        },
                        {
                            "start": 1162,
                            "end": 1267
                        },
                        {
                            "start": 1268,
                            "end": 1410
                        },
                        {
                            "start": 1411,
                            "end": 1580
                        },
                        {
                            "start": 1583,
                            "end": 1745
                        },
                        {
                            "start": 1746,
                            "end": 1852
                        },
                        {
                            "start": 1853,
                            "end": 1933
                        },
                        {
                            "start": 1934,
                            "end": 2034
                        },
                        {
                            "start": 2035,
                            "end": 2184
                        }
                    ],
                    "ref_mentions": [],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.7333984375
                }
            ],
            "relevance_judgement": 0.7333984375,
            "relevance_judgment_input_expanded": "# Title: Embed2Detect: temporally clustered embedded words for event detection in social media\n# Venue: Machine-mediated learning\n# Authors: Hansi Hettiarachchi, Mariam Adedoyin-Olowe, Jagdev Bhogal, M. Gaber\n## Abstract\nSocial media is becoming a primary medium to discuss what is happening around the world. Therefore, the data generated by social media platforms contain rich information which describes the ongoing events. Further, the timeliness associated with these data is capable of facilitating immediate insights. However, considering the dynamic nature and high volume of data production in social media data streams, it is impractical to filter the events manually and therefore, automated event detection mechanisms are invaluable to the community. Apart from a few notable exceptions, most previous research on automated event detection have focused only on statistical and syntactical features in data and lacked the involvement of underlying semantics which are important for effective information retrieval from text since they represent the connections between words and their meanings. In this paper, we propose a novel method termed Embed2Detect for event detection in social media by combining the characteristics in word embeddings and hierarchical agglomerative clustering. The adoption of word embeddings gives Embed2Detect the capability to incorporate powerful semantical features into event detection and overcome a major limitation inherent in previous approaches. We experimented our method on two recent real social media data sets which represent the sports and political domain and also compared the results to several state-of-the-art methods. The obtained results show that Embed2Detect is capable of effective and efficient event detection and it outperforms the recent event detection methods. For the sports data set, Embed2Detect achieved 27% higher F-measure than the best-performed baseline and for the political data set, it was an increase of 29%.\n## Extension to other word embedding models\nAccording to the obtained results, classic word embedding models (e.g. Skip-gram and fastText) learn the representations faster than transformer-based models (e.g. BERT and DistilBERT). \n\nComparing fastText and Skip-gram, fastText took more time because it processes subword information. But, incorporation of subwords allows this model to capture connections between modified words. For example, consider the goal-related words found within the top 20 words with high cluster change during a goal score: Skip-gramgoal, goalll, rashyyy, scores fastTextgoalll, goooaaalll, rashford, rashyyy, @marcusrashford, scored, scores fastText captures more modified words than Skip-gram. We could not run a complete evaluation using fastText embeddings, because it requires a manual process since GT keywords only contain the words in actual form. \n\nTransformer-based models took more time than both Skip-gram and fastText due to their complex architecture to learn contextualised word embeddings. DistilBERT is found to be faster than BERT, however, the learning time of DistilBERT is not fast enough for real-time processing because it exceeds the tweet generation time. For example to learn from tweets posted during a 2-minute time window, it took approximately 7.2 minutes. If this model can be further distilled, there is a possibility to achieve the required efficiency to become suitable for real-time processing. However, further distillation can reduce the language understanding capability of the model as there is a 3% reduction in DistilBERT compared to BERT (Sanh et al. 2019). \n\nAccording to recent literature, transformer-based models performed well on many NLPrelated tasks, because of the ability to capture the contextual sense of words. BERT is capable of generating different embeddings for the same word depending on its surrounding context. In other words, the main idea behind BERT is capturing spacial changes of words. From the perspective of processing formally written natural language, this is a very useful feature. But, in social media, language is mostly informal and for event detection using social media text, temporal changes of words need to be more focused.",
            "reference_string": "[219558245 | Hettiarachchi et al. | 2020 | Citations: 33]"
        },
        {
            "title": "Attention-aware with stacked embedding for sentiment analysis of student feedback through deep learning techniques",
            "venue": "PeerJ Computer Science",
            "year": 2024,
            "reference_count": 55,
            "citation_count": 1,
            "influential_citation_count": 0,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://doi.org/10.7717/peerj-cs.2283",
                "status": "GOLD",
                "license": "CCBY",
                "disclaimer": "Notice: Paper or abstract available at https://pmc.ncbi.nlm.nih.gov/articles/PMC11419625, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2327353221",
                    "name": "Shanza Zafar Malik"
                },
                {
                    "authorId": "2294254945",
                    "name": "Khalid Iqbal"
                },
                {
                    "authorId": "2319267821",
                    "name": "Muhammad Sharif"
                },
                {
                    "authorId": "2151313189",
                    "name": "Yaser Ali Shah"
                },
                {
                    "authorId": "2060253868",
                    "name": "Amaad Khalil"
                },
                {
                    "authorId": "2327345984",
                    "name": "Muhammad Abeer Irfan"
                },
                {
                    "authorId": "2316178914",
                    "name": "Joann Rosak-Szyrocka"
                }
            ],
            "abstract": "Automatic polarity prediction is a challenging assessment issue. Even though polarity assessment is a critical topic with many existing applications, it is probably not an easy challenge and faces several difficulties in natural language processing (NLP). Public polling data can give useful information, and polarity assessment or classification of comments on Twitter and Facebook may be an effective approach for gaining a better understanding of user sentiments. Text embedding techniques and models related to the artificial intelligence field and sub-fields with differing and almost accurate parameters are among the approaches available for assessing student comments. Existing state-of-the-art methodologies for sentiment analysis to analyze student responses were discussed in this study endeavor. An innovative hybrid model is proposed that uses ensemble learning-based text embedding, a multi-head attention mechanism, and a combination of deep learning classifiers. The proposed model outperforms the existing state-of-the-art deep learning-based techniques. The proposed model achieves 95% accuracy, 97% recall, having a precision of 95% with an F1-score of 96% demonstrating its effectiveness in sentiment analysis of student feedback.",
            "corpus_id": 272359317,
            "sentences": [
                {
                    "corpus_id": "272359317",
                    "title": "Attention-aware with stacked embedding for sentiment analysis of student feedback through deep learning techniques",
                    "text": "In the proposed work, three important issues in the field of natural language processing: polysemy, contextual meaning, and out-of-vocabulary terms were addressed. We employed three cutting-edge models to solve these problems: FastText, Elmo, and RoBERTa. \n\nFastText is a text categorization and representation learning library that is intended to be quick and efficient. It handles out-of-vocabulary terms using sub-word information, making it appropriate for NLP jobs. Embeddings from Language Models (ELMO) is a contextualized word representation model that learns word contexts from large-scale text corpora. It captures the contextual meaning of words, making it ideally suited for NLP jobs requiring context awareness. A Robustly Optimized BERT Pre-training Approach (Roberta) is a transformer-based language model that has been refined using large-scale text corpora. \n\nIt outperforms BERT on a variety of NLP tasks, making it an effective model for NLP. The outputs of these three models were combined using ensemble stacking techniques and the results were passed through numerous levels of multi-head attention. This made it possible to identify the benefits of each model and use their problem-solving abilities to solve polysemy, contextual meaning, and vocabulary terms. We were able to generate predictions after passing the final output via classifier layers. In comparison to several baseline models and recently released models, our suggested model performed better. A comparison of our model to other models is presented in Table 4. In the proposed model, three classification techniques were used: Bi-LSTM, GRU, and Bi-GRU, as discussed in the 'Stacking: ensemble encoded features'. Bi-GRU classifier gives better results comparatively, therefore in the Table 4 only the Bi-GRU results are given in detail. The results for sentiments and topics were presented separately. Here, the average results of both categories (sentiments and topics) are given in Table 4 against the proposed model. The results demonstrate that, in terms of accuracy and other performance metrics, the proposed model performs better than existing models. \n\nFigure 8 helps to further elaborate these results. This figure shows the comparative analysis of the proposed model with other techniques. Here the top four results of Table 4 are compared with our model.",
                    "score": 0.4235524663919619,
                    "section_title": "Comparative analysis of the proposed model with existing techniques",
                    "char_start_offset": 37979,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 163
                        },
                        {
                            "start": 164,
                            "end": 255
                        },
                        {
                            "start": 258,
                            "end": 371
                        },
                        {
                            "start": 372,
                            "end": 470
                        },
                        {
                            "start": 471,
                            "end": 612
                        },
                        {
                            "start": 613,
                            "end": 724
                        },
                        {
                            "start": 725,
                            "end": 874
                        },
                        {
                            "start": 877,
                            "end": 961
                        },
                        {
                            "start": 962,
                            "end": 1121
                        },
                        {
                            "start": 1122,
                            "end": 1283
                        },
                        {
                            "start": 1284,
                            "end": 1374
                        },
                        {
                            "start": 1375,
                            "end": 1483
                        },
                        {
                            "start": 1484,
                            "end": 1550
                        },
                        {
                            "start": 1551,
                            "end": 1701
                        },
                        {
                            "start": 1702,
                            "end": 1825
                        },
                        {
                            "start": 1826,
                            "end": 1890
                        },
                        {
                            "start": 1891,
                            "end": 2008
                        },
                        {
                            "start": 2009,
                            "end": 2147
                        },
                        {
                            "start": 2150,
                            "end": 2200
                        },
                        {
                            "start": 2201,
                            "end": 2288
                        },
                        {
                            "start": 2289,
                            "end": 2354
                        }
                    ],
                    "ref_mentions": [],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.7294921875
                }
            ],
            "relevance_judgement": 0.7294921875,
            "relevance_judgment_input_expanded": "# Title: Attention-aware with stacked embedding for sentiment analysis of student feedback through deep learning techniques\n# Venue: PeerJ Computer Science\n# Authors: Shanza Zafar Malik, Khalid Iqbal, Muhammad Sharif, Yaser Ali Shah, Amaad Khalil, Muhammad Abeer Irfan, Joann Rosak-Szyrocka\n## Abstract\nAutomatic polarity prediction is a challenging assessment issue. Even though polarity assessment is a critical topic with many existing applications, it is probably not an easy challenge and faces several difficulties in natural language processing (NLP). Public polling data can give useful information, and polarity assessment or classification of comments on Twitter and Facebook may be an effective approach for gaining a better understanding of user sentiments. Text embedding techniques and models related to the artificial intelligence field and sub-fields with differing and almost accurate parameters are among the approaches available for assessing student comments. Existing state-of-the-art methodologies for sentiment analysis to analyze student responses were discussed in this study endeavor. An innovative hybrid model is proposed that uses ensemble learning-based text embedding, a multi-head attention mechanism, and a combination of deep learning classifiers. The proposed model outperforms the existing state-of-the-art deep learning-based techniques. The proposed model achieves 95% accuracy, 97% recall, having a precision of 95% with an F1-score of 96% demonstrating its effectiveness in sentiment analysis of student feedback.\n## Comparative analysis of the proposed model with existing techniques\nIn the proposed work, three important issues in the field of natural language processing: polysemy, contextual meaning, and out-of-vocabulary terms were addressed. We employed three cutting-edge models to solve these problems: FastText, Elmo, and RoBERTa. \n\nFastText is a text categorization and representation learning library that is intended to be quick and efficient. It handles out-of-vocabulary terms using sub-word information, making it appropriate for NLP jobs. Embeddings from Language Models (ELMO) is a contextualized word representation model that learns word contexts from large-scale text corpora. It captures the contextual meaning of words, making it ideally suited for NLP jobs requiring context awareness. A Robustly Optimized BERT Pre-training Approach (Roberta) is a transformer-based language model that has been refined using large-scale text corpora. \n\nIt outperforms BERT on a variety of NLP tasks, making it an effective model for NLP. The outputs of these three models were combined using ensemble stacking techniques and the results were passed through numerous levels of multi-head attention. This made it possible to identify the benefits of each model and use their problem-solving abilities to solve polysemy, contextual meaning, and vocabulary terms. We were able to generate predictions after passing the final output via classifier layers. In comparison to several baseline models and recently released models, our suggested model performed better. A comparison of our model to other models is presented in Table 4. In the proposed model, three classification techniques were used: Bi-LSTM, GRU, and Bi-GRU, as discussed in the 'Stacking: ensemble encoded features'. Bi-GRU classifier gives better results comparatively, therefore in the Table 4 only the Bi-GRU results are given in detail. The results for sentiments and topics were presented separately. Here, the average results of both categories (sentiments and topics) are given in Table 4 against the proposed model. The results demonstrate that, in terms of accuracy and other performance metrics, the proposed model performs better than existing models. \n\nFigure 8 helps to further elaborate these results. This figure shows the comparative analysis of the proposed model with other techniques. Here the top four results of Table 4 are compared with our model.",
            "reference_string": "[272359317 | Malik et al. | 2024 | Citations: 1]"
        },
        {
            "title": "Bridging Qualitative Data Silos: The Potential of Reusing Codings Through Machine Learning Based Cross-Study Code Linking",
            "venue": "Social science computer review",
            "year": 2023,
            "reference_count": 47,
            "citation_count": 0,
            "influential_citation_count": 0,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://journals.sagepub.com/doi/pdf/10.1177/08944393231215459",
                "status": "HYBRID",
                "license": "CCBY",
                "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.1177/08944393231215459?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1177/08944393231215459, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "3041146",
                    "name": "Sergej Wildemann"
                },
                {
                    "authorId": "2257974973",
                    "name": "Claudia Nieder\u00e9e"
                },
                {
                    "authorId": "2098749",
                    "name": "Erick Elejalde"
                }
            ],
            "abstract": "For qualitative data analysis (QDA), researchers assign codes to text segments to arrange the information into topics or concepts. These annotations facilitate information retrieval and the identification of emerging patterns in unstructured data. However, this metadata is typically not published or reused after the research. Subsequent studies with similar research questions require a new definition of codes and do not benefit from other analysts\u2019 experience. Machine learning (ML) based classification seeded with such data remains a challenging task due to the ambiguity of code definitions and the inherent subjectivity of the exercise. Previous attempts to support QDA using ML rely on linear models and only examined individual datasets that were either smaller or coded specifically for this purpose. However, we show that modern approaches effectively capture at least part of the codes\u2019 semantics and may generalize to multiple studies. We analyze the performance of multiple classifiers across three large real-world datasets. Furthermore, we propose an ML-based approach to identify semantic relations of codes in different studies to show thematic faceting, enhance retrieval of related content, or bootstrap the coding process. These are encouraging results that suggest how analysts might benefit from prior interpretation efforts, potentially yielding new insights into qualitative data.",
            "corpus_id": 265162335,
            "sentences": [
                {
                    "corpus_id": "265162335",
                    "title": "Bridging Qualitative Data Silos: The Potential of Reusing Codings Through Machine Learning Based Cross-Study Code Linking",
                    "text": "The results of all models after optimization are presented in Table 2.For fastText, we include the results with and without pre-training to emphasize the improvement of using custom pre-trained word embeddings over the baseline.Since the labels in the ZuL and Bank datasets are organized in two-level hierarchies based on topics, we also report results for predicting first-level labels (suffixed with \"Base\").\n\nOverall, BERT achieves the best performance on the F 1 measure in all datasets except for Bank.Even after an extended hyperparameter search, we were not able to improve the results for this last dataset.We believe that the poor performance for Bank may be due to a combination of a low sample count (727) and a relatively high number of labels (67), which likewise affects the other classifiers.Previous studies have also observed decreased performance in BERT fine-tuning when dealing with a small training set (Pruksachatkun et al., 2020;Zhu et al., 2020).\n\nfastText proves to be the second-best classifier and even outperforms BERT on the Bank dataset.The additional use of pre-trained word embeddings clearly benefits the task and shows an average increase in F 1 -score of 15.3% across all datasets.This suggests a further improvement when using larger domain-specific corpora.Moreover, its fast training and the lack of dependency on GPUs may be advantageous for many practical applications.\n\nIn contrast to the improvements from custom word embeddings in fastText, Linear SVC, and ML-KNN show the best results when operating on TF-IDF vectors instead.Linear SVC achieves very similar, albeit lower, performance values than fastText.This is quite interesting, since both work on an identical form of processed text, with Linear SVC not having the benefit of a custom language model.On the other hand, ML-KNN showed the worst performance on our datasets.While it exhibits high precision, it retrieves fewer labels overall.",
                    "score": 0.5484618210758982,
                    "section_title": "Classifier Performance",
                    "char_start_offset": 23027,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 70
                        },
                        {
                            "start": 70,
                            "end": 228
                        },
                        {
                            "start": 228,
                            "end": 410
                        },
                        {
                            "start": 412,
                            "end": 507
                        },
                        {
                            "start": 507,
                            "end": 615
                        },
                        {
                            "start": 615,
                            "end": 807
                        },
                        {
                            "start": 807,
                            "end": 970
                        },
                        {
                            "start": 972,
                            "end": 1067
                        },
                        {
                            "start": 1067,
                            "end": 1216
                        },
                        {
                            "start": 1216,
                            "end": 1294
                        },
                        {
                            "start": 1294,
                            "end": 1409
                        },
                        {
                            "start": 1411,
                            "end": 1570
                        },
                        {
                            "start": 1570,
                            "end": 1651
                        },
                        {
                            "start": 1651,
                            "end": 1800
                        },
                        {
                            "start": 1800,
                            "end": 1871
                        },
                        {
                            "start": 1871,
                            "end": 1939
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 924,
                            "end": 952,
                            "matchedPaperCorpusId": "220045835"
                        },
                        {
                            "start": 952,
                            "end": 969,
                            "matchedPaperCorpusId": "209475786"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.71484375
                }
            ],
            "relevance_judgement": 0.71484375,
            "relevance_judgment_input_expanded": "# Title: Bridging Qualitative Data Silos: The Potential of Reusing Codings Through Machine Learning Based Cross-Study Code Linking\n# Venue: Social science computer review\n# Authors: Sergej Wildemann, Claudia Nieder\u00e9e, Erick Elejalde\n## Abstract\nFor qualitative data analysis (QDA), researchers assign codes to text segments to arrange the information into topics or concepts. These annotations facilitate information retrieval and the identification of emerging patterns in unstructured data. However, this metadata is typically not published or reused after the research. Subsequent studies with similar research questions require a new definition of codes and do not benefit from other analysts\u2019 experience. Machine learning (ML) based classification seeded with such data remains a challenging task due to the ambiguity of code definitions and the inherent subjectivity of the exercise. Previous attempts to support QDA using ML rely on linear models and only examined individual datasets that were either smaller or coded specifically for this purpose. However, we show that modern approaches effectively capture at least part of the codes\u2019 semantics and may generalize to multiple studies. We analyze the performance of multiple classifiers across three large real-world datasets. Furthermore, we propose an ML-based approach to identify semantic relations of codes in different studies to show thematic faceting, enhance retrieval of related content, or bootstrap the coding process. These are encouraging results that suggest how analysts might benefit from prior interpretation efforts, potentially yielding new insights into qualitative data.\n## Classifier Performance\nThe results of all models after optimization are presented in Table 2.For fastText, we include the results with and without pre-training to emphasize the improvement of using custom pre-trained word embeddings over the baseline.Since the labels in the ZuL and Bank datasets are organized in two-level hierarchies based on topics, we also report results for predicting first-level labels (suffixed with \"Base\").\n\nOverall, BERT achieves the best performance on the F 1 measure in all datasets except for Bank.Even after an extended hyperparameter search, we were not able to improve the results for this last dataset.We believe that the poor performance for Bank may be due to a combination of a low sample count (727) and a relatively high number of labels (67), which likewise affects the other classifiers.Previous studies have also observed decreased performance in BERT fine-tuning when dealing with a small training set (Pruksachatkun et al., 2020;Zhu et al., 2020).\n\nfastText proves to be the second-best classifier and even outperforms BERT on the Bank dataset.The additional use of pre-trained word embeddings clearly benefits the task and shows an average increase in F 1 -score of 15.3% across all datasets.This suggests a further improvement when using larger domain-specific corpora.Moreover, its fast training and the lack of dependency on GPUs may be advantageous for many practical applications.\n\nIn contrast to the improvements from custom word embeddings in fastText, Linear SVC, and ML-KNN show the best results when operating on TF-IDF vectors instead.Linear SVC achieves very similar, albeit lower, performance values than fastText.This is quite interesting, since both work on an identical form of processed text, with Linear SVC not having the benefit of a custom language model.On the other hand, ML-KNN showed the worst performance on our datasets.While it exhibits high precision, it retrieves fewer labels overall.",
            "reference_string": "[265162335 | Wildemann et al. | 2023 | Citations: 0]"
        },
        {
            "title": "Transformer-based ensemble model for dialectal Arabic sentiment classification",
            "venue": "PeerJ Computer Science",
            "year": 2025,
            "reference_count": 51,
            "citation_count": 1,
            "influential_citation_count": 1,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": "CCBY",
                "disclaimer": "Notice: Paper or abstract available at https://pmc.ncbi.nlm.nih.gov/articles/PMC11948314, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2349811417",
                    "name": "Omar Mansour"
                },
                {
                    "authorId": "2349800392",
                    "name": "Eman Aboelela"
                },
                {
                    "authorId": "113611994",
                    "name": "Remon Talaat"
                },
                {
                    "authorId": "2351981901",
                    "name": "Mahmoud Bustami"
                }
            ],
            "abstract": "Social media platforms such as X, Facebook, and Instagram have become essential avenues for individuals to articulate their opinions, especially during global emergencies. These platforms offer valuable insights that necessitate analysis for informed decision-making and a deeper understanding of societal trends. Sentiment analysis is crucial for assessing public sentiment toward specific issues; however, applying it to dialectal Arabic presents considerable challenges in natural language processing. The complexity arises from the language\u2019s intricate semantic and morphological structures, along with the existence of multiple dialects. This form of analysis, also referred to as sentiment classification, opinion mining, emotion mining, and review mining, is the focus of this study, which analyzes tweets from three benchmark datasets: the Arabic Sentiment Tweets Dataset (ASTD), the A Twitter-based Benchmark Arabic Sentiment Analysis Dataset (ASAD), and the Tweets Emoji Arabic Dataset (TEAD). The research involves experimentation with a variety of comparative models, including machine learning, deep learning, transformer-based models, and a transformer-based ensemble model. Feature extraction for both machine learning and deep learning approaches is performed using techniques such as AraVec, FastText, AraBERT, and Term Frequency-Inverse Document Frequency (TF-IDF). The study compares machine learning models such as support vector machine (SVM), na\u00efve Bayes (NB), decision tree (DT), and extreme gradient boosting (XGBoost) with deep learning models such as convolutional neural networks (CNN) and bidirectional long short-term memory (BLSTM) networks. Additionally, it explores transformer-based models such as CAMeLBERT, XLM-RoBERTa, and MARBERT, along with their ensemble configurations. The findings demonstrate that the proposed transformer-based ensemble model achieved superior performance, with average accuracy, recall, precision, and F1-score of 90.4%, 88%, 87.3%, and 87.7%, respectively.",
            "corpus_id": 277308146,
            "sentences": [
                {
                    "corpus_id": "277308146",
                    "title": "Transformer-based ensemble model for dialectal Arabic sentiment classification",
                    "text": "Key features of FastText include its ability to support both supervised and unsupervised learning, which aids in tasks such as text classification and assessing word similarity. The model utilizes hierarchical softmax to improve training efficiency, and its distinct method of representing words as collections of character n-grams helps capture semantic relationships and contextual variances effectively. In our study, we focused on the CBOW variant of FastText, applying position weights, set to a dimension of 300, using character n-grams of length 5, a context window of 5, and including 10 negative samples during the training process. \n\n. AraBERT: is a transformer-based model specifically created for understanding the Arabic language, built on the architecture of Bidirectional Encoder Representations from Transformers (BERT). It has undergone pre-training on a large dataset containing 70 million sentences collected from diverse sources, including Wikipedia, Arabic news sites, and extensive collections such as the Abulkhair Arabic Corpus and OSIAN (Antoun, Baly & Hajj, 2020). Several versions of AraBERT exist, and in our study, we focused on BERT-Large-Arabertv2, with a word embedding vector size of 300. This version is notable for its extensive structure, featuring 24 transformer layers, 1,024 hidden units, and 16 attention heads, which allow it to grasp intricate linguistic patterns in Arabic. \n\nAraBERT effectively handles both Modern Standard Arabic and various regional dialects. The model utilizes a WordPiece tokenizer tailored to the complex morphological characteristics of the Arabic language, ensuring efficient tokenization.",
                    "score": 0.4309245597818362,
                    "section_title": "Feature engineering",
                    "char_start_offset": 40459,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 177
                        },
                        {
                            "start": 178,
                            "end": 406
                        },
                        {
                            "start": 407,
                            "end": 641
                        },
                        {
                            "start": 644,
                            "end": 836
                        },
                        {
                            "start": 837,
                            "end": 1090
                        },
                        {
                            "start": 1091,
                            "end": 1221
                        },
                        {
                            "start": 1222,
                            "end": 1416
                        },
                        {
                            "start": 1419,
                            "end": 1505
                        },
                        {
                            "start": 1506,
                            "end": 1657
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 1062,
                            "end": 1089,
                            "matchedPaperCorpusId": "211678011"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.7119140625
                }
            ],
            "relevance_judgement": 0.7119140625,
            "relevance_judgment_input_expanded": "# Title: Transformer-based ensemble model for dialectal Arabic sentiment classification\n# Venue: PeerJ Computer Science\n# Authors: Omar Mansour, Eman Aboelela, Remon Talaat, Mahmoud Bustami\n## Abstract\nSocial media platforms such as X, Facebook, and Instagram have become essential avenues for individuals to articulate their opinions, especially during global emergencies. These platforms offer valuable insights that necessitate analysis for informed decision-making and a deeper understanding of societal trends. Sentiment analysis is crucial for assessing public sentiment toward specific issues; however, applying it to dialectal Arabic presents considerable challenges in natural language processing. The complexity arises from the language\u2019s intricate semantic and morphological structures, along with the existence of multiple dialects. This form of analysis, also referred to as sentiment classification, opinion mining, emotion mining, and review mining, is the focus of this study, which analyzes tweets from three benchmark datasets: the Arabic Sentiment Tweets Dataset (ASTD), the A Twitter-based Benchmark Arabic Sentiment Analysis Dataset (ASAD), and the Tweets Emoji Arabic Dataset (TEAD). The research involves experimentation with a variety of comparative models, including machine learning, deep learning, transformer-based models, and a transformer-based ensemble model. Feature extraction for both machine learning and deep learning approaches is performed using techniques such as AraVec, FastText, AraBERT, and Term Frequency-Inverse Document Frequency (TF-IDF). The study compares machine learning models such as support vector machine (SVM), na\u00efve Bayes (NB), decision tree (DT), and extreme gradient boosting (XGBoost) with deep learning models such as convolutional neural networks (CNN) and bidirectional long short-term memory (BLSTM) networks. Additionally, it explores transformer-based models such as CAMeLBERT, XLM-RoBERTa, and MARBERT, along with their ensemble configurations. The findings demonstrate that the proposed transformer-based ensemble model achieved superior performance, with average accuracy, recall, precision, and F1-score of 90.4%, 88%, 87.3%, and 87.7%, respectively.\n## Feature engineering\nKey features of FastText include its ability to support both supervised and unsupervised learning, which aids in tasks such as text classification and assessing word similarity. The model utilizes hierarchical softmax to improve training efficiency, and its distinct method of representing words as collections of character n-grams helps capture semantic relationships and contextual variances effectively. In our study, we focused on the CBOW variant of FastText, applying position weights, set to a dimension of 300, using character n-grams of length 5, a context window of 5, and including 10 negative samples during the training process. \n\n. AraBERT: is a transformer-based model specifically created for understanding the Arabic language, built on the architecture of Bidirectional Encoder Representations from Transformers (BERT). It has undergone pre-training on a large dataset containing 70 million sentences collected from diverse sources, including Wikipedia, Arabic news sites, and extensive collections such as the Abulkhair Arabic Corpus and OSIAN (Antoun, Baly & Hajj, 2020). Several versions of AraBERT exist, and in our study, we focused on BERT-Large-Arabertv2, with a word embedding vector size of 300. This version is notable for its extensive structure, featuring 24 transformer layers, 1,024 hidden units, and 16 attention heads, which allow it to grasp intricate linguistic patterns in Arabic. \n\nAraBERT effectively handles both Modern Standard Arabic and various regional dialects. The model utilizes a WordPiece tokenizer tailored to the complex morphological characteristics of the Arabic language, ensuring efficient tokenization.",
            "reference_string": "[277308146 | Mansour et al. | 2025 | Citations: 1]"
        },
        {
            "title": "Pharmacovigilance with Transformers: A Framework to Detect Adverse Drug Reactions Using BERT Fine-Tuned with FARM",
            "venue": "Computational and Mathematical Methods in Medicine",
            "year": 2021,
            "reference_count": 52,
            "citation_count": 24,
            "influential_citation_count": 1,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://downloads.hindawi.com/journals/cmmm/2021/5589829.pdf",
                "status": "GOLD",
                "license": "CCBY",
                "disclaimer": "Notice: Paper or abstract available at https://pmc.ncbi.nlm.nih.gov/articles/PMC8378963, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2113619919",
                    "name": "Sajid Hussain"
                },
                {
                    "authorId": "1777100",
                    "name": "H. Afzal"
                },
                {
                    "authorId": "5644298",
                    "name": "Ramsha Saeed"
                },
                {
                    "authorId": "2740595",
                    "name": "N. Iltaf"
                },
                {
                    "authorId": "33827057",
                    "name": "M. Umair"
                }
            ],
            "abstract": "Adverse drug reactions (ADRs) are the undesirable effects associated with the use of a drug due to some pharmacological action of the drug. During the last few years, social media has become a popular platform where people discuss their health problems and, therefore, has become a popular source to share information related to ADR in the natural language. This paper presents an end-to-end system for modelling ADR detection from the given text by fine-tuning BERT with a highly modular Framework for Adapting Representation Models (FARM). BERT overcame the predominant neural networks bringing remarkable performance gains. However, training BERT is a computationally expensive task which limits its usage for production environments and makes it difficult to determine the most important hyperparameters for the downstream task. Furthermore, developing an end-to-end ADR extraction system comprising two downstream tasks, i.e., text classification for filtering text containing ADRs and extracting ADR mentions from the classified text, is also challenging. The framework used in this work, FARM-BERT, provides support for multitask learning by combining multiple prediction heads which makes training of the end-to-end systems easier and computationally faster. In the proposed model, one prediction head is used for text classification and the other is used for ADR sequence labeling. Experiments are performed on Twitter, PubMed, TwiMed-Twitter, and TwiMed-PubMed datasets. The proposed model is compared with the baseline models and state-of-the-art techniques, and it is shown that it yields better results for the given task with the F-scores of 89.6%, 97.6%, 84.9%, and 95.9% on Twitter, PubMed, TwiMed-Twitter, and TwiMed-PubMed datasets, respectively. Moreover, training time and testing time of the proposed model are compared with BERT's, and it is shown that the proposed model is computationally faster than BERT.",
            "corpus_id": 237253665,
            "sentences": [
                {
                    "corpus_id": "237253665",
                    "title": "Pharmacovigilance with Transformers: A Framework to Detect Adverse Drug Reactions Using BERT Fine-Tuned with FARM",
                    "text": "By considering the subword information, fasttext unlike word2vec and glove generates the embeddings for out of vocabulary words as well. The training data used for any machine learning model, no matter how big it may be, can still not include all the words in a language's vocabulary. If such unseen words are found in the test data, their representations are not generated by word2vec and glove embedding models. However, fasttext overcomes this limitation and represents the out of vocabu-lary words by adding the embeddings for the constituent char n-grams found in the vocabulary. \n\nBERT outperforms both CNN and LSTM. The reason for the better performance of BERT is that it learns contextualized embeddings in a bidirectional way. In natural language, a word is likely to convey multiple meanings based on the context in which it is used. Word2vec, fasttext, and glove produce the same representations of a word even if it has different meanings in different contexts. BERT, on the other hand, produces context-dependent embeddings of a word. In BERT, an input word is represented by the sum of its token embeddings, sentence embeddings, and position embeddings. The proposed model FARM-BERT outperforms all the models by yielding the F-scores of 89.6% and 97.6% on Twitter and PubMed datasets, respectively. FARM-BERT performs better than BERT by 2% on Twitter and by 6% on PubMed datasets. Better performance of FARM-BERT than the standard BERT indicates the effectiveness of fine-tuning BERT with FARM with the modified values of hyperparameters.",
                    "score": 0.5064231735434807,
                    "section_title": "Experiments and Results",
                    "char_start_offset": 24459,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 136
                        },
                        {
                            "start": 137,
                            "end": 284
                        },
                        {
                            "start": 285,
                            "end": 413
                        },
                        {
                            "start": 414,
                            "end": 584
                        },
                        {
                            "start": 587,
                            "end": 622
                        },
                        {
                            "start": 623,
                            "end": 736
                        },
                        {
                            "start": 737,
                            "end": 844
                        },
                        {
                            "start": 845,
                            "end": 974
                        },
                        {
                            "start": 975,
                            "end": 1048
                        },
                        {
                            "start": 1049,
                            "end": 1168
                        },
                        {
                            "start": 1169,
                            "end": 1314
                        },
                        {
                            "start": 1315,
                            "end": 1397
                        },
                        {
                            "start": 1398,
                            "end": 1555
                        }
                    ],
                    "ref_mentions": [],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.71142578125
                }
            ],
            "relevance_judgement": 0.71142578125,
            "relevance_judgment_input_expanded": "# Title: Pharmacovigilance with Transformers: A Framework to Detect Adverse Drug Reactions Using BERT Fine-Tuned with FARM\n# Venue: Computational and Mathematical Methods in Medicine\n# Authors: Sajid Hussain, H. Afzal, Ramsha Saeed, N. Iltaf, M. Umair\n## Abstract\nAdverse drug reactions (ADRs) are the undesirable effects associated with the use of a drug due to some pharmacological action of the drug. During the last few years, social media has become a popular platform where people discuss their health problems and, therefore, has become a popular source to share information related to ADR in the natural language. This paper presents an end-to-end system for modelling ADR detection from the given text by fine-tuning BERT with a highly modular Framework for Adapting Representation Models (FARM). BERT overcame the predominant neural networks bringing remarkable performance gains. However, training BERT is a computationally expensive task which limits its usage for production environments and makes it difficult to determine the most important hyperparameters for the downstream task. Furthermore, developing an end-to-end ADR extraction system comprising two downstream tasks, i.e., text classification for filtering text containing ADRs and extracting ADR mentions from the classified text, is also challenging. The framework used in this work, FARM-BERT, provides support for multitask learning by combining multiple prediction heads which makes training of the end-to-end systems easier and computationally faster. In the proposed model, one prediction head is used for text classification and the other is used for ADR sequence labeling. Experiments are performed on Twitter, PubMed, TwiMed-Twitter, and TwiMed-PubMed datasets. The proposed model is compared with the baseline models and state-of-the-art techniques, and it is shown that it yields better results for the given task with the F-scores of 89.6%, 97.6%, 84.9%, and 95.9% on Twitter, PubMed, TwiMed-Twitter, and TwiMed-PubMed datasets, respectively. Moreover, training time and testing time of the proposed model are compared with BERT's, and it is shown that the proposed model is computationally faster than BERT.\n## Experiments and Results\nBy considering the subword information, fasttext unlike word2vec and glove generates the embeddings for out of vocabulary words as well. The training data used for any machine learning model, no matter how big it may be, can still not include all the words in a language's vocabulary. If such unseen words are found in the test data, their representations are not generated by word2vec and glove embedding models. However, fasttext overcomes this limitation and represents the out of vocabu-lary words by adding the embeddings for the constituent char n-grams found in the vocabulary. \n\nBERT outperforms both CNN and LSTM. The reason for the better performance of BERT is that it learns contextualized embeddings in a bidirectional way. In natural language, a word is likely to convey multiple meanings based on the context in which it is used. Word2vec, fasttext, and glove produce the same representations of a word even if it has different meanings in different contexts. BERT, on the other hand, produces context-dependent embeddings of a word. In BERT, an input word is represented by the sum of its token embeddings, sentence embeddings, and position embeddings. The proposed model FARM-BERT outperforms all the models by yielding the F-scores of 89.6% and 97.6% on Twitter and PubMed datasets, respectively. FARM-BERT performs better than BERT by 2% on Twitter and by 6% on PubMed datasets. Better performance of FARM-BERT than the standard BERT indicates the effectiveness of fine-tuning BERT with FARM with the modified values of hyperparameters.",
            "reference_string": "[237253665 | Hussain et al. | 2021 | Citations: 24]"
        },
        {
            "title": "WiC-TSV: An Evaluation Benchmark for Target Sense Verification of Words in Context",
            "venue": "Conference of the European Chapter of the Association for Computational Linguistics",
            "year": 2020,
            "reference_count": 26,
            "citation_count": 26,
            "influential_citation_count": 8,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://aclanthology.org/2021.eacl-main.140.pdf",
                "status": "HYBRID",
                "license": "CCBY",
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2004.15016, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "1453876854",
                    "name": "Anna Breit"
                },
                {
                    "authorId": "144688662",
                    "name": "Artem Revenko"
                },
                {
                    "authorId": "1667035673",
                    "name": "Kiamehr Rezaee"
                },
                {
                    "authorId": "1717641",
                    "name": "Mohammad Taher Pilehvar"
                },
                {
                    "authorId": "1387447871",
                    "name": "Jos\u00e9 Camacho-Collados"
                }
            ],
            "abstract": "We present WiC-TSV, a new multi-domain evaluation benchmark for Word Sense Disambiguation. More specifically, we introduce a framework for Target Sense Verification of Words in Context which grounds its uniqueness in the formulation as binary classification task thus being independent of external sense inventories, and the coverage of various domains. This makes the dataset highly flexible for the evaluation of a diverse set of models and systems in and across domains. WiC-TSV provides three different evaluation settings, depending on the input signals provided to the model. We set baseline performance on the dataset using state-of-the-art language models. Experimental results show that even though these models can perform decently on the task, there remains a gap between machine and human performance, especially in out-of-domain settings. WiC-TSV data is available at https://competitions.codalab.org/competitions/23683.",
            "corpus_id": 216914042,
            "sentences": [
                {
                    "corpus_id": "216914042",
                    "title": "WiC-TSV: An Evaluation Benchmark for Target Sense Verification of Words in Context",
                    "text": "Table 4 shows the overall results for the three tasks. As can be observed, BERT is clearly better than FastText in all measures. In fact, perhaps surprisingly, FastText does not perform better than a naive baseline that retrieves all instances as true. This also reinforces the challenging nature of the benchmark, as even BERT is far from the human annotator performance (estimated on 85.3% for accuracy). Clearly, the definition information is more helpful than the hypernyms for BERT, while the combination of both attains the best overall results. \n\nIt is also remarkable the high recall of BERT, in contrast to the precision. This is mainly attributed to the domain-specific datasets as we are going to analyze below. Domain-Specific Results. Table 5 presents the results split by domain. Interestingly, FastText faces a massive challenge in adapting domains and generalising from the general to the specific domains. However, BERT shows to be much more robust to domain changes. In fact, perhaps surprisingly, the results on the domain-specific domains do not drop substantially with respect to the WNT/WKT test set, even though the training and development instances came from the same source (i.e. WordNet and Wiktionary). This can be attributed to the fact that specific domains highly constrain the set of possible senses for a word, resulting in an easier WSD classification task (Magnini et al., 2002). On the other hand, WordNet is known to be quite fine-grained (e.g., the noun run has 16 different senses in WordNet, plus many other senses including run as a verb). \n\nIn general, the BERT model can attain a very high recall in the domain-specific datasets, while the precision is still not too low. This model can be helpful in a retrieval setting where the recall may be relevant -for example, when the data is going to pass to a human than can filter.",
                    "score": 0.4439982908604859,
                    "section_title": "Results",
                    "char_start_offset": 24967,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 54
                        },
                        {
                            "start": 55,
                            "end": 128
                        },
                        {
                            "start": 129,
                            "end": 252
                        },
                        {
                            "start": 253,
                            "end": 406
                        },
                        {
                            "start": 407,
                            "end": 551
                        },
                        {
                            "start": 554,
                            "end": 630
                        },
                        {
                            "start": 631,
                            "end": 722
                        },
                        {
                            "start": 723,
                            "end": 747
                        },
                        {
                            "start": 748,
                            "end": 793
                        },
                        {
                            "start": 794,
                            "end": 922
                        },
                        {
                            "start": 923,
                            "end": 984
                        },
                        {
                            "start": 985,
                            "end": 1205
                        },
                        {
                            "start": 1206,
                            "end": 1230
                        },
                        {
                            "start": 1231,
                            "end": 1414
                        },
                        {
                            "start": 1415,
                            "end": 1580
                        },
                        {
                            "start": 1583,
                            "end": 1714
                        },
                        {
                            "start": 1715,
                            "end": 1869
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 1391,
                            "end": 1413,
                            "matchedPaperCorpusId": "7857920"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.7080078125
                }
            ],
            "relevance_judgement": 0.7080078125,
            "relevance_judgment_input_expanded": "# Title: WiC-TSV: An Evaluation Benchmark for Target Sense Verification of Words in Context\n# Venue: Conference of the European Chapter of the Association for Computational Linguistics\n# Authors: Anna Breit, Artem Revenko, Kiamehr Rezaee, Mohammad Taher Pilehvar, Jos\u00e9 Camacho-Collados\n## Abstract\nWe present WiC-TSV, a new multi-domain evaluation benchmark for Word Sense Disambiguation. More specifically, we introduce a framework for Target Sense Verification of Words in Context which grounds its uniqueness in the formulation as binary classification task thus being independent of external sense inventories, and the coverage of various domains. This makes the dataset highly flexible for the evaluation of a diverse set of models and systems in and across domains. WiC-TSV provides three different evaluation settings, depending on the input signals provided to the model. We set baseline performance on the dataset using state-of-the-art language models. Experimental results show that even though these models can perform decently on the task, there remains a gap between machine and human performance, especially in out-of-domain settings. WiC-TSV data is available at https://competitions.codalab.org/competitions/23683.\n## Results\nTable 4 shows the overall results for the three tasks. As can be observed, BERT is clearly better than FastText in all measures. In fact, perhaps surprisingly, FastText does not perform better than a naive baseline that retrieves all instances as true. This also reinforces the challenging nature of the benchmark, as even BERT is far from the human annotator performance (estimated on 85.3% for accuracy). Clearly, the definition information is more helpful than the hypernyms for BERT, while the combination of both attains the best overall results. \n\nIt is also remarkable the high recall of BERT, in contrast to the precision. This is mainly attributed to the domain-specific datasets as we are going to analyze below. Domain-Specific Results. Table 5 presents the results split by domain. Interestingly, FastText faces a massive challenge in adapting domains and generalising from the general to the specific domains. However, BERT shows to be much more robust to domain changes. In fact, perhaps surprisingly, the results on the domain-specific domains do not drop substantially with respect to the WNT/WKT test set, even though the training and development instances came from the same source (i.e. WordNet and Wiktionary). This can be attributed to the fact that specific domains highly constrain the set of possible senses for a word, resulting in an easier WSD classification task (Magnini et al., 2002). On the other hand, WordNet is known to be quite fine-grained (e.g., the noun run has 16 different senses in WordNet, plus many other senses including run as a verb). \n\nIn general, the BERT model can attain a very high recall in the domain-specific datasets, while the precision is still not too low. This model can be helpful in a retrieval setting where the recall may be relevant -for example, when the data is going to pass to a human than can filter.",
            "reference_string": "[216914042 | Breit et al. | 2020 | Citations: 26]"
        },
        {
            "title": "Geometry matters: Exploring language examples at the decision boundary",
            "venue": "arXiv.org",
            "year": 2020,
            "reference_count": 43,
            "citation_count": 3,
            "influential_citation_count": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2010.07212, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2852125",
                    "name": "Debajyoti Datta"
                },
                {
                    "authorId": "2049004359",
                    "name": "Shashwat Kumar"
                },
                {
                    "authorId": "2093120019",
                    "name": "Laura E. Barnes"
                },
                {
                    "authorId": "48763056",
                    "name": "Tom Fletcher"
                }
            ],
            "abstract": "A growing body of recent evidence has highlighted the limitations of natural language processing (NLP) datasets and classifiers. These include the presence of annotation artifacts in datasets, classifiers relying on shallow features like a single word (e.g., if a movie review has the word \"romantic\", the review tends to be positive), or unnecessary words (e.g., learning a proper noun to classify a movie as positive or negative). The presence of such artifacts has subsequently led to the development of challenging datasets to force the model to generalize better. While a variety of heuristic strategies, such as counterfactual examples and contrast sets, have been proposed, the theoretical justification about what makes these examples difficult for the classifier is often lacking or unclear. In this paper, using tools from information geometry, we propose a theoretical way to quantify the difficulty of an example in NLP. Using our approach, we explore difficult examples for several deep learning architectures. We discover that both BERT, CNN and fasttext are susceptible to word substitutions in high difficulty examples. These classifiers tend to perform poorly on the FIM test set. (generated by sampling and perturbing difficult examples, with accuracy dropping below 50%). We replicate our experiments on 5 NLP datasets (YelpReviewPolarity, AGNEWS, SogouNews, YelpReviewFull and Yahoo Answers). On YelpReviewPolarity we observe a correlation coefficient of -0.4 between resilience to perturbations and the difficulty score. Similarly we observe a correlation of 0.35 between the difficulty score and the empirical success probability of random substitutions. Our approach is simple, architecture agnostic and can be used to study the fragilities of text classification models. All the code used will be made publicly available, including a tool to explore the difficult examples for other datasets.",
            "corpus_id": 222341529,
            "sentences": [
                {
                    "corpus_id": "222341529",
                    "title": "Geometry matters: Exploring language examples at the decision boundary",
                    "text": "We use multiple model architectures to understand the implications of the FIM. \n\n\u2022 Convolutional Neural Network (CNN): We use the same paradigm Kim [2014] for text classification using GloVe Pennington et al. [2014] embeddings. \n\n\u2022 Long Short Term Memory Network (LSTM): LSTM's have been used extensively in Natural Language Processing for variety of tasks like text classification and language modeling Schmidhuber [2015] \u2022 Bag of Tricks for Efficient Text Classification(Fasttext) We used the Fasttext based model Joulin et al. [2017] for classifying text and examining the quantitative aspects of Fisher Information Metric since the model is incredibly computational efficiency and strong performance across multiple datasets. \n\n\u2022 BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding Devlin et al. [2018] Large scale pretrained language models have become extremely popular for data efficiency when trained on downstream tasks. However it has been shown recently, that on large datasets ? performance increase from finetuning is often within 1% of BERT. Thus we limited our analysis of using BERT for qualitative analysis for verification of it's relationship with FIM since accuracy difference is minimal and it is computation intensive to calculate the eigenvalue of the FIM of BERT based fine tuned models. We instead used BERT embeddings in downstream tasks for qualitative evaluation of FIM. \n\nFor the Fasttext, CNN and LSTM models we used early stopping with a patience of 5.",
                    "score": 0.5771961249707197,
                    "section_title": "MODEL DESCRIPTIONS",
                    "char_start_offset": 16064,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 78
                        },
                        {
                            "start": 81,
                            "end": 227
                        },
                        {
                            "start": 230,
                            "end": 729
                        },
                        {
                            "start": 732,
                            "end": 958
                        },
                        {
                            "start": 959,
                            "end": 1019
                        },
                        {
                            "start": 1020,
                            "end": 1084
                        },
                        {
                            "start": 1085,
                            "end": 1340
                        },
                        {
                            "start": 1341,
                            "end": 1427
                        },
                        {
                            "start": 1430,
                            "end": 1512
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 191,
                            "end": 215,
                            "matchedPaperCorpusId": "1957433"
                        },
                        {
                            "start": 404,
                            "end": 422,
                            "matchedPaperCorpusId": "11715509"
                        },
                        {
                            "start": 516,
                            "end": 536,
                            "matchedPaperCorpusId": "1210515"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.70556640625
                }
            ],
            "relevance_judgement": 0.70556640625,
            "relevance_judgment_input_expanded": "# Title: Geometry matters: Exploring language examples at the decision boundary\n# Venue: arXiv.org\n# Authors: Debajyoti Datta, Shashwat Kumar, Laura E. Barnes, Tom Fletcher\n## Abstract\nA growing body of recent evidence has highlighted the limitations of natural language processing (NLP) datasets and classifiers. These include the presence of annotation artifacts in datasets, classifiers relying on shallow features like a single word (e.g., if a movie review has the word \"romantic\", the review tends to be positive), or unnecessary words (e.g., learning a proper noun to classify a movie as positive or negative). The presence of such artifacts has subsequently led to the development of challenging datasets to force the model to generalize better. While a variety of heuristic strategies, such as counterfactual examples and contrast sets, have been proposed, the theoretical justification about what makes these examples difficult for the classifier is often lacking or unclear. In this paper, using tools from information geometry, we propose a theoretical way to quantify the difficulty of an example in NLP. Using our approach, we explore difficult examples for several deep learning architectures. We discover that both BERT, CNN and fasttext are susceptible to word substitutions in high difficulty examples. These classifiers tend to perform poorly on the FIM test set. (generated by sampling and perturbing difficult examples, with accuracy dropping below 50%). We replicate our experiments on 5 NLP datasets (YelpReviewPolarity, AGNEWS, SogouNews, YelpReviewFull and Yahoo Answers). On YelpReviewPolarity we observe a correlation coefficient of -0.4 between resilience to perturbations and the difficulty score. Similarly we observe a correlation of 0.35 between the difficulty score and the empirical success probability of random substitutions. Our approach is simple, architecture agnostic and can be used to study the fragilities of text classification models. All the code used will be made publicly available, including a tool to explore the difficult examples for other datasets.\n## MODEL DESCRIPTIONS\nWe use multiple model architectures to understand the implications of the FIM. \n\n\u2022 Convolutional Neural Network (CNN): We use the same paradigm Kim [2014] for text classification using GloVe Pennington et al. [2014] embeddings. \n\n\u2022 Long Short Term Memory Network (LSTM): LSTM's have been used extensively in Natural Language Processing for variety of tasks like text classification and language modeling Schmidhuber [2015] \u2022 Bag of Tricks for Efficient Text Classification(Fasttext) We used the Fasttext based model Joulin et al. [2017] for classifying text and examining the quantitative aspects of Fisher Information Metric since the model is incredibly computational efficiency and strong performance across multiple datasets. \n\n\u2022 BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding Devlin et al. [2018] Large scale pretrained language models have become extremely popular for data efficiency when trained on downstream tasks. However it has been shown recently, that on large datasets ? performance increase from finetuning is often within 1% of BERT. Thus we limited our analysis of using BERT for qualitative analysis for verification of it's relationship with FIM since accuracy difference is minimal and it is computation intensive to calculate the eigenvalue of the FIM of BERT based fine tuned models. We instead used BERT embeddings in downstream tasks for qualitative evaluation of FIM. \n\nFor the Fasttext, CNN and LSTM models we used early stopping with a patience of 5.",
            "reference_string": "[222341529 | Datta et al. | 2020 | Citations: 3]"
        },
        {
            "title": "On the Validity of Pre-Trained Transformers for Natural Language Processing in the Software Engineering Domain",
            "venue": "IEEE Transactions on Software Engineering",
            "year": 2021,
            "reference_count": 78,
            "citation_count": 68,
            "influential_citation_count": 4,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://arxiv.org/pdf/2109.04738",
                "status": "GREEN",
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2109.04738, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2214761830",
                    "name": "Julian von der Mosel"
                },
                {
                    "authorId": "8058979",
                    "name": "Alexander Trautsch"
                },
                {
                    "authorId": "3063461",
                    "name": "Steffen Herbold"
                }
            ],
            "abstract": "Transformers are the current state-of-the-art of natural language processing in many domains and are using traction within software engineering research as well. Such models are pre-trained on large amounts of data, usually from the general domain. However, we only have a limited understanding regarding the validity of transformers within the software engineering domain, i.e., how good such models are at understanding words and sentences within a software engineering context and how this improves the state-of-the-art. Within this article, we shed light on this complex, but crucial issue. We compare BERT transformer models trained with software engineering data with transformers based on general domain data in multiple dimensions: their vocabulary, their ability to understand which words are missing, and their performance in classification tasks. Our results show that for tasks that require understanding of the software engineering context, pre-training with software engineering data is valuable, while general domain models are sufficient for general language understanding, also within the software engineering domain.",
            "corpus_id": 237485425,
            "sentences": [
                {
                    "corpus_id": "237485425",
                    "title": "On the Validity of Pre-Trained Transformers for Natural Language Processing in the Software Engineering Domain",
                    "text": "The results show that seBERT and BERToverflow achieve the best performance for the issue type prediction tasks, outperforming both fastText and the general-domain BERT models. The improvement over fastText is very large with an about 11% higher F1 score for the issue type prediction. The violin plot in Figure 5 indicates that the performance improvement in F1 score is due to an improvement of both recall and precision, which means the models reduced both false positives and false negatives in comparison to fastText. The Bayesian signed rank test determined that this improvement of the SE-specific models over the other models is significant. 15 The difference between seBERT and BERToverflow is not significant The comparison between fastText and BERT BASE shows that the general-domain models may be better than smaller text processing models without pretraining on the issue type prediction. BERT BASE significantly outperforms fastText on the issue type prediction task. The BERT LARGE model from the general domain has severe problems with this use case, i.e., there are several cases where the 14. The full list of polysemes can be found in the supplemental material. \n\n15. Posterior probabilities determined by the Bayesian signed rank test can be found in the supplemental material. models completely failed. This happened with none of the other models and may be an indication that the amount of data is too small to fine-tune such a large model from the general domain on a domain-specific corpus.",
                    "score": 0.49958786992155807,
                    "section_title": "Issue Type Prediction",
                    "char_start_offset": 48155,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 175
                        },
                        {
                            "start": 176,
                            "end": 284
                        },
                        {
                            "start": 285,
                            "end": 521
                        },
                        {
                            "start": 522,
                            "end": 651
                        },
                        {
                            "start": 652,
                            "end": 900
                        },
                        {
                            "start": 901,
                            "end": 980
                        },
                        {
                            "start": 981,
                            "end": 1109
                        },
                        {
                            "start": 1110,
                            "end": 1179
                        },
                        {
                            "start": 1182,
                            "end": 1296
                        },
                        {
                            "start": 1297,
                            "end": 1322
                        },
                        {
                            "start": 1323,
                            "end": 1513
                        }
                    ],
                    "ref_mentions": [],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.703125
                }
            ],
            "relevance_judgement": 0.703125,
            "relevance_judgment_input_expanded": "# Title: On the Validity of Pre-Trained Transformers for Natural Language Processing in the Software Engineering Domain\n# Venue: IEEE Transactions on Software Engineering\n# Authors: Julian von der Mosel, Alexander Trautsch, Steffen Herbold\n## Abstract\nTransformers are the current state-of-the-art of natural language processing in many domains and are using traction within software engineering research as well. Such models are pre-trained on large amounts of data, usually from the general domain. However, we only have a limited understanding regarding the validity of transformers within the software engineering domain, i.e., how good such models are at understanding words and sentences within a software engineering context and how this improves the state-of-the-art. Within this article, we shed light on this complex, but crucial issue. We compare BERT transformer models trained with software engineering data with transformers based on general domain data in multiple dimensions: their vocabulary, their ability to understand which words are missing, and their performance in classification tasks. Our results show that for tasks that require understanding of the software engineering context, pre-training with software engineering data is valuable, while general domain models are sufficient for general language understanding, also within the software engineering domain.\n## Issue Type Prediction\nThe results show that seBERT and BERToverflow achieve the best performance for the issue type prediction tasks, outperforming both fastText and the general-domain BERT models. The improvement over fastText is very large with an about 11% higher F1 score for the issue type prediction. The violin plot in Figure 5 indicates that the performance improvement in F1 score is due to an improvement of both recall and precision, which means the models reduced both false positives and false negatives in comparison to fastText. The Bayesian signed rank test determined that this improvement of the SE-specific models over the other models is significant. 15 The difference between seBERT and BERToverflow is not significant The comparison between fastText and BERT BASE shows that the general-domain models may be better than smaller text processing models without pretraining on the issue type prediction. BERT BASE significantly outperforms fastText on the issue type prediction task. The BERT LARGE model from the general domain has severe problems with this use case, i.e., there are several cases where the 14. The full list of polysemes can be found in the supplemental material. \n\n15. Posterior probabilities determined by the Bayesian signed rank test can be found in the supplemental material. models completely failed. This happened with none of the other models and may be an indication that the amount of data is too small to fine-tune such a large model from the general domain on a domain-specific corpus.",
            "reference_string": "[237485425 | Mosel et al. | 2021 | Citations: 68]"
        },
        {
            "title": "Identification of mycoplasma pneumonia in children based on fusion of multi-modal clinical free-text description and structured test data",
            "venue": "Health Informatics Journal",
            "year": 2024,
            "reference_count": 46,
            "citation_count": 1,
            "influential_citation_count": 0,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://journals.sagepub.com/doi/pdf/10.1177/14604582241255818",
                "status": "GOLD",
                "license": "CCBYNC",
                "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.1177/14604582241255818?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1177/14604582241255818, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2302859526",
                    "name": "Jingna Xie"
                },
                {
                    "authorId": "2364687744",
                    "name": "Yingshuo Wang"
                },
                {
                    "authorId": "1576956902",
                    "name": "Qiuyang Sheng"
                },
                {
                    "authorId": "2283403594",
                    "name": "Xiaoqing Liu"
                },
                {
                    "authorId": "2336703688",
                    "name": "Jing Li"
                },
                {
                    "authorId": "47450065",
                    "name": "Fenglei Sun"
                },
                {
                    "authorId": "2155376102",
                    "name": "Yuqi Wang"
                },
                {
                    "authorId": "2302784979",
                    "name": "Shuxian Li"
                },
                {
                    "authorId": "2110531396",
                    "name": "Yiming Li"
                },
                {
                    "authorId": "2302875109",
                    "name": "Yizhou Yu"
                },
                {
                    "authorId": "2303601854",
                    "name": "Gang Yu"
                }
            ],
            "abstract": "Mycoplasma pneumonia may lead to hospitalizations and pose life-threatening risks in children. The automated identification of mycoplasma pneumonia from electronic medical records holds significant potential for improving the efficiency of hospital resource allocation. In this study, we proposed a novel method for identifying mycoplasma pneumonia by integrating multi-modal features derived from both free-text descriptions and structured test data in electronic medical records. Our approach begins with the extraction of free-text and structured data from clinical records through a systematic preprocessing pipeline. Subsequently, we employ a pre-trained transformer language model to extract features from the free-text, while multiple additive regression trees are used to transform features from the structured data. An attention-based fusion mechanism is then applied to integrate these multi-modal features for effective classification. We validated our method using clinic records of 7157 patients, retrospectively collected for training and testing purposes. The experimental results demonstrate that our proposed multi-modal fusion approach achieves significant improvements over other methods across four key performance metrics.",
            "corpus_id": 269973968,
            "sentences": [
                {
                    "corpus_id": "269973968",
                    "title": "Identification of mycoplasma pneumonia in children based on fusion of multi-modal clinical free-text description and structured test data",
                    "text": "Influence of word representation context on free-text stream analysis \n\nIn our study, we analyzed various word embedding representations and their corresponding classification methods, as shown in Table 3. When comparing VSM-based methods with Word2Vec-based methods, we equate MLP and FastText due to their similar complexities as shallow neural networks. FastText, however, demonstrates better performance than MLP (0.696 vs 0.670 in Acc and 0.767 vs 0.727 in F 1 ), indicating Word2Vec's superior word representation capabilities. VSM's corpus is restricted to the training set, while Word2Vec benefits from a much larger corpus. Despite VSM's corpus being more relevant to specific tasks, word frequency alone proves insufficient for capturing the full importance of a word. This limitation includes the inability to reflect the positional information of words and their interrelationships. In contrast, Word2Vec, through unsupervised training on extensive corpora, learns more nuanced word representations, capturing implicit word relationships. Interestingly, original BERT, despite its wide-ranging successes in natural language processing, does not show a marked advantage over Word2Vec in our analysis and is slightly outperformed by TextCNN. This observation is further evidenced by the improved performance of MLM BERT over the original BERT, suggesting that contextual word embeddings benefit from additional data and training. This leads us to conclude that for specialized domain language tasks like clinical text analysis, pre-training is essential to fully leverage BERT's capabilities.",
                    "score": 0.42144553682688757,
                    "section_title": "Discussions",
                    "char_start_offset": 18508,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 69
                        },
                        {
                            "start": 72,
                            "end": 205
                        },
                        {
                            "start": 206,
                            "end": 356
                        },
                        {
                            "start": 357,
                            "end": 533
                        },
                        {
                            "start": 534,
                            "end": 632
                        },
                        {
                            "start": 633,
                            "end": 778
                        },
                        {
                            "start": 779,
                            "end": 894
                        },
                        {
                            "start": 895,
                            "end": 1050
                        },
                        {
                            "start": 1051,
                            "end": 1251
                        },
                        {
                            "start": 1252,
                            "end": 1439
                        },
                        {
                            "start": 1440,
                            "end": 1602
                        }
                    ],
                    "ref_mentions": [],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.69921875
                }
            ],
            "relevance_judgement": 0.69921875,
            "relevance_judgment_input_expanded": "# Title: Identification of mycoplasma pneumonia in children based on fusion of multi-modal clinical free-text description and structured test data\n# Venue: Health Informatics Journal\n# Authors: Jingna Xie, Yingshuo Wang, Qiuyang Sheng, Xiaoqing Liu, Jing Li, Fenglei Sun, Yuqi Wang, Shuxian Li, Yiming Li, Yizhou Yu, Gang Yu\n## Abstract\nMycoplasma pneumonia may lead to hospitalizations and pose life-threatening risks in children. The automated identification of mycoplasma pneumonia from electronic medical records holds significant potential for improving the efficiency of hospital resource allocation. In this study, we proposed a novel method for identifying mycoplasma pneumonia by integrating multi-modal features derived from both free-text descriptions and structured test data in electronic medical records. Our approach begins with the extraction of free-text and structured data from clinical records through a systematic preprocessing pipeline. Subsequently, we employ a pre-trained transformer language model to extract features from the free-text, while multiple additive regression trees are used to transform features from the structured data. An attention-based fusion mechanism is then applied to integrate these multi-modal features for effective classification. We validated our method using clinic records of 7157 patients, retrospectively collected for training and testing purposes. The experimental results demonstrate that our proposed multi-modal fusion approach achieves significant improvements over other methods across four key performance metrics.\n## Discussions\nInfluence of word representation context on free-text stream analysis \n\nIn our study, we analyzed various word embedding representations and their corresponding classification methods, as shown in Table 3. When comparing VSM-based methods with Word2Vec-based methods, we equate MLP and FastText due to their similar complexities as shallow neural networks. FastText, however, demonstrates better performance than MLP (0.696 vs 0.670 in Acc and 0.767 vs 0.727 in F 1 ), indicating Word2Vec's superior word representation capabilities. VSM's corpus is restricted to the training set, while Word2Vec benefits from a much larger corpus. Despite VSM's corpus being more relevant to specific tasks, word frequency alone proves insufficient for capturing the full importance of a word. This limitation includes the inability to reflect the positional information of words and their interrelationships. In contrast, Word2Vec, through unsupervised training on extensive corpora, learns more nuanced word representations, capturing implicit word relationships. Interestingly, original BERT, despite its wide-ranging successes in natural language processing, does not show a marked advantage over Word2Vec in our analysis and is slightly outperformed by TextCNN. This observation is further evidenced by the improved performance of MLM BERT over the original BERT, suggesting that contextual word embeddings benefit from additional data and training. This leads us to conclude that for specialized domain language tasks like clinical text analysis, pre-training is essential to fully leverage BERT's capabilities.",
            "reference_string": "[269973968 | Xie et al. | 2024 | Citations: 1]"
        },
        {
            "title": "IIITG-ADBU at SemEval-2020 Task 12: Comparison of BERT and BiLSTM in Detecting Offensive Language",
            "venue": "International Workshop on Semantic Evaluation",
            "year": 2020,
            "reference_count": 14,
            "citation_count": 2,
            "influential_citation_count": 0,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://aclanthology.org/2020.semeval-1.204.pdf",
                "status": "HYBRID",
                "license": "CCBY",
                "disclaimer": "Notice: Paper or abstract available at https://aclanthology.org/2020.semeval-1.204, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "48140293",
                    "name": "Arup Baruah"
                },
                {
                    "authorId": "2006187174",
                    "name": "K. Das"
                },
                {
                    "authorId": "2568004",
                    "name": "F. Barbhuiya"
                },
                {
                    "authorId": "144710196",
                    "name": "K. Dey"
                }
            ],
            "abstract": "Task 12 of SemEval 2020 consisted of 3 subtasks, namely offensive language identification (Subtask A), categorization of offense type (Subtask B), and offense target identification (Subtask C). This paper presents the results our classifiers obtained for the English language in the 3 subtasks. The classifiers used by us were BERT and BiLSTM. On the test set, our BERT classifier obtained macro F1 score of 0.90707 for subtask A, and 0.65279 for subtask B. The BiLSTM classifier obtained macro F1 score of 0.57565 for subtask C. The paper also performs an analysis of the errors made by our classifiers. We conjecture that the presence of few misleading instances in the dataset is affecting the performance of the classifiers. Our analysis also discusses the need of temporal context and world knowledge to determine the offensiveness of few comments.",
            "corpus_id": 227230531,
            "sentences": [
                {
                    "corpus_id": "227230531",
                    "title": "IIITG-ADBU at SemEval-2020 Task 12: Comparison of BERT and BiLSTM in Detecting Offensive Language",
                    "text": "BERT (Devlin et al., 2019) is a bi-directional model based on the transformer architecture. The transformer architecture is an architecture based solely on attention mechanism (Vaswani et al., 2017). The word embeddings produced by fastText is static in nature. Each word has a single embedding irrespective of the context in which the word appears. Static embeddings fail to handle polysemy. The embeddings produced by BERT are contextualized embeddings. The same word may have multiple embeddings depending on the context in which it appears. \n\nIn our study, we used the uncased large version of BERT2 to generate an embedding for each comment. This version has 24 layers and 16 attention heads. It generates a 1024-dimensional vector for each word. We used the 1024-dimensional vector of the Extract layer as the representation of the comment. Our classification layer consisted of a single Dense layer. For tasks A and B, this layer used the sigmoid activation function. For task C, it used the softmax activation function. The classifier was trained using the Adam optimizer with a learning rate of 2e-5. For tasks A and B, the binary crossentropy loss function was used. For task C, the sparse categorical crossentropy loss function was used. Tables 5 to 7 show the confusion matrices obtained by our classifiers for subtask A, B, and C respectively. In subtask A, BiLSTM performed better that BERT in predicting the majority class (NOT). BERT, however, performed better in predicting the minority class (OFF). In subtask B, BERT performed better than BiLSTM in predicting both the classes. In subtask C, BiLSTM performed better than BERT in predicting the minority classes (GRP and OTH). This was the reason for its superior performance in the subtask. BERT, however, performed better in predicting the majority class.",
                    "score": 0.45020185605452373,
                    "section_title": "BERT",
                    "char_start_offset": 6551,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 91
                        },
                        {
                            "start": 92,
                            "end": 199
                        },
                        {
                            "start": 200,
                            "end": 261
                        },
                        {
                            "start": 262,
                            "end": 349
                        },
                        {
                            "start": 350,
                            "end": 392
                        },
                        {
                            "start": 393,
                            "end": 455
                        },
                        {
                            "start": 456,
                            "end": 544
                        },
                        {
                            "start": 547,
                            "end": 646
                        },
                        {
                            "start": 647,
                            "end": 697
                        },
                        {
                            "start": 698,
                            "end": 751
                        },
                        {
                            "start": 752,
                            "end": 846
                        },
                        {
                            "start": 847,
                            "end": 906
                        },
                        {
                            "start": 907,
                            "end": 974
                        },
                        {
                            "start": 975,
                            "end": 1027
                        },
                        {
                            "start": 1028,
                            "end": 1109
                        },
                        {
                            "start": 1110,
                            "end": 1176
                        },
                        {
                            "start": 1177,
                            "end": 1248
                        },
                        {
                            "start": 1249,
                            "end": 1356
                        },
                        {
                            "start": 1357,
                            "end": 1444
                        },
                        {
                            "start": 1445,
                            "end": 1516
                        },
                        {
                            "start": 1517,
                            "end": 1596
                        },
                        {
                            "start": 1597,
                            "end": 1694
                        },
                        {
                            "start": 1695,
                            "end": 1759
                        },
                        {
                            "start": 1760,
                            "end": 1825
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 5,
                            "end": 26,
                            "matchedPaperCorpusId": "52967399"
                        },
                        {
                            "start": 176,
                            "end": 198,
                            "matchedPaperCorpusId": "13756489"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.6982421875
                }
            ],
            "relevance_judgement": 0.6982421875,
            "relevance_judgment_input_expanded": "# Title: IIITG-ADBU at SemEval-2020 Task 12: Comparison of BERT and BiLSTM in Detecting Offensive Language\n# Venue: International Workshop on Semantic Evaluation\n# Authors: Arup Baruah, K. Das, F. Barbhuiya, K. Dey\n## Abstract\nTask 12 of SemEval 2020 consisted of 3 subtasks, namely offensive language identification (Subtask A), categorization of offense type (Subtask B), and offense target identification (Subtask C). This paper presents the results our classifiers obtained for the English language in the 3 subtasks. The classifiers used by us were BERT and BiLSTM. On the test set, our BERT classifier obtained macro F1 score of 0.90707 for subtask A, and 0.65279 for subtask B. The BiLSTM classifier obtained macro F1 score of 0.57565 for subtask C. The paper also performs an analysis of the errors made by our classifiers. We conjecture that the presence of few misleading instances in the dataset is affecting the performance of the classifiers. Our analysis also discusses the need of temporal context and world knowledge to determine the offensiveness of few comments.\n## BERT\nBERT (Devlin et al., 2019) is a bi-directional model based on the transformer architecture. The transformer architecture is an architecture based solely on attention mechanism (Vaswani et al., 2017). The word embeddings produced by fastText is static in nature. Each word has a single embedding irrespective of the context in which the word appears. Static embeddings fail to handle polysemy. The embeddings produced by BERT are contextualized embeddings. The same word may have multiple embeddings depending on the context in which it appears. \n\nIn our study, we used the uncased large version of BERT2 to generate an embedding for each comment. This version has 24 layers and 16 attention heads. It generates a 1024-dimensional vector for each word. We used the 1024-dimensional vector of the Extract layer as the representation of the comment. Our classification layer consisted of a single Dense layer. For tasks A and B, this layer used the sigmoid activation function. For task C, it used the softmax activation function. The classifier was trained using the Adam optimizer with a learning rate of 2e-5. For tasks A and B, the binary crossentropy loss function was used. For task C, the sparse categorical crossentropy loss function was used. Tables 5 to 7 show the confusion matrices obtained by our classifiers for subtask A, B, and C respectively. In subtask A, BiLSTM performed better that BERT in predicting the majority class (NOT). BERT, however, performed better in predicting the minority class (OFF). In subtask B, BERT performed better than BiLSTM in predicting both the classes. In subtask C, BiLSTM performed better than BERT in predicting the minority classes (GRP and OTH). This was the reason for its superior performance in the subtask. BERT, however, performed better in predicting the majority class.",
            "reference_string": "[227230531 | Baruah et al. | 2020 | Citations: 2]"
        },
        {
            "title": "Studying Taxonomy Enrichment on Diachronic WordNet Versions",
            "venue": "International Conference on Computational Linguistics",
            "year": 2020,
            "reference_count": 26,
            "citation_count": 12,
            "influential_citation_count": 0,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://www.aclweb.org/anthology/2020.coling-main.276.pdf",
                "status": "HYBRID",
                "license": "CCBY",
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2011.11536, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "38650969",
                    "name": "Irina Nikishina"
                },
                {
                    "authorId": "144033941",
                    "name": "Alexander Panchenko"
                },
                {
                    "authorId": "145089549",
                    "name": "V. Logacheva"
                },
                {
                    "authorId": "1724001",
                    "name": "Natalia V. Loukachevitch"
                }
            ],
            "abstract": "Ontologies, taxonomies, and thesauri have always been in high demand in a large number of NLP tasks. However, most studies are focused on the creation of lexical resources rather than the maintenance of the existing ones and keeping them up-to-date. In this paper, we address the problem of taxonomy enrichment. Namely, we explore the possibilities of taxonomy extension in a resource-poor setting and present several methods which are applicable to a large number of languages. We also create novel English and Russian datasets for training and evaluating taxonomy enrichment systems and describe a technique of creating such datasets for other languages.",
            "corpus_id": 227126558,
            "sentences": [
                {
                    "corpus_id": "227126558",
                    "title": "Studying Taxonomy Enrichment on Diachronic WordNet Versions",
                    "text": "We test our methods on non-contextualised fastText (Bojanowski et al., 2017) and contextualised BERT (Devlin et al., 2019) embeddings. We choose fastText embeddings because pre-trained fastText models are easy to deploy, and do not require additional data or training for the out-of-vocabulary words. In this paper we use the fastText embeddings from the official website3 for both English and Russian, trained on Common Crawl from 2019 and Wikipedia CC including lexicon from the previous periods as well. \n\nWhile fastText embeddings can be generated for individual words, BERT requires a context for a word (i.e. a sentence containing it) to generate its embedding. For experiments with English datasets, we extract contexts from Wikipedia. For the experiments with Russian, we use a news corpus provided by the organisers of RUSSE'2020,4 which contains at least 50 occurrences for each word in the dataset. \n\nWe use the pre-trained BERT-base model for English from (Devlin et al., 2019). For Russian, we utilize RuBERT model from (Kuratov and Arkhipov, 2019), which proved to outperform the Multilingual BERT from the original paper. To compute BERT embeddings for orphans and synsets, we extract sentences containing them from the corresponding corpora. If the words are absent in the corpora, we computed the average of lemmas without context for synsets and the embedding of the input word without context. We also averaged word-pieces for the OOV words. We lemmatise corpora with UDPipe (Straka and Strakov\u00e1, 2017) to be able to find not only exact word matches but also their grammatical forms. We rely on UDPipe as it supports many languages and shows reasonable performance on our data. In case of multiple occurrences of the same orphan, we average the retrieved contextualised embeddings.",
                    "score": 0.47388706845930895,
                    "section_title": "Pre-trained Embeddings",
                    "char_start_offset": 14357,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 134
                        },
                        {
                            "start": 135,
                            "end": 300
                        },
                        {
                            "start": 301,
                            "end": 506
                        },
                        {
                            "start": 509,
                            "end": 667
                        },
                        {
                            "start": 668,
                            "end": 742
                        },
                        {
                            "start": 743,
                            "end": 909
                        },
                        {
                            "start": 912,
                            "end": 990
                        },
                        {
                            "start": 991,
                            "end": 1136
                        },
                        {
                            "start": 1137,
                            "end": 1257
                        },
                        {
                            "start": 1258,
                            "end": 1412
                        },
                        {
                            "start": 1413,
                            "end": 1460
                        },
                        {
                            "start": 1461,
                            "end": 1602
                        },
                        {
                            "start": 1603,
                            "end": 1696
                        },
                        {
                            "start": 1697,
                            "end": 1800
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 51,
                            "end": 76,
                            "matchedPaperCorpusId": "207556454"
                        },
                        {
                            "start": 101,
                            "end": 122,
                            "matchedPaperCorpusId": "52967399"
                        },
                        {
                            "start": 968,
                            "end": 989,
                            "matchedPaperCorpusId": "52967399"
                        },
                        {
                            "start": 1033,
                            "end": 1061,
                            "matchedPaperCorpusId": "158047013"
                        },
                        {
                            "start": 1494,
                            "end": 1521,
                            "matchedPaperCorpusId": "6205777"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.6943359375
                }
            ],
            "relevance_judgement": 0.6943359375,
            "relevance_judgment_input_expanded": "# Title: Studying Taxonomy Enrichment on Diachronic WordNet Versions\n# Venue: International Conference on Computational Linguistics\n# Authors: Irina Nikishina, Alexander Panchenko, V. Logacheva, Natalia V. Loukachevitch\n## Abstract\nOntologies, taxonomies, and thesauri have always been in high demand in a large number of NLP tasks. However, most studies are focused on the creation of lexical resources rather than the maintenance of the existing ones and keeping them up-to-date. In this paper, we address the problem of taxonomy enrichment. Namely, we explore the possibilities of taxonomy extension in a resource-poor setting and present several methods which are applicable to a large number of languages. We also create novel English and Russian datasets for training and evaluating taxonomy enrichment systems and describe a technique of creating such datasets for other languages.\n## Pre-trained Embeddings\nWe test our methods on non-contextualised fastText (Bojanowski et al., 2017) and contextualised BERT (Devlin et al., 2019) embeddings. We choose fastText embeddings because pre-trained fastText models are easy to deploy, and do not require additional data or training for the out-of-vocabulary words. In this paper we use the fastText embeddings from the official website3 for both English and Russian, trained on Common Crawl from 2019 and Wikipedia CC including lexicon from the previous periods as well. \n\nWhile fastText embeddings can be generated for individual words, BERT requires a context for a word (i.e. a sentence containing it) to generate its embedding. For experiments with English datasets, we extract contexts from Wikipedia. For the experiments with Russian, we use a news corpus provided by the organisers of RUSSE'2020,4 which contains at least 50 occurrences for each word in the dataset. \n\nWe use the pre-trained BERT-base model for English from (Devlin et al., 2019). For Russian, we utilize RuBERT model from (Kuratov and Arkhipov, 2019), which proved to outperform the Multilingual BERT from the original paper. To compute BERT embeddings for orphans and synsets, we extract sentences containing them from the corresponding corpora. If the words are absent in the corpora, we computed the average of lemmas without context for synsets and the embedding of the input word without context. We also averaged word-pieces for the OOV words. We lemmatise corpora with UDPipe (Straka and Strakov\u00e1, 2017) to be able to find not only exact word matches but also their grammatical forms. We rely on UDPipe as it supports many languages and shows reasonable performance on our data. In case of multiple occurrences of the same orphan, we average the retrieved contextualised embeddings.",
            "reference_string": "[227126558 | Nikishina et al. | 2020 | Citations: 12]"
        },
        {
            "title": "Imbalanced Multi-label Classification for Business-related Text with Moderately Large Label Spaces",
            "venue": "arXiv.org",
            "year": 2023,
            "reference_count": 13,
            "citation_count": 3,
            "influential_citation_count": 0,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "http://arxiv.org/pdf/2306.07046",
                "status": "CLOSED",
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2306.07046, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "46478966",
                    "name": "Muhammad Arslan"
                },
                {
                    "authorId": "145378327",
                    "name": "C. Cruz"
                }
            ],
            "abstract": "In this study, we compared the performance of four different methods for multi label text classification using a specific imbalanced business dataset. The four methods we evaluated were fine tuned BERT, Binary Relevance, Classifier Chains, and Label Powerset. The results show that fine tuned BERT outperforms the other three methods by a significant margin, achieving high values of accuracy, F1 Score, Precision, and Recall. Binary Relevance also performs well on this dataset, while Classifier Chains and Label Powerset demonstrate relatively poor performance. These findings highlight the effectiveness of fine tuned BERT for multi label text classification tasks, and suggest that it may be a useful tool for businesses seeking to analyze complex and multifaceted texts.",
            "corpus_id": 259138635,
            "sentences": [
                {
                    "corpus_id": "259138635",
                    "title": "Imbalanced Multi-label Classification for Business-related Text with Moderately Large Label Spaces",
                    "text": "The study focused on the problem of imbalanced multi-label classification for business-related text with moderately large label spaces. The experiment compared the performance of four methods, namely Binary Relevance, Classifier Chains, Label Power-set, and Fine-tuned BERT, and evaluated their effectiveness based on accuracy, F1-Score, Precision, and Recall values. The results revealed that the fine-tuned BERT method significantly outperformed the other three methods, achieving high accuracy, F1-Score, Precision, and Recall values. Binary Relevance also performed well, but Classifier Chains and Label Power-set exhibited relatively poor performance on this imbalanced dataset. \n\nOverall, the findings suggest that finetuning the pre-trained BERT model is a good idea because it enables the model to adapt to a specific application. BERT is a powerful language model that has been pre-trained on a large corpus of text, allowing it to learn the intricacies of language and syntax. However, while pre-training provides a strong foundation, it does not necessarily optimize the model for specific tasks such as sentiment analysis, question answering, or text classification. Finetuning allows us to take the pre-trained BERT model and tailor it to our specific needs by training it on a smaller dataset that is specific to the task at hand. \n\nReproducing the results of a finetuned BERT model requires the same preprocessing, architecture, and hyperparameters used in the original experiment. To reproduce the results, the same evaluation metrics should be used, and the results should be compared to the original experiment. However, the dataset used for finetuning the BERT model is highly dependent on the application. Different tasks require different datasets, and the quality and size of the dataset can greatly affect the model's performance. A small, poorly labeled dataset may not provide enough information for the model to learn, while a large, well-labeled dataset may enable the model to generalize better. Therefore, it is essential to choose an appropriate dataset for the specific task at hand to achieve optimal results. The primary objective of this paper was not to provide a step-by-step guide for reproducing a specific model.",
                    "score": 0.4975211533721764,
                    "section_title": "Discussion and Conclusion",
                    "char_start_offset": 15652,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 135
                        },
                        {
                            "start": 136,
                            "end": 367
                        },
                        {
                            "start": 368,
                            "end": 537
                        },
                        {
                            "start": 538,
                            "end": 683
                        },
                        {
                            "start": 686,
                            "end": 838
                        },
                        {
                            "start": 839,
                            "end": 986
                        },
                        {
                            "start": 987,
                            "end": 1178
                        },
                        {
                            "start": 1179,
                            "end": 1344
                        },
                        {
                            "start": 1347,
                            "end": 1496
                        },
                        {
                            "start": 1497,
                            "end": 1629
                        },
                        {
                            "start": 1630,
                            "end": 1725
                        },
                        {
                            "start": 1726,
                            "end": 1853
                        },
                        {
                            "start": 1854,
                            "end": 2023
                        },
                        {
                            "start": 2024,
                            "end": 2141
                        },
                        {
                            "start": 2142,
                            "end": 2251
                        }
                    ],
                    "ref_mentions": [],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.6904296875
                }
            ],
            "relevance_judgement": 0.6904296875,
            "relevance_judgment_input_expanded": "# Title: Imbalanced Multi-label Classification for Business-related Text with Moderately Large Label Spaces\n# Venue: arXiv.org\n# Authors: Muhammad Arslan, C. Cruz\n## Abstract\nIn this study, we compared the performance of four different methods for multi label text classification using a specific imbalanced business dataset. The four methods we evaluated were fine tuned BERT, Binary Relevance, Classifier Chains, and Label Powerset. The results show that fine tuned BERT outperforms the other three methods by a significant margin, achieving high values of accuracy, F1 Score, Precision, and Recall. Binary Relevance also performs well on this dataset, while Classifier Chains and Label Powerset demonstrate relatively poor performance. These findings highlight the effectiveness of fine tuned BERT for multi label text classification tasks, and suggest that it may be a useful tool for businesses seeking to analyze complex and multifaceted texts.\n## Discussion and Conclusion\nThe study focused on the problem of imbalanced multi-label classification for business-related text with moderately large label spaces. The experiment compared the performance of four methods, namely Binary Relevance, Classifier Chains, Label Power-set, and Fine-tuned BERT, and evaluated their effectiveness based on accuracy, F1-Score, Precision, and Recall values. The results revealed that the fine-tuned BERT method significantly outperformed the other three methods, achieving high accuracy, F1-Score, Precision, and Recall values. Binary Relevance also performed well, but Classifier Chains and Label Power-set exhibited relatively poor performance on this imbalanced dataset. \n\nOverall, the findings suggest that finetuning the pre-trained BERT model is a good idea because it enables the model to adapt to a specific application. BERT is a powerful language model that has been pre-trained on a large corpus of text, allowing it to learn the intricacies of language and syntax. However, while pre-training provides a strong foundation, it does not necessarily optimize the model for specific tasks such as sentiment analysis, question answering, or text classification. Finetuning allows us to take the pre-trained BERT model and tailor it to our specific needs by training it on a smaller dataset that is specific to the task at hand. \n\nReproducing the results of a finetuned BERT model requires the same preprocessing, architecture, and hyperparameters used in the original experiment. To reproduce the results, the same evaluation metrics should be used, and the results should be compared to the original experiment. However, the dataset used for finetuning the BERT model is highly dependent on the application. Different tasks require different datasets, and the quality and size of the dataset can greatly affect the model's performance. A small, poorly labeled dataset may not provide enough information for the model to learn, while a large, well-labeled dataset may enable the model to generalize better. Therefore, it is essential to choose an appropriate dataset for the specific task at hand to achieve optimal results. The primary objective of this paper was not to provide a step-by-step guide for reproducing a specific model.",
            "reference_string": "[259138635 | Arslan et al. | 2023 | Citations: 3]"
        },
        {
            "title": "Intent Detection Problem Solving via Automatic DNN Hyperparameter Optimization",
            "venue": "Applied Sciences",
            "year": 2020,
            "reference_count": 48,
            "citation_count": 13,
            "influential_citation_count": 0,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://www.mdpi.com/2076-3417/10/21/7426/pdf?version=1603767208",
                "status": "GOLD",
                "license": "CCBY",
                "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.3390/app10217426?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.3390/app10217426, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "1403992181",
                    "name": "J. Kapo\u010di\u016bt\u0117-Dzikien\u0117"
                },
                {
                    "authorId": "3288489",
                    "name": "K. Balodis"
                },
                {
                    "authorId": "3283640",
                    "name": "Raivis Skadins"
                }
            ],
            "abstract": "Accurate intent detection-based chatbots are usually trained on larger datasets that are not available for some languages. Seeking the most accurate models, three English benchmark datasets that were human-translated into four morphologically complex languages (i.e., Estonian, Latvian, Lithuanian, Russian) were used. Two types of word embeddings (fastText and BERT), three types of deep neural network (DNN) classifiers (convolutional neural network (CNN); long short-term memory method (LSTM), and bidirectional LSTM (BiLSTM)), different DNN architectures (shallower and deeper), and various DNN hyperparameter values were investigated. DNN architecture and hyperparameter values were optimized automatically using the Bayesian method and random search. On three datasets of 2/5/8 intents for English, Estonian, Latvian, Lithuanian, and Russian languages, accuracies of 0.991/0.890/0.712, 0.972/0.890/0.644, 1.000/0.890/0.644, 0.981/0.872/0.712, and 0.972/0.881/0.661 were achieved, respectively. The BERT multilingual vectorization with the CNN classifier was proven to be a good choice for all datasets for all languages. Moreover, in the majority of models, the same set of optimal hyperparameter values was determined. The results obtained in this research were also compared with the previously reported values (where hyperparameter values of DNN models were selected by an expert). This comparison revealed that automatically optimized models are competitive or even more accurate when created with larger training datasets.",
            "corpus_id": 226337596,
            "sentences": [
                {
                    "corpus_id": "226337596",
                    "title": "Intent Detection Problem Solving via Automatic DNN Hyperparameter Optimization",
                    "text": "Since experimental conditions are equalized for all languages (the same number of intents, the same distribution of instances among different classes, the same classifiers, the same optimization algorithms), the only difference lies in the language processing, i.e., vectorization. However, with neural vectorization, none of these languages have an advantage due to a smaller vocabulary (because the vector space dimensionality is the same for all languages) or less variety in inflection forms (because word embeddings are not discrete but distributional). However, some differences between the choices of word embedding types still exist. As seen from Figure 2, BERT vectorization is a better choice compared to fastText for all morphologically complex languages for all datasets, and this is not surprising. Morphologically complex languages (especially fusional languages) suffer from disambiguation problems, but BERT has mechanisms that are able to vectorize even those words that are written the same but have different meanings, depending on their context, differently. Despite the fact that fastText embeddings are also trained to consider a context around the target word, that context is restricted to only a few words. Despite this, fastText is a suitable vectorization solution for languages (such as English) with strict word order in a sentence. In contrast, BERT is able to consider a much broader context (words, sentences, their order) compared to fastText and is, therefore, more suitable for languages that have a relatively free word order in a sentence (such as Latvian, Lithuanian, and Russian). \n\nDespite LSTM, BiLSTM classifiers can sometimes be very accurate (especially on the chatbot dataset, having only two intents and enough representative training data); the domination of the CNN classifier is obvious (see Figure 2). Since we are solving the topic-based intent detection problem (where different intents are related to different topics), the contextual words or their n-grams seem to play a more important role than the sequential nature of the text. \n\nFurthermore, our focus is on the DNN architectures (the most accurate DNN architectures are visualized using the plot_model utility function in Keras) and the hyperparameter values of the most accurate models.",
                    "score": 0.5138130783523996,
                    "section_title": "Discussion",
                    "char_start_offset": 28586,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 281
                        },
                        {
                            "start": 282,
                            "end": 558
                        },
                        {
                            "start": 559,
                            "end": 641
                        },
                        {
                            "start": 642,
                            "end": 811
                        },
                        {
                            "start": 812,
                            "end": 1078
                        },
                        {
                            "start": 1079,
                            "end": 1231
                        },
                        {
                            "start": 1232,
                            "end": 1361
                        },
                        {
                            "start": 1362,
                            "end": 1619
                        },
                        {
                            "start": 1622,
                            "end": 1851
                        },
                        {
                            "start": 1852,
                            "end": 2085
                        },
                        {
                            "start": 2088,
                            "end": 2297
                        }
                    ],
                    "ref_mentions": [],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.689453125
                }
            ],
            "relevance_judgement": 0.689453125,
            "relevance_judgment_input_expanded": "# Title: Intent Detection Problem Solving via Automatic DNN Hyperparameter Optimization\n# Venue: Applied Sciences\n# Authors: J. Kapo\u010di\u016bt\u0117-Dzikien\u0117, K. Balodis, Raivis Skadins\n## Abstract\nAccurate intent detection-based chatbots are usually trained on larger datasets that are not available for some languages. Seeking the most accurate models, three English benchmark datasets that were human-translated into four morphologically complex languages (i.e., Estonian, Latvian, Lithuanian, Russian) were used. Two types of word embeddings (fastText and BERT), three types of deep neural network (DNN) classifiers (convolutional neural network (CNN); long short-term memory method (LSTM), and bidirectional LSTM (BiLSTM)), different DNN architectures (shallower and deeper), and various DNN hyperparameter values were investigated. DNN architecture and hyperparameter values were optimized automatically using the Bayesian method and random search. On three datasets of 2/5/8 intents for English, Estonian, Latvian, Lithuanian, and Russian languages, accuracies of 0.991/0.890/0.712, 0.972/0.890/0.644, 1.000/0.890/0.644, 0.981/0.872/0.712, and 0.972/0.881/0.661 were achieved, respectively. The BERT multilingual vectorization with the CNN classifier was proven to be a good choice for all datasets for all languages. Moreover, in the majority of models, the same set of optimal hyperparameter values was determined. The results obtained in this research were also compared with the previously reported values (where hyperparameter values of DNN models were selected by an expert). This comparison revealed that automatically optimized models are competitive or even more accurate when created with larger training datasets.\n## Discussion\nSince experimental conditions are equalized for all languages (the same number of intents, the same distribution of instances among different classes, the same classifiers, the same optimization algorithms), the only difference lies in the language processing, i.e., vectorization. However, with neural vectorization, none of these languages have an advantage due to a smaller vocabulary (because the vector space dimensionality is the same for all languages) or less variety in inflection forms (because word embeddings are not discrete but distributional). However, some differences between the choices of word embedding types still exist. As seen from Figure 2, BERT vectorization is a better choice compared to fastText for all morphologically complex languages for all datasets, and this is not surprising. Morphologically complex languages (especially fusional languages) suffer from disambiguation problems, but BERT has mechanisms that are able to vectorize even those words that are written the same but have different meanings, depending on their context, differently. Despite the fact that fastText embeddings are also trained to consider a context around the target word, that context is restricted to only a few words. Despite this, fastText is a suitable vectorization solution for languages (such as English) with strict word order in a sentence. In contrast, BERT is able to consider a much broader context (words, sentences, their order) compared to fastText and is, therefore, more suitable for languages that have a relatively free word order in a sentence (such as Latvian, Lithuanian, and Russian). \n\nDespite LSTM, BiLSTM classifiers can sometimes be very accurate (especially on the chatbot dataset, having only two intents and enough representative training data); the domination of the CNN classifier is obvious (see Figure 2). Since we are solving the topic-based intent detection problem (where different intents are related to different topics), the contextual words or their n-grams seem to play a more important role than the sequential nature of the text. \n\nFurthermore, our focus is on the DNN architectures (the most accurate DNN architectures are visualized using the plot_model utility function in Keras) and the hyperparameter values of the most accurate models.",
            "reference_string": "[226337596 | Kapociute-Dzikiene et al. | 2020 | Citations: 13]"
        },
        {
            "title": "Application of Knowledge Distillation Based on Transfer Learning of ERNIE Model in Intelligent Dialogue Intention Recognition",
            "venue": "Italian National Conference on Sensors",
            "year": 2022,
            "reference_count": 22,
            "citation_count": 8,
            "influential_citation_count": 0,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://www.mdpi.com/1424-8220/22/3/1270/pdf?version=1644376437",
                "status": "GOLD",
                "license": "CCBY",
                "disclaimer": "Notice: Paper or abstract available at https://pmc.ncbi.nlm.nih.gov/articles/PMC8838728, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2119112458",
                    "name": "Shiguang Guo"
                },
                {
                    "authorId": "2117944486",
                    "name": "Qing Wang"
                }
            ],
            "abstract": "The \u2018intention\u2019 classification of a user question is an important element of a task-engine driven chatbot. The essence of a user question\u2019s intention understanding is the text classification. The transfer learning, such as BERT (Bidirectional Encoder Representations from Transformers) and ERNIE (Enhanced Representation through Knowledge Integration), has put the text classification task into a new level, but the BERT and ERNIE model are difficult to support high QPS (queries per second) intelligent dialogue systems due to computational performance issues. In reality, the simple classification model usually shows a high computational performance, but they are limited by low accuracy. In this paper, we use knowledge of the ERNIE model to distill the FastText model; the ERNIE model works as a teacher model to predict the massive online unlabeled data for data enhancement, and then guides the training of the student model of FastText with better computational efficiency. The FastText model is distilled by the ERNIE model in chatbot intention classification. This not only guarantees the superiority of its original computational performance, but also the intention classification accuracy has been significantly improved.",
            "corpus_id": 246738423,
            "sentences": [
                {
                    "corpus_id": "246738423",
                    "title": "Application of Knowledge Distillation Based on Transfer Learning of ERNIE Model in Intelligent Dialogue Intention Recognition",
                    "text": "In 2017, Mikolov et al. [12] proposed the FastText model; the network architecture of FastText is very similar to CBOW (Continuous Bag-of-Words) [13], and FastText is mainly used for text classification and displays positive computational efficiency. Peters et al. [14] generalizes traditional word embedding research ELMo (Embedding from Language Model) along a different dimension in 2018, and they suggested extracting context sensitive features and applying pre-training tasks to text classification tasks. \n\nFine-tuning based on pre-training for a language model has been developed for text generation tasks (e.g., GPT (Radford et al., 2018) [15]; Google proposed the BERT (Bidirectional Encoder Representations from Transformers) model in 2018 [16]. The BERT model is based on a self-attention model architecture. The successful application of BERT has put the natural language text classification task into a new level. \n\nThe BERT model mainly focuses on words or more fine-grained cloze learning and does not make full use of the lexical structure in the training data. Li [17] used the application of the BERT model in text classification at the sub-word level. Baidu proposed the ERNIE (Enhanced Representation through Knowledge Integration) model in 2018 [18]. Compared with BERT, ERNIE has improved two masking strategies; the first is the phrase-based masking strategy, and the second is the entity (name, location, organization, product, etc.) based masking strategy. Although the BERT and ERNIE models can achieve positive results in various tasks in the field of natural language processing, it is difficult for scientific research teams in universities as well as in small-and medium-sized enterprises to pre-train BERT from the base model due to the large size of the model, the large amount of data, and the large number of resources required for training. In the field of computer vision, Hinton et al. [19] proposed a training method based on the \"teacherstudent network idea\" in 2015. The basic principle was to transfer the features learned from the \"teacher\" network with a complex network structure and strong learning ability to the \"student\" network with a small number of parameters and weak learning ability.",
                    "score": 0.568424008621727,
                    "section_title": "Introduction",
                    "char_start_offset": 3875,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 250
                        },
                        {
                            "start": 251,
                            "end": 510
                        },
                        {
                            "start": 513,
                            "end": 755
                        },
                        {
                            "start": 756,
                            "end": 819
                        },
                        {
                            "start": 820,
                            "end": 926
                        },
                        {
                            "start": 929,
                            "end": 1077
                        },
                        {
                            "start": 1078,
                            "end": 1170
                        },
                        {
                            "start": 1171,
                            "end": 1271
                        },
                        {
                            "start": 1272,
                            "end": 1481
                        },
                        {
                            "start": 1482,
                            "end": 1875
                        },
                        {
                            "start": 1876,
                            "end": 2006
                        },
                        {
                            "start": 2007,
                            "end": 2237
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 1081,
                            "end": 1085,
                            "matchedPaperCorpusId": "222020002"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.6806640625
                }
            ],
            "relevance_judgement": 0.6806640625,
            "relevance_judgment_input_expanded": "# Title: Application of Knowledge Distillation Based on Transfer Learning of ERNIE Model in Intelligent Dialogue Intention Recognition\n# Venue: Italian National Conference on Sensors\n# Authors: Shiguang Guo, Qing Wang\n## Abstract\nThe \u2018intention\u2019 classification of a user question is an important element of a task-engine driven chatbot. The essence of a user question\u2019s intention understanding is the text classification. The transfer learning, such as BERT (Bidirectional Encoder Representations from Transformers) and ERNIE (Enhanced Representation through Knowledge Integration), has put the text classification task into a new level, but the BERT and ERNIE model are difficult to support high QPS (queries per second) intelligent dialogue systems due to computational performance issues. In reality, the simple classification model usually shows a high computational performance, but they are limited by low accuracy. In this paper, we use knowledge of the ERNIE model to distill the FastText model; the ERNIE model works as a teacher model to predict the massive online unlabeled data for data enhancement, and then guides the training of the student model of FastText with better computational efficiency. The FastText model is distilled by the ERNIE model in chatbot intention classification. This not only guarantees the superiority of its original computational performance, but also the intention classification accuracy has been significantly improved.\n## Introduction\nIn 2017, Mikolov et al. [12] proposed the FastText model; the network architecture of FastText is very similar to CBOW (Continuous Bag-of-Words) [13], and FastText is mainly used for text classification and displays positive computational efficiency. Peters et al. [14] generalizes traditional word embedding research ELMo (Embedding from Language Model) along a different dimension in 2018, and they suggested extracting context sensitive features and applying pre-training tasks to text classification tasks. \n\nFine-tuning based on pre-training for a language model has been developed for text generation tasks (e.g., GPT (Radford et al., 2018) [15]; Google proposed the BERT (Bidirectional Encoder Representations from Transformers) model in 2018 [16]. The BERT model is based on a self-attention model architecture. The successful application of BERT has put the natural language text classification task into a new level. \n\nThe BERT model mainly focuses on words or more fine-grained cloze learning and does not make full use of the lexical structure in the training data. Li [17] used the application of the BERT model in text classification at the sub-word level. Baidu proposed the ERNIE (Enhanced Representation through Knowledge Integration) model in 2018 [18]. Compared with BERT, ERNIE has improved two masking strategies; the first is the phrase-based masking strategy, and the second is the entity (name, location, organization, product, etc.) based masking strategy. Although the BERT and ERNIE models can achieve positive results in various tasks in the field of natural language processing, it is difficult for scientific research teams in universities as well as in small-and medium-sized enterprises to pre-train BERT from the base model due to the large size of the model, the large amount of data, and the large number of resources required for training. In the field of computer vision, Hinton et al. [19] proposed a training method based on the \"teacherstudent network idea\" in 2015. The basic principle was to transfer the features learned from the \"teacher\" network with a complex network structure and strong learning ability to the \"student\" network with a small number of parameters and weak learning ability.",
            "reference_string": "[246738423 | Guo et al. | 2022 | Citations: 8]"
        },
        {
            "title": "Analysis and Evaluation of Language Models for Word Sense Disambiguation",
            "venue": "International Conference on Computational Logic",
            "year": 2021,
            "reference_count": 135,
            "citation_count": 72,
            "influential_citation_count": 6,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://direct.mit.edu/coli/article-pdf/47/2/387/1938124/coli_a_00405.pdf",
                "status": "GOLD",
                "license": "CCBYNCND",
                "disclaimer": "Notice: Paper or abstract available at https://aclanthology.org/2021.cl-2.14, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "144653901",
                    "name": "Daniel Loureiro"
                },
                {
                    "authorId": "1667035673",
                    "name": "Kiamehr Rezaee"
                },
                {
                    "authorId": "1717641",
                    "name": "Mohammad Taher Pilehvar"
                },
                {
                    "authorId": "1387447871",
                    "name": "Jos\u00e9 Camacho-Collados"
                }
            ],
            "abstract": "Abstract Transformer-based language models have taken many fields in NLP by storm. BERT and its derivatives dominate most of the existing evaluation benchmarks, including those for Word Sense Disambiguation (WSD), thanks to their ability in capturing context-sensitive semantic nuances. However, there is still little knowledge about their capabilities and potential limitations in encoding and recovering word senses. In this article, we provide an in-depth quantitative and qualitative analysis of the celebrated BERT model with respect to lexical ambiguity. One of the main conclusions of our analysis is that BERT can accurately capture high-level sense distinctions, even when a limited number of examples is available for each word sense. Our analysis also reveals that in some cases language models come close to solving coarse-grained noun disambiguation under ideal conditions in terms of availability of training data and computing resources. However, this scenario rarely occurs in real-world settings and, hence, many practical challenges remain even in the coarse-grained setting. We also perform an in-depth comparison of the two main language model-based WSD strategies, namely, fine-tuning and feature extraction, finding that the latter approach is more robust with respect to sense bias and it can better exploit limited available training data. In fact, the simple feature extraction strategy of averaging contextualized embeddings proves robust even using only three training sentences per word sense, with minimal improvements obtained by increasing the size of this training data.",
            "corpus_id": 232278102,
            "sentences": [
                {
                    "corpus_id": "232278102",
                    "title": "Analysis and Evaluation of Language Models for Word Sense Disambiguation",
                    "text": "Word-specific results for different configurations of BERT and ALBERT as well as the FastText baseline are shown in Table 5. In general, results are high for all Transformerbased models, over 90% in most cases. This reinforces the potential of language models for WSD, both in its light-weight 1NN and in the fine-tuning settings. Although BERT-Large slightly improves over BERT-Base, the performance of the former is very Table 5 Micro-F1 (top) and macro-F1 (bottom) performance on the full CoarseWSD-20 data set for eight different models: FastText-Base (FTX-B) and -Crawl (FTX-C), 1NN and fine-tuned BERT-Base (BRT-B), -Large (BRT-L), and ALBERT-XXL (ALBRT). An estimation of the human performance (see Section 4.2 for more details) and the most frequent sense (MFS) baseline are also reported for each word. Rows in each table are sorted by the entropy of sense distribution (see Table 3), in descending order. similar to that of ALBERT-XXL across different configurations, despite having different architectures, number of parameters, and training objectives. Overall, performance variations in different models are similar to those for the human baseline. For instance, words such as java and digit seem easy for both humans and models to disambiguate, whereas words such as bass and club are challenging perhaps because of their more finegrained distinctions. 20 As a perhaps surprising result, having more training instances does not necessarily lead to better performance, indicated by the very low Pearson correlation (0.2 or lower) of the number of training instances with results in all BERT configurations. Also, higher polysemy is not a strong indicator of lower performance (see Table 4.3 for statistics of the 20 words, including polysemy), as one would expect from a classification task with a higher number of classes (near zero average correlation across settings). In the following we also discuss other relevant points with respect to Most Frequent Sense (MFS) bias and fine-tuning.\n\nMFS Bias. As expected, macro-F1 results degrade for the purely supervised classification models (FastText and fine-tuned BERT), indicating the inherent sense bias",
                    "score": 0.558253483032436,
                    "section_title": "Results",
                    "char_start_offset": 52285,
                    "sentence_offsets": [],
                    "ref_mentions": [],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.67626953125
                },
                {
                    "corpus_id": "232278102",
                    "title": "Analysis and Evaluation of Language Models for Word Sense Disambiguation",
                    "text": "Most Frequent Class 50.0 Upperbound\n\nHuman performance 80.0 task consists of deciding whether the two target words in context refer to the same sense or not. Even though no sense inventory is explicitly given, this data set was also constructed based on WordNet. Table 12 shows a few examples from the data set.\n\nBERT-Based Model. Given that the task in WiC is a binary classification, the 1NN model is not applicable because a training to learn sense margins is necessary. Therefore, we experimented with the BERT model fine-tuned on WiC's training data. We followed Wang et al. (2019) and fused the two sentences and fed them as input to BERT. A classifier was then trained on the concatenation of the resulting BERT contextual embeddings.\n\nBaselines. In addition to our BERT-based model, we include results for two Fast-Text supervised classifiers (Joulin et al. 2017) as baselines: a basic one with random initialization (FastText-B) and another initialized with FastText embeddings trained on the Common Crawl (FastText-C). As other indicative reference points, we added two language models that are enriched with WordNet (Levine et al. 2020;Loureiro and Jorge 2019b) and another with WordNet and Wikipedia .\n\nResults. Table 13 shows the result of BERT models and the other baselines on the WiC benchmark. 24 We can see that BERT significantly outperforms the FastText static word embedding. The two versions of BERT (Base and Large) perform equally well on this task, achieving results close to the state of the art. As with fine-grained all-words WSD, the additional knowledge drawn from WordNet proves to be beneficial, as shown by the results for KnowBERT and SenseBERT. Table 17 shows for each sense their ID (as per their Wikipedia page title), definition, and example usage from the data set.",
                    "score": 0.5249271226648802,
                    "section_title": "Lowerbound",
                    "char_start_offset": 86334,
                    "sentence_offsets": [],
                    "ref_mentions": [
                        {
                            "start": 568,
                            "end": 586,
                            "matchedPaperCorpusId": "143424870"
                        },
                        {
                            "start": 851,
                            "end": 870,
                            "matchedPaperCorpusId": "1210515"
                        },
                        {
                            "start": 1127,
                            "end": 1147,
                            "matchedPaperCorpusId": "199668663"
                        },
                        {
                            "start": 1147,
                            "end": 1172,
                            "matchedPaperCorpusId": "195345080"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.66943359375
                },
                {
                    "corpus_id": "232278102",
                    "title": "Analysis and Evaluation of Language Models for Word Sense Disambiguation",
                    "text": "CoarseWSD-20 consists of 20 separate sets, each containing sentences for different senses of the corresponding target word. Therefore, the evaluation can be framed as a standard classification task for each word.\n\nGiven the classification nature of the CoarseWSD-20 data sets, we can perform experiments with our 1NN BERT system and compare it with a standard fine-tuned BERT model (see Section 3.3 for more details on the LM-based WSD approaches). Note that fine-tuning for individual target words results in many models (one per word). Therefore, this setup would not be computationally feasible in a general WSD setting, as the number of models would approach the vocabulary size. However, in our experiments we are interested in verifying the limits of BERT, without any other confounds or model-specific restrictions.\n\nTo ensure that our conclusions are generalizable, we also report 1NN and finetuning results using ALBERT. In spite of substantial operational differences, BERT and ALBERT have the most similar training objectives and tokenization methods out of several other prominent Transformer-based models (Yang et al. 2019;Liu et al. 2019b), thus being the most directly comparable. Given the similar performance between BERT-Large and ALBERT-XXLarge on the main CoarseWSD-20 data set, we proceed with further experiments using only BERT.\n\nWe also include two FastText linear classifiers (Joulin et al. 2017) as baselines: FTX-B (base model without pretrained embeddings) and FTX-C (using pretrained embeddings from Common Crawl). We chose FastText as the baseline given its efficiency and competitive results for sentence classification.\n\nConfiguration. Our experiments with BERT and ALBERT used the Transformers framework (v2.5.1) developed by Wolf et al. (2020), and we used the uncased pretrained base and large models released by Devlin et al. (2019) for BERT, and the xxlarge (v2) models released by Lan et al. (2020) for ALBERT. We use the uncased variants of Transformers models to match the casing in CoarseWSD-20 (except for ALBERT, which is only available in cased variants). Following previous feature extraction works (including our experiment in Section 3.4.",
                    "score": 0.5255120166422131,
                    "section_title": "Experimental Setting",
                    "char_start_offset": 46842,
                    "sentence_offsets": [],
                    "ref_mentions": [
                        {
                            "start": 1118,
                            "end": 1136,
                            "matchedPaperCorpusId": "195069387"
                        },
                        {
                            "start": 1401,
                            "end": 1420,
                            "matchedPaperCorpusId": "1210515"
                        },
                        {
                            "start": 1759,
                            "end": 1777,
                            "matchedPaperCorpusId": "182952883"
                        },
                        {
                            "start": 1848,
                            "end": 1868,
                            "matchedPaperCorpusId": "52967399"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.6533203125
                }
            ],
            "relevance_judgement": 0.67626953125,
            "relevance_judgment_input_expanded": "# Title: Analysis and Evaluation of Language Models for Word Sense Disambiguation\n# Venue: International Conference on Computational Logic\n# Authors: Daniel Loureiro, Kiamehr Rezaee, Mohammad Taher Pilehvar, Jos\u00e9 Camacho-Collados\n## Abstract\nAbstract Transformer-based language models have taken many fields in NLP by storm. BERT and its derivatives dominate most of the existing evaluation benchmarks, including those for Word Sense Disambiguation (WSD), thanks to their ability in capturing context-sensitive semantic nuances. However, there is still little knowledge about their capabilities and potential limitations in encoding and recovering word senses. In this article, we provide an in-depth quantitative and qualitative analysis of the celebrated BERT model with respect to lexical ambiguity. One of the main conclusions of our analysis is that BERT can accurately capture high-level sense distinctions, even when a limited number of examples is available for each word sense. Our analysis also reveals that in some cases language models come close to solving coarse-grained noun disambiguation under ideal conditions in terms of availability of training data and computing resources. However, this scenario rarely occurs in real-world settings and, hence, many practical challenges remain even in the coarse-grained setting. We also perform an in-depth comparison of the two main language model-based WSD strategies, namely, fine-tuning and feature extraction, finding that the latter approach is more robust with respect to sense bias and it can better exploit limited available training data. In fact, the simple feature extraction strategy of averaging contextualized embeddings proves robust even using only three training sentences per word sense, with minimal improvements obtained by increasing the size of this training data.\n## Experimental Setting\nCoarseWSD-20 consists of 20 separate sets, each containing sentences for different senses of the corresponding target word. Therefore, the evaluation can be framed as a standard classification task for each word.\n\nGiven the classification nature of the CoarseWSD-20 data sets, we can perform experiments with our 1NN BERT system and compare it with a standard fine-tuned BERT model (see Section 3.3 for more details on the LM-based WSD approaches). Note that fine-tuning for individual target words results in many models (one per word). Therefore, this setup would not be computationally feasible in a general WSD setting, as the number of models would approach the vocabulary size. However, in our experiments we are interested in verifying the limits of BERT, without any other confounds or model-specific restrictions.\n\nTo ensure that our conclusions are generalizable, we also report 1NN and finetuning results using ALBERT. In spite of substantial operational differences, BERT and ALBERT have the most similar training objectives and tokenization methods out of several other prominent Transformer-based models (Yang et al. 2019;Liu et al. 2019b), thus being the most directly comparable. Given the similar performance between BERT-Large and ALBERT-XXLarge on the main CoarseWSD-20 data set, we proceed with further experiments using only BERT.\n\nWe also include two FastText linear classifiers (Joulin et al. 2017) as baselines: FTX-B (base model without pretrained embeddings) and FTX-C (using pretrained embeddings from Common Crawl). We chose FastText as the baseline given its efficiency and competitive results for sentence classification.\n\nConfiguration. Our experiments with BERT and ALBERT used the Transformers framework (v2.5.1) developed by Wolf et al. (2020), and we used the uncased pretrained base and large models released by Devlin et al. (2019) for BERT, and the xxlarge (v2) models released by Lan et al. (2020) for ALBERT. We use the uncased variants of Transformers models to match the casing in CoarseWSD-20 (except for ALBERT, which is only available in cased variants). Following previous feature extraction works (including our experiment in Section 3.4.\n\n## Results\nWord-specific results for different configurations of BERT and ALBERT as well as the FastText baseline are shown in Table 5. In general, results are high for all Transformerbased models, over 90% in most cases. This reinforces the potential of language models for WSD, both in its light-weight 1NN and in the fine-tuning settings. Although BERT-Large slightly improves over BERT-Base, the performance of the former is very Table 5 Micro-F1 (top) and macro-F1 (bottom) performance on the full CoarseWSD-20 data set for eight different models: FastText-Base (FTX-B) and -Crawl (FTX-C), 1NN and fine-tuned BERT-Base (BRT-B), -Large (BRT-L), and ALBERT-XXL (ALBRT). An estimation of the human performance (see Section 4.2 for more details) and the most frequent sense (MFS) baseline are also reported for each word. Rows in each table are sorted by the entropy of sense distribution (see Table 3), in descending order. similar to that of ALBERT-XXL across different configurations, despite having different architectures, number of parameters, and training objectives. Overall, performance variations in different models are similar to those for the human baseline. For instance, words such as java and digit seem easy for both humans and models to disambiguate, whereas words such as bass and club are challenging perhaps because of their more finegrained distinctions. 20 As a perhaps surprising result, having more training instances does not necessarily lead to better performance, indicated by the very low Pearson correlation (0.2 or lower) of the number of training instances with results in all BERT configurations. Also, higher polysemy is not a strong indicator of lower performance (see Table 4.3 for statistics of the 20 words, including polysemy), as one would expect from a classification task with a higher number of classes (near zero average correlation across settings). In the following we also discuss other relevant points with respect to Most Frequent Sense (MFS) bias and fine-tuning.\n\nMFS Bias. As expected, macro-F1 results degrade for the purely supervised classification models (FastText and fine-tuned BERT), indicating the inherent sense bias\n\n## Lowerbound\nMost Frequent Class 50.0 Upperbound\n\nHuman performance 80.0 task consists of deciding whether the two target words in context refer to the same sense or not. Even though no sense inventory is explicitly given, this data set was also constructed based on WordNet. Table 12 shows a few examples from the data set.\n\nBERT-Based Model. Given that the task in WiC is a binary classification, the 1NN model is not applicable because a training to learn sense margins is necessary. Therefore, we experimented with the BERT model fine-tuned on WiC's training data. We followed Wang et al. (2019) and fused the two sentences and fed them as input to BERT. A classifier was then trained on the concatenation of the resulting BERT contextual embeddings.\n\nBaselines. In addition to our BERT-based model, we include results for two Fast-Text supervised classifiers (Joulin et al. 2017) as baselines: a basic one with random initialization (FastText-B) and another initialized with FastText embeddings trained on the Common Crawl (FastText-C). As other indicative reference points, we added two language models that are enriched with WordNet (Levine et al. 2020;Loureiro and Jorge 2019b) and another with WordNet and Wikipedia .\n\nResults. Table 13 shows the result of BERT models and the other baselines on the WiC benchmark. 24 We can see that BERT significantly outperforms the FastText static word embedding. The two versions of BERT (Base and Large) perform equally well on this task, achieving results close to the state of the art. As with fine-grained all-words WSD, the additional knowledge drawn from WordNet proves to be beneficial, as shown by the results for KnowBERT and SenseBERT. Table 17 shows for each sense their ID (as per their Wikipedia page title), definition, and example usage from the data set.",
            "reference_string": "[232278102 | Loureiro et al. | 2021 | Citations: 72]"
        },
        {
            "title": "Word Embedding Techniques for Classification of Star Ratings",
            "venue": "arXiv.org",
            "year": 2025,
            "reference_count": 36,
            "citation_count": 0,
            "influential_citation_count": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2504.13653, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2233683143",
                    "name": "Hesham Abdelmotaleb"
                },
                {
                    "authorId": "2342295801",
                    "name": "Craig McNeile"
                },
                {
                    "authorId": "2273117131",
                    "name": "Ma\u0142gorzata Wojty\u015b"
                }
            ],
            "abstract": "Telecom services are at the core of today's societies' everyday needs. The availability of numerous online forums and discussion platforms enables telecom providers to improve their services by exploring the views of their customers to learn about common issues that the customers face. Natural Language Processing (NLP) tools can be used to process the free text collected. One way of working with such data is to represent text as numerical vectors using one of many word embedding models based on neural networks. This research uses a novel dataset of telecom customers' reviews to perform an extensive study showing how different word embedding algorithms can affect the text classification process. Several state-of-the-art word embedding techniques are considered, including BERT, Word2Vec and Doc2Vec, coupled with several classification algorithms. The important issue of feature engineering and dimensionality reduction is addressed and several PCA-based approaches are explored. Moreover, the energy consumption used by the different word embeddings is investigated. The findings show that some word embedding models can lead to consistently better text classifiers in terms of precision, recall and F1-Score. In particular, for the more challenging classification tasks, BERT combined with PCA stood out with the highest performance metrics. Moreover, our proposed PCA approach of combining word vectors using the first principal component shows clear advantages in performance over the traditional approach of taking the average.",
            "corpus_id": 277940080,
            "sentences": [
                {
                    "corpus_id": "277940080",
                    "title": "Word Embedding Techniques for Classification of Star Ratings",
                    "text": "This paper examined various word embedding techniques for classifying customer star ratings based on telecom reviews, comparing their effectiveness across different classification models. The results showed that while traditional TF-IDF performed poorly in most cases, it excelled when used with the Support Vector Classifier. BERT-PCA and Doc2Vec-PCA emerged as the most effective WEs, particularly for complex multiclass classification tasks. Word2Vec-PCA and FastText-PCA performed consistently well with Logistic Regression and Stochastic Gradient Descent, highlighting their robustness in certain models. Decision Trees, KNN and Gradient Boosting classifiers demonstrated relative insensitivity to WEs, except in larger multiclass datasets where BERT-PCA led to superior results. Regarding feature engineering, for Word2Vec and FastText, our proposed PCA approach of combining word vectors using the first principal component shows clear advantages in performance over the traditional approach of taking the average. Additionally, energy consumption analysis revealed that computational efficiency varies significantly among embedding techniques, with TF-IDF being the most resource-efficient and FastText the most computationally de-manding. The findings emphasize the importance of selecting the appropriate WE based on classification complexity, model compatibility and computational constraints.",
                    "score": 0.437529853708956,
                    "section_title": "Conclusion",
                    "char_start_offset": 59367,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 187
                        },
                        {
                            "start": 188,
                            "end": 326
                        },
                        {
                            "start": 327,
                            "end": 444
                        },
                        {
                            "start": 445,
                            "end": 609
                        },
                        {
                            "start": 610,
                            "end": 784
                        },
                        {
                            "start": 785,
                            "end": 1021
                        },
                        {
                            "start": 1022,
                            "end": 1247
                        },
                        {
                            "start": 1248,
                            "end": 1404
                        }
                    ],
                    "ref_mentions": [],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.67138671875
                }
            ],
            "relevance_judgement": 0.67138671875,
            "relevance_judgment_input_expanded": "# Title: Word Embedding Techniques for Classification of Star Ratings\n# Venue: arXiv.org\n# Authors: Hesham Abdelmotaleb, Craig McNeile, Ma\u0142gorzata Wojty\u015b\n## Abstract\nTelecom services are at the core of today's societies' everyday needs. The availability of numerous online forums and discussion platforms enables telecom providers to improve their services by exploring the views of their customers to learn about common issues that the customers face. Natural Language Processing (NLP) tools can be used to process the free text collected. One way of working with such data is to represent text as numerical vectors using one of many word embedding models based on neural networks. This research uses a novel dataset of telecom customers' reviews to perform an extensive study showing how different word embedding algorithms can affect the text classification process. Several state-of-the-art word embedding techniques are considered, including BERT, Word2Vec and Doc2Vec, coupled with several classification algorithms. The important issue of feature engineering and dimensionality reduction is addressed and several PCA-based approaches are explored. Moreover, the energy consumption used by the different word embeddings is investigated. The findings show that some word embedding models can lead to consistently better text classifiers in terms of precision, recall and F1-Score. In particular, for the more challenging classification tasks, BERT combined with PCA stood out with the highest performance metrics. Moreover, our proposed PCA approach of combining word vectors using the first principal component shows clear advantages in performance over the traditional approach of taking the average.\n## Conclusion\nThis paper examined various word embedding techniques for classifying customer star ratings based on telecom reviews, comparing their effectiveness across different classification models. The results showed that while traditional TF-IDF performed poorly in most cases, it excelled when used with the Support Vector Classifier. BERT-PCA and Doc2Vec-PCA emerged as the most effective WEs, particularly for complex multiclass classification tasks. Word2Vec-PCA and FastText-PCA performed consistently well with Logistic Regression and Stochastic Gradient Descent, highlighting their robustness in certain models. Decision Trees, KNN and Gradient Boosting classifiers demonstrated relative insensitivity to WEs, except in larger multiclass datasets where BERT-PCA led to superior results. Regarding feature engineering, for Word2Vec and FastText, our proposed PCA approach of combining word vectors using the first principal component shows clear advantages in performance over the traditional approach of taking the average. Additionally, energy consumption analysis revealed that computational efficiency varies significantly among embedding techniques, with TF-IDF being the most resource-efficient and FastText the most computationally de-manding. The findings emphasize the importance of selecting the appropriate WE based on classification complexity, model compatibility and computational constraints.",
            "reference_string": "[277940080 | Abdelmotaleb et al. | 2025 | Citations: 0]"
        },
        {
            "title": "BERT-BiGRU Intelligent Classification of Metro On-Board Equipment Faults Based on Key Layer Fusion",
            "venue": "Wireless Communications and Mobile Computing",
            "year": 2022,
            "reference_count": 45,
            "citation_count": 3,
            "influential_citation_count": 1,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://downloads.hindawi.com/journals/wcmc/2022/6588425.pdf",
                "status": "GOLD",
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.1155/2022/6588425?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1155/2022/6588425, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2055330299",
                    "name": "Endong Liu"
                },
                {
                    "authorId": "9183411",
                    "name": "Junting Lin"
                }
            ],
            "abstract": "Metro train operation may result in massive and complex unstructured fault text data. To solve the problem of low classification accuracy and incomplete classification effect of unstructured fault data automatic classification task, a BERT-BiGRU fault text classification model based on key layer fusion is proposed. Firstly, the unstructured text data is processed into word vectors with location information in the word embedding layer and then input into the BERT layer. Based on the traditional 12-layer BERT model, the semantic information is fully obtained by encoding the two-way transformer encoder in layers 2, 4, 6, 8, and 12 for fusion and dimensionality reduction, which is then input into the BiGRU layer to extract the context information to obtain the high-level feature representation of the text. After that, the final classification results are output in the output layer through the full connection layer FC and softmax functions. This model is tested with other mainstream models on the fault text data of metro on-board equipment. The experiment results show that on the same data set, the \n \n \n \n F\n \n \n 1\n \n \n \n -score of this model is 7%~8% higher than that of the current mainstream classification model, that is, about 1~2% higher than that of other PLMs, and the \n \n \n \n F\n \n \n 1\n \n \n \n -score of this model is higher than that of BERT model with different transformer layers, reaching the highest value of 0.9272, and the convergence speed in the training process is fast.",
            "corpus_id": 250058024,
            "sentences": [
                {
                    "corpus_id": "250058024",
                    "title": "BERT-BiGRU Intelligent Classification of Metro On-Board Equipment Faults Based on Key Layer Fusion",
                    "text": "Several mainstream text classification models in NLP field are used to train the data set in this paper. After 30 epochs, the experimental results shown in Table 4 are obtained (2-BERT, 4-BERT, 6-BERT, 8-BERT, and 12-BERT, respectively, represent the BERT models with different transformer layers). The following comparative analysis is carried out: \n\nFasttext, CNN, BiLSTM, and BiGRU classification models all take the word vector generated by embedding as the upstream task and then send it into the model itself for the completion of text classification. The three evaluation indexes of BiGRU and BiLSTM models are close, and the difference in F 1 -score is only 0.1%, which is because that BiGRU, as a variant of BiLSTM, has a similar structure, and the F 1 -score of CNN model is about 2% higher than that of CNN model. Due to the fact that CNN has strong ability to extract local features of text, however, which cannot capture text information with longer distance between front and back, the effect of slightly longer text classification task will be discounted. Compared with the fasttext model, the F 1 -score is increased by about 1%. As an improved version of word2vec model, the fasttext has simple classifier and fast classification speed, but its simple structure has general effect on the classification task of large-scale data. Both BiGRU and BiLSTM can effectively capture context information and generate high-level text semantic representation. Therefore, BiGRU or BiLSTM can be used as fault text classifier to achieve better classification effect. \n\nAs shown in Table 4, the comparison between 3 and 6 shows that the F 1 -score of ALBERT-BiLSTM is 6.66% higher than BiLSTM. Compared with 4 and 7, the F 1 -score of AlBERT-BiGRU is increased by 7.14% based on BiGRU. Then, by comparing the models after 4 and 8, the BiGRU with different layers of BERT model can increase the F 1 -score of the original model by more than 7.5% based on the original model.",
                    "score": 0.44216545120690665,
                    "section_title": "The Classification Effect of Different Classifiers on the Data Set.",
                    "char_start_offset": 28835,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 104
                        },
                        {
                            "start": 105,
                            "end": 298
                        },
                        {
                            "start": 299,
                            "end": 349
                        },
                        {
                            "start": 352,
                            "end": 557
                        },
                        {
                            "start": 558,
                            "end": 824
                        },
                        {
                            "start": 825,
                            "end": 1070
                        },
                        {
                            "start": 1071,
                            "end": 1145
                        },
                        {
                            "start": 1146,
                            "end": 1345
                        },
                        {
                            "start": 1346,
                            "end": 1465
                        },
                        {
                            "start": 1466,
                            "end": 1570
                        },
                        {
                            "start": 1573,
                            "end": 1696
                        },
                        {
                            "start": 1697,
                            "end": 1788
                        },
                        {
                            "start": 1789,
                            "end": 1976
                        }
                    ],
                    "ref_mentions": [],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.66943359375
                }
            ],
            "relevance_judgement": 0.66943359375,
            "relevance_judgment_input_expanded": "# Title: BERT-BiGRU Intelligent Classification of Metro On-Board Equipment Faults Based on Key Layer Fusion\n# Venue: Wireless Communications and Mobile Computing\n# Authors: Endong Liu, Junting Lin\n## Abstract\nMetro train operation may result in massive and complex unstructured fault text data. To solve the problem of low classification accuracy and incomplete classification effect of unstructured fault data automatic classification task, a BERT-BiGRU fault text classification model based on key layer fusion is proposed. Firstly, the unstructured text data is processed into word vectors with location information in the word embedding layer and then input into the BERT layer. Based on the traditional 12-layer BERT model, the semantic information is fully obtained by encoding the two-way transformer encoder in layers 2, 4, 6, 8, and 12 for fusion and dimensionality reduction, which is then input into the BiGRU layer to extract the context information to obtain the high-level feature representation of the text. After that, the final classification results are output in the output layer through the full connection layer FC and softmax functions. This model is tested with other mainstream models on the fault text data of metro on-board equipment. The experiment results show that on the same data set, the \n \n \n \n F\n \n \n 1\n \n \n \n -score of this model is 7%~8% higher than that of the current mainstream classification model, that is, about 1~2% higher than that of other PLMs, and the \n \n \n \n F\n \n \n 1\n \n \n \n -score of this model is higher than that of BERT model with different transformer layers, reaching the highest value of 0.9272, and the convergence speed in the training process is fast.\n## The Classification Effect of Different Classifiers on the Data Set.\nSeveral mainstream text classification models in NLP field are used to train the data set in this paper. After 30 epochs, the experimental results shown in Table 4 are obtained (2-BERT, 4-BERT, 6-BERT, 8-BERT, and 12-BERT, respectively, represent the BERT models with different transformer layers). The following comparative analysis is carried out: \n\nFasttext, CNN, BiLSTM, and BiGRU classification models all take the word vector generated by embedding as the upstream task and then send it into the model itself for the completion of text classification. The three evaluation indexes of BiGRU and BiLSTM models are close, and the difference in F 1 -score is only 0.1%, which is because that BiGRU, as a variant of BiLSTM, has a similar structure, and the F 1 -score of CNN model is about 2% higher than that of CNN model. Due to the fact that CNN has strong ability to extract local features of text, however, which cannot capture text information with longer distance between front and back, the effect of slightly longer text classification task will be discounted. Compared with the fasttext model, the F 1 -score is increased by about 1%. As an improved version of word2vec model, the fasttext has simple classifier and fast classification speed, but its simple structure has general effect on the classification task of large-scale data. Both BiGRU and BiLSTM can effectively capture context information and generate high-level text semantic representation. Therefore, BiGRU or BiLSTM can be used as fault text classifier to achieve better classification effect. \n\nAs shown in Table 4, the comparison between 3 and 6 shows that the F 1 -score of ALBERT-BiLSTM is 6.66% higher than BiLSTM. Compared with 4 and 7, the F 1 -score of AlBERT-BiGRU is increased by 7.14% based on BiGRU. Then, by comparing the models after 4 and 8, the BiGRU with different layers of BERT model can increase the F 1 -score of the original model by more than 7.5% based on the original model.",
            "reference_string": "[250058024 | Liu et al. | 2022 | Citations: 3]"
        },
        {
            "title": "A Random Oversampling and BERT-based Model Approach for Handling Imbalanced Data in Essay Answer Correction",
            "venue": "Jurnal Infotel",
            "year": 2024,
            "reference_count": 21,
            "citation_count": 0,
            "influential_citation_count": 0,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://doi.org/10.20895/infotel.v16i4.1224",
                "status": "GOLD",
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.20895/infotel.v16i4.1224?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.20895/infotel.v16i4.1224, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2341084544",
                    "name": "Dian Ahkam Sani"
                }
            ],
            "abstract": "The task of automated essay scoring has long been plagued by the challenge of imbalanced datasets, where the distribution of scores or labels is skewed towards certain categories. This imbalance can lead to poor performance of machine learning models, as they tend to be biased towards the majority class. One potential solution to this problem is the use of oversampling techniques, which aim to balance the dataset by increasing the representation of the minority class. In this paper, we propose a novel approach that combines random oversampling with a BERT-base uncased model for essay answer correction. This research explores various scenario of text pre-processing techniques to optimize model accuracy. Using a dataset of essay answers obtained from eighth-grade middle school students in Indonesian language, our approach demonstrates good performance in terms of precision, recall, F1-score and accuracy compared to traditional methods such as Backpropagation Neural Network, Na\u00efve Bayes and Random Forest Classifier using FastText word embedding with Wikipedia 300 vector size pretrained model. The best performance was obtained using the BERT-base uncased model with 2e-5 learning rate and a simplified pre-processing approach. By retaining punctuation, numbers, and stop words, the model achieved a precision of 0.9463, recall of 0.9377, F1-score of 0.9346, and an accuracy of 94%.",
            "corpus_id": 275704171,
            "sentences": [
                {
                    "corpus_id": "275704171",
                    "title": "A Random Oversampling and BERT-based Model Approach for Handling Imbalanced Data in Essay Answer Correction",
                    "text": "This section provides an overview of the steps taken in this study, from text preprocessing to the evaluation phase, as shown in Figure 1. This research begins by inputting multiclass data derived from the essay responses of 8th-grade junior high school students in Indonesian language subjects, with scores ranging from 1 to 5. The data will be processed in two scenarios: one scenario employing traditional methods for data cleaning, which in-cludes case folding, tokenization, and stopword removal; and another one is only case folding and tokenization using the BERT model. After the preprocessing stage is completed, the data will be analyzed to identify the number of minority and majority classes, followed by processing using the Random Over Sampling technique. For traditional model, the balanced data will be processed using pretrained models with FastText (Traditional Method). The pretrained model scenario in the traditional method is divided into two parts: one using the Facebook model with a vector size of 300, and the other using the Wikipedia model with a vector size of 300. After the process with pretrained data is finish, the next step is sentence vector normalization to capture the meaning of each sentence. The final step involves classification using the Random Forest Classifier, Na\u00efve Bayes, and Backpropagation methods. Meanwhile, for the BERT model, the pretrained process is performed using only BERT base Uncased model. This dataset will be classified using the BERT Classifier with the BERT base Uncased model. The dataset used for classification is an oversampled dataset, which will then be analyzed to obtain the confusion matrix, including the calculation of Precision, Recall, F1-Score, and Accuracy. Based on the classification results and the values obtained from the confusion matrix, conclusions will be drawn regarding which method scenario demonstrates superior performance.",
                    "score": 0.45837962830208856,
                    "section_title": "Research Method",
                    "char_start_offset": 5105,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 138
                        },
                        {
                            "start": 139,
                            "end": 577
                        },
                        {
                            "start": 578,
                            "end": 769
                        },
                        {
                            "start": 770,
                            "end": 888
                        },
                        {
                            "start": 889,
                            "end": 1094
                        },
                        {
                            "start": 1095,
                            "end": 1232
                        },
                        {
                            "start": 1233,
                            "end": 1349
                        },
                        {
                            "start": 1350,
                            "end": 1452
                        },
                        {
                            "start": 1453,
                            "end": 1544
                        },
                        {
                            "start": 1545,
                            "end": 1739
                        },
                        {
                            "start": 1740,
                            "end": 1919
                        }
                    ],
                    "ref_mentions": [],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.65869140625
                }
            ],
            "relevance_judgement": 0.65869140625,
            "relevance_judgment_input_expanded": "# Title: A Random Oversampling and BERT-based Model Approach for Handling Imbalanced Data in Essay Answer Correction\n# Venue: Jurnal Infotel\n# Authors: Dian Ahkam Sani\n## Abstract\nThe task of automated essay scoring has long been plagued by the challenge of imbalanced datasets, where the distribution of scores or labels is skewed towards certain categories. This imbalance can lead to poor performance of machine learning models, as they tend to be biased towards the majority class. One potential solution to this problem is the use of oversampling techniques, which aim to balance the dataset by increasing the representation of the minority class. In this paper, we propose a novel approach that combines random oversampling with a BERT-base uncased model for essay answer correction. This research explores various scenario of text pre-processing techniques to optimize model accuracy. Using a dataset of essay answers obtained from eighth-grade middle school students in Indonesian language, our approach demonstrates good performance in terms of precision, recall, F1-score and accuracy compared to traditional methods such as Backpropagation Neural Network, Na\u00efve Bayes and Random Forest Classifier using FastText word embedding with Wikipedia 300 vector size pretrained model. The best performance was obtained using the BERT-base uncased model with 2e-5 learning rate and a simplified pre-processing approach. By retaining punctuation, numbers, and stop words, the model achieved a precision of 0.9463, recall of 0.9377, F1-score of 0.9346, and an accuracy of 94%.\n## Research Method\nThis section provides an overview of the steps taken in this study, from text preprocessing to the evaluation phase, as shown in Figure 1. This research begins by inputting multiclass data derived from the essay responses of 8th-grade junior high school students in Indonesian language subjects, with scores ranging from 1 to 5. The data will be processed in two scenarios: one scenario employing traditional methods for data cleaning, which in-cludes case folding, tokenization, and stopword removal; and another one is only case folding and tokenization using the BERT model. After the preprocessing stage is completed, the data will be analyzed to identify the number of minority and majority classes, followed by processing using the Random Over Sampling technique. For traditional model, the balanced data will be processed using pretrained models with FastText (Traditional Method). The pretrained model scenario in the traditional method is divided into two parts: one using the Facebook model with a vector size of 300, and the other using the Wikipedia model with a vector size of 300. After the process with pretrained data is finish, the next step is sentence vector normalization to capture the meaning of each sentence. The final step involves classification using the Random Forest Classifier, Na\u00efve Bayes, and Backpropagation methods. Meanwhile, for the BERT model, the pretrained process is performed using only BERT base Uncased model. This dataset will be classified using the BERT Classifier with the BERT base Uncased model. The dataset used for classification is an oversampled dataset, which will then be analyzed to obtain the confusion matrix, including the calculation of Precision, Recall, F1-Score, and Accuracy. Based on the classification results and the values obtained from the confusion matrix, conclusions will be drawn regarding which method scenario demonstrates superior performance.",
            "reference_string": "[275704171 | Sani | 2024 | Citations: 0]"
        },
        {
            "title": "Character-level HyperNetworks for Hate Speech Detection",
            "venue": "Expert systems with applications",
            "year": 2021,
            "reference_count": 55,
            "citation_count": 13,
            "influential_citation_count": 0,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://arxiv.org/pdf/2111.06336",
                "status": "GREEN",
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2111.06336, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "1693988594",
                    "name": "Tomer Wullach"
                },
                {
                    "authorId": "39243222",
                    "name": "A. Adler"
                },
                {
                    "authorId": "1816914",
                    "name": "Einat Minkov"
                }
            ],
            "abstract": null,
            "corpus_id": 243985678,
            "sentences": [
                {
                    "corpus_id": "243985678",
                    "title": "Character-level HyperNetworks for Hate Speech Detection",
                    "text": "More recently, larger and deeper language models (LMs), that have been pretrained on massive heterogeneous corpora, were shown to yield state-of-the-art contextual text representations, leading to further improvement in text classification. We focus our attention on the popular transformer-based language encoder of BERT (Devlin et al., 2019) and its variants, as described below. \n\nBERT. Text classification using pretrained language models like BERT (Devlin et al., 2019) has been shown to give state-of-the-art performance on a variety of text processing tasks. While BERT generates task-agnostic contextual word embeddings, it can be optimized to target tasks via fine-tuning (Devlin et al., 2019). We follow the common practice of turning BERT into a text classifier by adding a final feed-forward network, which receives as input the embedding of the input text as processed by BERT (the '[CLS]' token embedding, which has been tuned to represent the meaning of the whole input text sequence), and outputs the target class probabilities via a softmax layer. Given labeled training data, the extended network parameters, including the weights of the terminal network, as well as BERT parameters, are fine-tuned jointly to optimize classification performance. \n\nRoBERTa. This model applies the same architecture as BERT, but has been trained on ten times more data, including news articles and Web content. Due to this augmentation of training data, and other modifications to the pretraining procedure and cost function, RoBERTa has been shown to outperform BERT on multiple benchmark datasets (Liu et al., 2019). \n\nALBERT. The architecture of ALBERT (Lan et al., 2020) was designed as a light variant of the transformer-based BERT model. It enhances BERT in several ways, including a factorization of the embedding parameters and cross-layer parameter sharing; both measures are intended to improve parameter efficiency as well as a form of regularization. ALBERT also replaces the next-sentence-prediction loss that is used in training BERT with sentence-order prediction loss, which focuses on modeling inter-sentence coherence.",
                    "score": 0.4569033069152746,
                    "section_title": "Hate Speech Detection with Pre-trained Language Models",
                    "char_start_offset": 17128,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 240
                        },
                        {
                            "start": 241,
                            "end": 381
                        },
                        {
                            "start": 384,
                            "end": 389
                        },
                        {
                            "start": 390,
                            "end": 565
                        },
                        {
                            "start": 566,
                            "end": 703
                        },
                        {
                            "start": 704,
                            "end": 1064
                        },
                        {
                            "start": 1065,
                            "end": 1264
                        },
                        {
                            "start": 1267,
                            "end": 1275
                        },
                        {
                            "start": 1276,
                            "end": 1411
                        },
                        {
                            "start": 1412,
                            "end": 1619
                        },
                        {
                            "start": 1622,
                            "end": 1629
                        },
                        {
                            "start": 1630,
                            "end": 1744
                        },
                        {
                            "start": 1745,
                            "end": 1963
                        },
                        {
                            "start": 1964,
                            "end": 2137
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 322,
                            "end": 343,
                            "matchedPaperCorpusId": "52967399"
                        },
                        {
                            "start": 453,
                            "end": 474,
                            "matchedPaperCorpusId": "52967399"
                        },
                        {
                            "start": 681,
                            "end": 702,
                            "matchedPaperCorpusId": "52967399"
                        },
                        {
                            "start": 1657,
                            "end": 1675,
                            "matchedPaperCorpusId": "202888986"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.65380859375
                }
            ],
            "relevance_judgement": 0.65380859375,
            "relevance_judgment_input_expanded": "# Title: Character-level HyperNetworks for Hate Speech Detection\n# Venue: Expert systems with applications\n# Authors: Tomer Wullach, A. Adler, Einat Minkov\n## Abstract\nNone\n## Hate Speech Detection with Pre-trained Language Models\nMore recently, larger and deeper language models (LMs), that have been pretrained on massive heterogeneous corpora, were shown to yield state-of-the-art contextual text representations, leading to further improvement in text classification. We focus our attention on the popular transformer-based language encoder of BERT (Devlin et al., 2019) and its variants, as described below. \n\nBERT. Text classification using pretrained language models like BERT (Devlin et al., 2019) has been shown to give state-of-the-art performance on a variety of text processing tasks. While BERT generates task-agnostic contextual word embeddings, it can be optimized to target tasks via fine-tuning (Devlin et al., 2019). We follow the common practice of turning BERT into a text classifier by adding a final feed-forward network, which receives as input the embedding of the input text as processed by BERT (the '[CLS]' token embedding, which has been tuned to represent the meaning of the whole input text sequence), and outputs the target class probabilities via a softmax layer. Given labeled training data, the extended network parameters, including the weights of the terminal network, as well as BERT parameters, are fine-tuned jointly to optimize classification performance. \n\nRoBERTa. This model applies the same architecture as BERT, but has been trained on ten times more data, including news articles and Web content. Due to this augmentation of training data, and other modifications to the pretraining procedure and cost function, RoBERTa has been shown to outperform BERT on multiple benchmark datasets (Liu et al., 2019). \n\nALBERT. The architecture of ALBERT (Lan et al., 2020) was designed as a light variant of the transformer-based BERT model. It enhances BERT in several ways, including a factorization of the embedding parameters and cross-layer parameter sharing; both measures are intended to improve parameter efficiency as well as a form of regularization. ALBERT also replaces the next-sentence-prediction loss that is used in training BERT with sentence-order prediction loss, which focuses on modeling inter-sentence coherence.",
            "reference_string": "[243985678 | Wullach et al. | 2021 | Citations: 13]"
        }
    ],
    "retrieved": [
        {
            "corpus_id": "227231089",
            "title": "Go Simple and Pre-Train on Domain-Specific Corpora: On the Role of Training Data for Text Classification",
            "text": "Pre-trained language models provide the foundations for state-of-the-art performance across a wide range of natural language processing tasks, including text classification. However, most classification datasets assume a large amount labeled data, which is commonly not the case in practical settings. In particular, in this paper we compare the performance of a light-weight linear classifier based on word embeddings, i.e., fastText (Joulin et al., 2017), versus a pre-trained language model, i.e., BERT (Devlin et al., 2019), across a wide range of datasets and classification tasks. In general, results show the importance of domain-specific unlabeled data, both in the form of word embeddings or language models. As for the comparison, BERT outperforms all baselines in standard datasets with large training sets. However, in settings with small training datasets a simple method like fastText coupled with domain-specific word embeddings performs equally well or better than BERT, even when pre-trained on domain-specific data.",
            "score": 0.6852936110644444,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.9541015625
        },
        {
            "corpus_id": "272689526",
            "title": "Rethinking KenLM: Good and Bad Model Ensembles for Efficient Text Quality Filtering in Large Web Corpora",
            "text": "Classifier-based filtering. FastText (Joulin et al., 2016) is widely used to distinguish the quality of data (Computer, 2023;Wei et al., 2023;Li et al., 2024). Similar to KenLM, FastText is also an efficient model that operates on CPUs. However, as detailed in Section 4, KenLM demonstrated superior performance compared to FastText when both were trained on the same dataset. Furthermore, recent research (Gunasekar et al., 2023;Li et al., 2024;Penedo et al., 2024) has focused on finetuning pre-trained embedding models to serve as classifiers for quality filtering. Especially, Fineweb demonstrated that training relatively small-sized LLMs (1.82 billion parameters) on data filtered by a trained classifier (resulting in 350 billion tokens), rather than on unfiltered data, led to performance improvements across various benchmarks. However, these methods are impractical for processing large web corpora due to their high computational costs, which necessitate significant GPU resources.",
            "score": 0.6564641369084506,
            "section_title": "Introduction",
            "char_start_offset": 1709,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 27
                },
                {
                    "start": 28,
                    "end": 159
                },
                {
                    "start": 160,
                    "end": 236
                },
                {
                    "start": 237,
                    "end": 376
                },
                {
                    "start": 377,
                    "end": 568
                },
                {
                    "start": 569,
                    "end": 836
                },
                {
                    "start": 837,
                    "end": 992
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.501953125
        },
        {
            "corpus_id": "260379057",
            "title": "Towards Better Query Classification with Multi-Expert Knowledge Condensation in JD Ads Search",
            "text": "FastText has been widely used in various text classification tasks. \n\nEspecially in E-commerce search tasks, benefited from its high inference efficiency, the FastText with low online latency and stable performance become one of the most popular classification models in the industry. On the other hand, the BERT model has also achieved great success in various natural language processing (NLP) tasks based on its powerful semantic representation. However, there are limited works to investigate the property of both models. Thus we conduct toy experiments on the JD search ads dataset to explore the property of the FastText and the BERT models. Firstly, we collected the training set with nearly 160 million user queries from historical click-through data. Then, we respectively train the FastText and BERT models on this training data. Next, we generate two testing sets from T+0 and T+1 days respectively, and each testing set has nearly 2 million user queries. Importantly, in the testing set of T+0 and T+1 days, 99.89% and 57.47% of test queries have appeared in the training set respectively. Finally, we respectively test both models and record their performance on the T+0 and T+1 days. \n\nTable 1 demonstrates the performance comparison of the Fast-Text and BERT models on various evaluation datasets. We find that the FastText model can do better on the training set and testing set (T+0 day), while the BERT model performs better on the testing set (T+1 day). As 99.89% of test queries on T+0 day have appeared in the training set, it means that the FastText model can be remembered better. On the other hand, as only 57.47% of test queries on T+1 day has exposed in the training set, it means that the BERT model can generalize better on some unseen or low-frequency search query. In other words, in the E-commerce search query classification task, the FastText is better at remembering while the BERT is better at generalization. This empirical finding motivates us to combine the advantages of both models to further improve the online performance. To achieve this goal, we introduce knowledge condensation, a Teacher-Student learning framework illustrated in the following subsection.",
            "score": 0.648502663941603,
            "section_title": "A Closer Look on FastText and BERT Models",
            "char_start_offset": 10344,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 67
                },
                {
                    "start": 70,
                    "end": 284
                },
                {
                    "start": 285,
                    "end": 448
                },
                {
                    "start": 449,
                    "end": 525
                },
                {
                    "start": 526,
                    "end": 647
                },
                {
                    "start": 648,
                    "end": 759
                },
                {
                    "start": 760,
                    "end": 839
                },
                {
                    "start": 840,
                    "end": 966
                },
                {
                    "start": 967,
                    "end": 1101
                },
                {
                    "start": 1102,
                    "end": 1197
                },
                {
                    "start": 1200,
                    "end": 1312
                },
                {
                    "start": 1313,
                    "end": 1472
                },
                {
                    "start": 1473,
                    "end": 1603
                },
                {
                    "start": 1604,
                    "end": 1794
                },
                {
                    "start": 1795,
                    "end": 1944
                },
                {
                    "start": 1945,
                    "end": 2064
                },
                {
                    "start": 2065,
                    "end": 2201
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.91552734375
        },
        {
            "corpus_id": "244895506",
            "title": "Transformer based Contextual Model for Sentiment Analysis of Customer Reviews: A Fine-tuned BERT",
            "text": "The proposed BERT model outperforms in terms of accuracy and model performance compare to other models. The results of the fastText model showed low accuracy when unigram and bigram methods were used for training the model. The overall model training and data preparation tasks took less time for BERT model in comparison to others. This experiment reveals that the BERT model required more computational resources to train compared with other traditional models. The fastText model performed well with a standard server environment with minimal computational resources compare to other models. The fine-tuned BERT model simplifies the sentiment analysis tasks on large datasets. \n\nA proposal for future research can be made to build transformer models for various domains, and framework for a continuous model trained to streamline data processing methods. In the future, more work will be done to conduct pre-training a BERT-base model on datasets of customer reviews for sentiment analysis and detecting emotions.",
            "score": 0.6015251771203731,
            "section_title": "V. CONCLUSION AND FUTURE WORK",
            "char_start_offset": 14497,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 103
                },
                {
                    "start": 104,
                    "end": 223
                },
                {
                    "start": 224,
                    "end": 332
                },
                {
                    "start": 333,
                    "end": 463
                },
                {
                    "start": 464,
                    "end": 594
                },
                {
                    "start": 595,
                    "end": 679
                },
                {
                    "start": 682,
                    "end": 857
                },
                {
                    "start": 858,
                    "end": 1016
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.7607421875
        },
        {
            "corpus_id": "225094545",
            "title": "Predicting Themes within Complex Unstructured Texts: A Case Study on Safeguarding Reports",
            "text": "We evaluate the performance of the machine learning algorithms by using precision, recall, and F1-measure metrics. The summary results are calculated using microand macro-based measures. Early experiments using Word2Vec embeddings [9] and SVM classifier showed unsatisfactory performance compared to fastText embeddings and GNB classifier. Thus, these results are omitted from Table 2. \n\nThe results in Table 2 show that a simple terminology-based pre-processing step leads to slight improvements over the baseline with micro F1 of 0.59 in comparison to baseline micro-F1 of 0.57. Despite the small amount of data, we found that corpus trained embedding provide a notable advantage over pre-trained embeddings in the classifiers performance. fastText classifier outperformed GNB model, especially when domain-based embeddings were used. A non-verbatim example of a sentence where fastText model, based on corpus-trained embeddings performs better than pre-trained embedding models is: 'The police received information that the subject was selling crack'. A potential reason for fastText to classify correctly this sentence versus the classifiers using pre-trained embeddings is that the word 'crack' has the meaning of a 'drug' in the reports. However, this is not the widely accepted meaning for this word and thus it cannot be interpreted correctly by pre-trained models. The GNB based on pre-trained BERT model outperforms the classifiers based on pre-trained embeddings, however it does not lead to improvements over the domain-based models. Fine-tuning BERT is the best performing classifier with micro-F1 of 0.64 and macro-F1 of 0.59 which gives 0.5 improvement over the baseline. The improvement in the results achieved by fine-tuning BERT indicate the importance of adapting even the more context-aware pre-trained language models to the specific domain, especially when the domain contains highly specialised language. Further, the poor performance of classifiers based on pre-trained word models shows the lack of transferability of pre-trained embeddings for a highly specialised domain such as the safeguarding reports.",
            "score": 0.5962114039332037,
            "section_title": "Results",
            "char_start_offset": 10054,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 114
                },
                {
                    "start": 115,
                    "end": 186
                },
                {
                    "start": 187,
                    "end": 339
                },
                {
                    "start": 340,
                    "end": 385
                },
                {
                    "start": 388,
                    "end": 580
                },
                {
                    "start": 581,
                    "end": 741
                },
                {
                    "start": 742,
                    "end": 836
                },
                {
                    "start": 837,
                    "end": 1054
                },
                {
                    "start": 1055,
                    "end": 1243
                },
                {
                    "start": 1244,
                    "end": 1373
                },
                {
                    "start": 1374,
                    "end": 1545
                },
                {
                    "start": 1546,
                    "end": 1686
                },
                {
                    "start": 1687,
                    "end": 1927
                },
                {
                    "start": 1928,
                    "end": 2131
                }
            ],
            "ref_mentions": [
                {
                    "start": 231,
                    "end": 234,
                    "matchedPaperCorpusId": "16447573"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.646484375
        },
        {
            "corpus_id": "264935645",
            "title": "ChineseWebText: Large-scale High-quality Chinese Web Text Extracted with Effective Evaluation Model",
            "text": "Rule-based Text Filtering. Rule-based text filtering methods are the dominant paradigm to identify content-rich and semantically coherent data from collected raw datasets with handcrafted rules. During the collection of pre-training data, there are a large number of text data on the web. However, these data include a lot of noise, such as violence, pornographic, advertisement and error characters. Consequently, in order to extract high-quality data, several rule-based methods have been proposed to explore how to automatically filter undesired content from noisy web data. In these work, deduplication [10] methods are employed to remove duplicate text from the data, while some handcrafted rules [11; 12] are adopted to filter out violence, pornographic, advertisement and other explicit noisy data. Besides, perplexity [13] is also usually used to evaluate the fluency of the texts. However, these work mainly focus on English and lack a complete tool-chain for Chinese. \n\nText Classification Model. Different from rule-based text filtering methods, text classification model is an alternative approach to identify high-quality data with a well-designed classifier. The simplest text classification model is logistic regression [5], which uses the logistic function to calculate the probability values for each text, and then classifies them into positive or negative with a designed threshold. Currently, BERT [14] and FastText [15] are both commonly used text classification models. BERT is a transformer-based [16] pre-training language model that has achieved remarkable performance in various text classification and understanding tasks. Through pre-training on masked language model and next sentence prediction tasks with a large dataset, this model learns powerful language understanding and representation abilities, which makes it perform well on text classification tasks. FastText [15] is also a neural network based approach which is similar to CBOW [17]. It is characterized by its ability to train efficiently and quickly on large-scale data, while achieving competitive classification performance. In this paper, both of these two approaches will be employed to evaluate the qualities of the Chinese texts. \n\nDatasets for Pre-training. In recent years as the scale of pre-trained language models expands, there is a concomitant increase in the demand for large-scale pre-training datasets.",
            "score": 0.5902865156753752,
            "section_title": "Related Work",
            "char_start_offset": 3950,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 26
                },
                {
                    "start": 27,
                    "end": 194
                },
                {
                    "start": 195,
                    "end": 288
                },
                {
                    "start": 289,
                    "end": 400
                },
                {
                    "start": 401,
                    "end": 577
                },
                {
                    "start": 578,
                    "end": 805
                },
                {
                    "start": 806,
                    "end": 889
                },
                {
                    "start": 890,
                    "end": 977
                },
                {
                    "start": 980,
                    "end": 1006
                },
                {
                    "start": 1007,
                    "end": 1172
                },
                {
                    "start": 1173,
                    "end": 1401
                },
                {
                    "start": 1402,
                    "end": 1491
                },
                {
                    "start": 1492,
                    "end": 1649
                },
                {
                    "start": 1650,
                    "end": 1890
                },
                {
                    "start": 1891,
                    "end": 1975
                },
                {
                    "start": 1976,
                    "end": 2120
                },
                {
                    "start": 2121,
                    "end": 2229
                },
                {
                    "start": 2232,
                    "end": 2258
                },
                {
                    "start": 2259,
                    "end": 2412
                }
            ],
            "ref_mentions": [
                {
                    "start": 607,
                    "end": 611,
                    "matchedPaperCorpusId": "235829052"
                },
                {
                    "start": 826,
                    "end": 830,
                    "matchedPaperCorpusId": "207870323"
                },
                {
                    "start": 1235,
                    "end": 1238,
                    "matchedPaperCorpusId": "218971783"
                },
                {
                    "start": 1436,
                    "end": 1440,
                    "matchedPaperCorpusId": "1210515"
                },
                {
                    "start": 1520,
                    "end": 1524,
                    "matchedPaperCorpusId": "13756489"
                },
                {
                    "start": 1900,
                    "end": 1904,
                    "matchedPaperCorpusId": "1210515"
                },
                {
                    "start": 1970,
                    "end": 1974,
                    "matchedPaperCorpusId": "5959482"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.6083984375
        },
        {
            "corpus_id": "253860591",
            "title": "Sentiment Analysis and Stance Detection on German YouTube Comments on Gender Diversity",
            "text": "The two sentiment classification models Guhr et al. (2020) trained and tested on this corpus were FastText and BERT. BERT, Bidirectional Encoder Representations from Transformers, is a language representation model \"designed to pre-train deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers\" (Devlin et al. 2019, 4171). It can be fine-tuned by adding just one more output layer, and it can be applied to solving a variety of NLP tasks without task-specific modifications of the model architecture. BERT relies on two core processes: pretraining and fine-tuning. During the pretraining stage, the model is trained on unlabeled data with a variety of tasks, and after initializing the model with the pretrained parameters, these will be fine-tuned with the use of labeled data. The major advantage of BERT is the existence of a wide spectrum of large pretrained models, which minimizes the need for a big amount of training data for the model to provide representative results. Guhr et al. (2020) used a model pretrained by the developers on the German BERT small model, making use of \"bidirectional training of a deep transformer-based network architecture\" (Guhr et al. 2020(Guhr et al. , 1630)), and trained their German BERT model for SA using the implementation provided by the HuggingFace repository. While the authors recognize certain advantages of FastText, being a traditional word embedding model, and the model's performance was relatively high for both data from known domains as well as from an unknown domain, the BERT model scored better in all of these settings and in classifying unseen data, reaching an F1 score of 0.80 and outperforming FastText by 9.4%. \n\nWe used a Python script to apply the pretrained model to our data. We sought to predict the sentiment of each comment in our dataset separately and to receive a clear result as to whether the polarity is positive, negative, or neutral.",
            "score": 0.5804318853337339,
            "section_title": "SENTIMENT ANALYSIS WITH BERT",
            "char_start_offset": 10302,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 116
                },
                {
                    "start": 117,
                    "end": 390
                },
                {
                    "start": 391,
                    "end": 568
                },
                {
                    "start": 569,
                    "end": 632
                },
                {
                    "start": 633,
                    "end": 846
                },
                {
                    "start": 847,
                    "end": 1046
                },
                {
                    "start": 1047,
                    "end": 1375
                },
                {
                    "start": 1376,
                    "end": 1744
                },
                {
                    "start": 1747,
                    "end": 1813
                },
                {
                    "start": 1814,
                    "end": 1982
                }
            ],
            "ref_mentions": [
                {
                    "start": 40,
                    "end": 58,
                    "matchedPaperCorpusId": "218974283"
                },
                {
                    "start": 1047,
                    "end": 1065,
                    "matchedPaperCorpusId": "218974283"
                },
                {
                    "start": 1228,
                    "end": 1245,
                    "matchedPaperCorpusId": "218974283"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.78955078125
        },
        {
            "corpus_id": "222341529",
            "title": "Geometry matters: Exploring language examples at the decision boundary",
            "text": "We use multiple model architectures to understand the implications of the FIM. \n\n\u2022 Convolutional Neural Network (CNN): We use the same paradigm Kim [2014] for text classification using GloVe Pennington et al. [2014] embeddings. \n\n\u2022 Long Short Term Memory Network (LSTM): LSTM's have been used extensively in Natural Language Processing for variety of tasks like text classification and language modeling Schmidhuber [2015] \u2022 Bag of Tricks for Efficient Text Classification(Fasttext) We used the Fasttext based model Joulin et al. [2017] for classifying text and examining the quantitative aspects of Fisher Information Metric since the model is incredibly computational efficiency and strong performance across multiple datasets. \n\n\u2022 BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding Devlin et al. [2018] Large scale pretrained language models have become extremely popular for data efficiency when trained on downstream tasks. However it has been shown recently, that on large datasets ? performance increase from finetuning is often within 1% of BERT. Thus we limited our analysis of using BERT for qualitative analysis for verification of it's relationship with FIM since accuracy difference is minimal and it is computation intensive to calculate the eigenvalue of the FIM of BERT based fine tuned models. We instead used BERT embeddings in downstream tasks for qualitative evaluation of FIM. \n\nFor the Fasttext, CNN and LSTM models we used early stopping with a patience of 5.",
            "score": 0.5771961249707197,
            "section_title": "MODEL DESCRIPTIONS",
            "char_start_offset": 16064,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 78
                },
                {
                    "start": 81,
                    "end": 227
                },
                {
                    "start": 230,
                    "end": 729
                },
                {
                    "start": 732,
                    "end": 958
                },
                {
                    "start": 959,
                    "end": 1019
                },
                {
                    "start": 1020,
                    "end": 1084
                },
                {
                    "start": 1085,
                    "end": 1340
                },
                {
                    "start": 1341,
                    "end": 1427
                },
                {
                    "start": 1430,
                    "end": 1512
                }
            ],
            "ref_mentions": [
                {
                    "start": 191,
                    "end": 215,
                    "matchedPaperCorpusId": "1957433"
                },
                {
                    "start": 404,
                    "end": 422,
                    "matchedPaperCorpusId": "11715509"
                },
                {
                    "start": 516,
                    "end": 536,
                    "matchedPaperCorpusId": "1210515"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.70556640625
        },
        {
            "corpus_id": "278237416",
            "title": "Aleph-Alpha-GermanWeb: Improving German-language LLM pre-training with model-based data curation and synthetic data generation",
            "text": "We also trained a binary classifier based on fastText. \n\nFor the binary fastText classifier, the low-and high-quality training data subsets consisted of 185,403 documents each. The low quality subset consisted of documents with educational quality scores of one or two, whereas the high quality subset consisted of documents scoring four or five. We used 95% of the data (and the remaining 5% for validation) to train a fastText model to classify between high-and low-quality data. It reached 92% precision and 91.5% recall on the validation set. \n\nTo train the BERT classifier, we randomly selected a maximum of 75,000 documents from each class, which we had previously labelled with the LLM-as-a-judge with scores ranging from one to five. The resulting dataset consisted of 75,000 documents for each score above one, and 25,981 documents with scores of one. We used 95% of this dataset for training to predict the one to five scores. The model achieved an overall accuracy of 42% and a macro-average accuracy of 46% when evaluated on the remaining 5% of the data, which served as the validation set.",
            "score": 0.5711034472472413,
            "section_title": "Educational quality",
            "char_start_offset": 12720,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 54
                },
                {
                    "start": 57,
                    "end": 176
                },
                {
                    "start": 177,
                    "end": 346
                },
                {
                    "start": 347,
                    "end": 481
                },
                {
                    "start": 482,
                    "end": 546
                },
                {
                    "start": 549,
                    "end": 741
                },
                {
                    "start": 742,
                    "end": 860
                },
                {
                    "start": 861,
                    "end": 936
                },
                {
                    "start": 937,
                    "end": 1102
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.444091796875
        },
        {
            "corpus_id": "246738423",
            "title": "Application of Knowledge Distillation Based on Transfer Learning of ERNIE Model in Intelligent Dialogue Intention Recognition",
            "text": "In 2017, Mikolov et al. [12] proposed the FastText model; the network architecture of FastText is very similar to CBOW (Continuous Bag-of-Words) [13], and FastText is mainly used for text classification and displays positive computational efficiency. Peters et al. [14] generalizes traditional word embedding research ELMo (Embedding from Language Model) along a different dimension in 2018, and they suggested extracting context sensitive features and applying pre-training tasks to text classification tasks. \n\nFine-tuning based on pre-training for a language model has been developed for text generation tasks (e.g., GPT (Radford et al., 2018) [15]; Google proposed the BERT (Bidirectional Encoder Representations from Transformers) model in 2018 [16]. The BERT model is based on a self-attention model architecture. The successful application of BERT has put the natural language text classification task into a new level. \n\nThe BERT model mainly focuses on words or more fine-grained cloze learning and does not make full use of the lexical structure in the training data. Li [17] used the application of the BERT model in text classification at the sub-word level. Baidu proposed the ERNIE (Enhanced Representation through Knowledge Integration) model in 2018 [18]. Compared with BERT, ERNIE has improved two masking strategies; the first is the phrase-based masking strategy, and the second is the entity (name, location, organization, product, etc.) based masking strategy. Although the BERT and ERNIE models can achieve positive results in various tasks in the field of natural language processing, it is difficult for scientific research teams in universities as well as in small-and medium-sized enterprises to pre-train BERT from the base model due to the large size of the model, the large amount of data, and the large number of resources required for training. In the field of computer vision, Hinton et al. [19] proposed a training method based on the \"teacherstudent network idea\" in 2015. The basic principle was to transfer the features learned from the \"teacher\" network with a complex network structure and strong learning ability to the \"student\" network with a small number of parameters and weak learning ability.",
            "score": 0.568424008621727,
            "section_title": "Introduction",
            "char_start_offset": 3875,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 250
                },
                {
                    "start": 251,
                    "end": 510
                },
                {
                    "start": 513,
                    "end": 755
                },
                {
                    "start": 756,
                    "end": 819
                },
                {
                    "start": 820,
                    "end": 926
                },
                {
                    "start": 929,
                    "end": 1077
                },
                {
                    "start": 1078,
                    "end": 1170
                },
                {
                    "start": 1171,
                    "end": 1271
                },
                {
                    "start": 1272,
                    "end": 1481
                },
                {
                    "start": 1482,
                    "end": 1875
                },
                {
                    "start": 1876,
                    "end": 2006
                },
                {
                    "start": 2007,
                    "end": 2237
                }
            ],
            "ref_mentions": [
                {
                    "start": 1081,
                    "end": 1085,
                    "matchedPaperCorpusId": "222020002"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.6806640625
        },
        {
            "corpus_id": "276558383",
            "title": "GneissWeb: Preparing High Quality Data for LLMs at Scale",
            "text": "FastText [22] family of binary classifiers have been used in prior datasets [9,11] for identifying highquality pre-training documents. Recently, [11] showed that fastText classifier trained on carefully selected data can outperform sophisticated model-based filtering approaches such as AskLLM (prompting an LLM to ask if a document is helpful). Inspired by their effectiveness coupled with the computational efficiency of fastText classifiers, we use fastText classifiers for quality annotations. \n\nWe employ two fastText classifiers: (i) the fastText classifier from [11] trained on a mix of instruction-formatted data (OpenHermes-2.5 [35]) and high-scoring posts from ELI5 subreddit [36] and (ii) our own fastText classifier trained on a mix of high-quality synthetic data and data annotated by an LLM for high educational value. \n\nSpecifically, we use the supervised fastText package from [22] to train a classifier on 400k documents, equality split between positive (i.e., high-quality) and negative (i.e., low-quality) classes, selected as follows. \n\n\u2022 Positive documents: \n\n-190k synthetic documents randomly sampled from the Cosmopedia dataset -an open synthetic dataset consisting of textbooks, blogposts, stories, posts and WikiHow articles generated by Mixtral-8x7B-Instruct-v0.1 [37]. -10k documents with high educational value selected as follows: we annotated 600k random documents from FineWeb-V1.1.0 asking Mixtral-8x22B-Instruct to score each document between 1 to 5 for its educational quality (with 5 being the highest quality), using a prompt similar to the one used by FineWeb-Edu. Next, we selected 10k random documents from the documents with scores \u2265 4. \n\n\u2022 Negative documents: 200k random documents out of the 600k Mixtral-annotated documents with scores \u2264 2. \n\nWe denote the DCLM-fastText as \u03d5 DCLM and our custom fastText as \u03d5 Cosmo .",
            "score": 0.5669667674869511,
            "section_title": "FastText Classifiers",
            "char_start_offset": 10794,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 134
                },
                {
                    "start": 135,
                    "end": 345
                },
                {
                    "start": 346,
                    "end": 497
                },
                {
                    "start": 500,
                    "end": 832
                },
                {
                    "start": 835,
                    "end": 1054
                },
                {
                    "start": 1057,
                    "end": 1078
                },
                {
                    "start": 1081,
                    "end": 1296
                },
                {
                    "start": 1297,
                    "end": 1415
                },
                {
                    "start": 1416,
                    "end": 1602
                },
                {
                    "start": 1603,
                    "end": 1677
                },
                {
                    "start": 1680,
                    "end": 1784
                },
                {
                    "start": 1787,
                    "end": 1861
                }
            ],
            "ref_mentions": [
                {
                    "start": 9,
                    "end": 13,
                    "matchedPaperCorpusId": "1210515"
                },
                {
                    "start": 76,
                    "end": 79,
                    "matchedPaperCorpusId": "274140947"
                },
                {
                    "start": 893,
                    "end": 897,
                    "matchedPaperCorpusId": "1210515"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.19580078125
        },
        {
            "corpus_id": "227231089",
            "title": "Go Simple and Pre-Train on Domain-Specific Corpora: On the Role of Training Data for Text Classification",
            "text": "Table 2 shows the results with different sizes of training data randomly extracted from the training set. Surprisingly, classification models based on corpus-trained embeddings achieve higher performance with less labelled data compared to the classifier based on pre-trained contextualised models. However, for cases with more than 5,000 training samples, the performance of fine-tuned BERT significantly outperforms fastText corpus-based classifier, especially when domain-trained BERT model (i.e., BERT (Twitter)) is used. Further to that, the fine-tuned model performance improves at a higher rate than the classifier based on corpus-trained embeddings for training sets with more than 2,000 instances. For instance, for the SE-18 dataset, fastText with domain embeddings improves 0.112 micro-F1 points when the entire dataset is used with respect to using only 200 instances, while BERT-Twitter provides a 0.360 absolute improvement. In contrast, fastText with pre-trained embeddings performs similarly to the baseline. This shows the advantage for pre-trained models to be fine-tuned to the given domain and task.",
            "score": 0.5641025241806263,
            "section_title": "Effect of training size",
            "char_start_offset": 9068,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 105
                },
                {
                    "start": 106,
                    "end": 298
                },
                {
                    "start": 299,
                    "end": 525
                },
                {
                    "start": 526,
                    "end": 706
                },
                {
                    "start": 707,
                    "end": 938
                },
                {
                    "start": 939,
                    "end": 1024
                },
                {
                    "start": 1025,
                    "end": 1119
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.9248046875
        },
        {
            "corpus_id": "244270194",
            "title": "Guiding Generative Language Models for Data Augmentation in Few-Shot Text Classification",
            "text": "In this final step, we use the augmented training data to train a fastText classifier (Joulin et al., 2017) coupled with domain-trained fastText word embeddings. The reason to use a simple model such as fastText is its efficiency and that transformer-based models tend to not perform well with limited data in document classification and in general tasks that do not require a fine granularity (Joshi et al., 2020). Indeed, fastText has been shown to perform equally or better with limited labeled data in document classification, compared to more sophisticated models such as BERT (Edwards et al., 2020).",
            "score": 0.5620173433512801,
            "section_title": "Text Classification",
            "char_start_offset": 8951,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 161
                },
                {
                    "start": 162,
                    "end": 415
                },
                {
                    "start": 416,
                    "end": 605
                }
            ],
            "ref_mentions": [
                {
                    "start": 86,
                    "end": 107,
                    "matchedPaperCorpusId": "1210515"
                },
                {
                    "start": 394,
                    "end": 414,
                    "matchedPaperCorpusId": "226236726"
                },
                {
                    "start": 582,
                    "end": 604,
                    "matchedPaperCorpusId": "227231089"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.55517578125
        },
        {
            "corpus_id": "274280560",
            "title": "Non-Contextual BERT or FastText? A Comparative Analysis",
            "text": "To enhance the comparison, we replicate the experiments using widely utilized contextual BERT embeddings. We also evaluate the impact of compression on both contextual and non-contextual BERT-based embeddings. Our analysis shows that non-contextual BERT embeddings generally perform better than FastText in most tasks. Furthermore, contextual BERT embeddings consistently outperform Fast-Text across all evaluated tasks. However, compressing non-contextual embeddings reduces their performance, making FastText more effective than compressed non-contextual BERT. \n\nThe key contributions of this work are as follows: \n\n\u2022 We conduct a detailed study comparing noncontextual BERT embeddings and FastText embeddings for Marathi, a low-resource language. The evaluation covers multiple classification tasks, including sentiment analysis, news classification, and hate speech detection. \n\n\u2022 To ensure a fair comparison, we compress BERT embeddings from 768 to 300 dimensions using Singular Value Decomposition (SVD). This allows us to analyze how dimensionality reduction impacts BERT's performance compared to its uncompressed version and FastText. \n\n\u2022 We explore the differences between contextual and non-contextual BERT embeddings, examining their impact on classification performance in low-resource settings. \n\nThe paper is organized as follows: Section 2 provides a concise review of prior research on Fast-Text and BERT. Section 3 includes the datasets and model embeddings that are utilized for the experiments. Section 4 presents the methodology used. Section 5 presents the results and key insights drawn from the findings along with a comparative analysis of FastText embeddings and BERT. In Section 6, we analyze our results and explain the reasons behind them. In Section 7, we conclude our discussion.",
            "score": 0.5615528190707377,
            "section_title": "Introduction",
            "char_start_offset": 3764,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 105
                },
                {
                    "start": 106,
                    "end": 209
                },
                {
                    "start": 210,
                    "end": 318
                },
                {
                    "start": 319,
                    "end": 420
                },
                {
                    "start": 421,
                    "end": 562
                },
                {
                    "start": 565,
                    "end": 615
                },
                {
                    "start": 618,
                    "end": 749
                },
                {
                    "start": 750,
                    "end": 880
                },
                {
                    "start": 883,
                    "end": 1010
                },
                {
                    "start": 1011,
                    "end": 1143
                },
                {
                    "start": 1146,
                    "end": 1308
                },
                {
                    "start": 1311,
                    "end": 1422
                },
                {
                    "start": 1423,
                    "end": 1514
                },
                {
                    "start": 1515,
                    "end": 1555
                },
                {
                    "start": 1556,
                    "end": 1694
                },
                {
                    "start": 1695,
                    "end": 1768
                },
                {
                    "start": 1769,
                    "end": 1810
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.93896484375
        },
        {
            "corpus_id": "227231089",
            "title": "Go Simple and Pre-Train on Domain-Specific Corpora: On the Role of Training Data for Text Classification",
            "text": "5 instances per label 10 instances per label FT(gen) FT(dom) BERT(gen) BERT(T) FT(gen) FT(dom) BERT(gen) BERT(T) FT(gen) FT(dom) BERT(gen) BERT(T)  . Similarly to the experiments with randomized data samples, fastText based on corpus-trained embeddings is the best performing classification model for very small amounts of balanced labeled data (see Figure 2). However, as the amount of training data increases, BERT model outperforms fastText on average by 0.0442%. As in the previous experiment, the classification model based on pre-trained embeddings perform poorly compared to the corpus-trained embeddings and models fine-tuned to the task. Further, BERT (Twitter) leads to significant improvements over BERT when only 10 instances per label are used (i.e., for SE-16, BERT (Twitter) has macro-F1 = 0.370, similar to domain-based fastText with macro-F1 = 0.384 versus base BERT with macro-F1 = 0.200).",
            "score": 0.5613743190071048,
            "section_title": "instances per label",
            "char_start_offset": 13305,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 149
                },
                {
                    "start": 150,
                    "end": 360
                },
                {
                    "start": 361,
                    "end": 466
                },
                {
                    "start": 467,
                    "end": 646
                },
                {
                    "start": 647,
                    "end": 907
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.806640625
        },
        {
            "corpus_id": "232278102",
            "title": "Analysis and Evaluation of Language Models for Word Sense Disambiguation",
            "text": "Word-specific results for different configurations of BERT and ALBERT as well as the FastText baseline are shown in Table 5. In general, results are high for all Transformerbased models, over 90% in most cases. This reinforces the potential of language models for WSD, both in its light-weight 1NN and in the fine-tuning settings. Although BERT-Large slightly improves over BERT-Base, the performance of the former is very Table 5 Micro-F1 (top) and macro-F1 (bottom) performance on the full CoarseWSD-20 data set for eight different models: FastText-Base (FTX-B) and -Crawl (FTX-C), 1NN and fine-tuned BERT-Base (BRT-B), -Large (BRT-L), and ALBERT-XXL (ALBRT). An estimation of the human performance (see Section 4.2 for more details) and the most frequent sense (MFS) baseline are also reported for each word. Rows in each table are sorted by the entropy of sense distribution (see Table 3), in descending order. similar to that of ALBERT-XXL across different configurations, despite having different architectures, number of parameters, and training objectives. Overall, performance variations in different models are similar to those for the human baseline. For instance, words such as java and digit seem easy for both humans and models to disambiguate, whereas words such as bass and club are challenging perhaps because of their more finegrained distinctions. 20 As a perhaps surprising result, having more training instances does not necessarily lead to better performance, indicated by the very low Pearson correlation (0.2 or lower) of the number of training instances with results in all BERT configurations. Also, higher polysemy is not a strong indicator of lower performance (see Table 4.3 for statistics of the 20 words, including polysemy), as one would expect from a classification task with a higher number of classes (near zero average correlation across settings). In the following we also discuss other relevant points with respect to Most Frequent Sense (MFS) bias and fine-tuning.\n\nMFS Bias. As expected, macro-F1 results degrade for the purely supervised classification models (FastText and fine-tuned BERT), indicating the inherent sense bias",
            "score": 0.558253483032436,
            "section_title": "Results",
            "char_start_offset": 52285,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.67626953125
        },
        {
            "corpus_id": "254563733",
            "title": "Domain Adaptation of Transformer-Based Models Using Unlabeled Data for Relevance and Polarity Classification of German Customer Feedback",
            "text": "In the attempt to reach high scores in the first two subtasks, different systems and models were used. For the additional pre-training and fine-tuning of transformer-based models, an NVIDIA A100 GPU was used. The pre-training time usually varied between two and three hours, whereas the fine-tuning took around 8 minutes for the base models and 20 minutes for the large ones. As a baseline, a fastText classifier [30] was trained on the preprocessed text. This classifier is constructed with the goal to predict a class or a label instead of a word, which is the case when using an unsupervised algorithm like continuous bag-of-words (CBOW) to generate word embeddings. In addition to word embeddings, fastText also uses character-level n-grams, which makes it capable of handling morphologically rich languages like German, and sentences with a variety of words. Deviating from the default configuration, the dimensionality of the word vectors was set to 50, the learning rate was initialized with 0.1. For the loss computation, softmax was used, and the number of word n-grams was set to 4. The classifier was then trained for 20 epochs. To ensure the reproducibility of the results, the number of used threads was set to 1. As an additional model, these parameters and the collected tweets were used to generate word embeddings, which then were utilized with the fastText classifier. \n\nTo further improve results, transformer-based models were used, starting with BERT [13]. This model was designed to pre-train deep bidirectional representations from an unlabeled text by jointly exploring left and right contexts on all levels. As a result, the pre-trained BERT language model can be fine-tuned with only one additional output layer to develop models that achieve remarkable results for different tasks. The authors published two models: BERT Base and BERT Large . Details of these models are shown in Table 9. In addition, they also released a multilingual model, which was trained on cased data in 104 languages. \n\nThese models were pre-trained in two phases: \"masked language modeling\", and \"next sentence prediction\".",
            "score": 0.5518246777209722,
            "section_title": "System Description",
            "char_start_offset": 18655,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 102
                },
                {
                    "start": 103,
                    "end": 208
                },
                {
                    "start": 209,
                    "end": 375
                },
                {
                    "start": 376,
                    "end": 455
                },
                {
                    "start": 456,
                    "end": 669
                },
                {
                    "start": 670,
                    "end": 863
                },
                {
                    "start": 864,
                    "end": 1003
                },
                {
                    "start": 1004,
                    "end": 1139
                },
                {
                    "start": 1140,
                    "end": 1226
                },
                {
                    "start": 1227,
                    "end": 1386
                },
                {
                    "start": 1389,
                    "end": 1477
                },
                {
                    "start": 1478,
                    "end": 1632
                },
                {
                    "start": 1633,
                    "end": 1808
                },
                {
                    "start": 1809,
                    "end": 1869
                },
                {
                    "start": 1870,
                    "end": 1915
                },
                {
                    "start": 1916,
                    "end": 2019
                },
                {
                    "start": 2022,
                    "end": 2126
                }
            ],
            "ref_mentions": [
                {
                    "start": 413,
                    "end": 417,
                    "matchedPaperCorpusId": "1210515"
                },
                {
                    "start": 1472,
                    "end": 1476,
                    "matchedPaperCorpusId": "52967399"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.6279296875
        },
        {
            "corpus_id": "265162335",
            "title": "Bridging Qualitative Data Silos: The Potential of Reusing Codings Through Machine Learning Based Cross-Study Code Linking",
            "text": "The results of all models after optimization are presented in Table 2.For fastText, we include the results with and without pre-training to emphasize the improvement of using custom pre-trained word embeddings over the baseline.Since the labels in the ZuL and Bank datasets are organized in two-level hierarchies based on topics, we also report results for predicting first-level labels (suffixed with \"Base\").\n\nOverall, BERT achieves the best performance on the F 1 measure in all datasets except for Bank.Even after an extended hyperparameter search, we were not able to improve the results for this last dataset.We believe that the poor performance for Bank may be due to a combination of a low sample count (727) and a relatively high number of labels (67), which likewise affects the other classifiers.Previous studies have also observed decreased performance in BERT fine-tuning when dealing with a small training set (Pruksachatkun et al., 2020;Zhu et al., 2020).\n\nfastText proves to be the second-best classifier and even outperforms BERT on the Bank dataset.The additional use of pre-trained word embeddings clearly benefits the task and shows an average increase in F 1 -score of 15.3% across all datasets.This suggests a further improvement when using larger domain-specific corpora.Moreover, its fast training and the lack of dependency on GPUs may be advantageous for many practical applications.\n\nIn contrast to the improvements from custom word embeddings in fastText, Linear SVC, and ML-KNN show the best results when operating on TF-IDF vectors instead.Linear SVC achieves very similar, albeit lower, performance values than fastText.This is quite interesting, since both work on an identical form of processed text, with Linear SVC not having the benefit of a custom language model.On the other hand, ML-KNN showed the worst performance on our datasets.While it exhibits high precision, it retrieves fewer labels overall.",
            "score": 0.5484618210758982,
            "section_title": "Classifier Performance",
            "char_start_offset": 23027,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 70
                },
                {
                    "start": 70,
                    "end": 228
                },
                {
                    "start": 228,
                    "end": 410
                },
                {
                    "start": 412,
                    "end": 507
                },
                {
                    "start": 507,
                    "end": 615
                },
                {
                    "start": 615,
                    "end": 807
                },
                {
                    "start": 807,
                    "end": 970
                },
                {
                    "start": 972,
                    "end": 1067
                },
                {
                    "start": 1067,
                    "end": 1216
                },
                {
                    "start": 1216,
                    "end": 1294
                },
                {
                    "start": 1294,
                    "end": 1409
                },
                {
                    "start": 1411,
                    "end": 1570
                },
                {
                    "start": 1570,
                    "end": 1651
                },
                {
                    "start": 1651,
                    "end": 1800
                },
                {
                    "start": 1800,
                    "end": 1871
                },
                {
                    "start": 1871,
                    "end": 1939
                }
            ],
            "ref_mentions": [
                {
                    "start": 924,
                    "end": 952,
                    "matchedPaperCorpusId": "220045835"
                },
                {
                    "start": 952,
                    "end": 969,
                    "matchedPaperCorpusId": "209475786"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.71484375
        },
        {
            "corpus_id": "227231751",
            "title": "ALT at SemEval-2020 Task 12: Arabic and English Offensive Language Identification in Social Media",
            "text": "Since we are not provided with a development set, we perform 5-fold cross-validation and then average the results to evaluate different individual classifiers (Table 1). Then, we create a system combination from individual classifiers to generate labels for the test set (Table 2). We report accuracy (Acc.), macro averaged precision (P), recall (R), F1-score (F1). We use macro-averaged F1-score (the official metric) for comparison in our discussion.\n\nfastText fastText (Joulin et al., 2016) is a light-weight deep-learning based system that learns word embedding representation of text and can perform text classification. Although it offers a fast and easy way to perform text classification, it can be outperformed by more sophisticated models. fastText primarily serves purpose of a baseline in our experiments. From Table 1, we can see that fastText (F1-score of 79.8%) is outperformed by all other classifiers.\n\nMultilingual-BERT Deep contextualized transformer models such as Bidirectional Encoder Representations from Transformers (Devlin et al., 2019) have been shown to be extremely effective in many NLP tasks. For our task. we fine-tune BERT base-multilingual (Mult-BERT) which is pretrained on Wikipedia text from 104 languages (including Arabic). The architecture of the model consists of 12 transformer blocks, hidden size of 768 and 12 self-attention heads. During fine-tuning, we add a softmax layer that projects class probabilities of the labels. We use PyTorch 2 implementation by HuggingFace 3 for the fine-tuning.\n\nWe can see from Table 1 that, with F1-score of 82.9%, Mult-BERT is outperformed by all classifiers except fastText. This could be attributed to the fact that wikipedia text differs significantly from informal Twitter text (domain of the shared task data).\n\nConvolutional-Long Short Term Memory (C-LSTM) We design a neural network that consists of two parts. First part of the network acts as a character-level feature extractor. The first layer is an embedding layer that projects input to character embeddings, which is then passed through a",
            "score": 0.5454733660983735,
            "section_title": "Experiments and Results",
            "char_start_offset": 5854,
            "sentence_offsets": [],
            "ref_mentions": [
                {
                    "start": 1041,
                    "end": 1062,
                    "matchedPaperCorpusId": "52967399"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.6494140625
        },
        {
            "corpus_id": "236912522",
            "title": "A Comparison of Different Source Code Representation Methods for Vulnerability Prediction in Python",
            "text": "FastText: We use the exact same process for calculating fastText [15] embeddings as for word2vec. This means, we apply the same data analysis and tokenizer scripts and train the embedding model with the same Python code corpus. We chose fastText as a study subject because word2vec only learns vectors for words that are complete in the training corpus. FastText, on the other hand, learns vectors for both the n-grams and the full words contained inside each word. Fast-Text uses the mean of the target word vector and its component n-gram vectors for training at each step. The change derived from the error is then applied uniformly to all of the vectors that were combined to form the target. This adds a significant amount of extra computation to the training step. A word must sum and average its n-gram component parts at each point. BERT: Bidirectional Encoder Representations from Transformers (BERT) is a Google-developed Transformer-based machine learning technique for natural language processing (NLP) pre-training [11]. Jacob Devlin and his Google colleagues developed and released BERT in 2018. We selected this embedding method for comparison due to its recent successes within the NLP field. As the BERT model is pre-trained on natural language texts, to adopt it to source code, we used its Microsoft's variant, called CodeBERT [12]. CodeBERT is a pre-trained BERT model for programming languages. In the context of this paper, we used BERT only as an embedding method and we feed all output tokens to an LSTM model. The biggest difference between BERT and the other two embedding methods is that the training part of the embedding model is done in advance using a huge corpus in case of BERT, while for word2vec and fastText, we need to do the training locally. This means that CodeBERT can be used out of the box, without having to train a model for token embeddings.",
            "score": 0.5429055436771162,
            "section_title": "Fig. 3. Transforming code into vectors",
            "char_start_offset": 14449,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 97
                },
                {
                    "start": 98,
                    "end": 227
                },
                {
                    "start": 228,
                    "end": 353
                },
                {
                    "start": 354,
                    "end": 465
                },
                {
                    "start": 466,
                    "end": 575
                },
                {
                    "start": 576,
                    "end": 696
                },
                {
                    "start": 697,
                    "end": 770
                },
                {
                    "start": 771,
                    "end": 840
                },
                {
                    "start": 841,
                    "end": 1033
                },
                {
                    "start": 1034,
                    "end": 1109
                },
                {
                    "start": 1110,
                    "end": 1208
                },
                {
                    "start": 1209,
                    "end": 1351
                },
                {
                    "start": 1352,
                    "end": 1415
                },
                {
                    "start": 1416,
                    "end": 1534
                },
                {
                    "start": 1535,
                    "end": 1780
                },
                {
                    "start": 1781,
                    "end": 1887
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.76953125
        },
        {
            "corpus_id": "258797890",
            "title": "Data-Augmentation for Bangla-English Code-Mixed Sentiment Analysis: Enhancing Cross Linguistic Contextual Understanding",
            "text": "This conclusion is also confirmed by XGBoost's performance, which outperforms the rest of the algorithms in Table 5. We also found that, in the case of Word2vec and Fasttext word embeddings, LR and SVM models perform poorly on classes with a limited sample size. However, cm-FastText shines for LR and SVM models in that case, since training on our augmented data can increase the distribution of instances by utilizing different sampling 51666 VOLUME 11, 2023 Authorized licensed use limited to the terms of the applicable license agreement with IEEE. Restrictions apply. rates. Thirdly, in DL-based approaches, 1DConv-LSTM and base-Bert perform relatively well across all five sentiment classes. Among the BERT models, although, m-BERT is trained on 106 distinct languages [18], it lacks samples of transliterated Bengali text. In this work, we implemented these BERT-based models while only fine-tuning the parameters of the pre-trained BERT models with our proposed codemixed dataset. As a result, with smaller training parameters (110 millions), the BERT Base model [77], [78] outperforms the m-BERT that has a greater number of learnable parameters (178 million), implying that m-BERT requires more training data to train. This observation is reinforced by the 1DConv-LSTM model, where it outperforms all Bert-based architectures since it is a smaller architecture with the fewest number of learnable parameters as compared to Bert models. However, Distill-BERT [17] is the smallest architecture among the Bert models (40% smaller, 60% faster), but it still underperforms Bert-base architecture since it is a distill approximation of Bert architecture, where the performance is upper-bound by the actual Bert-base model. Finally, when we compare each algorithm with respect to word embeddings, we notice that Fasttext combined with our proposed data augmentation method (cm-FastText) outperforms the baseline models for the majority of classes. The reason for this is that word2vec embedding learns to capture a word's semantic information based on its context.",
            "score": 0.5352725944362093,
            "section_title": "D. OVERALL RESULTS",
            "char_start_offset": 37853,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 116
                },
                {
                    "start": 117,
                    "end": 262
                },
                {
                    "start": 263,
                    "end": 552
                },
                {
                    "start": 553,
                    "end": 572
                },
                {
                    "start": 573,
                    "end": 579
                },
                {
                    "start": 580,
                    "end": 697
                },
                {
                    "start": 698,
                    "end": 829
                },
                {
                    "start": 830,
                    "end": 988
                },
                {
                    "start": 989,
                    "end": 1228
                },
                {
                    "start": 1229,
                    "end": 1445
                },
                {
                    "start": 1446,
                    "end": 1726
                },
                {
                    "start": 1727,
                    "end": 1950
                },
                {
                    "start": 1951,
                    "end": 2067
                }
            ],
            "ref_mentions": [
                {
                    "start": 1071,
                    "end": 1075,
                    "matchedPaperCorpusId": "3518190"
                },
                {
                    "start": 1077,
                    "end": 1081,
                    "matchedPaperCorpusId": "222291680"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.5
        },
        {
            "corpus_id": "258309541",
            "title": "Pre-trained Embeddings for Entity Resolution: An Experimental Analysis",
            "text": "Word2Vec and Glove are the fastest models by far, exhibiting the lowest processing runtimes in practically all cases. Word2Vec is an order of magnitude faster than the third most efficient model per dataset, which interchangeably corresponds to FastText and S-MiniLM. Except for  1 , GloVe outperforms these two models by at least 6 times. In absolute terms, both Word2Vec and GloVe process the eight smallest datasets,  1 - 8 , in much less than 1 second, while requiring less than 2.5 seconds for the two larger ones. \n\nAmong the BERT-based models, DistilBERT is significantly faster than BERT, as expected, with its processing time being lower by 33%, on average. Note, though, that it is slower than FastText by >50%, on average, except for  3 . The next most efficient models of this category are ALBERT and RoBERTa, which outperform BERT by 11% and 13%, on average, respectively. XLNet is the most time consuming BERT-based model, being slower than BERT by \u223c30%, on average across all datasets but  3 . This can be explained by the fact that  3 has larger sentences, as shown in Table 2(a). \n\nAmong the SentenceBERT models, the lowest time is achieved by S-MiniLM, which, as mentioned above, is the third fastest model together with FastText. The second best model in this category is S-DistilRoBERTa, which is slower by 30%, on average. Both models are faster than BERT by 63% and 53%, respectively, on average. In contrast, the quite complex learned model of S-GTR-T5 yields the highest processing time among all models in all datasets but the smallest one. S-MPNet lies between these two extremes. Summary. The static models excel in processing time, but suffer from very high initialization time. More notably, FastText is the slowest model in all datasets, but  9 , where S-GTR-T5 exhibits a much higher processing time. This means that FastText's high initialization cost does not pay off in datasets with few thousand entities like those in Table 2(a). On average, FastText requires 2 minutes per dataset.",
            "score": 0.5297413022717982,
            "section_title": "Supervised Matching",
            "char_start_offset": 51772,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 117
                },
                {
                    "start": 118,
                    "end": 267
                },
                {
                    "start": 268,
                    "end": 339
                },
                {
                    "start": 340,
                    "end": 519
                },
                {
                    "start": 522,
                    "end": 666
                },
                {
                    "start": 667,
                    "end": 749
                },
                {
                    "start": 750,
                    "end": 885
                },
                {
                    "start": 886,
                    "end": 1008
                },
                {
                    "start": 1009,
                    "end": 1096
                },
                {
                    "start": 1099,
                    "end": 1248
                },
                {
                    "start": 1249,
                    "end": 1343
                },
                {
                    "start": 1344,
                    "end": 1418
                },
                {
                    "start": 1419,
                    "end": 1565
                },
                {
                    "start": 1566,
                    "end": 1606
                },
                {
                    "start": 1607,
                    "end": 1615
                },
                {
                    "start": 1616,
                    "end": 1706
                },
                {
                    "start": 1707,
                    "end": 1831
                },
                {
                    "start": 1832,
                    "end": 1965
                },
                {
                    "start": 1966,
                    "end": 2018
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.364990234375
        },
        {
            "corpus_id": "257806425",
            "title": "A Stacking-based Efficient Method for Toxic Language Detection on Live Streaming Chat",
            "text": "Table 2 shows classification results of our method obtained with the threshold optimized in terms of Macro F1 score. Our method achieved 0.942 in terms of average of Macro F1 across the two classes. Table 3 shows results and the effectiveness of each component. Looking at classifier. In Table 3, our ablation study showed that our five features contributed to enhancing F1 score, especially black words and fastText prediction. Thus, we believe that each of our proposed features was independently effective for toxic language detection tasks, and that the improvement was even greater when used together. \n\nTable 4 shows the comparison between our method and BERT-based methods on various configurations. The average inference time for our method was 22.9 seconds for 16,807 test samples, with a standard deviation of 1.18. In the Table , we put QPS score (i.e., Throughput), the number of comments which can be processed per a second. Looking at the table, our method achieved much faster inference speed (734QPS\u00b141) than any other BERT configurations. We found that the differences in inference speed between our method and BERT models were statistically significant at the 1% level, irrespective of the configurations by the two-tailed paired t-test for statistical testing. \n\nBERT-large and ALBERT yielded slightly better performance than our method when the maximum sequence length was 64 (F1 = 0.948) and 32 (F1 = 0.943), respectively. However, these models sacrificed inference speed. For instance, the inference speed (47QPS\u00b16) of BERT-large is 1.7 times slower than BERT-base (82QPS\u00b14) and 15.6 times much slower than our method. On the other hand, if attaching great importance to inference speed,  Distil-BERT achieved the highest inference speed (145QPS\u00b16) of all the BERT-based settings when the maximum sequence length was 4, but F1-score dramatically went down to 0.773. It seems that Poor-Man's BERT and Distil-BERT had harmonized results of BERT settings when the maximum length was 32 or 64. However, all of the values did not reach those for our method.",
            "score": 0.5286585424260466,
            "section_title": "Results",
            "char_start_offset": 13764,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 116
                },
                {
                    "start": 117,
                    "end": 198
                },
                {
                    "start": 199,
                    "end": 261
                },
                {
                    "start": 262,
                    "end": 284
                },
                {
                    "start": 285,
                    "end": 428
                },
                {
                    "start": 429,
                    "end": 606
                },
                {
                    "start": 609,
                    "end": 706
                },
                {
                    "start": 707,
                    "end": 825
                },
                {
                    "start": 826,
                    "end": 937
                },
                {
                    "start": 938,
                    "end": 1055
                },
                {
                    "start": 1056,
                    "end": 1279
                },
                {
                    "start": 1282,
                    "end": 1443
                },
                {
                    "start": 1444,
                    "end": 1493
                },
                {
                    "start": 1494,
                    "end": 1640
                },
                {
                    "start": 1641,
                    "end": 1887
                },
                {
                    "start": 1888,
                    "end": 2011
                },
                {
                    "start": 2012,
                    "end": 2074
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.35595703125
        },
        {
            "corpus_id": "270514462",
            "title": "Investigating Web Corpus Filtering Methods for Language Model Development in Japanese",
            "text": "The evaluation results on WCQEB are shown in Table 2.In the classifier, a document is high quality if its predicted probability exceeds a threshold, while in the language model, it is if its perplexity is below a threshold.\n\nFor the classifier-based methods, ROC-AUC exceeds 0.7 for fastText and the multilayer perceptron (MLP).In particular, fastText scores higher than the MLP for recall and detection-power and has better filtering ability.Furthermore, the performance of the 3 and more-gram language models is higher in detection-power than that of fastText by 17.6 points, even though the recall is lower than fastText by only 3 points, which can remove more low-quality documents.The Transformer-based language model has a low detection-power of 0.165 and cannot remove even 20% of low-quality documents.This is lower than the classification performance of all other methods.\n\nTable 2 also shows the classification speed of each method.FastText has the fastest inference speed at 2.41 seconds, followed by perceptron, MLP, and 3-gram language models at approximately 20 seconds.It should be noted that although the Transformer model is relatively slow, taking 29.13 seconds, it can be significantly accelerated through the use of GPUs.With the 3-gram language model, it is calculated to take approximately 5 hours and 30 minutes to process 10 million documents, which can be made even faster by parallelization.\n\nIn sum, the filtering method based on the perplexity of the N-gram language model is the best and has high classification ability even with 3-grams.Example benchmark documents and their perplexity of the 3-gram language model are shown in Figure 1.\n\nThe document above in Figure 1 contains many alphabets, numbers, symbols, etc., and has a high perplexity of 722,324.09.The document below is out of context but has a low perplexity of 184.16.\n\nWe also observe that, in most cases, documents that are seemingly written in fluent and meaningful Japanese have mediocre perplexity, ranging widely between these high and low extremes.",
            "score": 0.52782253882424,
            "section_title": "Experimental Results",
            "char_start_offset": 8549,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 53
                },
                {
                    "start": 53,
                    "end": 223
                },
                {
                    "start": 225,
                    "end": 328
                },
                {
                    "start": 328,
                    "end": 443
                },
                {
                    "start": 443,
                    "end": 686
                },
                {
                    "start": 686,
                    "end": 810
                },
                {
                    "start": 810,
                    "end": 881
                },
                {
                    "start": 883,
                    "end": 942
                },
                {
                    "start": 942,
                    "end": 1084
                },
                {
                    "start": 1084,
                    "end": 1241
                },
                {
                    "start": 1241,
                    "end": 1417
                },
                {
                    "start": 1419,
                    "end": 1567
                },
                {
                    "start": 1567,
                    "end": 1667
                },
                {
                    "start": 1669,
                    "end": 1789
                },
                {
                    "start": 1789,
                    "end": 1861
                },
                {
                    "start": 1863,
                    "end": 2048
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.37158203125
        },
        {
            "corpus_id": "274422859",
            "title": "ChineseWebText 2.0: Large-Scale High-quality Chinese Web Text with Multi-dimensional and fine-grained information",
            "text": "Text Filtering. During the construction of pre-training datasets, most raw data from the web contains various forms of noise, such as violence, pornography, advertisements, corrupted characters, and poorly formatted text. To extract high-quality data from these raw contents, numerous text filtering methods have been proposed to automatically remove undesirable elements. Among these methods, handcrafted rules [9,17] are commonly used to filter out explicit noise, followed by deduplication techniques [18] to eliminate duplicate text from different sources. Building on this prior work, this paper introduces a rule-based preprocessing module designed to extract high-quality text from raw web content. \n\nQuality Evaluation. In addition to rule-based methods, quality evaluation approaches are also employed to identify high-quality texts using well-designed classifiers. Unlike rule-based methods, which primarily filter out explicit noise from raw content, quality evaluation approaches offer greater robustness and flexibility. These approaches leverage models such as logistic regression [19], BERT [20], FastText [21], and others to calculate probability scores for each text. Based on these scores, texts are classified as positive or negative according to a predefined threshold. Among these models, BERT has emerged as one of the most widely used architectures for quality evaluation due to its exceptional performance in text classification and understanding tasks. BERT's effectiveness stems from its pre-training objectives, including masked language modeling and next-sentence prediction, which enable it to learn powerful text representation and comprehension capabilities. In this paper, we adopt the BERT architecture to build a robust quality evaluation model for text. Domain Classification. The Pre-training of LLMs on vast amounts of diverse text data often results in a generalized understanding of language. However, to enhance their effectiveness for specific applications, it is advantageous to ChineseWebText 2.0: Large-Scale High-quality Chinese Web Text with Multi-dimensional and Fine-grained Information classify and curate data from various domains, such as medicine, law, and finance. This process, known as domain classification, helps refine the model to better capture domain-specific language patterns, terminology, and contextual nuances.",
            "score": 0.5263978252780579,
            "section_title": "Related Work",
            "char_start_offset": 1704,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 15
                },
                {
                    "start": 16,
                    "end": 221
                },
                {
                    "start": 222,
                    "end": 372
                },
                {
                    "start": 373,
                    "end": 560
                },
                {
                    "start": 561,
                    "end": 705
                },
                {
                    "start": 708,
                    "end": 727
                },
                {
                    "start": 728,
                    "end": 874
                },
                {
                    "start": 875,
                    "end": 1033
                },
                {
                    "start": 1034,
                    "end": 1184
                },
                {
                    "start": 1185,
                    "end": 1289
                },
                {
                    "start": 1290,
                    "end": 1477
                },
                {
                    "start": 1478,
                    "end": 1689
                },
                {
                    "start": 1690,
                    "end": 1788
                },
                {
                    "start": 1789,
                    "end": 1811
                },
                {
                    "start": 1812,
                    "end": 1931
                },
                {
                    "start": 1932,
                    "end": 2217
                },
                {
                    "start": 2218,
                    "end": 2376
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.48876953125
        },
        {
            "corpus_id": "232278102",
            "title": "Analysis and Evaluation of Language Models for Word Sense Disambiguation",
            "text": "CoarseWSD-20 consists of 20 separate sets, each containing sentences for different senses of the corresponding target word. Therefore, the evaluation can be framed as a standard classification task for each word.\n\nGiven the classification nature of the CoarseWSD-20 data sets, we can perform experiments with our 1NN BERT system and compare it with a standard fine-tuned BERT model (see Section 3.3 for more details on the LM-based WSD approaches). Note that fine-tuning for individual target words results in many models (one per word). Therefore, this setup would not be computationally feasible in a general WSD setting, as the number of models would approach the vocabulary size. However, in our experiments we are interested in verifying the limits of BERT, without any other confounds or model-specific restrictions.\n\nTo ensure that our conclusions are generalizable, we also report 1NN and finetuning results using ALBERT. In spite of substantial operational differences, BERT and ALBERT have the most similar training objectives and tokenization methods out of several other prominent Transformer-based models (Yang et al. 2019;Liu et al. 2019b), thus being the most directly comparable. Given the similar performance between BERT-Large and ALBERT-XXLarge on the main CoarseWSD-20 data set, we proceed with further experiments using only BERT.\n\nWe also include two FastText linear classifiers (Joulin et al. 2017) as baselines: FTX-B (base model without pretrained embeddings) and FTX-C (using pretrained embeddings from Common Crawl). We chose FastText as the baseline given its efficiency and competitive results for sentence classification.\n\nConfiguration. Our experiments with BERT and ALBERT used the Transformers framework (v2.5.1) developed by Wolf et al. (2020), and we used the uncased pretrained base and large models released by Devlin et al. (2019) for BERT, and the xxlarge (v2) models released by Lan et al. (2020) for ALBERT. We use the uncased variants of Transformers models to match the casing in CoarseWSD-20 (except for ALBERT, which is only available in cased variants). Following previous feature extraction works (including our experiment in Section 3.4.",
            "score": 0.5255120166422131,
            "section_title": "Experimental Setting",
            "char_start_offset": 46842,
            "sentence_offsets": [],
            "ref_mentions": [
                {
                    "start": 1118,
                    "end": 1136,
                    "matchedPaperCorpusId": "195069387"
                },
                {
                    "start": 1401,
                    "end": 1420,
                    "matchedPaperCorpusId": "1210515"
                },
                {
                    "start": 1759,
                    "end": 1777,
                    "matchedPaperCorpusId": "182952883"
                },
                {
                    "start": 1848,
                    "end": 1868,
                    "matchedPaperCorpusId": "52967399"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.6533203125
        },
        {
            "corpus_id": "232278102",
            "title": "Analysis and Evaluation of Language Models for Word Sense Disambiguation",
            "text": "Most Frequent Class 50.0 Upperbound\n\nHuman performance 80.0 task consists of deciding whether the two target words in context refer to the same sense or not. Even though no sense inventory is explicitly given, this data set was also constructed based on WordNet. Table 12 shows a few examples from the data set.\n\nBERT-Based Model. Given that the task in WiC is a binary classification, the 1NN model is not applicable because a training to learn sense margins is necessary. Therefore, we experimented with the BERT model fine-tuned on WiC's training data. We followed Wang et al. (2019) and fused the two sentences and fed them as input to BERT. A classifier was then trained on the concatenation of the resulting BERT contextual embeddings.\n\nBaselines. In addition to our BERT-based model, we include results for two Fast-Text supervised classifiers (Joulin et al. 2017) as baselines: a basic one with random initialization (FastText-B) and another initialized with FastText embeddings trained on the Common Crawl (FastText-C). As other indicative reference points, we added two language models that are enriched with WordNet (Levine et al. 2020;Loureiro and Jorge 2019b) and another with WordNet and Wikipedia .\n\nResults. Table 13 shows the result of BERT models and the other baselines on the WiC benchmark. 24 We can see that BERT significantly outperforms the FastText static word embedding. The two versions of BERT (Base and Large) perform equally well on this task, achieving results close to the state of the art. As with fine-grained all-words WSD, the additional knowledge drawn from WordNet proves to be beneficial, as shown by the results for KnowBERT and SenseBERT. Table 17 shows for each sense their ID (as per their Wikipedia page title), definition, and example usage from the data set.",
            "score": 0.5249271226648802,
            "section_title": "Lowerbound",
            "char_start_offset": 86334,
            "sentence_offsets": [],
            "ref_mentions": [
                {
                    "start": 568,
                    "end": 586,
                    "matchedPaperCorpusId": "143424870"
                },
                {
                    "start": 851,
                    "end": 870,
                    "matchedPaperCorpusId": "1210515"
                },
                {
                    "start": 1127,
                    "end": 1147,
                    "matchedPaperCorpusId": "199668663"
                },
                {
                    "start": 1147,
                    "end": 1172,
                    "matchedPaperCorpusId": "195345080"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.66943359375
        },
        {
            "corpus_id": "221319787",
            "title": "Language Models and Word Sense Disambiguation: An Overview and Analysis",
            "text": "CoarseWSD-20 consists of 20 separate sets, each containing sentences for different senses of the corresponding target word. Therefore, the evaluation can be framed as a standard classification task for each word. \n\nGiven the classification nature of the CoarseWSD-20 datasets, we can perform experiments with our 1NN BERT system and compare it with a standard fine-tuned BERT model (see Section 3.3 for more details on the LM-based WSD approaches). Note that fine-tuning for individual target words results in many models (one per word). Therefore, this setup would not be computationally feasible in a general WSD setting, as the number of models would approach the vocabulary size. However, in our experiments we are interested in verifying the limits of BERT, without any other confounds or modelspecific restrictions. We also include two FastText linear classifiers (Joulin et al. 2017) as baselines: FTX-B (base model without pre-trained embeddings) and FTX-C (using pre-trained embeddings from Common Crawl). We chose FastText as baseline given its efficiency and competitive results for sentence classification.",
            "score": 0.5232972853013131,
            "section_title": "Experimental setting",
            "char_start_offset": 39186,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 123
                },
                {
                    "start": 124,
                    "end": 212
                },
                {
                    "start": 215,
                    "end": 448
                },
                {
                    "start": 449,
                    "end": 537
                },
                {
                    "start": 538,
                    "end": 683
                },
                {
                    "start": 684,
                    "end": 821
                },
                {
                    "start": 822,
                    "end": 1014
                },
                {
                    "start": 1015,
                    "end": 1118
                }
            ],
            "ref_mentions": [
                {
                    "start": 870,
                    "end": 890,
                    "matchedPaperCorpusId": "195477534"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.68994140625
        },
        {
            "corpus_id": "231607591",
            "title": "Fast Sentiment Analysis Algorithm Based on Double Model Fusion",
            "text": "al Encoder Representations from Transformers (BERT) [17], XLNet [18] and ERNIE [19]. Among them, BERT is a milestone model, which is an attention-based Transformer model and auto-encoding model. BERT was trained on a large-scale unlabeled data to obtain a general word vector representation, which results demonstrated that this learning strategy can achieve state-of-the-art in tasks such as text classification and translation. Based on this work, Manish et al. [20] implemented fine-grained sentiment classification based on BERT. However, an ocean of parameters in BERT and BERT-like models place restrictions on practical applications.\n\nMost of the current researches tend to exploit the sentiment relationship between vocabulary, and do not make full use of the semantic relationship in context. Moreover, when convert the text into a word vector, a time-consuming word vector algorithm is required, which has poor time efficiency. Towards this end, we introduce a new fast sentiment classification method called FAST-BiLSTM. First of all, FastText [21] was trained to get the basic classification model, which can generate pre-trained word vectors as a by-product. Secondly, Bi-LSTM utilizes the generated word vectors for training and then merges with FastText to make comprehensive sentiment analysis. The fusion model can further exploit the semantic information in the context and achieve higher time efficiency than traditional models.",
            "score": 0.5224443170520141,
            "section_title": "Introduction",
            "char_start_offset": 4551,
            "sentence_offsets": [],
            "ref_mentions": [
                {
                    "start": 52,
                    "end": 56,
                    "matchedPaperCorpusId": "52967399"
                },
                {
                    "start": 64,
                    "end": 68,
                    "matchedPaperCorpusId": "195069387"
                },
                {
                    "start": 79,
                    "end": 83,
                    "matchedPaperCorpusId": "158046772"
                },
                {
                    "start": 464,
                    "end": 468,
                    "matchedPaperCorpusId": "203905846"
                },
                {
                    "start": 1055,
                    "end": 1059,
                    "matchedPaperCorpusId": "1210515"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.57958984375
        },
        {
            "corpus_id": "219558245",
            "title": "Embed2Detect: temporally clustered embedded words for event detection in social media",
            "text": "According to the obtained results, classic word embedding models (e.g. Skip-gram and fastText) learn the representations faster than transformer-based models (e.g. BERT and DistilBERT). \n\nComparing fastText and Skip-gram, fastText took more time because it processes subword information. But, incorporation of subwords allows this model to capture connections between modified words. For example, consider the goal-related words found within the top 20 words with high cluster change during a goal score: Skip-gramgoal, goalll, rashyyy, scores fastTextgoalll, goooaaalll, rashford, rashyyy, @marcusrashford, scored, scores fastText captures more modified words than Skip-gram. We could not run a complete evaluation using fastText embeddings, because it requires a manual process since GT keywords only contain the words in actual form. \n\nTransformer-based models took more time than both Skip-gram and fastText due to their complex architecture to learn contextualised word embeddings. DistilBERT is found to be faster than BERT, however, the learning time of DistilBERT is not fast enough for real-time processing because it exceeds the tweet generation time. For example to learn from tweets posted during a 2-minute time window, it took approximately 7.2 minutes. If this model can be further distilled, there is a possibility to achieve the required efficiency to become suitable for real-time processing. However, further distillation can reduce the language understanding capability of the model as there is a 3% reduction in DistilBERT compared to BERT (Sanh et al. 2019). \n\nAccording to recent literature, transformer-based models performed well on many NLPrelated tasks, because of the ability to capture the contextual sense of words. BERT is capable of generating different embeddings for the same word depending on its surrounding context. In other words, the main idea behind BERT is capturing spacial changes of words. From the perspective of processing formally written natural language, this is a very useful feature. But, in social media, language is mostly informal and for event detection using social media text, temporal changes of words need to be more focused.",
            "score": 0.519166803175414,
            "section_title": "Extension to other word embedding models",
            "char_start_offset": 80185,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 70
                },
                {
                    "start": 71,
                    "end": 163
                },
                {
                    "start": 164,
                    "end": 185
                },
                {
                    "start": 188,
                    "end": 287
                },
                {
                    "start": 288,
                    "end": 383
                },
                {
                    "start": 384,
                    "end": 676
                },
                {
                    "start": 677,
                    "end": 836
                },
                {
                    "start": 839,
                    "end": 986
                },
                {
                    "start": 987,
                    "end": 1161
                },
                {
                    "start": 1162,
                    "end": 1267
                },
                {
                    "start": 1268,
                    "end": 1410
                },
                {
                    "start": 1411,
                    "end": 1580
                },
                {
                    "start": 1583,
                    "end": 1745
                },
                {
                    "start": 1746,
                    "end": 1852
                },
                {
                    "start": 1853,
                    "end": 1933
                },
                {
                    "start": 1934,
                    "end": 2034
                },
                {
                    "start": 2035,
                    "end": 2184
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.7333984375
        },
        {
            "corpus_id": "227231089",
            "title": "Go Simple and Pre-Train on Domain-Specific Corpora: On the Role of Training Data for Text Classification",
            "text": "Datasets. For our experiments we selected a suite of datasets with different domains and nature. These are: SemEval 2016 task on sentiment analysis (Nakov et al., 2019), SemEval 2018 task on emoji predic-tion (Barbieri et al., 2018), AG News (Zhang et al., 2015), Newsgroups (Lang, 1995) and IMDB (Maas et al., 2011). The main features and statistics of each dataset are summarized in Table 1 Comparison models. As mentioned in Section 2, our evaluation is focused on fastText (Joulin et al., 2017, FT) and BERT (Devlin et al., 2019). For completeness we include a simple baseline based on frequency-based features and a suite of classification algorithms available in the Scikit-Learn library (Pedregosa et al., 2011), namely Gaussian Naive Bayes (GNB), Logistic Regression and Support Vector Machines (SVM). Of the three, the best results were achieved using Logistic Regression, which is the model we include in this paper as a baseline for our experiments. \n\nTraining. As pre-trained word embeddings we downloaded 300-dimensional fastText embeddings trained on Common Crawl (Bojanowski et al., 2017). In order to learn domain-specific word embedding models we used the corresponding training sets for each dataset, except for the Twitter datasets where we leveraged an existing collection of unlabeled tweets from October 2015 to July 2018 to train 300-dimensional fastText embeddings (Camacho Collados et al., 2020). Word embeddings are then fed as input to a fastText classifier where we used default parameters and softmax as the loss function. As for BERT, we fine-tune it for the classification task using a sequence classifier, a learning rate of 2e-5 and 4 epochs. In particular, we made use of BERT's Hugging Face default transformers implementation for classifying sentences (Wolf et al., 2019) and the hierarchical principles described in Pappagari et al. (2019) for pre-processing long texts before feeding them to BERT.",
            "score": 0.5180798294610637,
            "section_title": "Experimental Setting",
            "char_start_offset": 6392,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 9
                },
                {
                    "start": 10,
                    "end": 96
                },
                {
                    "start": 97,
                    "end": 317
                },
                {
                    "start": 318,
                    "end": 411
                },
                {
                    "start": 412,
                    "end": 534
                },
                {
                    "start": 535,
                    "end": 809
                },
                {
                    "start": 810,
                    "end": 960
                },
                {
                    "start": 963,
                    "end": 972
                },
                {
                    "start": 973,
                    "end": 1104
                },
                {
                    "start": 1105,
                    "end": 1421
                },
                {
                    "start": 1422,
                    "end": 1551
                },
                {
                    "start": 1552,
                    "end": 1675
                },
                {
                    "start": 1676,
                    "end": 1935
                }
            ],
            "ref_mentions": [
                {
                    "start": 209,
                    "end": 232,
                    "matchedPaperCorpusId": "44145836"
                },
                {
                    "start": 242,
                    "end": 262,
                    "matchedPaperCorpusId": "368182"
                },
                {
                    "start": 275,
                    "end": 287,
                    "matchedPaperCorpusId": "1921714"
                },
                {
                    "start": 297,
                    "end": 316,
                    "matchedPaperCorpusId": "1428702"
                },
                {
                    "start": 512,
                    "end": 533,
                    "matchedPaperCorpusId": "52967399"
                },
                {
                    "start": 694,
                    "end": 718,
                    "matchedPaperCorpusId": "10659969"
                },
                {
                    "start": 1078,
                    "end": 1103,
                    "matchedPaperCorpusId": "207556454"
                },
                {
                    "start": 1389,
                    "end": 1420,
                    "matchedPaperCorpusId": "213824149"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.6728515625
        },
        {
            "corpus_id": "252617788",
            "title": "Comparison of Deep Learning Approaches for Lithuanian Sentiment Analysis",
            "text": "It is important to notice that all applied approaches represent very different research directions and therefore are interesting from the research perspective. FastText + CNN is a completely monolingual approach, whereas the other two approaches use multilingual models for text vectorization. FastText + CNN and BERT-w + CNN both perform word-level vectorization, whereas BERT-s + FFNN -the sentence level vectorization. FastText + CNN differs from BERT-w + CNN and BERT-s + FFNN as it does not use transformer models. FastText + CNN and BERT-w + CNN approach both use CNN as the classifier and therefore are representatives of the traditional DL; whereas BERT-s + FFNN is the representative of the innovative purely transformer-based DL technique.",
            "score": 0.5172793712307043,
            "section_title": "Summary",
            "char_start_offset": 18136,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 159
                },
                {
                    "start": 160,
                    "end": 293
                },
                {
                    "start": 294,
                    "end": 421
                },
                {
                    "start": 422,
                    "end": 519
                },
                {
                    "start": 520,
                    "end": 749
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.438232421875
        },
        {
            "corpus_id": "226283907",
            "title": "Using BERT for Qualitative Content Analysis in Psychosocial Online Counseling",
            "text": "BERT is a multi-layer bidirectional Transformer encoder based on the original Transformer implementation described in Vaswani et al. (2017). BERT is typically pre-trained on two unsupervised learning tasks. After the pre-training, the model can be fine-tuned according to the downstream task (Vaswani et al., 2017).\n\nFor the classification task in our approach, we used the BertForSequenceClassification implementation from the Hugging Face's Transformers library  that combines the BERT Transformer model with a sequence classification head on top (Hugging Face, 2020).\n\nIn total, we tested thirteen pre-trained BERT models. Among the ten tested German language models, the results varied between a weighted F1 score of 69.3% and 74.4% on the test data set, whereby the best result was achieved with the pre-  All of the following analyses are, therefore, based on the best performing DBMDZ BERT model.\n\nThe hyperparameters used for the fine-tuning were taken from the original BERT publication (Devlin e.a., 2018). Since we are using text sequences with a length of 256 WordPiece tokens, a batch size value of no more than 16 was possible due to GPU memory limitations. Larger models, especially multi-lingual models, even only allowed a batch size of 8. Further testing has shown that the best results can be achieved with a learning rate of 2e-5 and 4 epochs. Table 2 shows the different evaluation metrics for both, the SVM and the best BERT classifier.",
            "score": 0.5152353310728738,
            "section_title": "BERT as Advanced Classifier",
            "char_start_offset": 13965,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.1436767578125
        },
        {
            "corpus_id": "274982292",
            "title": "Research on Violent Text Detection System Based on BERT-fasttext Model",
            "text": "This study proposed a text detection method based on the fusion of BERT and fasttext models, explored the detection ability of violent text, and aimed to combine the text understanding ability of BERT with the efficiency of fasttext to improve the accuracy and robustness of text detection. Experimental results show that this model successfully achieved the best text detection effect on standard datasets. Specifically, the BERT model provides accurate text representation for sentiment analysis through its powerful language understanding ability, while using FastText for fast text classification has advantages in computational efficiency and reasoning speed. The contribution of this study is to combine BERT with fasttext models for violent text detection tasks, which provides new ideas for research and application in the field of text detection, and injects new vitality into the field of violent text. Future work can further explore more complex deep learning technologies and their application effects in practical scenarios.",
            "score": 0.5149005580769268,
            "section_title": "Conclusion",
            "char_start_offset": 14639,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 290
                },
                {
                    "start": 291,
                    "end": 407
                },
                {
                    "start": 408,
                    "end": 664
                },
                {
                    "start": 665,
                    "end": 912
                },
                {
                    "start": 913,
                    "end": 1038
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.74560546875
        },
        {
            "corpus_id": "267407058",
            "title": "Survey of transformers and towards ensemble learning using transformers for natural language processing",
            "text": "The traditional machine learning NLP method uses the TF-IDF algorithm to train the model. The article compares and analyzes the text analysis experiments of the four methods. In all these classifications, we use two different classifiers: BERT and traditional classifiers. \n\nBaruah et al. [23] use classifiers based on BERT, RoBERTa, and SVM to detect aggressiveness in English, Hindi, and Bengali texts. Their SVM classifier performed very well on the test set, with 3 out of 6 tests ranking second in the official results and fourth in the other. However, through more careful analysis, it can be seen that the SVM classifier performs better because the SVM model has a better classification effect. It is found that the BERT-based classifier can better predict minority groups. It was also discovered that their classifier did not correctly handle spelling errors and deliberate spelling errors. FastText word embedding works better when dealing with orthographic changes. \n\nLee et al. [24] trained the Korean version of the BERT model, KR-BERT, by using a small corpus. Due to the particularity of Korean and the lack of corpus, it is also very important to use the BERT model for language representation. For this reason, they compared different tokenizers and gradually narrowed the minimum range of tokens to build a better vocabulary for their model. After these modifications, the KR-BERT model they proposed can achieve better results even with a small corpus. \n\nIn this paper [25], Li et al. compared the BERT and XLNet models, especially from the comparison of the computational characteristics of the two. Through comparison, they found two points. The first is that the two models have similar computational characteristics. The second is that the XLNet model has a relative position encoding function. On modern CPUs, they have 1.2 times the arithmetic operation and 1.5 times the execution time. At this cost, a better benchmark score was obtained. \n\nAs multiple geographic locations are involved, the data is inherently multilingual, leading to frequent code-mixing. Sentiment analysis of the code-mixed text can provide insights into popular trends in different regions, but it is a challenge due to the nontrivial nature of inferring the semantics of such data.",
            "score": 0.5148310920386396,
            "section_title": "Review of related works",
            "char_start_offset": 37907,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 89
                },
                {
                    "start": 90,
                    "end": 174
                },
                {
                    "start": 175,
                    "end": 272
                },
                {
                    "start": 275,
                    "end": 404
                },
                {
                    "start": 405,
                    "end": 548
                },
                {
                    "start": 549,
                    "end": 701
                },
                {
                    "start": 702,
                    "end": 780
                },
                {
                    "start": 781,
                    "end": 898
                },
                {
                    "start": 899,
                    "end": 975
                },
                {
                    "start": 978,
                    "end": 1073
                },
                {
                    "start": 1074,
                    "end": 1209
                },
                {
                    "start": 1210,
                    "end": 1358
                },
                {
                    "start": 1359,
                    "end": 1470
                },
                {
                    "start": 1473,
                    "end": 1618
                },
                {
                    "start": 1619,
                    "end": 1661
                },
                {
                    "start": 1662,
                    "end": 1738
                },
                {
                    "start": 1739,
                    "end": 1816
                },
                {
                    "start": 1817,
                    "end": 1911
                },
                {
                    "start": 1912,
                    "end": 1964
                },
                {
                    "start": 1967,
                    "end": 2083
                },
                {
                    "start": 2084,
                    "end": 2280
                }
            ],
            "ref_mentions": [
                {
                    "start": 289,
                    "end": 293,
                    "matchedPaperCorpusId": "218974027"
                },
                {
                    "start": 1487,
                    "end": 1491,
                    "matchedPaperCorpusId": "215722669"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.28955078125
        },
        {
            "corpus_id": "226337596",
            "title": "Intent Detection Problem Solving via Automatic DNN Hyperparameter Optimization",
            "text": "Since experimental conditions are equalized for all languages (the same number of intents, the same distribution of instances among different classes, the same classifiers, the same optimization algorithms), the only difference lies in the language processing, i.e., vectorization. However, with neural vectorization, none of these languages have an advantage due to a smaller vocabulary (because the vector space dimensionality is the same for all languages) or less variety in inflection forms (because word embeddings are not discrete but distributional). However, some differences between the choices of word embedding types still exist. As seen from Figure 2, BERT vectorization is a better choice compared to fastText for all morphologically complex languages for all datasets, and this is not surprising. Morphologically complex languages (especially fusional languages) suffer from disambiguation problems, but BERT has mechanisms that are able to vectorize even those words that are written the same but have different meanings, depending on their context, differently. Despite the fact that fastText embeddings are also trained to consider a context around the target word, that context is restricted to only a few words. Despite this, fastText is a suitable vectorization solution for languages (such as English) with strict word order in a sentence. In contrast, BERT is able to consider a much broader context (words, sentences, their order) compared to fastText and is, therefore, more suitable for languages that have a relatively free word order in a sentence (such as Latvian, Lithuanian, and Russian). \n\nDespite LSTM, BiLSTM classifiers can sometimes be very accurate (especially on the chatbot dataset, having only two intents and enough representative training data); the domination of the CNN classifier is obvious (see Figure 2). Since we are solving the topic-based intent detection problem (where different intents are related to different topics), the contextual words or their n-grams seem to play a more important role than the sequential nature of the text. \n\nFurthermore, our focus is on the DNN architectures (the most accurate DNN architectures are visualized using the plot_model utility function in Keras) and the hyperparameter values of the most accurate models.",
            "score": 0.5138130783523996,
            "section_title": "Discussion",
            "char_start_offset": 28586,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 281
                },
                {
                    "start": 282,
                    "end": 558
                },
                {
                    "start": 559,
                    "end": 641
                },
                {
                    "start": 642,
                    "end": 811
                },
                {
                    "start": 812,
                    "end": 1078
                },
                {
                    "start": 1079,
                    "end": 1231
                },
                {
                    "start": 1232,
                    "end": 1361
                },
                {
                    "start": 1362,
                    "end": 1619
                },
                {
                    "start": 1622,
                    "end": 1851
                },
                {
                    "start": 1852,
                    "end": 2085
                },
                {
                    "start": 2088,
                    "end": 2297
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.689453125
        },
        {
            "corpus_id": "267122910",
            "title": "SPCM: A Machine Learning Approach for Sentiment-Based Stock Recommendation System",
            "text": "Subsequent to the categorization of labeled data, we partition the dataset into training and test sets in a 7:3 ratio. Following the methodology proposed by Sousa et al. [53], we employ the BERT model as a pre-trained model, finetuning it on our labeled training set for three-class sentiment classification. We then conduct the performance evaluation using 10-fold cross validation. Remarkably, the BERT model achieves an impressive accuracy of 84.12% on the test set. \n\nTo assess the effectiveness of the BERT-based classification approach, we compare it with commonly used classification algorithms from previous studies, including Support Vector Machines (SVM), Convolutional Neural Networks (CNN), and Long Short-Term Memory networks (LSTM), all trained on the same labeled dataset. For the experiment, we used Intel(R) Core(TM) i5-9300H CPU, 16g DDR4 memory, NVIDIA Geforce GTX 1660Ti, and Python 3.7 environment. The performance results are presented in Appendix Table 4. Notably, the BERT model outperforms all other algorithms in sentiment classification within the Guba discussion. This outcome underscores the predictive superiority of pre-trained models in this context.",
            "score": 0.5130615296733663,
            "section_title": "4) SENTIMENT CLASSIFICATION",
            "char_start_offset": 23849,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 118
                },
                {
                    "start": 119,
                    "end": 308
                },
                {
                    "start": 309,
                    "end": 383
                },
                {
                    "start": 384,
                    "end": 469
                },
                {
                    "start": 472,
                    "end": 787
                },
                {
                    "start": 788,
                    "end": 919
                },
                {
                    "start": 920,
                    "end": 1091
                },
                {
                    "start": 1092,
                    "end": 1182
                }
            ],
            "ref_mentions": [
                {
                    "start": 170,
                    "end": 174,
                    "matchedPaperCorpusId": "211208214"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.1531982421875
        },
        {
            "corpus_id": "246608158",
            "title": "Extracting Software Requirements from Unstructured Documents",
            "text": "The main purpose of this study was about assessing three major classification metrics: Precision, Recall, and F1-score. Those metrics are perceived as the golden standard in Machine Learning and Deep Learning areas. \n\nResult of the first phase of evaluation is presented in Table 1. As it was expected, more advanced model (BERT) showed better results in terms of F1-score. BERT-based model showed high precision (0.92) and lower recall (0.8). Results of the BERT model is available at https://bit.ly/3oPElMm. However, the values of precision and recall metrics behave differently for the fastText and ELMobased baselines. fastText-based classifier showed better Recall (0.93) comparing with other architectures. This property might be useful in some cases when it is necessary to extract more relevant sentences and text patterns associated with requirements.",
            "score": 0.5125530142348842,
            "section_title": "Experiments with PURE documents",
            "char_start_offset": 15016,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 119
                },
                {
                    "start": 120,
                    "end": 215
                },
                {
                    "start": 218,
                    "end": 282
                },
                {
                    "start": 283,
                    "end": 373
                },
                {
                    "start": 374,
                    "end": 509
                },
                {
                    "start": 510,
                    "end": 622
                },
                {
                    "start": 623,
                    "end": 712
                },
                {
                    "start": 713,
                    "end": 860
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.775390625
        },
        {
            "corpus_id": "222359862",
            "title": "Impact Assessment of Food Safety News Using Stacking Ensemble Learning",
            "text": "We carried out comparative experiments to verify the advantage of the proposed method versus existing methods. We first compared the ensemble classifier with the seven base classifiers, and then compared it with four classic classifiers, saying, the original version of NB, SVM, CNN and LSTM without using TF-IDF weights or pretrained word vectors. The result of the comparison is shown in Table 2. From the results in Table 2 we can see that the proposed ensemble classifier achieves the highest scores regarding all the performance indexes. The second runner is FastText, which scores 0.35% lower than the ensemble learner in terms of accuracy. LSTM with Word2Vec embedding performs similarly with FastText, but the version of LSTM with random embedding performs much poorer. By comparing NB, SVM, CNN and LSTM between their classic versions and base classifier versions, it can be seen that using information and knowledge extracted from the corpus can improve the performance of the classifiers. For BERT, which uses the most prior knowledge and represents the state-of-the-art of natural language understanding, it scores below the ensemble classifier, FastText, LSTM(base classifier version) and SVM(base classifier version), but scores above all the other classifiers. This is probably due to the shortened input of BERT.",
            "score": 0.5115268179302588,
            "section_title": "Comparative study",
            "char_start_offset": 14770,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 110
                },
                {
                    "start": 111,
                    "end": 348
                },
                {
                    "start": 349,
                    "end": 542
                },
                {
                    "start": 543,
                    "end": 646
                },
                {
                    "start": 647,
                    "end": 777
                },
                {
                    "start": 778,
                    "end": 999
                },
                {
                    "start": 1000,
                    "end": 1275
                },
                {
                    "start": 1276,
                    "end": 1328
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.56103515625
        },
        {
            "corpus_id": "221319787",
            "title": "Language Models and Word Sense Disambiguation: An Overview and Analysis",
            "text": "Word-specific results for different configurations of BERT and the FastText baseline are shown in Table 5. In general, results are high for all BERT-based models, over 90% in most cases. This reinforces the potential of language models for WSD, both in its lightweight 1NN and in the fine-tuning settings. in all BERT configurations. Also, higher polysemy is not a strong indicator of lower performance (see Table 4.2 for statistics of the twenty words, including polysemy), as one would expect from a classification task with higher number of classes (near zero average correlation across settings). In the following we also discuss other relevant points with respect to Most Frequent Sense (MFS) bias and fine-tuning. \n\nMFS Bias. As expected, macro F1 results degrade for the purely supervised classification models (FastText and fine-tuned BERT), indicating the inherent sense biases captured by the model which lead to lowered performance for the obscure senses (see the work by Postma et al. (2016) for a more thorough analysis on this issue). However, BERT proves to be much more robust with this respect whereas FastText suffers heavily (highlighted in the macro setting). \n\nImpact of fine-tuning. By average, fine-tuning improves the performance for BERT-Large by 1.6 points in terms of micro-F1 (from 95.8% to 97.5%) but decreases on macro-F1 (from 96.4% to 95.1%). While BERT-Base significantly correlates with BERT-Large in the 1NN setting (Pearson correlation above 0.9 for both micro and macro), it has a relatively low correlation with the fine-tuned BERT-Base (0.60 on Micro-F1 and 0.75 on macro-F1). The same trend is observed for BERT-Large, where the correlation between fine-tuning and 1NN is 0.71 and 0.63 on micro-F1 and macro-F1, respectively. The operating principles behind both approaches are significantly different, which may explain this relatively low correlation. While fine-tuning is optimizing a loss function during training, the 1NN approach is simply memorizing states. By optimizing losses, fine-tuning is more susceptible to overfit on the MFS.",
            "score": 0.5084060089200342,
            "section_title": "Results",
            "char_start_offset": 43862,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 106
                },
                {
                    "start": 107,
                    "end": 186
                },
                {
                    "start": 187,
                    "end": 305
                },
                {
                    "start": 306,
                    "end": 333
                },
                {
                    "start": 334,
                    "end": 600
                },
                {
                    "start": 601,
                    "end": 719
                },
                {
                    "start": 722,
                    "end": 731
                },
                {
                    "start": 732,
                    "end": 1048
                },
                {
                    "start": 1049,
                    "end": 1179
                },
                {
                    "start": 1182,
                    "end": 1204
                },
                {
                    "start": 1205,
                    "end": 1374
                },
                {
                    "start": 1375,
                    "end": 1615
                },
                {
                    "start": 1616,
                    "end": 1765
                },
                {
                    "start": 1766,
                    "end": 1893
                },
                {
                    "start": 1894,
                    "end": 2004
                },
                {
                    "start": 2005,
                    "end": 2081
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.796875
        },
        {
            "corpus_id": "231627772",
            "title": "Hostility Detection and Covid-19 Fake News Detection in Social Media",
            "text": "FastText based LSTM + FastText Classifier FastText represents a text by average of word vector and it also allows the word vector to be updated through back-propagation while training. FastText classifier uses (multinomial) logistic regression for training on the vector average and leverages Hierarchical Softmax to reduce computational complexity. We pass the cleaned training data to the FastText, and use grid search to find the optimal hyperparameter for the model. For the multi-label classification we use one-vs-all strategy. \n\nFastText based LSTM + Features In this paper, we built the hostility classification model using the LSTM [8] model with FastText embedding and metadata. We use the LSTM model owing to its internal memory to process long-range dependencies in sequential inputs like tweets. This model takes two inputs, the first being V = {v1, v2, ..., vn} the FastText Embedding, where each v i represents a cleaned tweet in vector space and the second MD = {A, E, H, M, U} the 5-tuplet metadata representation where A, E, H, M, U are the count of abusive words, emoji, hashtag, mention, and URL per clean tweet respectively. We use keras 6 to define and train our model. \n\nBERT-Hindi Embedding with BERT model Recent papers using BERT based contextual embedding models have shown SOTA performance in Hostile language identification [16,17]. We use Hindi-BERT pre-trained model 7 and finetune it on our dataset. The Hindi BERT model has 12 hidden layers, with an encoding layer of 256 dimensions instead of 786, the model has 4 attention heads for each attention layer. The dimension of the feed-forward layer is kept as 1024 instead of 3072 which is the default in BERT models. We find the best learning rate (i.e., 1.2e-4 here) for the model by simulating the training data briefly with different learning rates and choose the one with minimum loss. For training, we pass the preprocessed tweets into the BERT model and train the model for 12 epochs with a batch size of 8.",
            "score": 0.5076576534495255,
            "section_title": "Fig. 2: Data and modeling pipeline for Hindi Hostility Classification",
            "char_start_offset": 12491,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 184
                },
                {
                    "start": 185,
                    "end": 349
                },
                {
                    "start": 350,
                    "end": 470
                },
                {
                    "start": 471,
                    "end": 533
                },
                {
                    "start": 536,
                    "end": 688
                },
                {
                    "start": 689,
                    "end": 808
                },
                {
                    "start": 809,
                    "end": 1145
                },
                {
                    "start": 1146,
                    "end": 1191
                },
                {
                    "start": 1194,
                    "end": 1361
                },
                {
                    "start": 1362,
                    "end": 1431
                },
                {
                    "start": 1432,
                    "end": 1589
                },
                {
                    "start": 1590,
                    "end": 1698
                },
                {
                    "start": 1699,
                    "end": 1871
                },
                {
                    "start": 1872,
                    "end": 1995
                }
            ],
            "ref_mentions": [
                {
                    "start": 641,
                    "end": 644,
                    "matchedPaperCorpusId": "1915014"
                },
                {
                    "start": 1353,
                    "end": 1357,
                    "matchedPaperCorpusId": "204907377"
                },
                {
                    "start": 1357,
                    "end": 1360,
                    "matchedPaperCorpusId": "184483272"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.544921875
        },
        {
            "corpus_id": "237253665",
            "title": "Pharmacovigilance with Transformers: A Framework to Detect Adverse Drug Reactions Using BERT Fine-Tuned with FARM",
            "text": "By considering the subword information, fasttext unlike word2vec and glove generates the embeddings for out of vocabulary words as well. The training data used for any machine learning model, no matter how big it may be, can still not include all the words in a language's vocabulary. If such unseen words are found in the test data, their representations are not generated by word2vec and glove embedding models. However, fasttext overcomes this limitation and represents the out of vocabu-lary words by adding the embeddings for the constituent char n-grams found in the vocabulary. \n\nBERT outperforms both CNN and LSTM. The reason for the better performance of BERT is that it learns contextualized embeddings in a bidirectional way. In natural language, a word is likely to convey multiple meanings based on the context in which it is used. Word2vec, fasttext, and glove produce the same representations of a word even if it has different meanings in different contexts. BERT, on the other hand, produces context-dependent embeddings of a word. In BERT, an input word is represented by the sum of its token embeddings, sentence embeddings, and position embeddings. The proposed model FARM-BERT outperforms all the models by yielding the F-scores of 89.6% and 97.6% on Twitter and PubMed datasets, respectively. FARM-BERT performs better than BERT by 2% on Twitter and by 6% on PubMed datasets. Better performance of FARM-BERT than the standard BERT indicates the effectiveness of fine-tuning BERT with FARM with the modified values of hyperparameters.",
            "score": 0.5064231735434807,
            "section_title": "Experiments and Results",
            "char_start_offset": 24459,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 136
                },
                {
                    "start": 137,
                    "end": 284
                },
                {
                    "start": 285,
                    "end": 413
                },
                {
                    "start": 414,
                    "end": 584
                },
                {
                    "start": 587,
                    "end": 622
                },
                {
                    "start": 623,
                    "end": 736
                },
                {
                    "start": 737,
                    "end": 844
                },
                {
                    "start": 845,
                    "end": 974
                },
                {
                    "start": 975,
                    "end": 1048
                },
                {
                    "start": 1049,
                    "end": 1168
                },
                {
                    "start": 1169,
                    "end": 1314
                },
                {
                    "start": 1315,
                    "end": 1397
                },
                {
                    "start": 1398,
                    "end": 1555
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.71142578125
        },
        {
            "corpus_id": "261426527",
            "title": "Sentiment Analysis in Indonesian Healthcare Applications using IndoBERT Approach",
            "text": "BERT is a pre-training model that has undergone extensive training on a vast amount of data. The process of creating a BERT model involves two key steps: pre-training and finetuning. During pre-training, the model is trained on unlabeled data using various pre-training tasks. Subsequently, the BERT model is fine-tuned using labeled data from downstream tasks, starting with the pre-trained parameters. Despite commencing with the same pre-trained parameters, each downstream task results in a well-tuned model [12]. Fig. 2 illustrates the finetuning process on the pre-trained BERT model. The versatility of the BERT technique has been demonstrated through various studies aimed at addressing research gaps. For instance, researchers have successfully improved accuracy with transformer-based models when dealing with large, complex documents [22]. Additionally, BERT-based text classification has been enhanced by incorporating additional sentences and domain knowledge [23]. Notably, the impact of these improvements has been particularly evident in high-resource languages like English. To train pre-training models effectively, it is crucial to use specific languages. Since our dataset consists of user reviews in Indonesian, it necessitates pre-training models tailored for the Indonesian language. As a result, the BERT-based model has undergone significant enhancements, leading to the development of IndoBERT [24]. This improved version is built upon the Indonesian vocabulary, achieved by modifying the Huggingface framework. IndoBERT has been meticulously trained on an extensive dataset comprising over 220 million words, sourced from various Indonesian platforms, including Indonesian Wikipedia, news articles from Kompas, Tempo, Liputan6, and Korpus Web Indonesia. The training process involved running IndoBERT through 2.4 million steps or 180 epochs, taking approximately two months to complete. The positive attributes of IndoBERT motivated us to utilize this model for classifying Indonesian app reviews. For this study, we specifically employed \"IndoBERT-base-p1,\" which represents one variant of the IndoBERT model [8].",
            "score": 0.503699683759481,
            "section_title": "D. BERT Fine Tuning",
            "char_start_offset": 10510,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 92
                },
                {
                    "start": 93,
                    "end": 182
                },
                {
                    "start": 183,
                    "end": 276
                },
                {
                    "start": 277,
                    "end": 403
                },
                {
                    "start": 404,
                    "end": 517
                },
                {
                    "start": 518,
                    "end": 590
                },
                {
                    "start": 591,
                    "end": 709
                },
                {
                    "start": 710,
                    "end": 850
                },
                {
                    "start": 851,
                    "end": 978
                },
                {
                    "start": 979,
                    "end": 1091
                },
                {
                    "start": 1092,
                    "end": 1174
                },
                {
                    "start": 1175,
                    "end": 1306
                },
                {
                    "start": 1307,
                    "end": 1425
                },
                {
                    "start": 1426,
                    "end": 1537
                },
                {
                    "start": 1538,
                    "end": 1780
                },
                {
                    "start": 1781,
                    "end": 1913
                },
                {
                    "start": 1914,
                    "end": 2024
                },
                {
                    "start": 2025,
                    "end": 2141
                }
            ],
            "ref_mentions": [
                {
                    "start": 512,
                    "end": 516,
                    "matchedPaperCorpusId": "52967399"
                },
                {
                    "start": 973,
                    "end": 977,
                    "matchedPaperCorpusId": "209382386"
                },
                {
                    "start": 1420,
                    "end": 1424,
                    "matchedPaperCorpusId": "226226744"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.4267578125
        },
        {
            "corpus_id": "258668089",
            "title": "A Review on Deep-Learning-Based Cyberbullying Detection",
            "text": "The Facebook research team created FastText [36] as a library. It has two uses. The first is efficient word representation learning, and the second is sentence categorization. The method supports both supervised and unsupervised representations of words and sentences. On Facebook, if anyone puts a status update on their Facebook timeline about purchasing a bike, after a few moments, they may see an ad related to bikes. Facebook uses the text data to serve you better ads by using FastText. Figure 6 shows the word embedding for 3-gram in FastText. 4.1.7. BERT Bidirectional Encoder Representation from Transformers (BERT) [37] is based on the transformer architecture. Wikipedia (2500 million words) and Book Corpus (800 million words) are part of a vast corpus of unlabeled text that has been pre-trained. The success of BERT mainly lies in the pre-trained step, which has been trained with a large number of texts. The BERT model gathers information from both the left and right sides of a sentence context. Figure 7 shows an example of bi-directionality. If we forecast the nature of a word by choosing other words to its left or right sides, by selecting both sides of this term, BERT precisely predicts the exact meaning. The transformer is the foundation of the BERT architecture. BERT has two variants: BERT base and BERT large. BERT base has 12 layers of transformer blocks, 12 attention heads, and 110 million of parameters. BERT large, on the other hand, has 24 transformer layers, 16 attention heads, and 340 million parameters. Figure 8 shows the architecture of BERT base and BERT large. Figure 9 and 10 shows the input representation of BERT model and output as the embedding of BERT base respectively. BERT has been pre-trained on two natural language challenges. The first is Masked Language Modeling (MLM), which studies word relationships. The second is Next Sentence Prediction (NSP), which is necessary to comprehend how sentences relate to one another. There are some variations of BERT that are also used for the cyberbullying detection problem. (3) train on longer sequences; and (4) dynamically change the masking pattern over the training data.",
            "score": 0.5035157580297953,
            "section_title": "FastText",
            "char_start_offset": 23151,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 62
                },
                {
                    "start": 63,
                    "end": 79
                },
                {
                    "start": 80,
                    "end": 175
                },
                {
                    "start": 176,
                    "end": 268
                },
                {
                    "start": 269,
                    "end": 422
                },
                {
                    "start": 423,
                    "end": 493
                },
                {
                    "start": 494,
                    "end": 551
                },
                {
                    "start": 552,
                    "end": 558
                },
                {
                    "start": 559,
                    "end": 672
                },
                {
                    "start": 673,
                    "end": 810
                },
                {
                    "start": 811,
                    "end": 920
                },
                {
                    "start": 921,
                    "end": 1013
                },
                {
                    "start": 1014,
                    "end": 1061
                },
                {
                    "start": 1062,
                    "end": 1230
                },
                {
                    "start": 1231,
                    "end": 1290
                },
                {
                    "start": 1291,
                    "end": 1339
                },
                {
                    "start": 1340,
                    "end": 1437
                },
                {
                    "start": 1438,
                    "end": 1604
                },
                {
                    "start": 1605,
                    "end": 1720
                },
                {
                    "start": 1721,
                    "end": 1782
                },
                {
                    "start": 1783,
                    "end": 1861
                },
                {
                    "start": 1862,
                    "end": 1977
                },
                {
                    "start": 1978,
                    "end": 2071
                },
                {
                    "start": 2072,
                    "end": 2173
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.4365234375
        },
        {
            "corpus_id": "237485425",
            "title": "On the Validity of Pre-Trained Transformers for Natural Language Processing in the Software Engineering Domain",
            "text": "The results show that seBERT and BERToverflow achieve the best performance for the issue type prediction tasks, outperforming both fastText and the general-domain BERT models. The improvement over fastText is very large with an about 11% higher F1 score for the issue type prediction. The violin plot in Figure 5 indicates that the performance improvement in F1 score is due to an improvement of both recall and precision, which means the models reduced both false positives and false negatives in comparison to fastText. The Bayesian signed rank test determined that this improvement of the SE-specific models over the other models is significant. 15 The difference between seBERT and BERToverflow is not significant The comparison between fastText and BERT BASE shows that the general-domain models may be better than smaller text processing models without pretraining on the issue type prediction. BERT BASE significantly outperforms fastText on the issue type prediction task. The BERT LARGE model from the general domain has severe problems with this use case, i.e., there are several cases where the 14. The full list of polysemes can be found in the supplemental material. \n\n15. Posterior probabilities determined by the Bayesian signed rank test can be found in the supplemental material. models completely failed. This happened with none of the other models and may be an indication that the amount of data is too small to fine-tune such a large model from the general domain on a domain-specific corpus.",
            "score": 0.49958786992155807,
            "section_title": "Issue Type Prediction",
            "char_start_offset": 48155,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 175
                },
                {
                    "start": 176,
                    "end": 284
                },
                {
                    "start": 285,
                    "end": 521
                },
                {
                    "start": 522,
                    "end": 651
                },
                {
                    "start": 652,
                    "end": 900
                },
                {
                    "start": 901,
                    "end": 980
                },
                {
                    "start": 981,
                    "end": 1109
                },
                {
                    "start": 1110,
                    "end": 1179
                },
                {
                    "start": 1182,
                    "end": 1296
                },
                {
                    "start": 1297,
                    "end": 1322
                },
                {
                    "start": 1323,
                    "end": 1513
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.703125
        },
        {
            "corpus_id": "221173062",
            "title": "Experts and authorities receive disproportionate attention on Twitter during the COVID-19 crisis",
            "text": "Based on the pretrained models described above, we compare downstream classifier performance scores in Fig. S 16. Unexpectedly, BERT models trained on English-only data outperform the multilingual BERT model. Generally, we also see a performance boost due to domain-specific pretraining. The best English-only model (bert-english-pt) gives a F1-macro score of 0.71 and 0.62, on the category and type datasets, respectively. The smaller FastText models (fasttext-english-pt) perform comparably to other models on the type dataset but give slightly lower scores on the category dataset. The best multilingual model (bert-multilang-pt) yields F1-macro scores of 0.56 (category) and 0.63 (type). \n\nFor further analysis we focus on the multilingual BERT model (bert-multilang-pt), which was the final model used for label expansion in this work. When inspecting the confusion matrices (Fig. S 14 and Fig. S 15), classifier scores for this model are generally satisfying. Certain classes for which only very few observations are present show lower scores in comparison. In particular, this is concerning the classes \"Religion\" and \"Public Services\" (for category) and \"Unclear\" (for type). The smallest error rates can be expected for the classes \"Healthcare\", \"News Media\", and \"Government and Politics\". No significant deviations from the mean accuracy could be observed for individual languages. Testing was performed using a two-sided binomial test at significance level \u03b1 = 0.5 (before Bonferroni correction). To alleviate a potential bias in our analysis caused by the hypothetical surge of new users joining the platform during the crisis, we conducted an alternative analysis where we restricted ourselves to a set of users who created their Twitter account before the studied period. In that way, we excluded 482, out of 14000 annotated users. The observed trends are not impacted by the presence of such newcomers.",
            "score": 0.4993236661219417,
            "section_title": "Classifier results",
            "char_start_offset": 37394,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 113
                },
                {
                    "start": 114,
                    "end": 208
                },
                {
                    "start": 209,
                    "end": 287
                },
                {
                    "start": 288,
                    "end": 423
                },
                {
                    "start": 424,
                    "end": 584
                },
                {
                    "start": 585,
                    "end": 691
                },
                {
                    "start": 694,
                    "end": 840
                },
                {
                    "start": 841,
                    "end": 965
                },
                {
                    "start": 966,
                    "end": 1063
                },
                {
                    "start": 1064,
                    "end": 1183
                },
                {
                    "start": 1184,
                    "end": 1299
                },
                {
                    "start": 1300,
                    "end": 1392
                },
                {
                    "start": 1393,
                    "end": 1508
                },
                {
                    "start": 1509,
                    "end": 1786
                },
                {
                    "start": 1787,
                    "end": 1846
                },
                {
                    "start": 1847,
                    "end": 1918
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.47119140625
        },
        {
            "corpus_id": "259138635",
            "title": "Imbalanced Multi-label Classification for Business-related Text with Moderately Large Label Spaces",
            "text": "The study focused on the problem of imbalanced multi-label classification for business-related text with moderately large label spaces. The experiment compared the performance of four methods, namely Binary Relevance, Classifier Chains, Label Power-set, and Fine-tuned BERT, and evaluated their effectiveness based on accuracy, F1-Score, Precision, and Recall values. The results revealed that the fine-tuned BERT method significantly outperformed the other three methods, achieving high accuracy, F1-Score, Precision, and Recall values. Binary Relevance also performed well, but Classifier Chains and Label Power-set exhibited relatively poor performance on this imbalanced dataset. \n\nOverall, the findings suggest that finetuning the pre-trained BERT model is a good idea because it enables the model to adapt to a specific application. BERT is a powerful language model that has been pre-trained on a large corpus of text, allowing it to learn the intricacies of language and syntax. However, while pre-training provides a strong foundation, it does not necessarily optimize the model for specific tasks such as sentiment analysis, question answering, or text classification. Finetuning allows us to take the pre-trained BERT model and tailor it to our specific needs by training it on a smaller dataset that is specific to the task at hand. \n\nReproducing the results of a finetuned BERT model requires the same preprocessing, architecture, and hyperparameters used in the original experiment. To reproduce the results, the same evaluation metrics should be used, and the results should be compared to the original experiment. However, the dataset used for finetuning the BERT model is highly dependent on the application. Different tasks require different datasets, and the quality and size of the dataset can greatly affect the model's performance. A small, poorly labeled dataset may not provide enough information for the model to learn, while a large, well-labeled dataset may enable the model to generalize better. Therefore, it is essential to choose an appropriate dataset for the specific task at hand to achieve optimal results. The primary objective of this paper was not to provide a step-by-step guide for reproducing a specific model.",
            "score": 0.4975211533721764,
            "section_title": "Discussion and Conclusion",
            "char_start_offset": 15652,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 135
                },
                {
                    "start": 136,
                    "end": 367
                },
                {
                    "start": 368,
                    "end": 537
                },
                {
                    "start": 538,
                    "end": 683
                },
                {
                    "start": 686,
                    "end": 838
                },
                {
                    "start": 839,
                    "end": 986
                },
                {
                    "start": 987,
                    "end": 1178
                },
                {
                    "start": 1179,
                    "end": 1344
                },
                {
                    "start": 1347,
                    "end": 1496
                },
                {
                    "start": 1497,
                    "end": 1629
                },
                {
                    "start": 1630,
                    "end": 1725
                },
                {
                    "start": 1726,
                    "end": 1853
                },
                {
                    "start": 1854,
                    "end": 2023
                },
                {
                    "start": 2024,
                    "end": 2141
                },
                {
                    "start": 2142,
                    "end": 2251
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.6904296875
        },
        {
            "corpus_id": "231719790",
            "title": "Semi-automatic Generation of Multilingual Datasets for Stance Detection in Twitter",
            "text": "The models are evaluated with the metric and script provided by the organizers of SemEval 2016 (Mohammad et al., 2016) which reports F1 macro-average score of two classes: FAVOR and AGAINST, although the NONE class is also represented in the test data:\n\nThe fastText system is used off-the-shelf, training domain-specific word embeddings on each target's training set with one exception: after some experimentation on the \"Feminist Movement\" train set, we increased the number of epochs to 60.\n\nThe train set from the \"Feminist Movement\" target was also used to obtain the hyperparameters for mBERT, XLNet, RoBERTa and XLM-RoBERTa.Thus, for mBERT we used a maximum sequence length of 256, 32 batch size, 5e-5 learning rate and 5 epochs.\n\nThe results show that, as it has been the case for other text classification tasks (Devlin et al., 2019;Yang et al., 2019) the Tranformers-based pre-trained language models outperform any other approach.In fact, we obtain state-of-the art results, improving over any other approach presented in Section 2, using the base version of RoBERTa (Liu et al., 2019).Ghosh et al. (2019) obtains best overall results by applying BERT Large uncased with the hyperparameters specified in the last row of Table 9.This suggests that these large pre-trained language models require less training data than classic machine learning approaches.Finally, they also show that for fine-tuning on small datasets it is convenient to increase the number of epochs.",
            "score": 0.49717269141262865,
            "section_title": "Evaluation",
            "char_start_offset": 38707,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 252
                },
                {
                    "start": 254,
                    "end": 493
                },
                {
                    "start": 495,
                    "end": 631
                },
                {
                    "start": 631,
                    "end": 736
                },
                {
                    "start": 738,
                    "end": 941
                },
                {
                    "start": 941,
                    "end": 1097
                },
                {
                    "start": 1097,
                    "end": 1239
                },
                {
                    "start": 1239,
                    "end": 1366
                },
                {
                    "start": 1366,
                    "end": 1479
                }
            ],
            "ref_mentions": [
                {
                    "start": 821,
                    "end": 842,
                    "matchedPaperCorpusId": "52967399"
                },
                {
                    "start": 1097,
                    "end": 1116,
                    "matchedPaperCorpusId": "201711347"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.373779296875
        },
        {
            "corpus_id": "245005741",
            "title": "\u201cWhat Can I Cook with these Ingredients?\u201d - Understanding Cooking-Related Information Needs in Conversational Search",
            "text": "Baseline Models: To generate a baseline for our experiments, we employed some of the models, which have been shown to be effective in past experiments predicting information needs in conversational settings [38,72,74]. For our baseline models, we used 300-dimensional FastText embeddings [47] combined with BytePairEmbeddings provided by Flair [3], which are able to capture out of vocabulary words using sub-word information [42]. This feature was the main reason for choosing FastText embeddings, as it was considered helpful when dealing with spoken dialogue data where out of vocabulary words might occur more likely due to its natural style. For the baseline experiments, we use Gaussian Naive Bayes (=GNB or GaussianNB), Random Forest and SVM classificators provided by the Python package scikit-learn [64].\n\nBERT-based models: Since Schwabl [72] showed that information need detection in the cooking domain benefits from using BERT-based models, we employed them for comparison. BERT was chosen due to its success in natural language understanding problems [98], such as Named Entity Recognition and Text Classification. For our experiments, we leveraged pre-trained BERT models: GermanBERT 6 and the multilingual BERT models mBERT 7 and XLM-RoBERTa. 8 Due to the small amount of data, 85% was kept as training and 15% as test data to ensure that there are still samples available for the test dataset even for categories with few data. For each BERT model, we chose the BERT BAS E architecture which was available for all three models in contrast to BERT LARGE . Since fine-tuning pre-trained BERT models on downstream tasks has shown to improve classification performance [79], this technique was also applied in our experiments. All BERT-related experiments were conducted using the transfer-learning library FARM. 9 To fine-tune each of the 11 binary classifiers, a TextClassficationHead 10 was added on top of the BERT language model. The TextClassificationHead consists of a one-layer feed-forward neural network (FFNN) of dimension (768,2) and a softmax activation function. This follows the recommendations made by Sun et al. [79] and Devlin et al. [31]. The classification head receives the final hidden",
            "score": 0.4964302650519236,
            "section_title": "Models.",
            "char_start_offset": 42679,
            "sentence_offsets": [],
            "ref_mentions": [
                {
                    "start": 207,
                    "end": 211,
                    "matchedPaperCorpusId": "209432405"
                },
                {
                    "start": 214,
                    "end": 217,
                    "matchedPaperCorpusId": "11307799"
                },
                {
                    "start": 344,
                    "end": 347,
                    "matchedPaperCorpusId": "181704107"
                },
                {
                    "start": 426,
                    "end": 430,
                    "matchedPaperCorpusId": "21697629"
                },
                {
                    "start": 808,
                    "end": 812,
                    "matchedPaperCorpusId": "10659969"
                },
                {
                    "start": 1258,
                    "end": 1259,
                    "matchedPaperCorpusId": "215587100"
                },
                {
                    "start": 1681,
                    "end": 1685,
                    "matchedPaperCorpusId": "153312532"
                },
                {
                    "start": 1825,
                    "end": 1826,
                    "matchedPaperCorpusId": "12719479"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.5771484375
        },
        {
            "corpus_id": "252651524",
            "title": "Multimodal Model with Text and Drug Embeddings for Adverse Drug Reaction Classification",
            "text": "We compared the performance of our proposed bi-modal models with three uni-modal (text-only) baselines: \n\n\u2022 Support Vector Machines (SVM) [35] classifier over TF-IDF vectors. Before TF-DF vectorization, we tokenized and lemmatized each text using Spacy4 and filtered out stop words using language-specific lists of stopwords obtained from NLTK [36]. We used TF-IDF and SVM implementations from Scikit-learn [37]. \n\n\u2022 Convolutional Neural Network (CNN) [38,39] over pretrained Word2vec [40] or Fasttext [41] word embeddings. We tokenized each text using Spacy and filtered out stop words using stopwords lists obtained from NLTK. CNN baselines, unimodal and bi-modal BERT-based models were implemented using the PyTorch library [44]. Pretrained BERT-based models were obtained using the Transformers library [45]. For each dataset and training setup, we trained 10 BERT-based models with different initializations of classifier weights and calculated the mean quality and standard deviation of F 1 -scores (F 1 ), precision (P), and recall (R) for the ADE class.",
            "score": 0.4950594085128788,
            "section_title": "Experimental setup",
            "char_start_offset": 18243,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 103
                },
                {
                    "start": 106,
                    "end": 174
                },
                {
                    "start": 175,
                    "end": 349
                },
                {
                    "start": 350,
                    "end": 412
                },
                {
                    "start": 415,
                    "end": 523
                },
                {
                    "start": 524,
                    "end": 628
                },
                {
                    "start": 629,
                    "end": 732
                },
                {
                    "start": 733,
                    "end": 812
                },
                {
                    "start": 813,
                    "end": 1061
                }
            ],
            "ref_mentions": [
                {
                    "start": 138,
                    "end": 142,
                    "matchedPaperCorpusId": "207165665"
                },
                {
                    "start": 407,
                    "end": 411,
                    "matchedPaperCorpusId": "10659969"
                },
                {
                    "start": 452,
                    "end": 456,
                    "matchedPaperCorpusId": "14542261"
                },
                {
                    "start": 456,
                    "end": 459,
                    "matchedPaperCorpusId": "9672033"
                },
                {
                    "start": 485,
                    "end": 489,
                    "matchedPaperCorpusId": "5959482"
                },
                {
                    "start": 502,
                    "end": 506,
                    "matchedPaperCorpusId": "207556454"
                },
                {
                    "start": 727,
                    "end": 731,
                    "matchedPaperCorpusId": "202786778"
                },
                {
                    "start": 807,
                    "end": 811,
                    "matchedPaperCorpusId": "269498086"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.384765625
        },
        {
            "corpus_id": "258013230",
            "title": "Ticket automation: An insight into current research with applications to multi-level classification scenarios",
            "text": "Performance metrics across baselines are quite close; in some cases, we found that more recent approaches would perform worse than the SVM-based classifier, which instead performed remarkably well on both datasets. The most crucial advantage that we would expect BERT to have over models based on traditional text representations is the ability to extract more expressive features that can embed both contextual and sequential information from the tokens. However, the Linux Bugs dataset is very noisy, with many grammatical inconsistencies and technical readings, like stack traces or memory addresses. These likely make little sense for a LM pre-trained on more structured natural language. On the other hand, the SVM-based approach utilizes BoW features weighted with TF-IDF; therefore, the classifier only looks at global word frequency without considering any structural information. Indeed, it is conceivable that the strength of the SVM classifier can be explained by the lack of particular expressiveness in the structural information of these datasets. While the Financial dataset is less cluttered in terms of technical jargon, it is still rather noisy, while also being characterized characterized by many structurally unsound sentences. Still, sentence structure is more expressive here, as demonstrated by the stronger performance of both our baseline LMs and our proposed approaches. Again, as mentioned in Section 4.1, the labeling of this dataset is not ideal for the task, which explains the poor performances in terms of precision and recall. \n\nThe FastText classifier obtained worse performances than other methods on both datasets. Given the amount of noise in the datasets we experimented with, it is possible that fine-tuning the embeddings before applying them to the classification task may improve the results of this approach -similar to what we do in our Transformer-based approaches. \n\nDeepTriage obtains decent results, though not on par with BERT-based models. This was to be expected, as Transformers are capable of higher semantic and syntactic understanding as compared to recurrent models, and therefore create more meaningful representations for the documents. A noteworthy disadvantage of DeepTriage is indeed its recurrent nature; the computational expensiveness for training becomes quickly unmanageable when attempting to process longer sequences.",
            "score": 0.4902120828711583,
            "section_title": "Baselines",
            "char_start_offset": 61398,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 214
                },
                {
                    "start": 215,
                    "end": 455
                },
                {
                    "start": 456,
                    "end": 603
                },
                {
                    "start": 604,
                    "end": 692
                },
                {
                    "start": 693,
                    "end": 888
                },
                {
                    "start": 889,
                    "end": 1061
                },
                {
                    "start": 1062,
                    "end": 1248
                },
                {
                    "start": 1249,
                    "end": 1397
                },
                {
                    "start": 1398,
                    "end": 1560
                },
                {
                    "start": 1563,
                    "end": 1651
                },
                {
                    "start": 1652,
                    "end": 1911
                },
                {
                    "start": 1914,
                    "end": 1990
                },
                {
                    "start": 1991,
                    "end": 2195
                },
                {
                    "start": 2196,
                    "end": 2386
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.323486328125
        },
        {
            "corpus_id": "235421844",
            "title": "A Sentence-level Hierarchical BERT Model for Document Classification with Limited Labelled Data",
            "text": "Partially following [15], we compare HBM with several baseline methods. We selected an SVM model using a document representation based on FastText, an SVM model using text representations based on a pre-trained Roberta model and a fine-tuned Roberta model as baselines due to their strong performance. We also used another sentence-level model, Hierarchical Attention Network (HAN) [18], as a baseline. \n\nFastText + SVM: We use 300-dimensional word vectors constructed by a Fast-Text language model pre-trained with the Wikipedia corpus [7]. Averaged word embeddings are used as the representation of the document. For preprocessing, all text is converted to lowercase and we remove all punctuation and stop words. SVM is used as the classifier. We tune the hyper-parameters of the SVM classifier using a grid-search based on 5-fold cross-validation performed on the training set, after that, we re-train the classifier with optimised hyper-parameters. This hyper-parameter tuning method is applied in Roberta + SVM as well. \n\nRoberta + SVM: We use 768-dimensional word vectors generated by a pre-trained Roberta language model [9]. We do not fine-tune the pre-trained language model and use the averaged word vectors as the representation of the document. Since all BERT-based models are configured to take as input a maximum of 512 tokens, we divided the long documents with W words into k = W/511 fractions, which is then fed into the model to infer the representation of each fraction (each fraction has a \"[CLS]\" token in front of 511 tokens, so, 512 tokens in total). Based on the approach of [14], the vector of each fraction is the average embeddings of words in that fraction, and the representation of the whole text sequence is the mean of all k fraction vectors. For preprocessing, the only operation performed is to convert all tokens to lowercase. SVM is used as the classifier. \n\nFine-tuned Roberta: For the document classification task, fine-tuning Roberta means adding a softmax layer on top of the Roberta encoder output and finetuning all parameters in the model.",
            "score": 0.48859100703291714,
            "section_title": "Baselines and Setup",
            "char_start_offset": 14126,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 71
                },
                {
                    "start": 72,
                    "end": 301
                },
                {
                    "start": 302,
                    "end": 402
                },
                {
                    "start": 405,
                    "end": 541
                },
                {
                    "start": 542,
                    "end": 614
                },
                {
                    "start": 615,
                    "end": 714
                },
                {
                    "start": 715,
                    "end": 745
                },
                {
                    "start": 746,
                    "end": 952
                },
                {
                    "start": 953,
                    "end": 1024
                },
                {
                    "start": 1027,
                    "end": 1132
                },
                {
                    "start": 1133,
                    "end": 1256
                },
                {
                    "start": 1257,
                    "end": 1573
                },
                {
                    "start": 1574,
                    "end": 1774
                },
                {
                    "start": 1775,
                    "end": 1861
                },
                {
                    "start": 1862,
                    "end": 1892
                },
                {
                    "start": 1895,
                    "end": 2082
                }
            ],
            "ref_mentions": [
                {
                    "start": 382,
                    "end": 386,
                    "matchedPaperCorpusId": "6857205"
                },
                {
                    "start": 1599,
                    "end": 1603,
                    "matchedPaperCorpusId": "153312532"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.5537109375
        },
        {
            "corpus_id": "246738423",
            "title": "Application of Knowledge Distillation Based on Transfer Learning of ERNIE Model in Intelligent Dialogue Intention Recognition",
            "text": "Li [17] used the application of the BERT model in text classification at the sub-word level. Baidu proposed the ERNIE (Enhanced Representation through Knowledge Integration) model in 2018 [18]. Compared with BERT, ERNIE has improved two masking strategies; the first is the phrase-based masking strategy, and the second is the entity (name, location, organization, product, etc.) based masking strategy. Although the BERT and ERNIE models can achieve positive results in various tasks in the field of natural language processing, it is difficult for scientific research teams in universities as well as in smalland medium-sized enterprises to pre-train BERT from the base model due to the large size of the model, the large amount of data, and the large number of resources required for training. In the field of computer vision, Hinton et al. [19] proposed a training method based on the \"teacher-student network idea\" in 2015. The basic principle was to transfer the features learned from the \"teacher\" network with a complex network structure and strong learning ability to the \"student\" network with a small number of parameters and weak learning ability. Sau et al. [20] used a noise-based regularizer to train the student from the teacher for a higher training efficiency in 2016. Liao et al. [21] tried to use the BERT model for distillation learning in LSTM and CNN. Sun et al. [22] proposed Patient Knowledge Distillation for the BERT Model. \n\nIn e-commerce, it is usually computationally expensive when the chatbot handles a large number of real customers online; the simple intention classification model can show a beneficial computational performance but is limited to the low classification accuracy. To improve the performance of the small-scale model, the knowledge obtained by the \"teacher\" model can guide the training of the original small-scale model to achieve better results. In this study, we develop a chatbot for after-sale service in e-commerce to help manage the high computational price issue. The pre-trained ERNIE model will distill the FastText model to increase the efficiency and maintain the classification accuracy.",
            "score": 0.48822006228932646,
            "section_title": "Introduction",
            "char_start_offset": 7929,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 92
                },
                {
                    "start": 93,
                    "end": 193
                },
                {
                    "start": 194,
                    "end": 403
                },
                {
                    "start": 404,
                    "end": 796
                },
                {
                    "start": 797,
                    "end": 928
                },
                {
                    "start": 929,
                    "end": 1159
                },
                {
                    "start": 1160,
                    "end": 1286
                },
                {
                    "start": 1287,
                    "end": 1374
                },
                {
                    "start": 1375,
                    "end": 1450
                },
                {
                    "start": 1453,
                    "end": 1714
                },
                {
                    "start": 1715,
                    "end": 1897
                },
                {
                    "start": 1898,
                    "end": 2021
                },
                {
                    "start": 2022,
                    "end": 2150
                }
            ],
            "ref_mentions": [
                {
                    "start": 3,
                    "end": 7,
                    "matchedPaperCorpusId": "222020002"
                },
                {
                    "start": 1171,
                    "end": 1175,
                    "matchedPaperCorpusId": "7200347"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.414306640625
        },
        {
            "corpus_id": "220330867",
            "title": "Context-Aware Sarcasm Detection Using BERT",
            "text": "BERT (Devlin et al., 2019) is a transformer based architecture (Vaswani et al., 2017). It is a bidirectional model. As opposed to static embeddings that are produced by fastText, BERT produces contextualized word embeddings where the vector for the word is computed based on the context in which it appears. \n\nIn our study, we used the uncased large version of BERT 2 . This version has 24 layers and 16 attention heads. This model generates 1024 dimensional vector for each word. We used 1024 dimensional vector of the Extract layer as the representation of the text. Our classification layer consisted of a single Dense layer. This layer used the sigmoid activation layer. The classifier was trained using the Adam optimizer with a learning rate of 2e-5. The binary crossentropy loss function was used. \n\n1 https://fasttext.cc/docs/en/english-vectors.html 2 https://github.com/google-research/ bert",
            "score": 0.48808937527330765,
            "section_title": "BERT",
            "char_start_offset": 6148,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 86
                },
                {
                    "start": 87,
                    "end": 115
                },
                {
                    "start": 116,
                    "end": 307
                },
                {
                    "start": 310,
                    "end": 369
                },
                {
                    "start": 370,
                    "end": 420
                },
                {
                    "start": 421,
                    "end": 480
                },
                {
                    "start": 481,
                    "end": 568
                },
                {
                    "start": 569,
                    "end": 628
                },
                {
                    "start": 629,
                    "end": 674
                },
                {
                    "start": 675,
                    "end": 756
                },
                {
                    "start": 757,
                    "end": 804
                },
                {
                    "start": 807,
                    "end": 895
                },
                {
                    "start": 896,
                    "end": 900
                }
            ],
            "ref_mentions": [
                {
                    "start": 5,
                    "end": 26,
                    "matchedPaperCorpusId": "52967399"
                },
                {
                    "start": 63,
                    "end": 85,
                    "matchedPaperCorpusId": "13756489"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.4453125
        },
        {
            "corpus_id": "252901000",
            "title": "Assessing Guest Nationality Composition from Hotel Reviews",
            "text": "Multiple machine learning models of varying complexity were trained on the training set. The results from the validation set are displayed in Table I. The best performing model consists of pre-trained FastText embeddings followed by a stack of bidirectional LSTM layers. This rather simple architecture performs more that 20% better in F 1 compared to a support vector machine baseline with TF-IDF feature engineering and also beats transfer learning with a pretrained BERT model 3 by a small margin. On the one hand, this indicates that such large language models as BERT can generalize pretty well even in the presence of small data. On the other hand, BERT comes with an increase of 160% in the number of parameters and thus significantly higher inference time compared to the winning model. Hence, even with more data and an expected increase in the BERT model performance, the LSTM architecture will show the better performance-runtime tradeoff. Table II shows an excerpt of a qualitative evaluation of the best performing model by displaying its predictions in specific edge cases. On the hidden test set prepared with independent annotators, the winning model achieved an F 1 score of 93.2%, convincingly demonstrating its capability to generalize to reviews from other sources. For comparison, the keyword-based approach without supervised learning would only achieve an F 1 score as low as 31.3%.",
            "score": 0.4877318385480758,
            "section_title": "IV. RESULTS",
            "char_start_offset": 5875,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.76904296875
        },
        {
            "corpus_id": "253463207",
            "title": "Building the European Social Innovation Database with Natural Language Processing and Machine Learning",
            "text": "Using BERT requires fine-tuning on a small dataset of the designated classes. To this end, we used 5,162 projects, 80% of the dataset for training and 20% for testing. Before the classification, we pre-processed the text by applying the following: (i) removing HTML tags if exists and special characters like #, =, and &, (ii) eliminating long sequence of characters, i.e., more than 20 characters, and (iii) dropping duplicated sentences but with maintaining the order of the text. To train BERT classifier we fine-tuned BERT base cased version using the SimpleTransformers framework 32 . \n\nThe best results were yielded by fine-tuning BERT followed by the LR classifier (Table 3). BERT model has state-of-the-art results in several Natural Language Processing tasks 33,34 . BERT is a context-based transformer language model that generates word representations for each word based on distributional semantics hypothesis from linguistics 35 . The hypothesis says that words that are used and occur in the same contexts tend to communicate similar meanings. The word representations in BERT are based on an unsupervised language model over large amount of text, where certain words are masked, and the task of the model is to predict masked words based on the surrounding context. Contextuality, attention mechanism 36 that gives importance to certain portion of the context and the depth of the model in BERT contribute to its superiority compared to previous approaches. \n\nSummarisation. Availability of the short descriptions of projects is important for two reasons. First, short descriptions provide a quick snapshot to the users and offer them the opportunity of extracting further features through topic modelling. Second, the BERT model we employed to score the four social innovation criteria requires a relatively short text length (>512 tokens). To obtain short descriptions which are representative of the text, we experimented with three different models, and we used a combination of these (see Figure Fig. 2). \n\n\u2022 Support Vector Machine (SVM) is a machine learning algorithm that works on high dimensional datasets and works by finding the best hyperplane that separates the datasets into the most differentiative classes.",
            "score": 0.4859301839539657,
            "section_title": "Deep crawling of project URLs using Apache Nutch with Selenium plugin, indexing with Solr",
            "char_start_offset": 10659,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 77
                },
                {
                    "start": 78,
                    "end": 167
                },
                {
                    "start": 168,
                    "end": 482
                },
                {
                    "start": 483,
                    "end": 589
                },
                {
                    "start": 592,
                    "end": 682
                },
                {
                    "start": 683,
                    "end": 775
                },
                {
                    "start": 776,
                    "end": 943
                },
                {
                    "start": 944,
                    "end": 1057
                },
                {
                    "start": 1058,
                    "end": 1280
                },
                {
                    "start": 1281,
                    "end": 1472
                },
                {
                    "start": 1475,
                    "end": 1489
                },
                {
                    "start": 1490,
                    "end": 1570
                },
                {
                    "start": 1571,
                    "end": 1721
                },
                {
                    "start": 1722,
                    "end": 1856
                },
                {
                    "start": 1857,
                    "end": 2024
                },
                {
                    "start": 2027,
                    "end": 2237
                }
            ],
            "ref_mentions": [
                {
                    "start": 768,
                    "end": 771,
                    "matchedPaperCorpusId": "207833162"
                },
                {
                    "start": 1316,
                    "end": 1318,
                    "matchedPaperCorpusId": "13756489"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.162353515625
        },
        {
            "corpus_id": "234813543",
            "title": "Filtered BERT: Similarity Filter-Based Augmentation with Bidirectional Transfer Learning for Protected Health Information Prediction in Clinical Documents",
            "text": "We applied an optimized pre-training model for filtered BERT-based data augmentation. Bio-Clinical BERT [31], which was pretrained using bio and clinical data, was applied to the BERT model that predicted the masked word vector. It was confirmed in a previous study that the BERT model pretrained with a corpus suitable for the data showed good performance. To calculate the cosine similarity at the filter stage, fastText-based embedding was used when vectorization was performed by the embedding vector. In this case, fastText embedding, trained using the BioWordVec corpora [32], was used.",
            "score": 0.48585546609764363,
            "section_title": "Filtered BERT-Based Clinical Documents Augmentation",
            "char_start_offset": 13636,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 85
                },
                {
                    "start": 86,
                    "end": 228
                },
                {
                    "start": 229,
                    "end": 357
                },
                {
                    "start": 358,
                    "end": 505
                },
                {
                    "start": 506,
                    "end": 592
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.5380859375
        },
        {
            "corpus_id": "271582354",
            "title": "AN AUTOMATED NEW APPROACH IN FAST TEXT CLASSIFICATION: A CASE STUDY FOR KURDISH TEXT",
            "text": "The findings indicated that Bert achieved an accuracy of 92%, surpassing the performance of traditional machine-learning algorithms by two percentage points (Badawi, 2023). In another study, researchers addressed the issue of high dimensionality in feature space and the limitations of conventional machine learning algorithms such as SVM, NB, and KNN by introducing FastText as a novel classification model. The study revealed that the FastText model attained an impressive F1-score of 0.9286, outperforming the other models (Yao et al., 2020). Birol Kuyucu used the FastText classifier to analyze the TTC-3600 Turkish dataset. Remarkably, no preprocessing steps such as tokenization, stemming, lemmatization, stop word removal, lowercase conversion, or dimensionality reduction were applied. Following this, the performance of FastText was compared with K-NN, decision tree J48, and Multinomial Na\u00efve Bayes (NV). The results demonstrated that FastText surpassed the other models, achieving an impressive accuracy score of 93.52%, despite the absence of preprocessing steps (Kuyumcu et al., 2019). Additionally, Amalia conducted a comparison between the FastText model and TF-IDF as one of the BOW models for 500 new articles in a low-resource Bahasa Indonesia. The study revealed that TF-IDF requires more preprocessing steps and is time-consuming for model prediction. Moreover, it was observed that FastText classification exhibited superior performance with a 0.97 F1-score compared to TF-IDF (Amalia et al., 2020).",
            "score": 0.4853129009429891,
            "section_title": "RELATED WORK",
            "char_start_offset": 5197,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 172
                },
                {
                    "start": 173,
                    "end": 408
                },
                {
                    "start": 409,
                    "end": 545
                },
                {
                    "start": 546,
                    "end": 628
                },
                {
                    "start": 629,
                    "end": 793
                },
                {
                    "start": 794,
                    "end": 914
                },
                {
                    "start": 915,
                    "end": 1098
                },
                {
                    "start": 1099,
                    "end": 1262
                },
                {
                    "start": 1263,
                    "end": 1371
                },
                {
                    "start": 1372,
                    "end": 1520
                }
            ],
            "ref_mentions": [
                {
                    "start": 526,
                    "end": 544,
                    "matchedPaperCorpusId": "221719237"
                },
                {
                    "start": 1075,
                    "end": 1097,
                    "matchedPaperCorpusId": "201103512"
                },
                {
                    "start": 1498,
                    "end": 1519,
                    "matchedPaperCorpusId": "221715127"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.50927734375
        },
        {
            "corpus_id": "244895506",
            "title": "Transformer based Contextual Model for Sentiment Analysis of Customer Reviews: A Fine-tuned BERT",
            "text": "The pre-trained BERT model's performance, accuracy and approach motivated the authors to experiment with this model for customer sentiment analysis. The main objectives and contributions are as follows; \n\n\uf0b7 Overcome known challenges of SA model performance, accuracy and context-based prediction. \n\n\uf0b7 Train BERT-base-cased model on Twitter, IMDB Movie Reviews, Yelp, and Amazon customer reviews datasets to improve the accuracy and performance of the model. \n\n\uf0b7 Evaluate the custom deep learning sequential model of BiLSTM, hybrid fastText-BiLSTM model and linear models of LSVM, fastText models using the same datasets. \n\n\uf0b7 Compare the results of the BERT model with the results of the deep learning sequential and linear model. \n\n\uf0b7 Customize the data pre-processing steps for hybrid and linear model training. \n\n\uf0b7 Fine-tune the hyperparameters for fastText-BiLSTM models. \n\nThis paper presented with several recent BERT and SA related research papers reviews and major contributions from various researchers in Section II. The literature reviews of BERT, fastText, BiLSTM, and LSVM models are presented in Section III. The experimental setting and model evaluation results are discussed in the Section IV. In Section V, concluded this paper with model results, findings of this research work and future work.",
            "score": 0.48516327737409287,
            "section_title": "I. INTRODUCTION",
            "char_start_offset": 3847,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 148
                },
                {
                    "start": 149,
                    "end": 202
                },
                {
                    "start": 205,
                    "end": 296
                },
                {
                    "start": 299,
                    "end": 457
                },
                {
                    "start": 460,
                    "end": 620
                },
                {
                    "start": 623,
                    "end": 729
                },
                {
                    "start": 732,
                    "end": 811
                },
                {
                    "start": 814,
                    "end": 873
                },
                {
                    "start": 876,
                    "end": 1024
                },
                {
                    "start": 1025,
                    "end": 1120
                },
                {
                    "start": 1121,
                    "end": 1207
                },
                {
                    "start": 1208,
                    "end": 1310
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.5380859375
        },
        {
            "corpus_id": "227230658",
            "title": "New Benchmark Corpus and Models for Fine-grained Event Classification: To BERT or not to BERT?",
            "text": "We have evaluated five SVM-based classifiers (two of which are feeded with BERT embeddings) and one end-to-end deep NN-based BERT classifier. More specifically, the following models are included in our experimental setup: (a) SVM with TF-IDF char n-grams (SVM-CHAR), (b) SVM with FASTTEXT embeddings (SVM-FAST), (c) SVM with GLOVE embeddings (SVM-GLOVE), (d) SVM with non finetuned BERT embeddings (SVM-BERT), (e) SVM with fine-tuned BERT embeddings (SVM-F-BERT), and finally (f) the deep bidirectional transformer encoder BERT (BERT). All models were used for running experiments on all four ACLED datasets.",
            "score": 0.48406401739345195,
            "section_title": "Experiments",
            "char_start_offset": 20308,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 141
                },
                {
                    "start": 142,
                    "end": 535
                },
                {
                    "start": 536,
                    "end": 608
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.4521484375
        },
        {
            "corpus_id": "257577511",
            "title": "Predicting health-related quality of life change using natural language processing in thyroid cancer",
            "text": "We used LIWC as an initial benchmark encoding method. LIWC is a dictionary-based text analysis tool widely used to ascertain the linguistic characteristics of text [15]. It assigns every word in the text to one or more categories in an internal dictionary and produces an output specifying the percentage of the total words in the text that fall into each linguistic category. LIWC does not consider the underlying structure or sequence of speech, but only its lexical contents. Though LIWC is limited by the accuracy and comprehensiveness of the dictionary they are based upon, it has been used in many studies evaluating indicators of psychosocial well-being [16] and has also served as a comparison point for deep learning methods [17][18][19]. \n\n1 If transcript 2 was not available, transcript 3 was used in its place. \n\nFor each sequence, frequencies were calculated for 9 LIWC categories of interest (Table 1), selected by an expert in communications research and medical sentiment analysis (JH) based on the research question and dataset properties. The LIWC frequencies became input features for the training of the logistic regression (LR) and support vector machine (SVM) classifiers. \n\n2.3.2. Analysis using BERT-BERT is a transformer-based language model which has achieved state-of-the-art performance on many natural language processing tasks [20]. BERT works by parsing large quantities of data and representing words and their statistical correlations with other words in a high dimensional feature space. BERT is then fine-tuned on a domain-specific corpus of text and applied to downstream tasks. In our experiments, we used the fine-tuned BERT as an encoder to extract sequence embeddings, which were then input into the classifier. Specifically, we added a linear layer followed by a sigmoid function on top of the pre-trained BERT, and they were fine-tuned together for classification. We used the BERT implementation of Wolf et al. [21]. The first token of the BERT output ([CLS]) was used as the sequence embedding, a vector with dimension 768 (Fig. 3a). An independent BERT model was fine-tuned separately for the analysis of each HRQOL measure (PCS, MCS, and average THYCA).",
            "score": 0.48284290567732335,
            "section_title": "Analysis with LIWC features-",
            "char_start_offset": 7436,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 53
                },
                {
                    "start": 54,
                    "end": 169
                },
                {
                    "start": 170,
                    "end": 376
                },
                {
                    "start": 377,
                    "end": 478
                },
                {
                    "start": 479,
                    "end": 747
                },
                {
                    "start": 750,
                    "end": 822
                },
                {
                    "start": 825,
                    "end": 1056
                },
                {
                    "start": 1057,
                    "end": 1194
                },
                {
                    "start": 1197,
                    "end": 1203
                },
                {
                    "start": 1204,
                    "end": 1362
                },
                {
                    "start": 1363,
                    "end": 1521
                },
                {
                    "start": 1522,
                    "end": 1614
                },
                {
                    "start": 1615,
                    "end": 1751
                },
                {
                    "start": 1752,
                    "end": 1906
                },
                {
                    "start": 1907,
                    "end": 1959
                },
                {
                    "start": 1960,
                    "end": 2077
                },
                {
                    "start": 2078,
                    "end": 2199
                }
            ],
            "ref_mentions": [
                {
                    "start": 661,
                    "end": 665,
                    "matchedPaperCorpusId": "23950817"
                },
                {
                    "start": 738,
                    "end": 742,
                    "matchedPaperCorpusId": "226283501"
                },
                {
                    "start": 1357,
                    "end": 1361,
                    "matchedPaperCorpusId": "52967399"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.11419677734375
        },
        {
            "corpus_id": "259187951",
            "title": "ChatGPT for Suicide Risk Assessment on Social Media: Quantitative Evaluation of Model Performance, Potentials and Limitations",
            "text": "We used pre-trained transformer-based language models to train two text classifiers. Transformers are a class of deep learning models, first introduced by Vaswani et al. (2017) in 2017.\n\nResearchers build state-of-the-art NLP models using transformer-based architectures because they can be quickly trained on large datasets and studies have shown that they are better at modeling longterm dependencies in natural language text. (Wolf et al., 2020). Moreover, the growth of pre-trained transformer-based structures has made it easier to adapt a high-capacity model trained on a large text to downstream tasks (Devlin et al., 2018;Howard and Ruder, 2018). We utilize ALBERT 6 and DistilBERT 7 language models and fine-tune them with the UMD dataset to build the classifiers. For implementation, we employed the Huggingface library , an open-source library and data science platform that provides tools to build, train and deploy ML models.\n\nThe ALBERT model was proposed by Lan et al. (2019) as a variation of BERT that is optimized in terms of memory consumption and training speed. In other words, ALBERT is a more lightweight version of BERT that maintains its high level of accuracy, making it a powerful tool for various NLP applications. The DistilBERT model was proposed by  which has %40 fewer parameters than BERT and runs %60 faster while preserving over %95 of BERT's performances.\n\nWe used the Trainer 8 class from Huggingface transformers 9 for feature-complete training in PyTorch. The hyperparameters were selected based on the default values commonly used in similar studies. The final hyperparameters used in our experiments were Learning Rate= 2e \u22125 , Batch Size = 4, Dropout Rate = 0.1, and Maximum Sequence Length = 512.",
            "score": 0.482637653562038,
            "section_title": "Fine-Tuned Classifiers",
            "char_start_offset": 13175,
            "sentence_offsets": [],
            "ref_mentions": [
                {
                    "start": 429,
                    "end": 448,
                    "matchedPaperCorpusId": "269498086"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.251708984375
        },
        {
            "corpus_id": "253015767",
            "title": "A machine learning approach to recognize bias and discrimination in job advertisements",
            "text": "In this section, we present the results of various machine learning models on the EMSCAD dataset. The dataset was divided into 80% training and 20% testing set. The evaluation metrics: accuracy, precision, recall and F1-score were computed for each model. Figure 1 presents the evaluation metrics for various classifiers with different feature sets. The results indicate that the RF classifier with BERT word embeddings as textual feature achieved the best performance. This illustrates that contextual word embedding representations such as BERT had a superior performance over the non-contextual word embeddings such as FastText and Word-2vec. We also observe that tree-based (Random Forest and Decision Tree) classifiers had a better performance in classifying biased and discriminatory language as compared to the remaining classifiers. Among the textual features, word embedding representations BERT, FastText and ELMo in combination with the RF classifier had the best performance. This was followed by FastText, ELMo and Flair word embeddings in combination with the DT classifier. \n\nWe further evaluate the various machine learning classifiers with different word embedding representations as features using tenfolds cross-validation. Figure 2 presents the macro-averages of the precision, recall and F1-score Figures 7 and 8 present the confusion matrices of the two best performing models: (1) RF classifier with FastText word embeddings and (2) RF classifier with BERT word embeddings. The results in Figs. 7 and 8 indicate that all the five classes of biased and discriminatory language were distinguishable from each other. \n\nWe observe a linear improvement in the performance of a sample of our models as training size increases in our primary experiment. We wanted to see if the behavior was by any chance related to these particular models or statistically grounded across all our implemented models. To validate this improvement, we included additional data (3000 additional job descriptions) and ran a new experiment with for the lightweight classifiers (DT, LR and NB). \n\nThe results obtained from the new experiment are shown in Fig. 9 for the regular models (80% training set and 20% test set), and Fig. 10 for the tenfold cross-validated models.",
            "score": 0.48252122184862584,
            "section_title": "Results and analysis",
            "char_start_offset": 21097,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 97
                },
                {
                    "start": 98,
                    "end": 160
                },
                {
                    "start": 161,
                    "end": 255
                },
                {
                    "start": 256,
                    "end": 349
                },
                {
                    "start": 350,
                    "end": 469
                },
                {
                    "start": 470,
                    "end": 645
                },
                {
                    "start": 646,
                    "end": 840
                },
                {
                    "start": 841,
                    "end": 987
                },
                {
                    "start": 988,
                    "end": 1088
                },
                {
                    "start": 1091,
                    "end": 1242
                },
                {
                    "start": 1243,
                    "end": 1496
                },
                {
                    "start": 1497,
                    "end": 1517
                },
                {
                    "start": 1518,
                    "end": 1636
                },
                {
                    "start": 1639,
                    "end": 1769
                },
                {
                    "start": 1770,
                    "end": 1916
                },
                {
                    "start": 1917,
                    "end": 2088
                },
                {
                    "start": 2091,
                    "end": 2267
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.77197265625
        },
        {
            "corpus_id": "248603983",
            "title": "Exploring the Limits of Lexicon-based Natural Language Processing Techniques for Measuring Engagement and Predicting MOOC's Certification",
            "text": "Our last experiment was to use non-specialized linguistically-based indicators as features for predictive models, in the hope that the language models used would detect elements or patterns in the text that would predict learner success or failure. We performed all these experiments on the corpus composed of a mix of ffl1, ffl2 and ffl3. We chose to mix the datasets in order to have a larger amount of data, which allowed to create large training, test and validation sets (for the FastText and BERT models). Using as input features the values given by the T F.IDF method, we obtain the confusion matrix given in Fig- ure 4b. Figures 5a and 5b give respectively the confusion matrices obtained on the test sets using the Fast-Text and BERT models. Figure 4a gives for comparison the results obtained with the specific features as input, i.e. the same linguistic features as those used in the second experiment. The table 6 gives the calculations of accuracy, precision, recall and F1-score in each configuration. The model using linguistically-based features performs very poorly in prediction, with an F1-score close to 0. In the same way as in the first experiment, we observe that this model classifies almost all the items in the \"failure\" category (0). The model taking T F.IDF features as input makes more correct predictions of student success (true positives), but these pre- dictions account for less than half of the successful students, with the rest incorrectly predicted as failing. \n\nAmong these four models taking linguisticallybased features only as input, the best prediction results are obtained by the models based on FastText and BERT word embeddings. These two models have very similar results, with slightly higher precision for BERT and, conversely, slightly higher recall for FastText. However, even though the results are better compared to the other models, they still perform poorly, with many misclassified items. This makes their contribution uninteresting compared to the models taking event-based indicators as features.",
            "score": 0.48207925054304834,
            "section_title": "Third Experiment: T F.IDF and Word Embeddings",
            "char_start_offset": 21748,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 248
                },
                {
                    "start": 249,
                    "end": 339
                },
                {
                    "start": 340,
                    "end": 511
                },
                {
                    "start": 512,
                    "end": 628
                },
                {
                    "start": 629,
                    "end": 750
                },
                {
                    "start": 751,
                    "end": 913
                },
                {
                    "start": 914,
                    "end": 1015
                },
                {
                    "start": 1016,
                    "end": 1260
                },
                {
                    "start": 1261,
                    "end": 1498
                },
                {
                    "start": 1501,
                    "end": 1674
                },
                {
                    "start": 1675,
                    "end": 1812
                },
                {
                    "start": 1813,
                    "end": 1944
                },
                {
                    "start": 1945,
                    "end": 2054
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.52294921875
        },
        {
            "corpus_id": "233423520",
            "title": "Evaluating document representations for content-based legal literature recommendations",
            "text": "Regarding the word vector-based methods, we see that the methods which are trained on the legal corpus (Paragraph Vectors, fastText Legal , GloVe Legal ) perform similarly well with a minor advantage by fastText Legal . Moreover, there is a margin between the generic and legal word vectors even though the legal word vectors are trained on a small corpus compared to ones from the generic vectors. The advantage of Paragraph Vectors over TF-IDF is consistent with the results from Mandal et al. [28]. Limiting the document length to 512 or 4096 decreases the effectiveness of fastText Legal . A limit of 512 tokens decreases the MAP score to 59% compared to all tokens on Open Case Book. With 4096 tokens, the performance decline is only minor (90% compared to all tokens). The token limitation effect is also larger on Open Case Book than Wikisource. The 4096 tokens version of fastText Legal even outperforms all Transformer methods.\n\nLongformer-large is the best Transformer for Open Case Book with 0.031 MAP. For Wikisource, Legal-AUEB-BERT achieves the highest MAP of 0.022, closely followed by Legal-JHU-BERT. The Longformer's theoretical advantage of processing 4096 instead of 512 tokens does not lead to better results for Wikisource, for which even BERT scores the same MAP of 0.018. We generally observe that large models outperform their base counterparts 7 . Likewise, RoBERTa has higher scores than BERT as Liu et al. [27] suggested. From the Transformers category, Sentence Transformers yield the worst results. We assume that fine-tuning on the similarity datasets like NLI or STSB does not increase the performance since the models do not generalize well to other domains. However, the language model fine-tuning from Legal-JHU-BERT and Legal-AUEB-BERT  In the category of hybrid methods, the combination of text and citations improves the performance. For Open Case Book, the score summation Poincar\u00e9 + fastText Legal has the same MAP of 0.05 as fastText Legal but a higher MRR of 0.746. The MRR of Poincar\u00e9 + fastText Legal is even higher than the MRR of its sub-",
            "score": 0.48059433758182774,
            "section_title": "Quantitative Evaluation",
            "char_start_offset": 18331,
            "sentence_offsets": [],
            "ref_mentions": [
                {
                    "start": 496,
                    "end": 500,
                    "matchedPaperCorpusId": "44070470"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.20654296875
        },
        {
            "corpus_id": "233189561",
            "title": "Implicitly Abusive Comparisons \u2013 A New Dataset and Linguistic Analysis",
            "text": "As a supervised classifier, we chose BERT-Large (Devlin et al., 2019). We initially experimented with two versions: one in which we fine-tune the model by adding a layer on top of the pre-trained model and a SVM (Joachims, 1999) that is trained on the BERT embeddings of the final layer. Since we did not measure any statistically significant difference between these models, we decided in favor of SVM due to its simplicity. We carry out a 5fold cross validation. The folds comprise mutually exclusive patterns (Table 1). Thus test instances always comprise patterns not observed in the training data. We consider this the most difficult and realistic scenario. We report macro-average precision, recall and F1-score. (The supplementary notes contain more details regarding all classifiers of our experiments.) \n\nAs baselines, we consider a majority classifier, a random classifier and two classifiers trained on fastText: one without and one with pre-trained embeddings (Common Crawl). As an upper bound we also provide a human baseline in which we randomly sampled the judgment of one individual annotator from the crowdsourced annotation. This upper bound may notably differ from the gold standard label since the latter benefited from being calculated from the majority of 5 annotators. \n\nIn order to demonstrate the importance of cleaning/debiasing the dataset and show that, otherwise classification performance will be unrealistically high, we also train a classifier on a biased comparison dataset. For that, we sampled a set of the identical size from the original data we collected via crowdsourcing ( \u00a73.2) as our final debiased dataset (i.e. 1000 comparisons) but skipped the data cleaning step, particularly the steps on balancing the pattern distribution and removing patterns that are highly skewed towards either of the two classes (Table 2). We also arranged the folds at random, so that patterns in the test data could also be observed in the training data. Thus a classifier could benefit from memorizing biased patterns. \n\nTable 5 shows the performance of the different classifiers. FastText strongly benefits from the pretrained embeddings and already outperforms the other baselines by a large degree. BERT outperforms fastText.",
            "score": 0.48051836594576036,
            "section_title": "Classification Performance",
            "char_start_offset": 23452,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 70
                },
                {
                    "start": 71,
                    "end": 287
                },
                {
                    "start": 288,
                    "end": 425
                },
                {
                    "start": 426,
                    "end": 464
                },
                {
                    "start": 465,
                    "end": 522
                },
                {
                    "start": 523,
                    "end": 602
                },
                {
                    "start": 603,
                    "end": 662
                },
                {
                    "start": 663,
                    "end": 718
                },
                {
                    "start": 719,
                    "end": 811
                },
                {
                    "start": 814,
                    "end": 987
                },
                {
                    "start": 988,
                    "end": 1142
                },
                {
                    "start": 1143,
                    "end": 1291
                },
                {
                    "start": 1294,
                    "end": 1507
                },
                {
                    "start": 1508,
                    "end": 1654
                },
                {
                    "start": 1655,
                    "end": 1859
                },
                {
                    "start": 1860,
                    "end": 1976
                },
                {
                    "start": 1977,
                    "end": 2041
                },
                {
                    "start": 2044,
                    "end": 2103
                },
                {
                    "start": 2104,
                    "end": 2224
                },
                {
                    "start": 2225,
                    "end": 2251
                }
            ],
            "ref_mentions": [
                {
                    "start": 48,
                    "end": 69,
                    "matchedPaperCorpusId": "52967399"
                },
                {
                    "start": 212,
                    "end": 228,
                    "matchedPaperCorpusId": "61116019"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.626953125
        },
        {
            "corpus_id": "264935645",
            "title": "ChineseWebText: Large-scale High-quality Chinese Web Text Extracted with Effective Evaluation Model",
            "text": "To further enhance data processing efficiency and reduce hardware resource requirements, in this paper we also develop a text evaluation model based on FastText3 in addition to BERTEval. FastText is libiary for efficient learning of word In the last section, we have built a BERT-based evaluation model BERTEval which performs a good performance on the quality evaluation of Chinese texts. With this model, we can classify the preprocessed web data into high-quality texts (positive) and low-quality texts (negative). Inspired by the idea of knowledge distillation, we will use these classified texts to guide the training of our FastText model. In our approach, we select 400,000 high-quality texts classified by BERTEval as our positive data, while choosing 1,600,000 low-quality texts as our negative data. In order to increase the diversity of training data, our positive data also include some high-quality Chinese data from some other websites and books, such as Baidubaike, Zhihu, Cbook, ChinaNews and so on. These data have been manually proofread and processed. In this way, we can build a good training dataset with 3200K samples. As shown in Table 3, it presents the composition of our training data. \n\nAfter collecting these training data, we will use a word segmentation tool to process all the texts, and then input the processed data into FastText to train the model. Through this approach, we can obtain a more efficient quality evaluation model.",
            "score": 0.4797060675838384,
            "section_title": "FastText-based Evaluation Model",
            "char_start_offset": 20656,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 186
                },
                {
                    "start": 187,
                    "end": 389
                },
                {
                    "start": 390,
                    "end": 517
                },
                {
                    "start": 518,
                    "end": 645
                },
                {
                    "start": 646,
                    "end": 809
                },
                {
                    "start": 810,
                    "end": 1015
                },
                {
                    "start": 1016,
                    "end": 1070
                },
                {
                    "start": 1071,
                    "end": 1140
                },
                {
                    "start": 1141,
                    "end": 1211
                },
                {
                    "start": 1214,
                    "end": 1382
                },
                {
                    "start": 1383,
                    "end": 1462
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.6123046875
        },
        {
            "corpus_id": "248780276",
            "title": "DepressionOne@LT-EDI-ACL2022: Using Machine Learning with SMOTE and Random UnderSampling to Detect Signs of Depression on Social Media Text.",
            "text": "The baselines used are: SVM with TF-IDF Term frequency and inverse document frequency-based vectorization is used to represent the text data, and the support vector machine is used to classify the data. \n\nCNN (Kim, 2014) This convolutional neural network-based text classifier is trained by considering pre-trained FastText word vectors. \n\nBi-LSTM (Hochreiter and Schmidhuber, 1997) A two-layer, bi-directional LSTM text classifier with pre-trained FastText word embeddings as input was considered for the task of text classification. \n\nPre-trained BERT (Devlin et al., 2018) A pretrained BERT model with a feed-forward network for classification",
            "score": 0.4790127547808055,
            "section_title": "Baselines",
            "char_start_offset": 5851,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 202
                },
                {
                    "start": 205,
                    "end": 337
                },
                {
                    "start": 340,
                    "end": 534
                },
                {
                    "start": 537,
                    "end": 646
                }
            ],
            "ref_mentions": [
                {
                    "start": 209,
                    "end": 220,
                    "matchedPaperCorpusId": "9672033"
                },
                {
                    "start": 348,
                    "end": 382,
                    "matchedPaperCorpusId": "1915014"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.286376953125
        },
        {
            "corpus_id": "271582354",
            "title": "AN AUTOMATED NEW APPROACH IN FAST TEXT CLASSIFICATION: A CASE STUDY FOR KURDISH TEXT",
            "text": "In natural language processing, reduced performance in text classification challenges the accuracy of neural networks. To address this issue, the Facebook research team developed FastText. FastText is a library designed for both supervised and unsupervised learning. Supervised learning can be applied to tasks such as text classification, while unsupervised learning is utilized for learning word embedding from the training dataset. Two primary data sources, Wikipedia and Common Crawl corpus, contribute to collecting the training data for FastText. It is worth noting that FastText offers pre-trained word vectors for 157 different languages, comprising a vast number of vocabulary (600 billion words) from 2 million combined texts, each represented in 300 dimensions (Umer et al., 2023). The structure of FastText closely resembles that of Continuous Bag of Words (CBOW). However, the primary distinction lies in FastText's architecture compared to CBOW, as CBOW utilizes intermediate words rather than labels as shown in Figure . 1 As shown in Table 1, the document is ready for the input layer. After converting the document format, another step is done which is representing words in the documents. The word representation of FastText is different from other models such as word2vec. In word2vec each word is represented as a bag of words while in FastText each word is represented as a bag of character n-gram which is generated vector for unknown words to improve generalization. For example, in character n-gram architecture the word \u202b)\u062f\u06a9\u062a\u06c6\u0631(\u202c is: \u202b\u06c6\u0631<\u202c \u202b\u062a\u06c6\u0631,\u202c \u202b\u06a9\u062a\u06c6,\u202c \u202b\u062f\u06a9\u062a,\u202c \u202b>\u062f\u06a9,\u202c when (n-gram=3) The increase in character n-grams represents a significant improvement over word n-grams and helps address \"out of vocabulary\" errors, especially in high-dimensional feature spaces (Khomsah et al., 2022). \n\nThe hidden layer, which averages several feature vectors, constructs the Huffman tree. This tree is utilized to determine the most probable function based on the weight and parameters of the class, and it serves output purposes since calculating the tag based on the Huffman coding path can significantly reduce computational load. The SoftMax function is employed in FastText to estimate the likelihood distribution of classes.",
            "score": 0.4787777963044876,
            "section_title": "FASTTEXT ARCHITECTURE",
            "char_start_offset": 6743,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 118
                },
                {
                    "start": 119,
                    "end": 188
                },
                {
                    "start": 189,
                    "end": 266
                },
                {
                    "start": 267,
                    "end": 434
                },
                {
                    "start": 435,
                    "end": 552
                },
                {
                    "start": 553,
                    "end": 792
                },
                {
                    "start": 793,
                    "end": 876
                },
                {
                    "start": 877,
                    "end": 1101
                },
                {
                    "start": 1102,
                    "end": 1206
                },
                {
                    "start": 1207,
                    "end": 1291
                },
                {
                    "start": 1292,
                    "end": 1489
                },
                {
                    "start": 1490,
                    "end": 1813
                },
                {
                    "start": 1816,
                    "end": 1902
                },
                {
                    "start": 1903,
                    "end": 2147
                },
                {
                    "start": 2148,
                    "end": 2244
                }
            ],
            "ref_mentions": [
                {
                    "start": 772,
                    "end": 791,
                    "matchedPaperCorpusId": "251829920"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.193359375
        },
        {
            "corpus_id": "258784982",
            "title": "Combating the COVID-19 infodemic using Prompt-Based curriculum learning",
            "text": "on the general BooksCorpus and   Wikipedia. Consequently, the semantic difference between the corpus of COVID-19-related information and the corpus used to train BERT was significant; the former is on a small to medium scale, which was inadequate to fine-tune BERT to the specific COVID-19 domain. Among other results, the text length of tweets was found to be relatively short. NLP research has shown that the technique usually performs worse for short texts on some tasks (including name entity recognition) due to the lack of sufficient contextual information in short texts (Giuseppe et al., 2017). As a result, the power of LSTM and CNN to extract semantic information from the text cannot be fully exploited. In our experiment, their performances were even worse than that of the simple fastText, which generally does not rely on a deep neural network structure and thus requires less data to train.\n\nAlthough fastText provided the second-best overall performance, it also demonstrated problems. As mentioned above, class imbalance is extreme in the fight agaisnt infodemics, and the texts in our experiment-tweets-are short and contain many informal expressions, which makes feature extraction especially challenging. With normal classifiers, such as fastText, the classes with more training data usually dominate the classification process, and the testing samples tend to be categorized into the dominant class. As can be seen in Fig. 3(a), the confusion matrices of fastText's multi-classification result for Q4 reveal that all of the testing data were classified into class 1-the dominant class. In contrast, our prompt-based curriculum learning approach mitigated that problem through its superior ability to employ the PLM (i. e., BERT) to extract semantic and syntactic features from short texts. Thus, the testing data could be more accurately assigned to the correct classes, as shown in Fig. 3(b). However, for some classification tasks with exceptionally skewed label distribution, the proposed method did not exhibit superior performance. For example, it performed equally well as fastText and BERT on Q5 \u2032 s multi-classification task. The reason for this is that all three methods categorized all testing samples to the same dominant class and thus yielded the same results. Nevertheless, with more balanced data distribution, this problem could be mitigated",
            "score": 0.47744963727047085,
            "section_title": "Overall performance",
            "char_start_offset": 34064,
            "sentence_offsets": [],
            "ref_mentions": [
                {
                    "start": 578,
                    "end": 601,
                    "matchedPaperCorpusId": "26185046"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.50146484375
        },
        {
            "corpus_id": "252666305",
            "title": "Using Transformer models for gender attribution in Polish",
            "text": "1) FastText: FastText [15] is a shallow neural network library created for fast text classification model training and evaluation. We used a supervised setting with hyperparameter tuning, the word embeddings were initialized randomly. The best result was obtained with wordNgrams set to 2, word dimension set to 156, and context size window set to 5. \n\n2) LSTM: Long Short Term Memory Networks [13] were used to obtain a state-of-the-art results on most NLP tasks before the era of Transformer language models [7]. In our tasks, for bidirectional LSTM, SentencePiece [19] tokenization performs better than word-level lowercase tokenization. Vocab size 50k was used with randomly initialized embeddings of size 100. We tried embedding size 300, but resulted in slightly worse classifier quality. We used one layer of 256 units, trained with Adam [16] optimizer with learning rate 0.001. The batch size used for training was 400 and sequences were trimmed and padded to 256 tokens. \n\n3) Transformer: In the last time Transformer [26] and its modification like BERT [7], RoBERTa [20] or XLM-R [4] achieve state-of-the-art in the benchmarks such as GLUE [29] or SuperGLUE [28] benchmark. Most often used bidirectional Transformers are pre-trained on huge amounts of monolingual data in the Masked Language Model (MLM) process, where the model learns a bidirectional representation of tokens. Next, pre-trained models are finetuned to the specific task. This process reduces the time to train a new model from scratch and can be easily adapted to other tasks. In our case, the downstream task is classification, where the model uses a special token ([CLS], classification token), which represents the whole sentence and helps achieve better results. \n\nWe train self-contained classifier based on the RoBERTa model in two ways: with pre-training and without pre-training (train classifier from the scratch) stage. We only used the data that was available in the HSSS challenge to avoid any data leaks in the other data sets.",
            "score": 0.4766741878393186,
            "section_title": "C. Neural Methods (self-contained)",
            "char_start_offset": 7626,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 130
                },
                {
                    "start": 131,
                    "end": 234
                },
                {
                    "start": 235,
                    "end": 350
                },
                {
                    "start": 353,
                    "end": 514
                },
                {
                    "start": 515,
                    "end": 640
                },
                {
                    "start": 641,
                    "end": 714
                },
                {
                    "start": 715,
                    "end": 794
                },
                {
                    "start": 795,
                    "end": 885
                },
                {
                    "start": 886,
                    "end": 979
                },
                {
                    "start": 982,
                    "end": 1183
                },
                {
                    "start": 1184,
                    "end": 1387
                },
                {
                    "start": 1388,
                    "end": 1448
                },
                {
                    "start": 1449,
                    "end": 1554
                },
                {
                    "start": 1555,
                    "end": 1744
                },
                {
                    "start": 1747,
                    "end": 1907
                },
                {
                    "start": 1908,
                    "end": 2018
                }
            ],
            "ref_mentions": [
                {
                    "start": 394,
                    "end": 398,
                    "matchedPaperCorpusId": "1915014"
                },
                {
                    "start": 567,
                    "end": 571,
                    "matchedPaperCorpusId": "52051958"
                },
                {
                    "start": 1150,
                    "end": 1154,
                    "matchedPaperCorpusId": "5034059"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.26611328125
        },
        {
            "corpus_id": "271691319",
            "title": "Sentiment Analysis: Predicting Product Reviews for E-Commerce Recommendations Using Deep Learning and Transformers",
            "text": "FastText: a Word2Vec plugin that breaks down words into n-grams, or smaller units, like \"application\" into \"app,\" with the goal of teaching word morphology. Every word in the text is converted by the model into a bag of embedded vectors. \n\nPolysemous words might provide difficulties when using Word2Vec and FastText since they always assign the same embedding vector, regardless of context. Researchers have been using transformer-based embeddings, such as BERT and its derivatives, to address this problem. Word contexts from BooksCorpus and Wikipedia were used to pre-train models like BERT, and the resulting embeddings were then used in classifiers to make predictions. These models have demonstrated state of the art performance in natural language processing tasks by providing contextualized word embeddings. \n\nThe bidirectional transformer used in the BERT-based model has been pre-trained on large amounts of unlabeled textual data to provide a language representation that can be tailored to different classification tasks. One noteworthy variation is RoBERTa, which Facebook unveiled. It is an improved version of BERT with increased processing power and expanded prediction capabilities that can handle larger amounts of data. Furthermore, ALBERT, a condensed and effective BERT variant that is far smaller than BERT, was created by Google and Toyota. In particular, two variants of a BERT-based model, RoBERTa and ALBERT, were studied in this work.",
            "score": 0.47606143898843756,
            "section_title": "\u2022",
            "char_start_offset": 30142,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 156
                },
                {
                    "start": 157,
                    "end": 237
                },
                {
                    "start": 240,
                    "end": 391
                },
                {
                    "start": 392,
                    "end": 508
                },
                {
                    "start": 509,
                    "end": 674
                },
                {
                    "start": 675,
                    "end": 816
                },
                {
                    "start": 819,
                    "end": 1034
                },
                {
                    "start": 1035,
                    "end": 1096
                },
                {
                    "start": 1097,
                    "end": 1239
                },
                {
                    "start": 1240,
                    "end": 1364
                },
                {
                    "start": 1365,
                    "end": 1462
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.80615234375
        },
        {
            "corpus_id": "268536862",
            "title": "Adaptive Ensembles of Fine-Tuned Transformers for LLM-Generated Text Detection",
            "text": "The classifiers are pre-trained transformer-based LMs, which were subsequently fine-tuned for the LLM-generated text detection task.As illustrated in Fig. 3, each classifier is associated with a classification head, comprising dropout and dense layers, integrated with the backbone of a transformerbased LM.We employed five distinct transformer-based LMs as the backbones for five individual classifiers.The pre-trained layers of the LM are frozen (not updated during training) to preserve their learned features.Only the classifier \"head\" (the last layer) is trained.In addition, we use cross-entropy loss and Adam optimizer with a learning rate of 5e-4.During model training, each text sample is truncated to a maximum of 256 The summary of the pre-trained LMs is provided below: 1) DistilBert: DistilBERT is developed through the distillation of the BERT base model, is notably smaller and faster [64].We use the \"distil bert base en uncased\" as pretrained backbone weight.2) DeBERTaV3: DeBERTaV3 is an enhanced pretrained language model that outperforms its predecessor by implementing replaced token detection (RTD) and introducing a gradient-disentangled embedding sharing method [65].We use the \"deberta v3 base en\" as pretrained backbone weight.\n\n3) FNet: FNet introduces a novel approach to speed up Transformer encoders by replacing self-attention sublayers with linear transformations, achieving 92-97% of BERT's accuracy on the GLUE benchmark while training significantly faster [66].We use the \"f net base en\" as pretrained backbone weight.4) Albert: Albert presents two parameter-reduction techniques that lower memory usage and accelerate training of BERT, achieving superior scaling and performance [67].We use the \"albert base en uncased\" as pretrained backbone weight.5) XLMRoberta: XLMRoberta is a large-scale multilingual transformer-based LM pretrained on a diverse set of one hundred languages using extensive CommonCrawl data [68].",
            "score": 0.47576499598692706,
            "section_title": "A. Single Classifier Detection",
            "char_start_offset": 10151,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 132
                },
                {
                    "start": 132,
                    "end": 307
                },
                {
                    "start": 307,
                    "end": 404
                },
                {
                    "start": 404,
                    "end": 513
                },
                {
                    "start": 513,
                    "end": 568
                },
                {
                    "start": 568,
                    "end": 655
                },
                {
                    "start": 655,
                    "end": 905
                },
                {
                    "start": 905,
                    "end": 976
                },
                {
                    "start": 976,
                    "end": 1191
                },
                {
                    "start": 1191,
                    "end": 1253
                },
                {
                    "start": 1255,
                    "end": 1496
                },
                {
                    "start": 1496,
                    "end": 1553
                },
                {
                    "start": 1553,
                    "end": 1720
                },
                {
                    "start": 1720,
                    "end": 1786
                },
                {
                    "start": 1786,
                    "end": 1954
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.26318359375
        },
        {
            "corpus_id": "221319787",
            "title": "Language Models and Word Sense Disambiguation: An Overview and Analysis",
            "text": "Our experiments with BERT used the Transformers framework (v2.5.1) developed by Wolf et al. (2019), and we used the uncased pre-trained base and large models released by Devlin et al. (2019). Following previous feature extraction works (including our experiment in Section 3.4.1), with CoarseWSD-20 we also average subword representations and use the sum of the last four layers when extracting contextual embeddings. For fine-tuning experiments, we used a concatenation of the average embedding of target word's sub-words with the embedding of the [CLS] token, and fed them to a classifier. We used the same default hyper-parameter configuration for all the experiments. Given the fluctuation of results with fine-tuning, all the experiments are based on the average of three independent runs. Our experiments with FastText used the official package 18 (v0.9.1), with FastText-Base corresponding to the default supervised classification pipeline using randomly-initialized vectors, and FastText-Crawl corresponding to the same pipeline but starting with pre-trained 300-dimensional vectors based on Common Crawl. Following Joulin et al. ( 2017), classification with FastText is performed using multinomial logistic regression and averaged sub-word representations. \n\nEvaluation measures. In a classification setting, the performance of a model is measured by various metrics, among which precision, recall and F-score are the most popular. Let T P i (true-positive) and F P i (false-positive) be the number of instances correctly / incorrectly classified as class c i respectively. Also, let T N i (true-negative) and F N i (false-negative) be the number of instances correctly / incorrectly classified as class c j for any j = i. Therefore, for class c i , precision P i and recall R i are defined as follows: \n\nIn other words, precision is the fraction of relevant instances among the retrieved instances, while recall is the fraction of the total number of relevant instances that were actually retrieved. The F-score F i for class c i is then defined as the harmonic mean of its precision and recall values:",
            "score": 0.47545443975092055,
            "section_title": "Configuration.",
            "char_start_offset": 40323,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 191
                },
                {
                    "start": 192,
                    "end": 280
                },
                {
                    "start": 281,
                    "end": 417
                },
                {
                    "start": 418,
                    "end": 591
                },
                {
                    "start": 592,
                    "end": 671
                },
                {
                    "start": 672,
                    "end": 794
                },
                {
                    "start": 795,
                    "end": 1113
                },
                {
                    "start": 1114,
                    "end": 1265
                },
                {
                    "start": 1268,
                    "end": 1288
                },
                {
                    "start": 1289,
                    "end": 1440
                },
                {
                    "start": 1441,
                    "end": 1582
                },
                {
                    "start": 1583,
                    "end": 1731
                },
                {
                    "start": 1732,
                    "end": 1811
                },
                {
                    "start": 1814,
                    "end": 2009
                },
                {
                    "start": 2010,
                    "end": 2112
                }
            ],
            "ref_mentions": [
                {
                    "start": 170,
                    "end": 190,
                    "matchedPaperCorpusId": "52967399"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.6630859375
        },
        {
            "corpus_id": "230611802",
            "title": "Comparison of Deep Learning Models and Various Text Pre-Processing Techniques for the Toxic Comments Classification",
            "text": "GloVe-is one of the newest methods for calculating the vector representation of words. However, this approach does not use the whole corpus. It is learned only based on global statistics on the occurrence of words in a current context. The method captures various linguistic patterns and can successfully solve problems based on the principle of analogy [59]. \n\nfastText-is a library created by Facebook's research team to learn and calculate the word representation and sentence classification. Its principle is to assign a vector representation to n-grams of characters that contain individual words [60]. \n\nWe used GloVe embeddings pre-trained on Common Crawl (300 dimensions) and Twitter (200 dimensions), for the fastText we used pre-trained word vectors for 157 languages, trained on Common Crawl and Wikipedia (also in dimension 300). \n\nCompared to word embeddings, BERT model includes an attention mechanism which is able to learn contextual relations between words in text. BERT consists of an encoder which processes the text input and decoder used to perform the prediction. Since BERT's goal is to generate a language model, only the encoder mechanism is necessary. BERT, which uses the Transformer encoder is able to learn the context of a word based on its entire surrounding (both left and right context of the words). When comparing to other directional models, that read the text input sequentially, transformer models read it as an entire sequence at once (from that point of view, it can be considered as non-directional). We used following tokenizers from Transformers (https://huggingface.co/transformers/: \n\n\u2022 BERT tokenizer -bert-base and bert-large cased/uncased tokenizer-based on word piece; \u2022 DistilBERT Tokenizer-distilbert-base-cased tokenizer-is identical to BertTokenizer and runs end-to-end tokenization: punctuation splitting and wordpiece; \u2022 XLNet tokenizer-xlnet-base-cased tokenizer-based on sentence piece.",
            "score": 0.47531748999588874,
            "section_title": "\u2022",
            "char_start_offset": 28601,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 86
                },
                {
                    "start": 87,
                    "end": 140
                },
                {
                    "start": 141,
                    "end": 235
                },
                {
                    "start": 236,
                    "end": 359
                },
                {
                    "start": 362,
                    "end": 495
                },
                {
                    "start": 496,
                    "end": 607
                },
                {
                    "start": 610,
                    "end": 841
                },
                {
                    "start": 844,
                    "end": 982
                },
                {
                    "start": 983,
                    "end": 1085
                },
                {
                    "start": 1086,
                    "end": 1177
                },
                {
                    "start": 1178,
                    "end": 1333
                },
                {
                    "start": 1334,
                    "end": 1541
                },
                {
                    "start": 1542,
                    "end": 1627
                },
                {
                    "start": 1630,
                    "end": 1943
                }
            ],
            "ref_mentions": [
                {
                    "start": 354,
                    "end": 358,
                    "matchedPaperCorpusId": "1957433"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.4091796875
        },
        {
            "corpus_id": "257806425",
            "title": "A Stacking-based Efficient Method for Toxic Language Detection on Live Streaming Chat",
            "text": "We did experiments with the annotated dataset in \u00a73 to show the effectiveness and the efficiency of our method. Model trainings were conducted on Intel(R) Xeon(R) Gold 6148 CPU @ 2.40GHz with a single processor. RAM size is 26GB. Cache memory consists of 32KB for L1d, 32KB for L1i, 1024KB for L2, and 28,160KB for L3. The model implementation is as follows: \n\nfastText classification To get the word representations, we create a pre-trained model using the fastText module (Bojanowski et al., 2017) with all the comments in \u00a73. For pre-processing, this data is tokenized by the Japanese morphological analyzer MeCab (Kudo, 2006). Additionally, a word was lemmatized and half-width characters were converted to full-width. Hyperparameters for the pre-training are as follows: The number of dimensions for word representation is 300. We used skip-gram to train word representation. The threshold \u03b8 in \u00a74.2 was determined with development set (\u03b8 = 0.98). \n\nSVM We calibrated the prediction results using a calibrated ClassifierCV 3 provided by scikit-learn to remove the effect of bias in the unbalanced data. \n\nLightGBM This model was optimized with optuna (Akiba et al., 2019). \n\nAs baseline models, we used each component of our method and existing pre-trained BERT-based models. We used the BERT-base4 and BERT-large5 models pre-trained from Japanese Wikipedia. Additionally, it is essential to compare with light BERT models for a fair evaluation of inference speed. Over the last couple of years, variants of BERT have been proposed to make the model size light and inference speed efficient. Specifically, we tried Distil-BERT6 (Sanh et al., 2019), ALBERT7 (Lan et al., 2019), and Poor-Man's BERT (Sajjad et al., 2020). Poor-Man's BERT is the method that removes some layers from an original BERT. Their experimental results showed that dropping the top layers works consistently well across different tasks when dropping 4 and 6 layers.",
            "score": 0.4740282173596999,
            "section_title": "Settings",
            "char_start_offset": 11210,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 111
                },
                {
                    "start": 112,
                    "end": 211
                },
                {
                    "start": 212,
                    "end": 229
                },
                {
                    "start": 230,
                    "end": 318
                },
                {
                    "start": 319,
                    "end": 358
                },
                {
                    "start": 361,
                    "end": 528
                },
                {
                    "start": 529,
                    "end": 630
                },
                {
                    "start": 631,
                    "end": 722
                },
                {
                    "start": 723,
                    "end": 832
                },
                {
                    "start": 833,
                    "end": 880
                },
                {
                    "start": 881,
                    "end": 952
                },
                {
                    "start": 955,
                    "end": 1107
                },
                {
                    "start": 1110,
                    "end": 1177
                },
                {
                    "start": 1180,
                    "end": 1280
                },
                {
                    "start": 1281,
                    "end": 1363
                },
                {
                    "start": 1364,
                    "end": 1469
                },
                {
                    "start": 1470,
                    "end": 1596
                },
                {
                    "start": 1597,
                    "end": 1724
                },
                {
                    "start": 1725,
                    "end": 1802
                },
                {
                    "start": 1803,
                    "end": 1942
                }
            ],
            "ref_mentions": [
                {
                    "start": 474,
                    "end": 499,
                    "matchedPaperCorpusId": "207556454"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.418212890625
        },
        {
            "corpus_id": "266977420",
            "title": "Stylometry Analysis of Multi-authored Documents for Authorship and Author Style Change Detection",
            "text": "For text classification in all the tasks, we rely on various transformer-based language models including BERT (Bidirectional Encoder Representations from Transformers) and its multiple variations, such as DistilBERT, ALBERT, Roberta (Robustly Optimized BERT), and XML-Roberta. The selection of these algorithms is made on the basis of their performances in similar text classification applications, such as natural disaster analysis [21], water quality analysis in text [2], and healthcare [1]. A brief overview of these transformers-based models is provided as follows. \n\n-BERT: BERT [8] is one of the pre-trained transformer-based models that have been proven very effective in text analysis. It has been trained on a large collection of Wikipedia unlabeled text and can be fine-tuned for text classification in different application domains. One of the key characteristics of BERT includes its ability to extract contextual information/features through bi-directional learning (i.e., left \u2192 right and right\u2192 left). Despite being very effective in text analysis BERT also has some limitations. For instance, it is computationally intensive. The literature also reports that the model has not been efficiently trained. To overcome these limitations, several variants of BERT have been introduced in the literature, such as RoBERTa and Distil BERT. Moreover, the original model is available in two different configurations having a different number of layers and attention heads. We employed the version of the model with 12 layers and attention heads and 110 million parameters. We used the Tensorflow implementation of the model with Adam optimizer and binary cross entropy loss function. We trained the model for 5 epochs with a learning rate of 1e \u2212 5. -DistilBERT: DistilBERT [22] is one of the variants of BERT designed to overcome some limitations of the original implementation. It provides a computationally lighter and faster solution for text analysis. The model is based on the concept of knowledge distillation [11], during the pre-trained phase, resulting in a 40% reduction in model size. While the overall architecture remains similar to the original BERT, DistilBERT removes token-type embeddings and the pooling layer, thereby reducing the number of parameters and model size.",
            "score": 0.47400111121976113,
            "section_title": "Feature Extraction and Classification",
            "char_start_offset": 14789,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 276
                },
                {
                    "start": 277,
                    "end": 494
                },
                {
                    "start": 495,
                    "end": 570
                },
                {
                    "start": 573,
                    "end": 694
                },
                {
                    "start": 695,
                    "end": 844
                },
                {
                    "start": 845,
                    "end": 1017
                },
                {
                    "start": 1018,
                    "end": 1095
                },
                {
                    "start": 1096,
                    "end": 1142
                },
                {
                    "start": 1143,
                    "end": 1219
                },
                {
                    "start": 1220,
                    "end": 1348
                },
                {
                    "start": 1349,
                    "end": 1479
                },
                {
                    "start": 1480,
                    "end": 1579
                },
                {
                    "start": 1580,
                    "end": 1690
                },
                {
                    "start": 1691,
                    "end": 1886
                },
                {
                    "start": 1887,
                    "end": 1963
                },
                {
                    "start": 1964,
                    "end": 2103
                },
                {
                    "start": 2104,
                    "end": 2294
                }
            ],
            "ref_mentions": [
                {
                    "start": 433,
                    "end": 437,
                    "matchedPaperCorpusId": "58004701"
                },
                {
                    "start": 470,
                    "end": 473,
                    "matchedPaperCorpusId": "250402891"
                },
                {
                    "start": 490,
                    "end": 493,
                    "matchedPaperCorpusId": "247504387"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.32666015625
        },
        {
            "corpus_id": "227126558",
            "title": "Studying Taxonomy Enrichment on Diachronic WordNet Versions",
            "text": "We test our methods on non-contextualised fastText (Bojanowski et al., 2017) and contextualised BERT (Devlin et al., 2019) embeddings. We choose fastText embeddings because pre-trained fastText models are easy to deploy, and do not require additional data or training for the out-of-vocabulary words. In this paper we use the fastText embeddings from the official website3 for both English and Russian, trained on Common Crawl from 2019 and Wikipedia CC including lexicon from the previous periods as well. \n\nWhile fastText embeddings can be generated for individual words, BERT requires a context for a word (i.e. a sentence containing it) to generate its embedding. For experiments with English datasets, we extract contexts from Wikipedia. For the experiments with Russian, we use a news corpus provided by the organisers of RUSSE'2020,4 which contains at least 50 occurrences for each word in the dataset. \n\nWe use the pre-trained BERT-base model for English from (Devlin et al., 2019). For Russian, we utilize RuBERT model from (Kuratov and Arkhipov, 2019), which proved to outperform the Multilingual BERT from the original paper. To compute BERT embeddings for orphans and synsets, we extract sentences containing them from the corresponding corpora. If the words are absent in the corpora, we computed the average of lemmas without context for synsets and the embedding of the input word without context. We also averaged word-pieces for the OOV words. We lemmatise corpora with UDPipe (Straka and Strakov\u00e1, 2017) to be able to find not only exact word matches but also their grammatical forms. We rely on UDPipe as it supports many languages and shows reasonable performance on our data. In case of multiple occurrences of the same orphan, we average the retrieved contextualised embeddings.",
            "score": 0.47388706845930895,
            "section_title": "Pre-trained Embeddings",
            "char_start_offset": 14357,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 134
                },
                {
                    "start": 135,
                    "end": 300
                },
                {
                    "start": 301,
                    "end": 506
                },
                {
                    "start": 509,
                    "end": 667
                },
                {
                    "start": 668,
                    "end": 742
                },
                {
                    "start": 743,
                    "end": 909
                },
                {
                    "start": 912,
                    "end": 990
                },
                {
                    "start": 991,
                    "end": 1136
                },
                {
                    "start": 1137,
                    "end": 1257
                },
                {
                    "start": 1258,
                    "end": 1412
                },
                {
                    "start": 1413,
                    "end": 1460
                },
                {
                    "start": 1461,
                    "end": 1602
                },
                {
                    "start": 1603,
                    "end": 1696
                },
                {
                    "start": 1697,
                    "end": 1800
                }
            ],
            "ref_mentions": [
                {
                    "start": 51,
                    "end": 76,
                    "matchedPaperCorpusId": "207556454"
                },
                {
                    "start": 101,
                    "end": 122,
                    "matchedPaperCorpusId": "52967399"
                },
                {
                    "start": 968,
                    "end": 989,
                    "matchedPaperCorpusId": "52967399"
                },
                {
                    "start": 1033,
                    "end": 1061,
                    "matchedPaperCorpusId": "158047013"
                },
                {
                    "start": 1494,
                    "end": 1521,
                    "matchedPaperCorpusId": "6205777"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.6943359375
        },
        {
            "corpus_id": "259859023",
            "title": "Text Augmentation Using Dataset Reconstruction for Low-Resource Classification",
            "text": "By generating new sentences for our classifier, we risk the creation of low-quality data. This can hap-pen if we set an incorrect pseudo-label or if the language model generates out-of-domain examples. Therefore, it is common to apply a consistency filtering mechanism (Anaby-Tavor et al., 2020;Wang et al., 2022). \n\nThe consistency filtering suggested by Anaby-Tavor et al. (2020) used the pre-trained classifier and considered the top-K sentences (ordered by their softmax scores). Wang et al. ( 2022) also used the trained classifier. However, instead of using the top-K approach, they kept all the generated sentences for which the classifier agrees with the pseudo-label. \n\nClearly, the chosen filtration method has a large effect on the final classifier, as it controls the data quality of the final trained classifier. The top-K approach might be too conservative, keeping a large safety margin, which results in filtering out most of the generated instances. On the other hand, keeping all the instances on which the classifier agrees with the pseudo-label might include many noisy-label sentences, resulting in a degraded classifier. \n\nWe now present Dynamic Consistency Filtering -our filtering approach for a case where an evaluation set exists. In Section 4.5, we discuss the no-evaluation case. Our method relies upon the evaluation dataset to approximate the optimal portion of the generated instances to include in the augmented dataset. We do so by training k classifiers, one of which trained on a different quantile of the generated instances, ordered by their softmax scores (received from the pre-trained classifier C base ). After training the k instances, we choose the best preforming classifier using the evaluation dataset. \n\nIt is important to note that there is a possibility of applying the filtering mechanism in a recursive manner, for example, training a classifier on the filtered data and running that classifier on the original",
            "score": 0.47349876284230497,
            "section_title": "Dynamic Consistency Filtering",
            "char_start_offset": 12310,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 89
                },
                {
                    "start": 90,
                    "end": 201
                },
                {
                    "start": 202,
                    "end": 314
                },
                {
                    "start": 317,
                    "end": 483
                },
                {
                    "start": 484,
                    "end": 537
                },
                {
                    "start": 538,
                    "end": 676
                },
                {
                    "start": 679,
                    "end": 825
                },
                {
                    "start": 826,
                    "end": 966
                },
                {
                    "start": 967,
                    "end": 1142
                },
                {
                    "start": 1145,
                    "end": 1256
                },
                {
                    "start": 1257,
                    "end": 1307
                },
                {
                    "start": 1308,
                    "end": 1452
                },
                {
                    "start": 1453,
                    "end": 1645
                },
                {
                    "start": 1646,
                    "end": 1748
                },
                {
                    "start": 1751,
                    "end": 1961
                }
            ],
            "ref_mentions": [
                {
                    "start": 269,
                    "end": 295,
                    "matchedPaperCorpusId": "212821571"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.1634521484375
        },
        {
            "corpus_id": "255775280",
            "title": "Linking a Hypothesis Network From the Domain of Invasion Biology to a Corpus of Scientific Abstracts: The INAS Dataset",
            "text": "Table 2 displays the classification results in terms of the macro F1 score for both the single-label and the multi-label classification setting. \n\nNotably, the naive Bayes classifier performs reasonably well and even outperforms the standard BERT classifiers, indicating that simple word frequency statistics provide significant information about the correct label. An analysis of the naive Bayes classifier weights revealed that hypothesisspecific concepts, as well as parts of the hypothesis names, were strong indicators for the specific classes, but also some species and country names that mostly appear in the context of specific hypotheses were used as a basis for the classification. The advantage of the naive Bayes classifier compared to the BERT classifiers might originate in the fact that many domain-specific terms might be unknown to the BERT models and the small training set might not be enough to fully learn these new concepts. \n\nThe classifiers based on variants of BERT that are adapted to texts from the biomedical domain consistently outperformed the naive Bayes classifier, which is consistent with earlier results that show that in-domain fine-tuning generally leads to improved performance (Gururangan et al., 2020). Notably, especially the smaller BERT base models show better performance as well as reduced variance, making them the best performing models in our study. We also observe that the ability to do multi-label predictions generally does not yield an improvement, which can be explained by the small number of cases where multi-label prediction is necessary. \n\nEven though the BioBERT and PubMedBERT models show increased performance compared to the naive Bayes classifier, the difference appears to be moderate considering the large difference in complexity. All BERT models should be able to process the same word frequency information as the naive Bayes classifier, meaning that their ability to combine the information from different words and sentences is only responsible for a 7% performance increase. We believe that this indicates that the BERT classifiers are not able to understand the full semantic content of hypothesis statements, especially if they are only made implicitly. Instead, the increase in performance might simply be caused by the classifier's ability to detect slightly more complex patterns than unigrams (e.g., n-grams) and by its ability to nonlinearly combine the information about the presence of these still simple patterns.",
            "score": 0.4727745106390112,
            "section_title": "Results",
            "char_start_offset": 27443,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 144
                },
                {
                    "start": 147,
                    "end": 365
                },
                {
                    "start": 366,
                    "end": 691
                },
                {
                    "start": 692,
                    "end": 946
                },
                {
                    "start": 949,
                    "end": 1242
                },
                {
                    "start": 1243,
                    "end": 1397
                },
                {
                    "start": 1398,
                    "end": 1596
                },
                {
                    "start": 1599,
                    "end": 1797
                },
                {
                    "start": 1798,
                    "end": 2046
                },
                {
                    "start": 2047,
                    "end": 2227
                },
                {
                    "start": 2228,
                    "end": 2495
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.232177734375
        },
        {
            "corpus_id": "264935645",
            "title": "ChineseWebText: Large-scale High-quality Chinese Web Text Extracted with Effective Evaluation Model",
            "text": "During testing procedure, we will classify the samples of testset with Regression, Perplexity, BERTEval and FastText models repectively, and then compute the precisions of them on positive data. As in Table 4, it shows the precisions of different evaluaiton models on the testset. TP represents the number of \"True Positive\" samples, while FP represents the number of \"False Positive\" samples. From this table, we can see that our BERTEval evaluaiton model gets a much better performance than the regression and perplexity approaches. Besides, benefiting from the good classified results of our BERTEval model, the FastText-based model could further improve the classification precision. This result indicates that using BERTEval to guide the construction of the FastText-based evaluation model is effective. And with this FastText-based evaluation model, our EvalWeb tool-chain could achieve a better performance while effectively improving processing efficiency and resource utilization. In section 3.3, after filtering data with a desired quality threshold, we can obtain a Chinese text dataset. In order to ensure the quality of this dataset, We will hire some human evaluators to evaluate its quality. In this method, we will randomly sample 1000 examples from the dataset for three times. After that, three human evaluators will be hired to assess the quality of these data respectively, and the quality of these data is required to be evaluated from the following four aspects: \n\n\u2022 Informativeness: Whether the text contains enough knowledge and information, or is just meaningless crap. \n\n\u2022 Fluency: Whether the text has formatting issues, capitalization mistakes, or evident grammatical errors that impair readability. \n\n\u2022 Coherence: Whether the text progressively forms a coherent body of information on a topic through its successive sentences. \n\n\u2022 Toxicity: Texts used for pre-training should endeavor to exclude offensive remarks, sexually explicit content, and politically sensitive statements to mitigate potential generative risks. \n\n\u6237\u968f\u65f6\u6765\u5382\u53c2\u89c2\u4e0e\u6307\u5bfc\uff01\" <url> http://www.juyesh.com/SonList-1094890.html <source_domain> www.juyesh.com During evaluation procedure, each text is assigned a label of either \"True\" or \"False.\"",
            "score": 0.4723591990275243,
            "section_title": "Comparison Results",
            "char_start_offset": 23296,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 194
                },
                {
                    "start": 195,
                    "end": 280
                },
                {
                    "start": 281,
                    "end": 393
                },
                {
                    "start": 394,
                    "end": 534
                },
                {
                    "start": 535,
                    "end": 687
                },
                {
                    "start": 688,
                    "end": 808
                },
                {
                    "start": 809,
                    "end": 989
                },
                {
                    "start": 990,
                    "end": 1098
                },
                {
                    "start": 1099,
                    "end": 1206
                },
                {
                    "start": 1207,
                    "end": 1294
                },
                {
                    "start": 1295,
                    "end": 1484
                },
                {
                    "start": 1487,
                    "end": 1594
                },
                {
                    "start": 1597,
                    "end": 1727
                },
                {
                    "start": 1730,
                    "end": 1855
                },
                {
                    "start": 1858,
                    "end": 2047
                },
                {
                    "start": 2050,
                    "end": 2142
                },
                {
                    "start": 2143,
                    "end": 2230
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.5947265625
        },
        {
            "corpus_id": "232106462",
            "title": "Interpretability in Word Sense Disambiguation using Tsetlin Machine",
            "text": "Even though the selected datasets have binary sense classification, we will use Micro-F1 and Macro-F1 as the evaluation metrics as shown in (Loureiro et al., 2020). Since, interpretation of WSD is the main concern of the paper, we will compare our work with the latest benchmark (Loureiro et al., 2020). Table 2 show the comparison of Macro and Micro F1 score on CoarseWSD dataset for 4 different methods: FastText-Base (FTX-B), FastText-CommonCrawl (FTX-C), 1 neural network (NN) BERT base, and our proposed TM. FTX-B is a fast text linear classifier without pre-trained embeddings and FTX-C is a fast text linear classifier with pre-trained embedding from Common Crawl. These are considered as the standard baseline for this dataset (Loureiro et al., 2020). Our proposed TM based WSD easily outperforms FTX-B baseline and is close to FTX-C without even considering the pretrained embedding. However, TM falls short of BERT's performance given that it is a huge language model that achieves the state-of-the-art performance on most of the task. This shows that TM not only possesses the interpretation of the WSD but also has performance close to the state of the art.",
            "score": 0.4722607620192681,
            "section_title": "EVALUATIONS",
            "char_start_offset": 20334,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 164
                },
                {
                    "start": 165,
                    "end": 303
                },
                {
                    "start": 304,
                    "end": 512
                },
                {
                    "start": 513,
                    "end": 671
                },
                {
                    "start": 672,
                    "end": 759
                },
                {
                    "start": 760,
                    "end": 892
                },
                {
                    "start": 893,
                    "end": 1045
                },
                {
                    "start": 1046,
                    "end": 1169
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.501953125
        },
        {
            "corpus_id": "247447069",
            "title": "SciNLI: A Corpus for Natural Language Inference on Scientific Text",
            "text": "We fine-tune four transformer based pre-trained language models: (a) BERT (Devlin et al., 2019) -pre-trained by masked language modeling (MLM) on Book-Corpus (Zhu et al., 2015) and Wikipedia; (b) SciB-ERT (Beltagy et al., 2019) -a variant of BERT pretrained with a similar procedure but exclusively on scientific text; (c) RoBERTa (Liu et al., 2019b)an extension of BERT which was pre-trained using dynamic masked language modeling, i.e., unlike BERT, different words were masked in each epoch during training. It was also trained for a longer period of time on a larger amount of text compared with BERT; and (d) XLNet (Yang et al., 2019)pre-trained with a \"Permutation Language Modeling\" objective instead of MLM. We employ the base variants of each of these models using the huggingface 5 transformers library. The input sequence for these models is derived by concatenating the two sentences in a pair with a [SEP] token in between. The [CLS] token is then projected with a weight matrix W \u2208 R d\u00d74 by sending it as the input to a softmax layer to get the output class. We finetune each transformer based model for 5 epochs where we minimize the cross-entropy loss using Adam optimizer (Kingma and Ba, 2014) with an initial learning rate of 2e \u2212 5. Early stopping with a patience size 2 is employed.\n\nThe experiments are run on a single Tesla V10 GPU. The transformer based models took approximately four hours to train and the traditional machine learning and neural network models were trained in less than one hour. We run each experiment three times with different random seeds and report the average and standard deviation of the F1 scores for each of the four classes, their Macro av-  erage and overall accuracy in Table 6. Our findings are discussed below.\n\nTransformer based models consistently outperform the traditional models The transformer based models have a very high performance gap with the traditional lexicalized and neural models. Their better performance can be attributed to their superior design for capturing the language semantics and their pre-training on large amounts of texts.\n\nMore sophisticated pre-training methods lead to better performance RoBERTa and XL",
            "score": 0.4707275935462265,
            "section_title": "Pre-trained Language Models",
            "char_start_offset": 24530,
            "sentence_offsets": [],
            "ref_mentions": [
                {
                    "start": 74,
                    "end": 94,
                    "matchedPaperCorpusId": "52967399"
                },
                {
                    "start": 158,
                    "end": 176,
                    "matchedPaperCorpusId": "6866988"
                },
                {
                    "start": 205,
                    "end": 226,
                    "matchedPaperCorpusId": "202558505"
                },
                {
                    "start": 620,
                    "end": 638,
                    "matchedPaperCorpusId": "195069387"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.2060546875
        },
        {
            "corpus_id": "256593479",
            "title": "Empirical Study: How Issue Classification Influences Software Defect Prediction",
            "text": "Earlier work in the area of Natuaral language processing (NLP) focused on word2vec models [74], convolutional models [75], [76], [77] recurrent models [78], [79] and, more recently, attention-based models [80], [81]. Substantial earlier work has shown that pre-trained models on large corpora are beneficial for text classification and other NLP tasks [82]. Using pre-trained models offers the benefit of avoiding model training from scratch, thus, speeding up the fine-tuning process and producing higher performance models than those trained only on one specific task. \n\nFastText [73] is a word embedding method which is an extension of the word2vec model. It is considered a bag of words model. Instead of learning each word directly it represents each word as a set on n-grams. In its original publication it was shown to be much faster than the deep models of that time, with comparative performance. \n\nThe current widely adopted language models are BERT [83] and RoBERTa [84]. Both models offer the same architectural design, using the encoder part of the multi-layer bidirectional Transformer architecture [85] which was pre-trained on large text corpora -the BooksCorpus (800M words) [86] and English Wikipedia (2,500M words). The Cloze task [87] inspired the masked language model (MLM) objective used to train BERT and RoBERTa in conjunction with the next sentence prediction task. The method of using a large pre-trained model and fine-tuning in downstream tasks has made a breakthrough in several natural language understanding tasks [83], [84]. \n\nResearchers [11], [12], [13], [14] have looked into issue classification with various motivations. \n\nWang et al. [11] used BERT to recommend GitHub labels based on issue descriptions. Since issue creators often do not label issues, it is left to repository maintainers to label them which can become very time consuming, thus accurate automatic labeling would help reduce the amount of necessary manual labor. \n\nHerbold et al. [12] stress that reported issue types often do not match the description of the issue.",
            "score": 0.4706800580569762,
            "section_title": "D. NATURAL LANGUAGE PROCESSING AND ISSUE CLASSIFICATION",
            "char_start_offset": 18608,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 216
                },
                {
                    "start": 217,
                    "end": 357
                },
                {
                    "start": 358,
                    "end": 570
                },
                {
                    "start": 573,
                    "end": 658
                },
                {
                    "start": 659,
                    "end": 697
                },
                {
                    "start": 698,
                    "end": 781
                },
                {
                    "start": 782,
                    "end": 905
                },
                {
                    "start": 908,
                    "end": 982
                },
                {
                    "start": 983,
                    "end": 1234
                },
                {
                    "start": 1235,
                    "end": 1391
                },
                {
                    "start": 1392,
                    "end": 1557
                },
                {
                    "start": 1560,
                    "end": 1658
                },
                {
                    "start": 1661,
                    "end": 1743
                },
                {
                    "start": 1744,
                    "end": 1969
                },
                {
                    "start": 1972,
                    "end": 2073
                }
            ],
            "ref_mentions": [
                {
                    "start": 123,
                    "end": 127,
                    "matchedPaperCorpusId": "368182"
                },
                {
                    "start": 129,
                    "end": 133,
                    "matchedPaperCorpusId": "1240989"
                },
                {
                    "start": 205,
                    "end": 209,
                    "matchedPaperCorpusId": "6857205"
                },
                {
                    "start": 1192,
                    "end": 1196,
                    "matchedPaperCorpusId": "6866988"
                },
                {
                    "start": 1250,
                    "end": 1254,
                    "matchedPaperCorpusId": "206666846"
                },
                {
                    "start": 1572,
                    "end": 1576,
                    "matchedPaperCorpusId": "239633540"
                },
                {
                    "start": 1584,
                    "end": 1588,
                    "matchedPaperCorpusId": "233224674"
                },
                {
                    "start": 1590,
                    "end": 1594,
                    "matchedPaperCorpusId": "250182232"
                },
                {
                    "start": 1673,
                    "end": 1677,
                    "matchedPaperCorpusId": "239633540"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.58935546875
        },
        {
            "corpus_id": "274280560",
            "title": "Non-Contextual BERT or FastText? A Comparative Analysis",
            "text": "While BERT consistently outperforms other word embeddings in various tasks for high-resource languages (HRLs) like English (Malik et al. (2021)), its effectiveness in low-resource languages (LRLs) remains relatively underexplored. This gap is particularly pronounced when balancing model performance with computational efficiency, which becomes a critical factor in low-resource settings. \n\nPrevious studies (D'Sa et al. (2020)) have focused on contextual BERT embeddings, which outperform FastText due to their ability to capture contextual information. However, the use of non-contextual BERT embeddings for classification tasks in low-resource languages like Marathi remains unexplored. Unlike contextual embeddings, which require a full forward pass through the model, non-contextual embeddings can be obtained through a simple table lookup. To our knowledge, no prior work has examined the effectiveness of non-contextual BERT embeddings. We investigate how these embeddings, extracted from the model's first layer, compare to FastText embeddings for tasks such as news classification, sentiment analysis, and hate speech detection in Marathi. \n\nAdditionally, past comparisons often used BERT's 768-dimensional embeddings against Fast-Text's 300-dimensional ones, which is unfair since higher dimensions naturally provide better feature extraction. To address this, we ensure a fair comparison by reducing the BERT embeddings to 300 dimensions. \n\nThis paper focuses on utilizing FastText and non-contextual BERT for the Marathi language for the following tasks: Sentiment Classification, 2-Class and 4-Class Hate Speech Detection, and News Article Classification for headlines, long paragraphs, and long documents. We construct a comprehensive analysis of FastText embeddings, IndicFT (Kakwani et al., 2020) and MahaFT (Joshi, 2022a) embeddings, and BERT embeddings, including muril-base-cased (Khanuja et al., 2021) and marathi-bert-v2 (Joshi, 2022a). To enhance the comparison, we replicate the experiments using widely utilized contextual BERT embeddings.",
            "score": 0.4705380476992397,
            "section_title": "Introduction",
            "char_start_offset": 1806,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 230
                },
                {
                    "start": 231,
                    "end": 388
                },
                {
                    "start": 391,
                    "end": 554
                },
                {
                    "start": 555,
                    "end": 689
                },
                {
                    "start": 690,
                    "end": 845
                },
                {
                    "start": 846,
                    "end": 943
                },
                {
                    "start": 944,
                    "end": 1148
                },
                {
                    "start": 1151,
                    "end": 1353
                },
                {
                    "start": 1354,
                    "end": 1449
                },
                {
                    "start": 1452,
                    "end": 1719
                },
                {
                    "start": 1720,
                    "end": 1957
                },
                {
                    "start": 1958,
                    "end": 2063
                }
            ],
            "ref_mentions": [
                {
                    "start": 123,
                    "end": 143,
                    "matchedPaperCorpusId": "233990897"
                },
                {
                    "start": 408,
                    "end": 427,
                    "matchedPaperCorpusId": "213529509"
                },
                {
                    "start": 1790,
                    "end": 1812,
                    "matchedPaperCorpusId": "267935553"
                },
                {
                    "start": 1824,
                    "end": 1838,
                    "matchedPaperCorpusId": "246473294"
                },
                {
                    "start": 1942,
                    "end": 1956,
                    "matchedPaperCorpusId": "246473294"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.7138671875
        },
        {
            "corpus_id": "271691319",
            "title": "Sentiment Analysis: Predicting Product Reviews for E-Commerce Recommendations Using Deep Learning and Transformers",
            "text": "FastText: a Word2Vec plugin that breaks down words into n-grams, or smaller units, like \"application\" into \"app,\" with the goal of teaching word morphology. Every word in the text is converted by the model into a bag of embedded vectors. \n\nPolysemous words might provide difficulties when using Word2Vec and FastText since they always assign the same embedding vector, regardless of context. Researchers have been using transformer-based embeddings, such as BERT and its derivatives, to address this problem. Word contexts from BooksCorpus and Wikipedia were used to pretrain models like BERT, and the resulting embeddings were then used in classifiers to make predictions. These models have demonstrated state of the art performance in natural language processing tasks by providing contextualized word embeddings. \n\nThe bidirectional transformer used in the BERT-based model has been pre-trained on large amounts of unlabeled textual data to provide a language representation that can be tailored to different classification tasks. One noteworthy variation is RoBERTa, which Facebook unveiled. It is an improved version of BERT with increased processing power and expanded prediction capabilities that can handle larger amounts of data. Furthermore, ALBERT, a condensed and effective BERT variant that is far smaller than BERT, was created by Google and Toyota. In particular, two variants of a BERT-based model, RoBERTa and ALBERT, were studied in this work.",
            "score": 0.4702444435125115,
            "section_title": "\u2022",
            "char_start_offset": 31744,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 156
                },
                {
                    "start": 157,
                    "end": 237
                },
                {
                    "start": 240,
                    "end": 391
                },
                {
                    "start": 392,
                    "end": 508
                },
                {
                    "start": 509,
                    "end": 673
                },
                {
                    "start": 674,
                    "end": 815
                },
                {
                    "start": 818,
                    "end": 1033
                },
                {
                    "start": 1034,
                    "end": 1095
                },
                {
                    "start": 1096,
                    "end": 1238
                },
                {
                    "start": 1239,
                    "end": 1363
                },
                {
                    "start": 1364,
                    "end": 1461
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.80224609375
        },
        {
            "corpus_id": "278394813",
            "title": "Ultra-FineWeb: Efficient Data Filtering and Verification for High-Quality LLM Training Data",
            "text": "Data quality has become a key factor in enhancing model performance with the rapid development of large language models (LLMs). Model-driven data filtering has increasingly become a primary approach for acquiring high-quality data. However, it still faces two main challenges: (1) the lack of an efficient data verification strategy makes it difficult to provide timely feedback on data quality; and (2) the selection of seed data for training classifiers lacks clear criteria and relies heavily on human expertise, introducing a degree of subjectivity. To address the first challenge, we introduce an efficient verification strategy that enables rapid evaluation of the impact of data on LLM training with minimal computational cost. To tackle the second challenge, we build upon the assumption that high-quality seed data is beneficial for LLM training, and by integrating the proposed verification strategy, we optimize the selection of positive and negative samples and propose an efficient data filtering pipeline. This pipeline not only improves filtering efficiency, classifier quality, and robustness, but also significantly reduces experimental and inference costs. In addition, to efficiently filter high-quality data, we employ a lightweight classifier based on fastText, and successfully apply the filtering pipeline to two widely-used pre-training corpora, FineWeb and Chinese FineWeb datasets, resulting in the creation of the higher-quality Ultra-FineWeb dataset. Ultra-FineWeb contains approximately 1 trillion English tokens and 120 billion Chinese tokens. Empirical results demonstrate that the LLMs trained on Ultra-FineWeb exhibit significant performance improvements across multiple benchmark tasks, validating the effectiveness of our pipeline in enhancing both data quality and training efficiency.",
            "score": 0.4701249890570162,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.5888671875
        },
        {
            "corpus_id": "235489736",
            "title": "A Brief Study on the Effects of Training Generative Dialogue Models with a Semantic loss",
            "text": "We experiment on two differently sized data sets -Frames (Asri et al., 2017) and MultiWoZ 2.0 (Budzianowski et al., 2018) -which are relatively small and large. We compute L SEM using the commonly used language model embeddings BERT-Base (Devlin et al., 2018), GloVe (Pennington et al., 2014) and fastText (Bojanowski et al., 2017) to compare the benefit of using different embeddings. Evaluation Metrics: We measure the performance on overlap based metric BLEU (Papineni et al., 2002); and diversity in the generated text by computing the fraction of distinct-1 and distinct-2 grams, similar to Welleck et al. (2019); Li et al. (2015), on validation set. Also, as a proxy to evaluate generalization to generating n-grams that the decoder was never trained to, we measure the fraction of bigrams generated by the model during validation that were not in the training targets, as % Unseen. Also, to measure the effects of minimizing the semantic loss on language quality, we perform human evaluation for comparing the different training techniques. Further we compare the improvements in diversity between using BERT for initialization of word embeddings and using it in a semantic loss objective.",
            "score": 0.46881521899249257,
            "section_title": "Experiments",
            "char_start_offset": 4733,
            "sentence_offsets": [],
            "ref_mentions": [
                {
                    "start": 267,
                    "end": 292,
                    "matchedPaperCorpusId": "1957433"
                },
                {
                    "start": 462,
                    "end": 485,
                    "matchedPaperCorpusId": "11080756"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.48291015625
        },
        {
            "corpus_id": "246738423",
            "title": "Application of Knowledge Distillation Based on Transfer Learning of ERNIE Model in Intelligent Dialogue Intention Recognition",
            "text": "The basic principle was to transfer the features learned from the \"teacher\" network with a complex network structure and strong learning ability to the \"student\" network with a small number of parameters and weak learning ability. Sau et al. [20] used a noise-based regularizer to train the student from the teacher for a higher training efficiency in 2016. Liao et al. [21] tried to use the BERT model for distillation learning in LSTM and CNN. Sun et al. [22] proposed Patient Knowledge Distillation for the BERT Model. RNN-based models are limited by performance issues, and it is difficult to have beneficial real-time applications, such as high-concurrency scenarios in the field of chatbots. Yang et al. [9] discussed the application of deep learning in the intention classification of chatbots. Bahdanau et al. in 2015 [10] developed an attention mechanism, while Vaswani et al. in 2017 [11] proposed a transformer as seq2seq model with parallelized attentions. In 2017, Mikolov et al. [12] proposed the FastText model; the network architecture of FastText is very similar to CBOW (Continuous Bag-of-Words) [13], and FastText is mainly used for text classification and displays positive computational efficiency. Peters et al. [14] generalizes traditional word embedding research ELMo (Embedding from Language Model) along a different dimension in 2018, and they suggested extracting context sensitive features and applying pre-training tasks to text classification tasks. \n\nFine-tuning based on pre-training for a language model has been developed for text generation tasks (e.g., GPT (Radford et al., 2018) [15]; Google proposed the BERT (Bidirectional Encoder Representations from Transformers) model in 2018 [16]. The BERT model is based on a self-attention model architecture. The successful application of BERT has put the natural language text classification task into a new level. \n\nThe BERT model mainly focuses on words or more fine-grained cloze learning and does not make full use of the lexical structure in the training data. Li [17] used the application of the BERT model in text classification at the sub-word level.",
            "score": 0.4680976260895103,
            "section_title": "Introduction",
            "char_start_offset": 5882,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 230
                },
                {
                    "start": 231,
                    "end": 357
                },
                {
                    "start": 358,
                    "end": 445
                },
                {
                    "start": 446,
                    "end": 521
                },
                {
                    "start": 522,
                    "end": 697
                },
                {
                    "start": 698,
                    "end": 801
                },
                {
                    "start": 802,
                    "end": 968
                },
                {
                    "start": 969,
                    "end": 1219
                },
                {
                    "start": 1220,
                    "end": 1479
                },
                {
                    "start": 1482,
                    "end": 1724
                },
                {
                    "start": 1725,
                    "end": 1788
                },
                {
                    "start": 1789,
                    "end": 1895
                },
                {
                    "start": 1898,
                    "end": 2046
                },
                {
                    "start": 2047,
                    "end": 2139
                }
            ],
            "ref_mentions": [
                {
                    "start": 242,
                    "end": 246,
                    "matchedPaperCorpusId": "7200347"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.29638671875
        },
        {
            "corpus_id": "252211903",
            "title": "Automated Classification for Open-Ended Questions with BERT",
            "text": "We compared BERT against other popular machine learning algorithms: random forest (RF) (Breiman, 2001), XGBoost (XGB) (Chen and Guestrin, 2016) and support vector machines (SVM) (Vapnik, 2000). All models used the same (random) sets of training/validation/test data. \n\nHyperparameter tuning details are found in Appendix A.   BERT without fine-tuning was beaten by the ngram-based models. This suggests that the pre-trained language model requires fine-tuning even for very small training data sets. \n\nBoth data sets from open-ended questions have imbalanced class distributions. The class-wise accuracy results show BERT's better classification performance relative to ngram-based models is not particularly biased towards certain major classes. BERT tends to achieve higher accuracy for most classes without a noticeable drop in performance for any one class. We also examined the methods in terms of macro-accuracy (i.e., the average of the class-wise accuracy) that values all categories equally regardless of their frequencies. The results were similar to those we had obtained from overall accuracy. \n\nWe see the following limitations: First, for 400 training observations, the accuracy in the two examples is roughly 75-85%. While considerably improved, this may not be enough for some purposes. If so, we recommend semi-automatic classification (Schonlau and Couper, 2016;Gweon et al., 2020), where there is a threshold probability below which answers are coded manually. Also, BERT is harder to program than non-pre-trained methods. At present, most analysts are programming in Python to access the pre-trained BERT model. It is also computationally intensive as fine-tuning requires at least a high-end laptop or online resources such as Google Colab. Lastly, BERT has been very successful in English. BERT models in other languages are being developed. While we are hopeful these results will extend to other languages, this is not certain. \n\nThe social sciences emphasize data quality. While clearly useful, models such as BERT still require close scrutiny. Nonetheless, we see some advantages of fine tuning a BERT model even for classifying just a few thousand observations. One, automatic classification makes classification results reproducible (though they still depend on the initial training data).",
            "score": 0.4677669802004355,
            "section_title": "Results",
            "char_start_offset": 12348,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 193
                },
                {
                    "start": 194,
                    "end": 266
                },
                {
                    "start": 269,
                    "end": 388
                },
                {
                    "start": 389,
                    "end": 499
                },
                {
                    "start": 502,
                    "end": 579
                },
                {
                    "start": 580,
                    "end": 746
                },
                {
                    "start": 747,
                    "end": 861
                },
                {
                    "start": 862,
                    "end": 1032
                },
                {
                    "start": 1033,
                    "end": 1105
                },
                {
                    "start": 1108,
                    "end": 1231
                },
                {
                    "start": 1232,
                    "end": 1302
                },
                {
                    "start": 1303,
                    "end": 1479
                },
                {
                    "start": 1480,
                    "end": 1541
                },
                {
                    "start": 1542,
                    "end": 1631
                },
                {
                    "start": 1632,
                    "end": 1761
                },
                {
                    "start": 1762,
                    "end": 1811
                },
                {
                    "start": 1812,
                    "end": 1863
                },
                {
                    "start": 1864,
                    "end": 1951
                },
                {
                    "start": 1954,
                    "end": 1997
                },
                {
                    "start": 1998,
                    "end": 2069
                },
                {
                    "start": 2070,
                    "end": 2188
                },
                {
                    "start": 2189,
                    "end": 2317
                }
            ],
            "ref_mentions": [
                {
                    "start": 87,
                    "end": 102,
                    "matchedPaperCorpusId": "89141"
                },
                {
                    "start": 118,
                    "end": 143,
                    "matchedPaperCorpusId": "4650265"
                },
                {
                    "start": 1353,
                    "end": 1380,
                    "matchedPaperCorpusId": "55573625"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.262451171875
        },
        {
            "corpus_id": "272353831",
            "title": "Enhancing Multilingual Hate Speech Detection: From Language-Specific Insights to Cross-Linguistic Integration",
            "text": "In this evaluation, we conducted experiments after the integration of language-specific unsupervised FastText embeddings alongside language-specific transformer-based models for each language present in the dataset. The table 8 represents the results of language-specific transformers with unsupervised FastText. \n\nFor Arabic, the models paired with AraBERT and AraELECTRA notably achieve high precision and recall, both peaking at 0.88, which indicates a strong alignment between the models' predictions and the actual data. In the English context, the combination of RF+BERT stands out with a precision and recall at 0.94, suggesting an exceptional ability to correctly identify HS instances while maintaining a low rate of false positives. In Chinese, the models integrated with ERNIE and MacBERT show improved metrics, with SVM + MacBERT reaching precision and recall of 0.81, reflecting a commendable proficiency in understanding and classifying Chinese text. Turning to French, we observe that the SVM + CAMEMBERT model achieves a precision, accuracy, recall, and recall of 0.85, underscoring its effectiveness in the nuanced task of HS detection in French. German language models, such as SVM + DeepestBERT , demonstrate a performance with an accuracy and F1-score of 0.67, indicating a moderate score accuracy in the classification process. \n\nRussian language classifiers coupled with BERT and XLM \u2212 RoBERTa exhibit precision and recall rates at 0.88, highlighting the robustness of these models in dealing with the Russian language. Turkish classifiers, particularly when combined with TurkishBERT , achieve a solid score of 0.94 in terms of accuracy and F1-score, reflecting an outstanding capability to discern HS content in Turkish. For the less commonly represented Roman Urdu, the SVM + RomanBERT model also achieves high precision and recall, both at 0.84, demonstrating the efficacy of this language-model pairing. The Korean classifiers, despite being challenged by the complexity of the language, show improved performance as compared to simple unsupervised FastText in table 7. It has evaluation scores of 0.74 across all four matrices when combined with KRBERT , this significant enhancement is the result of our designed architecture.",
            "score": 0.467588358723238,
            "section_title": "B. ML MODELS: EVALUATION SCORES WITH UNSUPERVISED FASTTEXT AND LANGUAGE SPECIFIC TRANSFORMERS",
            "char_start_offset": 46611,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 215
                },
                {
                    "start": 216,
                    "end": 312
                },
                {
                    "start": 315,
                    "end": 525
                },
                {
                    "start": 526,
                    "end": 742
                },
                {
                    "start": 743,
                    "end": 964
                },
                {
                    "start": 965,
                    "end": 1163
                },
                {
                    "start": 1164,
                    "end": 1348
                },
                {
                    "start": 1351,
                    "end": 1541
                },
                {
                    "start": 1542,
                    "end": 1744
                },
                {
                    "start": 1745,
                    "end": 1930
                },
                {
                    "start": 1931,
                    "end": 2255
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.51953125
        },
        {
            "corpus_id": "247618867",
            "title": "Supporting Developers in Addressing Human-Centric Issues in Mobile Apps",
            "text": "We used BERT [34], RoBERTa [35], and DistilBERT [36], which are the state of the art classifiers designed to bidirectionally train on the overall context of data in all layers. BERT is pre-trained on Wikipedia and Book corpus that is helpful in English text. However, since our dataset may have words that are technical and unknown to the model [37], we processed and trained the model on pre-trained data and by adding more output layers. Robustly optimised BERT approach (RoBERTa) is a replication study of BERT pretraining that measures the impact of key hyperparameters and training data size on top of BERT. RoBERTa is trained with dynamic masking, FULL-SENTENCES without Next Sentence Prediction (NSP) loss, large mini-batches and a larger byte-level Byte-Pair Encoding (BPE) [35]. DistilBERT is another extension of BERT that is pre-trained on a smaller general-purpose language representation model and can then be fine-tuned on a wide range of tasks resulting in good performances. DistilBERT is reported to reduce the size of a BERT model by 40%, while retaining 97% of its language understanding capabilities and being 60% faster, and also cheaper to pre-train [36]. \n\nAs the first step, we needed to transform the data as required for these three models. Hence, we followed data pre-processing that allows them to interpret the data. \n\nPre-processing. For BERT, RoBERTa and DistilBERT classifiers, we followed the same steps as described in Section 5.1.1. Additionally, we needed to transform the text data into numerical values within the word limit specified by Transformer models (512 words at a time). Therefore, we tokenised and encoded the data in a structure as specified by HuggingFace (transformers) library [38]. The package provides API to perform NLP tasks to utilise its capacity over a vast variety of tasks [27]. \n\nClassification Models. The BERT classifier is ready to be utilised directly on the model, but it needs to be finetuned to produce better results. In a text classification task, the BERT base model outputs a vector of length 768 for each word (token) and pools the output.",
            "score": 0.4674957639841997,
            "section_title": "Deep Learning",
            "char_start_offset": 39464,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 176
                },
                {
                    "start": 177,
                    "end": 258
                },
                {
                    "start": 259,
                    "end": 439
                },
                {
                    "start": 440,
                    "end": 612
                },
                {
                    "start": 613,
                    "end": 787
                },
                {
                    "start": 788,
                    "end": 990
                },
                {
                    "start": 991,
                    "end": 1177
                },
                {
                    "start": 1180,
                    "end": 1266
                },
                {
                    "start": 1267,
                    "end": 1345
                },
                {
                    "start": 1348,
                    "end": 1363
                },
                {
                    "start": 1364,
                    "end": 1467
                },
                {
                    "start": 1468,
                    "end": 1617
                },
                {
                    "start": 1618,
                    "end": 1734
                },
                {
                    "start": 1735,
                    "end": 1839
                },
                {
                    "start": 1842,
                    "end": 1864
                },
                {
                    "start": 1865,
                    "end": 1987
                },
                {
                    "start": 1988,
                    "end": 2113
                }
            ],
            "ref_mentions": [
                {
                    "start": 1834,
                    "end": 1838,
                    "matchedPaperCorpusId": "15756880"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.38427734375
        },
        {
            "corpus_id": "241583356",
            "title": "Stance Detection in German News Articles",
            "text": "We evaluate two baselines: a bag-of-words linear classifier and a BERT-based model, and report their F 1 scores on the CHeeSE dataset. \n\nBag-of-Words Linear Classifier Our first baseline is a bag-of-words linear classifier built using fastText15 as described in Joulin et al. (2017). We train the model for 50 epochs and a learning rate of 1 as recommended in the standard fastText settings. \n\nFine-tuned German BERT model As a second baseline, we fine-tune a pre-trained BERT model. Specifically, we first adapt the pre-trained German BERT model bert-base-german-uncased from Hugginface (Wolf et al., 2020) 16 to the stance detection task and extend it with a multiclass classification head. We then fine-tune the model on our data for four epochs and a learning rate of 3e-05.",
            "score": 0.465464451653145,
            "section_title": "Baseline Experiments",
            "char_start_offset": 22277,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 134
                },
                {
                    "start": 137,
                    "end": 283
                },
                {
                    "start": 284,
                    "end": 391
                },
                {
                    "start": 394,
                    "end": 483
                },
                {
                    "start": 484,
                    "end": 692
                },
                {
                    "start": 693,
                    "end": 778
                }
            ],
            "ref_mentions": [
                {
                    "start": 262,
                    "end": 282,
                    "matchedPaperCorpusId": "1210515"
                },
                {
                    "start": 588,
                    "end": 607,
                    "matchedPaperCorpusId": "269498086"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.5576171875
        },
        {
            "corpus_id": "240488728",
            "title": "Text-Based Emotion Recognition in English and Polish for Therapeutic Chatbot",
            "text": "Recent works have shown that shallow neural networks can also perform well for sentiment classification. Reference [18] presents the use of fastText word embeddings [19] as representation of words to perform the task of sentiment analysis. The results showed that the proposed approach yielded better results than many classic baseline models. \n\nCurrently, BERT (Bidirectional Encoder Representations from Transformers) is reported to be the state-of-the-art language model and has achieved amazing results in many language understanding tasks [20], including sentiment recognition. In Reference [21], the authors used the pretrained BERT model and fine-tuned it for the fine-grained sentimentclassification task on the Stanford Sentiment Treebank dataset. The proposed model performed better than complicated architectures, such as paragraphVectors, or typical recursive and convolutional neural networks. However, BERT-based models also exhibit some limitations, e.g., they have large computational and memory requirements, and the black-box model characteristics make their predictions hardly interpretable. \n\nDeep-learning approaches, such as BERT-based models, have recently achieved stateof-the-art results [22] in the SemEval 2018 competition's task related to detecting affect in Tweets [23], with objectives ranging from emotion classification to emotion-intensity prediction. The competition dataset was annotated with 12 different emotions (incl. no emotion) for English, Spanish and Arabic in a multi-label manner. The most successful approaches during the competition in 2018 used combinations of sentence embeddings with features extracted from affective lexicons. \n\nThe interest in emotion analysis as part of cyclical competitions materialized one year later in SemEval 2019, in a task called EmoContext [24]. Its goal was to classify the emotion represented by a short informal dialogue utterance, also taking into account the preceding two turns of the dialogue. There were only four classes of emotion (Happy, Sad, Angry, and Others) and three different classification subtasks. Among the top systems, there were again examples of models leveraging both vector representations of sentences and emotion-related features.",
            "score": 0.4647108985619038,
            "section_title": "Text-Based Sentiment and Emotion Recognition",
            "char_start_offset": 6149,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 104
                },
                {
                    "start": 105,
                    "end": 239
                },
                {
                    "start": 240,
                    "end": 343
                },
                {
                    "start": 346,
                    "end": 582
                },
                {
                    "start": 583,
                    "end": 756
                },
                {
                    "start": 757,
                    "end": 906
                },
                {
                    "start": 907,
                    "end": 1110
                },
                {
                    "start": 1113,
                    "end": 1385
                },
                {
                    "start": 1386,
                    "end": 1457
                },
                {
                    "start": 1458,
                    "end": 1526
                },
                {
                    "start": 1527,
                    "end": 1678
                },
                {
                    "start": 1681,
                    "end": 1825
                },
                {
                    "start": 1826,
                    "end": 1980
                },
                {
                    "start": 1981,
                    "end": 2097
                },
                {
                    "start": 2098,
                    "end": 2238
                }
            ],
            "ref_mentions": [
                {
                    "start": 115,
                    "end": 119,
                    "matchedPaperCorpusId": "46879618"
                },
                {
                    "start": 165,
                    "end": 169,
                    "matchedPaperCorpusId": "207556454"
                },
                {
                    "start": 596,
                    "end": 600,
                    "matchedPaperCorpusId": "203905846"
                },
                {
                    "start": 1295,
                    "end": 1299,
                    "matchedPaperCorpusId": "4941467"
                },
                {
                    "start": 1820,
                    "end": 1824,
                    "matchedPaperCorpusId": "184483331"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.5810546875
        },
        {
            "corpus_id": "260334432",
            "title": "Multi-output Headed Ensembles for Product Item Classification",
            "text": "Macro-F1 scores induce equal weighting of genre performance and hence are a much stricter standard and that is the measure that we use henceforth for comparing models. For all models except AutoML and fastText NNI, the scores reported are averages of five runs. We note that our Ensemble of CNNs baseline (i.e. MoHE without the coupling) is a strong classifier and outperforms MoE and Aggregator baselines significantly. Additionally, MoHE-2 outperforms Ensemble significantly on the validation set. 5 Microsoft's Neural Network Intelligence (NNI): https://github.com/Microsoft/nni and Ray Tune: https://docs.ray.io/en/latest/tune/index.html The main issue with BERT is that it is a more generalized multitask model where fine-tuning is dependent on a large-scale language model, which is trained according to specific objectives of next word prediction based on a suitably chosen context. For the case of classification of item titles, the NSP (Next Sentence Prediction) objective of BERT is irrelevant if we are to even pre-train on item titles and so is SOP (Sentence Order Prediction) of AlBERT [16]. RoBERTa [20] removes the NSP objective, however, its training time does not meet our business SLA. Hence, we drop BERT and similar models from future comparisons in this paper.",
            "score": 0.46378295954360466,
            "section_title": "EXPERIMENTS AND EVALUATION",
            "char_start_offset": 20999,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 167
                },
                {
                    "start": 168,
                    "end": 261
                },
                {
                    "start": 262,
                    "end": 310
                },
                {
                    "start": 311,
                    "end": 420
                },
                {
                    "start": 421,
                    "end": 581
                },
                {
                    "start": 582,
                    "end": 641
                },
                {
                    "start": 642,
                    "end": 889
                },
                {
                    "start": 890,
                    "end": 1104
                },
                {
                    "start": 1105,
                    "end": 1203
                },
                {
                    "start": 1204,
                    "end": 1281
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.1165771484375
        },
        {
            "corpus_id": "270862694",
            "title": "Extracting Semantic Topics About Development in Africa From Social Media",
            "text": "FastText is based on Bojanowski's SG model, which treats each word as a bag of character n-grams [54]. It has improved the representation of out-of-vocabulary words and works well with large corpora. The limitations of FastText include ignoring the grammatical sense of words. Our study compared the embeddings of Word2Vec-SMA, pretrained Word2Vec, and FastText. \n\nContextualized Word Embeddings are words based on the meanings derived from the sentences in which they appear. Language model-driven embeddings are contextual; they consider the terms surrounding the target words to generate linguistically-based representations [55]. Contextualized embeddings are widely used in various NLP applications, including classification, question-answering, and summarization. The most popular models for generating dynamic word embeddings are from the Bidirectional Encoder Representation Transformer (BERT) family. BERT uses self-supervised learning to learn the contextual connections between words without labeled data. In the pre-training phase, BERT utilizes two main strategies: the Masked Language Model (MLM) and Next Sentence Prediction (NSP) to learn the grammatical semantics of sentences. In our study, we used the pretrained sentence BERT embeddings called \"paraphrase-MiniLM-L6-v2,\" which supports paraphrasing text for NLP tasks with efficiency and high computation power [56].",
            "score": 0.4625721616208924,
            "section_title": "Wording Embeddings",
            "char_start_offset": 23486,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 102
                },
                {
                    "start": 103,
                    "end": 199
                },
                {
                    "start": 200,
                    "end": 276
                },
                {
                    "start": 277,
                    "end": 362
                },
                {
                    "start": 365,
                    "end": 476
                },
                {
                    "start": 477,
                    "end": 633
                },
                {
                    "start": 634,
                    "end": 769
                },
                {
                    "start": 770,
                    "end": 909
                },
                {
                    "start": 910,
                    "end": 1016
                },
                {
                    "start": 1017,
                    "end": 1194
                },
                {
                    "start": 1195,
                    "end": 1386
                }
            ],
            "ref_mentions": [
                {
                    "start": 97,
                    "end": 101,
                    "matchedPaperCorpusId": "244833681"
                },
                {
                    "start": 1381,
                    "end": 1385,
                    "matchedPaperCorpusId": "52967399"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.82421875
        },
        {
            "corpus_id": "276617859",
            "title": "Kanana: Compute-efficient Bilingual Language Models",
            "text": "After the cascaded filtering, we further apply language-specific model-based filtering on high quality documents (Su et al., 2024;Shao et al., 2024b;Li et al., 2024;Penedo et al., 2024a) separately on English and Korean. For English web documents, we utilize a DCLM (Li et al., 2024) classifier. \n\nFor Korean web documents, due to the lack of publicly available high quality classifiers, we iteratively train edu filter as high quality classifier using FastText (Joulin et al., 2017) based on the FineWeb-Edu pipeline (Penedo et al., 2024a). When applying the FineWeb-Edu pipeline, we observe that most of the documents are classified as uneducational, leading to a distribution imbalance. To address this issue, we iteratively retrain the classifier by augmenting educational documents from the previous iteration. \n\nTo assess the quality of our edu filter and Korean web corpus, we perform experiments by continual pre-training Llama 3 8B with 25B tokens. As shown in Table 2, the quality of our Korean web corpus is comparable to that of FineWeb 2 (Penedo et al., 2024b), which is the largest open-source Korean corpus. Furthermore, when using our edu filter to extract high quality data from Korean web corpus, we observe a significant performance improvement in the experimental results through training. Interestingly, we observe that using high quality English data, regardless of the quality of Korean data, can improve the scores on Korean benchmarks such as KMMLU and HAE-RAE, as well as the English benchmark MMLU. The results from this experiment make a foundation of our intuition for data mixture strategy in the staged pre-training in the following section. In summary, we share two insights to consider when building bilingual corpora with underrepresented language for enhanced computational efficiency. (1) Prioritize quality over quantity. For languages that do not have vast tokens available, such as Korean, prioritizing quality over quantity is an effective solution.",
            "score": 0.4625203238964576,
            "section_title": "Data",
            "char_start_offset": 7679,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 220
                },
                {
                    "start": 221,
                    "end": 295
                },
                {
                    "start": 298,
                    "end": 541
                },
                {
                    "start": 542,
                    "end": 689
                },
                {
                    "start": 690,
                    "end": 815
                },
                {
                    "start": 818,
                    "end": 957
                },
                {
                    "start": 958,
                    "end": 1122
                },
                {
                    "start": 1123,
                    "end": 1309
                },
                {
                    "start": 1310,
                    "end": 1525
                },
                {
                    "start": 1526,
                    "end": 1672
                },
                {
                    "start": 1673,
                    "end": 1820
                },
                {
                    "start": 1821,
                    "end": 1858
                },
                {
                    "start": 1859,
                    "end": 1989
                }
            ],
            "ref_mentions": [
                {
                    "start": 113,
                    "end": 130,
                    "matchedPaperCorpusId": "274446256"
                },
                {
                    "start": 149,
                    "end": 165,
                    "matchedPaperCorpusId": "270560330"
                },
                {
                    "start": 266,
                    "end": 283,
                    "matchedPaperCorpusId": "270560330"
                },
                {
                    "start": 462,
                    "end": 483,
                    "matchedPaperCorpusId": "1210515"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.1624755859375
        },
        {
            "corpus_id": "270521825",
            "title": "Analyzing Gender Polarity in Short Social Media Texts with BERT: The Role of Emojis and Emoticons",
            "text": "For the purpose of this effort, we used a Bidirectional Encoder Representations from Transformers (BERT) architecture.BERT is one of the advanced models that is developed for large language model applications published in 2018 [7].The BERT model excels in tasks such as text classification, question answering, sentiment analysis, and language translation.BERT is pre-trained on vast amounts of text data using unsupervised learning techniques that helps BERT to learn rich and comprehensive representations of words and sentences, enabling it to understand language nuances, semantics, and relationships effectively.The model we selected is a BERT Base uncased model, pre-trained with 110 million parameters for different tasks including classification.We also used the BERT tokenizer and BERT embedding to feed our input into the model.We then added a drop out layer with the rate of 0.1% as drop out percentage.We then added the dense layer with the sigmoid function to perform the binary classification on our dataset.for the training process, we achieved the best results with learning rate of 2e-5.In each experiment, we trained the classifier for 10 epochs on Tesla T4 GPU with each of them taking 25 minutes to run on our dataset.",
            "score": 0.46070954046591683,
            "section_title": "III. PRETRAINED MODEL",
            "char_start_offset": 4321,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 118
                },
                {
                    "start": 118,
                    "end": 231
                },
                {
                    "start": 231,
                    "end": 356
                },
                {
                    "start": 356,
                    "end": 617
                },
                {
                    "start": 617,
                    "end": 754
                },
                {
                    "start": 754,
                    "end": 838
                },
                {
                    "start": 838,
                    "end": 914
                },
                {
                    "start": 914,
                    "end": 1022
                },
                {
                    "start": 1022,
                    "end": 1104
                },
                {
                    "start": 1104,
                    "end": 1238
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.234619140625
        },
        {
            "corpus_id": "271866185",
            "title": "Machine Learning-Augmented Ontology-Based Data Access for Renewable Energy Data",
            "text": "We perform two main steps of data preprocessing. We first perform a data cleaning pass, whose goal is to remove noise from input data, i.e., we uniform lowercase and uppercase text and remove frequent and non-informative patterns, including excessive spaces and symbols (the latter is essential in our context, due to the prevalence of abbreviations in the dataset). Lastly, we filter out data irrelevant to the classification task, such as those representing internal component enumerations within plants (e.g., \"Engine 1\" vs \"Engine 2\"). Then, we apply standard text tokenization to decompose raw text input into smaller, manageable \"chunks\". After this preprocessing phase, we move to the model selection. \n\nModel selection. For the models, we considered BERT [28], Naive Bayes, and Random Forests. These models vary significantly from one another and therefore were selected to more accurately assessing the generalizability of the proposed solution. \n\nBERT is a state-of-the-art transformer-based model known for its capabilities in processing natural language data with a bidirectional approach, which allows for a deep understanding of the context within text data. Naive Bayes is a probabilistic classifier and provides a straightforward approach to classification. Random Forests are known for their high accuracy and ability to handle large datasets with numerous input variables. \n\nThe hyperparameters of each model were tuned through an extensive combination trial employing a grid search. The optimal parameter configurations for each model proved to be the same through all the breakdown levels due to the similarity of the tasks. \n\nThe values for the BERT base uncased model were selected among the parameters most used in the literature, where typical values for learning rate range between 3 \u00d7 10 -5 and 3 \u00d7 10 -4 . Recommended values for batch sizes vary from 8 to 64, depending both on the problem and the available hardware, and the optimal number of epochs is dependent on the task, still being typically few and between 1 and 5 [29,30]. Combining a learning rate of 4 \u00d7 10 -5 , a batch size of 32, and 5 epochs, without weight decay, using the Cross-Entropy loss function, resulted in the most efficient combination for this specific task; the model employed its tokenizer.",
            "score": 0.4595833142624688,
            "section_title": "Evaluation",
            "char_start_offset": 24023,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 48
                },
                {
                    "start": 49,
                    "end": 366
                },
                {
                    "start": 367,
                    "end": 539
                },
                {
                    "start": 540,
                    "end": 644
                },
                {
                    "start": 645,
                    "end": 708
                },
                {
                    "start": 711,
                    "end": 727
                },
                {
                    "start": 728,
                    "end": 801
                },
                {
                    "start": 802,
                    "end": 954
                },
                {
                    "start": 957,
                    "end": 1172
                },
                {
                    "start": 1173,
                    "end": 1273
                },
                {
                    "start": 1274,
                    "end": 1390
                },
                {
                    "start": 1393,
                    "end": 1501
                },
                {
                    "start": 1502,
                    "end": 1644
                },
                {
                    "start": 1647,
                    "end": 1832
                },
                {
                    "start": 1833,
                    "end": 2058
                },
                {
                    "start": 2059,
                    "end": 2295
                }
            ],
            "ref_mentions": [
                {
                    "start": 763,
                    "end": 767,
                    "matchedPaperCorpusId": "52967399"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.21435546875
        },
        {
            "corpus_id": "253202077",
            "title": "Supporting Argumentation Dialogues in Group Decision Support Systems: An Approach Based on Dynamic Clustering",
            "text": "It is available on GitHub (github.com/facebookresearch/fastText, accessed on 26 September 2022), where they offer their state-of-the-art model for English word vectors and word vectors for 157 additional languages. It diverges from Word2Vec by using subword information on word similarity tasks to improve its results. We used crawl-300d-2M (fasttext.cc/docs/en/english-vectors, accessed on 26 September 2022), which consists of 2-million-word vectors trained on common crawl with 600 B tokens, and we used a max length equaling 200. \n\nBidirectional encoder representations from transformers (BERT) is a language representation model released by Google. It considers the context when creating word and sentenceembedding vectors, where the exact two words can have two different vectors, [44,45]. We utilized the BERT-Base pre-trained model with 12 layers, 768 hidden states, 12 heads, and 110 M parameters found on their GitHub page (github.com/google-research/bert, accessed on 26 September 2022). \n\nSentence-BERT (SBERT) (github.com/UKPLab/sentence-transformers, accessed on 26 September 2022) is a modification of the original pre-trained BERT network that uses Siamese and triplet network structures to derive semantically meaningful sentence embeddings that can be compared using cosine similarity [46]. We utilized the all-MiniLM-L6-v2 (sbert.net/docs/pretrained_models, accessed on 26 September 2022) pre-trained model with 6 layers and 384 hidden states totaling 1 billion training pairs. Embeddings from language models (ELMo) is a state-of-the-art NLP framework developed by AllenNLP (allenai.org/allennlp/software/elmo, accessed on 26 September 2022). ELMo's representations differ from traditional ones because each token is assigned a representation that is a function of the entire input sentence.",
            "score": 0.4593886556790846,
            "section_title": "Word Embedders",
            "char_start_offset": 28323,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 64
                },
                {
                    "start": 65,
                    "end": 214
                },
                {
                    "start": 215,
                    "end": 318
                },
                {
                    "start": 319,
                    "end": 378
                },
                {
                    "start": 379,
                    "end": 533
                },
                {
                    "start": 536,
                    "end": 653
                },
                {
                    "start": 654,
                    "end": 795
                },
                {
                    "start": 796,
                    "end": 966
                },
                {
                    "start": 967,
                    "end": 998
                },
                {
                    "start": 1001,
                    "end": 1064
                },
                {
                    "start": 1065,
                    "end": 1308
                },
                {
                    "start": 1309,
                    "end": 1376
                },
                {
                    "start": 1377,
                    "end": 1496
                },
                {
                    "start": 1497,
                    "end": 1630
                },
                {
                    "start": 1631,
                    "end": 1662
                },
                {
                    "start": 1663,
                    "end": 1811
                }
            ],
            "ref_mentions": [
                {
                    "start": 787,
                    "end": 791,
                    "matchedPaperCorpusId": "226096901"
                },
                {
                    "start": 1303,
                    "end": 1307,
                    "matchedPaperCorpusId": "201646309"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.204833984375
        },
        {
            "corpus_id": "272524632",
            "title": "Improving Pretraining Data Using Perplexity Correlations",
            "text": "In addition to these commonsense baselines, we also run DSIR (Xie et al., 2023b), a lightweight training data selection technique based on n-gram overlaps that Li et al. (2024) found to be competitive with proxy LLM-based techniques and was also validated at scale (Parmar et al., 2024). Finally, we run the state-of-the-art method for pretraining data quality filtering found by Li et al., which is a fastText classifier that beats all of the heavier-weight proxy-LLM methods tested. The classifier was trained on a benchmark-agnostic and handcrafted objective, which is to classify data as Common Crawl3 (low quality) or OH2.5 (Teknium, 2023)  Results. We report average rankings over initial benchmarks in Table 1, and we find that our approach significantly outperforms the basic baselines of random sampling, language filtering, and DSIR. Compared to the existing state of the art from Li et al. (2024), our approach beats the performance of the default, English-filtered fastText classifier, but loses slightly once we add in a manual language filtering step to enable better performance on the multilingual LAMBADA datasets. For the maintext comparisons, we use the optional fastText classifier from our algorithm to select pretraining data at the page levels, but we show ablations without the classifier in Appendix H. \n\nFigure 2 shows how each data selection method affects benchmark performance in more detail. Each block of rows represents a data selection method, while an individual row represents an LLM within a method that targets a particular benchmark or set of benchmarks. Columns represent benchmarks. We see that language filtering and perplexity correlations both clearly optimize for the target benchmark: within each block, the benchmark column matching each row typically performs best. The pattern is much less obvious for DSIR -the heatmap looks more uniform across LLMs with different task targets. We also see that while language filtering has significant impacts on model performance, our performance significantly exceeds the impact of language filtering across all tested benchmarks. For some more analysis, including the types of domains selected, see Appendix O and J.",
            "score": 0.45891068759392534,
            "section_title": "INITIAL PRETRAINING EXPERIMENTS",
            "char_start_offset": 26288,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 287
                },
                {
                    "start": 288,
                    "end": 484
                },
                {
                    "start": 485,
                    "end": 654
                },
                {
                    "start": 655,
                    "end": 843
                },
                {
                    "start": 844,
                    "end": 1131
                },
                {
                    "start": 1132,
                    "end": 1327
                },
                {
                    "start": 1330,
                    "end": 1421
                },
                {
                    "start": 1422,
                    "end": 1592
                },
                {
                    "start": 1593,
                    "end": 1622
                },
                {
                    "start": 1623,
                    "end": 1812
                },
                {
                    "start": 1813,
                    "end": 1927
                },
                {
                    "start": 1928,
                    "end": 2116
                },
                {
                    "start": 2117,
                    "end": 2203
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.454833984375
        },
        {
            "corpus_id": "229292709",
            "title": "DH-FBK @ HaSpeeDe2: Italian Hate Speech Detection via Self-Training and Oversampling",
            "text": "The authors of the best-performing system, ItaliaNLP (Cimino et al., 2018), experiment with three different classification models: one based on linear SVM, another one based on a 1-layer BiLSTM and a newlyintroduced one based on a 2-layer BiLSTM which exploits multi-task learning with additional data from the 2016 SENTIPOLC task (Barbieri et al., 2016). The same training and test set released for HaSpeeDe have been recently used also for other types of evaluation, for example to compare classifier performance and settings across different languages (Corazza et al., 2020), confirming the importance of domain-specific language models and the effectiveness of deep learning approaches (in this case, LSTM + fasttext embeddings). Since the development of BERT-like transformer-based models, however, they have become state-of-theart approaches in several NLP tasks. This includes also hate speech detection for Italian, with the BERT model AlBERTo (Polignano et al., 2019b), which has recently achieved top-scores in two out of three HaSpeeDe 2018 tasks (Polignano et al., 2019a). For this reason, we decided to develop a classifier using the same model and the same approach.",
            "score": 0.45887197722109285,
            "section_title": "Introduction",
            "char_start_offset": 3208,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 355
                },
                {
                    "start": 356,
                    "end": 733
                },
                {
                    "start": 734,
                    "end": 869
                },
                {
                    "start": 870,
                    "end": 1084
                },
                {
                    "start": 1085,
                    "end": 1180
                }
            ],
            "ref_mentions": [
                {
                    "start": 555,
                    "end": 577,
                    "matchedPaperCorpusId": "215848824"
                },
                {
                    "start": 952,
                    "end": 977,
                    "matchedPaperCorpusId": "204914950"
                },
                {
                    "start": 1058,
                    "end": 1083,
                    "matchedPaperCorpusId": "209443031"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.306884765625
        },
        {
            "corpus_id": "275704171",
            "title": "A Random Oversampling and BERT-based Model Approach for Handling Imbalanced Data in Essay Answer Correction",
            "text": "This section provides an overview of the steps taken in this study, from text preprocessing to the evaluation phase, as shown in Figure 1. This research begins by inputting multiclass data derived from the essay responses of 8th-grade junior high school students in Indonesian language subjects, with scores ranging from 1 to 5. The data will be processed in two scenarios: one scenario employing traditional methods for data cleaning, which in-cludes case folding, tokenization, and stopword removal; and another one is only case folding and tokenization using the BERT model. After the preprocessing stage is completed, the data will be analyzed to identify the number of minority and majority classes, followed by processing using the Random Over Sampling technique. For traditional model, the balanced data will be processed using pretrained models with FastText (Traditional Method). The pretrained model scenario in the traditional method is divided into two parts: one using the Facebook model with a vector size of 300, and the other using the Wikipedia model with a vector size of 300. After the process with pretrained data is finish, the next step is sentence vector normalization to capture the meaning of each sentence. The final step involves classification using the Random Forest Classifier, Na\u00efve Bayes, and Backpropagation methods. Meanwhile, for the BERT model, the pretrained process is performed using only BERT base Uncased model. This dataset will be classified using the BERT Classifier with the BERT base Uncased model. The dataset used for classification is an oversampled dataset, which will then be analyzed to obtain the confusion matrix, including the calculation of Precision, Recall, F1-Score, and Accuracy. Based on the classification results and the values obtained from the confusion matrix, conclusions will be drawn regarding which method scenario demonstrates superior performance.",
            "score": 0.45837962830208856,
            "section_title": "Research Method",
            "char_start_offset": 5105,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 138
                },
                {
                    "start": 139,
                    "end": 577
                },
                {
                    "start": 578,
                    "end": 769
                },
                {
                    "start": 770,
                    "end": 888
                },
                {
                    "start": 889,
                    "end": 1094
                },
                {
                    "start": 1095,
                    "end": 1232
                },
                {
                    "start": 1233,
                    "end": 1349
                },
                {
                    "start": 1350,
                    "end": 1452
                },
                {
                    "start": 1453,
                    "end": 1544
                },
                {
                    "start": 1545,
                    "end": 1739
                },
                {
                    "start": 1740,
                    "end": 1919
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.65869140625
        },
        {
            "corpus_id": "260438804",
            "title": "Supply chain emission estimation using large language models",
            "text": "We additionally experimented with fine-tuning encoder based Large Language Models (LLMs) for classifying transaction data into commodity classes. These models are pre-trained using masked language modeling (MLM) and next sentence prediction (NSP) objectives. We tried the following commonly used models available on Huggingface. \n\nbert-base-uncased. The BERT (Bidirectional Encoder Representations from Transformers) is a popular pre-trained model developed by [4]. BERT-base-uncased is uncased version of BERT model which is trained on a massive amount of textual data from books, articles, and websites. The training process involves a transformer-based architecture, which enables BERT to capture bidirectional dependencies between words and context. BERT-base-uncased consists of 12 transformer layers, each with a hidden size of 768, and has 110 million parameters. roberta-base. RoBERTa (Robustly Optimized BERT pretraining Approach) is a variant of the BERT (Bidirectional Encoder Representations from Transformers) model developed by [10]. Similar to BERT, the RoBERTa-base also utilises dynamic masking during pre-training where tokens are randomly masked out. However, RoBERTa-base extends this approach by training with larger batch sizes, more iterations, and data from a more diverse range of sources. It also utilises a larger vocabulary and removes the next sentence prediction objective used in BERT, focusing solely on the masked language modeling objective. It excels in understanding the context and semantics of text, and captures subtle relationships and finegrained nuances in language. It consists of 12 transformer layers, each with a hidden size of 768, and has 125 million parameters. \n\nclimatebert/distilroberta-base-climate-f. The ClimateBERT language model is additionally pre-trained, on top of DistilRoBERTa model, on text corpus comprising climate-related research paper abstracts, corporate and general news and reports from companies [25]. It has been shown improve the performance of various climate related downstream tasks [6]. The model has 6 transformer layers, 768 dimension and 12 heads, totalizing 82M parameters (compared to 125M parameters for RoBERTa-base).",
            "score": 0.45826609115047967,
            "section_title": "Supervised fine-tuning for Scope3 (SFT3)",
            "char_start_offset": 16295,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 145
                },
                {
                    "start": 146,
                    "end": 258
                },
                {
                    "start": 259,
                    "end": 328
                },
                {
                    "start": 331,
                    "end": 349
                },
                {
                    "start": 350,
                    "end": 465
                },
                {
                    "start": 466,
                    "end": 605
                },
                {
                    "start": 606,
                    "end": 753
                },
                {
                    "start": 754,
                    "end": 870
                },
                {
                    "start": 871,
                    "end": 884
                },
                {
                    "start": 885,
                    "end": 1047
                },
                {
                    "start": 1048,
                    "end": 1169
                },
                {
                    "start": 1170,
                    "end": 1314
                },
                {
                    "start": 1315,
                    "end": 1475
                },
                {
                    "start": 1476,
                    "end": 1608
                },
                {
                    "start": 1609,
                    "end": 1710
                },
                {
                    "start": 1713,
                    "end": 1754
                },
                {
                    "start": 1755,
                    "end": 1973
                },
                {
                    "start": 1974,
                    "end": 2064
                },
                {
                    "start": 2065,
                    "end": 2202
                }
            ],
            "ref_mentions": [
                {
                    "start": 461,
                    "end": 464,
                    "matchedPaperCorpusId": "52967399"
                },
                {
                    "start": 1968,
                    "end": 1972,
                    "matchedPaperCorpusId": "239768231"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.12042236328125
        },
        {
            "corpus_id": "252199452",
            "title": "Adaptive Meta-learner via Gradient Similarity for Few-shot Text Classification",
            "text": "BERT base is used as the text encoder of all baselines. Because DS and MLADA have special requirements for textual representation and feature extraction, forcibly using BERT as encoder will be counterproductive. Thus, we re-implement the pre-train fastText embeddings (Joulin et al., 2016) for those model, and follow other settings in the original papers (Bao et al., 2020;Han et al., 2021). For the sake of fairness, the classifiers of these two algorithms use R2D2, so we constructed a comparison item with BERT as encoder. \n\nAll parameters are optimized with Adam optimizer (Kingma and Ba, 2015). The initial learning rates \u03b1, \u03b2 are separately set to 5e\u22125 and 2e\u22125, and we utilize 5 gradient updates for the base adaptation step. As for the N-way K-shot classification setting, all experiments use 25 examples for the query set. We randomly sample 100 training episodes, 100 validation episodes, and 1000 testing episodes per epoch and apply early stopping on validation for 20 epochs. We evaluate the performance of the model based on 5 different random seeds. All experiments are conducted on a GEFORCE RTX 3090 GPU.",
            "score": 0.4580959366142451,
            "section_title": "Implementation details",
            "char_start_offset": 19506,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 55
                },
                {
                    "start": 56,
                    "end": 211
                },
                {
                    "start": 212,
                    "end": 392
                },
                {
                    "start": 393,
                    "end": 526
                },
                {
                    "start": 529,
                    "end": 600
                },
                {
                    "start": 601,
                    "end": 733
                },
                {
                    "start": 734,
                    "end": 832
                },
                {
                    "start": 833,
                    "end": 989
                },
                {
                    "start": 990,
                    "end": 1065
                },
                {
                    "start": 1066,
                    "end": 1122
                }
            ],
            "ref_mentions": [
                {
                    "start": 356,
                    "end": 374,
                    "matchedPaperCorpusId": "201058594"
                },
                {
                    "start": 374,
                    "end": 391,
                    "matchedPaperCorpusId": "236428542"
                },
                {
                    "start": 578,
                    "end": 599,
                    "matchedPaperCorpusId": "6628106"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.603515625
        },
        {
            "corpus_id": "243985678",
            "title": "Character-level HyperNetworks for Hate Speech Detection",
            "text": "More recently, larger and deeper language models (LMs), that have been pretrained on massive heterogeneous corpora, were shown to yield state-of-the-art contextual text representations, leading to further improvement in text classification. We focus our attention on the popular transformer-based language encoder of BERT (Devlin et al., 2019) and its variants, as described below. \n\nBERT. Text classification using pretrained language models like BERT (Devlin et al., 2019) has been shown to give state-of-the-art performance on a variety of text processing tasks. While BERT generates task-agnostic contextual word embeddings, it can be optimized to target tasks via fine-tuning (Devlin et al., 2019). We follow the common practice of turning BERT into a text classifier by adding a final feed-forward network, which receives as input the embedding of the input text as processed by BERT (the '[CLS]' token embedding, which has been tuned to represent the meaning of the whole input text sequence), and outputs the target class probabilities via a softmax layer. Given labeled training data, the extended network parameters, including the weights of the terminal network, as well as BERT parameters, are fine-tuned jointly to optimize classification performance. \n\nRoBERTa. This model applies the same architecture as BERT, but has been trained on ten times more data, including news articles and Web content. Due to this augmentation of training data, and other modifications to the pretraining procedure and cost function, RoBERTa has been shown to outperform BERT on multiple benchmark datasets (Liu et al., 2019). \n\nALBERT. The architecture of ALBERT (Lan et al., 2020) was designed as a light variant of the transformer-based BERT model. It enhances BERT in several ways, including a factorization of the embedding parameters and cross-layer parameter sharing; both measures are intended to improve parameter efficiency as well as a form of regularization. ALBERT also replaces the next-sentence-prediction loss that is used in training BERT with sentence-order prediction loss, which focuses on modeling inter-sentence coherence.",
            "score": 0.4569033069152746,
            "section_title": "Hate Speech Detection with Pre-trained Language Models",
            "char_start_offset": 17128,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 240
                },
                {
                    "start": 241,
                    "end": 381
                },
                {
                    "start": 384,
                    "end": 389
                },
                {
                    "start": 390,
                    "end": 565
                },
                {
                    "start": 566,
                    "end": 703
                },
                {
                    "start": 704,
                    "end": 1064
                },
                {
                    "start": 1065,
                    "end": 1264
                },
                {
                    "start": 1267,
                    "end": 1275
                },
                {
                    "start": 1276,
                    "end": 1411
                },
                {
                    "start": 1412,
                    "end": 1619
                },
                {
                    "start": 1622,
                    "end": 1629
                },
                {
                    "start": 1630,
                    "end": 1744
                },
                {
                    "start": 1745,
                    "end": 1963
                },
                {
                    "start": 1964,
                    "end": 2137
                }
            ],
            "ref_mentions": [
                {
                    "start": 322,
                    "end": 343,
                    "matchedPaperCorpusId": "52967399"
                },
                {
                    "start": 453,
                    "end": 474,
                    "matchedPaperCorpusId": "52967399"
                },
                {
                    "start": 681,
                    "end": 702,
                    "matchedPaperCorpusId": "52967399"
                },
                {
                    "start": 1657,
                    "end": 1675,
                    "matchedPaperCorpusId": "202888986"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.65380859375
        },
        {
            "corpus_id": "225094545",
            "title": "Predicting Themes within Complex Unstructured Texts: A Case Study on Safeguarding Reports",
            "text": "This characteristic allows the model to learn the context of a word based on all of its surroundings, thus it generates more contextually-aware word representations. There are two steps in the BERT framework: pre-training and fine-tuning. In this step of the methodology, we use the base pre-trained BERT model, trained on the Books Corpus and English Wikipedia, for extracting contextualized sentence embeddings. The fine-tuning step consists of further training on the downstream tasks. \n\nStep 4: Classifiers We perform classification on a sentence level where each sentence had been assigned the theme of the passage the sentence belonged to. Here, we take 'ground truth' to be the annotations made by the social scientist expert annotators who were involved in creating the thematic framework (see Section2). For a baseline we use GNB classifier based on frequency-based features available in Scikit-Learn library [10], since it is considered a strong baseline for many text classification tasks [6]. A potential problem with linear classifiers is that they struggle with OOV words, fine-grained distinctions and unbalanced datasets. The fastText classifier [8] addresses this problem by integrating a linear model with a rank constraint, allowing sharing parameters among features and classes. Further, we fine-tune BERT for the classification task using a sequence classifier, a learning rate of 5e-5 and 4 epochs. In particular, we made use of the BERT's Hugging Face default transformers implementation for classifying sentences [17].",
            "score": 0.45541511873899304,
            "section_title": "Methods",
            "char_start_offset": 8500,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 165
                },
                {
                    "start": 166,
                    "end": 238
                },
                {
                    "start": 239,
                    "end": 413
                },
                {
                    "start": 414,
                    "end": 488
                },
                {
                    "start": 491,
                    "end": 645
                },
                {
                    "start": 646,
                    "end": 812
                },
                {
                    "start": 813,
                    "end": 1004
                },
                {
                    "start": 1005,
                    "end": 1137
                },
                {
                    "start": 1138,
                    "end": 1298
                },
                {
                    "start": 1299,
                    "end": 1420
                },
                {
                    "start": 1421,
                    "end": 1542
                }
            ],
            "ref_mentions": [
                {
                    "start": 918,
                    "end": 922,
                    "matchedPaperCorpusId": "10659969"
                },
                {
                    "start": 1000,
                    "end": 1003,
                    "matchedPaperCorpusId": "3116168"
                },
                {
                    "start": 1162,
                    "end": 1165,
                    "matchedPaperCorpusId": "1210515"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.470947265625
        },
        {
            "corpus_id": "276742203",
            "title": "Predictive Data Selection: The Data That Predicts Is the Data That Teaches",
            "text": "Our core hypothesis is that the data on which the losses can help predict the performance well is the data that can contribute to training effectively, thus the documents' predictive strength scores as defined in Eq. 1 are our metric to select data. However, computing Eq. 1 for every document is expensive since it requires computing losses of several models on all the data, thus we follow Li et al. (2024) and only select a small seed set of documents that are later used to train a fastText classifier (Joulin et al., 2017). As shown in Figure 2, we first randomly sample a small subset of data from the pre-training corpus, which is kept separate from the data used for later training, to compute the normalized losses. Specifically, we pick the 3,000 most frequent domains (e.g., wikipedia.org) in the corpus and randomly sample 300 examples for each domain for a wide coverage over the pre-training corpora, thus obtaining 900K samples in total. Then we choose the models from the Llama 1 and 2 series (Touvron et al., 2023a;b) ranging from 7B to 65B parameters -in total 6 models are used to compute the predictive strength for the data. We do not choose various models from different families to compute the predictive strength on purpose -in our preliminary experiments, we found that models from various families introduce significant evaluation noises, for example, they are sensitive to prompts and the prompt that is good for one model may not be suitable for another. This issue is particularly salient given that we are evaluating base models that are generally more sensitive to prompts than instruction-tuned models. Such noises would dramatically affect the computation of predictive strength, and thus we found that data selected this way could not outperform a random data selection baseline in our initial trials. This design differs from Thrush et al. (2024) which employs diverse models to estimate perplexity correlation. Such distinction probably explains why perplexity correlation fails to improve random data selection baselines significantly on a high-quality, pre-filtered data pool, as shown in the concurrent version of their paper. We run inference of these models on the 900K documents to obtain the loss ranks of these models on each document respectively.",
            "score": 0.45514986855047984,
            "section_title": "The Overall Framework",
            "char_start_offset": 9540,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 249
                },
                {
                    "start": 250,
                    "end": 528
                },
                {
                    "start": 529,
                    "end": 724
                },
                {
                    "start": 725,
                    "end": 952
                },
                {
                    "start": 953,
                    "end": 1145
                },
                {
                    "start": 1146,
                    "end": 1482
                },
                {
                    "start": 1483,
                    "end": 1634
                },
                {
                    "start": 1635,
                    "end": 1835
                },
                {
                    "start": 1836,
                    "end": 1946
                },
                {
                    "start": 1947,
                    "end": 2165
                },
                {
                    "start": 2166,
                    "end": 2292
                }
            ],
            "ref_mentions": [
                {
                    "start": 506,
                    "end": 527,
                    "matchedPaperCorpusId": "1210515"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.235107421875
        },
        {
            "corpus_id": "245124484",
            "title": "Unraveling Social Perceptions & Behaviors towards Migrants on Twitter",
            "text": ") or GloVe (Pennington, Socher, and Manning 2014), have considered a single word embedding representation for each word without considering the context of that specific word. Therefore, these language representations failed to capture the context. In contrast, BERT considers the context of a particular word from both directions -both from the left and right direction. As we noted earlier, BERT and RoBERTa are pre-trained on a diverse and large corpus. This allows these models to effectively understand most of the words used in online content compared to word2vec or GloVe. To sum up, BERT's fundamental principle is to employ bidirectional transformers for the feature extraction layer to extract the contextual meaning of the words. Following prior studies, we have fine-tuned our transformer-based models. BERT requires input data to be in a specific format. Thus, the [CLS] special token was used to indicate the beginning, and for the separation or the end of the sentence, the [SEP] was used. The next step was to tokenize the text corpus and extracting tokens that match BERT's vocabulary. For this task, we have used the Hug-gingFace python library (Wolf et al. 2020). This library includes pre-trained models and allows fine-tuning for the classification task. We have used 'BertForSequenceClassification' for our classification task. We have considered the BERT-Base-Uncased model comprised of 12-layers and 12heads with a total of 110M parameters. We have considered max_seq_length of 256.\n\nAs we mentioned earlier, the convolutional filters of CNN models play a crucial role in classification tasks. Thus, in combination with BERT, CNN can outperform BERT Base or BERT + LSTM (Dong et al. 2020;He et al. 2019;Mozafari, Farahbakhsh, and Crespi 2020). Accordingly, we consider the outputs from all individual layers of BERT architecture, and the outputs of each layer of the transformer are concatenated for the final result. We perform the convolutional operation with a window size -3, hidden size of BERT -768, and applying the max pooling on the convolution output from each transformer layer. Lastly, we concatenate these values,",
            "score": 0.4533597648364727,
            "section_title": "Results:",
            "char_start_offset": 36666,
            "sentence_offsets": [],
            "ref_mentions": [
                {
                    "start": 11,
                    "end": 49,
                    "matchedPaperCorpusId": "1957433"
                },
                {
                    "start": 1693,
                    "end": 1711,
                    "matchedPaperCorpusId": "219862286"
                },
                {
                    "start": 1711,
                    "end": 1726,
                    "matchedPaperCorpusId": "214595839"
                },
                {
                    "start": 1726,
                    "end": 1765,
                    "matchedPaperCorpusId": "221136077"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.159423828125
        },
        {
            "corpus_id": "267335406",
            "title": "Detecting mental disorder on social media: a ChatGPT-augmented explainable approach",
            "text": "Among the main alternative models that can be effectively used for detecting depres sion on social media, we carried out a comparative evaluation of BERTweet against different BERT-like models: \n\n\u2022 BERT [4]: it is a pre-trained language representation model based on the trans former architecture; \u2022 RoBERTa [34]: it is an improved version of BERT, which removes the Next Sentence Prediction (NSP) task from the pre-training phase and introduces dynamic masking to vary the masked tokens during language modeling; and \u2022 ALBERT [35]: it is a lightweight variant of BERT. It introduces some improvements such as factorized embedding parameterization, and inter-sentence coherence loss, by replacing the Next Sentence Prediction with the Sentence order prediction task during pre-training. \n\nAs shown in Table 1, BERTweet outperformed other models reaching a macro-Fl of 0.566 and an accuracy of 0.664, highlighting its superior ability in handling social media data with respect to the other tested models. Due to this, we choose BERTweet as the transformer-based model to be used within BERT-XDD.",
            "score": 0.4533343880855255,
            "section_title": "Model comparison for the pre-fine-tuning step",
            "char_start_offset": 21440,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 193
                },
                {
                    "start": 196,
                    "end": 569
                },
                {
                    "start": 570,
                    "end": 786
                },
                {
                    "start": 789,
                    "end": 1004
                },
                {
                    "start": 1005,
                    "end": 1095
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.05975341796875
        },
        {
            "corpus_id": "246210160",
            "title": "Privacy Policies Across the Ages: Content and Readability of Privacy Policies 1996-2021",
            "text": "We train one top-level classifier and 21 attribute-level classifiers using the fast-bert library [28], which is based on HuggingFace transformers [29]. We first fine-tune the bertbase-uncased language model using all unique policy texts in our corpus (4 epochs, batch size=8). Fine-tuning is a computationally expensive step (33 hours per epoch on our hardware), but improves classifier performance. \n\nFor the top-level classifier, we use the train-testvalidation split reported in [16], which is a random split with a 3:1:1 ratio using the majority-vote version of the OPP-115 dataset. We train the classifier for 100 epochs with a batch size of 8 using the train portion of the dataset. We use the validation portion of the dataset to evaluate the loss after each epoch. \n\nTable 1 shows the performance of our top-level classifier based on the test portion of the dataset in terms of F1 score, compared with prior work. The performance on average is slightly better than the fine-tuned BERT model reported in [16] (differences are due to the fine-tuning step, where we used a different corpus, and possible differences in the batch size and number of epochs). Our results are slightly worse than the results from [6] which may be due to differences in their train/test split and data augmentation process. Overall, the classifier performance is at about the same level as the inter-rater consistency reported for the OPP-115 dataset [15]. \n\nFor the attribute-level classifiers, the train-test-validation split from [16] results in imbalanced splits where some attribute labels are missing from some splits. Therefore, we create a separate stratified 3:1:1 split for each attribute. Because we have multi-label data, we apply an algorithm for multi-label stratification instead of the default stratifiers in scikit-learn [30], [31]. We tune the number of training epochs by comparing training loss and validation loss and selecting the final epoch as the one just before the two losses start to diverge.",
            "score": 0.45332010811427,
            "section_title": "Classifier training",
            "char_start_offset": 24874,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 151
                },
                {
                    "start": 152,
                    "end": 276
                },
                {
                    "start": 277,
                    "end": 399
                },
                {
                    "start": 402,
                    "end": 586
                },
                {
                    "start": 587,
                    "end": 688
                },
                {
                    "start": 689,
                    "end": 772
                },
                {
                    "start": 775,
                    "end": 921
                },
                {
                    "start": 922,
                    "end": 1161
                },
                {
                    "start": 1162,
                    "end": 1307
                },
                {
                    "start": 1308,
                    "end": 1440
                },
                {
                    "start": 1443,
                    "end": 1608
                },
                {
                    "start": 1609,
                    "end": 1683
                },
                {
                    "start": 1684,
                    "end": 1833
                },
                {
                    "start": 1834,
                    "end": 2004
                }
            ],
            "ref_mentions": [
                {
                    "start": 146,
                    "end": 150,
                    "matchedPaperCorpusId": "269498086"
                },
                {
                    "start": 482,
                    "end": 486,
                    "matchedPaperCorpusId": "218469499"
                },
                {
                    "start": 1011,
                    "end": 1015,
                    "matchedPaperCorpusId": "218469499"
                },
                {
                    "start": 1215,
                    "end": 1218,
                    "matchedPaperCorpusId": "3656756"
                },
                {
                    "start": 1435,
                    "end": 1439,
                    "matchedPaperCorpusId": "15817938"
                },
                {
                    "start": 1517,
                    "end": 1521,
                    "matchedPaperCorpusId": "218469499"
                },
                {
                    "start": 1828,
                    "end": 1832,
                    "matchedPaperCorpusId": "31086488"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.1231689453125
        },
        {
            "corpus_id": "243800633",
            "title": "A deep learning approach in predicting products\u2019 sentiment ratings: a comparative analysis",
            "text": "Features are individual measurable properties or dimensions for algorithms to process whereas feature extraction is the process of translating the processed texts into informative format. In general, the feature extractions techniques are dependent on the prediction models used in a sentiment analysis. In this study, word embeddings (i.e., vector representations of a particular word) were extracted as features, through several techniques, namely: A deep learning approach in predicting products' sentiment\u2026 1. Word2Vec: a pre-trained model that learns the relationship between the words in a corpus, and returns an embedded vector for each word in the text [42], 2. FastText: an extension of Word2Vec that breaks words into n-grams (smaller parts), e.g., 'apple' to 'app' with the intention of learning the morphology of the words. The model also returns a bag of embedded vectors for each word in the text [43]. \n\nWord2Vec and FastText might not handle polysemous words (i.e., words with multiple meaning) as they are deemed to be context-free (i.e., map the same word to the same embedding vector). For example, 'fire' would have the same representation in 'building on fire' and 'fire someone.' To mitigate this problem, scholars have begun to explore transformer-based embeddings, including BERT and its variants. BERT-variant models were pre-trained by incorporating the context of the word within the text in Wikipedia and BooksCorpus [44], and the embedding are then used through a classifier for predictions. As they produce contextualized word embeddings, they produce state-of-the-art results on Natural Language Processing tasks [12,34]. The BERT-base model is a bi-directional (both left-to-right and rightto-left direction) transformer for pre-training over a lot of unlabeled textual data to learn a language representation that can be used to fine-tune for specific classification tasks (see [44] for further details). One of its popular variant is RoBERTa (Robustly Optimized BERT approach), which was introduced by Facebook.",
            "score": 0.4527273752581382,
            "section_title": "Feature extraction",
            "char_start_offset": 15058,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 187
                },
                {
                    "start": 188,
                    "end": 303
                },
                {
                    "start": 304,
                    "end": 835
                },
                {
                    "start": 836,
                    "end": 916
                },
                {
                    "start": 919,
                    "end": 1104
                },
                {
                    "start": 1105,
                    "end": 1201
                },
                {
                    "start": 1202,
                    "end": 1321
                },
                {
                    "start": 1322,
                    "end": 1520
                },
                {
                    "start": 1521,
                    "end": 1652
                },
                {
                    "start": 1653,
                    "end": 1937
                },
                {
                    "start": 1938,
                    "end": 2045
                }
            ],
            "ref_mentions": [
                {
                    "start": 911,
                    "end": 915,
                    "matchedPaperCorpusId": "207556454"
                },
                {
                    "start": 1445,
                    "end": 1449,
                    "matchedPaperCorpusId": "52967399"
                },
                {
                    "start": 1644,
                    "end": 1648,
                    "matchedPaperCorpusId": "236184589"
                },
                {
                    "start": 1911,
                    "end": 1915,
                    "matchedPaperCorpusId": "52967399"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.50830078125
        },
        {
            "corpus_id": "265216855",
            "title": "Indonesian multilabel classification using IndoBERT embedding and MBERT classification",
            "text": "Based on this, pre-trained models can learn language representation as a whole. bidirectional encoder representations from transformers (BERT) are one of the pre-trained models widely used for various NLP tasks. BERT is a model that uses the encoder architecture of the transformer and is designed to use a bidirectional representation [8]. An example of the application of BERT is research conducted by D'Sa et al. [9], by classifying toxic comments on Twitter. In this study, a comparison was made using the FastText and BERT word embedding methods classified by the CNN model and bidirectional LSTM (BiLSTM). In addition, a comparison was also made using the entire word embedding model and classification using BERT. The optimal result of this study is using BERT on word embedding and classification with an F1 value of 84% in multi-class classification. Another study by Nabiilah et al. [10] also used a pre-trained model with BERT architecture. This study compared several pre-trained models that had been trained with Indonesian language corpus data, namely multilingual BERT (MBERT), Indonesia BERT (IndoBERT), and Indonesia robustly optimized BERT pretraining approach (IndoRoBERTa) small. The optimal result of this study is to use IndoBERT with an F1 value of 0.88978. \n\nPrevious research also aligns with recent developments in applying NLP for text classification cases using social media data, where pre-trained models can provide model classification results with better performance in recognizing text patterns. However, the increased access to communication by various groups on social media causes the text and language conveyed to be increasingly unstructured and difficult to analyze the pattern. So, to improve the performance of the model in analyzing the meaning and context of the text conveyed, the development of pre-trained model architecture needs to be further developed. Therefore, this research performs multilabel classification of toxic comments using a BERT-based pre-trained model. The striking difference from this research is using different pre-trained models at several stages.",
            "score": 0.45180008954545975,
            "section_title": "INTRODUCTION",
            "char_start_offset": 2376,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 79
                },
                {
                    "start": 80,
                    "end": 211
                },
                {
                    "start": 212,
                    "end": 340
                },
                {
                    "start": 341,
                    "end": 462
                },
                {
                    "start": 463,
                    "end": 611
                },
                {
                    "start": 612,
                    "end": 720
                },
                {
                    "start": 721,
                    "end": 859
                },
                {
                    "start": 860,
                    "end": 951
                },
                {
                    "start": 952,
                    "end": 1199
                },
                {
                    "start": 1200,
                    "end": 1280
                },
                {
                    "start": 1283,
                    "end": 1528
                },
                {
                    "start": 1529,
                    "end": 1717
                },
                {
                    "start": 1718,
                    "end": 1901
                },
                {
                    "start": 1902,
                    "end": 2017
                },
                {
                    "start": 2018,
                    "end": 2117
                }
            ],
            "ref_mentions": [
                {
                    "start": 336,
                    "end": 339,
                    "matchedPaperCorpusId": "52967399"
                },
                {
                    "start": 416,
                    "end": 419,
                    "matchedPaperCorpusId": "213529509"
                },
                {
                    "start": 893,
                    "end": 897,
                    "matchedPaperCorpusId": "255690182"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.64111328125
        },
        {
            "corpus_id": "270766804",
            "title": "DUTh at SemEval 2024 Task 8: Comparing classic Machine Learning Algorithms and LLM based methods for Multigenerator, Multidomain and Multilingual Machine-Generated Text Detection",
            "text": "For the monolingual part of this subtask, we select multilingualBERT (bert-base-multilingualcased) (Devlin et al., 2018).After comparing lots of classifiers, the two most performing are BERT and multilingualBERT.Previous research finds that there is no apparent benefit in training dedicated monolingual models for single language tasks, and actually by using a multilingual model instead may yield slightly improved performance de Vargas Feij\u00f3 and Moreira (2007).Our case is no different.We can see that multilingualBERT slightly outperforms BERT on Table 8.MultilingualBERT is a pretrained model on 104 languages and has 179M parameters.For the multilingual part, we select XLM-RoBERTa (xlm-roberta-base) (Conneau et al., 2019) as it demonstrates the best performance between all models we examine.We do not apply any preprocessing on the input text, so XLM-R gets used as a cased model.XLM-R is pre-trained on 2.5TB of filtered CommonCrawl data containing 100 languages and has 279M parameters.",
            "score": 0.45142875105103686,
            "section_title": "SubtaskA models",
            "char_start_offset": 5654,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 121
                },
                {
                    "start": 121,
                    "end": 212
                },
                {
                    "start": 212,
                    "end": 464
                },
                {
                    "start": 464,
                    "end": 489
                },
                {
                    "start": 489,
                    "end": 559
                },
                {
                    "start": 559,
                    "end": 639
                },
                {
                    "start": 639,
                    "end": 800
                },
                {
                    "start": 800,
                    "end": 889
                },
                {
                    "start": 889,
                    "end": 997
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.049957275390625
        },
        {
            "corpus_id": "226262236",
            "title": "Sparsity Makes Sense: Word Sense Disambiguation Using Sparse Contextualized Word Representations",
            "text": "We next provide detailed performance results broken down for the individual subcorpora of the evaluation dataset. Table 1 includes comparative results to previous methods that also use SemCor and optionally WordNet glosses as their training data. In Table 1 we report our results obtained by our model which derives sparse contextual word embeddings based on the averaged representations retrieved from the last four layers of BERT identical to how it was done in (Loureiro and Jorge, 2019). Figure 4 illustrates that reporting results from any of the last 4 layers would not change our overall results substantially. \n\nTable 1 reveals that it is only the LMMS 2348 (Loureiro and Jorge, 2019) approach which performs comparably to our algorithm. LMMS 2348 determines dense sense representations relying on the large BERT model as well. The sense representations used by LMMS 2348 are a concatenation of the 1024-dimensional centroids of each senses encountered in the training data, an 1024-dimensional vectors derived from the glosses of WordNet synsets and a 300-dimensional static fasttext embeddings. Even though our approach does not rely on static fasttext embeddings, we still managed to improve upon the best results reported in (Loureiro and Jorge, 2019). The improvement of our approach which uses the SemCor training data alone is 1.9 points compared to the LMMS 1024 , i.e. such a variant of the LMMS system (Loureiro and Jorge, 2019) which also relies solely on BERT representations for the SemCor training set.",
            "score": 0.45092259875779217,
            "section_title": "Comparative results",
            "char_start_offset": 24352,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 113
                },
                {
                    "start": 114,
                    "end": 246
                },
                {
                    "start": 247,
                    "end": 491
                },
                {
                    "start": 492,
                    "end": 617
                },
                {
                    "start": 620,
                    "end": 745
                },
                {
                    "start": 746,
                    "end": 835
                },
                {
                    "start": 836,
                    "end": 1104
                },
                {
                    "start": 1105,
                    "end": 1264
                },
                {
                    "start": 1265,
                    "end": 1524
                }
            ],
            "ref_mentions": [
                {
                    "start": 464,
                    "end": 490,
                    "matchedPaperCorpusId": "195345201"
                },
                {
                    "start": 666,
                    "end": 692,
                    "matchedPaperCorpusId": "195345201"
                },
                {
                    "start": 1237,
                    "end": 1263,
                    "matchedPaperCorpusId": "195345201"
                },
                {
                    "start": 1420,
                    "end": 1446,
                    "matchedPaperCorpusId": "195345201"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.350830078125
        },
        {
            "corpus_id": "207870323",
            "title": "CCNet: Extracting High Quality Monolingual Datasets from Web Crawl Data",
            "text": "We assess the quality of the resulting dataset by learning unsupervised word and sentence representations through fastText and BERT models. For fastText, we train 300dimensional word embeddings on the head, middle and tail subsets of the English and Polish CommonCrawl corpora, sorted by document perplexity. We evaluate these on standard semantic and syntactic analogy datasets (Mikolov et al., 2013). We observe in Table 1 a steady increase in performance as we go from the tail to the head of the dataset, confirming the positive impact of our filtering method based on document perplexity. We also train BERT models on the English (en), Russian (ru), Chinese (zh) and Urdu (ur) languages, using either the Wikipedia corpora or our new CommonCrawl datasets. For these languages, we use respectively 16G, 5G, 1.1G and 106M of raw Wikipedia data (full datasets), and we cap the head CommonCrawl data to 21G, 21G, 17G, 2.2G for English, Russian, Chinese and Urdu. That is, we consider roughly the same amount of data for English, but increase the amount of data for Russian, Chinese and Urdu. We train a BERT-BASE architecture (Devlin et al., 2018) on each of these corpora, without next sentence prediction (NSP) as in (Lample and Conneau, 2019). For better comparison, we early-stop all our models after two days of training on 16 Volta32 GPUs, and use the exact same number of steps for each model. We evaluate each model on the XNLI (Conneau et al., 2018) corpus by using the training data in each language. Results presented in",
            "score": 0.450888801114642,
            "section_title": "Training models on this dataset",
            "char_start_offset": 16864,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 139
                },
                {
                    "start": 140,
                    "end": 308
                },
                {
                    "start": 309,
                    "end": 402
                },
                {
                    "start": 403,
                    "end": 593
                },
                {
                    "start": 594,
                    "end": 760
                },
                {
                    "start": 761,
                    "end": 963
                },
                {
                    "start": 964,
                    "end": 1092
                },
                {
                    "start": 1093,
                    "end": 1247
                },
                {
                    "start": 1248,
                    "end": 1401
                },
                {
                    "start": 1402,
                    "end": 1511
                },
                {
                    "start": 1512,
                    "end": 1532
                }
            ],
            "ref_mentions": [
                {
                    "start": 1220,
                    "end": 1246,
                    "matchedPaperCorpusId": "202888986"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.55029296875
        },
        {
            "corpus_id": "227230531",
            "title": "IIITG-ADBU at SemEval-2020 Task 12: Comparison of BERT and BiLSTM in Detecting Offensive Language",
            "text": "BERT (Devlin et al., 2019) is a bi-directional model based on the transformer architecture. The transformer architecture is an architecture based solely on attention mechanism (Vaswani et al., 2017). The word embeddings produced by fastText is static in nature. Each word has a single embedding irrespective of the context in which the word appears. Static embeddings fail to handle polysemy. The embeddings produced by BERT are contextualized embeddings. The same word may have multiple embeddings depending on the context in which it appears. \n\nIn our study, we used the uncased large version of BERT2 to generate an embedding for each comment. This version has 24 layers and 16 attention heads. It generates a 1024-dimensional vector for each word. We used the 1024-dimensional vector of the Extract layer as the representation of the comment. Our classification layer consisted of a single Dense layer. For tasks A and B, this layer used the sigmoid activation function. For task C, it used the softmax activation function. The classifier was trained using the Adam optimizer with a learning rate of 2e-5. For tasks A and B, the binary crossentropy loss function was used. For task C, the sparse categorical crossentropy loss function was used. Tables 5 to 7 show the confusion matrices obtained by our classifiers for subtask A, B, and C respectively. In subtask A, BiLSTM performed better that BERT in predicting the majority class (NOT). BERT, however, performed better in predicting the minority class (OFF). In subtask B, BERT performed better than BiLSTM in predicting both the classes. In subtask C, BiLSTM performed better than BERT in predicting the minority classes (GRP and OTH). This was the reason for its superior performance in the subtask. BERT, however, performed better in predicting the majority class.",
            "score": 0.45020185605452373,
            "section_title": "BERT",
            "char_start_offset": 6551,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 91
                },
                {
                    "start": 92,
                    "end": 199
                },
                {
                    "start": 200,
                    "end": 261
                },
                {
                    "start": 262,
                    "end": 349
                },
                {
                    "start": 350,
                    "end": 392
                },
                {
                    "start": 393,
                    "end": 455
                },
                {
                    "start": 456,
                    "end": 544
                },
                {
                    "start": 547,
                    "end": 646
                },
                {
                    "start": 647,
                    "end": 697
                },
                {
                    "start": 698,
                    "end": 751
                },
                {
                    "start": 752,
                    "end": 846
                },
                {
                    "start": 847,
                    "end": 906
                },
                {
                    "start": 907,
                    "end": 974
                },
                {
                    "start": 975,
                    "end": 1027
                },
                {
                    "start": 1028,
                    "end": 1109
                },
                {
                    "start": 1110,
                    "end": 1176
                },
                {
                    "start": 1177,
                    "end": 1248
                },
                {
                    "start": 1249,
                    "end": 1356
                },
                {
                    "start": 1357,
                    "end": 1444
                },
                {
                    "start": 1445,
                    "end": 1516
                },
                {
                    "start": 1517,
                    "end": 1596
                },
                {
                    "start": 1597,
                    "end": 1694
                },
                {
                    "start": 1695,
                    "end": 1759
                },
                {
                    "start": 1760,
                    "end": 1825
                }
            ],
            "ref_mentions": [
                {
                    "start": 5,
                    "end": 26,
                    "matchedPaperCorpusId": "52967399"
                },
                {
                    "start": 176,
                    "end": 198,
                    "matchedPaperCorpusId": "13756489"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.6982421875
        },
        {
            "corpus_id": "265514554",
            "title": "Uncovering the semantics of concepts using GPT-4",
            "text": "probabilities based on the position x of the text document. \n\nA measure of the typicality of a text document in concept c is obtained by taking the logarithm of the categorization probability in c. \n\nBook Descriptions. In the comparative analysis reported in ref. 6, the model that achieved the highest correlation with the average of human typicality ratings used BERT as the language model. We fine-tuned the more than 100 million parameters of the BERT text classifier on a training set comprising 680 thousand book descriptions and their genre labels. \"Fine-tuning\" means that the parameters of the language models and of the categorization component are adjusted by minimizing a classification loss on the training data using deep learning techniques-this optimizes the language representation to the particular task (categorization) and data. We then applied the fine-tuned BERT text classifier to obtain the predicted categorization probabilities in the Mystery genre. What we called \"BERT Typicality\" is the logarithm of the categorization probability. \n\nThis measure performed well. The correlation over all book descriptions and the within-category correlations are very high for both genres (Table 1). Yet, it falls short of the performance of the typicality measures produced with GPT-4. \n\nWhat is important is not so much the performance difference between the measure produced with GPT-4 and the previous state-of-the-art but the fact that the measure produced with GPT-4 does not involve any training on research data, whereas the previous state-of-the-art involved model fine-tuning on a large training set of 680K text documents. Notably, fine-tuning BERT with a training set of 10K book descriptions instead of the full training set results in a 10% point decrease in the intraclass correlation. Additionally, a typicality measure based on pretrained BERT embeddings and the cosine similarity; thus, involving no training on research data, performs very poorly (SI Appendix, Table S4). \n\nTweets. In this context, the probabilistic text classifier predicts the party of the Congress member who published it. The typicality of a tweet in the focal party is obtained by taking the logarithm of the categorization probability produced by the text classifier.",
            "score": 0.4499144532633448,
            "section_title": "A categorization component that produces categorization",
            "char_start_offset": 21604,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 59
                },
                {
                    "start": 62,
                    "end": 197
                },
                {
                    "start": 200,
                    "end": 218
                },
                {
                    "start": 219,
                    "end": 392
                },
                {
                    "start": 393,
                    "end": 555
                },
                {
                    "start": 556,
                    "end": 848
                },
                {
                    "start": 849,
                    "end": 975
                },
                {
                    "start": 976,
                    "end": 1060
                },
                {
                    "start": 1063,
                    "end": 1091
                },
                {
                    "start": 1092,
                    "end": 1212
                },
                {
                    "start": 1213,
                    "end": 1299
                },
                {
                    "start": 1302,
                    "end": 1646
                },
                {
                    "start": 1647,
                    "end": 1813
                },
                {
                    "start": 1814,
                    "end": 2003
                },
                {
                    "start": 2006,
                    "end": 2013
                },
                {
                    "start": 2014,
                    "end": 2124
                },
                {
                    "start": 2125,
                    "end": 2272
                }
            ],
            "ref_mentions": [
                {
                    "start": 264,
                    "end": 265,
                    "matchedPaperCorpusId": "257267523"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.203369140625
        },
        {
            "corpus_id": "243865615",
            "title": "Detecting Health Advice in Medical Research Literature",
            "text": "LinearSVM: We chose the SVM algorithm with different vectorization methods to train the advicetype classifier using the Scikit-learn python package (Pedregosa et al., 2011). The penalty value C in LinearSVM was set to 1. A comparison of different word vector representation methods showed that the tf-idf vectorization performed similarly to the count vectorization, and adding bigrams also improved the SVM model's performance. BERT: BERT is a recent method for pre-training language representations, and it has achieved stateof-the-art results in a number of NLP tasks (Devlin et al., 2019;Fan et al., 2020). As for suggestion mining, the BERT-based model also outperformed  (Lee et al., 2020). In this study, we used the same BERT parameter settings as described above, except with the utilization of the BioBERT pre-trained model rather than the cased BERT-base one. We hypothesized that BioBERT would perform the best, followed by BERT and SVM.",
            "score": 0.44969482636410674,
            "section_title": "Prediction Model",
            "char_start_offset": 14295,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 173
                },
                {
                    "start": 174,
                    "end": 220
                },
                {
                    "start": 221,
                    "end": 428
                },
                {
                    "start": 429,
                    "end": 610
                },
                {
                    "start": 611,
                    "end": 696
                },
                {
                    "start": 697,
                    "end": 870
                },
                {
                    "start": 871,
                    "end": 949
                }
            ],
            "ref_mentions": [
                {
                    "start": 571,
                    "end": 592,
                    "matchedPaperCorpusId": "52967399"
                },
                {
                    "start": 592,
                    "end": 609,
                    "matchedPaperCorpusId": "208092878"
                },
                {
                    "start": 677,
                    "end": 695,
                    "matchedPaperCorpusId": "59291975"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.360595703125
        },
        {
            "corpus_id": "254246713",
            "title": "Human-in-the-Loop Hate Speech Classification in a Multilingual Context",
            "text": "For training the classifier, we relied on two main architectures: multinomial Na\u00efve Bayes (MNB), given its capability of performing text classification with few data points and in a computationally efficient manner, and on mBERT( Base uncased) (Devlin et al., 2019;Pires et al., 2019;Aluru et al., 2020), the thus-far most promising model for performing classification tasks in non-English languages due to its bidirectional nature and pre-training on 104 languages. We also finetuned and tested a series of other BERT models using the HuggingFace open source library (Wolf et al., 2019): GELECTRA Base and GBERT Base (Chan et al., 2020), FlauBERT (Le et al., 2019), French RoBERTa Base (Majumder, 2021), XLM-RoBERTa Base (Conneau et al., 2019) and Twitter-XLM-RoBERTa Base (Barbieri et al., 2021). These models were chosen because they feature a wide variety of writing and language styles in their pretraining, which is important given the heterogeneous composition of our corpus; they also yield the most promising benchmark results for our type of task.",
            "score": 0.44948302257921097,
            "section_title": "Classification Models",
            "char_start_offset": 16026,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 466
                },
                {
                    "start": 467,
                    "end": 798
                },
                {
                    "start": 799,
                    "end": 1057
                }
            ],
            "ref_mentions": [
                {
                    "start": 244,
                    "end": 265,
                    "matchedPaperCorpusId": "52967399"
                },
                {
                    "start": 618,
                    "end": 637,
                    "matchedPaperCorpusId": "224814107"
                },
                {
                    "start": 687,
                    "end": 703,
                    "matchedPaperCorpusId": "247011521"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.10125732421875
        },
        {
            "corpus_id": "258022214",
            "title": "Flood-Related Multimedia Benchmark Evaluation: Challenges, Results and a Novel GNN Approach",
            "text": "Our model is compared against baseline text classification methods (BERT, CNN, BiL-STM) and in multimodal approaches which proved great results in similar text classification tasks that was reproduced for evaluation in the corresponding dataset: \n\n\u2022 BERT [31] is based on transformer encoder architectures and has been successfully used on a variety of tasks in NLP (natural language processing). They compute vectorspace representations of natural language that are suitable for use in deep learning models. BERT models are usually pre-trained on a large corpus of text, then fine-tuned for specific tasks. \u2022 CNN [58] classify text by working through the 1-dimensional convolving filters which are used as ngram detectors and each filter specializing in a closely-related family of ngrams. Afterwards, max-pooling over time extracts the relevant ngrams for making a decision. The rest of the network classifies the text based on this information. \u2022 BiLSTM proved to be effective with high accuracy in text classification tasks [59] compared to RNN, CNN and LSTM.",
            "score": 0.4481815516295917,
            "section_title": "Baselines",
            "char_start_offset": 30891,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 245
                },
                {
                    "start": 248,
                    "end": 396
                },
                {
                    "start": 397,
                    "end": 508
                },
                {
                    "start": 509,
                    "end": 607
                },
                {
                    "start": 608,
                    "end": 790
                },
                {
                    "start": 791,
                    "end": 876
                },
                {
                    "start": 877,
                    "end": 947
                },
                {
                    "start": 948,
                    "end": 1063
                }
            ],
            "ref_mentions": [
                {
                    "start": 1028,
                    "end": 1032,
                    "matchedPaperCorpusId": "139108396"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.17041015625
        },
        {
            "corpus_id": "245853972",
            "title": "The GINCO Training Dataset for Web Genre Identification of Documents Out in the Wild",
            "text": "Dev split was used to optimize hyperparameters for different models. The main focus of the hyperparameter search was the number of training epochs to prevent overfitting and optimize micro and macro F1 scores. In accordance to this, 30 epochs for Transformer models and 200 epochs for fastText models were used. For the Transformer models, the sequence length of 512 tokens was used, and the learning rate was set to 10 \u22125 . The models, trained on the train split, were evaluated on the test split via micro F1 and macro F1 to measure both the instance-level and the label-level performance of a specific setup. In each experiment, at least 5 training runs were performed to assure a reasonable sample for measuring statistical significance of differences in performance of specific setups, which was tested via the Mann-Whitney U rank test.\n\nIn the remainder of this section, we present the results of our experiments:  To assess which technology is the most suitable for the automatic genre identification task, we compared fastText (Joulin et al., 2016) and two basesized Transformer-based pre-trained language models -the monolingual Slovenian SloBERTa (Ul\u010dar and Robnik-\u0160ikonja, 2021) (Ljube\u0161i\u0107 and Erjavec, 2018) only marginally increased its performance. The monolingual model SloBERTa and the multilingual model XLM-RoBERTa revealed to be the most suitable for the AGI task, with SloBERTa reaching 0.629 in micro F1 and 0.575 in macro F1, and XLM-RoBERTa 0.624 in micro F1 and 0.579 in macro F1. XLM-RoBERTa was included in the comparison following the findings of Repo et al. (2021) where it outperformed monolingual BERT models in this task. However, in our case it did not perform statistically significantly different from SloBERTa. Two main conclusions can be drawn from these results. For identifying genre, CNN-like classifiers seem not to be up to the task, while Transformer-based models achieve a drastic improvement of the results. Furthermore, similar to results of previous research, multilingual BERT models seem to be as good for modelling the phenomenon as monolingual BERT models. We currently have two hyp",
            "score": 0.4474164875426656,
            "section_title": "Experimental setup",
            "char_start_offset": 21353,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.53271484375
        },
        {
            "corpus_id": "264172780",
            "title": "Revisiting Sentiment Analysis for Software Engineering in the Era of Large Language Models",
            "text": "\u2022 BERT (bert-base-uncased) [15], which stands for Bidirectional Encoder Representations from Transformers, introduces two key pre-training tasks. The first is mask language modeling (MLM), where BERT learns to predict masked words in a given text. Additionally, BERT incorporates the next sentence prediction (NSP) task, training to determine whether the second sentence logically follows the first or is a random sentence from the training data. \u2022 RoBERTa (roberta-base) [35] is short for \"A Robustly Optimized BERT Pretraining Approach\". RoBERTa is a BERT variant distinguished by its innovative training strategies and hyperparameter choices. Notably, it eliminates the NSP task, employs a larger batch size, trains on a larger corpus than BERT, and utilizes a dynamic masking strategy during training. \u2022 ALBERT (albert-base-v2) [29], or \"A Lite BERT\", is another BERT variant designed to reduce model size and computational requirements while maintaining or improving performance. ALBERT retains the MLM task but replaces the NSP task with the sentence order prediction (SOP) task. In SOP, ALBERT is trained to predict whether pairs of sentences are correctly ordered or if their positions have been swapped. \u2022 XLNet (xlnet-base-cased) [67] primarily focuses on capturing contextual information and long-range dependencies in text. It employs an autoregressive pretraining method and introduces permutation language modeling, where word order in a sentence is randomly shuffled, and the model is trained to predict the original sequence. XLNet also incorporates innovations such as the \"two-stream self-attention\" mechanism. \u2022 DistilBERT (distilbert-base-uncased) [51] is a distilled and smaller version of the BERT model. DistilBERT is designed to be faster and more memory-efficient. DistilBERT adopts model compression or knowledge distillation to learn from a teacher BERT to capture the same knowledge but with fewer parameters. As DistilBERT combines efficiency and strong performance, it has been popular in research and industry settings.",
            "score": 0.4468390404551749,
            "section_title": "Evaluated Language Models",
            "char_start_offset": 23916,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 145
                },
                {
                    "start": 146,
                    "end": 247
                },
                {
                    "start": 248,
                    "end": 446
                },
                {
                    "start": 447,
                    "end": 539
                },
                {
                    "start": 540,
                    "end": 645
                },
                {
                    "start": 646,
                    "end": 805
                },
                {
                    "start": 806,
                    "end": 984
                },
                {
                    "start": 985,
                    "end": 1085
                },
                {
                    "start": 1086,
                    "end": 1212
                },
                {
                    "start": 1213,
                    "end": 1335
                },
                {
                    "start": 1336,
                    "end": 1541
                },
                {
                    "start": 1542,
                    "end": 1628
                },
                {
                    "start": 1629,
                    "end": 1726
                },
                {
                    "start": 1727,
                    "end": 1789
                },
                {
                    "start": 1790,
                    "end": 1937
                },
                {
                    "start": 1938,
                    "end": 2050
                }
            ],
            "ref_mentions": [
                {
                    "start": 27,
                    "end": 31,
                    "matchedPaperCorpusId": "52967399"
                },
                {
                    "start": 1240,
                    "end": 1244,
                    "matchedPaperCorpusId": "195069387"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.235595703125
        },
        {
            "corpus_id": "196203342",
            "title": "Context-specific Language Modeling for Human Trafficking Detection from Online Advertisements",
            "text": "In our study, we employ the feature models described above and compare the results of the binary classification corresponding to them. We use logistic regression and compute the precision and recall curve (PRC) to evaluate the performance of different models. Moreover, in this application, it is important to have a model with good recall while keeping high precision, i.e., a high positive predictive value (PPV) to avoid unnecessary actions. To do so, we investigate the sensitivity of models in different high PPVs. \n\nPre-processing. We choose to not remove stop words or not use any stemming or lemmatization techniques as we are faced with different writing structures which could be informative for our model. We test the impact of emojis and punctuation by training and testing our model by creating two text sets. In the first text set, we keep the emojis and punctuation and remove them in the second set. In the second set, we convert the emojis to words. Numbers in the texts are removed, because: 1) we have made the labels based on phone numbers and 2), the ads are likely to have the same age or same price throughout the texts. We then divide the data into an 80/20% training/testing set. In the following sections, we describe how each set of features is processed while using logistic regression as our fixed classification model. \n\nLDA Features. We begin with features coming from LDA topic modeling scores where we assign it to 12 topics. Gensim LDA is implemented by making a bag of words dictionary of our training set. We find this optimal topic number where we examined the explained LDA feature set via crossvalidation on January 2017 alone. \n\nAWV Features. Our FastText model is trained on a set including a minimum count of 2 words and a window size of 3 to give us a vector of dimension 100. After training the FastText model, the average word vector of the training set is computed. Using this saved language model from the training set, we compute the feature test vectors. \n\nBERT Features. For encoding our texts using BERT, we make a list of all documents and use the BERT service client.",
            "score": 0.44664971862382985,
            "section_title": "Experiments",
            "char_start_offset": 8512,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 134
                },
                {
                    "start": 135,
                    "end": 259
                },
                {
                    "start": 260,
                    "end": 444
                },
                {
                    "start": 445,
                    "end": 519
                },
                {
                    "start": 522,
                    "end": 537
                },
                {
                    "start": 538,
                    "end": 716
                },
                {
                    "start": 717,
                    "end": 822
                },
                {
                    "start": 823,
                    "end": 915
                },
                {
                    "start": 916,
                    "end": 966
                },
                {
                    "start": 967,
                    "end": 1143
                },
                {
                    "start": 1144,
                    "end": 1204
                },
                {
                    "start": 1205,
                    "end": 1348
                },
                {
                    "start": 1351,
                    "end": 1364
                },
                {
                    "start": 1365,
                    "end": 1458
                },
                {
                    "start": 1459,
                    "end": 1541
                },
                {
                    "start": 1542,
                    "end": 1666
                },
                {
                    "start": 1669,
                    "end": 1682
                },
                {
                    "start": 1683,
                    "end": 1819
                },
                {
                    "start": 1820,
                    "end": 1911
                },
                {
                    "start": 1912,
                    "end": 2003
                },
                {
                    "start": 2006,
                    "end": 2020
                },
                {
                    "start": 2021,
                    "end": 2120
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.381103515625
        },
        {
            "corpus_id": "258309541",
            "title": "Pre-trained Embeddings for Entity Resolution: An Experimental Analysis",
            "text": "As a result, the difference between the maximum and minimum F1 of dynamic models ranges from 4.9% ( 1 ) to 17% ( 2 ). \n\nThe second group of models includes the static, context-agnostic ones, which perform relatively poorly, even though they are combined with the best configuration of DeepMatcher, i.e., the stateof-the-art algorithm for this task and type of models. GloVe and FastText are actually combined with a two layer fully connected ReLU HighwayNet classifier followed by a softmax layer in the  3. \n\nclassification module in combination with a hybrid model for the attribute similarity vector module. On average, GloVe and FastText underperform the top model per dataset by 22% and 37%, respectively. Only in  3 and  4 , where all dynamic models exceed 0.95, the two static models exhibit high performance, with their F1 within 5% of the maximum one (this is another indication about the straightforward matching task posed by the bibliographic data of these two datasets). Note that FastText consistently outperforms GloVe across all datasets, since its character-level functionality is capable of addressing the out-of-vocabulary tokens that arise, due to the domain-specific terminology of each dataset, unlike GloVe. Overall, the static models underperform the dynamic ones in most cases, as reported in the literature [25], while the BERT-based models match the SentenceBERT ones, unlike for the previous ER tasks, due to the fine-tuning of their last layer. SentenceBERT models also benefit from fine-tuning, but to a lesser extent, probably because the sentence representing each entity is constructed in an ad-hoc manner, lacking any cohesiveness. Comparison to SotA. Figure 11(d) depicts the performance of the state-of-the-art supervised matching algorithms that leverage static and dynamic models, DeepMatcher+ [18] and DITTO [25]. We actually consider their optimized F1 that is reported in [25]. \n\nComparing DITTO to the dynamic models, we observe that its F1 is directly comparable to the best performing language model in each dataset. In  2 ,  3 and  4 , DITTO's F1 is lower by just \u22640.5%.",
            "score": 0.446283554580185,
            "section_title": "Supervised Matching",
            "char_start_offset": 47890,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 117
                },
                {
                    "start": 120,
                    "end": 367
                },
                {
                    "start": 368,
                    "end": 507
                },
                {
                    "start": 510,
                    "end": 610
                },
                {
                    "start": 611,
                    "end": 710
                },
                {
                    "start": 711,
                    "end": 983
                },
                {
                    "start": 984,
                    "end": 1230
                },
                {
                    "start": 1231,
                    "end": 1473
                },
                {
                    "start": 1474,
                    "end": 1665
                },
                {
                    "start": 1666,
                    "end": 1852
                },
                {
                    "start": 1853,
                    "end": 1918
                },
                {
                    "start": 1921,
                    "end": 2060
                },
                {
                    "start": 2061,
                    "end": 2115
                }
            ],
            "ref_mentions": [
                {
                    "start": 1333,
                    "end": 1337,
                    "matchedPaperCorpusId": "214743579"
                },
                {
                    "start": 1832,
                    "end": 1836,
                    "matchedPaperCorpusId": "195069071"
                },
                {
                    "start": 1847,
                    "end": 1851,
                    "matchedPaperCorpusId": "214743579"
                },
                {
                    "start": 1913,
                    "end": 1917,
                    "matchedPaperCorpusId": "214743579"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.357666015625
        },
        {
            "corpus_id": "251665011",
            "title": "Research on Implicit Intent Recognition Method Based on Prompt Learning",
            "text": "Table 5 shows the experimental results with different prompt templates for classification on the proposed CIID dataset. It can be seen that different prompts affect the recognition accuracy of the model. But no matter which prompt template is used (P 1 , P 2 , P 3 or P 4 ), the recognition rate of the corresponding model is higher than that of other five classification models in Table 6. Among these prompts, the model P4 obtains the best performance, so this study uses the results of P 4 to compare with other models below. Table 6 shows the experimental results of all the evaluated models on the CIID dataset, where Precision, Recall and F1 refer to the macro average. As can be seen from Table 6, the F1 score and accuracy of our proposed model are 0.975 and 0.976 respectively, and achieve the state-of-the-art results. From the results we can conclude that: (1) BERT model has certain advantages over other non-pretrained models such as BiLSTM [17], FastText [1] and TextCNN [14] because it owns the general language representation obtained by pre-training based on the large-scale corpus. (2) Compared with the BERT [7] model, our prompt-based model can more fully utilize the knowledge learned in the pre-training stage, furthermore, the prompts also play an important role in understanding text semantics. 6 analyzes the F1 scores of the baseline models BiLSTM+Att [27], BERT [7] and the proposed prompt-based model on each category. The BiL-STM+Att model adds the attention layer on the basis of the BiLSTM model which can assign weights for the words to enhance the performance and interpretability of the model. However, the BiLSTM+Att model has the lowest F1 score in each category in Fig. 6, which shows that only relying on optimizing the neural network cannot fully capture the implicit semantics in the sentence. The BERT model is pre-trained on a large-scale corpus and acquires the general knowledge, which is helpful for understanding implicit semantics. Therefore, F1 scores of all categories are higher than BiLSTM+Att model.",
            "score": 0.4460592486130728,
            "section_title": "Experimental results and analysis",
            "char_start_offset": 22912,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 119
                },
                {
                    "start": 120,
                    "end": 203
                },
                {
                    "start": 204,
                    "end": 390
                },
                {
                    "start": 391,
                    "end": 528
                },
                {
                    "start": 529,
                    "end": 675
                },
                {
                    "start": 676,
                    "end": 828
                },
                {
                    "start": 829,
                    "end": 1099
                },
                {
                    "start": 1100,
                    "end": 1318
                },
                {
                    "start": 1319,
                    "end": 1446
                },
                {
                    "start": 1447,
                    "end": 1627
                },
                {
                    "start": 1628,
                    "end": 1833
                },
                {
                    "start": 1834,
                    "end": 1978
                },
                {
                    "start": 1979,
                    "end": 2051
                }
            ],
            "ref_mentions": [
                {
                    "start": 954,
                    "end": 958,
                    "matchedPaperCorpusId": "16017905"
                },
                {
                    "start": 969,
                    "end": 972,
                    "matchedPaperCorpusId": "155325847"
                },
                {
                    "start": 985,
                    "end": 989,
                    "matchedPaperCorpusId": "9672033"
                },
                {
                    "start": 1127,
                    "end": 1130,
                    "matchedPaperCorpusId": "52967399"
                },
                {
                    "start": 1378,
                    "end": 1382,
                    "matchedPaperCorpusId": "9870160"
                },
                {
                    "start": 1389,
                    "end": 1392,
                    "matchedPaperCorpusId": "52967399"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.51025390625
        },
        {
            "corpus_id": "273901295",
            "title": "Assessing the Performance of ChatGPT-4, Fine-tuned BERT and Traditional ML Models on Moroccan Arabic Sentiment Analysis",
            "text": "Large Language Models (LLMs) have demonstrated impressive capabilities in various natural language processing tasks across different languages. However, their performance in low-resource languages and dialects, such as Moroccan Arabic (MA), requires further investigation. This study evaluates the performance of ChatGPT-4, different fine-tuned BERT models, FastText as text representation, and traditional machine learning models on MA sentiment analysis. Experiments were done on two open source MA datasets: an X(Twitter) Moroccan Arabic corpus (MAC) and a Moroccan Arabic YouTube corpus (MYC) datasets to assess their capabilities on sentiment text classification. We compare the performance of fully fine-tuned and pre-trained Arabic BERT-based models with ChatGPT-4 in zero-shot settings.",
            "score": 0.4451053086664898,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.39111328125
        },
        {
            "corpus_id": "266293970",
            "title": "Automatic Detection in Twitter of Non-Traumatic Grief Due to Deaths by COVID-19: A Deep Learning Approach",
            "text": "Machine learning models have been widely used to address text classification tasks. Most classical machine learning-based models extract, in a first step, some features from the documents (e.g., ''Bag of Words'' -BoW) and then feed them to a classifier to make a prediction. Some of these classification algorithms that have performed better on text classification tasks are Naive Bayes [39], [40], Support Vector Machines [41], [42], Hidden Markov Model [43], Gradient Boosting Trees [44] and Random Forest [45], [46]. However, models based on deep learning have recently outperformed classical machine learning approaches [47]. \n\nTransformers [48] are the most prominent architectures used for a wide range of Natural Language Processing tasks. These models are pre-trained on a large text corpus, and they can be fine-tuned on domain-specific data to achieve the best results in tasks such as text classification. BERT is a transformer-based language representation model which was designed to pre-train deep bidirectional representations from unlabeled text from BooksCorpus and English Wikipedia. In addition to pre-trained models, fine-tuning models are tuned to a specific task using a previously trained model. \n\nIn this study, we comparatively evaluated several BERT-based pre-trained language models and their expected applicability to this task.",
            "score": 0.4446027056513122,
            "section_title": "IV. PROPOSED APPROACH",
            "char_start_offset": 19257,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 83
                },
                {
                    "start": 84,
                    "end": 274
                },
                {
                    "start": 275,
                    "end": 519
                },
                {
                    "start": 520,
                    "end": 629
                },
                {
                    "start": 632,
                    "end": 746
                },
                {
                    "start": 747,
                    "end": 916
                },
                {
                    "start": 917,
                    "end": 1101
                },
                {
                    "start": 1102,
                    "end": 1218
                },
                {
                    "start": 1221,
                    "end": 1356
                }
            ],
            "ref_mentions": [
                {
                    "start": 387,
                    "end": 391,
                    "matchedPaperCorpusId": "4378770"
                },
                {
                    "start": 393,
                    "end": 397,
                    "matchedPaperCorpusId": "199058192"
                },
                {
                    "start": 423,
                    "end": 427,
                    "matchedPaperCorpusId": "63156868"
                },
                {
                    "start": 429,
                    "end": 433,
                    "matchedPaperCorpusId": "215977538"
                },
                {
                    "start": 455,
                    "end": 459,
                    "matchedPaperCorpusId": "10230346"
                },
                {
                    "start": 485,
                    "end": 489,
                    "matchedPaperCorpusId": "219690355"
                },
                {
                    "start": 508,
                    "end": 512,
                    "matchedPaperCorpusId": "52074974"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.376220703125
        },
        {
            "corpus_id": "241583460",
            "title": "A Large-scale Comprehensive Abusiveness Detection Dataset with Multifaceted Labels from Reddit",
            "text": "In this section, we report the performance of natural language understanding models on our dataset (CADD). We trained each model on the CADD training set, optimized it on the CADD validation set, and evaluated it on the CADD test set. We randomly shuffled the training data at the beginning of each training epoch. In order to detect abusiveness, we conducted two tasks, using the two most prominent labels out of the multiple labels. The details of each task are as follows: a. Task 1 is binary classification to determine whether a text is abusive or not (L2). b. Task 2 is multi-class classification to choose one out of four abusive language types (L1). \n\nAs for the baselines, we implemented two dictionary-based classifiers, support vector machine (SVM) and random forest (RF), and three pre-trained transformer models. We experimented with them on the CADD dataset. We fine-tuned SVM with linear kernel and C=10, and RF where max depth is set to 100. We employed a BERT's vocabulary to train dictionary-based models. We fine-tuned transformer models employing the default settings from the Huggingface library (Wolf et al., 2019): a. BERT (Devlin et al., 2018) is designed to pretrain bidirectional representations using masked language models. We fine-tuned the bert-basecased model. b. ALBERT (Lan et al., 2019) has significantly fewer parameters than a traditional BERT by two parameter reduction techniques. We fine-tuned the albert-base-v2 model. \n\nTable 5: Results for two tasks on our dataset. We report weighted averaged precision (wPre.), recall (wRec.), wF1, and macro-F1 (MF1) for each model on two tasks. SD indicates the standard deviation. Model names with \u2020 are models using contextual information. c. RoBERTa (Liu et al., 2019) is a robustly optimized BERT, through pre-training on larger data and careful validation of hyperparameters. We finetuned the roberta-base model. \n\nThe batch size of all models is 32 and fine-tuned for three epochs.",
            "score": 0.4444657001728135,
            "section_title": "Experimental setup",
            "char_start_offset": 20453,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 106
                },
                {
                    "start": 107,
                    "end": 234
                },
                {
                    "start": 235,
                    "end": 314
                },
                {
                    "start": 315,
                    "end": 434
                },
                {
                    "start": 435,
                    "end": 562
                },
                {
                    "start": 563,
                    "end": 565
                },
                {
                    "start": 566,
                    "end": 657
                },
                {
                    "start": 660,
                    "end": 825
                },
                {
                    "start": 826,
                    "end": 872
                },
                {
                    "start": 873,
                    "end": 957
                },
                {
                    "start": 958,
                    "end": 1023
                },
                {
                    "start": 1024,
                    "end": 1251
                },
                {
                    "start": 1252,
                    "end": 1291
                },
                {
                    "start": 1292,
                    "end": 1294
                },
                {
                    "start": 1295,
                    "end": 1418
                },
                {
                    "start": 1419,
                    "end": 1458
                },
                {
                    "start": 1461,
                    "end": 1507
                },
                {
                    "start": 1508,
                    "end": 1554
                },
                {
                    "start": 1555,
                    "end": 1570
                },
                {
                    "start": 1571,
                    "end": 1623
                },
                {
                    "start": 1624,
                    "end": 1660
                },
                {
                    "start": 1661,
                    "end": 1720
                },
                {
                    "start": 1721,
                    "end": 1723
                },
                {
                    "start": 1724,
                    "end": 1859
                },
                {
                    "start": 1860,
                    "end": 1896
                },
                {
                    "start": 1899,
                    "end": 1966
                }
            ],
            "ref_mentions": [
                {
                    "start": 1117,
                    "end": 1136,
                    "matchedPaperCorpusId": "268093756"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.1669921875
        },
        {
            "corpus_id": "216914042",
            "title": "WiC-TSV: An Evaluation Benchmark for Target Sense Verification of Words in Context",
            "text": "Table 4 shows the overall results for the three tasks. As can be observed, BERT is clearly better than FastText in all measures. In fact, perhaps surprisingly, FastText does not perform better than a naive baseline that retrieves all instances as true. This also reinforces the challenging nature of the benchmark, as even BERT is far from the human annotator performance (estimated on 85.3% for accuracy). Clearly, the definition information is more helpful than the hypernyms for BERT, while the combination of both attains the best overall results. \n\nIt is also remarkable the high recall of BERT, in contrast to the precision. This is mainly attributed to the domain-specific datasets as we are going to analyze below. Domain-Specific Results. Table 5 presents the results split by domain. Interestingly, FastText faces a massive challenge in adapting domains and generalising from the general to the specific domains. However, BERT shows to be much more robust to domain changes. In fact, perhaps surprisingly, the results on the domain-specific domains do not drop substantially with respect to the WNT/WKT test set, even though the training and development instances came from the same source (i.e. WordNet and Wiktionary). This can be attributed to the fact that specific domains highly constrain the set of possible senses for a word, resulting in an easier WSD classification task (Magnini et al., 2002). On the other hand, WordNet is known to be quite fine-grained (e.g., the noun run has 16 different senses in WordNet, plus many other senses including run as a verb). \n\nIn general, the BERT model can attain a very high recall in the domain-specific datasets, while the precision is still not too low. This model can be helpful in a retrieval setting where the recall may be relevant -for example, when the data is going to pass to a human than can filter.",
            "score": 0.4439982908604859,
            "section_title": "Results",
            "char_start_offset": 24967,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 54
                },
                {
                    "start": 55,
                    "end": 128
                },
                {
                    "start": 129,
                    "end": 252
                },
                {
                    "start": 253,
                    "end": 406
                },
                {
                    "start": 407,
                    "end": 551
                },
                {
                    "start": 554,
                    "end": 630
                },
                {
                    "start": 631,
                    "end": 722
                },
                {
                    "start": 723,
                    "end": 747
                },
                {
                    "start": 748,
                    "end": 793
                },
                {
                    "start": 794,
                    "end": 922
                },
                {
                    "start": 923,
                    "end": 984
                },
                {
                    "start": 985,
                    "end": 1205
                },
                {
                    "start": 1206,
                    "end": 1230
                },
                {
                    "start": 1231,
                    "end": 1414
                },
                {
                    "start": 1415,
                    "end": 1580
                },
                {
                    "start": 1583,
                    "end": 1714
                },
                {
                    "start": 1715,
                    "end": 1869
                }
            ],
            "ref_mentions": [
                {
                    "start": 1391,
                    "end": 1413,
                    "matchedPaperCorpusId": "7857920"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.7080078125
        },
        {
            "corpus_id": "274280560",
            "title": "Non-Contextual BERT or FastText? A Comparative Analysis",
            "text": "Word embedding is a way of representing words into dense vectors in a continuous space such that the vectors capture the semantic relationship between the words for the models to understand the context and meaning of the text. FastText, a context-independent method, basically captures the subword information, enabling it to learn rare words, misspelled words, and out-of-vocabulary words. It is recognized in the NLP community for its efficient performance in tasks like text classification and sentiment analysis. Despite being relatively old, it still remains one of the most effective alternatives when performing tasks on large datasets across various languages due to its subword-based approach. \n\nBERT (Bidirectional Encoder Representations from Transformers) (Devlin et al., 2018) word embeddings understand the meaning of a word based on its context in a sentence. The embeddings extracted just before the first embedding layer of the BERT architecture are referred to as non-contextual embeddings, while those obtained from the last hidden layer of BERT are known as contextual embeddings (Refer Fig 1). Numerous variations of BERT like IndicBERT (Kakwani et al., 2020), MuRIL (Khanuja et al., 2021), AfriBERT (Ralethe, 2020), and mBERT (Devlin et al., 2018) to name a few, are available for experiments. \n\nRecent studies have experimented with both Fast-Text and BERT for various tasks; however, most of them focus on exploring contextual BERT embeddings. Experiments of D 'Sa et al. (2020) demonstrated that BERT embeddings outperformed Fast-Text for classifying English text into toxic and nontoxic. Findings of Ahmed et al. (2024) suggested that BERT embeddings outperformed those of Fast-Text with an F1 score of 84% when evaluated for depressive post-detection in Bangla. \n\nWhile BERT consistently outperforms other word embeddings in various tasks for high-resource languages (HRLs) like English (Malik et al. (2021)), its effectiveness in low-resource languages (LRLs) remains relatively underexplored.",
            "score": 0.4436409057270061,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 226
                },
                {
                    "start": 227,
                    "end": 390
                },
                {
                    "start": 391,
                    "end": 516
                },
                {
                    "start": 517,
                    "end": 702
                },
                {
                    "start": 705,
                    "end": 874
                },
                {
                    "start": 875,
                    "end": 1114
                },
                {
                    "start": 1115,
                    "end": 1315
                },
                {
                    "start": 1318,
                    "end": 1467
                },
                {
                    "start": 1468,
                    "end": 1613
                },
                {
                    "start": 1614,
                    "end": 1788
                },
                {
                    "start": 1791,
                    "end": 2021
                }
            ],
            "ref_mentions": [
                {
                    "start": 1158,
                    "end": 1180,
                    "matchedPaperCorpusId": "267935553"
                },
                {
                    "start": 1221,
                    "end": 1236,
                    "matchedPaperCorpusId": "218973721"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.8251953125
        },
        {
            "corpus_id": "229293251",
            "title": "SNK @ DANKMEMES: Leveraging Pretrained Embeddings for Multimodal Meme Detection (short paper)",
            "text": "For text representation, we chose to use pretrained word embeddings for the Italian language. \n\nOur first model used fastText word representations (Bojanowski et al., 2016): non-contextual word embeddings. fastText word embeddings rely on subword information (bag of character ngrams) and thus provide valid representations for rare, misspelled or out-of-vocabulary words. Particularly, we used word vectors for the Italian language officially distributed in 2018 (Grave et al., 2018). Word embeddings are trained on Common Crawl and Wikipedia, using CBOW with positionweights, in dimension 300, with character n-grams of length 5, a window of size 5 and 10 negatives. We calculated the sentence vectors starting from the word vectors and using get_sentence_vector method of fastText python wrapper: each word vector is divided by its L2 norm and then averaged. Obtained sentence vector has dimension 300. \n\nOur second classifier used BERT word representations (Devlin et al., 2018): context-based word embeddings. BERT model uses word-piece tokenization: therefore it too provides embeddings for unseen words. In particular, we used GilBERTo2 , an Italian pretrained language model based on Facebook RoBERTa architecture and CamemBERT text tokenization approach; it was trained with the subword masking technique for 100k steps managing 71GB of Italian text with more than 11 billion words. As an interface for this language model, we used python library Hug-gingFace's Transformers (Wolf et al., 2019). To obtain sentence vectors, we took the output from the [CLS] token, which is prepended to the sentence during the preprocessing phase and is typically used for classification tasks; undoubtedly, there are also other methods for extracting sentence embeddings from BERT models that may prove more effective. Obtained sentence vector has dimension 768.",
            "score": 0.4436331365771796,
            "section_title": "Text representation",
            "char_start_offset": 2666,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 93
                },
                {
                    "start": 96,
                    "end": 205
                },
                {
                    "start": 206,
                    "end": 372
                },
                {
                    "start": 373,
                    "end": 485
                },
                {
                    "start": 486,
                    "end": 668
                },
                {
                    "start": 669,
                    "end": 861
                },
                {
                    "start": 862,
                    "end": 905
                },
                {
                    "start": 908,
                    "end": 1014
                },
                {
                    "start": 1015,
                    "end": 1110
                },
                {
                    "start": 1111,
                    "end": 1391
                },
                {
                    "start": 1392,
                    "end": 1504
                },
                {
                    "start": 1505,
                    "end": 1812
                },
                {
                    "start": 1813,
                    "end": 1856
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.5048828125
        },
        {
            "corpus_id": "271108715",
            "title": "Enhancing Depressive Post Detection in Bangla: A Comparative Study of TF-IDF, BERT and FastText Embeddings",
            "text": "Word embeddings are a type of word representation that allows words to be mapped to the vectors of real numbers.Each word is represented as a dense vector with fixed dimensions.These vectors capture the semantic meanings of the words in a continuous vector space, where words with similar meanings are located close to each other.BERT and FastText are both powerful word-embedding models used in natural language processing tasks.While BERT provides deep contextualised embeddings that capture bidirectional context, FastText leverages Sub-word information to effectively handle morphological variations.\n\n\u2022 BERT Embedding: BERT is a state-of-the-art pre-trained language representation model developed by Google [7].It uses a transformer-based architecture, which enables it to capture bidirectional contexts in the texts.BERT is pre-trained on large text corpora using masked language modelling (MLM) and next-sentence prediction (NSP) tasks.BERT generates embeddings by tokenising input text, mapping tokens to initial word embeddings and passing them through multiple transformer encoder layers to capture contextualised representations.These embeddings provide a deep contextual understanding of words within sentences, allowing for accurate processing of natural language data across various tasks.words into smaller units such as character n-Grams and training word embeddings using a skipgram model with negative sampling.During the training process, the model learns to predict the context (surrounding words) given a target word based on its sub-word representations, adjusting parameters to minimise the loss.The resulting embeddings provide dense vector representations for words, capturing semantic relationships and meanings based on their sub-word compositions.",
            "score": 0.4432002529631194,
            "section_title": "Word Embedding",
            "char_start_offset": 20588,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 112
                },
                {
                    "start": 112,
                    "end": 177
                },
                {
                    "start": 177,
                    "end": 330
                },
                {
                    "start": 330,
                    "end": 430
                },
                {
                    "start": 430,
                    "end": 604
                },
                {
                    "start": 606,
                    "end": 717
                },
                {
                    "start": 717,
                    "end": 823
                },
                {
                    "start": 823,
                    "end": 944
                },
                {
                    "start": 944,
                    "end": 1141
                },
                {
                    "start": 1141,
                    "end": 1304
                },
                {
                    "start": 1304,
                    "end": 1430
                },
                {
                    "start": 1430,
                    "end": 1620
                },
                {
                    "start": 1620,
                    "end": 1776
                }
            ],
            "ref_mentions": [
                {
                    "start": 713,
                    "end": 716,
                    "matchedPaperCorpusId": "209456845"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.5927734375
        },
        {
            "corpus_id": "267406876",
            "title": "The Language Model, Resources, and Computational Pipelines for the Under-Resourced Iranian Azerbaijani",
            "text": "The objective of this research was to establish fundamental pipelines and resources for the Iranian Azerbaijani language. A collection of subword embedding (fastText), transformer language model (BERT), text classification, token classification (POS tagging), and machine translation models for Iranian Azerbaijani NLP is available at Hugging Face repository2 , with corresponding code found on GitHub3 .The obtained results are summarized in Table 1: Embedding intrinsic evaluation: Our fastText model obtained an MRR of 0.46 in word analogy intrinsic evaluation indicating that the model can guess the analogies on average in the second guess. Language modeling perplexity: \n\nWe evaluated the model perplexity of our BERT language model, and achieved a perplexity score of 48.05. Given the constraints of a low-resource language, achieving a perplexity of 48.05 is quite commendable and suggests that despite the scarcity of training data, our model was able to produce relatively accurate predictions. Text classification: our fine-tuned BERT models performed better than the other two models on the text classification task. \n\nAfter the BERT model, the fastText-based baseline showed superior performance in comparison with the TF-IDF baseline (an extrinsic evaluation of the fastText embedding). We conducted a text classification comparison to showcase the impact of transliteration data for Iranian Azerbaijani in BERT masked language model pretraining. Our BERT model, trained on both transliterated and original Iranian Azerbaijani data, achieved an impressive macro-F1 of 0.89 in supervised text categorization. \n\nIn contrast, the BERT model trained solely on Iranian Azerbaijani data attained a significantly lower macro-F1 of 0.48. Moreover, training the model on transliterated data resulted in a mBert score of 0.85 macro-F1, further confirming the efficacy of utilizing transliterated data in transformer language models for downstream tasks. Token classification: The transformer-based tagger achieved a satisfactory performance with an accuracy of 0.86 and an F1-score of 0.67. This performance indicates that the fine-tuned BERT tagger is able to identify and classify language elements in the dataset with a moderate degree of accuracy and completeness.",
            "score": 0.4427041051920224,
            "section_title": "Results",
            "char_start_offset": 9346,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 121
                },
                {
                    "start": 122,
                    "end": 645
                },
                {
                    "start": 646,
                    "end": 675
                },
                {
                    "start": 678,
                    "end": 781
                },
                {
                    "start": 782,
                    "end": 1004
                },
                {
                    "start": 1005,
                    "end": 1128
                },
                {
                    "start": 1131,
                    "end": 1300
                },
                {
                    "start": 1301,
                    "end": 1460
                },
                {
                    "start": 1461,
                    "end": 1621
                },
                {
                    "start": 1624,
                    "end": 1743
                },
                {
                    "start": 1744,
                    "end": 1957
                },
                {
                    "start": 1958,
                    "end": 2094
                },
                {
                    "start": 2095,
                    "end": 2272
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.6171875
        },
        {
            "corpus_id": "255569888",
            "title": "API Entity and Relation Joint Extraction from Text via Dynamic Prompt-tuned Language Model",
            "text": "Training. We choose BERT [24] as a relation classifier because its pre-training task (i.e., Next Sentence Prediction) is consistent with our task, both of which are classification tasks. However, the implementation of relation classifier is not limited to BERT, we can also use TextCNN [27] and FastText [28]. In our current implementation, we use the BERT-base classifier to classify each input sentence into N relation types. Based on the N relation types, dynamic prompt generator generates the corresponding dynamic prompt. \n\nA mask language model (BERT) [24] and a linear layer comprise the classifier. Due to the seven API relation types, the linear layer's output dimension is set to 7. We obtain the latent vector from the CLS token when we enter the sentence into BERT. The latent vector obtained from the CLS token characterizes the sentence features better than other positions, resulting in better classification performance. The latent vector is then fed into the linear layer, which produces a vector with seven dimensions, each corresponding to a relation type. Finally, the classifier is trained on the classifier training set. In back propagation, we use the cross-loss entropy to calculate the classifier's loss and adjust the BERT and linear layer parameters. The loss function is formulated as follows, where  = [ 0 , . . . ,  \u22121 ] represents the linear layer's output result, and C represents the sentence's label.",
            "score": 0.44225090590529914,
            "section_title": "BERT-based Relation Classifier",
            "char_start_offset": 20062,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 9
                },
                {
                    "start": 10,
                    "end": 186
                },
                {
                    "start": 187,
                    "end": 309
                },
                {
                    "start": 310,
                    "end": 427
                },
                {
                    "start": 428,
                    "end": 527
                },
                {
                    "start": 530,
                    "end": 607
                },
                {
                    "start": 608,
                    "end": 778
                },
                {
                    "start": 779,
                    "end": 937
                },
                {
                    "start": 938,
                    "end": 1076
                },
                {
                    "start": 1077,
                    "end": 1143
                },
                {
                    "start": 1144,
                    "end": 1278
                },
                {
                    "start": 1279,
                    "end": 1343
                },
                {
                    "start": 1344,
                    "end": 1435
                }
            ],
            "ref_mentions": [
                {
                    "start": 286,
                    "end": 290,
                    "matchedPaperCorpusId": "9672033"
                },
                {
                    "start": 304,
                    "end": 308,
                    "matchedPaperCorpusId": "1210515"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.3330078125
        },
        {
            "corpus_id": "250058024",
            "title": "BERT-BiGRU Intelligent Classification of Metro On-Board Equipment Faults Based on Key Layer Fusion",
            "text": "Several mainstream text classification models in NLP field are used to train the data set in this paper. After 30 epochs, the experimental results shown in Table 4 are obtained (2-BERT, 4-BERT, 6-BERT, 8-BERT, and 12-BERT, respectively, represent the BERT models with different transformer layers). The following comparative analysis is carried out: \n\nFasttext, CNN, BiLSTM, and BiGRU classification models all take the word vector generated by embedding as the upstream task and then send it into the model itself for the completion of text classification. The three evaluation indexes of BiGRU and BiLSTM models are close, and the difference in F 1 -score is only 0.1%, which is because that BiGRU, as a variant of BiLSTM, has a similar structure, and the F 1 -score of CNN model is about 2% higher than that of CNN model. Due to the fact that CNN has strong ability to extract local features of text, however, which cannot capture text information with longer distance between front and back, the effect of slightly longer text classification task will be discounted. Compared with the fasttext model, the F 1 -score is increased by about 1%. As an improved version of word2vec model, the fasttext has simple classifier and fast classification speed, but its simple structure has general effect on the classification task of large-scale data. Both BiGRU and BiLSTM can effectively capture context information and generate high-level text semantic representation. Therefore, BiGRU or BiLSTM can be used as fault text classifier to achieve better classification effect. \n\nAs shown in Table 4, the comparison between 3 and 6 shows that the F 1 -score of ALBERT-BiLSTM is 6.66% higher than BiLSTM. Compared with 4 and 7, the F 1 -score of AlBERT-BiGRU is increased by 7.14% based on BiGRU. Then, by comparing the models after 4 and 8, the BiGRU with different layers of BERT model can increase the F 1 -score of the original model by more than 7.5% based on the original model.",
            "score": 0.44216545120690665,
            "section_title": "The Classification Effect of Different Classifiers on the Data Set.",
            "char_start_offset": 28835,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 104
                },
                {
                    "start": 105,
                    "end": 298
                },
                {
                    "start": 299,
                    "end": 349
                },
                {
                    "start": 352,
                    "end": 557
                },
                {
                    "start": 558,
                    "end": 824
                },
                {
                    "start": 825,
                    "end": 1070
                },
                {
                    "start": 1071,
                    "end": 1145
                },
                {
                    "start": 1146,
                    "end": 1345
                },
                {
                    "start": 1346,
                    "end": 1465
                },
                {
                    "start": 1466,
                    "end": 1570
                },
                {
                    "start": 1573,
                    "end": 1696
                },
                {
                    "start": 1697,
                    "end": 1788
                },
                {
                    "start": 1789,
                    "end": 1976
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.66943359375
        },
        {
            "corpus_id": "266288269",
            "title": "RESEARCH ON NAMED ENTITY RECOGNITION METHODS FOR URBAN UNDERGROUND SPACE DISASTERS BASED ON TEXT INFORMATION EXTRACTION",
            "text": "Bidirectional Encoder Representations from Transformers (BERT) is a pre-trained language model based on the Transformer architecture (Vaswani et al., 2017). It possesses rich language representation capabilities and can be applied to various natural language processing tasks. Figure 3 illustrates the architecture of the BERT model. The pre-training process of BERT is conducted in an unsupervised manner, where the model learns contextual representations of sentences by predicting missing words from a large amount of unlabelled text. The main contribution of the BERT model is the incorporation of bidirectional contextual information into the pre-training process, enabling a better understanding of semantics and grammar in natural language (Rogers, Kovaleva, & Rumshisky, 2020). Additionally, ALBERT incorporates other techniques such as sentence order prediction and downsampling to enhance both efficiency and performance (Zhang, Li, & Du, 2020). In comparison to the BERT model, the main differences of the ALBERT model lie in its parameter count and training efficiency. \n\nALBERT has only half or even fewer parameters than BERT, while achieving higher training efficiency. In most natural language processing tasks, the ALBERT model outperforms the BERT model in terms of various performance metrics and is more efficient in terms of parameter count and training time.",
            "score": 0.4418896386445309,
            "section_title": "ALBERT Model",
            "char_start_offset": 8192,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 156
                },
                {
                    "start": 157,
                    "end": 276
                },
                {
                    "start": 277,
                    "end": 333
                },
                {
                    "start": 334,
                    "end": 537
                },
                {
                    "start": 538,
                    "end": 785
                },
                {
                    "start": 786,
                    "end": 955
                },
                {
                    "start": 956,
                    "end": 1081
                },
                {
                    "start": 1084,
                    "end": 1184
                },
                {
                    "start": 1185,
                    "end": 1380
                }
            ],
            "ref_mentions": [
                {
                    "start": 133,
                    "end": 155,
                    "matchedPaperCorpusId": "13756489"
                },
                {
                    "start": 747,
                    "end": 784,
                    "matchedPaperCorpusId": "211532403"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.162353515625
        },
        {
            "corpus_id": "226262272",
            "title": "Hate-Speech and Offensive Language Detection in Roman Urdu",
            "text": "The more complex models such as BERT+LASER+GBDT and BERT+LAMB yield a higher F1-score of 0.71 and 0.72 respectively. We note from Table 4 and 6 that all baseline models utilizing BERT embeddings show a consistent As far as the performance of the variants of the proposed model is concerned, the BERT+CNNgram has the highest F1-score of 0.75, which is an improvement over the baseline. This result corroborates with the result of coarse-grained classification task presented in Table 4. XLM-RoBERTa based variation exhibit the second highest score of 0.72, which is identical to the baseline BERT+LAMB model. The lowest performance is shown by FastText+CNN-gram with an F1-score of 0.42 while RomUrEm based variant has the F1score of 0.64. These results substantiate the findings on coarse-grained classification task which suggest that instead of training embeddings, using existing pre-trained embeddings by fine-tuning them on the task in hand is a more perceptive choice. However, a carefully tailored model on top of these embeddings is advantageous. The results of both coarse-grained and fine-grained classification experiments support this conclusion. \n\nWe show some examples of fine-grained classification predictions in Table 7 for best performing baseline and proposed model variation to showcase challenges faced with respect to classification at the granular level. It is observed that the models are more \"confused\" between Abusive/Offensive and Profane as compared to other labels. It shows the limitation of the models with respect to intricacies of human language for subtle differences between profane language and targeted abuse or offensive language.",
            "score": 0.44160293081608315,
            "section_title": "Fine-grained Classification",
            "char_start_offset": 28789,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 116
                },
                {
                    "start": 117,
                    "end": 384
                },
                {
                    "start": 385,
                    "end": 607
                },
                {
                    "start": 608,
                    "end": 738
                },
                {
                    "start": 739,
                    "end": 974
                },
                {
                    "start": 975,
                    "end": 1054
                },
                {
                    "start": 1055,
                    "end": 1158
                },
                {
                    "start": 1161,
                    "end": 1377
                },
                {
                    "start": 1378,
                    "end": 1495
                },
                {
                    "start": 1496,
                    "end": 1669
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.31982421875
        },
        {
            "corpus_id": "221136053",
            "title": "Negative Confidence-Aware Weakly Supervised Binary Classification for Effective Review Helpfulness Classification",
            "text": "The CNN-based classifier consists of three vertically concatenated convolutional layers. Each layer has different convolutional filter sizes (namely 3 \u00d7m, 4 \u00d7m and 5 \u00d7m respectively, where m is the embedding size of a word). The output of the third convolutional layer is then fed into a linear prediction layer to predict the review helpfulness label. Moreover, we use the public pre-trained word2vec vectors from Glove [32] with m = 100. In particular, we adopt the cross-entropy loss to train the CNN model. \n\nBERT: We implement a BERT-based classifier with a popular natural language processing architecture (from HuggingFace's Transformer [43] library), which enables a quick implementation of the BERT transformer to process text. In particular, we adopt the pretrained BERT model (i.e. 'bert-base-uncased'). Next, following the setup of the CNN-based classifier, we again use a linear prediction layer to make the review helpfulness predictions and train the model by using the classic cross-entropy loss function. Both NN-based classifiers are trained using batch size 100 for 10 epochs, using the Adam optimiser with a learning rate of 10 \u22124 . \n\nClassifier Correction Baselines: Our experiments also apply three existing correction approaches (namely SVM-P, C-PU and P-conf ) as baselines to correct the basic classifiers: \n\n\u2022 SVM-P: The SVM-P approach applies a larger penalty value to the positive class than to the negative class according to the ratio of the number of positive examples versus the negative examples as suggested by Tang et al. [38]. We use LIBSVM [4] to apply the penalty values to different classes in the loss function. However, the experimental setup of SVM-P is different from other classification approaches in its training process. Indeed, the penalty values for different classes correspond to the class ratios of an unbalanced dataset. Therefore, we calculate the class ratio of the training dataset for SVM-P before down-sampling. In particular, the SVM-P approach is limited to the SVM-based classifiers.",
            "score": 0.441576592780034,
            "section_title": "CNN:",
            "char_start_offset": 30930,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 88
                },
                {
                    "start": 89,
                    "end": 224
                },
                {
                    "start": 225,
                    "end": 352
                },
                {
                    "start": 353,
                    "end": 439
                },
                {
                    "start": 440,
                    "end": 510
                },
                {
                    "start": 513,
                    "end": 736
                },
                {
                    "start": 737,
                    "end": 792
                },
                {
                    "start": 793,
                    "end": 1021
                },
                {
                    "start": 1022,
                    "end": 1152
                },
                {
                    "start": 1155,
                    "end": 1331
                },
                {
                    "start": 1334,
                    "end": 1562
                },
                {
                    "start": 1563,
                    "end": 1651
                },
                {
                    "start": 1652,
                    "end": 1767
                },
                {
                    "start": 1768,
                    "end": 1873
                },
                {
                    "start": 1874,
                    "end": 1969
                },
                {
                    "start": 1970,
                    "end": 2044
                }
            ],
            "ref_mentions": [
                {
                    "start": 421,
                    "end": 425,
                    "matchedPaperCorpusId": "1957433"
                },
                {
                    "start": 1557,
                    "end": 1561,
                    "matchedPaperCorpusId": "14272714"
                },
                {
                    "start": 1577,
                    "end": 1580,
                    "matchedPaperCorpusId": "961425"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.2005615234375
        },
        {
            "corpus_id": "266049371",
            "title": "BERT based natural language processing for triage of adverse drug reaction reports shows close to human-level performance",
            "text": "Four different classification models were investigated as described in  Python version 3.8.10 was used together with scikit-learn version 1.1.1, transformers version 4.17.0, sentence_transformers version 2.2.0, tensorflow version 2.9.1, tensorflow_text version 2.9.0, and pytorch version 1.11.0+cu113. \n\nXGBoost (XGB) version 1.6.1 with scikit-learn XGBClassifier API was used to perform the classification task for CV+XGB and BERT+XGB. XGBoost is an optimized distributed gradient library with parallel tree boosting (i.e., a boosted random forest model). \n\nThe Swedish pre-trained BERT large model (340M parameters) from AI Nordics [7] was used for the BERT based models (AER-BERT+XGB and AER-BERT clf). BERT large models use 1024 dimensions to create the text embeddings. The CrossEncoder module from the sentence-transformer (ver. 2.2.0) library was used as data pipeline and trainer for AER-BERT classifier, feeding only single sequences during training. \n\nContinued language model pre-training. The pre-trained BERT model from AI Nordics is trained on multiple Swedish data sources including FASS, an open resource containing product information (including ADR listings) for medicinal products authorized for use in Sweden. In addition to training the model for classification as is, it was investigated if a continued pre-training of the BERT language model on a domain specific text corpus (i.e., summaries of reports of suspected ADRs) could increase the performance. To this end, the language model was trained by masked language modelling on the historic report (pre-training) dataset. \n\nAll reports were concatenated and tokenized into sub-words. The tokenized data was split into 98,040 chunks of equal length (128 tokens) and 10% of the words were replaced by the mask token.",
            "score": 0.4411172045902852,
            "section_title": "Models and training",
            "char_start_offset": 10249,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 144
                },
                {
                    "start": 145,
                    "end": 173
                },
                {
                    "start": 174,
                    "end": 301
                },
                {
                    "start": 304,
                    "end": 436
                },
                {
                    "start": 437,
                    "end": 556
                },
                {
                    "start": 559,
                    "end": 705
                },
                {
                    "start": 706,
                    "end": 774
                },
                {
                    "start": 775,
                    "end": 834
                },
                {
                    "start": 835,
                    "end": 959
                },
                {
                    "start": 962,
                    "end": 1000
                },
                {
                    "start": 1001,
                    "end": 1229
                },
                {
                    "start": 1230,
                    "end": 1476
                },
                {
                    "start": 1477,
                    "end": 1596
                },
                {
                    "start": 1599,
                    "end": 1658
                },
                {
                    "start": 1659,
                    "end": 1789
                }
            ],
            "ref_mentions": [
                {
                    "start": 634,
                    "end": 637,
                    "matchedPaperCorpusId": "209896898"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.23974609375
        },
        {
            "corpus_id": "259792901",
            "title": "BERT\u2010TriF: An inductive short text classification model for power equipment defect records",
            "text": "In order to verify the classification effect, we compared BERT-TriF model to those baseline models like TextCNN, Tex-tRNN, fastText and BERT on two Chinese benchmark datasets and our PowerFault dataset. In addition, three integrated state-of-the-art inductive models are leveraged for comparative experiments on five benchmark datasets as supplementary. The classification results of each model are shown in Tables 4 and 5, where each value represents the average accuracy of the model running ten times and has passed the Student's t-test (p < 0.05). In the Powerfault dataset experiment, due to the large-scale pre-trained semantic representation, even though the single BERT model outperforms other benchmark models. Based on the BERT model, the BERT-TriF model further fuses the historical family features of each category to enhance the semantic representation of the input text, thereby improving the accuracy of text classification (Average accuracy of 85.7 as compared to 84.8 for single BERT and 84.3 for fastText and 82.2 for TextRNN). Moreover, in the multi-language general text classification experiments, the BERT-TriF model also achieved better accuracy, which verified that the model has good robustness and universality. Consequently, it can be applied not only in power inspection scenarios with certain colloquialisms and high text sparsity, but also in other common English and Chinese short text scenarios.",
            "score": 0.4407926346380552,
            "section_title": "Experimental results",
            "char_start_offset": 23598,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 202
                },
                {
                    "start": 203,
                    "end": 353
                },
                {
                    "start": 354,
                    "end": 551
                },
                {
                    "start": 552,
                    "end": 719
                },
                {
                    "start": 720,
                    "end": 1045
                },
                {
                    "start": 1046,
                    "end": 1237
                },
                {
                    "start": 1238,
                    "end": 1427
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.58935546875
        },
        {
            "corpus_id": "274982292",
            "title": "Research on Violent Text Detection System Based on BERT-fasttext Model",
            "text": "From the above experimental results, it can be seen that the best performance on the dataset is achieved by using the BERT fastetext Model. [12] Acc:BERT fasttext has an accuracy of 87.6%, which means that the model has a high percentage of correct predictions in all test samples. A high accuracy indicates that the model is doing well overall, and BERT fasttext performs very well compared to other models. \n\nPrecision:The precision is 87.4%, which means that among all the samples predicted as positive by the model, 87.4% are true positive samples. A high precision indicates that the model has a low false positive rate when predicting the positive class. \n\nRecall:The recall rate is 87.6%, which means that among all the samples that are actually positive, the model can correctly predict 87.6% of the positive samples. A higher recall rate indicates that the model can better capture positive samples and reduce false negatives. \n\nF1 socre:The F1-score is 86.6%, which is the harmonic mean of precision and recall. A high F1-score indicates that the model has found a good balance between precision and recall. \n\nEven though our model performs well on this dataset, we can still improve it. We can target the social network Sina Weibo, crawl the comment text data of cyber violence incidents, and obtain a dataset for research through data denoising and text preprocessing. We can use semi-supervised learning methods, use a small amount of manual intervention and multiple generations to select features, and finally construct a high-quality cyber violence language corpus to fill the gap in the Chinese cyber violence language corpus. \n\nThen, we can train the Chinese dataset based on this model.",
            "score": 0.4407242637799234,
            "section_title": "Results Analysis",
            "char_start_offset": 12919,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 139
                },
                {
                    "start": 140,
                    "end": 281
                },
                {
                    "start": 282,
                    "end": 408
                },
                {
                    "start": 411,
                    "end": 552
                },
                {
                    "start": 553,
                    "end": 660
                },
                {
                    "start": 663,
                    "end": 825
                },
                {
                    "start": 826,
                    "end": 935
                },
                {
                    "start": 938,
                    "end": 1021
                },
                {
                    "start": 1022,
                    "end": 1117
                },
                {
                    "start": 1120,
                    "end": 1197
                },
                {
                    "start": 1198,
                    "end": 1380
                },
                {
                    "start": 1381,
                    "end": 1643
                },
                {
                    "start": 1646,
                    "end": 1705
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.496337890625
        },
        {
            "corpus_id": "254579479",
            "title": "Self-admitted technical debt classification using natural language processing word embeddings",
            "text": "FastText introduced based on two studies [35], [36] which is essentially an extension of the Word2vec model, but each word is broken down into character n-grams. As a result, a word's vector is made up of the number of this character's n-grams. For example, the vector of the word \"method\" is a sum of the vectors of the n-grams characters: \"me\", \"met\", \"meth\", \"metho\", \"method\", \"eth\", \"etho\", \"etho\", \"hod\", \"hod\", \"od\". \n\nThe \"crawl-300d-2M.vec\" will be used: 2 million word vectors trained on Common Crawl (600B tokens) with a 300-dimensional vector [37]. \u2212 BERT stands for bidirectional encoder representations from transformers. BERT [38] is introduced in two variants, such as BERT-BASE and BERT-LARGE. The BERT-BASE has a number of transformer blocks of 12, hidden layer size of 768, attention heads of 12, and total parameters of 110M. The BERT-LARGE has a number of transformer blocks of 24, a hidden layer size of 1024, attention heads of 16, and total parameters of 340M. We followed the recommendation in [39] for tuning BERT parameters. The BERT-BASE model is used. It has a number of transformer blocks (12), a hidden layer size of 768, and attention heads (12). The TensorFlow hub was used to load the BERT pre-trained model. \u2212 USE is a family of pre-trained sentence encoders introduced by Google. We used this method for embedding sentences for classic machine learning, instead of using an average of word embedding calculated for each sentence. The universal sentence encoder model is trained on huge data and supports more than 16 languages. The output of this model is 512-dimensional vectors for each sentence [40].",
            "score": 0.44005265450528774,
            "section_title": "Word embedding methods",
            "char_start_offset": 28607,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 161
                },
                {
                    "start": 162,
                    "end": 244
                },
                {
                    "start": 245,
                    "end": 423
                },
                {
                    "start": 426,
                    "end": 449
                },
                {
                    "start": 450,
                    "end": 560
                },
                {
                    "start": 561,
                    "end": 635
                },
                {
                    "start": 636,
                    "end": 710
                },
                {
                    "start": 711,
                    "end": 845
                },
                {
                    "start": 846,
                    "end": 984
                },
                {
                    "start": 985,
                    "end": 1051
                },
                {
                    "start": 1052,
                    "end": 1080
                },
                {
                    "start": 1081,
                    "end": 1178
                },
                {
                    "start": 1179,
                    "end": 1242
                },
                {
                    "start": 1243,
                    "end": 1315
                },
                {
                    "start": 1316,
                    "end": 1465
                },
                {
                    "start": 1466,
                    "end": 1563
                },
                {
                    "start": 1564,
                    "end": 1639
                }
            ],
            "ref_mentions": [
                {
                    "start": 47,
                    "end": 51,
                    "matchedPaperCorpusId": "207556454"
                },
                {
                    "start": 1019,
                    "end": 1023,
                    "matchedPaperCorpusId": "52967399"
                },
                {
                    "start": 1119,
                    "end": 1123,
                    "matchedPaperCorpusId": "214362735"
                },
                {
                    "start": 1173,
                    "end": 1177,
                    "matchedPaperCorpusId": "214362735"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.2493896484375
        },
        {
            "corpus_id": "266566817",
            "title": "Empowering Medical Data Analysis: An Advanced Deep Fusion Model for Sorting Medicine Document",
            "text": "The fundamental yet essential component of document classification is the transformation of text into numeric vectors, enabling computers to conduct subsequent calculations and processing. This process is known as text representation [13]. Word embedding is a common method for text representation, which embeds words from the text into a space and expresses them in the form of vectors. One-hot, Bag-Of-Words models, TF-IDF, etc., are frequently used text representation methods. However, the aforementioned feature representations disregard the context relationships within the text, treating each word as independent, which fails to capture semantic information. Moreover, these representations can lead to issues of sparsity and dimensionality disaster in the feature matrix. Consequently, the focus of subsequent research has been on constructing low-dimensional, distributed, dense word vectors. Word2Vec [14] is a neural network language model that considers both contextual semantics and avoids dimensionality issues, yielding superior performance compared to earlier models. Additionally, FastText [15], released by Facebook in 2016, is a tool for word vector computation and document classification. In classification tasks, FastText often achieves accuracy comparable to deep neural networks but with faster training times. However, both Word2Vec and FastText are static models since they have a one-to-one correspondence between words and vectors, making them incapable of dynamically adjusting and optimizing for specific tasks, thus unable to address the issue of polysemy. \n\nIn contrast, BERT is a pre-trained language model trained on a large-scale corpus, based on the multi-layer Transformer encoder architecture, utilizing attention mechanisms to directly encode word meanings, effectively addressing the polysemy issue based on contextual information [16]. BERT places greater emphasis on pre-training word meanings, allowing downstream NLP tasks to perform fine-tuning operations based on the specific task's requirements.",
            "score": 0.4399548832339821,
            "section_title": "A. WORD EMBEDDING",
            "char_start_offset": 7853,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 188
                },
                {
                    "start": 189,
                    "end": 239
                },
                {
                    "start": 240,
                    "end": 387
                },
                {
                    "start": 388,
                    "end": 480
                },
                {
                    "start": 481,
                    "end": 665
                },
                {
                    "start": 666,
                    "end": 779
                },
                {
                    "start": 780,
                    "end": 901
                },
                {
                    "start": 902,
                    "end": 1083
                },
                {
                    "start": 1084,
                    "end": 1209
                },
                {
                    "start": 1210,
                    "end": 1334
                },
                {
                    "start": 1335,
                    "end": 1587
                },
                {
                    "start": 1590,
                    "end": 1876
                },
                {
                    "start": 1877,
                    "end": 2043
                }
            ],
            "ref_mentions": [
                {
                    "start": 911,
                    "end": 915,
                    "matchedPaperCorpusId": "253828309"
                },
                {
                    "start": 1107,
                    "end": 1111,
                    "matchedPaperCorpusId": "259376879"
                },
                {
                    "start": 1871,
                    "end": 1875,
                    "matchedPaperCorpusId": "239459434"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.82958984375
        },
        {
            "corpus_id": "227231467",
            "title": "TTUI at SemEval-2020 Task 11: Propaganda Detection with Transfer Learning and Ensembles",
            "text": "In both subtasks, we utilized the transformer-based models BERT and RoBERTa as they have achieved successful performance on several natural language processing tasks recently. Because both models released multiple versions of pre-trained models trained on the different corpora, we examined all the versions to find the proper one for the dataset; The uncased version of BERT-base and BERT-large and RoBERTa-base and RoBERTa-large models were tested in this shared task. These models were fine-tuned on the training set with a learning rate of 2e-5 for 10 epochs. Then, we took the best epoch per model based on the F1 score on the development set. All models were trained on Nvidia P100 using the Tensorflow version of the Huggingface Transformers library (Wolf et al., 2019).\n\nAfter that, we applied an average ensemble on the results across the models. Empirically, bagging ensemble techniques allow better performance and minimize over-fitting (Opitz and Maclin, 1999). Most participants with top-ranked performance in many other machine learning competitions used this technique. We averaged the probability distributions of the classifiers and then took the maximum prediction over the average probability distribution. We adopt the best-performing ensemble models per each task on the development set for our final models on the test set.",
            "score": 0.4398318781790761,
            "section_title": "Fine Tuned Models",
            "char_start_offset": 2754,
            "sentence_offsets": [],
            "ref_mentions": [
                {
                    "start": 757,
                    "end": 776,
                    "matchedPaperCorpusId": "268093756"
                },
                {
                    "start": 948,
                    "end": 972,
                    "matchedPaperCorpusId": "2594813"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.1802978515625
        },
        {
            "corpus_id": "252426645",
            "title": "Semantic relational machine learning model for sentiment analysis using cascade feature selection and heterogeneous classifier ensemble",
            "text": "Johnson & Zhang (2014) employed a one-hot encoding technique and CNN to extract features for classification. By combining data augmentation approach with the pretrained language model, Lu et al. ( 2021) created a unified model termed PEA. It also includes an ensemble method that produced good results by combining the results of the fundamental RNN-based and BERT-based models. Mehta, Pandya & Kotecha (2021) developed and deployed a stock price prediction accuracy tool that considered public mood and used machine learning algorithms and Long Short-Term Memory to achieve the best accuracy of 92.45%. Chinnalagu & Durairaj (2021) suggested a high-performance yet cost-effective model that classified text and word embedding utilizing the fastText package from Facebook's AI research (FAIR) Lab, as well as the standard Linear Support Vector Machine (LSVM). The accuracy of the fastText model achieved is 90.71%. Barreto et al. (2021) presented an evaluation of text representation models from the classical to the modern era is carried out. Challenges in linguistic styles for NLP and sentiment analysis are addressed. Trendy BERT architecture and 22 datasets from distinct domains and five classification algorithms are used. Chinatalapudi, Battineni & Amenta (2021) proposed a Deep Learning (DL) based analysis of Indian covid-19 tweets through the lock down period. Different emotions were analyzed using Bi-Directional Encoder Representation from Transformers (BERT) and results compared with traditional logistic regression (LR), support vector machines (SVM), and long short-term memory (LSTM). Accuracy of BERT model, LR, SVM, LSTM are 89%, 75%, 74.75% and 65% respectively. Jiang et al. ( 2022) suggested a hybrid model (BERT-BiLSTM-TextCNN) to obtain better accuracy on textbased psychological analysis of online comments. BiLSTM and TextCNN are used to capture local correlation, while BERT is used to produce word vectors in this model. Mohammad et al. (2021) suggested a bidirectional CNN-RNN deep model based on attention (ABCDM).",
            "score": 0.4397919641991035,
            "section_title": "RELATED LITERATURE",
            "char_start_offset": 16907,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 108
                },
                {
                    "start": 109,
                    "end": 238
                },
                {
                    "start": 239,
                    "end": 378
                },
                {
                    "start": 379,
                    "end": 603
                },
                {
                    "start": 604,
                    "end": 859
                },
                {
                    "start": 860,
                    "end": 914
                },
                {
                    "start": 915,
                    "end": 1043
                },
                {
                    "start": 1044,
                    "end": 1121
                },
                {
                    "start": 1122,
                    "end": 1229
                },
                {
                    "start": 1230,
                    "end": 1371
                },
                {
                    "start": 1372,
                    "end": 1603
                },
                {
                    "start": 1604,
                    "end": 1684
                },
                {
                    "start": 1685,
                    "end": 1834
                },
                {
                    "start": 1835,
                    "end": 1950
                },
                {
                    "start": 1951,
                    "end": 2046
                }
            ],
            "ref_mentions": [
                {
                    "start": 379,
                    "end": 409,
                    "matchedPaperCorpusId": "233866568"
                },
                {
                    "start": 604,
                    "end": 632,
                    "matchedPaperCorpusId": "245318383"
                },
                {
                    "start": 1230,
                    "end": 1270,
                    "matchedPaperCorpusId": "233457093"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.265380859375
        },
        {
            "corpus_id": "259036855",
            "title": "Transfer Learning for Sentiment Classification Using Bidirectional Encoder Representations from Transformers (BERT) Model",
            "text": "It is worth nothing that despite the better accuracy produced by SVM as compared to KNN and NB, SVM performs efficiently, which conforms to the finding in [31]. Unlike the finding in [31], our study shows that BERT outperforms SVM in accuracy. It is also important to note that in order to produce word vectors with the best possible quality for traditional machine learning classifiers, text data preprocessing is essential. Machine learning approaches also have the disadvantage of producing a large number of features, from which the most important ones must be selected automatically through methods for choosing features. In contrast, BERT employs the BERT tokenizer to transform content directly into a designated input structure without needing any pre-processing. On top of that, unlike machine learning methods, BERT uses a bidirectional transformer that considers both context directions.",
            "score": 0.4396506628741437,
            "section_title": "Result and Discussion",
            "char_start_offset": 28709,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 160
                },
                {
                    "start": 161,
                    "end": 243
                },
                {
                    "start": 244,
                    "end": 425
                },
                {
                    "start": 426,
                    "end": 626
                },
                {
                    "start": 627,
                    "end": 771
                },
                {
                    "start": 772,
                    "end": 898
                }
            ],
            "ref_mentions": [
                {
                    "start": 155,
                    "end": 159,
                    "matchedPaperCorpusId": "218950523"
                },
                {
                    "start": 183,
                    "end": 187,
                    "matchedPaperCorpusId": "218950523"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.27734375
        },
        {
            "corpus_id": "252715834",
            "title": "Improving Sentiment Analysis By Emotion Lexicon Approach on Vietnamese Texts",
            "text": "We present our approach for constructing the classification models in this section. In addition, according to [1], to represent a text in vector, word embedding is a standard choice. Hence, we choose the fastText 3 word embedding which was created on Vietnamese text data [19] as the pre-trained word embedding because this pre-trained word embedding gives good results with the social media data [20]. The brief details of classification models are described below.\n\nText-CNN [21]: Text-CNN is an adaption of the convolutional model used for text classification. In the Text-CNN, text data can be viewed as sequential data such as time series data or a one-dimensional matrix. Then, the convolutional layers extract important features from the texts.\n\nBi-GRU [22]: GRU is a more advanced kind of Bi-LSTM, often referred to as update gate and reset gate, which was utilized to alleviate the gradient loss issue that traditional RNNs encountered. The two vectors essentially determine what data should be sent to the output. The unique feature is that it can be taught to retain old data without erasing output prediction-related data.\n\nBERT [18]: BERT is a contextualized word representation model pre-trained using bidirectional transformers and based on a masked language model. BERT showed power in many NLP tasks including the sentiment analysis task and is currently the SOTA method. BERT and its variants are called the BERTology [23].\n\nPhoBERT [24]: PhoBERT is a monolingual language model trained on Vietnamese text. The architecture of PhoBERT is based on the RoBERTa model and is optimized for training on large-scale Vietnamese corpus. For the downstream task, the PhoBERT model requires the VnCoreNLP [17] for pre-processing texts.\n\nXLM-R [25]: XLM-R is a multilingual model that was developed with more than two terabytes of filtered and cleaned 3 https://fasttext.cc/docs/en/crawl-vectors.html CommonCrawl data. Important contributions of XLM-R include up-sampling low-resource languages during training and vocabulary generation, creating a larger common vocabulary, and increasing the",
            "score": 0.43922303393011036,
            "section_title": "C. Classification models",
            "char_start_offset": 12986,
            "sentence_offsets": [],
            "ref_mentions": [
                {
                    "start": 110,
                    "end": 113,
                    "matchedPaperCorpusId": "235690410"
                },
                {
                    "start": 272,
                    "end": 276,
                    "matchedPaperCorpusId": "3411445"
                },
                {
                    "start": 397,
                    "end": 401,
                    "matchedPaperCorpusId": "221970944"
                },
                {
                    "start": 477,
                    "end": 481,
                    "matchedPaperCorpusId": "9672033"
                },
                {
                    "start": 1141,
                    "end": 1145,
                    "matchedPaperCorpusId": "52967399"
                },
                {
                    "start": 1436,
                    "end": 1440,
                    "matchedPaperCorpusId": "211532403"
                },
                {
                    "start": 1451,
                    "end": 1455,
                    "matchedPaperCorpusId": "211677475"
                },
                {
                    "start": 1713,
                    "end": 1717,
                    "matchedPaperCorpusId": "4547415"
                },
                {
                    "start": 1751,
                    "end": 1755,
                    "matchedPaperCorpusId": "207880568"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.299560546875
        },
        {
            "corpus_id": "277495873",
            "title": "An adaptive search mechanism with convolutional learning networks for online social media text summarization and classification model",
            "text": "The BERT model is utilized for the feature extraction process 29 . This model was chosen because it can capture deep contextual relationships within the text by processing them bi-directionally. Unlike conventional models that read text sequentially, BERT considers both a word's left and right context, giving a more comprehensive comprehension of language. This capacity significantly improves the model's performance in SA, classification, and summarization tasks. BERT has been shown to outperform other techniques, such as Word2Vec and GloVe, in understanding complex semantics and word dependencies. Pre-trained BERT models can also be fine-tuned for specific tasks, allowing faster training and better generalization. Its robustness across various natural language processing tasks makes it an ideal choice over other methods lacking such depth in contextual understanding. Figure 2 illustrates the structure of the BERT model. Computers can only understand mechanical language; the words should be converted into word vectors or numerical representations, then recorded as data matrices to recognize the textual language. The model's performance shows limits among the well-known context-based word embedding methods, like FastText, Glove, and Word2Vec. It uses bi-directional Transformer encoding as feature extractors, while the single encoder processes standard sequences and the other processes inverse sequences. Through this bi-directional language model training, BERT understands the contextual information of the language more intensely and has excellent semantic diversity. Compared with uni-directional encoders, which can solely rely on sequence information to remove semantics, it has a predominant capability to remove semantic information, decreasing the likelihood of making mistakes before making predictions. \n\nEven though the statement lengths in the dataset are different, it is required by inputs to BERT to preserve uniform length. Since barrage comments normally create small texts, the statement length of 200 characters is normalized. For all the cases where the statement surpasses 200 characters, the first 200 characters are maintained; when the length falls apart from 200 characters, padding with [0] is used. This paper uses the BERT method, particularly the BERT-base-Chinese form designed for the Chinese language. The method includes 12 layers of encoder networks, with every layer's hidden state dimension fixed to 768, producing 768-dimension feature vectors, and is furnished with 12 attention heads.",
            "score": 0.43907683199084485,
            "section_title": "BERT feature extraction",
            "char_start_offset": 10728,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 66
                },
                {
                    "start": 67,
                    "end": 194
                },
                {
                    "start": 195,
                    "end": 358
                },
                {
                    "start": 359,
                    "end": 467
                },
                {
                    "start": 468,
                    "end": 605
                },
                {
                    "start": 606,
                    "end": 724
                },
                {
                    "start": 725,
                    "end": 880
                },
                {
                    "start": 881,
                    "end": 934
                },
                {
                    "start": 935,
                    "end": 1129
                },
                {
                    "start": 1130,
                    "end": 1261
                },
                {
                    "start": 1262,
                    "end": 1425
                },
                {
                    "start": 1426,
                    "end": 1591
                },
                {
                    "start": 1592,
                    "end": 1834
                },
                {
                    "start": 1837,
                    "end": 1961
                },
                {
                    "start": 1962,
                    "end": 2067
                },
                {
                    "start": 2068,
                    "end": 2247
                },
                {
                    "start": 2248,
                    "end": 2355
                },
                {
                    "start": 2356,
                    "end": 2545
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.419189453125
        },
        {
            "corpus_id": "245124484",
            "title": "Unraveling Social Perceptions & Behaviors towards Migrants on Twitter",
            "text": "We have considered different hyperparameters, such as multiple batch sizes (BS: 16 and 32) and dropout rates (DR: 0.4 and 0.5), for our CNN and Bi-LSTM models. The hidden layer for all these models was 128. We have considered SoftMax activation in our final classification layer to predict the final class. We have considered 'adam' as our optimizer for the modeling. Table 5 reports the model performances. Performances of CNN + fastText models are better than Bi-LSTM + fastText models, and the highest F1weighted Score for CNN is 0.72 (BS -32, DR -0.4).\n\nTransformer-based Neural Models: Next, we have considered two transformer-based models: BERT (Devlin et al. 2019) and RoBERTa  for the classification task. Transformer-based models work reasonably well for text classification tasks because transformers are pretrained on a diverse and large corpus. Core aspects of these models are their multi-head self-attention to extract the input tokens' semantic aspects for contextual representation with multiple layers. Unlike RNNs, these models can handle long-term dependency problems. BERT has successfully performed numerous NLP-related tasks -including the classification task. BERT is a bidirectional unsupervised pre-trained model. Devlin et al. (2019) have considered BooksCorpus and English Wikipedia (16GB) for the training purpose. BERT was introduced in 2018. However, within a year, BERT's performance was further improved by adding more training corpus and incorporating minor adaptations to the training process . This advanced version of BERT is known as RoBERTa. In addition to the pre-training corpus of BERT, RoBERTa also used an additional corpus from CC-News (76 GB), Open Web Text (38 GB), and Storie's dataset (31 GB) for training.\n\nPrior embedding approaches, such as word2vec (Mikolov et al. 2013) or GloVe (Pennington, Socher, and Manning 2014), have considered a single word embedding representation for each word without considering the context of that specific word. Therefore, these language representations failed to capture the context. In contrast, BERT considers the context of a particular word from both directions -both from",
            "score": 0.4390677449442577,
            "section_title": "Results:",
            "char_start_offset": 34845,
            "sentence_offsets": [],
            "ref_mentions": [
                {
                    "start": 651,
                    "end": 671,
                    "matchedPaperCorpusId": "52967399"
                },
                {
                    "start": 1239,
                    "end": 1259,
                    "matchedPaperCorpusId": "52967399"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.517578125
        },
        {
            "corpus_id": "225639057",
            "title": "Ablations over transformer models for biomedical relationship extraction",
            "text": "Originally, we planned to conduct an analysis comparing a wide range of transformer architectures. However, our preliminary investigations suggested that many were too cumbersome to work with, either in terms of compute required, the quality of the pretrained model or the maturity of the codebase. To this end, we restricted our analysis to the pretrained models BERT Base, BioBERT 1.1, RoBERTa base and RoBERTa large, as described in Table 1. Our principal question compares the evaluation performance of BERT Base, BioBERT, and RoBERTa base, as models of approximately equal parameter counts. However, we additionally decided to include RoBERTa large to explore any potential benefits from using a larger model with a higher quality pretraining regime (based upon General Language Understanding Evaluation benchmark results 15 ). \n\nAll experiments were conducted with the HuggingFace Transformers implementations, version 2.4.1 16 Classification heads Pretrained models are frequently employed in classification tasks, wherein a linear layer is constructed on top of the final layer. Recently, some modifications of this approach have been proposed, to combine specific entity information into the classification layer, to support relationship classification tasks. Wu and He 10 suggested averaging the token pieces representing each entity, and concatenating the output with the sentence vector before applying a fully connected feed forward layer, giving rise to the RBERT classification head and setting a new benchmark in the Semeval 2010 Task 8 dataset. In this work, we compare both the simple linear layer classification head and the RBERT head. \n\nPreprocessing RE is commonly construed as a sentence classification task, wherein the label assigned to the relationship between two entities in a sentence are instead assigned to the sentence. However, such an approach can be problematic; for instance, if there are more than two entities in a sentence, and/or more than two relationships (a common occurrence in biomedical text), leading to a situation where the same sentence can yield two conflicting labels. \n\nTo mitigate this, various strategies have been used, such as substituting the entities of interest with nominal placeholder tokens, such that all strings seen by a classifier are unique and creating the possibility for a classifier to learn the syntactic importance of the placeholder tokens with regard to the relationship that binds them 17 .",
            "score": 0.4388800047439779,
            "section_title": "Pretrained model selection",
            "char_start_offset": 7417,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 98
                },
                {
                    "start": 99,
                    "end": 298
                },
                {
                    "start": 299,
                    "end": 444
                },
                {
                    "start": 445,
                    "end": 595
                },
                {
                    "start": 596,
                    "end": 832
                },
                {
                    "start": 835,
                    "end": 1086
                },
                {
                    "start": 1087,
                    "end": 1268
                },
                {
                    "start": 1269,
                    "end": 1561
                },
                {
                    "start": 1562,
                    "end": 1655
                },
                {
                    "start": 1658,
                    "end": 1851
                },
                {
                    "start": 1852,
                    "end": 2120
                },
                {
                    "start": 2123,
                    "end": 2467
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.3798828125
        },
        {
            "corpus_id": "252438677",
            "title": "Scope of Pre-trained Language Models for Detecting Conflicting Health Information",
            "text": "RQ1: How effective pre-trained language models are in detecting conflicting health information? \n\nModel Analysis We report the performance of 5 different transformer-based classifiers in Table 6 as well as a random guess, lexical feature, and word embedding-based baseline. The lexical model, which uses TFIDF (Manning and Prabhakar 2010) for feature generation with a class-weighted Random Forest classifier (Breiman 2001), under-performs random guess on 90% of experiments across all conflict types. This is expected as TFIDF generates features based on word count and frequency, ignoring the fact that the input text is comprised of two disjoint advice statements which need to be compared. Additionally, we note that the Random Forest model was unable to overcome the severe class im- balance with TFIDF features, often over-predicting the nonconflicting class for all labels. \n\nThe static embedding methods, SBERT and GloVE, provide an improvement over our lexical baseline while outperforming random guess in most experiments. However, static embedding methods are not constructed to perform pair-wise comparison of longer texts. Thus, all textual comparison must happen in the classification head, which is shown to be inferior to the dynamic embedding approaches shown in Table 6. \n\nTransformer-based models such as BERT provide significant improvement over both TFIDF and static embeddings, as their contextual embedding approach permits direct comparison of two texts. The BERT baseline, however, is shown to be inferior to more modern transformer models such as RoBERTa and DeBERTa-v3, both of which outperform BERT on 80% of experiments. From our experiments using Bio+Clinical BERT (B+C BERT), we show that BERT's performance issues are not due to BERT's lack of domain specific training data, as B+C BERT under-performs or matches BERT's performance on 70% of experiments. This again follows intuition as, while this task is called health conflict detection, the addition of knowledge from sources like biomedical corpora and clinical notes are intuitively not relevant to this task. Health advice contains common language alongside occasional references to medications or physiological effects (as shown in Table 1).",
            "score": 0.4387292717274073,
            "section_title": "Results",
            "char_start_offset": 22389,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 95
                },
                {
                    "start": 98,
                    "end": 273
                },
                {
                    "start": 274,
                    "end": 501
                },
                {
                    "start": 502,
                    "end": 693
                },
                {
                    "start": 694,
                    "end": 880
                },
                {
                    "start": 883,
                    "end": 1032
                },
                {
                    "start": 1033,
                    "end": 1135
                },
                {
                    "start": 1136,
                    "end": 1288
                },
                {
                    "start": 1291,
                    "end": 1478
                },
                {
                    "start": 1479,
                    "end": 1649
                },
                {
                    "start": 1650,
                    "end": 1886
                },
                {
                    "start": 1887,
                    "end": 2097
                },
                {
                    "start": 2098,
                    "end": 2231
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.372802734375
        },
        {
            "corpus_id": "212821571",
            "title": "Do Not Have Enough Data? Deep Learning to the Rescue!",
            "text": "We demonstrated that our augmentation approach is independent of the classification algorithm by inspecting three different classifiers, representing three text classification \"generations\".\n\nSVM Support Vector Machine classifiers were already commonly used before the deep neural network era. We employ a commercial SVM classifier (IBm Watson Natural Language Classifier) dedicated to natural language processing, which handles both the feature extraction process and the training of the classifier. While recent models are based on neural networks, in the context of our problem, SVM may have an advantage, since unlike neural-network-based models, it performs well even for relatively small datasets.\n\nLSTM Long Short Term Memory represents the type of classifiers that emerged after the advances in training recurrent neural networks, and the introduction of word embeddings (Mikolov et al. 2013), LSTMs are commonly used for sequential and textual data classification. We implemented a sequence-to-vector model based on an LSTM component followed by two fully connected layers and a softmax layer. For word embedding, we employed GLoVe (Pennington, Socher, and Manning 2014) of 100 dimensions. An LSTM classifier usually requires a large amount of data for training.\n\nBERT Bidirectional Encoder Representations from Transformers is a relatively new family of classifiers. Based on the transformer architecture, BERT is pre-trained using two unsupervised tasks: masked language model and next-sentence prediction, on the \"BooksCorpus\" (800 million words) (Zhu et al. 2015) and has proven state-of-the-art performance on several text classification tasks. Therefore, BERT, like GPT-2, leverages large amounts of data that were used as part of 1 www.kaggle.com/siddhadev/atis-dataset-from-ms-cntk 2 https://cogcomp.seas.upenn.edu/Data/QA/QC/ its pre-training phase, in order to perform well, even on relatively small datasets.",
            "score": 0.4385087423731993,
            "section_title": "Classifiers",
            "char_start_offset": 20384,
            "sentence_offsets": [],
            "ref_mentions": [
                {
                    "start": 1141,
                    "end": 1179,
                    "matchedPaperCorpusId": "1957433"
                },
                {
                    "start": 1559,
                    "end": 1576,
                    "matchedPaperCorpusId": "6866988"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.1497802734375
        },
        {
            "corpus_id": "268819482",
            "title": "A Comprehensive Study on NLP Data Augmentation for Hate Speech Detection: Legacy Methods, BERT, and LLMs",
            "text": "The experiment was conducted in three stages.Initially, we evaluated a dataset without any augmentation.Subsequently, we applied four distinct augmentation techniques: WordNet, FastText, Back-translation, and BERT masking.In the final stage, we applied BERT-base cosine similarity filtering to each augmented dataset.Additionally, we assessed the augmentation performance of GPT-3.\n\nTable 14 shows results of not using the data augmentation.In this analysis, we chose the optimal classifier architecture.The comparison reveals that, in the absence of data augmentation, BERT marginally surpasses CNN, showing a slight improvement of 0.9% in accuracy and 1.0% in F1 score.Consequently, for experiments involving data augmentation, our focus has been solely on BERT, the state-of-the-art (SOTA) model demonstrating the best performance.\n\nTable 15 exhibits the experiment results of non-augmented and augmented datasets with and without BERT-cosine for four different datasets: AskFm, HASOC, TREC, and SST1.The examination of BERT-based classifier performance across these four distinct datasets-AskFm, HASOC, TREC, and SST1-reveals insights into the effects of data augmentation and the role of BERT-cosine similarity filtration.Initially, the impact of data augmentation methods such as WordNet, FastText, and BERT, when applied without BERT-cosine similarity, shows minimal to no improvements in accuracy and F1 scores.For example, BERT-mask data augmentation for AskFm has resulted in a drop of 0.3% in accuracy and 0.2% in F1.This suggests that the mere addition of augmented sentences does not guarantee enhanced model performance, emphasizing the necessity for quality over quantity in augmentation strategies.\n\nAmong the various data augmentation methods evaluated, back-translation demonstrated the best performance, showing a 0.5% improvement despite generating a smaller number of augmented sentences compared to WordNet, FastText, and BERT-masking methods.One possible explanation for this is that, although back-translation produces fewer sentences, it results in less label alteration compared to other methods, as observed in the previous table 12.",
            "score": 0.43835803083347114,
            "section_title": "Classification Results",
            "char_start_offset": 46644,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 45
                },
                {
                    "start": 45,
                    "end": 104
                },
                {
                    "start": 104,
                    "end": 222
                },
                {
                    "start": 222,
                    "end": 317
                },
                {
                    "start": 317,
                    "end": 381
                },
                {
                    "start": 383,
                    "end": 441
                },
                {
                    "start": 441,
                    "end": 504
                },
                {
                    "start": 504,
                    "end": 671
                },
                {
                    "start": 671,
                    "end": 834
                },
                {
                    "start": 836,
                    "end": 1004
                },
                {
                    "start": 1004,
                    "end": 1227
                },
                {
                    "start": 1227,
                    "end": 1419
                },
                {
                    "start": 1419,
                    "end": 1528
                },
                {
                    "start": 1528,
                    "end": 1714
                },
                {
                    "start": 1716,
                    "end": 1965
                },
                {
                    "start": 1965,
                    "end": 2160
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.75341796875
        },
        {
            "corpus_id": "267724195",
            "title": "Categorization of tweets for damages: infrastructure and human damage assessment using fine-tuned BERT model",
            "text": "The BERT model was introduced by Devlin et al. (2018) at Google Lab and it has proven its significance for a variety of text-mining tasks in several application domains (Malik, Imran & Mamdouh, 2023).The benefits of BERT include faster development, automated feature generation, reduced data requirements, and improved performance.It has two architectures and we are interested in fine-tuning the pre-trained BERT model for damage assessment tweet identification task for binary as well as multi-class classification.The BERT model is pre-trained on a large corpus of English data in a self-supervised fashion and uses the context of language in both directions.Furthermore, BERT was pre-trained on next-sentence prediction and masked language modeling objectives.\n\nTo fine-tune the BERT base uncased model (https://huggingface.co/bert-base-uncased), some important steps are required.After applying the above-mentioned pre-processing steps, data transformation and training classifier steps are executed.We have chosen 64 and 128 sequence lengths because a maximum of 280 characters are allowed in a tweet and 128 sequence length is enough to handle the most lengthy tweets.Therefore, all tweets are padded up to the length of 64 and 128.After that, attention masks are added to locate real and padded tokens.The vector output of attention masks is then fed to the BERT model and fine-tuning step is performed.\n\nBERT classifier training: There are seven disasters in the CrisisMMD dataset.For training and validation of BERT classifiers, we split each disaster into 80-20 ratios using the stratified sampling approach.After that, we took 80% data from each disaster and combined them to make the training dataset.The remaining 20% of data from each disaster is used for testing the BERT classifier on that specific disaster data.Furthermore, the combined 80% data is further divided into a 90-10 split, in which 90% is used for training and 10% is used for validation.We utilize the BERT base model which contains 12 transfer layers, 12 attention heads, and 768 hidden layers.All entities (class labels, token ids, and attention masks) are combined into one set.",
            "score": 0.4382598094391262,
            "section_title": "Fine-tunning BERT",
            "char_start_offset": 14551,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 200
                },
                {
                    "start": 200,
                    "end": 331
                },
                {
                    "start": 331,
                    "end": 517
                },
                {
                    "start": 517,
                    "end": 662
                },
                {
                    "start": 662,
                    "end": 764
                },
                {
                    "start": 766,
                    "end": 885
                },
                {
                    "start": 885,
                    "end": 1005
                },
                {
                    "start": 1005,
                    "end": 1175
                },
                {
                    "start": 1175,
                    "end": 1239
                },
                {
                    "start": 1239,
                    "end": 1310
                },
                {
                    "start": 1310,
                    "end": 1411
                },
                {
                    "start": 1413,
                    "end": 1490
                },
                {
                    "start": 1490,
                    "end": 1619
                },
                {
                    "start": 1619,
                    "end": 1714
                },
                {
                    "start": 1714,
                    "end": 1830
                },
                {
                    "start": 1830,
                    "end": 1969
                },
                {
                    "start": 1969,
                    "end": 2077
                },
                {
                    "start": 2077,
                    "end": 2163
                }
            ],
            "ref_mentions": [
                {
                    "start": 33,
                    "end": 53,
                    "matchedPaperCorpusId": "52967399"
                },
                {
                    "start": 169,
                    "end": 199,
                    "matchedPaperCorpusId": "257060193"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.103759765625
        },
        {
            "corpus_id": "237532156",
            "title": "Comparing Feature-Engineering and Feature-Learning Approaches for Multilingual Translationese Classification",
            "text": "Paragraph-level translationese classification results with mean and standard deviations over 5 runs are reported in Table 2. Overall, the BERT model outperforms other architectures in all settings, followed closely by the other end-to-end neural architectures. Using the pre-trained Wiki embeddings helps improving the accuracy of the fastText method in all cases. Among the approaches with the SVM classifier, Wiki+SVM performs best in the single-source settings, but shows lower accuracy than Handcr.+SVM in the multi-source (TRG-ALL) settings. Wiki+Gauss.+SVM performs worst apart from on ES-EN and DE-ALL.\n\nIn the monolingual single-source settings, we observe that accuracy is slightly lower when the source language is typologically closer to the text language, i.e. it becomes more difficult to detect translationese. Specifically, DE-EN tends to have lower accuracy than DE-ES; EN-DE lower accuracy than EN-ES; and ES-EN lower accuracy than ES-DE. Accuracy generally drops when going from single-source to the multi-source setting, e.g. from DE-EN and DE-ES to DE-ALL. The EN-ALL dataset is the most difficult for most of the models among the TRG-ALL datasets. The ALL-ALL[3] setting exhibits comparable accuracy to the TRG-ALL setting for the neural models, but for the SVM there is a drop of around 9 points. Throughout our discussion we always report absolute differences between systems. The ALL-ALL[8] data results in reduced accuracy for most architectures, except Handcr.+SVM.\n\nNeural-classifier-based models substantially outperform the other architectures: the SVMs trained with hand-crafted linguistically-inspired features,  e.g., trail BERT by \u223c20 accuracy points.\n\nTo make sure our hand-crafted-feature-based SVM results are competitive, we compare them with  on our data.  show that training a SVM classifier on the top 1000 most frequent POS-or character-trigrams yields SOTA translationese classification results on Europarl data. On our data, POS-trigrams yield around 5 points increase in accuracy for most",
            "score": 0.4379705167810669,
            "section_title": "Results",
            "char_start_offset": 13920,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.36767578125
        },
        {
            "corpus_id": "227231110",
            "title": "SSN-NLP at SemEval-2020 Task 4: Text Classification and Generation on Common Sense Context Using Neural Networks",
            "text": "The models implemented were BERT, Micro TC and XLNet. The final outputs of the three models were composed into a singular result by majority voting methodology. \n\nMicro TC Micro TC is a multi-propose text-classifier that tackles task independently of domain and language. For any given text classification task, micro TC will try to find a suitable text model from a set of possible models defined in the configuration space, provided a corpus of defined text data (i.e.) micro TC will transform the text into vector that transforms the training set of pairs, text and label, into a training set of pairs, vectors and label, which is used by suitable supervised learning algorithm to obtain a text classifier. The model was initially fed with train data (corpus) to find the suitable hparams or suitable text model. Along with the derived hparams and the initial train set, the model was trained with use of Linear SVC. \n\nBERT BERT makes use of Transformers, an attention mechanism that learns contextual relations between words in a text. In its general form, Transformer includes two separate mechanisms -an encoder that reads the text input and a decoder that produces a prediction for the task. Since BERT's goal is to generate a language model, only the encoder mechanism is necessary. BERT's prediction goal is mainly achieved by two training strategies, Masked LM and Next Sentence Prediction. When training the BERT model, Masked LM and Next Sentence Prediction are trained together, with the goal of minimizing the combined loss function of the two strategies. For the task of classification using BERT, the Next Sentence Prediction (NSP) strategy training of BERT is optimised to add a classification layer on top. \n\nDuring training the available pre-trained model BERT-Base Un-Cased was used. The BERT-Base Uncased comprises of 12 Transformer blocks, 12 self-attention heads and 768 hidden dimension with a total parameters of 110M. The BERT-Large model was not used although it fared better than BERT-Base due to its computational intensive need for training the model.",
            "score": 0.43755815622205174,
            "section_title": "Model details",
            "char_start_offset": 3795,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 53
                },
                {
                    "start": 54,
                    "end": 160
                },
                {
                    "start": 163,
                    "end": 271
                },
                {
                    "start": 272,
                    "end": 709
                },
                {
                    "start": 710,
                    "end": 815
                },
                {
                    "start": 816,
                    "end": 919
                },
                {
                    "start": 922,
                    "end": 1039
                },
                {
                    "start": 1040,
                    "end": 1198
                },
                {
                    "start": 1199,
                    "end": 1290
                },
                {
                    "start": 1291,
                    "end": 1400
                },
                {
                    "start": 1401,
                    "end": 1569
                },
                {
                    "start": 1570,
                    "end": 1724
                },
                {
                    "start": 1727,
                    "end": 1803
                },
                {
                    "start": 1804,
                    "end": 1943
                },
                {
                    "start": 1944,
                    "end": 2081
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.193603515625
        },
        {
            "corpus_id": "277940080",
            "title": "Word Embedding Techniques for Classification of Star Ratings",
            "text": "This paper examined various word embedding techniques for classifying customer star ratings based on telecom reviews, comparing their effectiveness across different classification models. The results showed that while traditional TF-IDF performed poorly in most cases, it excelled when used with the Support Vector Classifier. BERT-PCA and Doc2Vec-PCA emerged as the most effective WEs, particularly for complex multiclass classification tasks. Word2Vec-PCA and FastText-PCA performed consistently well with Logistic Regression and Stochastic Gradient Descent, highlighting their robustness in certain models. Decision Trees, KNN and Gradient Boosting classifiers demonstrated relative insensitivity to WEs, except in larger multiclass datasets where BERT-PCA led to superior results. Regarding feature engineering, for Word2Vec and FastText, our proposed PCA approach of combining word vectors using the first principal component shows clear advantages in performance over the traditional approach of taking the average. Additionally, energy consumption analysis revealed that computational efficiency varies significantly among embedding techniques, with TF-IDF being the most resource-efficient and FastText the most computationally de-manding. The findings emphasize the importance of selecting the appropriate WE based on classification complexity, model compatibility and computational constraints.",
            "score": 0.437529853708956,
            "section_title": "Conclusion",
            "char_start_offset": 59367,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 187
                },
                {
                    "start": 188,
                    "end": 326
                },
                {
                    "start": 327,
                    "end": 444
                },
                {
                    "start": 445,
                    "end": 609
                },
                {
                    "start": 610,
                    "end": 784
                },
                {
                    "start": 785,
                    "end": 1021
                },
                {
                    "start": 1022,
                    "end": 1247
                },
                {
                    "start": 1248,
                    "end": 1404
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.67138671875
        },
        {
            "corpus_id": "221555780",
            "title": "Comparative Study of Language Models on Cross-Domain Data with Model Agnostic Explainability",
            "text": "\u2022 BERT: Pre-trained on Wikipedia (2500M words) and Books Corpus (800M words) for two tasks i.e. masked language modeling (MLM) and next sentence prediction (NSP) tasks. BERT-base model with 110M parameters was finetuned for our experiments [3]. \n\n\u2022 ELECTRA: The pre-training data and number of parameters of ELECTRA-base is same as BERT-base. \n\nReplace token detection task is employed instead of MLM which makes it more compute efficient and more accurate in terms of performance [7]. \n\n\u2022 DistilBERT: It leverages knowledge distillation in pre-training phase to reduce the size of BERT-base by 40%, while retaining 97% of its language understanding capabilities and being 60% faster with 66M number of parameters [5]. \n\n\u2022 ALBERT: A lite model based on BERT architecture and same pre-training corpus. ALBERT-base uses 12M parameters only as it achieves significant parameter reduction due to factorized embedding parameterization and cross layer parameter sharing. \n\nALBERT introduced a self-supervised loss for sentence-order prediction (SOP) to address the ineffectiveness of NSP task in BERT [6]. \n\n\u2022 RoBERTa: Pre-trained on CC News corpus apart from the BERT data with 125M parameters. It compromised on the NSP objective and pre-trained with larger mini batches and learning rate to improve upon MLM [4]. \n\nAn optimized SVM classifier was also included in the experiments to set the baseline for comparison. The experiments were conducted using Google's Tensorflow implementation of BERT and ELECTRA and HuggingFace PyTorch Transformers library for DistilBERT, ALBERT and RoBERTa [25] [26] [27].",
            "score": 0.43713170616363684,
            "section_title": "A. Language Models",
            "char_start_offset": 5030,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 168
                },
                {
                    "start": 169,
                    "end": 244
                },
                {
                    "start": 247,
                    "end": 342
                },
                {
                    "start": 345,
                    "end": 485
                },
                {
                    "start": 488,
                    "end": 718
                },
                {
                    "start": 721,
                    "end": 800
                },
                {
                    "start": 801,
                    "end": 964
                },
                {
                    "start": 967,
                    "end": 1099
                },
                {
                    "start": 1102,
                    "end": 1189
                },
                {
                    "start": 1190,
                    "end": 1309
                },
                {
                    "start": 1312,
                    "end": 1412
                },
                {
                    "start": 1413,
                    "end": 1600
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.2445068359375
        },
        {
            "corpus_id": "252673143",
            "title": "Parameter Optimization of Educational Network Ecosystem Based on BERT Deep Learning Model",
            "text": "is study uses BERT dynamic word vectors from the IMDB database. Furthermore, the BERT was combined with the proposed MC-BiGRU-capsule model and compared with the precision-adjusted SentiBERT model, a pre-prepared sensitivity dictionary from BERT. \n\nSince the BERT language is large and repeatable, many scholars use preplanned BERT models to refine the following activities. However, due to the limited space of the entries, many designs will cause problems such as quality and time. As shown in Table 3, the MC-BiGRU-capsule model in this study is not only designed for GloVe static word vectors but also has better class performance than the BERT model and ULMFIT (an LSTM-based preliminary scheme). e accuracy of the combined distribution of word dynamic vectors is improved by 1.2%, which is a 0.8% improvement over the SentiBERT model. Based on these models, Bert introduces a good vector design that enhances their performance and enhances the advantages of the mc-BiGRU model-capsule model. Mathematical Problems in Engineering",
            "score": 0.43675867549060005,
            "section_title": "Results and Analysis",
            "char_start_offset": 26234,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 63
                },
                {
                    "start": 64,
                    "end": 246
                },
                {
                    "start": 249,
                    "end": 374
                },
                {
                    "start": 375,
                    "end": 483
                },
                {
                    "start": 484,
                    "end": 701
                },
                {
                    "start": 702,
                    "end": 840
                },
                {
                    "start": 841,
                    "end": 997
                },
                {
                    "start": 998,
                    "end": 1034
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.0728759765625
        },
        {
            "corpus_id": "252792369",
            "title": "A text classification method based on LSTM and graph attention network",
            "text": "Table 2 shows the comparison of the accuracy of our model and the other ten baseline text classification models on the test set. We take the mean \u00b1 standard deviation of the results of running the model 10 times as the test results. Table 3 records the F1-score of each classification model. \n\nIt can be observed that our model outperforms other baseline models and the graphbased approach generally outperforms general neural network approaches such as CNN and LSTM. This shows that by converting the text to a graph-structured data representation, the model can be more flexible in accessing the hidden information in the text since each node can have a different number of neighbours. LSTM performs better than CNN. CNN pays more attention to the local information in the text, and LSTM can capture the context semantic information of the text. FastText uses N-gram and bag of words model as text features and performs well in experiments on various data sets. Although Bert performs well on all five datasets, it requires a lot of memory and longer training time. On MR, Bert outperforms GNN-based models thanks to large-scale pre-training. Furthermore, the text in MR is shorter, which limits the message passing ability of the graph structure. The reason why our model performs better than TextGCN and Text-level-GNN on MR dataset is that sentiment analysis requires a higher word order, and the different order of words will directly affect the category of text. TextGCN, Text-level-GNN and other models based on GNN don't utilise the word order information of the text, while the LSTM module in our model can capture the word order information of the text. TextGCN builds a graph for the whole corpus, and the short text of MR dataset limits the ability of message-passing between document nodes and word nodes. The result of Text-level-GNN on Ohsumed is poor because Ohsumed is a long text dataset, and the number of training set is less than the test set. This indicates that the Text-level-GNN model ignores the important global and long-distant semantic information in a long text, and has poor inductive learning ability of text.",
            "score": 0.4367150903904495,
            "section_title": "Experimental results",
            "char_start_offset": 20656,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 128
                },
                {
                    "start": 129,
                    "end": 232
                },
                {
                    "start": 233,
                    "end": 291
                },
                {
                    "start": 294,
                    "end": 467
                },
                {
                    "start": 468,
                    "end": 687
                },
                {
                    "start": 688,
                    "end": 718
                },
                {
                    "start": 719,
                    "end": 847
                },
                {
                    "start": 848,
                    "end": 963
                },
                {
                    "start": 964,
                    "end": 1067
                },
                {
                    "start": 1068,
                    "end": 1144
                },
                {
                    "start": 1145,
                    "end": 1249
                },
                {
                    "start": 1250,
                    "end": 1469
                },
                {
                    "start": 1470,
                    "end": 1664
                },
                {
                    "start": 1665,
                    "end": 1819
                },
                {
                    "start": 1820,
                    "end": 1965
                },
                {
                    "start": 1966,
                    "end": 2142
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.390625
        },
        {
            "corpus_id": "235658083",
            "title": "Neural Models for Offensive Language Detection",
            "text": "author emphasized the superiority of the FastText to obtain accurate embeddings for rare words which was a major factor of the performance improvements.\n\nThe work at [ITE20] focused on solving the problem of imbalanced classes using backtranslation based augmentation introduced in [SHB15]. The technique involves generating new data instances for the class that is underrepresented in the dataset by translating sentence to other languages and then translating back to English which held a different sentence with the overall same meaning and then assigning the same class. That method significantly improve the model performance. They have used Google translate API to perform the processing. The authors applied data augmentation for task B and task C dataset since they suffer more from imbalanced classes problem.  The authors at [Sot+20] used BERT based model in OffensEval 2020 for offensive language detection. Since BERT is pretrained on formal text (wikipedia and books) they started with fine tunning the model to our domain. This is accomplished by taking the original model and continuing to train the masked language model and next sentence prediction objectives using a large amount of unlabeled tweets. For predictions they have using an ensemble using the multi-view SVM with the addition of features from the contextualized language model classifier. Specifically, linear SVM classifier using various n-gram ranges are first trained for each task in addition to the BERT-based classifier. Then the output of the view-classifiers are concatenated as a final linear SVM classifier.\n\nAlso the work at [WYB20] noted that fine-tuning BERT yield state of the art results for text classification tasks. Fine tuning BERT is typically done on task specific datasets in a supervised manner, it can also be done in an unsupervised manner by the masked language modeling MLM. In-domain unsupervised MLM pretraining allows for domain adaptation of the model. The authors concluded that since there is no way that the 2020 weakly labeled data could carry useful information to a machine learning system, they decided to use the manually labeled 2019 dataset only. However 2020 dataset was an ample source to build models using unsupervised learning particularly for domain adaptation using MLM training. The problem is that training a transformer",
            "score": 0.4366299043710196,
            "section_title": "Methodology reviews on OffensEval shared task",
            "char_start_offset": 33318,
            "sentence_offsets": [],
            "ref_mentions": [
                {
                    "start": 166,
                    "end": 173,
                    "matchedPaperCorpusId": "227231712"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.52294921875
        },
        {
            "corpus_id": "227230342",
            "title": "Multilingual Epidemiological Text Classification: A Comparative Study",
            "text": "Polish  Table 4: F1-micro scores of the analyzed models for the relevant documents per language. The pretrained BERT models are base-multilingual. LR stands for Logistic Regression, RF for Random Forest, and SVM for Support Vector Machines, \u2020 fine-tuned.\n\nRegarding the difference between the fine-tuned BERT-based models and those that use the BERT encoder for generating features only, the performance is slightly better when BERT is fine-tuned on the task. However, in the case of additional layers on top of the BERT encoder, when fine-tuned, a considerable decrease in performance can be seen. Overall, these results suggest that the deep learning approaches are capable of much deeper and complex representations, such that they can utilize previously learned features for newer documents, even when the language of the document differs.\n\nAs observed in Table 4, all the machine learning models (LR, RF, SVM) display similar trends in their unequal performance based on language by not detecting (having the F1 values of zero) the relevant documents in Polish and Chinese. This is likely due to the size of the training data for these particular languages. Similarly, for all the low-resource languages (Polish, Chinese, Russian, and Greek), unsurprisingly, the CNN and BiLSTM -based models with pre-trained FastText embeddings were not able to distinguish relevant documents from irrelevant ones, as indicated by their low F1 scores. This might be due to the low embedding coverage of the languages. The F1 values for Chinese tend to be consistent for all BERT-based models while the performance for Polish varies a lot between models. VGCN+BERT had the highest F1 scores for the low-resourced languages Polish, Chinese, and Russian and the secondhighest for Greek.\n\nIn order to analyze the influence of the documents with a larger quantity of documents (French and English, around 2, 000 news articles) over the classification of low-resource languages, we consider every language as the source language and the other five languages as target languages. At every iteration, the best performing model from the previous experiments is trained on the data in the source language and applied directly to every target language.  Table 5: Evaluation scores of the BERT (multilingual-uncased) \u2020 fine-tuned model for the",
            "score": 0.4364931837907298,
            "section_title": "Models",
            "char_start_offset": 18570,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.544921875
        },
        {
            "corpus_id": "202539232",
            "title": "Abductive Reasoning as Self-Supervision for Common Sense Question Answering",
            "text": "We first evaluate our approach on the SWAG dataset and compare against fully supervised and weakly supervised. The results are presented in Table 1. We show the performance of fastText to highlight the importance of commonsense knowledge and the abductive reasoning process. Fast-Text models a given text as a bag of n-grams and predicts the probability of each ending being correct or incorrect. This approach is heavily reliant on word embeddings and does not generalize well to this task. It is also interesting to note that our approach outperforms all weakly supervised baselines such as the dual bag of words approach and the SNLI-based approaches. These approaches are the closest related to our approach since they are not trained directly on the SWAG training split. BERT currently has the best per- We also evaluate our approach on the tougher Hel-laSWAG dataset. The adversarial filtering technique on this dataset has been improved to increase the perplexity of BERT-like models on this task. The effect of this approach can be seen in Table 2. Again, we compare against the same baselines as in SWAG and find that our approach offers competitive performance to the fully supervised baselines. We find that using our abductive reasoning approach on BERT allows the model to retain up to 59% of its performance as compared to a model trained directly on the annotations. It should be noted that the use of ConceptNet-based contextualization on HellaSWAG has one significant drawback: the answers are too similar in their use of semantics, and hence the potential for indifference goes up. We observe that the number of examples where the second-best hypothesis had the same energy as the top hypothesis was 35% when the correct answer was in the top 2. This indifference could, arguably, be diminished by grounding the concepts in vision or other modalities. This is not an unreasonable assumption since the pre-training data used in BERT contains articles from Wikipedia which have vision-based explanations which constrain the semantics to those observed in vision. \n\nWe evaluate the performance of current QA systems un-der a Low Resource environment. We limit the amount of training data available to these models for a fair comparison with our approach.",
            "score": 0.4363936596288059,
            "section_title": "Quantitative Evaluation",
            "char_start_offset": 24877,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 110
                },
                {
                    "start": 111,
                    "end": 148
                },
                {
                    "start": 149,
                    "end": 274
                },
                {
                    "start": 275,
                    "end": 396
                },
                {
                    "start": 397,
                    "end": 491
                },
                {
                    "start": 492,
                    "end": 654
                },
                {
                    "start": 655,
                    "end": 775
                },
                {
                    "start": 776,
                    "end": 873
                },
                {
                    "start": 874,
                    "end": 1004
                },
                {
                    "start": 1005,
                    "end": 1205
                },
                {
                    "start": 1206,
                    "end": 1381
                },
                {
                    "start": 1382,
                    "end": 1599
                },
                {
                    "start": 1600,
                    "end": 1763
                },
                {
                    "start": 1764,
                    "end": 1869
                },
                {
                    "start": 1870,
                    "end": 2078
                },
                {
                    "start": 2081,
                    "end": 2165
                },
                {
                    "start": 2166,
                    "end": 2269
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.2685546875
        },
        {
            "corpus_id": "244895506",
            "title": "Transformer based Contextual Model for Sentiment Analysis of Customer Reviews: A Fine-tuned BERT",
            "text": "\u2014The Bidirectional Encoder Representations from Transformers (BERT) is a state-of-the-art language model used for multiple natural language processing tasks and sequential modeling applications. The accuracy of predictions from context-based sentiment and analysis of customer review data from various social media platforms are challenging and time-consuming tasks due to the high volumes of unstructured data. In recent years, more research has been conducted based on the recurrent neural network algorithm, Long Short-Term Memory (LSTM), Bidirectional LSTM (BiLSTM) as well as hybrid, neutral, and traditional text classification algorithms. This paper presents our experimental research work to overcome these known challenges of the sentiment analysis models, such as its performance, accuracy, and context-based predictions. We\u2019ve proposed a fine-tuned BERT model to predict customer sentiments through the utilization of customer reviews from Twitter, IMDB Movie Reviews, Yelp, Amazon. In addition, we compared the results of the proposed model with our custom Linear Support Vector Machine (LSVM), fastText, BiLSTM and hybrid fastText-BiLSTM models, as well as presented a comparative analysis dashboard report. This experiment result shows that the proposed model performs better than other models with respect to various performance measures.",
            "score": 0.4361959128747367,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.474365234375
        },
        {
            "corpus_id": "208291368",
            "title": "Word-class embeddings for multiclass text classification",
            "text": "The paper that introduces the BERT language model (Devlin et al. 2019) describes its use in supervised tasks as a two-step process. The first step consists of the pretraining phase, in which the actual language model is fit on very large amounts of text via a costly training process. The second step consists of the fine-tuning phase, in which the pretrained BERT model is extended via a few task-specific parameters. The entire network is trained, via a much faster process than the one used in pre-training, on a task-specific training set, thereby fine-tuning all the BERT parameters, plus the task-specific ones, on the task of interest. In the case of classification the additional parameters consist of a single dense layer that takes as input the embedding returned by BERT for \"[CLS]\", a token that is always appended at the beginning of any input text with the specific purpose of serving as a representation of the content of the entire text. This layer returns a probability distribution over the labels. The fine-tuning process is computationally cheaper than the pre-training process, because most of the model has already been fit, and the computation required to adapt it to the supervised task is usually limited. This fact allows to add the rich amount of general knowledge about language captured during the pre-training process (ideally to be run once for all), to many downstream supervised tasks. Note that the computational cost of fine-tuning the model is still orders of magnitude higher that any other learning approach used in our study, since the number of parameters involved in the fitting process is very large (110 millions for the BERT base model we used).\n\nThe BERT fine-tuning approach has obtained state-of-the-art results on many supervised tasks; for this reason we include it in our experiments. We use simpletransformers 28 and the BERT-base-uncased model, 29 which has been pre-trained by Devlin et al. (2019) on a collection of English text (consisting of Wikipedia and a large collection of books) for a total of 3300M word occurrences, requiring four days to be fit on 16 TPUs.\n\nWe also use BERT in a different set of experiments (see Sect. 4.9), in which we experiment with the contextualized word embeddings that BERT generates for each",
            "score": 0.43605927211552387,
            "section_title": "BERT fine-tuning",
            "char_start_offset": 48704,
            "sentence_offsets": [],
            "ref_mentions": [
                {
                    "start": 50,
                    "end": 69,
                    "matchedPaperCorpusId": "52967399"
                },
                {
                    "start": 1930,
                    "end": 1950,
                    "matchedPaperCorpusId": "52967399"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.369873046875
        },
        {
            "corpus_id": "254563901",
            "title": "A Study of Slang Representation Methods",
            "text": "We select commonly used classifiers such as Random Forest and Linear SVC to classify the FastText embeddings of the tweets in each of the evaluation tasks. Random Forest is an ensemble learning-based machine learning model that constructs multiple decision trees on the training data. The classification prediction of Random Forest is the class predicted by most decision trees. Linear SVC is a support vector machine-based classification model. This model tries to find the best hyperplane that maximizes the distance between the samples from different classes. \n\nThe first layers of the BERT model learn generic linguistic patterns and the last few layers learn task-specific patterns. For downstream tasks such as classification, the first layers are frozen and the last layers are trained. \n\n2 http://onlineslangdictionary.com/ 3 https://greensdictofslang.com/",
            "score": 0.4359929737150112,
            "section_title": "Classifiers",
            "char_start_offset": 8556,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 155
                },
                {
                    "start": 156,
                    "end": 284
                },
                {
                    "start": 285,
                    "end": 378
                },
                {
                    "start": 379,
                    "end": 445
                },
                {
                    "start": 446,
                    "end": 562
                },
                {
                    "start": 565,
                    "end": 687
                },
                {
                    "start": 688,
                    "end": 793
                },
                {
                    "start": 796,
                    "end": 864
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.21142578125
        },
        {
            "corpus_id": "153312532",
            "title": "How to Fine-Tune BERT for Text Classification?",
            "text": "One of the benefits of the pre-trained model is being able to train a model for downstream tasks within small training data. We evaluate BERT-FiT and BERT-ITPT-FiT on different numbers of training examples. We select a subset of IMDb training data and feed them into BERT-FiT and BERT-ITPT-FiT. We show the result in Figure 4. \n\nThis experiment result demonstrates that further pre-training brings a significant improvement for few-shot text classification. On IMDb dataset, when there are only 100 labeled data for each class, the accuracy of BERT-ITPT-FiT can reach 92%. In this subsection, we investigate whether the BERT LARGE model has similar findings to BERT BASE . We further pre-train Google's pre-trained BERT LARGE model5 on 1 Tesla-V100-PCIE 32G GPU with a batch size of 24, the max sequence length of 128 and 120K training steps. For target task classifier BERT fine-tuning, we set the batch size to 24 and fine-tune BERT LARGE on 4 Tesla-V100-PCIE 32G GPUs with the max sequence length of 512. \n\nAs shown in Table 6, ULMFiT performs better on almost all of the tasks compared to BERT BASE but not BERT LARGE . This changes however with the task-specific further pre-training where even BERT BASE outperforms ULMFiT on all tasks. BERT LARGE fine-tuning with task-specific further pre-training achieves state-of-the-art results.",
            "score": 0.4357000518438944,
            "section_title": "Exp-IV: Few-Shot Text Classification",
            "char_start_offset": 13477,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 124
                },
                {
                    "start": 125,
                    "end": 206
                },
                {
                    "start": 207,
                    "end": 294
                },
                {
                    "start": 295,
                    "end": 326
                },
                {
                    "start": 329,
                    "end": 457
                },
                {
                    "start": 458,
                    "end": 572
                },
                {
                    "start": 573,
                    "end": 672
                },
                {
                    "start": 673,
                    "end": 842
                },
                {
                    "start": 843,
                    "end": 1007
                },
                {
                    "start": 1010,
                    "end": 1123
                },
                {
                    "start": 1124,
                    "end": 1242
                },
                {
                    "start": 1243,
                    "end": 1340
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.329833984375
        },
        {
            "corpus_id": "278237416",
            "title": "Aleph-Alpha-GermanWeb: Improving German-language LLM pre-training with model-based data curation and synthetic data generation",
            "text": "Inspired by successful demonstrations for English with FineWeb [38] and Nemotron-CC [46], we created educational quality classifiers to curate high-quality German web data by training a classifier on scores given by an LLM-as-a-judge. The intuition behind this is to identify web documents that are more likely to be educational, informative, and useful for model training, as opposed to spam, low-quality, or otherwise unhelpful content. \n\nWe first use an LLM-as-a-judge to label a small subset of the data. We then leverage these labelled documents as training data to train more efficient and lightweight fastText and BERT classifiers. This two-stage approach enables us to score a large number of web documents in a computationallyefficient way, while at the same time introducing implicit regularisation to the judging process. \n\nWe again made use of Mistral-Nemo-Instruct-2407, this time to annotate a random set of 600,000 documents from German FineWeb2 according to three criteria: (1) content quality, assessing coherence, informativeness and overall quality of the content; (2) language quality, evaluating the use of language, including formality, objectivity, and the presence of errors or slang; and (3) orthography, assessing the correctness of grammar, spelling, and punctuation, including errors such as typos, incorrect verb conjugation, and incorrect declension. We show the prompt used for the LLM-as-a-judge in Appendix A, which provided scores from one to five for each criterion. \n\nFor each document, we calculated a combined educational quality score by taking the minimum over the three criteria rated by the LLM-as-a-judge. This is because we observed that the judge would often miss poor quality in one criterium, but rarely in all three. We then used these educational quality scores as the training signal for fastText and BERT quality classification models. We trained one BERT model tasked to predict the educational quality scores given the first 512 tokens of the document's text. We also trained a binary classifier based on fastText.",
            "score": 0.43548021647068963,
            "section_title": "Educational quality",
            "char_start_offset": 10707,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 234
                },
                {
                    "start": 235,
                    "end": 438
                },
                {
                    "start": 441,
                    "end": 508
                },
                {
                    "start": 509,
                    "end": 638
                },
                {
                    "start": 639,
                    "end": 832
                },
                {
                    "start": 835,
                    "end": 1380
                },
                {
                    "start": 1381,
                    "end": 1501
                },
                {
                    "start": 1504,
                    "end": 1648
                },
                {
                    "start": 1649,
                    "end": 1764
                },
                {
                    "start": 1765,
                    "end": 1886
                },
                {
                    "start": 1887,
                    "end": 2012
                },
                {
                    "start": 2013,
                    "end": 2067
                }
            ],
            "ref_mentions": [
                {
                    "start": 63,
                    "end": 67,
                    "matchedPaperCorpusId": "270711474"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.283203125
        },
        {
            "corpus_id": "277043827",
            "title": "Data Caricatures: On the Representation of African American Language in Pretraining Corpora",
            "text": "Descriptions of each of the 16 data filters evaluated are included in Table 9. Language filters are typically neural models used to filter datasets to a particular language (e.g., English in the case of C4 and others). Classifier-based filters train small classification models (typically using fastText (Joulin et al., 2017)) or n-gram models (Heafield, 2011) to model text quality either through prediction of similarity to designated high-quality sources, prediction of toxicity, or n-gram model perplexity. Finally, LLM-Based filters leverage LLMs to directly predict aspects of data quality or annotate data to train a distilled model to evaluate quality. Table 9: List of the 16 evaluated pretraining corpora filters, including an example dataset or study employing each filter and a brief description. * denotes that the filter was evaluated in the development of the cited dataset, but not used in a resulting corpora. \u2020 indicates filters that were replicated; all other filters are taken from their respective cited sources.",
            "score": 0.4353604024296749,
            "section_title": "I.1 Filter Details",
            "char_start_offset": 37598,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 218
                },
                {
                    "start": 219,
                    "end": 510
                },
                {
                    "start": 511,
                    "end": 660
                },
                {
                    "start": 661,
                    "end": 808
                },
                {
                    "start": 809,
                    "end": 926
                },
                {
                    "start": 927,
                    "end": 1033
                }
            ],
            "ref_mentions": [
                {
                    "start": 304,
                    "end": 325,
                    "matchedPaperCorpusId": "1210515"
                },
                {
                    "start": 344,
                    "end": 360,
                    "matchedPaperCorpusId": "8313873"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.147216796875
        },
        {
            "corpus_id": "261505094",
            "title": "Metamorphic Malware and Obfuscation: A Survey of Techniques, Variants, and Generation Kits",
            "text": "Similar poor results were obtained in [387] which reported F1 scores ranging from 0.33 to 0.72 for the 8 malware types based on a similar LSTM approach. Te work of [388] went one step further and compared an LSTM approach to that of a transformer and fnally to a bidirectional encoder representation from transformers (BERT). BERT relies on learning latent representations from both directional contexts from before and after sequences, meaning that it does a better job encoding context of the API sequence. In [388], they also used the Windows PE malware dataset and found similar issues classifying the 8 classes with a weighted F1 score of 0.51 on their best performing BERT model. One approach that did fnd success using BERT was that of [389] who implemented fastText [390], a text vectorizing technique based on n-gram. While removing redundant API calls, such as NtDelayExecution, accuracies as high as 96.76% using BERT were obtained.",
            "score": 0.43512680202484894,
            "section_title": "Natural Language Processing",
            "char_start_offset": 116224,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 152
                },
                {
                    "start": 153,
                    "end": 325
                },
                {
                    "start": 326,
                    "end": 508
                },
                {
                    "start": 509,
                    "end": 685
                },
                {
                    "start": 686,
                    "end": 826
                },
                {
                    "start": 827,
                    "end": 943
                }
            ],
            "ref_mentions": [
                {
                    "start": 38,
                    "end": 43,
                    "matchedPaperCorpusId": "201621126"
                },
                {
                    "start": 164,
                    "end": 169,
                    "matchedPaperCorpusId": "235384351"
                },
                {
                    "start": 512,
                    "end": 517,
                    "matchedPaperCorpusId": "235384351"
                },
                {
                    "start": 743,
                    "end": 748,
                    "matchedPaperCorpusId": "236190873"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.14599609375
        },
        {
            "corpus_id": "277147664",
            "title": "Synthetic Data-Driven Approaches for Chinese Medical Abstract Sentence Classification: Computational Study",
            "text": "We selected multiple algorithms of different kinds as baselines to demonstrate the performance of our algorithms. To begin with, we used classical non-BERT models. TextCNN is built upon the CNN paradigm, applying multiple convolutional filters and pooling operations to capture local features of varying lengths within the text. TextRNN [32] leverages RNNs to model the sequential information in the text, capturing dependencies between words. TextRNN-Att [33] extends TextRNN by incorporating attention mechanisms, enabling the model to focus on essential words. TextRCNN [34] combines convolutional and RNNs, simultaneously considering word order and contextual information. DPCNN [35] uses multiple layers of convolution and pooling operations to capture hierarchical text features. \n\nFastText [36] is a simple and efficient text classification method based on the bag-of-words model, representing text at the word level. \n\nAdditionally, we used BERT-based methods. BERT is a transformer-based pretrained language model, capable of learning rich contextual representations. BERTCNN [37] and BERT-RNN [8] integrate CNNs and RNNs, respectively, on top of BERT, combining the contextual representations from BERT with the ability to extract local features. BERT-RCNN [38] combines BERT with both convolutional and RNNs, simultaneously considering word order and contextual information. BERT-DPCNN [39] incorporates a deep pyramid CNN on top of BERT, combining the contextual representations from BERT with multilevel feature extraction capabilities. ERNIE [40] is another transformer-based pretrained language model, built upon BERT with further improvements and optimizations, with better performance in Chinese contexts. \n\nIn our ablation study, we conducted an experiment using the k-means algorithm to cluster SBERT embeddings for comparative analysis with DocSCAN. Additionally, we examined the impact of omitting the dynamic convolutional layer in the MEC module. Subsequent experiments involving BERT serve as an extension of this analysis, further exploring the effects of removing 3 encoders within MEC.",
            "score": 0.43507417573451634,
            "section_title": "Baselines",
            "char_start_offset": 15539,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 113
                },
                {
                    "start": 114,
                    "end": 163
                },
                {
                    "start": 164,
                    "end": 328
                },
                {
                    "start": 329,
                    "end": 443
                },
                {
                    "start": 444,
                    "end": 563
                },
                {
                    "start": 564,
                    "end": 676
                },
                {
                    "start": 677,
                    "end": 785
                },
                {
                    "start": 788,
                    "end": 924
                },
                {
                    "start": 927,
                    "end": 968
                },
                {
                    "start": 969,
                    "end": 1076
                },
                {
                    "start": 1077,
                    "end": 1256
                },
                {
                    "start": 1257,
                    "end": 1385
                },
                {
                    "start": 1386,
                    "end": 1549
                },
                {
                    "start": 1550,
                    "end": 1722
                },
                {
                    "start": 1725,
                    "end": 1869
                },
                {
                    "start": 1870,
                    "end": 1969
                },
                {
                    "start": 1970,
                    "end": 2112
                }
            ],
            "ref_mentions": [
                {
                    "start": 337,
                    "end": 341,
                    "matchedPaperCorpusId": "16017905"
                },
                {
                    "start": 456,
                    "end": 460,
                    "matchedPaperCorpusId": "9870160"
                },
                {
                    "start": 573,
                    "end": 577,
                    "matchedPaperCorpusId": "203605319"
                },
                {
                    "start": 683,
                    "end": 687,
                    "matchedPaperCorpusId": "29191669"
                },
                {
                    "start": 1085,
                    "end": 1089,
                    "matchedPaperCorpusId": "256491955"
                },
                {
                    "start": 1103,
                    "end": 1106,
                    "matchedPaperCorpusId": "6857205"
                },
                {
                    "start": 1397,
                    "end": 1401,
                    "matchedPaperCorpusId": "234222545"
                },
                {
                    "start": 1556,
                    "end": 1560,
                    "matchedPaperCorpusId": "158046772"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.607421875
        },
        {
            "corpus_id": "233169058",
            "title": "HumAID: Human-Annotated Disaster Incidents Data from Twitter with Deep Learning Benchmarks",
            "text": "To train the classifiers using the aforementioned classical algorithms, we converted the preprocessed tweets into bagof-n-gram vectors weighted with logarithmic term frequencies (tf) multiplied with inverse document frequencies (idf). Since contextual information, such as n-grams, are useful for classification, we extracted unigram, bigram, and trigram features. For both SVM and RF we use grid search to optimize the parameters. \n\nFor FastText, we used pre-trained embeddings trained on Common Crawl15 and default hyperparameter settings. \n\nFor transformer-based models, we use the Transformer Toolkit (Wolf et al. 2019) and fine-tune each model using the settings as described in (Devlin et al. 2019) with a taskspecific layer on top of the transformer model. Due to the instability of the pre-trained models as reported by (Devlin et al. 2019), we do 10 runs of each experiment using different random seeds and choose the model that performs the best on the development set. For training the BERT model for each event, event-type and all combined dataset, we use a batch size of 32, learning rate of 2e-5, maximum sequence length 128, and fine tune 10 epochs with the 'categorical cross-entropy' as the loss function.",
            "score": 0.4346847267644208,
            "section_title": "Classification Experiments",
            "char_start_offset": 27040,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 234
                },
                {
                    "start": 235,
                    "end": 364
                },
                {
                    "start": 365,
                    "end": 431
                },
                {
                    "start": 434,
                    "end": 541
                },
                {
                    "start": 544,
                    "end": 763
                },
                {
                    "start": 764,
                    "end": 979
                },
                {
                    "start": 980,
                    "end": 1222
                }
            ],
            "ref_mentions": [
                {
                    "start": 684,
                    "end": 704,
                    "matchedPaperCorpusId": "52967399"
                },
                {
                    "start": 828,
                    "end": 848,
                    "matchedPaperCorpusId": "52967399"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.279052734375
        },
        {
            "corpus_id": "264290005",
            "title": "Combating Short Circuit Behavior in Natural Language Reasoning: Crossover and Mutation Operations for Enhanced Robustness",
            "text": "We mainly investigate three popular classifiers based on pre-trained language models. There are several available versions of pre-trained models differing in the number of layers and parameters. We choose to use the base version of each model. We train and test all the models on a server with a GeForce GTX 1080 Ti GPU with 11G RAM and an Intel(R) Xeon(R) CPU E5-2630 with 128G of RAM. \n\nBERT RoBERTa (RB) is an improved pre-training procedure of BERT that involves training the model on more data, using larger batch sizes, and removing the NSP objective. These changes result in a more robust and better-performing model compared to the original BERT architecture.",
            "score": 0.4343331764756567,
            "section_title": "Models",
            "char_start_offset": 16624,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 85
                },
                {
                    "start": 86,
                    "end": 194
                },
                {
                    "start": 195,
                    "end": 243
                },
                {
                    "start": 244,
                    "end": 386
                },
                {
                    "start": 389,
                    "end": 557
                },
                {
                    "start": 558,
                    "end": 667
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.3544921875
        },
        {
            "corpus_id": "244895506",
            "title": "Transformer based Contextual Model for Sentiment Analysis of Customer Reviews: A Fine-tuned BERT",
            "text": "As BERT is pre-trained with large unlabeled text datasets, this model can be easily fine-tuned for small datasets that are specific to an NLP task like sentiment prediction on customer or employees' reviews and questionanswer system for chatbot applications. Fig. 2 shows the neural network architecture of BERT's deep bidirectional and OpenAI GPT's unidirectional (Left-to-Right) contextual models [1], in which the unidirectional model generates a representation for each word based on other words in the same sentence. The BERT bidirectional model represents both the previous and next context in a sentence. However, the context free Word2vec and Glove models generate a word representation based on each word in the vocabulary. \n\nThere are many organizations that rely on reviews to improve customer experience and increase revenue of their products and services. Positive sentiments are one of the key factors for the success of several online businesses. However, determining the context of the review, polarity, and sentiment in textual content of customer reviews remain a challenge. The custom and hybrid deep learning models (LSTM, BiLSTM, fastText and fastText-BiLSTM) perform higher in textual datasets compare to traditional models [Naive Bays (NB), Logistic Regression (LR), Decision Tree (DT), Random Forest (RF), and Support Vector Machines (SVM)]. www.ijacsa.thesai.org Evaluating Transformer based models and comparing their results with other state-of-the-art LTSM models are important for identifying the best model to utilize for Sentiment Analysis applications. Several researchers have used traditional models for sentiment analysis on datasets of customer reviews and discussed about their experience and issues occurred from it. These issues include time consumption of tasks such as data pre-processing, preparation for testing and training the datasets, as well as problems in performance and accuracy. We identified the gap in leveraging the pre-trained BERT model for datasets of customer reviews and found that the pretrained model solves the problems and issues that traditional models have. \n\nThe pre-trained BERT model's performance, accuracy and approach motivated the authors to experiment with this model for customer sentiment analysis.",
            "score": 0.43388891725100365,
            "section_title": "I. INTRODUCTION",
            "char_start_offset": 1721,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 258
                },
                {
                    "start": 259,
                    "end": 521
                },
                {
                    "start": 522,
                    "end": 611
                },
                {
                    "start": 612,
                    "end": 732
                },
                {
                    "start": 735,
                    "end": 868
                },
                {
                    "start": 869,
                    "end": 961
                },
                {
                    "start": 962,
                    "end": 1092
                },
                {
                    "start": 1093,
                    "end": 1365
                },
                {
                    "start": 1366,
                    "end": 1387
                },
                {
                    "start": 1388,
                    "end": 1584
                },
                {
                    "start": 1585,
                    "end": 1754
                },
                {
                    "start": 1755,
                    "end": 1930
                },
                {
                    "start": 1931,
                    "end": 2123
                },
                {
                    "start": 2126,
                    "end": 2274
                }
            ],
            "ref_mentions": [
                {
                    "start": 399,
                    "end": 402,
                    "matchedPaperCorpusId": "85518643"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.435791015625
        },
        {
            "corpus_id": "232071894",
            "title": "Automatic Exam Correction Framework (AECF) for the MCQs, Essays, and Equations Matching",
            "text": "FastText overcomes this drawback. The word embeddings from FastText look very similar to the ones generated by Word2Vec. FastText is not calculated directly, but there is a combination of lower-level embeddings. There are two main advantages: (1) generalization is available as new words have the same characters as known ones and (2) less training data is required since much more information can be extracted from each part of the text [25]. Table 2 summarizes Word2Vec, GloVe, and Fast-Text. The pre-trained model weights can be downloaded and used for Word2Vec, GloVe, and FastText (from their official website) https://code.google.com/archive/p/word2vec/, https://nlp.stanford.edu/projects/glove/, and https://fasttext. cc/ respectively. Document (i.e. paragraph or sentence) embedding considers the entire sentence and their semantic information represented as vectors. This will help machines in understanding the context of the entire text. There are many techniques including the state-of-the-art ones such as Doc2Vec, Sen-tenceBERT, InferSent, InferSent2, and Universal Sentence Encoder (USE). Doc2Vec is an extension of the Word2Vec. It is one of the most popular techniques that was used as an unsupervised algorithm above the Word2Vec. There are two ways to achieve that: (1) Distributed Memory version of Paragraph Vector (PVDM) and (2) Distributed Bag of Words version of Paragraph Vector (PVDOBW). Both of them are useful but the first is recommended for most of the tasks. \n\nBERT is a Bidirectional Encoder Representations from Transformers for the pre-training over a lot of unlabeled textual data. It is based on Google's released transformer architecture.",
            "score": 0.4338289053034261,
            "section_title": "3) EXTRACTING FEATURES FROM TEXT PHASE",
            "char_start_offset": 6527,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 33
                },
                {
                    "start": 34,
                    "end": 120
                },
                {
                    "start": 121,
                    "end": 211
                },
                {
                    "start": 212,
                    "end": 443
                },
                {
                    "start": 444,
                    "end": 494
                },
                {
                    "start": 495,
                    "end": 660
                },
                {
                    "start": 661,
                    "end": 702
                },
                {
                    "start": 703,
                    "end": 724
                },
                {
                    "start": 725,
                    "end": 742
                },
                {
                    "start": 743,
                    "end": 757
                },
                {
                    "start": 758,
                    "end": 875
                },
                {
                    "start": 876,
                    "end": 948
                },
                {
                    "start": 949,
                    "end": 1103
                },
                {
                    "start": 1104,
                    "end": 1144
                },
                {
                    "start": 1145,
                    "end": 1248
                },
                {
                    "start": 1249,
                    "end": 1413
                },
                {
                    "start": 1414,
                    "end": 1489
                },
                {
                    "start": 1492,
                    "end": 1616
                },
                {
                    "start": 1617,
                    "end": 1675
                }
            ],
            "ref_mentions": [
                {
                    "start": 438,
                    "end": 442,
                    "matchedPaperCorpusId": "210867029"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.37060546875
        },
        {
            "corpus_id": "235422530",
            "title": "Modeling Profanity and Hate Speech in Social Media with Semantic Subspaces",
            "text": "To achieve good coverage of profane language, we use 300-dimensional German FastText embeddings (Deriu et al., 2017) trained on 50M German tweets for the word-level experiments in Section 5. \n\nThe BERT models (Devlin et al., 2019) used in Section 6 are Bert-Base-German-Cased 4 and Bert-Base-Multilingual-Cased for the monolingual and multilingual experiments respectively, since they pose strong baselines. We fine-tune on the Twitter data (Section 4.1) using the masked language modeling objective and early stopping over the evaluation loss (\u03b4 = 0, patience = 3). All classification experiments use Linear Discriminant Analysis (LDA) as the classifier.",
            "score": 0.43316008376619564,
            "section_title": "Model Specifications",
            "char_start_offset": 12820,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 190
                },
                {
                    "start": 193,
                    "end": 407
                },
                {
                    "start": 408,
                    "end": 566
                },
                {
                    "start": 567,
                    "end": 655
                }
            ],
            "ref_mentions": [
                {
                    "start": 96,
                    "end": 116,
                    "matchedPaperCorpusId": "1562663"
                },
                {
                    "start": 209,
                    "end": 230,
                    "matchedPaperCorpusId": "52967399"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.2022705078125
        },
        {
            "corpus_id": "227231089",
            "title": "Go Simple and Pre-Train on Domain-Specific Corpora: On the Role of Training Data for Text Classification",
            "text": "Linear models. Linear models such as SVMs or logistic regression coupled with frequency-based handcrafted features have been traditionally used for text classification. Despite their simplicity, they are considered a strong baseline for many text classification tasks (Joachims, 1998;McCallum et al., 1998;Fan et al., 2008), even more recently on noisy corpora such as social media text (C \u00b8\u00f6ltekin and Rama, 2018;Mohammad et al., 2018). In general, however, these methods tend to struggle with OOV (Out-Of-Vocabulary) words, fine-grained distinctions and unbalanced datasets. FastText (Joulin et al., 2017), which is the model evaluated in this paper, partially addresses these issues by integrating a linear model with a rank constraint, allowing sharing parameters among features and classes, and integrates word embeddings that are then averaged into a text representation. \n\nNeural models. Neural models can learn non-linear and complex relationships which makes them a preferable method for many NLP tasks such as sentiment analysis or question answering (Sun et al., 2019). In particular, LSTMs, sometimes in combination with CNNs for text classification (Xiao and Cho, 2016;Pilehvar et al., 2017), enable capturing long-range dependencies in a sequential manner where data is read from only one direction (referred to as the 'unidirectionality constraint'). Recent state-of-the-art language models, such as BERT (Devlin et al., 2019), overcome the unidirectionality constraint by using transformer-based masked language models to learn pre-trained deep bidirectional representations. These pre-trained models leverage generic knowledge on large unlabeled corpora that can then be fine-tuned on the specific task by using the pre-trained parameters. BERT, which is the pretrained language model tested in this paper, has been proved to provide state-of-the-art results in most standard NLP benchmarks (Wang et al., 2019b), including text classification.",
            "score": 0.4331175836356697,
            "section_title": "Supervised machine learning models",
            "char_start_offset": 3077,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 14
                },
                {
                    "start": 15,
                    "end": 168
                },
                {
                    "start": 169,
                    "end": 437
                },
                {
                    "start": 438,
                    "end": 576
                },
                {
                    "start": 577,
                    "end": 877
                },
                {
                    "start": 880,
                    "end": 894
                },
                {
                    "start": 895,
                    "end": 1080
                },
                {
                    "start": 1081,
                    "end": 1365
                },
                {
                    "start": 1366,
                    "end": 1591
                },
                {
                    "start": 1592,
                    "end": 1756
                },
                {
                    "start": 1757,
                    "end": 1960
                }
            ],
            "ref_mentions": [
                {
                    "start": 268,
                    "end": 284,
                    "matchedPaperCorpusId": "2427083"
                },
                {
                    "start": 284,
                    "end": 306,
                    "matchedPaperCorpusId": "7311285"
                },
                {
                    "start": 306,
                    "end": 323,
                    "matchedPaperCorpusId": "3116168"
                },
                {
                    "start": 387,
                    "end": 414,
                    "matchedPaperCorpusId": "43958367"
                },
                {
                    "start": 414,
                    "end": 436,
                    "matchedPaperCorpusId": "4941467"
                },
                {
                    "start": 586,
                    "end": 607,
                    "matchedPaperCorpusId": "1210515"
                },
                {
                    "start": 1061,
                    "end": 1079,
                    "matchedPaperCorpusId": "153312532"
                },
                {
                    "start": 1182,
                    "end": 1204,
                    "matchedPaperCorpusId": "22604822"
                },
                {
                    "start": 1420,
                    "end": 1441,
                    "matchedPaperCorpusId": "52967399"
                },
                {
                    "start": 1908,
                    "end": 1928,
                    "matchedPaperCorpusId": "5034059"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.71630859375
        },
        {
            "corpus_id": "252590734",
            "title": "L-Boost: Identifying Offensive Texts from Social Media Post in Bengali",
            "text": "Receiver operating characteristic (ROC) curves are often employed to exhibit the graphical method of the relationship between sensitivity and specificity for a possible cut-off for an experimental combination. Figures 10(a To find the best feature extraction model, we applied conventional machine learning algorithms using the whole sentence instead of various features to calculate performance. Among the four feature extraction technologies, BERT showed satisfactory scores for all types of classification algorithms as shown in Figure 11. The fastText feature extraction technique performs optimally for some classifiers and also gives poor results for some machine learning algorithms and shows that it is not stable for the offensive text classification. In addition, the Word2Vec feature extraction technique requires a large amount of time for big datasets, and the accuracy score is poor compared to the TF-IDF feature extraction method. \n\nFor the above reason, our proposed method uses BERT feature extraction techniques to process large datasets and extract useful information from chaotic datasets. To further clarify, in this article, we used different N-Grams for the traditional machine learning models and full text for the BERT-based model.",
            "score": 0.4329261017180616,
            "section_title": "2) Graphical Evaluation",
            "char_start_offset": 38082,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 209
                },
                {
                    "start": 210,
                    "end": 396
                },
                {
                    "start": 397,
                    "end": 542
                },
                {
                    "start": 543,
                    "end": 760
                },
                {
                    "start": 761,
                    "end": 946
                },
                {
                    "start": 949,
                    "end": 1110
                },
                {
                    "start": 1111,
                    "end": 1257
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.461669921875
        },
        {
            "corpus_id": "259376554",
            "title": "Sefamerve at SemEval-2023 Task 12: Semantic Evaluation of Rarely Studied Languages",
            "text": "Through our experimentation, we aimed to determine the most effective model for sentiment classification on the provided dataset. Our best results were obtained using the language-specific transformer models, which achieved an F1 score of 0.702 for the Amharic language, ranking 3rd among all submissions. \n\nThe paper is organized in the following way. In section 2, system overview, we explore the best models for the classification tasks on the provided dataset by experimenting with different model approaches. In section 2.1, we start experimenting FastText word embedding model and adapt this model for all three sub-tasks in the competition dataset. In 2.2, we adapt the transformers-based language model AfriBERTa, an improvement over the original BERT model, trained on a large corpus of multilingual text. Finally, in section 2.3, we use the language-specific models that were developed specifically for African languages. In the results and conclusion sections, we provide details about the results of each model used, including their optimization, and evaluation metrics as well as our conclusions regarding the study.",
            "score": 0.43284649384587637,
            "section_title": "Introduction",
            "char_start_offset": 2237,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 129
                },
                {
                    "start": 130,
                    "end": 305
                },
                {
                    "start": 308,
                    "end": 352
                },
                {
                    "start": 353,
                    "end": 513
                },
                {
                    "start": 514,
                    "end": 655
                },
                {
                    "start": 656,
                    "end": 814
                },
                {
                    "start": 815,
                    "end": 931
                },
                {
                    "start": 932,
                    "end": 1129
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.422119140625
        },
        {
            "corpus_id": "278394813",
            "title": "Ultra-FineWeb: Efficient Data Filtering and Verification for High-Quality LLM Training Data",
            "text": "In addition, to effectively reduce computational cost, we adopt a lightweight classifier based on fastText (Joulin et al., 2016). Compared to LLM-based classifiers (Penedo et al., 2024), the fastText-based classifier demonstrates superior inference efficiency, enabling both the filtering of higher-quality training data for LLMs and significantly accelerating the high-quality data filtration pipeline. We apply the proposed data filtering pipeline to the FineWeb (Penedo et al., 2024) and Chinese FineWeb (Yu et al., 2025) datasets (source data from Chinese FineWeb-edu-v2, which includes IndustryCorpus2 (Shi et al., 2024), MiChao (Liu et al., 2023), WuDao (BAAI, 2023), SkyPile (Wei et al., 2023), WanJuan (Qiu et al., 2024), ChineseWebText (Chen et al., 2023), TeleChat (He et al., 2024), and CCI3 (Wang et al., 2024)), resulting in the creation of higher-quality Ultra-FineWeb-en and Ultra-FineWeb-zh datasets, collectively referred to as Ultra-FineWeb. Experimental results show that LLMs trained on Ultra-FineWeb perform excellently across multiple benchmark tasks, providing empirical validation for the effectiveness of our high-quality data filtering pipeline and its efficiency in reducing computational costs. \n\nOur main contributions are as follows. The datasets and classifier will be released to facilitate the development of LLMs. \n\n\u2022 Efficient Verification Strategy: We propose a computationally efficient verification strategy that enables rapid evaluation of the impact of data on LLM training performance with minimal computational cost, significantly improving the efficiency of high-quality data filtering experiments.",
            "score": 0.4326279546355262,
            "section_title": "Introduction",
            "char_start_offset": 3468,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 129
                },
                {
                    "start": 130,
                    "end": 403
                },
                {
                    "start": 404,
                    "end": 959
                },
                {
                    "start": 960,
                    "end": 1222
                },
                {
                    "start": 1225,
                    "end": 1263
                },
                {
                    "start": 1264,
                    "end": 1347
                },
                {
                    "start": 1350,
                    "end": 1641
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.29443359375
        },
        {
            "corpus_id": "233330456",
            "title": "Automatic detection of procedural knowledge in robotic-assisted surgical texts",
            "text": "In the first four rows of Table 1, we report the classification performance of the classical ML algorithms that exploit TF-IDF as features. The considered ML approaches based on TF-IDF have mediocre performance when used to solve this task. This fact could be due to the unbalanced dataset, which is difficult to be handled by standard ML algorithms. Classical ML approaches are often biased towards the majority class (F1 scores on the procedural class are substantially higher than on the non-procedural one), not considering the data distribution. In the worst case, minority classes are treated as outliers and ignored. Moreover, TF-IDF cannot account for the similarity between the words in a document since each word is independently presented as an index. Among the considered ML algorithms, Random-Forest obtains the highest F1 scores.\n\nThe fifth row of Table 1 summarizes the performance of the FastText classifier. All scores demonstrate that FastText obtains much higher classification performance than the best considered ML method (Ra-Fo). In particular, it provides an improvement of 10.56% over Macro-F1 and 8.15% over Weighted-F1.\n\nWe then fed the FastText word embeddings learned on the dataset to a 1D-CNN and a Bi-LSTM. The adoption of more complex classification models allows, in our task, to sub-   Table show that it is possible to achieve high classification performance also using transformer-based pre-trained language models. In particular, ClinicalBERT performs slightly better than Bi-LSTM (+ 0.12% of Macro-F1 and + 0.22% of Weighted-F1), while BERT performs slightly worse than Bi-LSTM. As somehow expected, given the characteristics of the source material used for pre-training the model (c.f., Sect. 3.3.4), ClinicalBERT (pre-trained on clinical notes) performs better than BERT (pre-trained on Wikipedia and BooksCorpus). Computational wise, fine-tuning transformers-based models on our dataset is 4 times slower than training Bi-LSTM.\n\nWe also investigated whether it is possible to boost classification performance by balancing the dataset. More precisely, we have applied standard random over-sampling techniques (i.e., the addition of random",
            "score": 0.43251936705703864,
            "section_title": "Results and discussion",
            "char_start_offset": 21224,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.49755859375
        },
        {
            "corpus_id": "267985600",
            "title": "Predicting the Type of Road Traffic Accident for Test Scenario Generation",
            "text": "As classifiers, we investigate a pre-trained model (PTM) focusing on text classification only. As a PTM, we consider a BERT [45] -based PTM called \"bert-based-german-cased\", adapted for the German language. When published, BERT was one of the leading PTM models for understanding textual data [46], owing to its ability to grasp the context of several sentences [45]. Second, we investigate a gradient-boosting machine (GBM) called CatBoost [47]. On the one hand, CatBoost can learn text features only, providing the same advantages as in the BERT case. On the other hand, CatBoost can handle quantitative and qualitative features as well as noisy and heterogeneous data [47], [48]. PD from one of the five federal states in Germany, coding 3AT, is used as exemplary training data. To reduce the complexity, the model focuses on classifying turning conflicts between two participants 2 in the first step. \n\nThe remainder of this study is organized as follows. In Section II we provide background information on 3AT and the models used. In Section III we illustrate our modelling approach including the derivation of detailed research questions. In Section IV we list all the results, which are then discussed in Section V. Finally, we draw an overall conclusion in Section VI and provide an outlook for future research.",
            "score": 0.4321520292114892,
            "section_title": "I. INTRODUCTION",
            "char_start_offset": 6187,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 94
                },
                {
                    "start": 95,
                    "end": 206
                },
                {
                    "start": 207,
                    "end": 367
                },
                {
                    "start": 368,
                    "end": 446
                },
                {
                    "start": 447,
                    "end": 553
                },
                {
                    "start": 554,
                    "end": 682
                },
                {
                    "start": 683,
                    "end": 781
                },
                {
                    "start": 782,
                    "end": 904
                },
                {
                    "start": 907,
                    "end": 959
                },
                {
                    "start": 960,
                    "end": 1035
                },
                {
                    "start": 1036,
                    "end": 1144
                },
                {
                    "start": 1145,
                    "end": 1319
                }
            ],
            "ref_mentions": [
                {
                    "start": 124,
                    "end": 128,
                    "matchedPaperCorpusId": "52967399"
                },
                {
                    "start": 293,
                    "end": 297,
                    "matchedPaperCorpusId": "212747830"
                },
                {
                    "start": 362,
                    "end": 366,
                    "matchedPaperCorpusId": "52967399"
                },
                {
                    "start": 441,
                    "end": 445,
                    "matchedPaperCorpusId": "5044218"
                },
                {
                    "start": 671,
                    "end": 675,
                    "matchedPaperCorpusId": "5044218"
                },
                {
                    "start": 677,
                    "end": 681,
                    "matchedPaperCorpusId": "226254770"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.206787109375
        },
        {
            "corpus_id": "272397823",
            "title": "Arctic-SnowCoder: Demystifying High-Quality Data in Code Pretraining",
            "text": "This filter selects high-quality The Stack v1 and StackOverflow content, significantly enhancing coding performance. FineWeb-Edu [30] employs a linear regressor built on Snowflake-arctic-embed-m [26], an advanced embedding model based on BERT [11]. This regressor, trained on 400k Llama-3 [12] annotations rating the educational value (0-5) of FineWeb dataset documents, significantly enhances STEM performance. DCLM-Baseline [18] uses a fastText [5] filter trained on positives from OpenHermes 2.5 [37], high-scoring posts from r/ExplainLikeImFive, and random negatives. It outperforms FineWeb-Edu in top-10% selection. DeepSeek-Coder-V2 [10] follows DeepSeek-Math [34] by leveraging a multistage fastText-based pipeline to recall high-quality code and math contents. Llama-3 [12] uses fastText for recognizing text referenced by Wikipedia [42] and Roberta-based [22] classifiers trained on Llama-2 [38] predictions. While prior work focuses on initial pretraining, Arctic-SnowCoder demonstrates that high-quality data from the pretraining corpus can significantly enhance model performance during continued pretraining. We are also the first to uncover the secret of data quality, revealing the importance of matching data distribution with downstream tasks. \n\nHigh-quality code data for pretraining Phi-1 [13] is one of the first to study the impact of highquality code data. It first uses a random forest classifier to filter out high-quality code data from \n\nThe Stack v1 and StackOverflow, and then creates synthetic textbook-like data and exercises using GPT-3.5 [29], showing significant coding performance with only 50B+ training tokens. DeepSeek-Coder-V2 [10], pretrained for around 14T tokens in total, achieves state-of-the-art coding performance, with a multi-stage fastText-based [5] pipeline to recall web-related code data as well as high-quality GitHub code.",
            "score": 0.43179246527105325,
            "section_title": "Related Work",
            "char_start_offset": 20921,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 116
                },
                {
                    "start": 117,
                    "end": 248
                },
                {
                    "start": 249,
                    "end": 411
                },
                {
                    "start": 412,
                    "end": 571
                },
                {
                    "start": 572,
                    "end": 620
                },
                {
                    "start": 621,
                    "end": 768
                },
                {
                    "start": 769,
                    "end": 917
                },
                {
                    "start": 918,
                    "end": 1121
                },
                {
                    "start": 1122,
                    "end": 1260
                },
                {
                    "start": 1263,
                    "end": 1378
                },
                {
                    "start": 1379,
                    "end": 1461
                },
                {
                    "start": 1464,
                    "end": 1646
                },
                {
                    "start": 1647,
                    "end": 1875
                }
            ],
            "ref_mentions": [
                {
                    "start": 243,
                    "end": 247,
                    "matchedPaperCorpusId": "52967399"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.384521484375
        },
        {
            "corpus_id": "257985024",
            "title": "Multi-label classification of open-ended questions with BERT",
            "text": "Multi-label BERT has several decisive advantages: a) Multi-label BERT convincingly beats the usual multi-label algorithms in terms of 0/1 loss (predicting all labels correctly vs not). Pre-training may be particularly advantageous when the training data are small. Our training data was larger (N=17,584) than the typical training data for open-ended questions in the social sciences, and yet BERT clearly wins. b) Because they are hard problems, multi-label approaches have struggled to predict well enough to be useful in settings where high quality is important. For example, the 0/1 loss for ECC ranged from 35.8% to 44.9% for classifying open-ended questions (Schonlau et al., 2021). For mildly multilabel problems, for the first time, we have reached a 0/1 loss that we believe is low enough to make (fully) automatic classification attractive. That is, semi-automatic classification is optional rather than necessary. c) Multi-label BERT obtains predictions from a single model. For the other multi-label algorithms introduced we need one model per label. Using multi-label BERT eliminates any need for iterating through the individual models to improve predictions. d) The analysis of bad predictions showed some of BERT's bad predictions were in fact plausible in light of the coding manual. Further, BERT was much more robust to spelling mistakes than the leading n-gram based approach, ECC. \n\nWe see the following limitations: a) Python is required. Social scientists are more familiar with programs like Stata and R. The use of BERT requires programming in Python. We make our code available in the supplementary material to reduce the burden somewhat. b) We used German BERT, a language specific model. Since German BERT is less well established than the original BERT, and because of success for single label classification (Gweon and Schonlau, 2023), there is no reason to believe this would not work in the English language. c) The multi-label algorithms we have shown use SVMs as the base learner.",
            "score": 0.43174025434405305,
            "section_title": "Discussion",
            "char_start_offset": 23969,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 184
                },
                {
                    "start": 185,
                    "end": 264
                },
                {
                    "start": 265,
                    "end": 411
                },
                {
                    "start": 412,
                    "end": 565
                },
                {
                    "start": 566,
                    "end": 688
                },
                {
                    "start": 689,
                    "end": 850
                },
                {
                    "start": 851,
                    "end": 924
                },
                {
                    "start": 925,
                    "end": 985
                },
                {
                    "start": 986,
                    "end": 1062
                },
                {
                    "start": 1063,
                    "end": 1173
                },
                {
                    "start": 1174,
                    "end": 1300
                },
                {
                    "start": 1301,
                    "end": 1401
                },
                {
                    "start": 1404,
                    "end": 1460
                },
                {
                    "start": 1461,
                    "end": 1576
                },
                {
                    "start": 1577,
                    "end": 1664
                },
                {
                    "start": 1665,
                    "end": 1715
                },
                {
                    "start": 1716,
                    "end": 1940
                },
                {
                    "start": 1941,
                    "end": 2014
                }
            ],
            "ref_mentions": [
                {
                    "start": 664,
                    "end": 687,
                    "matchedPaperCorpusId": "201880226"
                },
                {
                    "start": 1838,
                    "end": 1864,
                    "matchedPaperCorpusId": "252211903"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.1683349609375
        },
        {
            "corpus_id": "218468461",
            "title": "Few-Shot Learning for Chinese Legal Controversial Issues Classification",
            "text": "To capture the semantic information in controversial issues and convert legal text to low dimensional vectors, several embedding methods were adopted. \n\nWord embedding methods represent words as continuous vectors in a low dimensional space which capture lexical and semantic properties of words. The Word2vec model, which introduced by Tomas Mikolov et al. [2], has gained a lot of attention. The vector representations of words learned by Word2vec model have been proven to capture semantic meanings and are pivotal in various NLP tasks. Word2vec contains two architectures: continuous Bag-of-Words (CBOW) and continuous Skip-Gram. CBOW tends to predict the current word based on the contexts, while SkipGram tries to classify context words based on current words. Typically, we take the average of each term vectors as the meaning of a longer piece of text containing multiple terms. \n\nFastText is a library created by Facebook's AI Research (FAIR) lab for efficient learning of word representations and sentence classification. FastText has gained a lot of attention in the NLP community as it has shown outstanding results in various NLP domains. Piotr Bojanowski proposed the model that can learn word representations while taking into account morphology [3]. They modeled morphology by considering subword units, and representing words by a sum of its character n-grams. FastText enables to compute representations for words that do not appear in the training data. Also, it is fast, allowing to train models on large corpora quickly. \n\nBERT, which stands for Bidirectional Encoder Representations from Transformers, is a new language representation model proposed by Jacob Devlin et al. in 2018 [4]. BERT demonstrates new state-of-the-art performance on NLP tasks. In pre-training procedure, unlike Peters et al. (2018) [17] and Radford et al. (2018) [18], which uses traditional left-to-right or right-to-left language models for pre-training, BERT uses two unsupervised prediction tasks to pre-train deep bidirectional representations. They masked a part of the input tokens at random, and only those masked tokens are predicted.",
            "score": 0.4314171785671505,
            "section_title": "A. EMBEDDING METHODS",
            "char_start_offset": 18179,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 150
                },
                {
                    "start": 153,
                    "end": 296
                },
                {
                    "start": 297,
                    "end": 393
                },
                {
                    "start": 394,
                    "end": 539
                },
                {
                    "start": 540,
                    "end": 633
                },
                {
                    "start": 634,
                    "end": 766
                },
                {
                    "start": 767,
                    "end": 886
                },
                {
                    "start": 889,
                    "end": 1031
                },
                {
                    "start": 1032,
                    "end": 1151
                },
                {
                    "start": 1152,
                    "end": 1265
                },
                {
                    "start": 1266,
                    "end": 1377
                },
                {
                    "start": 1378,
                    "end": 1472
                },
                {
                    "start": 1473,
                    "end": 1541
                },
                {
                    "start": 1544,
                    "end": 1707
                },
                {
                    "start": 1708,
                    "end": 1772
                },
                {
                    "start": 1773,
                    "end": 2045
                },
                {
                    "start": 2046,
                    "end": 2139
                }
            ],
            "ref_mentions": [
                {
                    "start": 1261,
                    "end": 1264,
                    "matchedPaperCorpusId": "207556454"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.53564453125
        },
        {
            "corpus_id": "237635086",
            "title": "DACT-BERT: Differentiable Adaptive Computation Time for an Efficient BERT Inference",
            "text": "The use of pre-trained language models based on large-scale Transformers (Vaswani et al., 2017) has gained popularity after the release of BERT (Devlin et al., 2019). The usual pipeline consists of finetuning BERT by adapting and retraining its classification head to meet the requirements of a specific NLP task. Unfortunately, the benefits of using a powerful model are also accompanied by a highly demanding computational load. In effect, current pre-trained language models such as BERT have millions of parameters, making them computationally intensive both during training and inference. \n\nWhile high accuracy is usually the ultimate goal, computational efficiency is also desirable. The use of a demanding model not only causes longer processing times and limits applicability to lowend devices, but it also has major implications in terms of the environmental impact of AI technologies (Schwartz et al., 2019). As an example, Strubell et al. (2019) provides an estimation of the carbon footprint of several large NLP models, including BERT, concluding that they are becoming unfriendly to the environment. \n\nFortunately, recent works have shown that behind BERT's immense capacity, there is considerable redundancy and over-parametrization (Kovaleva et al., 2019;Rogers et al., 2020). Consequently, others works have explored strategies to develop efficient and compact versions of BERT. One such strategy known as dynamic Transformers consists of providing BERT with an adaptive mechanism to control how many Transformers blocks are used (Xin et al., 2020;Liu et al., 2020;Zhou et al., 2020). \n\nIn this paper, we present DACT-BERT, an alternative to current dynamic Transformers that uses an Adaptive Computation Time mechanism (Graves, 2016) to control the complexity of the processing pipeline of BERT. This mechanism controls the number of Transformer blocks executed at inference time by using additional classifiers. This allows resulting models to take advantage of the information already encoded in intermediate layers without the need to run all layers. Specifically, our model integrates DACT, a fully differentiable variant of the adaptive computation module (Eyzaguirre and Soto, 2020) that allows us to train a halting neuron after each Transformer block. This neuron indicates the confidence the model has on returning the correct answer after executing said block.",
            "score": 0.4313636233825362,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 166
                },
                {
                    "start": 167,
                    "end": 313
                },
                {
                    "start": 314,
                    "end": 430
                },
                {
                    "start": 431,
                    "end": 593
                },
                {
                    "start": 596,
                    "end": 689
                },
                {
                    "start": 690,
                    "end": 918
                },
                {
                    "start": 919,
                    "end": 1113
                },
                {
                    "start": 1116,
                    "end": 1292
                },
                {
                    "start": 1293,
                    "end": 1395
                },
                {
                    "start": 1396,
                    "end": 1601
                },
                {
                    "start": 1604,
                    "end": 1813
                },
                {
                    "start": 1814,
                    "end": 1930
                },
                {
                    "start": 1931,
                    "end": 2071
                },
                {
                    "start": 2072,
                    "end": 2277
                },
                {
                    "start": 2278,
                    "end": 2388
                }
            ],
            "ref_mentions": [
                {
                    "start": 73,
                    "end": 95,
                    "matchedPaperCorpusId": "13756489"
                },
                {
                    "start": 144,
                    "end": 165,
                    "matchedPaperCorpusId": "52967399"
                },
                {
                    "start": 1248,
                    "end": 1271,
                    "matchedPaperCorpusId": "201645145"
                },
                {
                    "start": 1547,
                    "end": 1565,
                    "matchedPaperCorpusId": "216552850"
                },
                {
                    "start": 1565,
                    "end": 1582,
                    "matchedPaperCorpusId": "214802887"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.2105712890625
        },
        {
            "corpus_id": "211258996",
            "title": "Improving BERT Fine-Tuning via Self-Ensemble and Self-Distillation",
            "text": "Pre-training language models on a large amount of unlabeled data then fine-tuning in downstream tasks has become a new paradigm for NLP and made a breakthrough in many NLP tasks. Most of the recent pre-trained language models (e.g., BERT [Devlin et al., 2018], XLNet [Yang et al., 2019] and RoBERTa [Liu et al., 2019b]) are built with Transformer architecture [Vaswani et al., 2017]. \n\nAs a wide-used model, BERT is pre-trained on Masked Language Model Task and Next Sentence Prediction Task via a large cross-domain unlabeled corpus. BERT has two different model size: BERT BASE with a 12-layer Transformer encoder and BERT LARGE with a 24-layer Transformer encoder. Both of them take an input of a sequence of no more than 512 tokens and outputs the representation of the sequence. The sequence has one segment for text classification task or two for text matching task. A special token [CLS] is added before segments, which contain the special classification embedding. Another special token [SEP] is used for separating segments. \n\nFine-tuning BERT can deal with different natural language tasks with task-specific output layers. For text classification or text matching, BERT takes the final hidden state h of the first token [CLS] as the representation of the input sentence or sentence-pair. A simple softmax classifier is added to the top of BERT to predict the probability of label y: \n\nwhere W is the task-specific parameter matrix. The crossentropy loss is used to fine-tune BERT as well as W jointly.",
            "score": 0.43119017310157204,
            "section_title": "Pre-trained Language Models",
            "char_start_offset": 5375,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 178
                },
                {
                    "start": 179,
                    "end": 383
                },
                {
                    "start": 386,
                    "end": 534
                },
                {
                    "start": 535,
                    "end": 667
                },
                {
                    "start": 668,
                    "end": 783
                },
                {
                    "start": 784,
                    "end": 872
                },
                {
                    "start": 873,
                    "end": 972
                },
                {
                    "start": 973,
                    "end": 1033
                },
                {
                    "start": 1036,
                    "end": 1133
                },
                {
                    "start": 1134,
                    "end": 1298
                },
                {
                    "start": 1299,
                    "end": 1393
                },
                {
                    "start": 1396,
                    "end": 1442
                },
                {
                    "start": 1443,
                    "end": 1512
                }
            ],
            "ref_mentions": [
                {
                    "start": 360,
                    "end": 382,
                    "matchedPaperCorpusId": "13756489"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.253662109375
        },
        {
            "corpus_id": "276250250",
            "title": "Efficient Scientific Full Text Classification: The Case of EICAT Impact Assessments",
            "text": "We evaluate the classification performance of BERT classifiers trained on the 15 most important sentences from each full text, as determined by the various sentence selectors. In most cases, only up to five sentences were chosen as evidence by the human annotators, but we chose the larger number of 15 to increase the likelihood of many relevant sentences being selected even if the selectors perform suboptimal, while still reducing the input size significantly. The results are presented in Table 1 (Deterministic Selection). \n\nFor BERT classifiers, the evidence-based selector proves to be the most effective, significantly improving classification performance. A possible explanation is that it removes unnecessary and distracting information, most importantly because it can filter out sentences describing impacts caused by other species, thereby eliminating misleading information and implicitly creating a focus on the target species. In contrast, the BERT and Mod-ernBERT models trained on the full input did not receive the species name, which was necessary to ensure they relied on textual evidence rather than simply associating species names with specific classifications, but leading to potentially incorrect predictions in the case of multiple species being addressed in a text. Since the BERT classifier was used as basis for training the importance and entropy selectors, these models likely did not learn to filter out sentences about other species as well. However, the LLM-based selector may have developed this ability, as the species name was included when generating the LLM assessments used for training. Nevertheless, it only marginally outperforms the entropy and importance selectors. \n\nOverall, all sentence selection strategies improve classification performance, even when compared to ModernBERT, which should have access to the same (and even more) information. This holds even true for a random selection strategy, which selects 15 sentences before a training run starts and does not change this predetermined selection to mirror the deterministic selection by the other models. We see this as evidence that an overflow of information decreases classification performance, thus making our sentence selection strategy highly effective.",
            "score": 0.43096441707853145,
            "section_title": "BERT Classification Results",
            "char_start_offset": 22374,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 175
                },
                {
                    "start": 176,
                    "end": 464
                },
                {
                    "start": 465,
                    "end": 528
                },
                {
                    "start": 531,
                    "end": 665
                },
                {
                    "start": 666,
                    "end": 943
                },
                {
                    "start": 944,
                    "end": 1294
                },
                {
                    "start": 1295,
                    "end": 1476
                },
                {
                    "start": 1477,
                    "end": 1629
                },
                {
                    "start": 1630,
                    "end": 1712
                },
                {
                    "start": 1715,
                    "end": 1893
                },
                {
                    "start": 1894,
                    "end": 2111
                },
                {
                    "start": 2112,
                    "end": 2267
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.3359375
        },
        {
            "corpus_id": "277308146",
            "title": "Transformer-based ensemble model for dialectal Arabic sentiment classification",
            "text": "Key features of FastText include its ability to support both supervised and unsupervised learning, which aids in tasks such as text classification and assessing word similarity. The model utilizes hierarchical softmax to improve training efficiency, and its distinct method of representing words as collections of character n-grams helps capture semantic relationships and contextual variances effectively. In our study, we focused on the CBOW variant of FastText, applying position weights, set to a dimension of 300, using character n-grams of length 5, a context window of 5, and including 10 negative samples during the training process. \n\n. AraBERT: is a transformer-based model specifically created for understanding the Arabic language, built on the architecture of Bidirectional Encoder Representations from Transformers (BERT). It has undergone pre-training on a large dataset containing 70 million sentences collected from diverse sources, including Wikipedia, Arabic news sites, and extensive collections such as the Abulkhair Arabic Corpus and OSIAN (Antoun, Baly & Hajj, 2020). Several versions of AraBERT exist, and in our study, we focused on BERT-Large-Arabertv2, with a word embedding vector size of 300. This version is notable for its extensive structure, featuring 24 transformer layers, 1,024 hidden units, and 16 attention heads, which allow it to grasp intricate linguistic patterns in Arabic. \n\nAraBERT effectively handles both Modern Standard Arabic and various regional dialects. The model utilizes a WordPiece tokenizer tailored to the complex morphological characteristics of the Arabic language, ensuring efficient tokenization.",
            "score": 0.4309245597818362,
            "section_title": "Feature engineering",
            "char_start_offset": 40459,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 177
                },
                {
                    "start": 178,
                    "end": 406
                },
                {
                    "start": 407,
                    "end": 641
                },
                {
                    "start": 644,
                    "end": 836
                },
                {
                    "start": 837,
                    "end": 1090
                },
                {
                    "start": 1091,
                    "end": 1221
                },
                {
                    "start": 1222,
                    "end": 1416
                },
                {
                    "start": 1419,
                    "end": 1505
                },
                {
                    "start": 1506,
                    "end": 1657
                }
            ],
            "ref_mentions": [
                {
                    "start": 1062,
                    "end": 1089,
                    "matchedPaperCorpusId": "211678011"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.7119140625
        },
        {
            "corpus_id": "269877815",
            "title": "Automatic identification of role-specific information in product development: a critical review on large language models",
            "text": "It can be seen that the models \"BERT\" and \"RoBERTa\" as well as \"LLaMa2\" fulfil the defined basic requirements.After the pre-selection, in a second step, the three language models are tested for the central task of text classification and, based on this, one model is selected for validation.For reasons of comparability, literature studies are used in which the models \"BERT\", \"RoBERTa\" and \"LLaMa2\" are analysed on the same training and test data sets.The results of the classification taskbased on four different datasets (the datasets differ in terms of content, number of data and number of labels -e.g. the dataset \"AGNews\" contains 120,000 news texts, of which 7,600 have to be classified with the labels \"World\", \"Sport\", \"Economy\" and \"Technology\") -show that the prediction quality of the three models has comparable results in the region of 60%.This means that each model classifies about 60% of the texts correctly.Another finding is that the larger LLaMa2 version with 13 billion parameters leads to a decrease in classification performance.This is due to underfitting, as there is not enough training data to optimally adjust the parameters of the model.(Li et al., 2023) Smaller models, such as BERT and RoBERTa, are particularly suitable for the use case defined here, where only a very small training data set can be used, due to their architecture and the size of the model.According to the studies by Briskilal et al. and Shaheen et al. it can be concluded that RoBERTa has a higher classification accuracy than BERT.In the study by Briskila et al. a classification of idioms is carried out.The results are transferable to standards as a data set, since idioms are text fragments that do not mean what they imply.Standards are also deliberately written implicitly, leaving room for interpretation.Here, the specific language can be seen as a unifying element that causes difficulties in the classification task.1470 pieces of text were classified and RoBERTa classified them correctly with an accuracy of 88%.In addition, the study by Shaheen et al. presents a pairwise comparison of language models in relation to the classification of legal texts.",
            "score": 0.4309221726581207,
            "section_title": "Analysis and selection of a pre-trained language model",
            "char_start_offset": 12876,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 110
                },
                {
                    "start": 110,
                    "end": 291
                },
                {
                    "start": 291,
                    "end": 453
                },
                {
                    "start": 453,
                    "end": 855
                },
                {
                    "start": 855,
                    "end": 926
                },
                {
                    "start": 926,
                    "end": 1053
                },
                {
                    "start": 1053,
                    "end": 1167
                },
                {
                    "start": 1167,
                    "end": 1391
                },
                {
                    "start": 1391,
                    "end": 1535
                },
                {
                    "start": 1535,
                    "end": 1609
                },
                {
                    "start": 1609,
                    "end": 1731
                },
                {
                    "start": 1731,
                    "end": 1815
                },
                {
                    "start": 1815,
                    "end": 1929
                },
                {
                    "start": 1929,
                    "end": 2027
                },
                {
                    "start": 2027,
                    "end": 2167
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.10931396484375
        },
        {
            "corpus_id": "265162335",
            "title": "Bridging Qualitative Data Silos: The Potential of Reusing Codings Through Machine Learning Based Cross-Study Code Linking",
            "text": "It relies on bidirectional language models pre-trained on extensive datasets.We select gbert-base (Chan et al., 2020) for our experiments, as this is one of the most popular monolingual German models currently available.Here, the only text cleanup necessary is the removal of special markers used in interview transcripts to separate questions and answers.\n\nWe perform hyperparameter tuning with 5-fold (80% training -20% validation) crossvalidation and stratified sampling to optimize the ML models and prevent overfitting to the training data.The target metric depends on the respective task.If used for suggestions during coding, the focus could be more on recall.Whereas if the classifier is used to identify sections in other documents that match existing codes, both high precision (P) and recall (R) are crucial.As this fits our goal, we tune the models for a high micro-averaged F 1 -score.Additionally, both fastText and BERT provide a confidence score for each label.Applying a confidence threshold allows limiting the number of outputs to trade higher precision for lower recall or vice versa.\n\nFinally, we examine the semantic similarities of the labels from the two larger datasets (i.e., BrueLeg and ZuL).For this, we train separate BERT classifiers on each dataset as before but predict labels on the other dataset.The share of samples with a label X in the test dataset that are predicted with label Y from the training dataset indicates the degree of correspondence between both labels of different codebooks.Subsequently, both cross-classification results are superimposed to identify bidirectional correlations (i.e., relations between pairs of labels from different studies on which both classifiers agreed).This approach allows us to limit the influence of uncertainties of a single classifier and to avoid excessive matches of frequent labels.Thus, by multiplying these values from both directions, we obtain a confidence score per label relation (see Figure 2).Formally, we define the relation score between labels l i 2 D 1 and l j 2 D 2 from the datasets D 1 and D 2 (BrueLeg and ZuL in our experiments) as follows:",
            "score": 0.43080621167190564,
            "section_title": "Methodology",
            "char_start_offset": 20362,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 77
                },
                {
                    "start": 77,
                    "end": 220
                },
                {
                    "start": 220,
                    "end": 356
                },
                {
                    "start": 358,
                    "end": 545
                },
                {
                    "start": 545,
                    "end": 594
                },
                {
                    "start": 594,
                    "end": 667
                },
                {
                    "start": 667,
                    "end": 819
                },
                {
                    "start": 819,
                    "end": 898
                },
                {
                    "start": 898,
                    "end": 977
                },
                {
                    "start": 977,
                    "end": 1104
                },
                {
                    "start": 1106,
                    "end": 1219
                },
                {
                    "start": 1219,
                    "end": 1330
                },
                {
                    "start": 1330,
                    "end": 1526
                },
                {
                    "start": 1526,
                    "end": 1728
                },
                {
                    "start": 1728,
                    "end": 1865
                },
                {
                    "start": 1865,
                    "end": 1984
                },
                {
                    "start": 1984,
                    "end": 2140
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.350830078125
        },
        {
            "corpus_id": "270703323",
            "title": "Context-augmented Retrieval: A Novel Framework for Fast Information Retrieval based Response Generation using Large Language Model",
            "text": "This capability is particularly valuable in scenarios where obtaining a large labelled dataset is impractical or time-consuming.For instance, a zero-shot learning model can classify a piece of text that has never been explicitly trained on by using natural language prompts.For example, to classify movie reviews as positive or negative, a prompt like \"Classify the following review as positive or negative: [review text]\" can be used.The model uses its understanding of language and context to generate the correct classification based on the prompt.\n\nThe aforementioned solutions involve LLMs with a high number of parameters, which increases processing time.Therefore, we introduce a query classifier into the existing RAG workflow.\n\nTraditional machine learning models like Random Forest, and Decision Trees have many strengths and can perform various tasks effectively [27] [33] [25] [21] [18].They excel at handling structured data, such as numbers and categories, making them ideal for tasks like predicting house prices, detecting spam emails, and recommending products.These models can learn patterns from the data they are trained on, allowing them to make accurate predictions and decisions.However, they often face difficulties when dealing with natural language.We can improve their accuracy by integrating advanced NLP techniques [23].\n\nTraditional machine learning algorithms are also known for their interpretability and transparency.Models like Decision Trees provide clear insights into how decisions are made, making them valuable in domains where understanding the reasoning behind predictions is crucial, such as in medical diagnosis or financial risk assessment.Moreover, ensemble methods such as Random Forest combine multiple decision trees to improve robustness and generalization, reducing overfitting and enhancing performance on complex datasets with noisy or incomplete information.\n\nTransformer models like BERT have gained widespread popularity due to their ability to model bidirectional dependencies in language data effectively [11].By pre-training on large databases of text data using unsupervised learning objectives like masked language modelling and next-sentence prediction, BERT learns contextual representations of words and sentences that capture their meanings and relationships within the surrounding context.However, BERT's computational complexity and large memory footprint make it challenging to fine-tune specific data [36].\n\nTo address this issue, we used a distilled version of BERT, DistilBERT.",
            "score": 0.43073259472197867,
            "section_title": "Figure 1: RAG Workflow",
            "char_start_offset": 4188,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 128
                },
                {
                    "start": 128,
                    "end": 274
                },
                {
                    "start": 274,
                    "end": 435
                },
                {
                    "start": 435,
                    "end": 551
                },
                {
                    "start": 553,
                    "end": 661
                },
                {
                    "start": 661,
                    "end": 735
                },
                {
                    "start": 737,
                    "end": 899
                },
                {
                    "start": 899,
                    "end": 1078
                },
                {
                    "start": 1078,
                    "end": 1202
                },
                {
                    "start": 1202,
                    "end": 1275
                },
                {
                    "start": 1275,
                    "end": 1349
                },
                {
                    "start": 1351,
                    "end": 1450
                },
                {
                    "start": 1450,
                    "end": 1684
                },
                {
                    "start": 1684,
                    "end": 1911
                },
                {
                    "start": 1913,
                    "end": 2067
                },
                {
                    "start": 2067,
                    "end": 2354
                },
                {
                    "start": 2354,
                    "end": 2474
                },
                {
                    "start": 2476,
                    "end": 2547
                }
            ],
            "ref_mentions": [
                {
                    "start": 874,
                    "end": 878,
                    "matchedPaperCorpusId": "258458433"
                },
                {
                    "start": 884,
                    "end": 888,
                    "matchedPaperCorpusId": "7311285"
                },
                {
                    "start": 894,
                    "end": 898,
                    "matchedPaperCorpusId": "2427083"
                },
                {
                    "start": 1344,
                    "end": 1348,
                    "matchedPaperCorpusId": "220961531"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.1937255859375
        },
        {
            "corpus_id": "246996849",
            "title": "Evaluating the construct validity of text embeddings with application to survey questions",
            "text": "In this paper, we investigate whether current text embedding techniques like fast-Text, GloVe, BERT and USE can produce valid representations for survey questions. We focus on pretrained embedding models (as opposed to fine-tuned models) because of their widespread use. However, our approach to construct validity analysis also applies to fine-tuned text embeddings and other vector representations of texts. \n\nTable 1 lists the pretrained embedding models we adopt. For fastText, we use the pretrained model developed by [3]. It is trained on Common Crawl with 600B tokens, and produces word embeddings with 300 dimensions for 2M words. For GloVe, we use the model pretrained on Common Crawl with 840B tokens and a vocabulary of 2.2M words. It also outputs word vectors of 300 dimensions. As for pre-trained Sentence-BERT models, there are many to choose from, which differ not only in the specific natural language tasks that they have been optimised for, but also in their model architecture. We select two pretrained models which have been trained on various data sources (e.g. Reddit, Wikipedia, Yahoo Answers; over 1B pair of sentences) and are thus designed as general purpose models [2]. They are \"All-DistilRoBERTa\" and \"All-MPNet-base\", where \"DistilRoBERTa\" [44,45] and \"MPNet\" [46] are two different extensions of the original BERT. \"Base\" indicates that the embedding dimension is 768, as opposed to \"Large\" where the dimension is 1024. Both \"All-DistilRoBERTa\" and \"All-MPNet-base\" have been shown to have the top average performance across various language tasks. For the purpose of comparison, we also include two pretrained models of the original BERT model [4]: \"BERT-base-uncased\" and \"BERT-large-uncased\". \"Uncased\" refers to BERT treating upper and lower cases equally. Both models have been trained on Wikipedia (2.5B words) and BookCorpus (800M words). \n\nAs for USE, we use the most recent (i.e.",
            "score": 0.43029709685791956,
            "section_title": "Pretrained Text Embedding Models",
            "char_start_offset": 19928,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 163
                },
                {
                    "start": 164,
                    "end": 270
                },
                {
                    "start": 271,
                    "end": 409
                },
                {
                    "start": 412,
                    "end": 467
                },
                {
                    "start": 468,
                    "end": 527
                },
                {
                    "start": 528,
                    "end": 638
                },
                {
                    "start": 639,
                    "end": 742
                },
                {
                    "start": 743,
                    "end": 790
                },
                {
                    "start": 791,
                    "end": 996
                },
                {
                    "start": 997,
                    "end": 1082
                },
                {
                    "start": 1083,
                    "end": 1196
                },
                {
                    "start": 1197,
                    "end": 1345
                },
                {
                    "start": 1346,
                    "end": 1450
                },
                {
                    "start": 1451,
                    "end": 1579
                },
                {
                    "start": 1580,
                    "end": 1791
                },
                {
                    "start": 1792,
                    "end": 1876
                },
                {
                    "start": 1879,
                    "end": 1919
                }
            ],
            "ref_mentions": [
                {
                    "start": 523,
                    "end": 526,
                    "matchedPaperCorpusId": "207556454"
                },
                {
                    "start": 1192,
                    "end": 1195,
                    "matchedPaperCorpusId": "201646309"
                },
                {
                    "start": 1274,
                    "end": 1277,
                    "matchedPaperCorpusId": "203626972"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.55224609375
        },
        {
            "corpus_id": "260063010",
            "title": "Czech-ing the News: Article Trustworthiness Dataset for Czech",
            "text": "We experimented with 5 model architectures: RoBERTa-based RobeCzech (Liu et al., 2019;Straka et al., 2021), BERT-based Czert (Devlin et al., 2019;Sido et al., 2021), Electra-based Smalle-Czech (Clark et al., 2020;Koci\u00e1n et al., 2022), Term frequency-inverse document frequency (TF-IDF)-based Support Vector Machines (SVM) classifier (Sammut and Webb, 2010b;Hearst et al., 1998), and FastText Embedding-based Logistic Regression classifier (Joulin et al., 2017). Respective training details follow.",
            "score": 0.4302635903869728,
            "section_title": "Model architectures",
            "char_start_offset": 18725,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 461
                },
                {
                    "start": 462,
                    "end": 497
                }
            ],
            "ref_mentions": [
                {
                    "start": 86,
                    "end": 106,
                    "matchedPaperCorpusId": "235166607"
                },
                {
                    "start": 125,
                    "end": 146,
                    "matchedPaperCorpusId": "52967399"
                },
                {
                    "start": 193,
                    "end": 213,
                    "matchedPaperCorpusId": "208229926"
                },
                {
                    "start": 213,
                    "end": 233,
                    "matchedPaperCorpusId": "244896079"
                },
                {
                    "start": 357,
                    "end": 377,
                    "matchedPaperCorpusId": "661123"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.1439208984375
        },
        {
            "corpus_id": "247447385",
            "title": "A Novel Perspective to Look At Attention: Bi-level Attention-based Explainable Topic Modeling for News Classification",
            "text": "For the purpose of assessing classification performance, we first compare the effectiveness of our BATM base model relative to a number of attentionbased and pre-trained language models:\n\n\u2022 BERT (Devlin et al., 2019) composes of a bidirectional encoder of transformer and is pretrained by using a combination of masked language modeling objective and next sentence prediction on a large corpus; \u2022 Fastformer (Wu et al., 2021) uses additive attention to perform multi-head attention, which is more efficient than a standard transformer.\n\nThe initial weights of these pre-trained language models (BERT, DistilBERT, XLNet, Roberta, and Longformer) are provided by Hugging Face Transformer (Wolf et al., 2020) library 3 . We use a linear classifier to receive the pooled output from previous transformer layers and then fine-tune these models to adapt them to the classification task. For the attention-based model, Fastformer, we initialize its embedding matrix using Glove embedding and follow the hyper-parameter settings in (Wu et al., 2021).",
            "score": 0.4301955250266778,
            "section_title": "Baseline Models",
            "char_start_offset": 14702,
            "sentence_offsets": [],
            "ref_mentions": [
                {
                    "start": 195,
                    "end": 216,
                    "matchedPaperCorpusId": "52967399"
                },
                {
                    "start": 408,
                    "end": 425,
                    "matchedPaperCorpusId": "237266377"
                },
                {
                    "start": 686,
                    "end": 704,
                    "matchedPaperCorpusId": "269498086"
                },
                {
                    "start": 1024,
                    "end": 1041,
                    "matchedPaperCorpusId": "237266377"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.5927734375
        },
        {
            "corpus_id": "252734820",
            "title": "Token Classification for Disambiguating Medical Abbreviations",
            "text": "BERT is a pre-trained language model and stands for Bidirectional Encoder Representations from Transformers [8]. BERT's model architecture is based on the Transformer model proposed by Vaswani et al. [37], which is an attention mechanism that can be used to learn relationships between words and subwords. Similar to the BiLSTM, the bidirectional nature of this model allows learning the context of a word based on the words both from left and right. A notable challenge in pre-training is defining a prediction task. As the main training strategy, BERT employs a masked language model objective that randomly masks a proportion of tokens in the input. The task is to predict the actual masked word based on its context. Additionally, BERT employs a next sentence prediction task which requires the model to predict whether the second of two input sentences actually follows the first. In pre-training stage for these tasks, the BooksCorpus of 800M words and English Wikipedia of 2,500M words are employed. BERT has achieved state-of-the-art performance on several NLP tasks. Accordingly, five BERT variants are considered in our experiments. These include a popular lighter BERT variant (DistillBERT), three models that are pre-trained on different medical corpora (BioBERT, BlueBERT, MS-BERT) and another model that is pre-trained on scientific text with its custom domain-specific vocabulary (SciBERT). Below, we briefly summarize each of these BERT variants. \n\n\u2022 DistilBERT : This model is the faster, cheaper, and lighter version of the original BERT [38]. While full-size transformers-based models offer outstanding performance, their usage is limited by high computational complexity. DistilBERT is based on the same architecture as BERT but in a condensed and more efficient form. DistilBERT was shown to retain 97% of BERT's performance while being 60% faster [38]. \u2022 BioBERT : BioBERT is a BERT variant that is additionally pre-trained on large-scale biomedical corpora to excel in biomedical text-mining tasks. Specifically, it is pre-trained on PubMed abstracts that contain 4.5B words and PubMed Central full-text articles with 13.5B words.",
            "score": 0.4301603023583035,
            "section_title": "BERT",
            "char_start_offset": 15390,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 112
                },
                {
                    "start": 113,
                    "end": 305
                },
                {
                    "start": 306,
                    "end": 450
                },
                {
                    "start": 451,
                    "end": 517
                },
                {
                    "start": 518,
                    "end": 652
                },
                {
                    "start": 653,
                    "end": 720
                },
                {
                    "start": 721,
                    "end": 885
                },
                {
                    "start": 886,
                    "end": 1006
                },
                {
                    "start": 1007,
                    "end": 1075
                },
                {
                    "start": 1076,
                    "end": 1142
                },
                {
                    "start": 1143,
                    "end": 1405
                },
                {
                    "start": 1406,
                    "end": 1462
                },
                {
                    "start": 1465,
                    "end": 1561
                },
                {
                    "start": 1562,
                    "end": 1691
                },
                {
                    "start": 1692,
                    "end": 1788
                },
                {
                    "start": 1789,
                    "end": 1874
                },
                {
                    "start": 1875,
                    "end": 2021
                },
                {
                    "start": 2022,
                    "end": 2153
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.28369140625
        },
        {
            "corpus_id": "257717135",
            "title": "Bat4RCT: A suite of benchmark data and baseline methods for text classification of randomized controlled trials",
            "text": "Conventional ML classifiers, CNN, and BERT-based models have been employed to classify MEDLINE data (title, abstract, combined title and abstract) for RCTs. In this work, we used three BERT-based models (BERT, BioBERT, and SciBERT) to achieve a maximum F1 score of 90.85% in predicting instances of RCT publication correctly. Our results show that the performance of BERT-based models is superior to those of traditional classifiers and the CNN model. The performance gains of BERT-based models were substantial in recall and F1. Such performance differences might be due to the fact that the BERT-based models used language models already pre-trained on large amounts of data. For example, the BERT version used for our study was trained on the corpus that includes more than 800M words. The language models trained on the massive text data were transferred to our study's modeling to account for a word's context in the text data we used. While previous methods of word-embedding would return the same vector for a word no matter how it is used, the BERT-based models return different vectors for the same word depending on the words surrounding it. This consideration of word context seems to enable the BERT-based classification models to find many true cases of RCT texts (which increased recall) that were missed by the conventional ML algorithms and CNN. \n\nIn addition to their superior performance, BERT-based models provided procedural benefits in the implementation of text classification tasks. In other words, there was no need for any feature extraction and selection before feeding into a classifier. This simplified the pre-processing steps used in traditional machine learning approaches. It is also worth noting that the time required to run SVM was excessive although it showed the best performance among the conventional machine learning models for all three types of text. For example, it took over 85 hours for training and testing SVM with abstract text only. On the contrary, it took only 15 hours to run the SciBERT model on the same data, which not only surpassed the results of SVM but also produced the best results among all six models in terms of F1 score. The total time to train and test all three BERT-based models with the three types of text was about 102 hours.",
            "score": 0.43008699285956986,
            "section_title": "Discussion",
            "char_start_offset": 25783,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 156
                },
                {
                    "start": 157,
                    "end": 325
                },
                {
                    "start": 326,
                    "end": 451
                },
                {
                    "start": 452,
                    "end": 529
                },
                {
                    "start": 530,
                    "end": 677
                },
                {
                    "start": 678,
                    "end": 788
                },
                {
                    "start": 789,
                    "end": 940
                },
                {
                    "start": 941,
                    "end": 1151
                },
                {
                    "start": 1152,
                    "end": 1361
                },
                {
                    "start": 1364,
                    "end": 1505
                },
                {
                    "start": 1506,
                    "end": 1614
                },
                {
                    "start": 1615,
                    "end": 1704
                },
                {
                    "start": 1705,
                    "end": 1892
                },
                {
                    "start": 1893,
                    "end": 1981
                },
                {
                    "start": 1982,
                    "end": 2185
                },
                {
                    "start": 2186,
                    "end": 2296
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.501953125
        },
        {
            "corpus_id": "231603368",
            "title": "On Informative Tweet Identification For Tracking Mass Events",
            "text": "The first one is the fact that BERT encoder uses a contextual representation in which it processes words in relation to all the other words in the sequence, rather than one by one separately, and the second reason is the fact that we ran several training iterations while adjusting weights, and using different optimization functions to minimise the training loss. \n\nWe also evaluated the proposed approach in the subsets of the original datasets. As mentioned before these subsets were created so we could also calculate features related to the user who posted the message. We noticed again that the handcrafted features alone did not produce satisfactory results. The Using the Fasttext, GloVe and BERT embeddings as input features to the conventional classifiers showed considerable improvements across all datasets, especially when using LOGISTIC REGRES-SION as base classifier, however this was not a pattern observed when using different classification methods. \n\nOur hybrid model BERT Hyb produced the best performance result for almost all the dataset with the exception of the CRISISLEXT6 SUBSET , however the difference is marginal. The best observed macro Fscore is shown when using the Bag-of-Words features model using RANDOM FOREST as base classifier (93.22), while our hybrid approach produced a score of 93.05. In the COVID SUBSET our model showed 84.64 macho F-score which is 2.3 percentage points improvement over the second best result (Bag-of-Words and LR = 82.35). Our model showed 76.68 and 76.54 macro F-score for the CRI-SISLEXT26 SUBSET and CRISISMMD SUBSET datasets respectively. These two datasets seem to be the two datasets where the performance of the models were lower than 80%. Further investigation and a more in depth analysis is required as there is still some room for improvements.",
            "score": 0.4299820781542247,
            "section_title": "RESULTS",
            "char_start_offset": 22854,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 364
                },
                {
                    "start": 367,
                    "end": 447
                },
                {
                    "start": 448,
                    "end": 574
                },
                {
                    "start": 575,
                    "end": 665
                },
                {
                    "start": 666,
                    "end": 967
                },
                {
                    "start": 970,
                    "end": 1142
                },
                {
                    "start": 1143,
                    "end": 1326
                },
                {
                    "start": 1327,
                    "end": 1485
                },
                {
                    "start": 1486,
                    "end": 1605
                },
                {
                    "start": 1606,
                    "end": 1709
                },
                {
                    "start": 1710,
                    "end": 1818
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.45556640625
        },
        {
            "corpus_id": "221535008",
            "title": "Is Everything Fine, Grandma? Acoustic and Linguistic Modeling for Robust Elderly Speech Emotion Recognition",
            "text": "Using the experimental setup defined in Section 3.3, we evaluated the performance of different combinations of linguistic features, as well as combinations of the models with hard-labelbased majority voting. The linguistic models were trained using SVMs with linear, sigmoid and radial basis function kernels. As can be seen in Table 1, each proposed linguistic approach performed significantly better than the baseline reported in the challenge paper for valence recognition [3]. Moreover, a combination of these models at the decision level, namely the Ensemble Model, contributed to the overall performance. On the other hand, as expected, we obtained poor performance using linguistic features for arousal classification. \n\nConsidering the best UAR performances of the baseline system for valence (56% for development and 49% for test) that uses the BERT model [21], and those of our FastText features [22] (4-fold CV UAR: 46.5%), we observed that using only the complex contextual/semantic word embeddings to represent the story may be insufficient when the amount of labeled data is relatively small. But enriching those features with the knowledge learned from some external resources (such as tonal dictionaries or sentiment analysis tools) that are trained on much larger data sets can greatly improve the performance. \n\nExcept high-level polarity features, we conducted experiments for all models using both English translations and German transcripts. For FastText and TF-IDF models, using only English text gave a slightly better performance than using bilingual text. Thus, for these features, we reported the models that employ feature extraction from English text. Regarding dictionary based features, although the original text was in German, English dictionary-based features performed better than German ones, while fusion of the two followed by feature selection yielded the best performance. Brute-forcing different feature combinations resulted in the following set of optimal dictionary-based features: 2 from SentiWordNet (maximum positive and sum of negative scores) and 3 from SentiWS (minimum, maximum and number of negative scores). This small set of dictionary-based features outperformed the state-of-theart BERT features on the development set by an absolute difference of 16.2%.",
            "score": 0.42943514038675007,
            "section_title": "Experiments with Linguistic Features",
            "char_start_offset": 16743,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 207
                },
                {
                    "start": 208,
                    "end": 309
                },
                {
                    "start": 310,
                    "end": 480
                },
                {
                    "start": 481,
                    "end": 610
                },
                {
                    "start": 611,
                    "end": 725
                },
                {
                    "start": 728,
                    "end": 1106
                },
                {
                    "start": 1107,
                    "end": 1327
                },
                {
                    "start": 1330,
                    "end": 1462
                },
                {
                    "start": 1463,
                    "end": 1580
                },
                {
                    "start": 1581,
                    "end": 1679
                },
                {
                    "start": 1680,
                    "end": 1911
                },
                {
                    "start": 1912,
                    "end": 2159
                },
                {
                    "start": 2160,
                    "end": 2309
                }
            ],
            "ref_mentions": [
                {
                    "start": 906,
                    "end": 910,
                    "matchedPaperCorpusId": "207556454"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.37353515625
        },
        {
            "corpus_id": "229705027",
            "title": "Service quality monitoring in confined spaces through mining Twitter data",
            "text": "However, in case of Macro-Average F-score values, it can be observed that BERT's performance is more than two times better than the second-best approach, the skip-grambased MLP. In other words, when it comes to aspects with fewer observations, it can be observed that baseline approaches are unable to detect tweets' associations with minor aspects of SQ. This can be justified by the fact that a small number of tweets means fewer www.josis.org occurrences of input features in word-embedding or topic models, which can lead to the low variability challenge mentioned before. Table 8 provides a more detailed comparison between our proposed method and skip-gram-based MLP classifier as the second-best approach in terms of F-score and AUC values. As Table 8 demonstrates, while skip-grambased MLP achieves considerable precision values in \"Comfort\" and \"Personnel\" aspects, i.e., 43% and 63% respectively, unlike BERT, it fails to keep the trade-off between the corresponding recall values. This shows that the model is not sensitive enough to different kinds of feature vectors that can show up. \n\nIn contrast to all baseline methods, BERT is pre-trained on a large corpus of data and can be fine-tuned for the context of public transport, thus, it can bring extra semantic features into the process of text classification, which increases the variability and reduces the bias in the classifier. Additionally, BERT's bidirectional training of a transformer can also provide a deeper understanding of the language context and improves the learning capacity. As Table 7 illustrates, BERT outperforms all baseline approaches in all other aspects of SQ significantly. This discussion can also be confirmed by the resulting AUC metric, where BERT can achieve 17% higher AUC values compared to the second-best approach, the skipgram-based MLP. Moreover, it can be seen that other baseline approaches have an AUC value around 0.5, which reflects their incapacity to effectively separate different classes. After BERT, skip-gram-based classifiers perform better than LDA based approaches. In particular, skip-gram-based MLP has a significantly better performance especially in minor aspects where it achieves an F-score of 55% in the \"Information\" aspect.",
            "score": 0.42941887673643975,
            "section_title": "Aspect",
            "char_start_offset": 54706,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 177
                },
                {
                    "start": 178,
                    "end": 355
                },
                {
                    "start": 356,
                    "end": 445
                },
                {
                    "start": 446,
                    "end": 576
                },
                {
                    "start": 577,
                    "end": 747
                },
                {
                    "start": 748,
                    "end": 991
                },
                {
                    "start": 992,
                    "end": 1097
                },
                {
                    "start": 1100,
                    "end": 1397
                },
                {
                    "start": 1398,
                    "end": 1558
                },
                {
                    "start": 1559,
                    "end": 1665
                },
                {
                    "start": 1666,
                    "end": 1839
                },
                {
                    "start": 1840,
                    "end": 2000
                },
                {
                    "start": 2001,
                    "end": 2082
                },
                {
                    "start": 2083,
                    "end": 2249
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.29150390625
        },
        {
            "corpus_id": "271126704",
            "title": "What distinguishes conspiracy from critical narratives? A computational analysis of oppositional discourse",
            "text": "Five-fold cross-validation was used to obtain performance estimates, in the same way as in Section 4.1.\n\nThe results, presented in the top half of Table 5, show the average and per-category span-F1 scores for both languages.For English, DeBERTaV3 outperforms RoBERTa, which outperforms BERT, while for Spanish RoBERTa outperforms BERT.The per-category performance correlates well with the number of spans annotated with a category, i.e. with the size of the training data for the corresponding token classifier.For example, the (less frequent) Facilitators and Objectives categories are harder to detect.Additionally, the Spanish BERT and RoBERTa models are better than their English counterparts for the Victims and Negative Effects categories which are more frequent in Spanish, while for most of the other categories the English models work better.For English, the most sophisticated DeBERTaV3 model achieves noticeable performance gains for most of the categories.\n\nAdditionally, we experimented with the SpanBERT model [Joshi et al., 2020] as the transformer backbone.SpanBERT is a model based on a pre-training method \"designed to better represent and predict spans of text\" [Joshi et al., 2020].\n\nThe SpanBERT model achieved an inferior performance of 0.31 average span-F1 on English texts (Spanish SpanBERT model does not exist).This is in line with the skill extraction experiment [Zhang et al., 2022], in which the SpanBERT model was not the best solution and it rarely outperformed BERT.\n\nFinally, we experimented using CRF [Lafferty et al., 2001], a probabilistic model that learns patterns of transition between token classes6 , as the final token classification layer.We experimented with two CRF-based approaches: one in which the CRF layers were optimized together with the transformer (end-to-end), and the other in which the CRF layers were optimized after training and freezing the transformer (fine-tuned).Both of the CRF-based approaches led to an inferior performance.",
            "score": 0.42941542257177157,
            "section_title": "Detection of Narrative Elements",
            "char_start_offset": 46138,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 103
                },
                {
                    "start": 105,
                    "end": 224
                },
                {
                    "start": 224,
                    "end": 335
                },
                {
                    "start": 335,
                    "end": 511
                },
                {
                    "start": 511,
                    "end": 604
                },
                {
                    "start": 604,
                    "end": 851
                },
                {
                    "start": 851,
                    "end": 968
                },
                {
                    "start": 970,
                    "end": 1073
                },
                {
                    "start": 1073,
                    "end": 1202
                },
                {
                    "start": 1204,
                    "end": 1337
                },
                {
                    "start": 1337,
                    "end": 1498
                },
                {
                    "start": 1500,
                    "end": 1682
                },
                {
                    "start": 1682,
                    "end": 1926
                },
                {
                    "start": 1926,
                    "end": 1990
                }
            ],
            "ref_mentions": [
                {
                    "start": 1390,
                    "end": 1410,
                    "matchedPaperCorpusId": "248405777"
                },
                {
                    "start": 1535,
                    "end": 1558,
                    "matchedPaperCorpusId": "219683473"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.06561279296875
        },
        {
            "corpus_id": "263799985",
            "title": "Emergency entity relationship extraction for water diversion project based on pre-trained model and multi-featured graph convolutional network",
            "text": "Pretrained language models aim to acquire extensive linguistic knowledge through self-supervised learning using text corpora [47]. Early iterations of pre-trained language models predominantly relied on n-gram and rule-based approaches. In the literature [48], the Global Vectors model was proposed, which accomplishes pre-training by incorporating both word co-occurrence statistics and global semantic information. In the literature [49], the Word Vector model was proposed, which achieves pre-training by representing words as continuous vectors through self-supervised learning. \n\nWith the advancement of deep learning, the literature [50] introduced the Transformer model, which employs self-attention mechanisms for sequence modeling. The Transformer model effectively captures long-range dependencies in text through its self-attention mechanism, leading to significant performance improvements in machine translation tasks. Subsequently, the literature [35] introduced the BERT model, building upon the foundation of Transformer. The BERT model employs a strategy of pre-training and fine-tuning, conducting extensive pre-training on large-scale unsupervised data and subsequently fine-tuning on downstream tasks. The groundbreaking innovation of the BERT model has yielded exceptional results across multiple natural language processing tasks, sparking widespread scholarly attention towards pre-trained language models.",
            "score": 0.4293966198850321,
            "section_title": "Pre-trained language models",
            "char_start_offset": 7016,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 130
                },
                {
                    "start": 131,
                    "end": 236
                },
                {
                    "start": 237,
                    "end": 416
                },
                {
                    "start": 417,
                    "end": 582
                },
                {
                    "start": 585,
                    "end": 740
                },
                {
                    "start": 741,
                    "end": 931
                },
                {
                    "start": 932,
                    "end": 1037
                },
                {
                    "start": 1038,
                    "end": 1221
                },
                {
                    "start": 1222,
                    "end": 1429
                }
            ],
            "ref_mentions": [
                {
                    "start": 125,
                    "end": 129,
                    "matchedPaperCorpusId": "227238757"
                },
                {
                    "start": 255,
                    "end": 259,
                    "matchedPaperCorpusId": "67789476"
                },
                {
                    "start": 435,
                    "end": 439,
                    "matchedPaperCorpusId": "232770534"
                },
                {
                    "start": 639,
                    "end": 643,
                    "matchedPaperCorpusId": "232076027"
                },
                {
                    "start": 961,
                    "end": 965,
                    "matchedPaperCorpusId": "245340702"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.13525390625
        },
        {
            "corpus_id": "261031193",
            "title": "A Comparative Study of Text Embedding Models for Semantic Text Similarity in Bug Reports",
            "text": "We generated embeddings for the bug reports using multiple methods, such as TF-IDF [24], BERT [17], Fasttext [18], Doc2Vec [19], and ADA (GPT3.5) [20]. To establish a baseline model, we specifically utilized TF-IDF embeddings. In numerous research studies, TF-IDF is widely adopted as a text representation technique, making it a well-established and commonly used reference point. By incorporating Scikitlearn's TF-IDF as the baseline, we were able to effectively compare the performance of other models against this established standard. This comparative analysis enabled us to evaluate the effectiveness of alternative approaches by contrasting their performance with the TF-IDF baseline. TF-IDF is expressed as: \n\nFor pre-processing, we employed default techniques for TF-IDF, FastText, ADA, and BERT embeddings, which include lower-casing, tokenization, and stop-word removal. We used regular expressions to refine the tokenization process further, defining the token pattern and facilitating the extraction of English and alphanumeric words. On the other hand, BERT embeddings did not require any specific pre-processing steps. However, for Doc2Vec embeddings, the \"simple preprocess\" method provided by the Gensim library ensured effective data pre-processing. \n\nC. Embedding models GPT3.5, BERT, Fasttext, and Doc2Vec models required loading pre-trained models for their respective embeddings. Specifically, for BERT, we utilized the \"all-mpnet-base-v2\" model, optimized for various use cases and trained on a large and diverse dataset comprising over 1 billion training pairs. For ADA, we used \"text-embedding-ada-002,\" a GPT3.5 large language model for text search, text similarity, and code search. For Fasttext, we employed the \"crawl-300d-2M-subword\" model, which consisted of 2 million word vectors trained with subword information on the Common Crawl dataset, encompassing 600 billion tokens.",
            "score": 0.4289626375539536,
            "section_title": "B. Preprocessing",
            "char_start_offset": 10259,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 145
                },
                {
                    "start": 146,
                    "end": 151
                },
                {
                    "start": 152,
                    "end": 226
                },
                {
                    "start": 227,
                    "end": 381
                },
                {
                    "start": 382,
                    "end": 539
                },
                {
                    "start": 540,
                    "end": 691
                },
                {
                    "start": 692,
                    "end": 715
                },
                {
                    "start": 718,
                    "end": 881
                },
                {
                    "start": 882,
                    "end": 1047
                },
                {
                    "start": 1048,
                    "end": 1133
                },
                {
                    "start": 1134,
                    "end": 1267
                },
                {
                    "start": 1270,
                    "end": 1401
                },
                {
                    "start": 1402,
                    "end": 1585
                },
                {
                    "start": 1586,
                    "end": 1709
                },
                {
                    "start": 1710,
                    "end": 1907
                }
            ],
            "ref_mentions": [
                {
                    "start": 83,
                    "end": 87,
                    "matchedPaperCorpusId": "218971783"
                },
                {
                    "start": 109,
                    "end": 113,
                    "matchedPaperCorpusId": "207556454"
                },
                {
                    "start": 123,
                    "end": 127,
                    "matchedPaperCorpusId": "18593743"
                },
                {
                    "start": 146,
                    "end": 150,
                    "matchedPaperCorpusId": "258564392"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.356689453125
        },
        {
            "corpus_id": "274149876",
            "title": "A Library Perspective on Supervised Text Processing in Digital Libraries: An Investigation in the Biomedical Domain",
            "text": "For our investigation, we compared traditional classification models to language models. We selected the following traditional models: 1) SVC (Support Vector Classifier): An SVC provides high accuracy in classification tasks, effectively handles complex decision boundaries and high-dimensional data, and, uses kernel functions to capture non-linear relationships in the data. 2) XGBoost (Extreme Gradient Boosting Classifier): XGBoost is known for its high performance and efficiency in handling large datasets and missing data. It utilizes regularization and gradient boosting, which sequentially adds models to correct errors of previous models, leading to improved accuracy. These traditional classification models require the transformation of texts into a vector representation. Therefore, we used two different strategies: tf-idf and sentence transformers. 1) tf-idf is a traditional vectorization technique based on the frequency of words in the document and across the corpus. It is a common method but may lack the ability to capture semantic similarities and context, especially in complex sentence structures. 2) Sentence transformers allow us to derive a semantic representation of a text by utilizing pre-trained transformer models like BERT to generate fixed-length vector representations of sentences [35]. They capture semantic meaning and context by considering the entire sentence rather than individual words, which is ideal for tasks where understanding the context and meaning of sentences is crucial. Thus, they may perform better than tf-idf, especially in capturing semantic similarities and nuances. We used the sBERT implementation with the all-MiniLM-L6-v2 model. \n\nLanguage Models do not require the transformation of texts into a vector representation a-priori, as they come with built-in encoders/decoders. For our investigation, we used three generic models: BERT [9] (bert-base-uncased), RoBERTa [31] (roberta-base), and XLNet [45] (xlnet-base-cased).",
            "score": 0.4288568134624531,
            "section_title": "Models and Vectorization",
            "char_start_offset": 8416,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 88
                },
                {
                    "start": 89,
                    "end": 376
                },
                {
                    "start": 377,
                    "end": 529
                },
                {
                    "start": 530,
                    "end": 678
                },
                {
                    "start": 679,
                    "end": 784
                },
                {
                    "start": 785,
                    "end": 863
                },
                {
                    "start": 864,
                    "end": 985
                },
                {
                    "start": 986,
                    "end": 1121
                },
                {
                    "start": 1122,
                    "end": 1322
                },
                {
                    "start": 1323,
                    "end": 1523
                },
                {
                    "start": 1524,
                    "end": 1625
                },
                {
                    "start": 1626,
                    "end": 1691
                },
                {
                    "start": 1694,
                    "end": 1837
                },
                {
                    "start": 1838,
                    "end": 1984
                }
            ],
            "ref_mentions": [
                {
                    "start": 1317,
                    "end": 1321,
                    "matchedPaperCorpusId": "201646309"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.284423828125
        },
        {
            "corpus_id": "258832491",
            "title": "A Pretrainer\u2019s Guide to Training Data: Measuring the Effects of Data Age, Domain Coverage, Quality, & Toxicity",
            "text": "Section Findings \n\n\u2022 Quality and toxicity filters have very different effects. \n\n\u2022 Quality filters improve performance significantly, despite removing training data. \n\n\u2022 Quality filtering effects are not easily predicted by dataset characteristics. Future filters should weigh more than one dimension of quality. \n\n\u2022 Toxicity filtering trades off generalization and toxicity identification ability for reduced risk of toxic generation. \n\n\u2022 When optimizing for toxicity identification tasks, practitioners should use an inverse toxicity filter. \n\nMost modern large language models use some form of quality and/or toxicity filtering for their pretraining datasets (Table 1). To curb toxicity, T5 uses n-gram filters, Gopher and Chinchilla use SafeSearch filters, and LaMDA uses \"safety discriminators\". Quality heuristics are universally applied for web-scraped data, with newer models like LLaMA, the GPT-series and the PaLM-series all relying on quality classifiers. To compare and quantify the effects of these two filter types, we implement quality and toxicity filters at various thresholds, as described in Section 2.2, to vary the quantity of toxic and low-quality text present when pretraining models on the Pile and C4. \n\nQuality filters significantly improve performance across nearly all tasks, despite reducing training data quantity and variety. We see the quality filters improve nearly all downstream tasks: toxicity identification by 2% (Figure 5, right) and most QA task categories by 1-6% (Figure 6). Of most interest, these improvements are realized despite removing 10%+ of the training data, even though we find that removing data usually leads to a decrease in performance (Section 6). While the average performance peaks at T = 0.975 for the QA tasks, greater quality filtering still outperforms the unfiltered baseline on average. For the toxicity identification experiments, the performance is still improving after T = 0.7, where 55% of the dataset has been filtered out. Dataset quality characteristics are not strongly indicative of filtering effects. In Section 3, Books, Wikipedia, and Web data are classified as highest quality. Figure 6 shows that despite this, quality filtering provides the least benefit to QA tasks in these categories, even hurting the performance for Books.",
            "score": 0.4286615405974501,
            "section_title": "Impact of Quality & Toxicity Filters on Pretrained Models",
            "char_start_offset": 27210,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 16
                },
                {
                    "start": 19,
                    "end": 78
                },
                {
                    "start": 81,
                    "end": 165
                },
                {
                    "start": 168,
                    "end": 248
                },
                {
                    "start": 249,
                    "end": 312
                },
                {
                    "start": 315,
                    "end": 435
                },
                {
                    "start": 438,
                    "end": 543
                },
                {
                    "start": 546,
                    "end": 672
                },
                {
                    "start": 673,
                    "end": 800
                },
                {
                    "start": 801,
                    "end": 966
                },
                {
                    "start": 967,
                    "end": 1226
                },
                {
                    "start": 1229,
                    "end": 1356
                },
                {
                    "start": 1357,
                    "end": 1516
                },
                {
                    "start": 1517,
                    "end": 1705
                },
                {
                    "start": 1706,
                    "end": 1852
                },
                {
                    "start": 1853,
                    "end": 1995
                },
                {
                    "start": 1996,
                    "end": 2077
                },
                {
                    "start": 2078,
                    "end": 2157
                },
                {
                    "start": 2158,
                    "end": 2309
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.25244140625
        },
        {
            "corpus_id": "220496559",
            "title": "Multi-dialect Arabic BERT for Country-level Dialect Identification",
            "text": "In this model, we applied the approach proposed by [22]. In order to balance the training data, a data augmentation technique based on random shuffling was performed to enlarge and balance the training data. \n\nAfter that, for each sentence, a vector of size 21 that represents a language model probability for each country was extracted and concatenated to a word and character level TF-IDF vectors. An MNB classifier is then applied with the One-vs-the-rest strategy. \n\n\u2022 FastText Model FastText [15] was originally implemented to help obtain enhanced word representations over simpler methods such as Word2Vec [14]. In our experiments, we pool the FastText vectors of each token in a given sentence, to obtain a fixed-size dense representation of the sentence at hand. This is in turn fed into a multinomial logistic regression for classification [23,24]. \n\n\u2022 Aravec fully connected Model Aravec is an Arabic based Word2Vec model, trained and published by [25]. Similar to our FastText model above, we pool the Aravec vectors of the constituent tokens of a sentence to obtain its fixed-size vector representation. However, instead of a conventional ML algorithm, we feed these representations into a feed forward classifier, which is trained to obtain the final predictions [26]. As mentioned above, multiple approaches have been investigated in the experiments we conducted, starting with traditional ML techniques then moving to DL approaches, before finally settling on our winning BERT based model. For our traditional ML experiments, we tried various models such as SVM, Logistic Regression (LR) and Naive Bayes (NB), along with features such as TF-IDF. We also tried other ML models that performed well on previous similar tasks such as MADAR Mawdoo3-AI and MADAR Safina models. However, all of these models came short when compared to the BERT models as can be seen in Table 1, with the best Macro-Averaged F1-score achieved using traditional ML approaches being 17.06%. We then experimented with a number DL models, along with pre-trained word embedding features, such as FastText and Word2Vec.",
            "score": 0.42846112440790085,
            "section_title": "\u2022 MADAR-JUST Model",
            "char_start_offset": 9100,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 56
                },
                {
                    "start": 57,
                    "end": 207
                },
                {
                    "start": 210,
                    "end": 399
                },
                {
                    "start": 400,
                    "end": 468
                },
                {
                    "start": 471,
                    "end": 617
                },
                {
                    "start": 618,
                    "end": 770
                },
                {
                    "start": 771,
                    "end": 857
                },
                {
                    "start": 860,
                    "end": 963
                },
                {
                    "start": 964,
                    "end": 1115
                },
                {
                    "start": 1116,
                    "end": 1281
                },
                {
                    "start": 1282,
                    "end": 1504
                },
                {
                    "start": 1505,
                    "end": 1660
                },
                {
                    "start": 1661,
                    "end": 1786
                },
                {
                    "start": 1787,
                    "end": 1979
                },
                {
                    "start": 1980,
                    "end": 2104
                }
            ],
            "ref_mentions": [
                {
                    "start": 51,
                    "end": 55,
                    "matchedPaperCorpusId": "199379341"
                },
                {
                    "start": 497,
                    "end": 501,
                    "matchedPaperCorpusId": "207556454"
                },
                {
                    "start": 958,
                    "end": 962,
                    "matchedPaperCorpusId": "27334946"
                },
                {
                    "start": 1276,
                    "end": 1280,
                    "matchedPaperCorpusId": "52138659"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.60986328125
        },
        {
            "corpus_id": "260091356",
            "title": "Jina Embeddings: A Novel Set of High-Performance Sentence Embedding Models",
            "text": "The substantial size and inconsistent quality of many large datasets necessitates a rigorous filtering pipeline. We apply the following steps to filter training data: \n\nDe-Duplication: Duplicated entries within training data can negatively impact model performance [Hernandez et al., 2022], and potentially lead to overfitting. Consequently, we remove duplicate entries from our dataset. Considering the dataset's volume, we employ hash functions to identify and eliminate text pairs that map to duplicate hash values. We normalize whitespace and capitalization before checking for duplicates. Empty pairs and pairs with identical elements are also removed. \n\nLanguage Filtering: Since we design our embedding models for English, we use the fasttext-language-identification model 2 based on the fasttext text classification method [Joulin et al., 2017] to remove non-English training items from the dataset. \n\nConsistency Filtering: Consistency filtering means excluding training pairs with low semantic similarity. Previous studies suggest that eliminating low-similarity pairs using an auxiliary, albeit less precise, model boosts performance [Dai et al., 2023, Wang et al., 2022]. We employ the all-MiniLM-L6-v2 model 3 for consistency filter-2 fasttext-language-identification (https: //huggingface.co/facebook/fasttext-language-identification) 3 all-MiniLM-L6-v2 model (https://huggingface.co/ sentence-transformers/all-MiniLM-L6-v2) ing in this manner: We generate embeddings for 1M pairs (q i , p i ) i randomly sampled from D pairs . For every pair (q, p) \u2208 D pairs in the dataset, we verify whether p is among the top two passages most similar to q based on the cosine similarity of their embeddings compared to all passages p i , i = 1, ..., 1M. \n\nThe application of these preprocessing steps reduces the size of the dataset from over 1.5 billion mixed-quality pairs to 385 million high-quality pairs. This reduction permits us to train our model with significantly less data than typical embedding models without sacrificing embedding quality. 4",
            "score": 0.428459661835081,
            "section_title": "Pairwise Data Preparation",
            "char_start_offset": 3487,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 112
                },
                {
                    "start": 113,
                    "end": 166
                },
                {
                    "start": 169,
                    "end": 327
                },
                {
                    "start": 328,
                    "end": 387
                },
                {
                    "start": 388,
                    "end": 518
                },
                {
                    "start": 519,
                    "end": 593
                },
                {
                    "start": 594,
                    "end": 657
                },
                {
                    "start": 660,
                    "end": 907
                },
                {
                    "start": 910,
                    "end": 1015
                },
                {
                    "start": 1016,
                    "end": 1183
                },
                {
                    "start": 1184,
                    "end": 1541
                },
                {
                    "start": 1542,
                    "end": 1755
                },
                {
                    "start": 1758,
                    "end": 1911
                },
                {
                    "start": 1912,
                    "end": 2056
                }
            ],
            "ref_mentions": [
                {
                    "start": 831,
                    "end": 852,
                    "matchedPaperCorpusId": "1210515"
                },
                {
                    "start": 1145,
                    "end": 1162,
                    "matchedPaperCorpusId": "252519173"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.3017578125
        },
        {
            "corpus_id": "257267523",
            "title": "Using Machine Learning to Uncover the Semantics of Concepts: How Well Do Typicality Measures Extracted from a BERT Text Classifier Match Human Judgments of Genre Typicality?",
            "text": "We replicated the comparison of the four BERT-based typicality measures using word embeddings as a language representation instead of the BERT language representation. More precisely, we used a word-embedding layer with pre-trained weights in our classifiers instead of the BERT language representation. (See Appendix: Methodological Details for further details). We employed GloVe word embeddings (Pennington, Socher, and Manning 2014) to transform text documents into vectors. GloVe is a word-embedding model, not a text-embedding model. Accordingly, we needed to combine word positions in the embedding space to create a unique position for text documents (book descriptions from Goodreads.com). We used the average position of the words in the book description as the position of the book description in feature space. \n\nThe results obtained with the four GloVe-based typicality measures are reported in Tables 3 and 4. The overall performance of the GloVe-based typicality measures is very good, although not as high as that obtained with the BERT language representation. Because the BERT classifier is sensitive to bidirectional dependencies between words but the GloVe classifier is not, this unsurprisingly suggests that typicality judgments are also sensitive to such dependencies. \n\nComparison of the performance of the four GloVe-based typicality measures leads to the same conclusion as that obtained from comparing the four BERT-based typicality measures: what is crucial in achieving a good performance is that measure construction involves training a probabilistic classifier.",
            "score": 0.42837151563779186,
            "section_title": "Typicality Based on GloVe Embeddings",
            "char_start_offset": 56976,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 167
                },
                {
                    "start": 168,
                    "end": 303
                },
                {
                    "start": 304,
                    "end": 363
                },
                {
                    "start": 364,
                    "end": 478
                },
                {
                    "start": 479,
                    "end": 539
                },
                {
                    "start": 540,
                    "end": 698
                },
                {
                    "start": 699,
                    "end": 822
                },
                {
                    "start": 825,
                    "end": 1077
                },
                {
                    "start": 1078,
                    "end": 1291
                },
                {
                    "start": 1294,
                    "end": 1592
                }
            ],
            "ref_mentions": [
                {
                    "start": 398,
                    "end": 436,
                    "matchedPaperCorpusId": "1957433"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.2384033203125
        },
        {
            "corpus_id": "232092338",
            "title": "Towards Efficiently Diversifying Dialogue Generation Via Embedding Augmentation",
            "text": "The augmentation results along with two baselines, the original model and replacement method, on two datasets are shown in Table 1. It can be found that our method outperforms them under most conditions in both n-gram accuracy and diversity evaluation. Compared to the original models, our method (EA) can significantly promote the n-gram diversity of responses and slightly benefit n-gram accuracy under most conditions while remaining equivalent under other cases. In other words, EA encourages models to generate diverse texts without the cost of their quality. In contrast, the replacement method usually degrades the generation accuracy despite it is also beneficial for more various replies.\n\nWhen comparing between base models, EA seems to have more effect on transformer than Seq2seq. The reason is that more data is needed to train a robust transformer than RNN due to its depth, and our method pose a similar effect. It can also be observed that the PPL values of both EA and replacement can result in a noticeable PPL decrease for transformer models, but it is not such a case for Seq2seq models. It means transformers may not be well trained using current raw data.\n\nWe also evaluate the performance of Seq2seq / transformer-BERT to better prove the merits of using fastText as a semantic-neighbor word prediction model. The performance of them is shown in Table 2 in terms of accuracy metrics, Ent, Dist, and Sen(the average of Ent/Dist/Sen-n and n=1, 2, 3). EA and BERT variant show equivalent performance on both base models, because BERT embeddings without tuning on a specific domain can even perform worse than a universal embedding [26]. However, the huge BERT model will increase the computational load since the training time of our EA is approximately 1.98\u00d7 time of raw models, while it is 5.87\u00d7 for BERT variant and 1.37\u00d7 for replacement according to experiments. Obviously, it is better to select fastText rather than BERT for augmentation candidates prediction.    Table 3. Results of the ablation study on DailyDialog dataset.",
            "score": 0.42832993523825846,
            "section_title": "Main results",
            "char_start_offset": 11831,
            "sentence_offsets": [],
            "ref_mentions": [
                {
                    "start": 1651,
                    "end": 1655,
                    "matchedPaperCorpusId": "201646309"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.45458984375
        },
        {
            "corpus_id": "269156967",
            "title": "From Uncertainty to Trust: Kernel Dropout for AI-Powered Medical Predictions",
            "text": "To assess how strong comparative models work alongside our novel method, we selected various models, with three of them being specifically trained on medical data.All models underwent fine-tuning for 100 epochs using the Adam optimizer (epsilon=1e-8, learning rate=2e-5) and implemented early stopping based on validation loss.Bio Bert: Bio Bert (Lee et al., 2020) has undergone pre-training on an extensive collection of biomedical domain corpora, which includes PubMed abstracts and PMC full-text articles.Blue Bert: Blue Bert (Peng et al., 2019b) trained on preprocessed texts from PubMed.Clinical Bert: By (Wang et al., 2023b) initialized from BERT underwent training on a substantial multicenter dataset, featuring a large corpus containing 1.2 billion words encompassing diverse diseases.Bert-base-uncased: By (Devlin et al., 2018) consists of 110 million parameters, 12 heads, 768 dimensions, and 12 layers, each with 12 self-attention heads.Xlnet-basecased: (Yang et al., 2019) is a generalized autoregressive pre-training model trained on the English language.RoBERTa-base: Proposed by (Liu et al., 2019), is trained in the English language using the Masked Language Modeling (MLM) technique.AlBERT-base-v1: (Lan et al., 2019) introduced Albert, which shares its layers across its Transformer.It has the same number of parameters and layers as the Bert model.DistilBERT-base-uncased: (Sanh et al., 2019) introduced a smaller and faster version of Bert.It is trained on the same dataset as the Bert model.",
            "score": 0.42804396943136974,
            "section_title": "Language Models Comparison",
            "char_start_offset": 19638,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 163
                },
                {
                    "start": 163,
                    "end": 327
                },
                {
                    "start": 327,
                    "end": 508
                },
                {
                    "start": 508,
                    "end": 592
                },
                {
                    "start": 592,
                    "end": 794
                },
                {
                    "start": 794,
                    "end": 949
                },
                {
                    "start": 949,
                    "end": 1069
                },
                {
                    "start": 1069,
                    "end": 1201
                },
                {
                    "start": 1201,
                    "end": 1302
                },
                {
                    "start": 1302,
                    "end": 1368
                },
                {
                    "start": 1368,
                    "end": 1461
                },
                {
                    "start": 1461,
                    "end": 1513
                }
            ],
            "ref_mentions": [
                {
                    "start": 346,
                    "end": 364,
                    "matchedPaperCorpusId": "59291975"
                },
                {
                    "start": 610,
                    "end": 630,
                    "matchedPaperCorpusId": "261884154"
                },
                {
                    "start": 966,
                    "end": 984,
                    "matchedPaperCorpusId": "195069387"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.0709228515625
        },
        {
            "corpus_id": "218665566",
            "title": "Spelling Error Correction with Soft-Masked BERT",
            "text": "For comparison, we adopted the following methods as baselines. We report the results of the methods from their original papers. \n\nNTOU is a method of using an n-gram model and a rule-based classifier (Tseng et al., 2015). NCTU-NTUT is a method of utilizing word vectors and conditional random field (Tseng et al., 2015). HanSpeller++ is an unified framework employing a hidden Markov model to generate candidates and a filter to re-rank candidates (Zhang et al., 2015). Hybrid uses a BiLSTM-based model trained on a generated dataset (Wang et al., 2018b). \n\nConfusionset is a Seq2Seq model consisting of a pointer network and copy mechanism (Wang et al., 2019). FASPell adopts a Seq2Seq model for CSC employing BERT as a denoising autoencoder and a decoder (Hong et al., 2019). BERT-Pretrain is the method of using a pre-trained BERT. BERT-Finetune is the method of using a fine-tuned BERT.",
            "score": 0.42799032089145805,
            "section_title": "Baselines",
            "char_start_offset": 13282,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 62
                },
                {
                    "start": 63,
                    "end": 127
                },
                {
                    "start": 130,
                    "end": 221
                },
                {
                    "start": 222,
                    "end": 320
                },
                {
                    "start": 321,
                    "end": 469
                },
                {
                    "start": 470,
                    "end": 555
                },
                {
                    "start": 558,
                    "end": 661
                },
                {
                    "start": 662,
                    "end": 777
                },
                {
                    "start": 778,
                    "end": 834
                },
                {
                    "start": 835,
                    "end": 890
                }
            ],
            "ref_mentions": [
                {
                    "start": 200,
                    "end": 220,
                    "matchedPaperCorpusId": "18312025"
                },
                {
                    "start": 299,
                    "end": 319,
                    "matchedPaperCorpusId": "18312025"
                },
                {
                    "start": 448,
                    "end": 468,
                    "matchedPaperCorpusId": "2422075"
                },
                {
                    "start": 534,
                    "end": 554,
                    "matchedPaperCorpusId": "53083061"
                },
                {
                    "start": 641,
                    "end": 660,
                    "matchedPaperCorpusId": "196181315"
                },
                {
                    "start": 757,
                    "end": 776,
                    "matchedPaperCorpusId": "208001011"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.10430908203125
        },
        {
            "corpus_id": "221319787",
            "title": "Language Models and Word Sense Disambiguation: An Overview and Analysis",
            "text": "This dataset can be reliably used for quantitative and qualitative analyses in coarse-grained WSD, as we performed. \n\nFinally, WSD is clearly not a solved problem, even in the coarse-grained setting, due to a few challenges: (1) it is an arduous process to manually create high-quality full-coverage training data; therefore, future research should also focus on reliable ways of automising this process (Taghipour and Ng 2015;Delli Bovi et al. 2017;Scarlini, Pasini, and Navigli 2019;Pasini and Navigli 2019) and/or leveraging specific knowledge from lexical resources (Luo et al. 2018;Kumar et al. 2019;Huang et al. 2019); and ( 2  BERT-based model. Given that the task in WiC is a binary classification, the 1NN model is not applicable since a training to learn sense margins is necessary. Therefore, we experimented with the BERT model fine-tuned on WiC's training data. We followed Wang et al. (2019) and fused the two sentences and fed them as input to BERT. A classifier was then trained on the concatenation of the resulting BERT contextual embeddings. Results. Table 13 shows the result of BERT models and the other baselines on the WiC benchmark. 21 We can see that BERT significantly outperforms the FastText static word embedding. The two versions of BERT (Base and Large) perform equally well on this task, achieving results close to the state of the art. As with fine-grained all-words WSD, the additional knowledge drawn from WordNet proves to be beneficial, as shown by the results for KnowBERT and SenseBERT.",
            "score": 0.4276968219897832,
            "section_title": "Conclusions",
            "char_start_offset": 70438,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 115
                },
                {
                    "start": 118,
                    "end": 651
                },
                {
                    "start": 652,
                    "end": 792
                },
                {
                    "start": 793,
                    "end": 874
                },
                {
                    "start": 875,
                    "end": 964
                },
                {
                    "start": 965,
                    "end": 1060
                },
                {
                    "start": 1061,
                    "end": 1069
                },
                {
                    "start": 1070,
                    "end": 1159
                },
                {
                    "start": 1160,
                    "end": 1242
                },
                {
                    "start": 1243,
                    "end": 1368
                },
                {
                    "start": 1369,
                    "end": 1525
                }
            ],
            "ref_mentions": [
                {
                    "start": 404,
                    "end": 427,
                    "matchedPaperCorpusId": "6642557"
                },
                {
                    "start": 427,
                    "end": 450,
                    "matchedPaperCorpusId": "1873829"
                },
                {
                    "start": 450,
                    "end": 485,
                    "matchedPaperCorpusId": "211628120"
                },
                {
                    "start": 485,
                    "end": 509,
                    "matchedPaperCorpusId": "210040274"
                },
                {
                    "start": 570,
                    "end": 587,
                    "matchedPaperCorpusId": "29160079"
                },
                {
                    "start": 587,
                    "end": 605,
                    "matchedPaperCorpusId": "196171668"
                },
                {
                    "start": 605,
                    "end": 623,
                    "matchedPaperCorpusId": "201103745"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.5234375
        },
        {
            "corpus_id": "259667515",
            "title": "Detecting Level of Depression from Social Media Posts for the Low-resource Bengali Language",
            "text": "In the pre-processing stage, we have removed various unwanted and noisy elements. At first, 118 duplicated instances are discarded. Then, HTML tags, URLs, punctuation marks, special characters, and digits are removed from the dataset. After that, we eliminated all other languages except Bengali. Lastly, we have performed stop-words removal and stemming [13] operations to the entire dataset. Throughout the empirical experiment, we have observed that stop-word removal and stemming operations degrade the overall performance of the classification system. In addition, retaining stop-words and not performing stemming leads to more variety of features [14], [15]. For that reason, these two pre-processing tasks are not considered in the final experiment. At this stage, we have extracted features from the preprocessed data. Here, we do this task separately for the three types of ML models, classical ML, DL, and transformer-based pre-trained models. For classical ML models, we have utilized character, word, and combinations of character-word N-gram techniques with TF-IDF scoring. Through the experimental observation, we have fixed the value of N as 3 to 5 for the character N-gram and 1 to 2 for the word N-gram. In the case of DL models, we have observed through empirical analysis that the fastText word embedding approach gives better output than the word2vec and GloVe methods. The main reason is that the fastText employs the character N-gram technique to get subword level features, which handle the unknown word vocabulary of the dataset [16], [17]. Here, we have used the max_len (the maximum number of tokens in each text) is 132 and the vector_size (the size of the feature vector) is 100 to maintain the same input dimension. In the case of transformerbased models, they use their own embedding techniques, where the BERT uses the WordPiece 4 technique and the XLM-R utilizes the Sentence Piece Model (SPM) [18] method to extract token-based sub-word level features. The SPM combines two sub-word segmentation methods: the uni-gram language model [19] and byte-pair-encoding (BPE) [20].",
            "score": 0.42763838727129894,
            "section_title": "Data Pre-processing",
            "char_start_offset": 11949,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 81
                },
                {
                    "start": 82,
                    "end": 131
                },
                {
                    "start": 132,
                    "end": 234
                },
                {
                    "start": 235,
                    "end": 296
                },
                {
                    "start": 297,
                    "end": 393
                },
                {
                    "start": 394,
                    "end": 556
                },
                {
                    "start": 557,
                    "end": 664
                },
                {
                    "start": 665,
                    "end": 756
                },
                {
                    "start": 757,
                    "end": 826
                },
                {
                    "start": 827,
                    "end": 953
                },
                {
                    "start": 954,
                    "end": 1086
                },
                {
                    "start": 1087,
                    "end": 1220
                },
                {
                    "start": 1221,
                    "end": 1389
                },
                {
                    "start": 1390,
                    "end": 1564
                },
                {
                    "start": 1565,
                    "end": 1744
                },
                {
                    "start": 1745,
                    "end": 1985
                },
                {
                    "start": 1986,
                    "end": 2105
                }
            ],
            "ref_mentions": [
                {
                    "start": 355,
                    "end": 359,
                    "matchedPaperCorpusId": "235079410"
                },
                {
                    "start": 653,
                    "end": 657,
                    "matchedPaperCorpusId": "234904792"
                },
                {
                    "start": 659,
                    "end": 663,
                    "matchedPaperCorpusId": "231598077"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.445556640625
        },
        {
            "corpus_id": "248006238",
            "title": "Mapping the Multilingual Margins: Intersectional Biases of Sentiment Analysis Systems in English, Spanish, and Arabic",
            "text": "We first show results on the Sem2018-T1 task, in order to verify the quality of the models we analyze for social bias (see Table 2).\n\nWe observe that the performance of pre-trained language models varies across languages and emotions. BERT+, mBERT, and RoBERTa performed best on the English tasks, compared to Spanish and Arabic. Additionally, BERT+ had better perfor-  mance than the multilingual models (e.g. mBERT and XLM-RoBERTa) across all languages and tasks, showing that language-specific models (e.g., BETO) can be superior to multilingual models. SVM-tfidf and fastText typically outperformed the multilingual models but were at-par or only slightly better than the language-specific models. This difference is likely due to the lack of fine-tuning performed on the transformer-based models. Our decision to not fine-tune does decrease performance on downstream tasks but is prudent given the risk of overfitting on a small training set and our interest in studying the social biases encoded in off-the-shelf pre-trained language models.",
            "score": 0.4275966674800211,
            "section_title": "Emotion Intensity Regression",
            "char_start_offset": 18400,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.406005859375
        },
        {
            "corpus_id": "260386212",
            "title": "A Full-fledged Commit Message Quality Checker Based on Machine Learning",
            "text": "Baselines. For comparison, the following baselines are used: (1) and (2) 5 , which are tools specifically for Task 2, and (3) to 5 These are the only tools we found that detect the mood of the verb of a CM. They both utilize the POS tagger provided by NLTK [1]. (7), which have proven to perform well for a variety of NLP tasks, for Task 2-5.\n\n(1) GitCommitBear (GCB) [9], (2) Bad Commit Message Blocker (BCMB) [12] (3) Support Vector Machines (SVM) trained with TF-IDF embeddings (4) Random Forests (RF) trained with TF-IDF embeddings (5) FastText, a word embedding and classification model that was on par with the performance of deep learning methods as of 2017 [39] (6) Feed-Forward Neural Network (NN) based on self-trained dense vector word embeddings (7) Convolutional Neural Network (CNN) with max pooling over time, similar to the architecture proposed by Collobert et al. [48], also based on self-trained dense vector word embeddings. Transformer-based models. We evaluate models based on BERT [30] that utilize the encoder part of the Transformer architecture [56]. BERT-based models have been pretrained on vast amounts of natural language text. Pretraining facilitates transfer-learning, allowing models to adapt to specific NLP tasks. BERT-based models have been preferred for classification of commit messages in research [32], [33], [51], [55].\n\nWe evaluate the original BERT [30] itself, which is pretrained on English Wikipedia and freely available books.\n\nAlong BERT, we evaluate 2 models that enhance the architecture, parameters and training data of BERT: RoBERTa [41], a robustly optimized version of BERT, and DeBERTa [35], featuring disentangled attention and an enhanced mask decoder, both trained with more data than BERT -DeBERTa also utilizing stories and data from Reddit, and RoBERTa news articles in addition to that.\n\nWe also evaluate DistilBERT",
            "score": 0.4275758184874832,
            "section_title": "A. Evaluation setting",
            "char_start_offset": 25750,
            "sentence_offsets": [],
            "ref_mentions": [
                {
                    "start": 665,
                    "end": 669,
                    "matchedPaperCorpusId": "1210515"
                },
                {
                    "start": 1004,
                    "end": 1008,
                    "matchedPaperCorpusId": "52967399"
                },
                {
                    "start": 1071,
                    "end": 1075,
                    "matchedPaperCorpusId": "13756489"
                },
                {
                    "start": 1343,
                    "end": 1347,
                    "matchedPaperCorpusId": "233690496"
                },
                {
                    "start": 1392,
                    "end": 1396,
                    "matchedPaperCorpusId": "52967399"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.282958984375
        },
        {
            "corpus_id": "233324253",
            "title": "Should we Stop Training More Monolingual Models, and Simply Use Machine Translation Instead?",
            "text": "In order to fairly select a representative pre-trained model for each considered Scandinavian language, we opt for the most popular native model according to Hugging Face. For each considered language, this corresponds to a BERT-Base model, hence each language is represented by a Language Model of identical architecture. The difference between these models is therefore mainly in the quantity and type of texts used during training, in addition to potential differences in training hyperparameters. \n\nWe compare these Scandinavian models against the English BERT-Base and BERT-Large models by Google. English BERT-Base is thus identical in architecture to the Scandinavian models, while BERT-Large is twice as deep and contains more than three times the amount of parameters as BERT-Base. Finally, we include XLM-R-Large, in order to compare with a model trained on significantly larger (and multilingual) training corpora.",
            "score": 0.42732062624259626,
            "section_title": "Models",
            "char_start_offset": 5220,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 171
                },
                {
                    "start": 172,
                    "end": 322
                },
                {
                    "start": 323,
                    "end": 500
                },
                {
                    "start": 503,
                    "end": 602
                },
                {
                    "start": 603,
                    "end": 790
                },
                {
                    "start": 791,
                    "end": 925
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.0726318359375
        },
        {
            "corpus_id": "274464991",
            "title": "Measuring Bias of Web-filtered Text Datasets and Bias Propagation Through Training",
            "text": "BERT initially achieves slightly lower accuracy but eventually reaches a marginally higher accuracy. The observation that BERT requires more training sequences is somewhat expected, as the autoregressive transformer has a loss associated with each subsequence, while BERT processes each sequence only once. \n\nFastText classifier: FastText [Jou+16] is an efficient text classification library designed to provide fast and scalable text classification tasks, particularly suitable for classification of largescale datasets. FastText relies on a simple shallow neural network architecture that enables rapid training and inference. Similar to BERT, FastText processes each sequence as a whole. \n\nWe plot FastText's performance as a function of the number of training sequences in Figure 9. The transformer-based classifier and BERT significantly outperform FastText, but are significantly slower, and require significantly more compute. \n\nMajority vote at test time: We classify a given sequence as a whole at test time throughout the paper. In this ablation study, we classify all subsequences of one test sequence, and then determine the final prediction as the majority vote. For instance, a sequence of length n tokens will yield n predictions. The final predicted class is the most frequent on of the individual predictions. Using majority voting reduces accuracy to 67.37%, which is a 7.39% decrease compared to the default whole sequence classification. \n\nAggregating sequences: Throughout the paper, we classify individual sequences. In this ablation study we combine the sequences of the same dataset together to form sequences of length 2048 tokens, aligning with the context length of our transformer model. This creates a uniform test set with sequences of equal length, were each sequence utilizes the entire attention span of the transformer. \n\nThe aggregation of sequences yields an impressive 95.18% classification accuracy, approximately 10% higher than the default sequence based testing with sequences of length 1800-2000 tokens as seen in Figure 8. This suggests that providing the classifier with multiple concatenated sequences simplifies the classification task, making it easier than classifying a single sequence of similar combined length. \n\nLinear probing: Linear probing refers to training a linear classifier on fixed pretrained representations.",
            "score": 0.4268795056642635,
            "section_title": "FineWeb-Edu",
            "char_start_offset": 45277,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 100
                },
                {
                    "start": 101,
                    "end": 306
                },
                {
                    "start": 309,
                    "end": 521
                },
                {
                    "start": 522,
                    "end": 628
                },
                {
                    "start": 629,
                    "end": 690
                },
                {
                    "start": 693,
                    "end": 786
                },
                {
                    "start": 787,
                    "end": 933
                },
                {
                    "start": 936,
                    "end": 1038
                },
                {
                    "start": 1039,
                    "end": 1175
                },
                {
                    "start": 1176,
                    "end": 1245
                },
                {
                    "start": 1246,
                    "end": 1326
                },
                {
                    "start": 1327,
                    "end": 1457
                },
                {
                    "start": 1460,
                    "end": 1538
                },
                {
                    "start": 1539,
                    "end": 1715
                },
                {
                    "start": 1716,
                    "end": 1853
                },
                {
                    "start": 1856,
                    "end": 2065
                },
                {
                    "start": 2066,
                    "end": 2262
                },
                {
                    "start": 2265,
                    "end": 2371
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.8291015625
        },
        {
            "corpus_id": "246652217",
            "title": "Universal Spam Detection using Transfer Learning of BERT Model",
            "text": "The selection of a pre-trained model is essential for the task. Data Preprocessing is considered an essential step for any natural language processing task. However, if using pretrained datasets, some rules need to be followed according to the model in which conditions were trained. BERT model can be used for applications like generate text embeddings, text classification, named entity recognition, and question answered. To classify spam messages, the BERT model is best for this task as it contains many versions. The base model suits the needs of this research, especially for spam detection. This version contains only 12 encoders with 110 Million parameters which are sufficient for our application. \n\nUnlike transformers, BERT uses only an encoder unit, and the decoder part will be discarded as the name suggests, as shown in Fig. 1. Each encoder consists of the same layers as transformers counterpart, namely Self-Attention and Feed Forward Neural Networks, as shown in Fig. 2. Hence, BERT is considered as a language-based model rather than a sequence-to-sequence-based model. Bidirectional means that the input sequence is processed from both directions so that the model can learn from both directions to predict the word in the context with better efficiency. This model was trained on Wikipedia's unlabelled text corpus (2.5 million words) and book corpus (800 million words). The word representations obtained from the intermediate layers through different weights after training will be helpful for our application in detecting the given sample is spam or ham. At the end of the model, the design the classifier performs better using adding some neural network layers. \n\nThe input sequence will directly feed into the tokenizer as there is no need for any preprocessing steps required for the BERT model. But with some preprocessing steps helped to reduce the sequence length selection which is considered as one of the hyperparameters of the model. Tokenizer will handle the input sequence and perform certain operations on the input data. These operations include tokenizing, contextual and positional encoding, padding, adding unique tokens like (CLS), (SEP), and (PAD), and finally converting the tokenized data into integer sequences.",
            "score": 0.42682292400807653,
            "section_title": "Data preprocessing and Model Selection Process",
            "char_start_offset": 16720,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 63
                },
                {
                    "start": 64,
                    "end": 156
                },
                {
                    "start": 157,
                    "end": 283
                },
                {
                    "start": 284,
                    "end": 424
                },
                {
                    "start": 425,
                    "end": 518
                },
                {
                    "start": 519,
                    "end": 598
                },
                {
                    "start": 599,
                    "end": 707
                },
                {
                    "start": 710,
                    "end": 843
                },
                {
                    "start": 844,
                    "end": 1089
                },
                {
                    "start": 1090,
                    "end": 1275
                },
                {
                    "start": 1276,
                    "end": 1393
                },
                {
                    "start": 1394,
                    "end": 1579
                },
                {
                    "start": 1580,
                    "end": 1687
                },
                {
                    "start": 1690,
                    "end": 1823
                },
                {
                    "start": 1824,
                    "end": 1968
                },
                {
                    "start": 1969,
                    "end": 2059
                },
                {
                    "start": 2060,
                    "end": 2258
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.1572265625
        },
        {
            "corpus_id": "273798700",
            "title": "Mastering the Craft of Data Synthesis for CodeLLMs",
            "text": "Several studies suggest using trainable small models for data filtering, moving beyond rule-based or interpreter-driven methods. Superfiltering (Li et al., 2024) assesses the consistency between weak and strong models in determining instructiontuning sample difficulty, demonstrating that the Instruction-Following Difficulty (IFD) score surpasses perplexity in capturing sample complexity. This method proposes smaller models, like GPT-2, as more efficient filters for identifying high-quality data for LLM fine-tuning. Similarly, Cao et al. (2024b) leverage natural language indicators to predict inference loss, offering a more efficient evaluation of data than fine-tuning LLMs. For code filtering, Zhou et al. (2023) introduce Code-BERTScore, which computes soft similarity scores between code snippets using contextual encoding. Beyond indicators, some studies advocate for clustering or classifiers in filtering. Chen et al. (2023b); Yu et al. (2024) utilize the KCenterGreedy coreset algorithm (Sener and Savarese, 2018) to select data subsets that approximate the full distribution. Dubey et al. (2024) further implement model-based classifiers, using fasttext (Joulin et al., 2017) and resource-heavy Roberta-based models (Liu et al., 2019), to identify high-quality tokens.",
            "score": 0.42663697700812053,
            "section_title": "Small Model-based Filtering",
            "char_start_offset": 23543,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 128
                },
                {
                    "start": 129,
                    "end": 390
                },
                {
                    "start": 391,
                    "end": 520
                },
                {
                    "start": 521,
                    "end": 682
                },
                {
                    "start": 683,
                    "end": 834
                },
                {
                    "start": 835,
                    "end": 919
                },
                {
                    "start": 920,
                    "end": 1091
                },
                {
                    "start": 1092,
                    "end": 1284
                }
            ],
            "ref_mentions": [
                {
                    "start": 144,
                    "end": 161,
                    "matchedPaperCorpusId": "267365346"
                },
                {
                    "start": 703,
                    "end": 721,
                    "matchedPaperCorpusId": "256827797"
                },
                {
                    "start": 920,
                    "end": 939,
                    "matchedPaperCorpusId": "258866038"
                },
                {
                    "start": 941,
                    "end": 957,
                    "matchedPaperCorpusId": "270258158"
                },
                {
                    "start": 1002,
                    "end": 1028,
                    "matchedPaperCorpusId": "3383786"
                },
                {
                    "start": 1170,
                    "end": 1191,
                    "matchedPaperCorpusId": "1210515"
                },
                {
                    "start": 1232,
                    "end": 1250,
                    "matchedPaperCorpusId": "258866038"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.1800537109375
        },
        {
            "corpus_id": "222005817",
            "title": "Identification of Cybersecurity Specific Content Using Different Language Models",
            "text": "In this paper we proposed an architecture for a system that could automatically classify and analyze the cyber threat related information to assist human operators. In order to implement the Natural Language Filter module of the proposed system, neural embedding Doc2Vec and pre-trained transformer-based language model BERT were trained with cybersecurity-specific text. \n\nThe evaluation results show that the BERT-based Natural Language Filter outperformed Doc2Vec-based Natural Language Filter by 28 points in F1 Score. \n\nFrom Table 7, it can be seen that a language model pre-trained with generic knowledge (Wikipedia and books) performs better when fine-tuned with few domain-specific data as compared to a language model that has been trained on a large amount of domain-specific data. Hence, we conclude that BERT would be the most suitable language model to implement the Natural Language Filter module for the proposed system. \n\nIn future research, we would like to explore the possibilities of retraining the BERT and Doc2Vec language models and determine the necessity of a Training Document Generator module for the proposed system. Also, using Named Entity Recognition methods, we are actively working on the implementation of an Analyzer module for the proposed system.",
            "score": 0.4264820093238153,
            "section_title": "Conclusion",
            "char_start_offset": 37434,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 164
                },
                {
                    "start": 165,
                    "end": 371
                },
                {
                    "start": 374,
                    "end": 522
                },
                {
                    "start": 525,
                    "end": 791
                },
                {
                    "start": 792,
                    "end": 935
                },
                {
                    "start": 938,
                    "end": 1144
                },
                {
                    "start": 1145,
                    "end": 1283
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.42333984375
        },
        {
            "corpus_id": "237485425",
            "title": "On the Validity of Pre-Trained Transformers for Natural Language Processing in the Software Engineering Domain",
            "text": "We conducted an experiment to evaluate the validity of the SE models seBERT and BERToverflow in comparison to the general domain BERT models. Since seBERT is based on BERT LARGE and BERT BASE , we use both of these models within our comparison. We follow the procedure outlined in Section 5. This means we first compare the vocabularies. Then, we proceed to look at the validity of the models without fine-tuning through their ability to infer the correct meaning of terms. Finally, we evaluated how the different models perform when fine-tuned with a limited amount of labeled data for a prediction task. \n\nAll results, as well as the required scripts and link to the data for pre-training seBERT are available as part of our replication kit. 10 Additionally, we prepared a playground, which can be used to fill in masked words using the models. 11  We restrict our comparison to BERT, as this is the most similar general domain model and, therefore, most suitable to explore the need for SE-specific models for SE domain tasks. Please note that we also exclude CodeBERT from this comparison, because CodeBERT is not natural language model, but rather a bi-modal \"natural language -programming language\" model, i.e., for working with source code. 12  Additionally, we use fastText for the comparison with respect to classification tasks, to understand if smaller NLP models may be sufficient.",
            "score": 0.4263499323558704,
            "section_title": "EXPERIMENTS",
            "char_start_offset": 35564,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 141
                },
                {
                    "start": 142,
                    "end": 244
                },
                {
                    "start": 245,
                    "end": 291
                },
                {
                    "start": 292,
                    "end": 337
                },
                {
                    "start": 338,
                    "end": 473
                },
                {
                    "start": 474,
                    "end": 605
                },
                {
                    "start": 608,
                    "end": 849
                },
                {
                    "start": 850,
                    "end": 1029
                },
                {
                    "start": 1030,
                    "end": 1250
                },
                {
                    "start": 1251,
                    "end": 1393
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.52978515625
        },
        {
            "corpus_id": "227231089",
            "title": "Go Simple and Pre-Train on Domain-Specific Corpora: On the Role of Training Data for Text Classification",
            "text": "Language models pre-trained on large amounts of text corpora form the foundation of today's NLP (Gururangan et al., 2020;Rogers et al., 2020). They have proved to provide state-of-the-art performance against most standard NLP benchmarks (Wang et al., 2019a;Wang et al., 2019b). However, these models require large computational resources that are not always available and have important environment implications (Strubell et al., 2019). Moreover, there is limited research in the applicability of pre-trained models in classification tasks with small amount of labelled data. Some related studies (Lee et al., 2020;Nguyen et al., 2020;Huang et al., 2019;Alsentzer et al., 2019) investigate whether it is helpful to tailor a pre-trained model to the domain while others (Sun et al., 2019;Chronopoulou et al., 2019;Radford et al., 2018) analyse methods for fine-tuning BERT to a given task. However, these studies perform evaluation on a limited range of datasets and classification models and do not consider scenarios with limited amounts of training data. \n\nIn particular, this paper aims to estimate the role of labeled and unlabeled data for supervised text classification. Our study is similar to Gururangan et al. (2020) where they investigate whether it is still helpful to tailor a pre-trained model to the domain of a target task. In this paper, however, we focus our evaluation on text classification and compare different types of classifiers on different domains (social media, news and reviews). Unlike other tasks such as natural language inference or question answering that may require a subtle understanding, feature-based linear models are still considered to be competitive in text classification (Kowsari et al., 2019). However, to the best of our knowledge there has not been an extensive comparison between such methods and newer pre-trained language models.",
            "score": 0.4260996981225103,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 142
                },
                {
                    "start": 143,
                    "end": 277
                },
                {
                    "start": 278,
                    "end": 436
                },
                {
                    "start": 437,
                    "end": 575
                },
                {
                    "start": 576,
                    "end": 888
                },
                {
                    "start": 889,
                    "end": 1056
                },
                {
                    "start": 1059,
                    "end": 1176
                },
                {
                    "start": 1177,
                    "end": 1338
                },
                {
                    "start": 1339,
                    "end": 1507
                },
                {
                    "start": 1508,
                    "end": 1738
                },
                {
                    "start": 1739,
                    "end": 1879
                }
            ],
            "ref_mentions": [
                {
                    "start": 96,
                    "end": 121,
                    "matchedPaperCorpusId": "216080466"
                },
                {
                    "start": 237,
                    "end": 257,
                    "matchedPaperCorpusId": "143424870"
                },
                {
                    "start": 257,
                    "end": 276,
                    "matchedPaperCorpusId": "5034059"
                },
                {
                    "start": 412,
                    "end": 435,
                    "matchedPaperCorpusId": "174802812"
                },
                {
                    "start": 597,
                    "end": 615,
                    "matchedPaperCorpusId": "59291975"
                },
                {
                    "start": 615,
                    "end": 635,
                    "matchedPaperCorpusId": "218719869"
                },
                {
                    "start": 654,
                    "end": 677,
                    "matchedPaperCorpusId": "102352093"
                },
                {
                    "start": 769,
                    "end": 787,
                    "matchedPaperCorpusId": "153312532"
                },
                {
                    "start": 787,
                    "end": 813,
                    "matchedPaperCorpusId": "67855637"
                },
                {
                    "start": 1201,
                    "end": 1225,
                    "matchedPaperCorpusId": "216080466"
                },
                {
                    "start": 1715,
                    "end": 1737,
                    "matchedPaperCorpusId": "119297355"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.37109375
        },
        {
            "corpus_id": "210932696",
            "title": "Discriminative Topic Mining via Category-Name Guided Text Embedding",
            "text": "pervised embeddings (Word2Vec, GloVe and fastText) do not really have notable differences as word representations to WeSTClass; (2) Despite its great effectiveness as a pre-trained deep language model for supervised tasks, BERT is not suitable for classification without sufficient training data, probably because BERT embedding has higher dimensionality (even the base model of BERT is 768-dimensional) which might require stronger supervision signals to tune; (3) CatE outperforms all unsupervised embeddings on NYT-Location and Yelp-Food Type and Yelp-Sentiment categories by a large margin,  Table 3: Quantitative evaluation on discriminative topic mining. and have marginal advantage on NYT-Topic. This is probably because different locations (e.g., \"Canada\" vs. \"The United States\"), food types (e.g., \"burgers\" vs. \"pizza\"), and sentiment polarities (e.g., \"good\" vs. \"bad\") can have highly similar local contexts, and are more difficult to be differentiated than themes. CatE explicitly regularizes the embedding space for the specific categories and becomes especially advantageous when the given category names are semantically similar. There have been very few previous efforts in the text classification literature that dedicate to learning discriminative word embeddings from word-level supervisions, and word embeddings are typically fine-tuned jointly with classification models [22,50,56] via document-level supervisions. However, our study shows that under label scarcity scenarios, using word-level supervision only can bring significant improvements to weakly-supervised models. Therefore, it might be beneficial for future weakly-supervised/semi-supervised studies to also consider leveraging word-level supervision to gain a performance boost.",
            "score": 0.4260192014279598,
            "section_title": "Weakly-Supervised Text Classification",
            "char_start_offset": 31851,
            "sentence_offsets": [],
            "ref_mentions": [
                {
                    "start": 1394,
                    "end": 1398,
                    "matchedPaperCorpusId": "9672033"
                },
                {
                    "start": 1398,
                    "end": 1401,
                    "matchedPaperCorpusId": "13701579"
                },
                {
                    "start": 1401,
                    "end": 1404,
                    "matchedPaperCorpusId": "6857205"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.3056640625
        },
        {
            "corpus_id": "257456908",
            "title": "Intelligent Recommendation for Departments Based on Medical Knowledge Graph",
            "text": "In order to verify the advantages of our model, we compared the effects of our model on department recommendation with those of several basic models on the test set. We compare BERT-BiLSTM-CRF, RoBERTa-BiLSTM-CRF, BERT-CRF, BERT [9], BiLSTM-CRF and MacNER [7]. BERT, released by Google, is the backbone of BERT-BiLSTM-CRF and BERT-CRF models. And RoBERTa and MacBERT were the Chinese-based full-mask language models released by the joint laboratory of Harbin Institute of Technology and iFLYTEK. The recommended results of the different models mentioned above are shown in TABLE 6. Our model achieves the best results with a 79.96% precision rate, 81.16% recall rate and 80.56% in F1-score. \n\nIt can be seen that the precision rate of BERT, RoBERTa and MacBERT with BiLSTM and CRF modules can achieve 78.79%, 78.07% and respectively, which are higher than the BiLSTM-CRF the pre-trained language model. In addition, the CRF layer is still very helpful for the final result. For BERT, the precision rate of BERT-CRF achieves 74.97%, which is 6.24% higher than only the BERT model. It can prove that BiLSTM and CRF can perform better on downstream tasks than single-language models [20], [21], [22].",
            "score": 0.42567834720574,
            "section_title": "1) EFFECTIVENESS OF MODEL",
            "char_start_offset": 32070,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 165
                },
                {
                    "start": 166,
                    "end": 260
                },
                {
                    "start": 261,
                    "end": 342
                },
                {
                    "start": 343,
                    "end": 495
                },
                {
                    "start": 496,
                    "end": 581
                },
                {
                    "start": 582,
                    "end": 690
                },
                {
                    "start": 693,
                    "end": 815
                },
                {
                    "start": 816,
                    "end": 902
                },
                {
                    "start": 903,
                    "end": 973
                },
                {
                    "start": 974,
                    "end": 1079
                },
                {
                    "start": 1080,
                    "end": 1197
                }
            ],
            "ref_mentions": [
                {
                    "start": 256,
                    "end": 259,
                    "matchedPaperCorpusId": "195068911"
                },
                {
                    "start": 1180,
                    "end": 1184,
                    "matchedPaperCorpusId": "19217401"
                },
                {
                    "start": 1186,
                    "end": 1190,
                    "matchedPaperCorpusId": "237520180"
                },
                {
                    "start": 1192,
                    "end": 1196,
                    "matchedPaperCorpusId": "210930043"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.152587890625
        },
        {
            "corpus_id": "249047549",
            "title": "Automated Requirement Dependency Analysis for Complex Technical Systems",
            "text": "Pre-training is designed to capture information by processing large amounts of textual data, such as Wikipedia texts and book corpuses (cf. section 1 in Figure 2). The pre-training task is divided into Masked Language Learning and Next Sentence Prediction. In Masked Language Learning, parts of sentences are randomly replaced by masked tokens and fed into the language model. The model's task is to classify the masked words based on probability to train word relations. Sentence prediction feeds a pair of sentences as input to the model, separated by segment embeddings. The model's task is to decide whether a sentence pair belongs together or not. The aim is to train the model to identify contextual related text data (Devlin et al., 2018). Within the research at hand a generic pre-trained BERT model was used provided open source by huggingface (huggingface, 2018). \n\nAfter pre-training, the model is fine-tuned on a labelled text dataset (cf. section 2 in Figure 2) in a supervised manner to learn the context specific task which is \"identify requirement dependencies\". Fine-tuning leaves trained hyper-parameters unchanged, but adjusts task-specific inputs and outputs in BERT end-to-end. By adding a classification layer on top of the transformer output, BERT is able to determine certain (dependency) classes (Devlin et al., 2018). Within this research, the requirement data was artificially enhanced using random selection and weighted by classes for data augmentation. This is required to compensate a highly unbalanced dataset with \"none\" as the predominantly class and few data on the other dependency types. Within the last step (cf. section 3 in Figure 2), the BERT model is exported and can be used in a working environment.",
            "score": 0.4255568735223053,
            "section_title": "Pre-Training and Context-Specific Fine-Tuning",
            "char_start_offset": 13097,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 139
                },
                {
                    "start": 140,
                    "end": 163
                },
                {
                    "start": 164,
                    "end": 256
                },
                {
                    "start": 257,
                    "end": 376
                },
                {
                    "start": 377,
                    "end": 471
                },
                {
                    "start": 472,
                    "end": 573
                },
                {
                    "start": 574,
                    "end": 652
                },
                {
                    "start": 653,
                    "end": 746
                },
                {
                    "start": 747,
                    "end": 873
                },
                {
                    "start": 876,
                    "end": 951
                },
                {
                    "start": 952,
                    "end": 1078
                },
                {
                    "start": 1079,
                    "end": 1198
                },
                {
                    "start": 1199,
                    "end": 1343
                },
                {
                    "start": 1344,
                    "end": 1482
                },
                {
                    "start": 1483,
                    "end": 1624
                },
                {
                    "start": 1625,
                    "end": 1650
                },
                {
                    "start": 1651,
                    "end": 1743
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.1878662109375
        },
        {
            "corpus_id": "263297581",
            "title": "A semantically enhanced text retrieval framework with abstractive summarization",
            "text": "In this section, we will present the results on some data sets of our proposed BERT ranking framework that integrates paragraph contexts, and compare them with the unsupervised baseline model and the supervised baseline model.Tables 1 and 2 show the results of BERT and all the comparative models on the Robust04 and GOV2 datasets in terms of MAP@1000, MAP@100, P@20, and NDCG@20, respectively.\"*\" means statistically improvements over BERT-Base (MS MARCO).\n\nIn Tables 1 and 2, the experimental results from both BERT ranking framework that integrates paragraph contexts and each comparison model can be found that the four indicators have been greatly improved, no matter whether it is the unsupervised baseline or the pretraining language model baseline.\n\nBERT-Base (MS MARCO) is the most advanced basic BERT model.It can be seen from Table 1 that the method proposed by us also has great advantages over this model.The P@20 and Map@1000 on Robust04 data set have been increased by 19.3% and 21.7%.The average length of documents in GOV2 data set is about three times that of Robust04 data set.The results of this framework model in this article on GOV2 data set are significantly higher than those of unsupervised model and BERT-Base (MS MARCO).It shows that the retrieval ability of the proposed method is obviously improved.\n\nIn order to further verify the effectiveness of the method proposed in this article, we compare the BERT ranking framework based on paragraph context proposed in this article with the advanced pseudo relevance feedback model and the BERT-based retrieval model.These models include:\n\nBERT-QE for expanding queries using the most relevant fragments and document relevance, and Co-BERT for scoring multiple documents simultaneously.Based on the principle of fairness, the above two advanced models also use the base scale model, but the parameter settings follow TA B L E 1 Effectiveness of SE-BERT relative to baselines and BERT-based re-rankers on Robust04.The comparisons are mainly relative to the best-performed BERT-based model that is fine-tuned on MS Marco, denoted as BERT-MaxP.The best result for each metric is shown in bold.",
            "score": 0.4255344052177589,
            "section_title": "RESULTS",
            "char_start_offset": 23076,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 226
                },
                {
                    "start": 226,
                    "end": 394
                },
                {
                    "start": 394,
                    "end": 457
                },
                {
                    "start": 459,
                    "end": 756
                },
                {
                    "start": 758,
                    "end": 817
                },
                {
                    "start": 817,
                    "end": 918
                },
                {
                    "start": 918,
                    "end": 1000
                },
                {
                    "start": 1000,
                    "end": 1096
                },
                {
                    "start": 1096,
                    "end": 1248
                },
                {
                    "start": 1248,
                    "end": 1329
                },
                {
                    "start": 1331,
                    "end": 1591
                },
                {
                    "start": 1591,
                    "end": 1612
                },
                {
                    "start": 1614,
                    "end": 1760
                },
                {
                    "start": 1760,
                    "end": 1987
                },
                {
                    "start": 1987,
                    "end": 2115
                },
                {
                    "start": 2115,
                    "end": 2164
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.1795654296875
        },
        {
            "corpus_id": "260366067",
            "title": "A Comprehensive Machine Learning Framework for Automated Book Genre Classifier",
            "text": "Thus, for each book text, the p closest graphs were picked, yielding a (nsamples \u00d7 p) matrix, which was then fed into a variety of classifiers, including SVM, Multi-Layer Perceptron, Random Forest, Adaboost, Gradient Boosting, and Gaussian Naive Bayes, Gaussian Process Classifier. However, pronouns are sometimes difficult to determine and resolve in the context of a book, which is a huge disadvantage. \n\nLi et al. [6] developed a text categorization model with the use of the Latent Dirichlet Allocation (LDA) approach. The information is initially obtained via twenty different news group data packets, which are a component that is widely utilised in classification algorithms. The authors decided to use three distinct varieties of 20 news group data packets for their research. After the data has been pre-processed, the LDA model is applied in order to determine the topic distribution of the news data used for the training collection. The distribution of topics is carried out with the assistance of a topic model in order to reduce the text dimension, which is too high, and to acquire features. After the training and modelling have been finished, the results of the anticipated categories are obtained with the use of a classifier called Softmax Regression. Yao et al. [7] demonstrated a fastText-based text classification model. This model included modules for data pre-processing, feature extraction, training, and assessment. Because the sample that was used in this research came from a Chinese source, the training samples that were used in the training phase of the method were all tokenized in advance. Once the word tokenization process is complete, the useless garbled letters, phrases, and stop-words are removed. This helps to improve the overall quality of the corpus. The input layer of the fast Text approach, in contrast to the typical bag-of-words model, takes into consideration the n-gram feature of the sentence in addition to the word forms for each word in the sentence as an additional feature to input. In addition to having elements that enable it to acquire word order information to a level, fastText also has features that enable it to generate a more accurate representation of a phrase.",
            "score": 0.4255150809605326,
            "section_title": "PRIOR WORK",
            "char_start_offset": 3569,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 281
                },
                {
                    "start": 282,
                    "end": 404
                },
                {
                    "start": 407,
                    "end": 522
                },
                {
                    "start": 523,
                    "end": 682
                },
                {
                    "start": 683,
                    "end": 784
                },
                {
                    "start": 785,
                    "end": 944
                },
                {
                    "start": 945,
                    "end": 1106
                },
                {
                    "start": 1107,
                    "end": 1270
                },
                {
                    "start": 1271,
                    "end": 1342
                },
                {
                    "start": 1343,
                    "end": 1441
                },
                {
                    "start": 1442,
                    "end": 1622
                },
                {
                    "start": 1623,
                    "end": 1736
                },
                {
                    "start": 1737,
                    "end": 1793
                },
                {
                    "start": 1794,
                    "end": 2038
                },
                {
                    "start": 2039,
                    "end": 2228
                }
            ],
            "ref_mentions": [
                {
                    "start": 417,
                    "end": 420,
                    "matchedPaperCorpusId": "4894808"
                },
                {
                    "start": 1282,
                    "end": 1285,
                    "matchedPaperCorpusId": "221719237"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.10723876953125
        },
        {
            "corpus_id": "264935716",
            "title": "An energy-based comparative analysis of common approaches to text classification in the Legal domain",
            "text": "For the study, we choose two families of models largely-used in the text classification task: LLM and SVM. From the first group (LLM), we selected three BERT-based models: BERT, LegalBERT and DistilBERT; from the second group (SVM), we chose two feature representations: the classic Bag-Of-Word (BoW) and an advanced representation enriched with linguistic and semantic features. In the experiments, where possible, we reproduced the same configurations reported in the original LexGLUE experimentation [5]. \n\nBERT-based models -BERT [7] is one of the most popular LLMs and it is based on the transformer architecture. It is available as a model pre-trained on a massive dataset of generalpurpose documents, thus representing a good generic language model. It has reported excellent results in the field of text analysis and NLP but, being based on a large deep neural network, requires a lot of resources to run. \n\nMoreover, when dealing with a specific domain, having a language model that builds language statistics from the terminology used in the particular domain could be helpful. Thus, some variants of BERT have been proposed in the literature where they have been re-trained on domain-specific documents. Since, we faced the legal domain, we included also LegalBERT [4] in the comparative analysis, a derived BERT model which has been pre-trained on legal corpora such as legislation, contracts, and court cases. \n\nFinally, since our analysis addresses energy consumption and this is closely related to the size of the model, we also included DistilBERT [16] in the evaluation, a scaled-down version of the original BERT model obtained by using distillation. \n\nSVM-based approaches -Support Vector Machines (SVMs) [6] are well-established Machine Learning models, that have been widely used also in text categorization for decades [13,19]. They work by identifying a small optimal subset of the training examples that best define the separation hyperplane. Furthermore, they involve the use of kernels that allow the identification of nonlinear hyperplanes of separations.",
            "score": 0.4253794880175293,
            "section_title": "3.2.Models",
            "char_start_offset": 8230,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 106
                },
                {
                    "start": 107,
                    "end": 379
                },
                {
                    "start": 380,
                    "end": 507
                },
                {
                    "start": 510,
                    "end": 618
                },
                {
                    "start": 619,
                    "end": 756
                },
                {
                    "start": 757,
                    "end": 913
                },
                {
                    "start": 916,
                    "end": 1087
                },
                {
                    "start": 1088,
                    "end": 1214
                },
                {
                    "start": 1215,
                    "end": 1422
                },
                {
                    "start": 1425,
                    "end": 1668
                },
                {
                    "start": 1671,
                    "end": 1849
                },
                {
                    "start": 1850,
                    "end": 1966
                },
                {
                    "start": 1967,
                    "end": 2082
                }
            ],
            "ref_mentions": [
                {
                    "start": 503,
                    "end": 506,
                    "matchedPaperCorpusId": "238259595"
                },
                {
                    "start": 534,
                    "end": 537,
                    "matchedPaperCorpusId": "52967399"
                },
                {
                    "start": 1724,
                    "end": 1727,
                    "matchedPaperCorpusId": "52874011"
                },
                {
                    "start": 1845,
                    "end": 1848,
                    "matchedPaperCorpusId": "3091"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.272216796875
        },
        {
            "corpus_id": "258309541",
            "title": "Pre-trained Embeddings for Entity Resolution: An Experimental Analysis",
            "text": "Figure 16 summarizes our experimental results on the three ER tasks. The horizontal axis in each diagram corresponds to the effectiveness measure of the respective task, while the vertical one corresponds to the normalized run-time, which is computed by dividing the overall run-time of a model with that of the fastest one (i.e., 1 is assigned to the fastest model). The space formed by these axes illustrates the trade-off between effectiveness and time efficiency, with the top performing model lying closer to (1,1), i.e., the lower right corner. Note that for each model, we have computed its average effectiveness and normalized time across all datasets in Table 2(a). We observe that we can distinguish the ER tasks into two groups based on the relative performance of the three model types. The first group includes the unsupervised tasks, i.e., Blocking and Unsupervised Matching, where the SentenceBERT models consistently outperform the other two types to a significant extent. The reason is that they are able to distinguish between matching and non-matching entities without fine-tuning their top attention layers. Among them, the differences are minor, on average. Typically, though, S-GTR-T5 excels in effectiveness, but is slower, while S-MiniLM offers a better balance, trading slightly lower effectiveness for slightly higher time efficiency. The second best type includes the static models, where GloVe clearly dominates FastText and  Word2Vec, which suffer from significantly higher initialization cost, while being slightly less effective. GloVe is actually the fastest model across all datasets in both ER tasks, thus being ideal for ER applications emphasizing time efficiency at the cost of lower effectiveness. Finally, all BERT-based models consistently underperform the other two types in all respects. Their poor effectiveness stems from the lack of fine-tuning, which causes them to assign low similarity scores to both matching and non-matching pairs alike. \n\nDifferent patterns are observed in Supervised Matching, where all BERT-based models excel in effectiveness. As expected, Distil-BERT sacrifices effectiveness for significantly higher run-time. Note that in this case, we exclusively consider the testing/prediction time per model, because the training time constitutes an one-off cost.",
            "score": 0.4251674485124159,
            "section_title": "Discussion & Conclusions",
            "char_start_offset": 60528,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 68
                },
                {
                    "start": 69,
                    "end": 367
                },
                {
                    "start": 368,
                    "end": 550
                },
                {
                    "start": 551,
                    "end": 674
                },
                {
                    "start": 675,
                    "end": 798
                },
                {
                    "start": 799,
                    "end": 988
                },
                {
                    "start": 989,
                    "end": 1127
                },
                {
                    "start": 1128,
                    "end": 1178
                },
                {
                    "start": 1179,
                    "end": 1360
                },
                {
                    "start": 1361,
                    "end": 1560
                },
                {
                    "start": 1561,
                    "end": 1735
                },
                {
                    "start": 1736,
                    "end": 1829
                },
                {
                    "start": 1830,
                    "end": 1987
                },
                {
                    "start": 1990,
                    "end": 2097
                },
                {
                    "start": 2098,
                    "end": 2182
                },
                {
                    "start": 2183,
                    "end": 2324
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.60009765625
        },
        {
            "corpus_id": "258187362",
            "title": "Exploring the Trade-Offs: Unified Large Language Models vs Local Fine-Tuned Models for Highly-Specific Radiology NLI Task",
            "text": "We utilized pretrained models, fine-tuned them on the training and validation sets, and then evaluated their performance on the test set for comparison. \n\nBERT: In 2018, Google released Bidirectional Encoder Representations from Transformers (BERT) [45], a collection of masked-language models. BERT stands out as the first unsupervised language representation model that is deeply bidirectional, meaning it is capable of analyzing language in both directions. This capability enables BERT to extract high-quality language features that can be fine-tuned on domain-specific data for improved performance. Notably, BERT's pre-training is conducted solely on plain text corpora. \n\nDistilBERT: DistilBERT is a compact and efficient Transformer model based on BERT. By distilling BERT, DistilBert possesses a smaller parameter count than bert-base-uncased by 40% and performs faster by 60%. DistilBERT achieves comparable performance to BERT on the General Language Understanding Evaluation (GLUE) language understanding benchmark, maintaining an accuracy rate of over 95%. \n\nRoBERTa: The RoBERTa model [6] was based on BERT and share same architecture. By adding dynamic masking, removing the next-sentence pretrain-ing objective and training with larger batches, RoBERTa is a better reimplementation of BERT with some modifications to the key hyperparameters. \n\nBioBERT: BioBERT (Bidirectional Encoder Representations from Transformers for Biomedical Text Mining) [46] is a domain-specific language representation model pre-trained on large-scale biomedical corpora PubMed abstracts (PubMed) and PubMed Central full-text articles (PMC). When compared to BERT and other pre-existing models, BioBERT demonstrates superior performance in various biomedical text mining tasks ClinicalBERT: ClinicalBERT [47], which is based on BERT, utilizes bidirectional transformers to analyze clinical notes from the Medical Information Mart for Intensive Care III (MIMIC-III) dataset. The model has been evaluated by medical professionals and has been found to establish high-quality relationships between medical concepts, as evidenced by their judgements.",
            "score": 0.42464579459999185,
            "section_title": "BERT-Based Fine-Tuned Models",
            "char_start_offset": 26699,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 152
                },
                {
                    "start": 155,
                    "end": 294
                },
                {
                    "start": 295,
                    "end": 460
                },
                {
                    "start": 461,
                    "end": 604
                },
                {
                    "start": 605,
                    "end": 676
                },
                {
                    "start": 679,
                    "end": 761
                },
                {
                    "start": 762,
                    "end": 886
                },
                {
                    "start": 887,
                    "end": 1069
                },
                {
                    "start": 1072,
                    "end": 1149
                },
                {
                    "start": 1150,
                    "end": 1357
                },
                {
                    "start": 1360,
                    "end": 1634
                },
                {
                    "start": 1635,
                    "end": 1966
                },
                {
                    "start": 1967,
                    "end": 2139
                }
            ],
            "ref_mentions": [
                {
                    "start": 1462,
                    "end": 1466,
                    "matchedPaperCorpusId": "59291975"
                },
                {
                    "start": 1797,
                    "end": 1801,
                    "matchedPaperCorpusId": "102352093"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.397705078125
        },
        {
            "corpus_id": "269525893",
            "title": "Analysis of the retraining strategies for multi-label text message classification in call/contact center systems",
            "text": "The paper also examines an approach to natural language representation based on a BERT-type model. Models of this type are trained in two stages. The first stage is pre-training, during which the model is trained on huge corpora of unlabeled text data. The second phase is fine-tuning, in which the BERT model is first initialized with pre-trained parameters, and then the parameters are adjusted to solve a specific problem, such as classifying text data in a specific subject area 13 . The original BERT models were dedicated to the English language. The corpora used in the phase were the BooksCorpus 18 and English Wikipedia. BERT-type models have become a popular tool used by researchers to classify text data due to their high efficiency. In 19 , the BERT model was used for multi-class classification of radiology reports. A hierarchical BERT model with an adaptive tuning strategy was proposed in 20 . In 21 , the use of a hybrid approach combining BERT with LSTM to binary classify news articles into fake or legitimate was analyzed. \n\nRecently, two BERT-type architectures have been proposed in the literature: the Polbert model and the HerBERT model. These models were dedicated to applications with the Polish language, which was a great advantage from the point of view of practical applications of the multi-label text message classification method developed by the authors. \n\nThe PolBERT model was developed in 2020 11,12 . It is available in two variants: cased and uncased, however, according to the author's recommendation, the cased versions were used in this paper. The cased version of the model was pre-trained on the following corpora: Polish subset of Open Subtitles, Polish subset of ParaCrawl, Polish Parliamentary Corpus, and Polish Wikipedia. \n\nThe HerBERT solution is a series of BERT models trained to analyze textual data in Polish 22 . HerBERT was pre-trained based on two collections. The first dataset consists of the National Corpus of Polish Language (NKJP) corpus, Wikipedia, and Free Readings. The second dataset additionally includes CCNet Head, CCNet Middle, and Open Subtitles texts. The HerBERT model uses Byte-Pair Encoding tokenization. In addition, BPEDropout was applied with a call rejection probability of 10%. HerBERT is a multilayer bidirectional transformer.",
            "score": 0.42458353778425484,
            "section_title": "Classifiers based on BERT-type models",
            "char_start_offset": 12066,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 98
                },
                {
                    "start": 99,
                    "end": 145
                },
                {
                    "start": 146,
                    "end": 252
                },
                {
                    "start": 253,
                    "end": 487
                },
                {
                    "start": 488,
                    "end": 552
                },
                {
                    "start": 553,
                    "end": 629
                },
                {
                    "start": 630,
                    "end": 745
                },
                {
                    "start": 746,
                    "end": 830
                },
                {
                    "start": 831,
                    "end": 910
                },
                {
                    "start": 911,
                    "end": 1043
                },
                {
                    "start": 1046,
                    "end": 1162
                },
                {
                    "start": 1163,
                    "end": 1389
                },
                {
                    "start": 1392,
                    "end": 1439
                },
                {
                    "start": 1440,
                    "end": 1586
                },
                {
                    "start": 1587,
                    "end": 1771
                },
                {
                    "start": 1774,
                    "end": 1868
                },
                {
                    "start": 1869,
                    "end": 1918
                },
                {
                    "start": 1919,
                    "end": 2032
                },
                {
                    "start": 2033,
                    "end": 2125
                },
                {
                    "start": 2126,
                    "end": 2181
                },
                {
                    "start": 2182,
                    "end": 2259
                },
                {
                    "start": 2260,
                    "end": 2310
                }
            ],
            "ref_mentions": [
                {
                    "start": 483,
                    "end": 485,
                    "matchedPaperCorpusId": "226096901"
                },
                {
                    "start": 749,
                    "end": 751,
                    "matchedPaperCorpusId": "254712379"
                },
                {
                    "start": 906,
                    "end": 908,
                    "matchedPaperCorpusId": "245055611"
                },
                {
                    "start": 914,
                    "end": 916,
                    "matchedPaperCorpusId": "247579720"
                },
                {
                    "start": 1864,
                    "end": 1866,
                    "matchedPaperCorpusId": "233365297"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.184326171875
        },
        {
            "corpus_id": "228980954",
            "title": "Towards Machine Learning Based Analysis of Quality of User Experience (QoUE)",
            "text": "However, despite these concerns, web crawling is gaining relevance and usage. This calls for more research on how to set up best practices to reach a tradeoff between its importance and potential threat. 4) Bidirectional Encoder Representations from Transformers (BERT): Recent works now employ Google's BERT for predicting experiences and usage in clinical dialogues [26], [27]. Google's BERT is a state-of the-art model released. It is commanding wide usage by researchers in the field of natural language processing. The open issues for researchers centre on the effective usage and adaptation to various scenarios. For instance, in a recent work, there was a reported challenge faced by their researchers on how to use the BERT [26]. They compared the performance of BERT to a rule-based system and found that BERT did not perform optimally as claimed by inventors. However, they quickly accepted that their findings may be due to inability to test all possible ways to use BERT [26]. It is therefore a research direction to see more BERT-based systems emerge in the coming future. Similarly, another research trained their BERT model and compared its performance with that of Google's pre-trained models [27]. The result showed that their model outperformed that of Google's BERT in predicting personal experience using medication-related tweets as a case study [27]. This paper presents an overview of machine learning particularly natural language processing tools for the analysis of users satisfaction measured by QoUE metrics. The results showed that the extracted sentiments using the web scraping technique corroborates the opinion of respondents in the questionnaire. This showed the potential and capability of machine learning in analysing and predicting large database of sentiments or reviews. This approach can be applied to industrial scenarios where quality control units can use QoUE to appraise the perception of users of their products and services. Finally, an improved logistic regression classifier was developed to test, train and classify the user experiences. Comparing the improved logistics regression classifier with standard logistic regression classifier shows that the training accuracy of the improved logistic regression gave 97.67% as against the standard logistic regression which gave accuracy of 86.01%.",
            "score": 0.42434388938161177,
            "section_title": "B. Open Research Issues",
            "char_start_offset": 20175,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 77
                },
                {
                    "start": 78,
                    "end": 203
                },
                {
                    "start": 204,
                    "end": 379
                },
                {
                    "start": 380,
                    "end": 431
                },
                {
                    "start": 432,
                    "end": 519
                },
                {
                    "start": 520,
                    "end": 618
                },
                {
                    "start": 619,
                    "end": 737
                },
                {
                    "start": 738,
                    "end": 869
                },
                {
                    "start": 870,
                    "end": 988
                },
                {
                    "start": 989,
                    "end": 1085
                },
                {
                    "start": 1086,
                    "end": 1214
                },
                {
                    "start": 1215,
                    "end": 1372
                },
                {
                    "start": 1373,
                    "end": 1536
                },
                {
                    "start": 1537,
                    "end": 1680
                },
                {
                    "start": 1681,
                    "end": 1810
                },
                {
                    "start": 1811,
                    "end": 1972
                },
                {
                    "start": 1973,
                    "end": 2088
                },
                {
                    "start": 2089,
                    "end": 2344
                }
            ],
            "ref_mentions": [
                {
                    "start": 368,
                    "end": 372,
                    "matchedPaperCorpusId": "202699998"
                },
                {
                    "start": 374,
                    "end": 378,
                    "matchedPaperCorpusId": "204229606"
                },
                {
                    "start": 732,
                    "end": 736,
                    "matchedPaperCorpusId": "202699998"
                },
                {
                    "start": 983,
                    "end": 987,
                    "matchedPaperCorpusId": "202699998"
                },
                {
                    "start": 1209,
                    "end": 1213,
                    "matchedPaperCorpusId": "204229606"
                },
                {
                    "start": 1367,
                    "end": 1371,
                    "matchedPaperCorpusId": "204229606"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.09844970703125
        },
        {
            "corpus_id": "262826017",
            "title": "When Automated Assessment Meets Automated Content Generation: Examining Text Quality in the Era of GPTs",
            "text": "Text scoring and sequence classification has seen significant advancement with the emergence of transformer-based pre-trained language models (PLMs) such as BERT, RoBERTa, and GPT. These models have revolutionized various NLP tasks and have shown promise in enhancing the accuracy and efficiency of automated assessment of text quality and traits. BERT (Bidirectional Encoder Representations from Transformers), a pretrained language model, has been widely adopted in various NLP applications due to its ability to capture contextual information from both left and right contexts in a sentence [29,68]. It has achieved state-of-the-art results in various NLP tasks [42]. BERT has been applied to AES [91], analysis of language competencies [74], automated short-answer assessment [82], and inference of traits from textual data [? ], providing state-of-the-art performance in many cases. BERT-based assessment models excel in handling complex literary devices and where a nuanced understanding of the topic is beneficial [68]. RoBERTa (A Robustly Optimized BERT Pretraining Approach) [50] builds upon the architecture of BERT by further optimizing the pretraining process via greater hyperparameter tuning, inclusion of additional training data, and more parameters [50]. For instance, BERT uses 3.3 million tokens from the BookCorpus and Wikipedia. RoBERTa also uses BookCorpus and Wikipedia, but in addition to common crawl news (CC-News), open web text, and stories, resulting in over 30 million tokens used for pre-training [50]. Both BERT and RoBERTa have performed well on text quality assessment and trait scoring tasks due to their ability to capture rich contextual information and linguistic nuances present in user-generated text [32,38,97]. \n\nThe key research gap we tackle is as follows. With advancements in generative capabilities due to LLMs, in the context involving automated scoring of text using ML, it is unclear how various types of ML assessment methods designed for and trained on human content might assess GPT-based text; and how commonalities/differences between assessment models and those used to generate text might factor into the human versus GPT scoring dynamics.",
            "score": 0.42429498462991055,
            "section_title": "Transformer-based Pre-trained Language Models (PLMs)",
            "char_start_offset": 19202,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 180
                },
                {
                    "start": 181,
                    "end": 347
                },
                {
                    "start": 348,
                    "end": 602
                },
                {
                    "start": 603,
                    "end": 670
                },
                {
                    "start": 671,
                    "end": 887
                },
                {
                    "start": 888,
                    "end": 1026
                },
                {
                    "start": 1027,
                    "end": 1271
                },
                {
                    "start": 1272,
                    "end": 1349
                },
                {
                    "start": 1350,
                    "end": 1533
                },
                {
                    "start": 1534,
                    "end": 1752
                },
                {
                    "start": 1755,
                    "end": 1800
                },
                {
                    "start": 1801,
                    "end": 2196
                }
            ],
            "ref_mentions": [
                {
                    "start": 665,
                    "end": 669,
                    "matchedPaperCorpusId": "52967399"
                },
                {
                    "start": 700,
                    "end": 704,
                    "matchedPaperCorpusId": "227230267"
                },
                {
                    "start": 740,
                    "end": 744,
                    "matchedPaperCorpusId": "245125483"
                },
                {
                    "start": 780,
                    "end": 784,
                    "matchedPaperCorpusId": "195353092"
                },
                {
                    "start": 1741,
                    "end": 1745,
                    "matchedPaperCorpusId": "234223811"
                },
                {
                    "start": 1745,
                    "end": 1748,
                    "matchedPaperCorpusId": "253236872"
                },
                {
                    "start": 1748,
                    "end": 1751,
                    "matchedPaperCorpusId": "247830293"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.319091796875
        },
        {
            "corpus_id": "251237780",
            "title": "Clickbait Detection of Indonesian News Headlines using Fine-Tune Bidirectional Encoder Representations from Transformers (BERT)",
            "text": "M. Bilal and A.A. Almazroi [11] used a generalized approach to identify helpful and unhelpful reviews from Yelp shopping reviews by fine-tuning the BERT base model. The bag-of-words approaches with machine learning classifiers including KNN, Na\u00efve Bayes, and SVM were compared. The results demonstrated that BERT-based classifiers with finetuning outperform bag-of-words techniques. S. Gonzalez-Carvajal and E.C. Garrido-Merchan [12] used several scenarios with varying languages and dataset sources to evaluate BERT, and the conventional TF-IDF vocabulary fed to machine learning algorithms. The experiment results added empirical support for using BERT as the default technique in the NLP problem by demonstrating its superiority and independence from NLP problem aspects. \n\nThe BERT method is a state-of-the-art deep learning approach for Natural Language Understanding (NLU) problems based on transformers. This method proposed a faster architecture to train a language model that eliminates recurrences by using a multi-head attention layer. More rapid development, lower data requirements, and improved performance are advantages of employing BERT [13]. B. Willie et al. [14] introduced the first-ever comprehensive resource for training, evaluating, and benchmarking Indonesian NLU (IndoNLU) tasks. This resource includes twelve tasks, ranging from the classification of a single sentence to the labeling sequences of sentences with different complexity levels. This study also provides a collection of pre-trained Indonesian models called IndoBERT. These models were trained using Indo4B, a large, clean Indonesian dataset. This dataset was obtained from publicly accessible websites, social media posts, blogs, and news articles. \n\nThe aim of this paper use fine-tuning IndoBERT to identify clickbait and non-clickbait from the Indonesian news headlines dataset CLICK-ID [8]. The BERT tokenizer generated contextualized token embeddings. Thus, no handcrafted features are required.",
            "score": 0.4238856306856335,
            "section_title": "I. INTRODUCTION",
            "char_start_offset": 4317,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 164
                },
                {
                    "start": 165,
                    "end": 277
                },
                {
                    "start": 278,
                    "end": 382
                },
                {
                    "start": 383,
                    "end": 592
                },
                {
                    "start": 593,
                    "end": 774
                },
                {
                    "start": 777,
                    "end": 910
                },
                {
                    "start": 911,
                    "end": 1046
                },
                {
                    "start": 1047,
                    "end": 1159
                },
                {
                    "start": 1160,
                    "end": 1305
                },
                {
                    "start": 1306,
                    "end": 1468
                },
                {
                    "start": 1469,
                    "end": 1556
                },
                {
                    "start": 1557,
                    "end": 1631
                },
                {
                    "start": 1632,
                    "end": 1738
                },
                {
                    "start": 1741,
                    "end": 1884
                },
                {
                    "start": 1885,
                    "end": 1946
                },
                {
                    "start": 1947,
                    "end": 1990
                }
            ],
            "ref_mentions": [
                {
                    "start": 27,
                    "end": 31,
                    "matchedPaperCorpusId": "248428847"
                },
                {
                    "start": 1154,
                    "end": 1158,
                    "matchedPaperCorpusId": "52967399"
                },
                {
                    "start": 1177,
                    "end": 1181,
                    "matchedPaperCorpusId": "221640658"
                },
                {
                    "start": 1880,
                    "end": 1883,
                    "matchedPaperCorpusId": "221704627"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.270751953125
        },
        {
            "corpus_id": "227230342",
            "title": "Multilingual Epidemiological Text Classification: A Comparative Study",
            "text": "Deep learning transfer approaches such as BERT have demonstrated the ability to outperform the stateof-the-art methods on larger datasets. However, when there exist only a few labeled examples per class, 100 to 1, 000 as is the case of the low-resourced languages present in the dataset used in this study, the choice of the most appropriate approach is less clear, with classical machine learning and deep transfer learning presenting plausible options. Results of the experiments for different machine learning and deep learning models, using the dataset splits indicated in Table 2 are presented in Table 3 and discussed below.\n\nRegarding the machine learning methods, we notice, from the results in Table 3, that SVM outperforms by a small margin the LR and RF on precision and recall, while the RF has not only the highest precision (95.70%) among this category of models, but the highest compared to all the models analyzed. We observe that the machine learning models (LR, RF, SVM) are greatly imbalanced, registering the highest values in precision and the lowest in the recall. This can be detrimental to the interests of an epidemiological detection system. Compared with the baseline results provided by DANIEL, this specialized model had a higher recall than precision which proves the specialized nature of such a tool, although its recall is the lowest among all the methods compared.\n\nOn the other hand, the models based on either CNN or BiLSTM with FastText embeddings have lower F1 scores than the classical machine learning methods (LR, RF, SVM). This could be explained by the fact that the training data is insufficient to train the models to have the ability to better distinguish between relevant and irrelevant documents.\n\nIn the case of deep transfer learning models, one can notice a great difference in the F1 score performance of BERT-based models, compared to all the other models. We can also observe that BERT-based models manage to balance recall and precision (precision remains consistent despite the increase in recall). The models benefit from the pre-trained language models that are either used as features or finetuned on the task. BERT relies on Byte Pair Encoding (BPE) based WordPiece tokenization (Wu et al., 2016) which makes it more robust to handle out-of-vocabulary words.",
            "score": 0.42382675539966425,
            "section_title": "Results",
            "char_start_offset": 16242,
            "sentence_offsets": [],
            "ref_mentions": [
                {
                    "start": 2239,
                    "end": 2256,
                    "matchedPaperCorpusId": "3603249"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.576171875
        },
        {
            "corpus_id": "272449378",
            "title": "Automation of Book Categorisation Based on Network Centric Quality Management System",
            "text": "The Fasttext model performs better, with high speed and second only to the TextRNN-Att model. The loss values for the Fasttext model and the TextRNN-Att model has a loss value of 0.62 and 0.59 respectively, while the TextRNN model with embedded attention mechanism has the best result in all comparisons, which highlights the feature extraction advantage of the attention algorithm, and verifies the feasibility of studying the classification research with the pretrained model based on the attention mechanism. The LERT model retains the original architecture of the BERT, but optimises the training tasks and methods, which provides a good opportunity to study the BERT series of models. The LERT model retains the original BERT architecture but optimises the training tasks and methods, providing guidance for the study of the BERT series of models. The learning rate is set to 1e-5, the batch size is 8, and 10 rounds of training are performed, taking the maximum sequence length of the input text as 16,32,64,128,192,256, and 512 for the experiments, respectively. The experimental results corresponding to Fig. 8 are obtained. As can be seen in Fig. 7, the performance of the LERT model when dealing with different maximum sequence lengths, where the best performance is achieved when 256 is the sequence length. A sequence that is too short (e.g., 16) will lose more information because the important content is truncated, while setting it too long results in filling too many invalid 0-values, which affects feature extraction and computational efficiency. Therefore, setting 256 as the most suitable text length for the model to handle on the task of secondary subject classification of Chinese books is a significantly preferred solution. Fig. 9 shows the experimental results of the LERT model under different batch sizes. Fig. 9 shows that the performance of the model typically improves with increasing batch size, although the effect decreases after a certain point. The minimum batch is theoretically good for optimisation, but too low may cause unstable convergence and prolong training time. On the contrary, too large batches tend to trigger memory overruns and impair accuracy. Considering the experimental results and computational efficiency, 16 is chosen as the preferred batch size setting for the BERT pre-training model on the Chinese book secondary subject classification dataset.",
            "score": 0.4237403013929248,
            "section_title": "IV. ANALYSIS OF EXPERIMENTAL RESULTS",
            "char_start_offset": 28206,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 93
                },
                {
                    "start": 94,
                    "end": 511
                },
                {
                    "start": 512,
                    "end": 689
                },
                {
                    "start": 690,
                    "end": 852
                },
                {
                    "start": 853,
                    "end": 1069
                },
                {
                    "start": 1070,
                    "end": 1132
                },
                {
                    "start": 1133,
                    "end": 1318
                },
                {
                    "start": 1319,
                    "end": 1564
                },
                {
                    "start": 1565,
                    "end": 1748
                },
                {
                    "start": 1749,
                    "end": 1833
                },
                {
                    "start": 1834,
                    "end": 1980
                },
                {
                    "start": 1981,
                    "end": 2108
                },
                {
                    "start": 2109,
                    "end": 2196
                },
                {
                    "start": 2197,
                    "end": 2406
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.7724609375
        },
        {
            "corpus_id": "256567406",
            "title": "Combining the External Medical Knowledge Graph Embedding to Improve the Performance of Syndrome Differentiation Model",
            "text": "As seen in Table 2, based on pretraining models, BERT and KDSD models outperform fastText, TextCNN, Text RNN, Transformer, and other models. KDSD increases by 1%, 0.3%, and 0.52% in the metrics P@1, P@3, and P@5, respectively, as compared to the BERT model. From the experimental results, the results of the pretraining model are better than the previous text-based multilabel classifcation method. Furthermore, including KGE information in the pretraining model can improve the model's performance. However, all KG-Based indicators were terrible, far lower than the KDSD results. Tis might be due to one of the two factors. One is that KG-based method only used the information embedded in the graph and obtains the labels through link prediction. Another possibility is that TCMKG is a KG constructed for TCM diagnosis. Although the diagnosis of each medical record is diferent, the reuse rate of the information of the four diagnoses and the chief complaint is high. Besides, various clinicians defne diseases diferently, the number of triples for some conditions may be inadequate for good prediction. \n\nFrom Table 3, it can be seen that the average precision of the KDSD model is 0.9% higher than that of the BERT model. Tis is due to the fact that in actual text task processing, some training corpora are difcult to obtain; their overall number and the total number of words contained are very small, making them unsuitable for training models with embedding layers. However, these data are suitable for training models without embedding layers. In addition, it provides useful rules that may be extracted by the model. Using a pretrained model to encode the original text is an excellent choice in this instance because the pre-trained model is derived from a large corpus and can make the current text meaningful, despite the fact that these implications may not be domain-specifc. However, these defciencies can be remedied by utilizing fne-tuned models.",
            "score": 0.4235865059106477,
            "section_title": "Principal Results.",
            "char_start_offset": 21042,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 140
                },
                {
                    "start": 141,
                    "end": 257
                },
                {
                    "start": 258,
                    "end": 398
                },
                {
                    "start": 399,
                    "end": 499
                },
                {
                    "start": 500,
                    "end": 580
                },
                {
                    "start": 581,
                    "end": 624
                },
                {
                    "start": 625,
                    "end": 748
                },
                {
                    "start": 749,
                    "end": 821
                },
                {
                    "start": 822,
                    "end": 969
                },
                {
                    "start": 970,
                    "end": 1105
                },
                {
                    "start": 1108,
                    "end": 1225
                },
                {
                    "start": 1226,
                    "end": 1473
                },
                {
                    "start": 1474,
                    "end": 1552
                },
                {
                    "start": 1553,
                    "end": 1626
                },
                {
                    "start": 1627,
                    "end": 1890
                },
                {
                    "start": 1891,
                    "end": 1964
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.312255859375
        },
        {
            "corpus_id": "272359317",
            "title": "Attention-aware with stacked embedding for sentiment analysis of student feedback through deep learning techniques",
            "text": "In the proposed work, three important issues in the field of natural language processing: polysemy, contextual meaning, and out-of-vocabulary terms were addressed. We employed three cutting-edge models to solve these problems: FastText, Elmo, and RoBERTa. \n\nFastText is a text categorization and representation learning library that is intended to be quick and efficient. It handles out-of-vocabulary terms using sub-word information, making it appropriate for NLP jobs. Embeddings from Language Models (ELMO) is a contextualized word representation model that learns word contexts from large-scale text corpora. It captures the contextual meaning of words, making it ideally suited for NLP jobs requiring context awareness. A Robustly Optimized BERT Pre-training Approach (Roberta) is a transformer-based language model that has been refined using large-scale text corpora. \n\nIt outperforms BERT on a variety of NLP tasks, making it an effective model for NLP. The outputs of these three models were combined using ensemble stacking techniques and the results were passed through numerous levels of multi-head attention. This made it possible to identify the benefits of each model and use their problem-solving abilities to solve polysemy, contextual meaning, and vocabulary terms. We were able to generate predictions after passing the final output via classifier layers. In comparison to several baseline models and recently released models, our suggested model performed better. A comparison of our model to other models is presented in Table 4. In the proposed model, three classification techniques were used: Bi-LSTM, GRU, and Bi-GRU, as discussed in the 'Stacking: ensemble encoded features'. Bi-GRU classifier gives better results comparatively, therefore in the Table 4 only the Bi-GRU results are given in detail. The results for sentiments and topics were presented separately. Here, the average results of both categories (sentiments and topics) are given in Table 4 against the proposed model. The results demonstrate that, in terms of accuracy and other performance metrics, the proposed model performs better than existing models. \n\nFigure 8 helps to further elaborate these results. This figure shows the comparative analysis of the proposed model with other techniques. Here the top four results of Table 4 are compared with our model.",
            "score": 0.4235524663919619,
            "section_title": "Comparative analysis of the proposed model with existing techniques",
            "char_start_offset": 37979,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 163
                },
                {
                    "start": 164,
                    "end": 255
                },
                {
                    "start": 258,
                    "end": 371
                },
                {
                    "start": 372,
                    "end": 470
                },
                {
                    "start": 471,
                    "end": 612
                },
                {
                    "start": 613,
                    "end": 724
                },
                {
                    "start": 725,
                    "end": 874
                },
                {
                    "start": 877,
                    "end": 961
                },
                {
                    "start": 962,
                    "end": 1121
                },
                {
                    "start": 1122,
                    "end": 1283
                },
                {
                    "start": 1284,
                    "end": 1374
                },
                {
                    "start": 1375,
                    "end": 1483
                },
                {
                    "start": 1484,
                    "end": 1550
                },
                {
                    "start": 1551,
                    "end": 1701
                },
                {
                    "start": 1702,
                    "end": 1825
                },
                {
                    "start": 1826,
                    "end": 1890
                },
                {
                    "start": 1891,
                    "end": 2008
                },
                {
                    "start": 2009,
                    "end": 2147
                },
                {
                    "start": 2150,
                    "end": 2200
                },
                {
                    "start": 2201,
                    "end": 2288
                },
                {
                    "start": 2289,
                    "end": 2354
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.7294921875
        },
        {
            "corpus_id": "259129434",
            "title": "A Unified Generative Approach to Product Attribute-Value Identification",
            "text": "ing to median frequency and number of values. Tables 7 and 8 list micro and macro F 1 values of each approach for each category of attributes on the MAVE and our in-house datasets, respectively. From the table, we can see that T5 shows the best performance in all categories. This suggests that T5 is more robust than BERT-NER and BERT-MLC in the PAVI task. We can also observe that the performance of BERT-MLC drops significantly for attributes with a small number of training examples compared to those with a large number of training examples; the classification-based approach makes an effort to better classify more frequent attributes. Meanwhile, the performance drops of BERT-NER and T5 are more moderate than BERT-MLC, especially on the MAVE dataset. Moreover, we can see that T5 shows better micro F 1 for attributes that have a smaller number of distinct values on our in-house dataset, whereas it shows better micro F 1 for attributes that have a larger number of distinct values on the MAVE dataset. This implies that, although it is easy for the generation-based approaches to extract diverse values from text, it is still difficult to canonicalize those diverse values.",
            "score": 0.4232293843386924,
            "section_title": "Results",
            "char_start_offset": 19689,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 45
                },
                {
                    "start": 46,
                    "end": 194
                },
                {
                    "start": 195,
                    "end": 275
                },
                {
                    "start": 276,
                    "end": 357
                },
                {
                    "start": 358,
                    "end": 641
                },
                {
                    "start": 642,
                    "end": 758
                },
                {
                    "start": 759,
                    "end": 1011
                },
                {
                    "start": 1012,
                    "end": 1183
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.1658935546875
        },
        {
            "corpus_id": "226283782",
            "title": "Word associations and the distance properties of context-aware word embeddings",
            "text": "To perform the studies indicated above, we use two sets of artificial word embedding data, and one set of human association norms. \n\n3.1 Word embedding data BERT Devlin et al. (2018) propose BERT, a transformer-based model that uses an attention mechanism to extract the context of words and subwords from text. The innovation of BERT is the application of a bi-directional training to the Transformer, achieving a better use of context from text than systems with unidirectional training. BERT is pretrained on the BookCorpus (800M words) and English Wikipedia (2500M words). The Book-Corpus (Zhu et al., 2015) is a collection of 11,038 books available on the Web, from 16 different genres, taking into account only books with more than 20K words to avoid noise coming from shorter stories. In all experiments below, we use the Huggin-face version of BERT1 , specifically the \"bert-baseuncased\" model that we expressly do not modify. \n\nFastText The original FASTTEXT model is based on Wikipedia dumps2 in nine differerent languages including English (Bojanowski et al., 2017). However, in this work, we used the pre-trained FAST-TEXT embeddings provided by the official site of FASTTEXT, that we expressly do not modify. 3 The embeddings are trained on 600-billion tokens from CommonCrawl4 , resulting in two-million word vectors with subword information. et al. (2004) propose a large dataset of free association, rhyme and word fragment norms, elicited from more than 6000 participants. The participants were asked to write the first word that came to mind when presented a particular stimulus word. More than 750'000 free associations (called targets) from a total of 5019 stimulus words (called cues) were collected. The related quantitative information (such as number of participants and measures of association strength) were calculated. 5",
            "score": 0.4228349365420476,
            "section_title": "Data",
            "char_start_offset": 4788,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 130
                },
                {
                    "start": 133,
                    "end": 311
                },
                {
                    "start": 312,
                    "end": 489
                },
                {
                    "start": 490,
                    "end": 576
                },
                {
                    "start": 577,
                    "end": 791
                },
                {
                    "start": 792,
                    "end": 934
                },
                {
                    "start": 937,
                    "end": 1077
                },
                {
                    "start": 1078,
                    "end": 1223
                },
                {
                    "start": 1224,
                    "end": 1356
                },
                {
                    "start": 1357,
                    "end": 1489
                },
                {
                    "start": 1490,
                    "end": 1602
                },
                {
                    "start": 1603,
                    "end": 1721
                },
                {
                    "start": 1722,
                    "end": 1847
                }
            ],
            "ref_mentions": [
                {
                    "start": 593,
                    "end": 611,
                    "matchedPaperCorpusId": "6866988"
                },
                {
                    "start": 1051,
                    "end": 1076,
                    "matchedPaperCorpusId": "207556454"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.4326171875
        },
        {
            "corpus_id": "226283907",
            "title": "Using BERT for Qualitative Content Analysis in Psychosocial Online Counseling",
            "text": "The low macro F1 score with 29.2% of the BERT classifier compared to the 39.7% of the SVM classifier shows that the BERT classifier performs significantly worse on classes with few samples than the SVM classifier. The result of the weighted F1 score of 74.4% of the BERT model compared to the 68.0% of the SVM model, however, indicates that the BERT classifier outperforms the SVM if the whole data set is considered. Table 3 shows an extract from the classification report. In general, the BERT classifier improves in its performance with the increase in available training samples for each class.\n\nIn specific categories, such as \"Empathy for others\", this observation is not true. Categories with this behavior often contain previously mentioned category-specific keywords or phrases which is why the simple bag-of-words approach outperforms the more complex BERT techniques from a statistical point of view. A detailed analysis of the misclassified sequences by the BERT model, however, has shown that the classification of these sequences is not inherently wrong but rather shows suitable alternative affiliations to categories. This behavior is examined in greater detail in Section 3.6.",
            "score": 0.4227106823263234,
            "section_title": "Analyzing the Classification Results",
            "char_start_offset": 15463,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.1634521484375
        },
        {
            "corpus_id": "227231089",
            "title": "Go Simple and Pre-Train on Domain-Specific Corpora: On the Role of Training Data for Text Classification",
            "text": "However, to the best of our knowledge there has not been an extensive comparison between such methods and newer pre-trained language models. To this end, we compare the light-weight linear classification model fastText (Joulin et al., 2017), coupled with generic and corpus-specific word embeddings, and the pre-trained language model BERT (Devlin et al., 2019), trained on generic data and domain-specific data. Specifically, we analyze the effect of training size over the performance of the classifiers in settings where such training data is limited, both in few-shot scenarios with a balanced set and keeping the original distributions. In both cases, our results show that a large pre-trained language model may not provide significant gains over a linear model that leverage word embeddings, especially when these belong to the given domain.",
            "score": 0.4227106823263234,
            "section_title": "Introduction",
            "char_start_offset": 1754,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 140
                },
                {
                    "start": 141,
                    "end": 412
                },
                {
                    "start": 413,
                    "end": 641
                },
                {
                    "start": 642,
                    "end": 848
                }
            ],
            "ref_mentions": [
                {
                    "start": 219,
                    "end": 240,
                    "matchedPaperCorpusId": "1210515"
                },
                {
                    "start": 340,
                    "end": 361,
                    "matchedPaperCorpusId": "52967399"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.90966796875
        },
        {
            "corpus_id": "238648863",
            "title": "Deep Neural Network and Boosting Based Hybrid Quality Ranking for e-Commerce Product Search",
            "text": "The performance of the product search model is reported in Table 2 in terms of precision, recall, and the F1-score at a cut-off K = 100 results. The model was tested on four datasets, using two types of word embedding representations: FastText and BERT. It can be seen from Table 2 that the retrieval performance of our product search model differs from a dataset to another. The performance is best in terms of Precision@K for the appliances dataset with 0.27, and worst for office products with 0.13, using the model with BERT embeddings. In terms of Recall@K and F1-Score@K, the appliances dataset obtained the best results using both the model with BERT and with FastText embeddings. On the other hand, toys and games achieved the worst Recall@K, and office products the worst F1-Score@K. \n\nComparing the results of the model using the different kinds of embeddings, the version with BERT embeddings generates results higher than FastText ones for all the datasets, which can be explained by the difference in dataset size on which the two models were pre-trained on, and most importantly, the capability of BERT in capturing and generating context-dependent word embeddings. \n\nTo analyze the performance of the sentiment analysis model in detecting the sentiment in reviews, we divided the reviews into two sets 80% for training the model and 20% for validation. The model was trained for 80 epochs, with a batch size of 512 and the Adam optimizer with a learning rate of 0.01, and the optimized AUC (Area Under the Curve) metric, which is commonly used to measure the quality of classification models. These parameter settings were obtained based on a grid-search approach for different values of the parameters. Following, we present plots of the loss and ROC (Receiver Operating Characteristic) curve for both the training and validation sets in Figures 2 and 3. In Figures 2 and 3, the loss and AUC metrics are depicted for both training and validation sets during the training phase of the sentiment analysis model. The loss decreases significantly during the first 20 epochs, while the accuracy increases remarkably during the first 10 epochs.",
            "score": 0.4226715577254444,
            "section_title": "Results and Discussions",
            "char_start_offset": 25359,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 144
                },
                {
                    "start": 145,
                    "end": 253
                },
                {
                    "start": 254,
                    "end": 375
                },
                {
                    "start": 376,
                    "end": 540
                },
                {
                    "start": 541,
                    "end": 687
                },
                {
                    "start": 688,
                    "end": 792
                },
                {
                    "start": 795,
                    "end": 1179
                },
                {
                    "start": 1182,
                    "end": 1367
                },
                {
                    "start": 1368,
                    "end": 1607
                },
                {
                    "start": 1608,
                    "end": 1718
                },
                {
                    "start": 1719,
                    "end": 1870
                },
                {
                    "start": 1871,
                    "end": 2025
                },
                {
                    "start": 2026,
                    "end": 2154
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.84765625
        },
        {
            "corpus_id": "247476025",
            "title": "On the Use of Fine-grained Vulnerable Code Statements for Software Vulnerability Assessment Models",
            "text": "This averaging method has been demonstrated to be effective [69]. Table 1 lists different values for the window and vector sizes of Word2vec used for tuning the performance of learning-based SV assessment models. fastText. fastText [7] enhances Word2vec by representing each token with an aggregated feature vector of its constituent subtokens. Technically, fastText combines the strengths of semantic representation of Word2vec and subtoken-augmented features of Bag-of-Subtokens. fastText has been shown to build yet compact report-level SV assessment models [44]. Like Word2vec, the feature vector of a code snippet was averaged from the vectors of all the input tokens. The length of sub-tokens also ranged from two to six, resembling that of Bag-of-Subtokens. Other hyperparameters of fastText for optimization are listed in Table 1. CodeBERT. CodeBERT [19] is an adaptation of BERT [15], the current state-of-the-art feature representation technique in NLP, to source code modeling. CodeBERT is a pre-trained model using both natural language and programming language data to produce contextual embedding for code tokens. The same code token can have different CodeBERT embedding vectors depending on other tokens in an input; whereas, word2vec/fastText produces a single vector for every token regardless of its context. In addition, the source code tokenizer of CodeBERT is built upon Byte-Pair Encoding (BPE) [68]. This tokenizer smartly retains sub-tokens that frequently appear in a training corpus rather than keeping all of them as in Bag-of-Subtokens and fastText, balancing between performance and cost. CodeBERT also preserves a special token, [CLS], to represent an entire code input. We leveraged the vector of this \n\n[CLS] token to extract the features for each code snippet. \n\nWe trained all the feature models from scratch, except CodeBERT as it is a pre-trained model. We used CodeBERT's pre-trained vocabulary and embeddings as commonly done in the literature [92].",
            "score": 0.42230907231575865,
            "section_title": "Code Feature Generation",
            "char_start_offset": 27236,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 65
                },
                {
                    "start": 66,
                    "end": 212
                },
                {
                    "start": 213,
                    "end": 222
                },
                {
                    "start": 223,
                    "end": 344
                },
                {
                    "start": 345,
                    "end": 481
                },
                {
                    "start": 482,
                    "end": 566
                },
                {
                    "start": 567,
                    "end": 673
                },
                {
                    "start": 674,
                    "end": 764
                },
                {
                    "start": 765,
                    "end": 838
                },
                {
                    "start": 839,
                    "end": 848
                },
                {
                    "start": 849,
                    "end": 988
                },
                {
                    "start": 989,
                    "end": 1127
                },
                {
                    "start": 1128,
                    "end": 1327
                },
                {
                    "start": 1328,
                    "end": 1423
                },
                {
                    "start": 1424,
                    "end": 1618
                },
                {
                    "start": 1619,
                    "end": 1701
                },
                {
                    "start": 1702,
                    "end": 1733
                },
                {
                    "start": 1736,
                    "end": 1794
                },
                {
                    "start": 1797,
                    "end": 1890
                },
                {
                    "start": 1891,
                    "end": 1988
                }
            ],
            "ref_mentions": [
                {
                    "start": 232,
                    "end": 235,
                    "matchedPaperCorpusId": "207556454"
                },
                {
                    "start": 561,
                    "end": 565,
                    "matchedPaperCorpusId": "195298474"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.55078125
        },
        {
            "corpus_id": "253552870",
            "title": "Critical assessment of transformer-based AI models for German clinical notes",
            "text": "We focused our experiments on the 3 transformer-based model architectures BERT, ELECTRA, and RoBERTa. \n\nBERT 19 is a bidirectional transformer-based encoder model, which is pre-trained on large amounts of unlabeled data using MLM and next sentence prediction (NSP) jointly as training objectives. During MLM, some input tokens are randomly masked and the objective is to predict the original tokens based only on their con-text. The NSP task is to determine if 2 sentences are consecutive or not. \n\nRoBERTa 20 is an optimized version of BERT. It is built on the same architecture as BERT but abandons the NSP objective and only uses masked-language modeling for pre-training. Unlike BERT, however, the data are not masked statically during preprocessing but dynamically during each epoch. In addition, some hyperparameters such as the batch size and the tokenizer have been changed. \n\nELECTRA 21 uses the same architecture as BERT but differs in its pre-training procedure. While BERT aims for MLM and NSP,  ELECTRA uses a method called replaced token detection (RTD). Two separate models are used for this purpose: a generator and a discriminator. The generator is trained by MLM, and its output is then used as input for the discriminator. The discriminator has to predict whether a token has been replaced or whether it is the original input. After pre-training, only the discriminator is used. \n\nTable 2 lists the models we used in this study and provides information on the data used for pre-training. All German language models were trained on general language corpora consisting of Wikipedia articles, books, news articles, or vast amounts of crawled textual data. \n\nTraining and assessment of language models for the German clinical domain",
            "score": 0.4222135958546103,
            "section_title": "Published transformer models",
            "char_start_offset": 11490,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 101
                },
                {
                    "start": 104,
                    "end": 296
                },
                {
                    "start": 297,
                    "end": 428
                },
                {
                    "start": 429,
                    "end": 496
                },
                {
                    "start": 499,
                    "end": 542
                },
                {
                    "start": 543,
                    "end": 675
                },
                {
                    "start": 676,
                    "end": 788
                },
                {
                    "start": 789,
                    "end": 882
                },
                {
                    "start": 885,
                    "end": 973
                },
                {
                    "start": 974,
                    "end": 1068
                },
                {
                    "start": 1069,
                    "end": 1148
                },
                {
                    "start": 1149,
                    "end": 1241
                },
                {
                    "start": 1242,
                    "end": 1345
                },
                {
                    "start": 1346,
                    "end": 1397
                },
                {
                    "start": 1400,
                    "end": 1506
                },
                {
                    "start": 1507,
                    "end": 1671
                },
                {
                    "start": 1674,
                    "end": 1747
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.1771240234375
        },
        {
            "corpus_id": "254563733",
            "title": "Domain Adaptation of Transformer-Based Models Using Unlabeled Data for Relevance and Polarity Classification of German Customer Feedback",
            "text": "Text classification is a common problem in NLP and arises in a wide range of applications, such as sentiment analysis, question answering, and topic labeling [4]. \n\nOver the years, numerous machine learning approaches have been proposed and applied to tackle the task of sentiment analysis. Earlier, non-deep learning methods, such as support vector machines (SVM) [5], na\u00efve Bayes (NB) [6] and maximum entropy, were widely used and were considered the stateof-the-art at the time [7]. These are usually paired with text representation approaches, such as bag-of-words (BOW) or term frequency-inverse document frequency (TF-IDF) [8]. With the rise of deep neural networks, new approaches have been developed, starting with the introduction of word embedding models using approaches such as Word2Vec [9], global vectors (GloVe) [10] and fastText [11], which create word vectors with the goal of neighboring similar words in a vector space. Later, new approaches were developed such as embeddings from language models (ELMo) [12], which is a deep contextualized word representation model that outperforms Word2Vec. Afterwards, Google introduced the first transformer-based model BERT (bidirectional encoder representations from transformers) [13], which achieved remarkable results in many tasks and started the trend of large transformer-based models. These models are usually pre-trained on large-scale unlabeled task-independent corpora to learn universal language representations. After BERT, models such as RoBERTa [14], ELECTRA [15] were introduced as an improvement over BERT by incorporating new pre-training methods (see Sect. 3.2). \n\nAlthough such transformer-based models outperform previous approaches in various NLP tasks, they might struggle when a task corpus is overly focused on a specific domain [16]. In this context, Gururangan et al. [17] investigated whether additional pre-training on domain-specific data can be beneficial. The authors propose a domain-adaptive pre-training (DAPT) and a task-adaptive pre-training (TAPT) and conduct experiments on eight classification tasks of four domains to verify the effectiveness of these approaches.",
            "score": 0.42210772291009035,
            "section_title": "Related Work",
            "char_start_offset": 4410,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 162
                },
                {
                    "start": 165,
                    "end": 290
                },
                {
                    "start": 291,
                    "end": 485
                },
                {
                    "start": 486,
                    "end": 633
                },
                {
                    "start": 634,
                    "end": 938
                },
                {
                    "start": 939,
                    "end": 1112
                },
                {
                    "start": 1113,
                    "end": 1350
                },
                {
                    "start": 1351,
                    "end": 1482
                },
                {
                    "start": 1483,
                    "end": 1639
                },
                {
                    "start": 1642,
                    "end": 1817
                },
                {
                    "start": 1818,
                    "end": 1945
                },
                {
                    "start": 1946,
                    "end": 2162
                }
            ],
            "ref_mentions": [
                {
                    "start": 365,
                    "end": 368,
                    "matchedPaperCorpusId": "52874011"
                },
                {
                    "start": 387,
                    "end": 390,
                    "matchedPaperCorpusId": "14891965"
                },
                {
                    "start": 629,
                    "end": 632,
                    "matchedPaperCorpusId": "2996187"
                },
                {
                    "start": 799,
                    "end": 802,
                    "matchedPaperCorpusId": "5959482"
                },
                {
                    "start": 827,
                    "end": 831,
                    "matchedPaperCorpusId": "1957433"
                },
                {
                    "start": 845,
                    "end": 849,
                    "matchedPaperCorpusId": "207556454"
                },
                {
                    "start": 1023,
                    "end": 1027,
                    "matchedPaperCorpusId": "3626819"
                },
                {
                    "start": 1240,
                    "end": 1244,
                    "matchedPaperCorpusId": "52967399"
                },
                {
                    "start": 1532,
                    "end": 1536,
                    "matchedPaperCorpusId": "208229926"
                },
                {
                    "start": 1812,
                    "end": 1816,
                    "matchedPaperCorpusId": "220793195"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.478271484375
        },
        {
            "corpus_id": "265150274",
            "title": "Comprehending Lexical and Affective Ontologies in the Demographically Diverse Spatial Social Media Discourse",
            "text": "The transformer-based pre-trained language models, such as BERT [24] and RoBERTa [25], have shown state-of-the-art results in various text classification tasks with limited labeled data.We fine-tune both transformer-based pre-trained models for categorizing reviews into two demographically diverse groups.Since this is a binary-level classification task, we utilize the classification module of these pre-trained models.The Hugging face library [26] is used for fine-tuning all the pre-trained models.As the initial layers of pre-trained models primarily learn general features, they are left unchanged during fine-tuning process.Only the last layer of the pre-trained model is trained using new data specifically for the binarylevel classification task.We tokenize the input data for fine-tuning the language model.As pre-trained models typically support texts with a maximum of 512 tokens, we divide reviews longer than 512 words into 512-word chunks.During training, all the 512token chunks are assigned the same class as the original review.During testing, the final class of the review is determined by majority voting.In the event of a tie, we consider the word length of the chunks to decide the final class label.For training, we use a mini-batch size of 8 and a learning rate of 4*10 -5 .During the training process, 20% of the samples are dedicated to validation.We optimize the pre-trained models using the Adam optimizer, with the loss parameter set to Fig. 4: The comparisons between the best results achieved by classical ML classifiers (i.e., RF with linguistic features and SVM with lexical features), and two transformer-based language models, for the demographic group prediction task categorical cross-entropy.The training procedure is carried out for four epochs with an early stopping criterion set.Note that all hyperparameters of deep learning models are determined based on empirical evaluation.",
            "score": 0.4220608026203419,
            "section_title": "C. Deep Learning-based Classification",
            "char_start_offset": 10067,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 186
                },
                {
                    "start": 186,
                    "end": 306
                },
                {
                    "start": 306,
                    "end": 421
                },
                {
                    "start": 421,
                    "end": 502
                },
                {
                    "start": 502,
                    "end": 631
                },
                {
                    "start": 631,
                    "end": 755
                },
                {
                    "start": 755,
                    "end": 817
                },
                {
                    "start": 817,
                    "end": 954
                },
                {
                    "start": 954,
                    "end": 1046
                },
                {
                    "start": 1046,
                    "end": 1125
                },
                {
                    "start": 1125,
                    "end": 1222
                },
                {
                    "start": 1222,
                    "end": 1298
                },
                {
                    "start": 1298,
                    "end": 1374
                },
                {
                    "start": 1374,
                    "end": 1730
                },
                {
                    "start": 1730,
                    "end": 1821
                },
                {
                    "start": 1821,
                    "end": 1920
                }
            ],
            "ref_mentions": [
                {
                    "start": 446,
                    "end": 450,
                    "matchedPaperCorpusId": "269498086"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.14990234375
        },
        {
            "corpus_id": "227231089",
            "title": "Go Simple and Pre-Train on Domain-Specific Corpora: On the Role of Training Data for Text Classification",
            "text": "In this paper, we analyzed the role of training and unlabeled domain-specific data in supervised text classification. We compared both linear and neural models based on transformer-based language models. In settings with small training data, a simple method such as fastText coupled with domain-specific word embeddings appear to be more robust than a more data-consuming model such as BERT, even when BERT is pre-trained on domain-relevant data. However, the same classifier with generic pretrained word embeddings does not perform consistently better than a traditional frequency-based linear baseline. 4 BERT, pre-trained on domain-specific data (i.e., Twitter) leads to improvements over generic BERT, especially for few-shot experiments. For future work it would be interesting to further delve into the role of unlabeled data in text classification, both in terms of word embeddings (e.g., by making use of meta-embeddings (Yin and Sch\u00fctze, 2016)) and the data used to train language models (Gururangan et al., 2020). Moreover, this quantitative analysis could be extended to more classification tasks and different models, e.g., larger language models such as RoBERTa (Liu et al., 2019) and GPT-3 (Brown et al., 2020), which appear to be more suited to few-shot experiments. However, the generic domain embeddings tend to fail to represent the meaning of more domainspecific words, which may explain their lower performance. This is confirmed by the nearest neighbour analysis (see Table 5) which showed that the generic domain embeddings do not provide accurate representations of more technical words such as 'Windows' and 'Sun'. In the IMDB reviews, words such as 'Toothless', used within a very specific context are also not correctly represented by the generic model. Moreover, tweets are rich with abbreviations which have domain-specific meaning such as 'SF' referring to 'San Francisco'.",
            "score": 0.4218332958693194,
            "section_title": "Conclusion and Future Work",
            "char_start_offset": 14243,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 117
                },
                {
                    "start": 118,
                    "end": 203
                },
                {
                    "start": 204,
                    "end": 446
                },
                {
                    "start": 447,
                    "end": 606
                },
                {
                    "start": 607,
                    "end": 742
                },
                {
                    "start": 743,
                    "end": 1023
                },
                {
                    "start": 1024,
                    "end": 1281
                },
                {
                    "start": 1282,
                    "end": 1431
                },
                {
                    "start": 1432,
                    "end": 1638
                },
                {
                    "start": 1639,
                    "end": 1779
                },
                {
                    "start": 1780,
                    "end": 1902
                }
            ],
            "ref_mentions": [
                {
                    "start": 929,
                    "end": 952,
                    "matchedPaperCorpusId": "2820782"
                },
                {
                    "start": 997,
                    "end": 1022,
                    "matchedPaperCorpusId": "216080466"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.87060546875
        },
        {
            "corpus_id": "264455121",
            "title": "ACP-BC: A Model for Accurate Identification of Anticancer Peptides Based on Fusion Features of Bidirectional Long Short-Term Memory and Chemically Derived Information",
            "text": "The selection of a suitable pre-trained BERT model is crucial in the feature channel of chemical Bert. We compared two different BERT models, namely chemBERTa [59] based on the robustly optimized BERT pre-training approach (RoBerta) [60] and BERT-base [53,54], for feature extraction of our SMILES-formatted data. chemBERTa was pre-trained on a large amount of SMILES-formatted data, while BERT-base was pre-trained on human natural language text. For chemBERTa, we also compared two different tokenization methods, namely SMILES-tokenizer (ST) and BPE's tokenizer (BT) [61]. Table 2 shows the results of our model validation on independent validation dataset ACPs164. The experimental results show that these BERT models exhibit trivial performance differences, but the chemBERTa model based on the SMILES tokenizer achieves better results. 3, it can be observed that these features represent different levels of information, and thus achieve better results when fused together.",
            "score": 0.42171501204233214,
            "section_title": "Parameters of ACP-BC",
            "char_start_offset": 20163,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 102
                },
                {
                    "start": 103,
                    "end": 313
                },
                {
                    "start": 314,
                    "end": 447
                },
                {
                    "start": 448,
                    "end": 575
                },
                {
                    "start": 576,
                    "end": 668
                },
                {
                    "start": 669,
                    "end": 841
                },
                {
                    "start": 842,
                    "end": 979
                }
            ],
            "ref_mentions": [
                {
                    "start": 252,
                    "end": 256,
                    "matchedPaperCorpusId": "255912653"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.1644287109375
        },
        {
            "corpus_id": "235658009",
            "title": "Predicting the objective and priority of issue reports in software repositories",
            "text": "Previously, we obtained and pre-processed our issue-objective dataset. In this step, we train our classifier for the first stage of our approach. To predict issue objectives (Bug report, Enhancement, or Support/Document), we train a transformer-based model on the issueobjective dataset. We fine-tune the RoBERTa (Liu et al. 2019) on our issue-objective dataset. RoBERTa includes pre-training improvements (compared to the vanilla BERT model Devlin et al. 2018) using only unlabeled text from the web, with minimal fine-tuning and no data augmentation. The authors modified the masked language modeling task of BERT by using dynamic masking based on a new masking pattern generated each time a sentence is fed into training. They also eliminated the next sentence prediction task since Facebook's analysis indicated that it indeed hurts the model's performance. The Input of our model is the concatenated pre-processed word vectors of title and description of issues. In this stage, we feed the models with pre-processed text (word vectors) and the models process them accordingly. The output of the first stage is the probability of an issue to be a Bug report, Enhancement, or Support.",
            "score": 0.4216868113216321,
            "section_title": "First stage: Objective Detection",
            "char_start_offset": 19089,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.1724853515625
        },
        {
            "corpus_id": "247158487",
            "title": "Bipartite Graph Matching Algorithms for Clean-Clean Entity Resolution: An Empirical Evaluation",
            "text": "This drawback is addressed by the character-level embeddings: fastText vectorizes a token by summing the embeddings of all its character n-grams [2]. For this reason, we exclusively consider the 300-dimensional fastText in the following. \n\n(ii) Transformer-based language models [8] go beyond the shallow, context-agnostic pre-trained embeddings by vectorizing an item based on its context. In this way, they assign different vectors to homonyms, which share the same form, but different meaning (e.g., \"bank\" as a financial institution or as the border of a river). They also assign similar vectors to synonyms, which have different form, but almost the same meaning (e.g., \"enormous\" and \"vast\"). Several BERT-based language models have been applied to ER in [3,29]. They do suffer from out-of-vocabulary tokens, but to the best of our knowledge, there is no established character-level language model that could address this issue, as fastText does for pre-trained embeddings. Among them, we exclusively consider the 768-dimensional ALBERT, due to its higher efficiency [27]. \n\nEvery similarity measure receives as input two representation models and produces a score that is proportional to the likelihood that the respective entities correspond to the same real world object: the higher the score, the more similar are the input models and their textual values and the higher is the matching likelihood. \n\nFor each type of representation models, we considered a large variety of established similarity measures. The following are combined with the character-level schema-based representation models: Damerau-Levenshtein, Levenshtein and q-grams distance, Jaro Similarity, Needleman Wunch, Longest Common Subsequence and Longest Common Subsequence. To the token-level, schema-based models we apply: Cosine, Dice and (Generalized) Jaccard similarity as well as Monge-Elkan, Overlap Coefficient, Block and Euclidean distance. The schema-agnostic n-gram vectors are coupled with Arcs and Jaccard similarity as well as with Cosine and Generalized Jaccard similarity with TF or TF-IDF weights.",
            "score": 0.42147125851117784,
            "section_title": "SIMILARITY GRAPHS",
            "char_start_offset": 24648,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 149
                },
                {
                    "start": 150,
                    "end": 237
                },
                {
                    "start": 240,
                    "end": 390
                },
                {
                    "start": 391,
                    "end": 566
                },
                {
                    "start": 567,
                    "end": 698
                },
                {
                    "start": 699,
                    "end": 768
                },
                {
                    "start": 769,
                    "end": 979
                },
                {
                    "start": 980,
                    "end": 1078
                },
                {
                    "start": 1081,
                    "end": 1408
                },
                {
                    "start": 1411,
                    "end": 1516
                },
                {
                    "start": 1517,
                    "end": 1752
                },
                {
                    "start": 1753,
                    "end": 1927
                },
                {
                    "start": 1928,
                    "end": 2092
                }
            ],
            "ref_mentions": [
                {
                    "start": 145,
                    "end": 148,
                    "matchedPaperCorpusId": "207556454"
                },
                {
                    "start": 279,
                    "end": 282,
                    "matchedPaperCorpusId": "52967399"
                },
                {
                    "start": 761,
                    "end": 764,
                    "matchedPaperCorpusId": "214613452"
                },
                {
                    "start": 1073,
                    "end": 1077,
                    "matchedPaperCorpusId": "202888986"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.572265625
        },
        {
            "corpus_id": "269973968",
            "title": "Identification of mycoplasma pneumonia in children based on fusion of multi-modal clinical free-text description and structured test data",
            "text": "Influence of word representation context on free-text stream analysis \n\nIn our study, we analyzed various word embedding representations and their corresponding classification methods, as shown in Table 3. When comparing VSM-based methods with Word2Vec-based methods, we equate MLP and FastText due to their similar complexities as shallow neural networks. FastText, however, demonstrates better performance than MLP (0.696 vs 0.670 in Acc and 0.767 vs 0.727 in F 1 ), indicating Word2Vec's superior word representation capabilities. VSM's corpus is restricted to the training set, while Word2Vec benefits from a much larger corpus. Despite VSM's corpus being more relevant to specific tasks, word frequency alone proves insufficient for capturing the full importance of a word. This limitation includes the inability to reflect the positional information of words and their interrelationships. In contrast, Word2Vec, through unsupervised training on extensive corpora, learns more nuanced word representations, capturing implicit word relationships. Interestingly, original BERT, despite its wide-ranging successes in natural language processing, does not show a marked advantage over Word2Vec in our analysis and is slightly outperformed by TextCNN. This observation is further evidenced by the improved performance of MLM BERT over the original BERT, suggesting that contextual word embeddings benefit from additional data and training. This leads us to conclude that for specialized domain language tasks like clinical text analysis, pre-training is essential to fully leverage BERT's capabilities.",
            "score": 0.42144553682688757,
            "section_title": "Discussions",
            "char_start_offset": 18508,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 69
                },
                {
                    "start": 72,
                    "end": 205
                },
                {
                    "start": 206,
                    "end": 356
                },
                {
                    "start": 357,
                    "end": 533
                },
                {
                    "start": 534,
                    "end": 632
                },
                {
                    "start": 633,
                    "end": 778
                },
                {
                    "start": 779,
                    "end": 894
                },
                {
                    "start": 895,
                    "end": 1050
                },
                {
                    "start": 1051,
                    "end": 1251
                },
                {
                    "start": 1252,
                    "end": 1439
                },
                {
                    "start": 1440,
                    "end": 1602
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.69921875
        },
        {
            "corpus_id": "220665885",
            "title": "CS-NET at SemEval-2020 Task 4: Siamese BERT for ComVE",
            "text": "Language models : Over the years, a great amount of effort has been directed towards creating benchmark datasets that can measure a system's performance on language processing tasks and provide an impetus for the development of new approaches to the tasks. These benchmark tasks have led to many computational models ranging from earlier symbolic and statistical approaches to recent approaches based on deep neural networks; which model context of language, take advantage of external data or knowledge resources, and achieve the state-of-the-art performance, and at times, even near or above human performance. A major landmark in NLP is the development of pre-trained models and embeddings that can be used as features or further fine-tuned for downstream tasks. These models are often trained based on large corpora of textual data to capture different word senses. The defining contribution of Embeddings from Language Models (ELMo) (Peters et al., 2018) is its contextual word embeddings, which are built relying on the entire input sentence that they belong to. The recent Bidirectional Encoder Representations from Transformers (BERT) model outperforms previous competitive approaches by better capturing the context. A more recent model, XLNET (Yang et al., 2019), exceeded the performance of the vanilla BERT variant on several benchmarks. Robustly Optimized BERT Approach (RoBERTa) (Liu et al., 2019) achieved further improvement by making changes to the pre-training approach used in BERT. It includes randomizing masked tokens in the cloze pre-training task for each epoch instead of keeping them the same over epochs. It also augments the next sentence prediction pre-training task with an additional task which compels the model to also predict whether a candidate next sentence comes from the same document or not. A Lite BERT (ALBERT) (Lan et al., 2019) implements several novel parameter reduction techniques to increase the training speed and efficiency of BERT, enabling a much deeper scale-up than the original large variant of BERT while having fewer parameters. ELECTRA (Clark et al., 2020) is used to pre-train transformers with comparatively less computation. This model is similar to the discriminator of a GAN.\n\nCommon sense validation : TriAN (Wang et al., 2018) achieved state-of-the-art performance",
            "score": 0.42141420062618007,
            "section_title": "Related Work",
            "char_start_offset": 3133,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.2161865234375
        },
        {
            "corpus_id": "258165997",
            "title": "The Covid-19 Disinformation Detection on Social Media Using the NLP Approaches",
            "text": "The BERT model is a pre-training model developed by Google AI lab in 2018, the layout can be shown in Figure 1. It aims to represent unlabeled text by combining the left and right text environments, which means it can generate contextualized embeddings. The BERT model can be modified without substantial structural modification. To adapt to multi-domain natural language processing tasks, it is determined that it is very suitable for use as a base model for transfer learning [8]. BERT is based on a transformer architecture. Figure1. To correctly represent a word, it uses attention mechanisms to obtain information about the pertinent context of a particular word and then encodes that context in a rich vector. The network employs an encoder-decoder architecture much like neural nets. The BERT model comes in two variations: BERT-base and BERT-large. BERT-base is equivalent in size to the OpenAI Transformer; BERT-big is a tremendously enormous model that produced cuttingedge findings. Both have a significant number of encoder layers; the difference is the quality of encoder. The comparison between BERT-base and BERT-large can be shown in Table 2. The BERT model mainly uses training techniques including mask language modeling (MLM), next sentence prediction (NSP), and sentence order prediction (SOP). The MLM makes the model bidirectional in nature because the model must fill the blank in sentence learning from the text before and after. All the techniques above lead BERT to be a suitable model to learn from scratch without expertise about sub-domain. There are currently two methods for using pre-trained language representations in downstream tasks. One is based on the features, the other on is fine-tuning. For dataset with lower volume, feature-based strategy is preferred. The simplest approach to apply BERT for sentence categorization is to use it to categorize a single message. Take the sentence as input, BERT as the first few layers of the model, and add a classifier to the output layer. To train such model, the main process is training the classifier with minimal changes happening to BERT model during the training phase. Based on the above reasons, the BERT model has been widely used in task-specific fields to solve related text representation problems.",
            "score": 0.4211534978815636,
            "section_title": "Classifier based on BERT model",
            "char_start_offset": 9111,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 111
                },
                {
                    "start": 112,
                    "end": 253
                },
                {
                    "start": 254,
                    "end": 329
                },
                {
                    "start": 330,
                    "end": 482
                },
                {
                    "start": 483,
                    "end": 527
                },
                {
                    "start": 528,
                    "end": 536
                },
                {
                    "start": 537,
                    "end": 715
                },
                {
                    "start": 716,
                    "end": 790
                },
                {
                    "start": 791,
                    "end": 856
                },
                {
                    "start": 857,
                    "end": 993
                },
                {
                    "start": 994,
                    "end": 1085
                },
                {
                    "start": 1086,
                    "end": 1158
                },
                {
                    "start": 1159,
                    "end": 1314
                },
                {
                    "start": 1315,
                    "end": 1453
                },
                {
                    "start": 1454,
                    "end": 1569
                },
                {
                    "start": 1570,
                    "end": 1669
                },
                {
                    "start": 1670,
                    "end": 1728
                },
                {
                    "start": 1729,
                    "end": 1796
                },
                {
                    "start": 1797,
                    "end": 1905
                },
                {
                    "start": 1906,
                    "end": 2018
                },
                {
                    "start": 2019,
                    "end": 2155
                },
                {
                    "start": 2156,
                    "end": 2290
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.1995849609375
        },
        {
            "corpus_id": "248300080",
            "title": "An Exploratory Study on Code Attention in BERT",
            "text": "There are two variants of BERT: BERT-Large and BERT-Base [10]. BERT-Base model contains 12 stacked encoders, with 12 attention heads in each layer. In our study, we use the BERT-Base cased language model as a direct comparison against the natural language, English. The cased language model is used so that the BERT model can learn the Java coding conventions for the identifiers, which promotes the use of different casing styles. As we intend to compare our results of the BERT model trained on code with natural language, we need to keep the models the same. There are multiple implementations available for BERT and its variants trained on code, which are pre-trained with different objectives (e.g., MLM and Replaced Token Detection). \n\nHowever, for the comparison, we required a BERT model that is pre-trained with the same objectives as the one used for the NLP study [8] and also should purely be pre-trained on code. We are unable to find an open-source pre-trained BERT-Base model that is trained solely on code. Therefore, we trained a BERT-Base cased model from scratch. All the pre-training decisions like the number of layers in the encoder and decoder, the casing of tokens, and the pre-training objectives are consistent with the experimental settings in the NLP study by Clark et al. [8]. For pre-training the BERT model, we used the benchmark dataset released by Husain et al. [18]. This dataset contains a large corpus for six different programming languages collected from open source repositories available on GitHub. However, we only confine our work to the Java programming language in this study (we do not require a multilingual model) to keep it consistent with the NLP study where the BERT model is only trained on a single natural language. Java is chosen as the language in our study because it is a widely adopted programming language to study program comprehension in the software engineering community. It is also a commonly used programming language in the industry. The statistics of the dataset used is shown in Table 1.",
            "score": 0.4211196122880343,
            "section_title": "Experimental Setup",
            "char_start_offset": 14043,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 62
                },
                {
                    "start": 63,
                    "end": 147
                },
                {
                    "start": 148,
                    "end": 265
                },
                {
                    "start": 266,
                    "end": 431
                },
                {
                    "start": 432,
                    "end": 561
                },
                {
                    "start": 562,
                    "end": 739
                },
                {
                    "start": 742,
                    "end": 925
                },
                {
                    "start": 926,
                    "end": 1022
                },
                {
                    "start": 1023,
                    "end": 1082
                },
                {
                    "start": 1083,
                    "end": 1305
                },
                {
                    "start": 1306,
                    "end": 1400
                },
                {
                    "start": 1401,
                    "end": 1538
                },
                {
                    "start": 1539,
                    "end": 1768
                },
                {
                    "start": 1769,
                    "end": 1934
                },
                {
                    "start": 1935,
                    "end": 1999
                },
                {
                    "start": 2000,
                    "end": 2055
                }
            ],
            "ref_mentions": [
                {
                    "start": 57,
                    "end": 61,
                    "matchedPaperCorpusId": "52967399"
                },
                {
                    "start": 875,
                    "end": 878,
                    "matchedPaperCorpusId": "184486746"
                },
                {
                    "start": 1301,
                    "end": 1304,
                    "matchedPaperCorpusId": "184486746"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.120361328125
        },
        {
            "paperId": "decc16b467790a68e07858fcdbbaa2ae4889854e",
            "corpusId": 273502685,
            "title": "Evaluation Of P300 Speller Performance Using Large Language Models Along With Cross-Subject Training",
            "venue": "arXiv.org",
            "year": 2024,
            "referenceCount": 41,
            "citationCount": 0,
            "influentialCitationCount": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2410.15161, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2302799828",
                    "name": "Nithin Parthasarathy"
                },
                {
                    "authorId": "2038510579",
                    "name": "J. Soetedjo"
                },
                {
                    "authorId": "91525969",
                    "name": "Saarang Panchavati"
                },
                {
                    "authorId": "2302799830",
                    "name": "Nitya Parthasarathy"
                },
                {
                    "authorId": "2237610561",
                    "name": "C. Arnold"
                },
                {
                    "authorId": "2275234911",
                    "name": "N. Pouratian"
                },
                {
                    "authorId": "144757370",
                    "name": "W. Speier"
                }
            ],
            "abstract": "Amyotrophic lateral sclerosis (ALS), a progressive neuromuscular degenerative disease, severely restricts patient communication capacity within a few years of onset, resulting in a significant deterioration of quality of life. The P300 speller brain computer interface (BCI) offers an alternative communication medium by leveraging a subject's EEG response to characters traditionally highlighted on a character grid on a graphical user interface (GUI). A recurring theme in P300-based research is enhancing performance to enable faster subject interaction. This study builds on that theme by addressing key limitations, particularly in the training of multi-subject classifiers, and by integrating advanced language models to optimize stimuli presentation and word prediction, thereby improving communication efficiency. Furthermore, various advanced large language models such as Generative Pre-Trained Transformer (GPT2), BERT, and BART, alongside Dijkstra's algorithm, are utilized to optimize stimuli and provide word completion choices based on the spelling history. In addition, a multi-layered smoothing approach is applied to allow for out-of-vocabulary (OOV) words. By conducting extensive simulations based on randomly sampled EEG data from subjects, we show substantial speed improvements in typing passages that include rare and out-of-vocabulary (OOV) words, with the extent of improvement varying depending on the language model utilized. The gains through such character-level interface optimizations are approximately 10%, and GPT2 for multi-word prediction provides gains of around 40%. In particular, some large language models achieve performance levels within 10% of the theoretical performance limits established in this study. In addition, both within and across subjects, training techniques are explored, and speed improvements are shown to hold in both cases.",
            "corpus_id": "273502685",
            "text": "Amyotrophic lateral sclerosis (ALS), a progressive neuromuscular degenerative disease, severely restricts patient communication capacity within a few years of onset, resulting in a significant deterioration of quality of life. The P300 speller brain computer interface (BCI) offers an alternative communication medium by leveraging a subject's EEG response to characters traditionally highlighted on a character grid on a graphical user interface (GUI). A recurring theme in P300-based research is enhancing performance to enable faster subject interaction. This study builds on that theme by addressing key limitations, particularly in the training of multi-subject classifiers, and by integrating advanced language models to optimize stimuli presentation and word prediction, thereby improving communication efficiency. Furthermore, various advanced large language models such as Generative Pre-Trained Transformer (GPT2), BERT, and BART, alongside Dijkstra's algorithm, are utilized to optimize stimuli and provide word completion choices based on the spelling history. In addition, a multi-layered smoothing approach is applied to allow for out-of-vocabulary (OOV) words. By conducting extensive simulations based on randomly sampled EEG data from subjects, we show substantial speed improvements in typing passages that include rare and out-of-vocabulary (OOV) words, with the extent of improvement varying depending on the language model utilized. The gains through such character-level interface optimizations are approximately 10%, and GPT2 for multi-word prediction provides gains of around 40%. In particular, some large language models achieve performance levels within 10% of the theoretical performance limits established in this study. In addition, both within and across subjects, training techniques are explored, and speed improvements are shown to hold in both cases.",
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "score": 0.0,
            "stype": "public_api",
            "pdf_hash": "",
            "rerank_score": 0.11065673828125
        },
        {
            "paperId": "538dd184cad7c84612268febb191027dcc96de17",
            "corpusId": 274383421,
            "title": "Using large language models for extracting and pre-annotating texts on mental health from noisy data in a low-resource language",
            "venue": "PeerJ Computer Science",
            "year": 2024,
            "referenceCount": 33,
            "citationCount": 1,
            "influentialCitationCount": 0,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://doi.org/10.7717/peerj-cs.2395",
                "status": "GOLD",
                "license": "CCBY",
                "disclaimer": "Notice: Paper or abstract available at https://pmc.ncbi.nlm.nih.gov/articles/PMC11623104, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "3406429",
                    "name": "S. Koltcov"
                },
                {
                    "authorId": "2277800954",
                    "name": "Anton Surkov"
                },
                {
                    "authorId": "145866516",
                    "name": "Olessia Koltsova"
                },
                {
                    "authorId": "103629820",
                    "name": "V. Ignatenko"
                }
            ],
            "abstract": "Recent advancements in large language models (LLMs) have opened new possibilities for developing conversational agents (CAs) in various subfields of mental healthcare. However, this progress is hindered by limited access to high-quality training data, often due to privacy concerns and high annotation costs for low-resource languages. A potential solution is to create human-AI annotation systems that utilize extensive public domain user-to-user and user-to-professional discussions on social media. These discussions, however, are extremely noisy, necessitating the adaptation of LLMs for fully automatic cleaning and pre-classification to reduce human annotation effort. To date, research on LLM-based annotation in the mental health domain is extremely scarce. In this article, we explore the potential of zero-shot classification using four LLMs to select and pre-classify texts into topics representing psychiatric disorders, in order to facilitate the future development of CAs for disorder-specific counseling. We use 64,404 Russian-language texts from online discussion threads labeled with seven most commonly discussed disorders: depression, neurosis, paranoia, anxiety disorder, bipolar disorder, obsessive-compulsive disorder, and borderline personality disorder. Our research shows that while preliminary data filtering using zero-shot technology slightly improves classification, LLM fine-tuning makes a far larger contribution to its quality. Both standard and natural language inference (NLI) modes of fine-tuning increase classification accuracy by more than three times compared to non-fine-tuned training with preliminarily filtered data. Although NLI fine-tuning achieves slightly higher accuracy (0.64) than the standard approach, it is six times slower, indicating a need for further experimentation with NLI hypothesis engineering. Additionally, we demonstrate that lemmatization does not affect classification quality and that multilingual models using texts in their original language perform slightly better than English-only models using automatically translated texts. Finally, we introduce our dataset and model as the first openly available Russian-language resource for developing conversational agents in the domain of mental health counseling.",
            "corpus_id": "274383421",
            "text": "Recent advancements in large language models (LLMs) have opened new possibilities for developing conversational agents (CAs) in various subfields of mental healthcare. However, this progress is hindered by limited access to high-quality training data, often due to privacy concerns and high annotation costs for low-resource languages. A potential solution is to create human-AI annotation systems that utilize extensive public domain user-to-user and user-to-professional discussions on social media. These discussions, however, are extremely noisy, necessitating the adaptation of LLMs for fully automatic cleaning and pre-classification to reduce human annotation effort. To date, research on LLM-based annotation in the mental health domain is extremely scarce. In this article, we explore the potential of zero-shot classification using four LLMs to select and pre-classify texts into topics representing psychiatric disorders, in order to facilitate the future development of CAs for disorder-specific counseling. We use 64,404 Russian-language texts from online discussion threads labeled with seven most commonly discussed disorders: depression, neurosis, paranoia, anxiety disorder, bipolar disorder, obsessive-compulsive disorder, and borderline personality disorder. Our research shows that while preliminary data filtering using zero-shot technology slightly improves classification, LLM fine-tuning makes a far larger contribution to its quality. Both standard and natural language inference (NLI) modes of fine-tuning increase classification accuracy by more than three times compared to non-fine-tuned training with preliminarily filtered data. Although NLI fine-tuning achieves slightly higher accuracy (0.64) than the standard approach, it is six times slower, indicating a need for further experimentation with NLI hypothesis engineering. Additionally, we demonstrate that lemmatization does not affect classification quality and that multilingual models using texts in their original language perform slightly better than English-only models using automatically translated texts. Finally, we introduce our dataset and model as the first openly available Russian-language resource for developing conversational agents in the domain of mental health counseling.",
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "score": 0.0,
            "stype": "public_api",
            "pdf_hash": "",
            "rerank_score": 0.159912109375
        },
        {
            "paperId": "78f599fbd62dcc4a8dbab9d2f6056815dfc5b84c",
            "corpusId": 258180536,
            "title": "The MiniPile Challenge for Data-Efficient Language Models",
            "venue": "arXiv.org",
            "year": 2023,
            "referenceCount": 61,
            "citationCount": 46,
            "influentialCitationCount": 6,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "http://arxiv.org/pdf/2304.08442",
                "status": "CLOSED",
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2304.08442, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "66914903",
                    "name": "Jean Kaddour"
                }
            ],
            "abstract": "The ever-growing diversity of pre-training text corpora has equipped language models with generalization capabilities across various downstream tasks. However, such diverse datasets are often too large for academic budgets; hence, most research on Transformer architectures, training procedures, optimizers, etc. gets conducted on smaller, homogeneous datasets. To this end, we present The MiniPile Challenge, where one pre-trains a language model on a diverse text corpus containing at most 1M documents. MiniPile is a 6GB subset of the deduplicated 825GB The Pile corpus. To curate MiniPile, we perform a simple, three-step data filtering process: we (1) infer embeddings for all documents of the Pile, (2) cluster the embedding space using $k$-means, and (3) filter out low-quality clusters. To verify MiniPile's suitability for language model pre-training, we use it to pre-train a BERT and T5 model, yielding a performance drop of only $1.9\\%$/$2.5\\%$ on the GLUE and SNI benchmarks compared to the original pre-trained checkpoints trained on $2.6$x/$745$x the amount of data. MiniPile is available at https://huggingface.co/datasets/JeanKaddour/minipile.",
            "corpus_id": "258180536",
            "text": "The ever-growing diversity of pre-training text corpora has equipped language models with generalization capabilities across various downstream tasks. However, such diverse datasets are often too large for academic budgets; hence, most research on Transformer architectures, training procedures, optimizers, etc. gets conducted on smaller, homogeneous datasets. To this end, we present The MiniPile Challenge, where one pre-trains a language model on a diverse text corpus containing at most 1M documents. MiniPile is a 6GB subset of the deduplicated 825GB The Pile corpus. To curate MiniPile, we perform a simple, three-step data filtering process: we (1) infer embeddings for all documents of the Pile, (2) cluster the embedding space using $k$-means, and (3) filter out low-quality clusters. To verify MiniPile's suitability for language model pre-training, we use it to pre-train a BERT and T5 model, yielding a performance drop of only $1.9\\%$/$2.5\\%$ on the GLUE and SNI benchmarks compared to the original pre-trained checkpoints trained on $2.6$x/$745$x the amount of data. MiniPile is available at https://huggingface.co/datasets/JeanKaddour/minipile.",
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "score": 0.0,
            "stype": "public_api",
            "pdf_hash": "",
            "rerank_score": 0.11376953125
        },
        {
            "paperId": "002b999ba056ad02a66238eaafb242266d0cd803",
            "corpusId": 277955900,
            "title": "Meta-rater: A Multi-dimensional Data Selection Method for Pre-training Language Models",
            "venue": "arXiv.org",
            "year": 2025,
            "referenceCount": 54,
            "citationCount": 0,
            "influentialCitationCount": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2504.14194, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2366068443",
                    "name": "Xinlin Zhuang"
                },
                {
                    "authorId": "2233445161",
                    "name": "Jiahui Peng"
                },
                {
                    "authorId": "2299118979",
                    "name": "Ren Ma"
                },
                {
                    "authorId": "2352285941",
                    "name": "Yinfan Wang"
                },
                {
                    "authorId": "2318978696",
                    "name": "Tianyi Bai"
                },
                {
                    "authorId": "2298376663",
                    "name": "Xingjian Wei"
                },
                {
                    "authorId": "2289911484",
                    "name": "Jiantao Qiu"
                },
                {
                    "authorId": "2325489246",
                    "name": "Chi Zhang"
                },
                {
                    "authorId": "2356585875",
                    "name": "Ying Qian"
                },
                {
                    "authorId": "2346476781",
                    "name": "Conghui He"
                }
            ],
            "abstract": "The composition of pre-training datasets for large language models (LLMs) remains largely undisclosed, hindering transparency and efforts to optimize data quality, a critical driver of model performance. Current data selection methods, such as natural language quality assessments, diversity-based filters, and classifier-based approaches, are limited by single-dimensional evaluation or redundancy-focused strategies. To address these gaps, we propose four dimensions to evaluate data quality: professionalism, readability, reasoning, and cleanliness. We further introduce Meta-rater,a multi-dimensional data selection method that integrates these dimensions with existing quality metrics through learned optimal weightings. Meta-rater employs proxy models to train a regression model that predicts validation loss, enabling the identification of optimal combinations of quality scores. Experiments demonstrate that Meta-rater doubles convergence speed for 1.3B parameter models and improves downstream task performance by 3.23, with advantages that scale to models as large as 7.2B parameters. Our work establishes that holistic, multi-dimensional quality integration significantly outperforms conventional single-dimension approaches, offering a scalable paradigm for enhancing pre-training efficiency and model capability. To advance future research, we release scripts, data, and models at https://github.com/opendatalab/Meta-rater.",
            "corpus_id": "277955900",
            "text": "The composition of pre-training datasets for large language models (LLMs) remains largely undisclosed, hindering transparency and efforts to optimize data quality, a critical driver of model performance. Current data selection methods, such as natural language quality assessments, diversity-based filters, and classifier-based approaches, are limited by single-dimensional evaluation or redundancy-focused strategies. To address these gaps, we propose four dimensions to evaluate data quality: professionalism, readability, reasoning, and cleanliness. We further introduce Meta-rater,a multi-dimensional data selection method that integrates these dimensions with existing quality metrics through learned optimal weightings. Meta-rater employs proxy models to train a regression model that predicts validation loss, enabling the identification of optimal combinations of quality scores. Experiments demonstrate that Meta-rater doubles convergence speed for 1.3B parameter models and improves downstream task performance by 3.23, with advantages that scale to models as large as 7.2B parameters. Our work establishes that holistic, multi-dimensional quality integration significantly outperforms conventional single-dimension approaches, offering a scalable paradigm for enhancing pre-training efficiency and model capability. To advance future research, we release scripts, data, and models at https://github.com/opendatalab/Meta-rater.",
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "score": 0.0,
            "stype": "public_api",
            "pdf_hash": "",
            "rerank_score": 0.323486328125
        },
        {
            "paperId": "2291b575f09dec8866a78916ffbcf46d1d915182",
            "corpusId": 276394428,
            "title": "Organize the Web: Constructing Domains Enhances Pre-Training Data Curation",
            "venue": "arXiv.org",
            "year": 2025,
            "referenceCount": 0,
            "citationCount": 16,
            "influentialCitationCount": 4,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2502.10341, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2127066887",
                    "name": "Alexander Wettig"
                },
                {
                    "authorId": "2315302377",
                    "name": "Kyle Lo"
                },
                {
                    "authorId": "48872685",
                    "name": "Sewon Min"
                },
                {
                    "authorId": "2264251662",
                    "name": "Hanna Hajishirzi"
                },
                {
                    "authorId": "2286629648",
                    "name": "Danqi Chen"
                },
                {
                    "authorId": "2280666145",
                    "name": "Luca Soldaini"
                }
            ],
            "abstract": "Modern language models are trained on large, unstructured datasets consisting of trillions of tokens and obtained by crawling the web. The unstructured nature makes it difficult to reason about their contents and develop systematic approaches to data curation. In this paper, we unpack monolithic web corpora by developing taxonomies of their contents and organizing them into domains. We introduce WebOrganizer, a framework for organizing web pages in terms of both their topic and format. Using these two complementary notions of domains, we automatically annotate pre-training data by distilling annotations from a large language model into efficient classifiers. This allows us to study how data from different domains should be mixed to improve models on downstream tasks, and we show that we can combine insights about effective topics and formats to further boost performance. We demonstrate that our domain mixing also improves existing methods that select data based on quality. Furthermore, we study and compare how quality-based methods will implicitly change the domain mixture. Overall, our work demonstrates that constructing and mixing domains provides a valuable complement to quality-based data curation methods, opening new avenues for effective and insightful pre-training data curation.",
            "corpus_id": "276394428",
            "text": "Modern language models are trained on large, unstructured datasets consisting of trillions of tokens and obtained by crawling the web. The unstructured nature makes it difficult to reason about their contents and develop systematic approaches to data curation. In this paper, we unpack monolithic web corpora by developing taxonomies of their contents and organizing them into domains. We introduce WebOrganizer, a framework for organizing web pages in terms of both their topic and format. Using these two complementary notions of domains, we automatically annotate pre-training data by distilling annotations from a large language model into efficient classifiers. This allows us to study how data from different domains should be mixed to improve models on downstream tasks, and we show that we can combine insights about effective topics and formats to further boost performance. We demonstrate that our domain mixing also improves existing methods that select data based on quality. Furthermore, we study and compare how quality-based methods will implicitly change the domain mixture. Overall, our work demonstrates that constructing and mixing domains provides a valuable complement to quality-based data curation methods, opening new avenues for effective and insightful pre-training data curation.",
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "score": 0.0,
            "stype": "public_api",
            "pdf_hash": "",
            "rerank_score": 0.2626953125
        },
        {
            "paperId": "330889d375407c9c261716d6700e70f8034872c5",
            "corpusId": 275570168,
            "title": "Augmenting Human-Annotated Training Data with Large Language Model Generation and Distillation in Open-Response Assessment",
            "venue": "arXiv.org",
            "year": 2025,
            "referenceCount": 0,
            "citationCount": 1,
            "influentialCitationCount": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2501.09126, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2273374932",
                    "name": "Conrad Borchers"
                },
                {
                    "authorId": "2209057634",
                    "name": "Danielle R. Thomas"
                },
                {
                    "authorId": "2248806319",
                    "name": "Jionghao Lin"
                },
                {
                    "authorId": "2311622910",
                    "name": "Ralph Abboud"
                },
                {
                    "authorId": "2291597316",
                    "name": "Ken Koedinger"
                }
            ],
            "abstract": "Large Language Models (LLMs) like GPT-4o can help automate text classification tasks at low cost and scale. However, there are major concerns about the validity and reliability of LLM outputs. By contrast, human coding is generally more reliable but expensive to procure at scale. In this study, we propose a hybrid solution to leverage the strengths of both. We combine human-coded data and synthetic LLM-produced data to fine-tune a classical machine learning classifier, distilling both into a smaller BERT model. We evaluate our method on a human-coded test set as a validity measure for LLM output quality. In three experiments, we systematically vary LLM-generated samples' size, variety, and consistency, informed by best practices in LLM tuning. Our findings indicate that augmenting datasets with synthetic samples improves classifier performance, with optimal results achieved at an 80% synthetic to 20% human-coded data ratio. Lower temperature settings of 0.3, corresponding to less variability in LLM generations, produced more stable improvements but also limited model learning from augmented samples. In contrast, higher temperature settings (0.7 and above) introduced greater variability in performance estimates and, at times, lower performance. Hence, LLMs may produce more uniform output that classifiers overfit to earlier or produce more diverse output that runs the risk of deteriorating model performance through information irrelevant to the prediction task. Filtering out inconsistent synthetic samples did not enhance performance. We conclude that integrating human and LLM-generated data to improve text classification models in assessment offers a scalable solution that leverages both the accuracy of human coding and the variety of LLM outputs.",
            "corpus_id": "275570168",
            "text": "Large Language Models (LLMs) like GPT-4o can help automate text classification tasks at low cost and scale. However, there are major concerns about the validity and reliability of LLM outputs. By contrast, human coding is generally more reliable but expensive to procure at scale. In this study, we propose a hybrid solution to leverage the strengths of both. We combine human-coded data and synthetic LLM-produced data to fine-tune a classical machine learning classifier, distilling both into a smaller BERT model. We evaluate our method on a human-coded test set as a validity measure for LLM output quality. In three experiments, we systematically vary LLM-generated samples' size, variety, and consistency, informed by best practices in LLM tuning. Our findings indicate that augmenting datasets with synthetic samples improves classifier performance, with optimal results achieved at an 80% synthetic to 20% human-coded data ratio. Lower temperature settings of 0.3, corresponding to less variability in LLM generations, produced more stable improvements but also limited model learning from augmented samples. In contrast, higher temperature settings (0.7 and above) introduced greater variability in performance estimates and, at times, lower performance. Hence, LLMs may produce more uniform output that classifiers overfit to earlier or produce more diverse output that runs the risk of deteriorating model performance through information irrelevant to the prediction task. Filtering out inconsistent synthetic samples did not enhance performance. We conclude that integrating human and LLM-generated data to improve text classification models in assessment offers a scalable solution that leverages both the accuracy of human coding and the variety of LLM outputs.",
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "score": 0.0,
            "stype": "public_api",
            "pdf_hash": "",
            "rerank_score": 0.2178955078125
        },
        {
            "paperId": "94ba1afaff1851eb03f93592c94b3f5adbdf4235",
            "corpusId": 276079581,
            "title": "Text Data Augmentation for Large Language Models: A Comprehensive Survey of Methods, Challenges, and Opportunities",
            "venue": "arXiv.org",
            "year": 2025,
            "referenceCount": 0,
            "citationCount": 3,
            "influentialCitationCount": 1,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2501.18845, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2315171567",
                    "name": "Yaping Chai"
                },
                {
                    "authorId": "2343698282",
                    "name": "Haoran Xie"
                },
                {
                    "authorId": "2275144277",
                    "name": "S. J. Qin"
                }
            ],
            "abstract": "The increasing size and complexity of pre-trained language models have demonstrated superior performance in many applications, but they usually require large training datasets to be adequately trained. Insufficient training sets could unexpectedly make the model overfit and fail to cope with complex tasks. Large language models (LLMs) trained on extensive corpora have prominent text generation capabilities, which improve the quality and quantity of data and play a crucial role in data augmentation. Specifically, distinctive prompt templates are given in personalised tasks to guide LLMs in generating the required content. Recent promising retrieval-based techniques further improve the expressive performance of LLMs in data augmentation by introducing external knowledge to enable them to produce more grounded-truth data. This survey provides an in-depth analysis of data augmentation in LLMs, classifying the techniques into Simple Augmentation, Prompt-based Augmentation, Retrieval-based Augmentation and Hybrid Augmentation. We summarise the post-processing approaches in data augmentation, which contributes significantly to refining the augmented data and enabling the model to filter out unfaithful content. Then, we provide the common tasks and evaluation metrics. Finally, we introduce existing challenges and future opportunities that could bring further improvement to data augmentation.",
            "corpus_id": "276079581",
            "text": "The increasing size and complexity of pre-trained language models have demonstrated superior performance in many applications, but they usually require large training datasets to be adequately trained. Insufficient training sets could unexpectedly make the model overfit and fail to cope with complex tasks. Large language models (LLMs) trained on extensive corpora have prominent text generation capabilities, which improve the quality and quantity of data and play a crucial role in data augmentation. Specifically, distinctive prompt templates are given in personalised tasks to guide LLMs in generating the required content. Recent promising retrieval-based techniques further improve the expressive performance of LLMs in data augmentation by introducing external knowledge to enable them to produce more grounded-truth data. This survey provides an in-depth analysis of data augmentation in LLMs, classifying the techniques into Simple Augmentation, Prompt-based Augmentation, Retrieval-based Augmentation and Hybrid Augmentation. We summarise the post-processing approaches in data augmentation, which contributes significantly to refining the augmented data and enabling the model to filter out unfaithful content. Then, we provide the common tasks and evaluation metrics. Finally, we introduce existing challenges and future opportunities that could bring further improvement to data augmentation.",
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "score": 0.0,
            "stype": "public_api",
            "pdf_hash": "",
            "rerank_score": 0.1563720703125
        },
        {
            "paperId": "1ace955ef6230d82cdb6c3153e4e140d9ff1c268",
            "corpusId": 247450879,
            "title": "Evaluating BERT-based Pre-training Language Models for Detecting Misinformation",
            "venue": "arXiv.org",
            "year": 2022,
            "referenceCount": 62,
            "citationCount": 7,
            "influentialCitationCount": 0,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "http://arxiv.org/pdf/2203.07731",
                "status": "GREEN",
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2203.07731, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "9179946",
                    "name": "Rini Anggrainingsih"
                },
                {
                    "authorId": "39747227",
                    "name": "G. Hassan"
                },
                {
                    "authorId": "48024471",
                    "name": "A. Datta"
                }
            ],
            "abstract": "It is challenging to control the quality of online information due to the lack of supervision over all the information posted online. Manual checking is almost impossible given the vast number of posts made on online media and how quickly they spread. Therefore, there is a need for automated rumour detection techniques to limit the adverse effects of spreading misinformation. Previous studies mainly focused on finding and extracting the significant features of text data. However, extracting features is time-consuming and not a highly effective process. This study proposes the BERT- based pre-trained language models to encode text data into vectors and utilise neural network models to classify these vectors to detect misinformation. Furthermore, different language models (LM) ' performance with different trainable parameters was compared. The proposed technique is tested on different short and long text datasets. The result of the proposed technique has been compared with the state-of-the-art techniques on the same datasets. The results show that the proposed technique performs better than the state-of-the-art techniques. We also tested the proposed technique by combining the datasets. The results demonstrated that the large data training and testing size considerably improves the technique's performance.",
            "corpus_id": "247450879",
            "text": "It is challenging to control the quality of online information due to the lack of supervision over all the information posted online. Manual checking is almost impossible given the vast number of posts made on online media and how quickly they spread. Therefore, there is a need for automated rumour detection techniques to limit the adverse effects of spreading misinformation. Previous studies mainly focused on finding and extracting the significant features of text data. However, extracting features is time-consuming and not a highly effective process. This study proposes the BERT- based pre-trained language models to encode text data into vectors and utilise neural network models to classify these vectors to detect misinformation. Furthermore, different language models (LM) ' performance with different trainable parameters was compared. The proposed technique is tested on different short and long text datasets. The result of the proposed technique has been compared with the state-of-the-art techniques on the same datasets. The results show that the proposed technique performs better than the state-of-the-art techniques. We also tested the proposed technique by combining the datasets. The results demonstrated that the large data training and testing size considerably improves the technique's performance.",
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "score": 0.0,
            "stype": "public_api",
            "pdf_hash": "",
            "rerank_score": 0.265380859375
        },
        {
            "paperId": "3e318160d4129da8a69d39fa6bd01c8b7d1038cf",
            "corpusId": 268431896,
            "title": "227\u2003Spine-tuned Natural Language Models and Bespoke Regular Expression Classifiers for Automated Spinal Surgery Registry Development",
            "venue": "Neurosurgery",
            "year": 2024,
            "referenceCount": 0,
            "citationCount": 0,
            "influentialCitationCount": 0,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://journals.lww.com/neurosurgery/abstract/2024/04001/227_spine_tuned_natural_language_models_and.138.aspx",
                "status": "BRONZE",
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.1227/neu.0000000000002809_227?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1227/neu.0000000000002809_227, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2291565300",
                    "name": "Daniel A. Alber"
                },
                {
                    "authorId": "2190754113",
                    "name": "Alexander T. M. Cheung"
                },
                {
                    "authorId": "5861736",
                    "name": "David B. Kurland"
                },
                {
                    "authorId": "2291564767",
                    "name": "Karl Lee Sangwon"
                },
                {
                    "authorId": "1604649792",
                    "name": "L. Jiang"
                },
                {
                    "authorId": "2191368134",
                    "name": "Chris Liu"
                },
                {
                    "authorId": "2181708076",
                    "name": "E. Oermann"
                }
            ],
            "abstract": "INTRODUCTION: Surgical research demands the development of clinical registries, often through time-intensive manual chart review. Natural language processing (NLP) may accelerate registry development, and an ideal automatic registry (autoregistry) algorithm would be highly accurate while requiring minimal manual data annotation. NLP approaches including bespoke Regular Expression (RegEx) classifiers and Large Language Models (LLM) possess distinct strengths and weaknesses and have not been compared in the setting of autoregistry development. METHODS: We used an institutional data lake to retrieve 31,502 neurosurgical operative notes. A standardized set of spinal procedures was chosen for inclusion in the autoregistry. 200 manually annotated notes were used for training and testing purposes. RegEx classifiers were engineered to retrieve procedural info from unprocessed notes. A family of 110-million parameter BERT models, including LLM pre-trained on clinical text, was fine-tuned for the same tasks. We also tested a open-source 7-billion parameter LLM chatbot, Vicuna, without fine-tuning. RESULTS: The RegEx classifiers were able to identify spinal procedures and associated vertebral levels in nearly 99% of operative notes. Fine-tuned LLM identified common procedures (e.g. spinal fusion and laminectomy) with greater than 95% accuracy but performed poorly for rarer procedures (e.g. XLIF, corpectomy) and vertebral body identification. Qualitative evaluation of the Vicuna chatbot showed potential for the same tasks, following iteratively refined prompting. CONCLUSIONS: The goal of autoregistry development is to minimize time- and labor-intensive manual chart review. We found that fine-tuned LLM could not match the accuracy and efficiency of the RegEx classifier. However, LLM may be well-suited to expand existing clinical databases that provide a robust training set. Further work combining NLP approaches will attempt to develop a pipeline for autoregistry development from natural language (plain English) queries.",
            "corpus_id": "268431896",
            "text": "INTRODUCTION: Surgical research demands the development of clinical registries, often through time-intensive manual chart review. Natural language processing (NLP) may accelerate registry development, and an ideal automatic registry (autoregistry) algorithm would be highly accurate while requiring minimal manual data annotation. NLP approaches including bespoke Regular Expression (RegEx) classifiers and Large Language Models (LLM) possess distinct strengths and weaknesses and have not been compared in the setting of autoregistry development. METHODS: We used an institutional data lake to retrieve 31,502 neurosurgical operative notes. A standardized set of spinal procedures was chosen for inclusion in the autoregistry. 200 manually annotated notes were used for training and testing purposes. RegEx classifiers were engineered to retrieve procedural info from unprocessed notes. A family of 110-million parameter BERT models, including LLM pre-trained on clinical text, was fine-tuned for the same tasks. We also tested a open-source 7-billion parameter LLM chatbot, Vicuna, without fine-tuning. RESULTS: The RegEx classifiers were able to identify spinal procedures and associated vertebral levels in nearly 99% of operative notes. Fine-tuned LLM identified common procedures (e.g. spinal fusion and laminectomy) with greater than 95% accuracy but performed poorly for rarer procedures (e.g. XLIF, corpectomy) and vertebral body identification. Qualitative evaluation of the Vicuna chatbot showed potential for the same tasks, following iteratively refined prompting. CONCLUSIONS: The goal of autoregistry development is to minimize time- and labor-intensive manual chart review. We found that fine-tuned LLM could not match the accuracy and efficiency of the RegEx classifier. However, LLM may be well-suited to expand existing clinical databases that provide a robust training set. Further work combining NLP approaches will attempt to develop a pipeline for autoregistry development from natural language (plain English) queries.",
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "score": 0.0,
            "stype": "public_api",
            "pdf_hash": "",
            "rerank_score": 0.134033203125
        },
        {
            "paperId": "e4f5bdfe1a1ffe3f584e684e0f3a768f4ffe4b9e",
            "corpusId": 276394757,
            "title": "Large Language Models and Synthetic Data for Monitoring Dataset Mentions in Research Papers",
            "venue": "arXiv.org",
            "year": 2025,
            "referenceCount": 0,
            "citationCount": 0,
            "influentialCitationCount": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2502.10263, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2079545802",
                    "name": "Aivin V. Solatorio"
                },
                {
                    "authorId": "2345697037",
                    "name": "Rafael Macalaba"
                },
                {
                    "authorId": "2345697039",
                    "name": "James Liounis"
                }
            ],
            "abstract": "Tracking how data is mentioned and used in research papers provides critical insights for improving data discoverability, quality, and production. However, manually identifying and classifying dataset mentions across vast academic literature is resource-intensive and not scalable. This paper presents a machine learning framework that automates dataset mention detection across research domains by leveraging large language models (LLMs), synthetic data, and a two-stage fine-tuning process. We employ zero-shot extraction from research papers, an LLM-as-a-Judge for quality assessment, and a reasoning agent for refinement to generate a weakly supervised synthetic dataset. The Phi-3.5-mini instruct model is pre-fine-tuned on this dataset, followed by fine-tuning on a manually annotated subset. At inference, a ModernBERT-based classifier efficiently filters dataset mentions, reducing computational overhead while maintaining high recall. Evaluated on a held-out manually annotated sample, our fine-tuned model outperforms NuExtract-v1.5 and GLiNER-large-v2.1 in dataset extraction accuracy. Our results highlight how LLM-generated synthetic data can effectively address training data scarcity, improving generalization in low-resource settings. This framework offers a pathway toward scalable monitoring of dataset usage, enhancing transparency, and supporting researchers, funders, and policymakers in identifying data gaps and strengthening data accessibility for informed decision-making.",
            "corpus_id": "276394757",
            "text": "Tracking how data is mentioned and used in research papers provides critical insights for improving data discoverability, quality, and production. However, manually identifying and classifying dataset mentions across vast academic literature is resource-intensive and not scalable. This paper presents a machine learning framework that automates dataset mention detection across research domains by leveraging large language models (LLMs), synthetic data, and a two-stage fine-tuning process. We employ zero-shot extraction from research papers, an LLM-as-a-Judge for quality assessment, and a reasoning agent for refinement to generate a weakly supervised synthetic dataset. The Phi-3.5-mini instruct model is pre-fine-tuned on this dataset, followed by fine-tuning on a manually annotated subset. At inference, a ModernBERT-based classifier efficiently filters dataset mentions, reducing computational overhead while maintaining high recall. Evaluated on a held-out manually annotated sample, our fine-tuned model outperforms NuExtract-v1.5 and GLiNER-large-v2.1 in dataset extraction accuracy. Our results highlight how LLM-generated synthetic data can effectively address training data scarcity, improving generalization in low-resource settings. This framework offers a pathway toward scalable monitoring of dataset usage, enhancing transparency, and supporting researchers, funders, and policymakers in identifying data gaps and strengthening data accessibility for informed decision-making.",
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "score": 0.0,
            "stype": "public_api",
            "pdf_hash": "",
            "rerank_score": 0.328369140625
        },
        {
            "paperId": "b3b71c8e01bd487fe8e40d0fc0bd6faed970da96",
            "corpusId": 275213063,
            "title": "KaLM-Embedding: Superior Training Data Brings A Stronger Embedding Model",
            "venue": "arXiv.org",
            "year": 2025,
            "referenceCount": 0,
            "citationCount": 3,
            "influentialCitationCount": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2501.01028, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2149467818",
                    "name": "Xinshuo Hu"
                },
                {
                    "authorId": "2257014318",
                    "name": "Zifei Shan"
                },
                {
                    "authorId": "2326046038",
                    "name": "Xinping Zhao"
                },
                {
                    "authorId": "2265614778",
                    "name": "Zetian Sun"
                },
                {
                    "authorId": "2230018369",
                    "name": "Zhenyu Liu"
                },
                {
                    "authorId": "2265618386",
                    "name": "Dongfang Li"
                },
                {
                    "authorId": "2330241155",
                    "name": "Shaolin Ye"
                },
                {
                    "authorId": "2338333120",
                    "name": "Xinyuan Wei"
                },
                {
                    "authorId": "2260297779",
                    "name": "Qian Chen"
                },
                {
                    "authorId": "2285172247",
                    "name": "Baotian Hu"
                },
                {
                    "authorId": "2340382675",
                    "name": "Haofen Wang"
                },
                {
                    "authorId": "2342081596",
                    "name": "Jun Yu"
                },
                {
                    "authorId": "2258690227",
                    "name": "Min Zhang"
                }
            ],
            "abstract": "As retrieval-augmented generation prevails in large language models, embedding models are becoming increasingly crucial. Despite the growing number of general embedding models, prior work often overlooks the critical role of training data quality. In this work, we introduce KaLM-Embedding, a general multilingual embedding model that leverages a large quantity of cleaner, more diverse, and domain-specific training data. Our model has been trained with key techniques proven to enhance performance: (1) persona-based synthetic data to create diversified examples distilled from LLMs, (2) ranking consistency filtering to remove less informative samples, and (3) semi-homogeneous task batch sampling to improve training efficacy. Departing from traditional BERT-like architectures, we adopt Qwen2-0.5B as the pre-trained model, facilitating the adaptation of auto-regressive language models for general embedding tasks. Extensive evaluations of the MTEB benchmark across multiple languages show that our model outperforms others of comparable size, setting a new standard for multilingual embedding models with<1B parameters.",
            "corpus_id": "275213063",
            "text": "As retrieval-augmented generation prevails in large language models, embedding models are becoming increasingly crucial. Despite the growing number of general embedding models, prior work often overlooks the critical role of training data quality. In this work, we introduce KaLM-Embedding, a general multilingual embedding model that leverages a large quantity of cleaner, more diverse, and domain-specific training data. Our model has been trained with key techniques proven to enhance performance: (1) persona-based synthetic data to create diversified examples distilled from LLMs, (2) ranking consistency filtering to remove less informative samples, and (3) semi-homogeneous task batch sampling to improve training efficacy. Departing from traditional BERT-like architectures, we adopt Qwen2-0.5B as the pre-trained model, facilitating the adaptation of auto-regressive language models for general embedding tasks. Extensive evaluations of the MTEB benchmark across multiple languages show that our model outperforms others of comparable size, setting a new standard for multilingual embedding models with<1B parameters.",
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "score": 0.0,
            "stype": "public_api",
            "pdf_hash": "",
            "rerank_score": 0.11199951171875
        },
        {
            "paperId": "f349e5e8f0d18c948c1ffd92d3791db2b0ba2e55",
            "corpusId": 263310452,
            "title": "Data Filtering Networks",
            "venue": "International Conference on Learning Representations",
            "year": 2023,
            "referenceCount": 47,
            "citationCount": 144,
            "influentialCitationCount": 26,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://arxiv.org/pdf/2309.17425",
                "status": "CLOSED",
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2309.17425, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "46372713",
                    "name": "Alex Fang"
                },
                {
                    "authorId": "2203363212",
                    "name": "Albin Madappally Jose"
                },
                {
                    "authorId": "2249714262",
                    "name": "Amit Jain"
                },
                {
                    "authorId": "2253541812",
                    "name": "Ludwig Schmidt"
                },
                {
                    "authorId": "2238621175",
                    "name": "Alexander Toshev"
                },
                {
                    "authorId": "34961417",
                    "name": "Vaishaal Shankar"
                }
            ],
            "abstract": "Large training sets have become a cornerstone of machine learning and are the foundation for recent advances in language modeling and multimodal learning. While data curation for pre-training is often still ad-hoc, one common paradigm is to first collect a massive pool of data from the Web and then filter this candidate pool down to an actual training set via various heuristics. In this work, we study the problem of learning a data filtering network (DFN) for this second step of filtering a large uncurated dataset. Our key finding is that the quality of a network for filtering is distinct from its performance on downstream tasks: for instance, a model that performs well on ImageNet can yield worse training sets than a model with low ImageNet accuracy that is trained on a small amount of high-quality data. Based on our insights, we construct new data filtering networks that induce state-of-the-art image-text datasets. Specifically, our best performing dataset DFN-5B enables us to train state-of-the-art models for their compute budgets: among other improvements on a variety of tasks, a ViT-H trained on our dataset achieves 83.0% zero-shot transfer accuracy on ImageNet, out-performing models trained on other datasets such as LAION-2B, DataComp-1B, or OpenAI's WIT. In order to facilitate further research in dataset design, we also release a new 2 billion example dataset DFN-2B and show that high performance data filtering networks can be trained from scratch using only publicly available data.",
            "corpus_id": "263310452",
            "text": "Large training sets have become a cornerstone of machine learning and are the foundation for recent advances in language modeling and multimodal learning. While data curation for pre-training is often still ad-hoc, one common paradigm is to first collect a massive pool of data from the Web and then filter this candidate pool down to an actual training set via various heuristics. In this work, we study the problem of learning a data filtering network (DFN) for this second step of filtering a large uncurated dataset. Our key finding is that the quality of a network for filtering is distinct from its performance on downstream tasks: for instance, a model that performs well on ImageNet can yield worse training sets than a model with low ImageNet accuracy that is trained on a small amount of high-quality data. Based on our insights, we construct new data filtering networks that induce state-of-the-art image-text datasets. Specifically, our best performing dataset DFN-5B enables us to train state-of-the-art models for their compute budgets: among other improvements on a variety of tasks, a ViT-H trained on our dataset achieves 83.0% zero-shot transfer accuracy on ImageNet, out-performing models trained on other datasets such as LAION-2B, DataComp-1B, or OpenAI's WIT. In order to facilitate further research in dataset design, we also release a new 2 billion example dataset DFN-2B and show that high performance data filtering networks can be trained from scratch using only publicly available data.",
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "score": 0.0,
            "stype": "public_api",
            "pdf_hash": "",
            "rerank_score": 0.11126708984375
        },
        {
            "paperId": "9bad3f50e919e3dc4138eb7aa5e6b0df17504adf",
            "corpusId": 278148385,
            "title": "Detection of patient metadata in published articles for genomic epidemiology using machine learning and large language models",
            "venue": "medRxiv",
            "year": 2025,
            "referenceCount": 27,
            "citationCount": 0,
            "influentialCitationCount": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": "CCBYNCND",
                "disclaimer": "Notice: Paper or abstract available at https://pmc.ncbi.nlm.nih.gov/articles/PMC12060954, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2070501",
                    "name": "A. Klein"
                },
                {
                    "authorId": "3153539",
                    "name": "D. Weissenbacher"
                },
                {
                    "authorId": "1400829836",
                    "name": "K. O\u2019Connor"
                },
                {
                    "authorId": "1380963613",
                    "name": "Amir Elyaderani"
                },
                {
                    "authorId": "2279328427",
                    "name": "I. Amaro"
                },
                {
                    "authorId": "2058274607",
                    "name": "T. Onishi"
                },
                {
                    "authorId": "2269434399",
                    "name": "Su Golder"
                },
                {
                    "authorId": "2357861028",
                    "name": "Kaelen Spiegel"
                },
                {
                    "authorId": "2161990",
                    "name": "M. Scotch"
                },
                {
                    "authorId": "2269434539",
                    "name": "Graciela Gonzalez-Hernandez"
                }
            ],
            "abstract": "Objective: Patient metadata exist in published articles, but are often disconnected from genome sequences in databases, limiting their utility for genomic epidemiology. The objective of this study was to develop and evaluate natural language processing methods to facilitate the large-scale detection of patient metadata associated with reports of genome sequencing in published articles, drawing on the case of SARS-CoV-2. Methods: We applied filters to select a sample of 245 PubMed articles (50,918 sentences) in LitCovid for manual annotation of sentences that reported generating SARS-CoV-2 sequences. We trained, deployed, and validated a BERT-based classifier, and selected a sample of 150 predicted articles (22,147 sentences) for manual annotation of sentences that reported patient metadata associated with the sequences. In addition to training BERT-based classifiers, we experimented with a generative AI approach, prompting the Llama-3-70B LLM using zero-shot, role-based, few-shot, chain-of-thought, and reasoning-eliciting prompting. Results: BERT-based models that were pre-trained on corpora in biomedical or, more specifically, COVID-19 domains outperformed those that were pre-trained on corpora in general domains for detecting reports of patient metadata associated with SARS-CoV-2 sequences, achieving the best performance with a classifier based on a BiomedBERT-Large-Abstract model (F1- score = 0.776). While the best performance of our generative AI approach was achieved using role-based, few-shot, and chain-of-thought prompting (F1-score = 0.558), it was nonetheless outperformed by all of our machine learning-based classifiers. Conclusion: Our methods were applied to more than 350,000 published articles and can be used to advance the utility and efficiency of genomic epidemiology for public health responses to virus outbreaks.",
            "corpus_id": "278148385",
            "text": "Objective: Patient metadata exist in published articles, but are often disconnected from genome sequences in databases, limiting their utility for genomic epidemiology. The objective of this study was to develop and evaluate natural language processing methods to facilitate the large-scale detection of patient metadata associated with reports of genome sequencing in published articles, drawing on the case of SARS-CoV-2. Methods: We applied filters to select a sample of 245 PubMed articles (50,918 sentences) in LitCovid for manual annotation of sentences that reported generating SARS-CoV-2 sequences. We trained, deployed, and validated a BERT-based classifier, and selected a sample of 150 predicted articles (22,147 sentences) for manual annotation of sentences that reported patient metadata associated with the sequences. In addition to training BERT-based classifiers, we experimented with a generative AI approach, prompting the Llama-3-70B LLM using zero-shot, role-based, few-shot, chain-of-thought, and reasoning-eliciting prompting. Results: BERT-based models that were pre-trained on corpora in biomedical or, more specifically, COVID-19 domains outperformed those that were pre-trained on corpora in general domains for detecting reports of patient metadata associated with SARS-CoV-2 sequences, achieving the best performance with a classifier based on a BiomedBERT-Large-Abstract model (F1- score = 0.776). While the best performance of our generative AI approach was achieved using role-based, few-shot, and chain-of-thought prompting (F1-score = 0.558), it was nonetheless outperformed by all of our machine learning-based classifiers. Conclusion: Our methods were applied to more than 350,000 published articles and can be used to advance the utility and efficiency of genomic epidemiology for public health responses to virus outbreaks.",
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "score": 0.0,
            "stype": "public_api",
            "pdf_hash": "",
            "rerank_score": 0.1649169921875
        },
        {
            "paperId": "8d95a1a3eacee47113a700482ce384f09a9136e9",
            "corpusId": 277468273,
            "title": "Open-Qwen2VL: Compute-Efficient Pre-Training of Fully-Open Multimodal LLMs on Academic Resources",
            "venue": "arXiv.org",
            "year": 2025,
            "referenceCount": 53,
            "citationCount": 0,
            "influentialCitationCount": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2504.00595, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2108438929",
                    "name": "Weizhi Wang"
                },
                {
                    "authorId": "2270211545",
                    "name": "Yu Tian"
                },
                {
                    "authorId": "2269846471",
                    "name": "Linjie Yang"
                },
                {
                    "authorId": "2265388083",
                    "name": "Heng Wang"
                },
                {
                    "authorId": "2243564882",
                    "name": "Xifeng Yan"
                }
            ],
            "abstract": "The reproduction of state-of-the-art multimodal LLM pre-training faces barriers at every stage of the pipeline, including high-quality data filtering, multimodal data mixture strategies, sequence packing techniques, and training frameworks. We introduce Open-Qwen2VL, a fully open-source 2B-parameter Multimodal Large Language Model pre-trained efficiently on 29M image-text pairs using only 220 A100-40G GPU hours. Our approach employs low-to-high dynamic image resolution and multimodal sequence packing to significantly enhance pre-training efficiency. The training dataset was carefully curated using both MLLM-based filtering techniques (e.g., MLM-Filter) and conventional CLIP-based filtering methods, substantially improving data quality and training efficiency. The Open-Qwen2VL pre-training is conducted on academic level 8xA100-40G GPUs at UCSB on 5B packed multimodal tokens, which is 0.36% of 1.4T multimodal pre-training tokens of Qwen2-VL. The final instruction-tuned Open-Qwen2VL outperforms partially-open state-of-the-art MLLM Qwen2-VL-2B on various multimodal benchmarks of MMBench, SEEDBench, MMstar, and MathVista, indicating the remarkable training efficiency of Open-Qwen2VL. We open-source all aspects of our work, including compute-efficient and data-efficient training details, data filtering methods, sequence packing scripts, pre-training data in WebDataset format, FSDP-based training codebase, and both base and instruction-tuned model checkpoints. We redefine\"fully open\"for multimodal LLMs as the complete release of: 1) the training codebase, 2) detailed data filtering techniques, and 3) all pre-training and supervised fine-tuning data used to develop the model.",
            "corpus_id": "277468273",
            "text": "The reproduction of state-of-the-art multimodal LLM pre-training faces barriers at every stage of the pipeline, including high-quality data filtering, multimodal data mixture strategies, sequence packing techniques, and training frameworks. We introduce Open-Qwen2VL, a fully open-source 2B-parameter Multimodal Large Language Model pre-trained efficiently on 29M image-text pairs using only 220 A100-40G GPU hours. Our approach employs low-to-high dynamic image resolution and multimodal sequence packing to significantly enhance pre-training efficiency. The training dataset was carefully curated using both MLLM-based filtering techniques (e.g., MLM-Filter) and conventional CLIP-based filtering methods, substantially improving data quality and training efficiency. The Open-Qwen2VL pre-training is conducted on academic level 8xA100-40G GPUs at UCSB on 5B packed multimodal tokens, which is 0.36% of 1.4T multimodal pre-training tokens of Qwen2-VL. The final instruction-tuned Open-Qwen2VL outperforms partially-open state-of-the-art MLLM Qwen2-VL-2B on various multimodal benchmarks of MMBench, SEEDBench, MMstar, and MathVista, indicating the remarkable training efficiency of Open-Qwen2VL. We open-source all aspects of our work, including compute-efficient and data-efficient training details, data filtering methods, sequence packing scripts, pre-training data in WebDataset format, FSDP-based training codebase, and both base and instruction-tuned model checkpoints. We redefine\"fully open\"for multimodal LLMs as the complete release of: 1) the training codebase, 2) detailed data filtering techniques, and 3) all pre-training and supervised fine-tuning data used to develop the model.",
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "score": 0.0,
            "stype": "public_api",
            "pdf_hash": "",
            "rerank_score": 0.07684326171875
        },
        {
            "paperId": "054f4506663e05e6d3e6bcf55f02a2d9d014bcc5",
            "corpusId": 270357359,
            "title": "Large Language Model-guided Document Selection",
            "venue": "arXiv.org",
            "year": 2024,
            "referenceCount": 49,
            "citationCount": 4,
            "influentialCitationCount": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2406.04638, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2291470563",
                    "name": "Xiang Kong"
                },
                {
                    "authorId": "2238621478",
                    "name": "Tom Gunter"
                },
                {
                    "authorId": "2238621132",
                    "name": "Ruoming Pang"
                }
            ],
            "abstract": "Large Language Model (LLM) pre-training exhausts an ever growing compute budget, yet recent research has demonstrated that careful document selection enables comparable model quality with only a fraction of the FLOPs. Inspired by efforts suggesting that domain-specific training document selection is in fact an interpretable process [Gunasekar et al., 2023], as well as research showing that instruction-finetuned LLMs are adept zero-shot data labelers [Gilardi et al.,2023], we explore a promising direction for scalable general-domain document selection; employing a prompted LLM as a document grader, we distill quality labels into a classifier model, which is applied at scale to a large, and already heavily-filtered, web-crawl-derived corpus autonomously. Following the guidance of this classifier, we drop 75% of the corpus and train LLMs on the remaining data. Results across multiple benchmarks show that: 1. Filtering allows us to quality-match a model trained on the full corpus across diverse benchmarks with at most 70% of the FLOPs, 2. More capable LLM labelers and classifier models lead to better results that are less sensitive to the labeler's prompt, 3. In-context learning helps to boost the performance of less-capable labeling models. In all cases we use open-source datasets, models, recipes, and evaluation frameworks, so that results can be reproduced by the community.",
            "corpus_id": "270357359",
            "text": "Large Language Model (LLM) pre-training exhausts an ever growing compute budget, yet recent research has demonstrated that careful document selection enables comparable model quality with only a fraction of the FLOPs. Inspired by efforts suggesting that domain-specific training document selection is in fact an interpretable process [Gunasekar et al., 2023], as well as research showing that instruction-finetuned LLMs are adept zero-shot data labelers [Gilardi et al.,2023], we explore a promising direction for scalable general-domain document selection; employing a prompted LLM as a document grader, we distill quality labels into a classifier model, which is applied at scale to a large, and already heavily-filtered, web-crawl-derived corpus autonomously. Following the guidance of this classifier, we drop 75% of the corpus and train LLMs on the remaining data. Results across multiple benchmarks show that: 1. Filtering allows us to quality-match a model trained on the full corpus across diverse benchmarks with at most 70% of the FLOPs, 2. More capable LLM labelers and classifier models lead to better results that are less sensitive to the labeler's prompt, 3. In-context learning helps to boost the performance of less-capable labeling models. In all cases we use open-source datasets, models, recipes, and evaluation frameworks, so that results can be reproduced by the community.",
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "score": 0.0,
            "stype": "public_api",
            "pdf_hash": "",
            "rerank_score": 0.217529296875
        }
    ],
    "quotes": {
        "cost": 0.20020800000000002,
        "quotes": [
            {
                "idx": 0,
                "key": "[216914042 | Breit et al. | 2020 | Citations: 26]",
                "snippets": "As can be observed, BERT is clearly better than FastText in all measures. In fact, perhaps surprisingly, FastText does not perform better than a naive baseline that retrieves all instances as true...Interestingly, FastText faces a massive challenge in adapting domains and generalising from the general to the specific domains. However, BERT shows to be much more robust to domain changes.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {},
                "metadata": [
                    {
                        "section_title": "Results",
                        "pdf_hash": "",
                        "start": 55,
                        "end": 251,
                        "sentence_offsets": [
                            {
                                "start": 55,
                                "end": 128
                            },
                            {
                                "start": 129,
                                "end": 252
                            }
                        ],
                        "ref_mentions": [],
                        "quote": "As can be observed, BERT is clearly better than FastText in all measures. In fact, perhaps surprisingly, FastText does not perform better than a naive baseline that retrieves all instances as true"
                    },
                    {
                        "section_title": "Results",
                        "pdf_hash": "",
                        "start": 794,
                        "end": 985,
                        "sentence_offsets": [
                            {
                                "start": 794,
                                "end": 922
                            },
                            {
                                "start": 923,
                                "end": 984
                            }
                        ],
                        "ref_mentions": [],
                        "quote": "Interestingly, FastText faces a massive challenge in adapting domains and generalising from the general to the specific domains. However, BERT shows to be much more robust to domain changes."
                    }
                ]
            },
            {
                "idx": 1,
                "key": "[219558245 | Hettiarachchi et al. | 2020 | Citations: 33]",
                "snippets": "According to the obtained results, classic word embedding models (e.g. Skip-gram and fastText) learn the representations faster than transformer-based models (e.g. BERT and DistilBERT). \n\nComparing fastText and Skip-gram, fastText took more time because it processes subword information. But, incorporation of subwords allows this model to capture connections between modified words. For example, consider the goal-related words found within the top 20 words with high cluster change during a goal score: Skip-gramgoal, goalll, rashyyy, scores fastTextgoalll, goooaaalll, rashford, rashyyy, @marcusrashford, scored, scores fastText captures more modified words than Skip-gram. We could not run a complete evaluation using fastText embeddings, because it requires a manual process since GT keywords only contain the words in actual form. \n\nTransformer-based models took more time than both Skip-gram and fastText due to their complex architecture to learn contextualised word embeddings. DistilBERT is found to be faster than BERT, however, the learning time of DistilBERT is not fast enough for real-time processing because it exceeds the tweet generation time. For example to learn from tweets posted during a 2-minute time window, it took approximately 7.2 minutes. If this model can be further distilled, there is a possibility to achieve the required efficiency to become suitable for real-time processing. However, further distillation can reduce the language understanding capability of the model as there is a 3% reduction in DistilBERT compared to BERT (Sanh et al. 2019).",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {},
                "metadata": [
                    {
                        "section_title": "Extension to other word embedding models",
                        "pdf_hash": "",
                        "start": 0,
                        "end": 1580,
                        "sentence_offsets": [
                            {
                                "start": 0,
                                "end": 70
                            },
                            {
                                "start": 71,
                                "end": 163
                            },
                            {
                                "start": 164,
                                "end": 185
                            },
                            {
                                "start": 188,
                                "end": 287
                            },
                            {
                                "start": 288,
                                "end": 383
                            },
                            {
                                "start": 384,
                                "end": 676
                            },
                            {
                                "start": 677,
                                "end": 836
                            },
                            {
                                "start": 839,
                                "end": 986
                            },
                            {
                                "start": 987,
                                "end": 1161
                            },
                            {
                                "start": 1162,
                                "end": 1267
                            },
                            {
                                "start": 1268,
                                "end": 1410
                            },
                            {
                                "start": 1411,
                                "end": 1580
                            }
                        ],
                        "ref_mentions": [],
                        "quote": "According to the obtained results, classic word embedding models (e.g. Skip-gram and fastText) learn the representations faster than transformer-based models (e.g. BERT and DistilBERT). \n\nComparing fastText and Skip-gram, fastText took more time because it processes subword information. But, incorporation of subwords allows this model to capture connections between modified words. For example, consider the goal-related words found within the top 20 words with high cluster change during a goal score: Skip-gramgoal, goalll, rashyyy, scores fastTextgoalll, goooaaalll, rashford, rashyyy, @marcusrashford, scored, scores fastText captures more modified words than Skip-gram. We could not run a complete evaluation using fastText embeddings, because it requires a manual process since GT keywords only contain the words in actual form. \n\nTransformer-based models took more time than both Skip-gram and fastText due to their complex architecture to learn contextualised word embeddings. DistilBERT is found to be faster than BERT, however, the learning time of DistilBERT is not fast enough for real-time processing because it exceeds the tweet generation time. For example to learn from tweets posted during a 2-minute time window, it took approximately 7.2 minutes. If this model can be further distilled, there is a possibility to achieve the required efficiency to become suitable for real-time processing. However, further distillation can reduce the language understanding capability of the model as there is a 3% reduction in DistilBERT compared to BERT (Sanh et al. 2019)."
                    }
                ]
            },
            {
                "idx": 2,
                "key": "[221319787 | Loureiro et al. | 2020 | Citations: 14]",
                "snippets": "We also include two FastText linear classifiers (Joulin et al. 2017) as baselines: FTX-B (base model without pre-trained embeddings) and FTX-C (using pre-trained embeddings from Common Crawl). We chose FastText as baseline given its efficiency and competitive results for sentence classification.\n\nWord-specific results for different configurations of BERT and the FastText baseline are shown in Table 5. In general, results are high for all BERT-based models, over 90% in most cases. This reinforces the potential of language models for WSD, both in its lightweight 1NN and in the fine-tuning settings. in all BERT configurations.\n\nMFS Bias. As expected, macro F1 results degrade for the purely supervised classification models (FastText and fine-tuned BERT), indicating the inherent sense biases captured by the model which lead to lowered performance for the obscure senses (see the work by Postma et al. (2016) for a more thorough analysis on this issue). However, BERT proves to be much more robust with this respect whereas FastText suffers heavily (highlighted in the macro setting).",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {},
                "metadata": [
                    {
                        "quote": "We also include two FastText linear classifiers (Joulin et al. 2017) as baselines: FTX-B (base model without pre-trained embeddings) and FTX-C (using pre-trained embeddings from Common Crawl). We chose FastText as baseline given its efficiency and competitive results for sentence classification.\n\nWord-specific results for different configurations of BERT and the FastText baseline are shown in Table 5. In general, results are high for all BERT-based models, over 90% in most cases. This reinforces the potential of language models for WSD, both in its lightweight 1NN and in the fine-tuning settings. in all BERT configurations.\n\nMFS Bias. As expected, macro F1 results degrade for the purely supervised classification models (FastText and fine-tuned BERT), indicating the inherent sense biases captured by the model which lead to lowered performance for the obscure senses (see the work by Postma et al. (2016) for a more thorough analysis on this issue). However, BERT proves to be much more robust with this respect whereas FastText suffers heavily (highlighted in the macro setting).",
                        "pdf_hash": ""
                    }
                ]
            },
            {
                "idx": 3,
                "key": "[226337596 | Kapociute-Dzikiene et al. | 2020 | Citations: 13]",
                "snippets": "As seen from Figure 2, BERT vectorization is a better choice compared to fastText for all morphologically complex languages for all datasets, and this is not surprising. Morphologically complex languages (especially fusional languages) suffer from disambiguation problems, but BERT has mechanisms that are able to vectorize even those words that are written the same but have different meanings, depending on their context, differently. Despite the fact that fastText embeddings are also trained to consider a context around the target word, that context is restricted to only a few words. Despite this, fastText is a suitable vectorization solution for languages (such as English) with strict word order in a sentence. In contrast, BERT is able to consider a much broader context (words, sentences, their order) compared to fastText and is, therefore, more suitable for languages that have a relatively free word order in a sentence (such as Latvian, Lithuanian, and Russian).",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {},
                "metadata": [
                    {
                        "section_title": "Discussion",
                        "pdf_hash": "",
                        "start": 642,
                        "end": 1619,
                        "sentence_offsets": [
                            {
                                "start": 642,
                                "end": 811
                            },
                            {
                                "start": 812,
                                "end": 1078
                            },
                            {
                                "start": 1079,
                                "end": 1231
                            },
                            {
                                "start": 1232,
                                "end": 1361
                            },
                            {
                                "start": 1362,
                                "end": 1619
                            }
                        ],
                        "ref_mentions": [],
                        "quote": "As seen from Figure 2, BERT vectorization is a better choice compared to fastText for all morphologically complex languages for all datasets, and this is not surprising. Morphologically complex languages (especially fusional languages) suffer from disambiguation problems, but BERT has mechanisms that are able to vectorize even those words that are written the same but have different meanings, depending on their context, differently. Despite the fact that fastText embeddings are also trained to consider a context around the target word, that context is restricted to only a few words. Despite this, fastText is a suitable vectorization solution for languages (such as English) with strict word order in a sentence. In contrast, BERT is able to consider a much broader context (words, sentences, their order) compared to fastText and is, therefore, more suitable for languages that have a relatively free word order in a sentence (such as Latvian, Lithuanian, and Russian)."
                    }
                ]
            },
            {
                "idx": 4,
                "key": "[227230531 | Baruah et al. | 2020 | Citations: 2]",
                "snippets": "The word embeddings produced by fastText is static in nature. Each word has a single embedding irrespective of the context in which the word appears. Static embeddings fail to handle polysemy. The embeddings produced by BERT are contextualized embeddings. The same word may have multiple embeddings depending on the context in which it appears.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {},
                "metadata": [
                    {
                        "section_title": "BERT",
                        "pdf_hash": "",
                        "start": 200,
                        "end": 544,
                        "sentence_offsets": [
                            {
                                "start": 200,
                                "end": 261
                            },
                            {
                                "start": 262,
                                "end": 349
                            },
                            {
                                "start": 350,
                                "end": 392
                            },
                            {
                                "start": 393,
                                "end": 455
                            },
                            {
                                "start": 456,
                                "end": 544
                            }
                        ],
                        "ref_mentions": [],
                        "quote": "The word embeddings produced by fastText is static in nature. Each word has a single embedding irrespective of the context in which the word appears. Static embeddings fail to handle polysemy. The embeddings produced by BERT are contextualized embeddings. The same word may have multiple embeddings depending on the context in which it appears."
                    }
                ]
            },
            {
                "idx": 5,
                "key": "[227231089 | Edwards et al. | 2020 | Citations: 25]",
                "snippets": "In settings with small training datasets a simple method like fastText coupled with domain-specific word embeddings performs equally well or better than BERT, even when pre-trained on domain-specific data.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {},
                "metadata": [
                    {
                        "section_title": "abstract",
                        "pdf_hash": "",
                        "start": 828,
                        "end": 1033,
                        "sentence_offsets": [],
                        "ref_mentions": [],
                        "quote": "In settings with small training datasets a simple method like fastText coupled with domain-specific word embeddings performs equally well or better than BERT, even when pre-trained on domain-specific data."
                    }
                ]
            },
            {
                "idx": 6,
                "key": "[232278102 | Loureiro et al. | 2021 | Citations: 72]",
                "snippets": "We also include two FastText linear classifiers (Joulin et al., 2016)) as baselines: FTX-B (base model without pretrained embeddings) and FTX-C (using pretrained embeddings from Common Crawl). We chose FastText as the baseline given its efficiency and competitive results for sentence classification...In general, results are high for all Transformerbased models, over 90% in most cases. This reinforces the potential of language models for WSD, both in its light-weight 1NN and in the fine-tuning settings...Most Frequent Sense (MFS) bias. As expected, macro-F1 results degrade for the purely supervised classification models (FastText and fine-tuned BERT), indicating the inherent sense bias...In addition to our BERT-based model, we include results for two Fast-Text supervised classifiers (Joulin et al., 2016)) as baselines: a basic one with random initialization (FastText-B) and another initialized with FastText embeddings trained on the Common Crawl (FastText-C)...We can see that BERT significantly outperforms the FastText static word embedding.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {
                    "[1210515 | Joulin et al. | 2016 | Citations: 4631]": "This paper explores a simple and efficient baseline for text classification. Our experiments show that our fast text classifier fastText is often on par with deep learning classifiers in terms of accuracy, and many orders of magnitude faster for training and evaluation. We can train fastText on more than one billion words in less than ten minutes using a standard multicore CPU, and classify half a million sentences among 312K classes in less than a minute."
                },
                "metadata": [
                    {
                        "section_title": "Experimental Setting",
                        "pdf_hash": "",
                        "start": 1353,
                        "end": 1650,
                        "sentence_offsets": [],
                        "ref_mentions": [
                            "1210515"
                        ],
                        "quote": "We also include two FastText linear classifiers (Joulin et al., 2016)) as baselines: FTX-B (base model without pretrained embeddings) and FTX-C (using pretrained embeddings from Common Crawl). We chose FastText as the baseline given its efficiency and competitive results for sentence classification"
                    },
                    {
                        "section_title": "Results",
                        "pdf_hash": "",
                        "start": 125,
                        "end": 330,
                        "sentence_offsets": [],
                        "ref_mentions": [],
                        "quote": "In general, results are high for all Transformerbased models, over 90% in most cases. This reinforces the potential of language models for WSD, both in its light-weight 1NN and in the fine-tuning settings"
                    },
                    {
                        "quote": "Most Frequent Sense (MFS) bias. As expected, macro-F1 results degrade for the purely supervised classification models (FastText and fine-tuned BERT), indicating the inherent sense bias",
                        "pdf_hash": ""
                    },
                    {
                        "section_title": "Lowerbound",
                        "pdf_hash": "",
                        "start": 754,
                        "end": 1028,
                        "sentence_offsets": [],
                        "ref_mentions": [
                            "1210515"
                        ],
                        "quote": "In addition to our BERT-based model, we include results for two Fast-Text supervised classifiers (Joulin et al., 2016)) as baselines: a basic one with random initialization (FastText-B) and another initialized with FastText embeddings trained on the Common Crawl (FastText-C)"
                    },
                    {
                        "section_title": "Lowerbound",
                        "pdf_hash": "",
                        "start": 1314,
                        "end": 1397,
                        "sentence_offsets": [],
                        "ref_mentions": [],
                        "quote": "We can see that BERT significantly outperforms the FastText static word embedding."
                    }
                ]
            },
            {
                "idx": 7,
                "key": "[237253665 | Hussain et al. | 2021 | Citations: 24]",
                "snippets": "BERT outperforms both CNN and LSTM. The reason for the better performance of BERT is that it learns contextualized embeddings in a bidirectional way. In natural language, a word is likely to convey multiple meanings based on the context in which it is used. Word2vec, fasttext, and glove produce the same representations of a word even if it has different meanings in different contexts. BERT, on the other hand, produces context-dependent embeddings of a word.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {},
                "metadata": [
                    {
                        "section_title": "Experiments and Results",
                        "pdf_hash": "",
                        "start": 587,
                        "end": 1048,
                        "sentence_offsets": [
                            {
                                "start": 587,
                                "end": 622
                            },
                            {
                                "start": 623,
                                "end": 736
                            },
                            {
                                "start": 737,
                                "end": 844
                            },
                            {
                                "start": 845,
                                "end": 974
                            },
                            {
                                "start": 975,
                                "end": 1048
                            }
                        ],
                        "ref_mentions": [],
                        "quote": "BERT outperforms both CNN and LSTM. The reason for the better performance of BERT is that it learns contextualized embeddings in a bidirectional way. In natural language, a word is likely to convey multiple meanings based on the context in which it is used. Word2vec, fasttext, and glove produce the same representations of a word even if it has different meanings in different contexts. BERT, on the other hand, produces context-dependent embeddings of a word."
                    }
                ]
            },
            {
                "idx": 8,
                "key": "[237485425 | Mosel et al. | 2021 | Citations: 68]",
                "snippets": "The results show that seBERT and BERToverflow achieve the best performance for the issue type prediction tasks, outperforming both fastText and the general-domain BERT models. The improvement over fastText is very large with an about 11% higher F1 score for the issue type prediction. The violin plot in Figure 5 indicates that the performance improvement in F1 score is due to an improvement of both recall and precision, which means the models reduced both false positives and false negatives in comparison to fastText. The Bayesian signed rank test determined that this improvement of the SE-specific models over the other models is significant.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {},
                "metadata": [
                    {
                        "section_title": "Issue Type Prediction",
                        "pdf_hash": "",
                        "start": 0,
                        "end": 648,
                        "sentence_offsets": [
                            {
                                "start": 0,
                                "end": 175
                            },
                            {
                                "start": 176,
                                "end": 284
                            },
                            {
                                "start": 285,
                                "end": 521
                            },
                            {
                                "start": 522,
                                "end": 651
                            }
                        ],
                        "ref_mentions": [],
                        "quote": "The results show that seBERT and BERToverflow achieve the best performance for the issue type prediction tasks, outperforming both fastText and the general-domain BERT models. The improvement over fastText is very large with an about 11% higher F1 score for the issue type prediction. The violin plot in Figure 5 indicates that the performance improvement in F1 score is due to an improvement of both recall and precision, which means the models reduced both false positives and false negatives in comparison to fastText. The Bayesian signed rank test determined that this improvement of the SE-specific models over the other models is significant."
                    }
                ]
            },
            {
                "idx": 9,
                "key": "[238648863 | Jbene et al. | 2021 | Citations: 6]",
                "snippets": "Comparing the results of the model using the different kinds of embeddings, the version with BERT embeddings generates results higher than FastText ones for all the datasets, which can be explained by the difference in dataset size on which the two models were pre-trained on, and most importantly, the capability of BERT in capturing and generating context-dependent word embeddings.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {},
                "metadata": [
                    {
                        "section_title": "Results and Discussions",
                        "pdf_hash": "",
                        "start": 795,
                        "end": 1179,
                        "sentence_offsets": [
                            {
                                "start": 795,
                                "end": 1179
                            }
                        ],
                        "ref_mentions": [],
                        "quote": "Comparing the results of the model using the different kinds of embeddings, the version with BERT embeddings generates results higher than FastText ones for all the datasets, which can be explained by the difference in dataset size on which the two models were pre-trained on, and most importantly, the capability of BERT in capturing and generating context-dependent word embeddings."
                    }
                ]
            },
            {
                "idx": 10,
                "key": "[244895506 | Durairaj et al. | 2021 | Citations: 29]",
                "snippets": "The proposed BERT model outperforms in terms of accuracy and model performance compare to other models. The results of the fastText model showed low accuracy when unigram and bigram methods were used for training the model. The overall model training and data preparation tasks took less time for BERT model in comparison to others. This experiment reveals that the BERT model required more computational resources to train compared with other traditional models. The fastText model performed well with a standard server environment with minimal computational resources compare to other models.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {},
                "metadata": [
                    {
                        "section_title": "V. CONCLUSION AND FUTURE WORK",
                        "pdf_hash": "",
                        "start": 0,
                        "end": 594,
                        "sentence_offsets": [
                            {
                                "start": 0,
                                "end": 103
                            },
                            {
                                "start": 104,
                                "end": 223
                            },
                            {
                                "start": 224,
                                "end": 332
                            },
                            {
                                "start": 333,
                                "end": 463
                            },
                            {
                                "start": 464,
                                "end": 594
                            }
                        ],
                        "ref_mentions": [],
                        "quote": "The proposed BERT model outperforms in terms of accuracy and model performance compare to other models. The results of the fastText model showed low accuracy when unigram and bigram methods were used for training the model. The overall model training and data preparation tasks took less time for BERT model in comparison to others. This experiment reveals that the BERT model required more computational resources to train compared with other traditional models. The fastText model performed well with a standard server environment with minimal computational resources compare to other models."
                    }
                ]
            },
            {
                "idx": 11,
                "key": "[246608158 | Ivanov et al. | 2022 | Citations: 11]",
                "snippets": "Result of the first phase of evaluation is presented in Table 1. As it was expected, more advanced model (BERT) showed better results in terms of F1-score. BERT-based model showed high precision (0.92) and lower recall (0.8). Results of the BERT model is available at https://bit.ly/3oPElMm. However, the values of precision and recall metrics behave differently for the fastText and ELMobased baselines. fastText-based classifier showed better Recall (0.93) comparing with other architectures. This property might be useful in some cases when it is necessary to extract more relevant sentences and text patterns associated with requirements.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {},
                "metadata": [
                    {
                        "section_title": "Experiments with PURE documents",
                        "pdf_hash": "",
                        "start": 218,
                        "end": 860,
                        "sentence_offsets": [
                            {
                                "start": 218,
                                "end": 282
                            },
                            {
                                "start": 283,
                                "end": 373
                            },
                            {
                                "start": 374,
                                "end": 509
                            },
                            {
                                "start": 510,
                                "end": 622
                            },
                            {
                                "start": 623,
                                "end": 712
                            },
                            {
                                "start": 713,
                                "end": 860
                            }
                        ],
                        "ref_mentions": [],
                        "quote": "Result of the first phase of evaluation is presented in Table 1. As it was expected, more advanced model (BERT) showed better results in terms of F1-score. BERT-based model showed high precision (0.92) and lower recall (0.8). Results of the BERT model is available at https://bit.ly/3oPElMm. However, the values of precision and recall metrics behave differently for the fastText and ELMobased baselines. fastText-based classifier showed better Recall (0.93) comparing with other architectures. This property might be useful in some cases when it is necessary to extract more relevant sentences and text patterns associated with requirements."
                    }
                ]
            },
            {
                "idx": 12,
                "key": "[246738423 | Guo et al. | 2022 | Citations: 8]",
                "snippets": "In this paper, we use knowledge of the ERNIE model to distill the FastText model; the ERNIE model works as a teacher model to predict the massive online unlabeled data for data enhancement, and then guides the training of the student model of FastText with better computational efficiency. The FastText model is distilled by the ERNIE model in chatbot intention classification. This not only guarantees the superiority of its original computational performance, but also the intention classification accuracy has been significantly improved.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {},
                "metadata": [
                    {
                        "quote": "In this paper, we use knowledge of the ERNIE model to distill the FastText model; the ERNIE model works as a teacher model to predict the massive online unlabeled data for data enhancement, and then guides the training of the student model of FastText with better computational efficiency. The FastText model is distilled by the ERNIE model in chatbot intention classification. This not only guarantees the superiority of its original computational performance, but also the intention classification accuracy has been significantly improved.",
                        "pdf_hash": "",
                        "section_title": "abstract"
                    }
                ]
            },
            {
                "idx": 13,
                "key": "[252901000 | Groger et al. | 2022 | Citations: 0]",
                "snippets": "The best performing model consists of pre-trained FastText embeddings followed by a stack of bidirectional LSTM layers. This rather simple architecture performs more that 20% better in F 1 compared to a support vector machine baseline with TF-IDF feature engineering and also beats transfer learning with a pretrained BERT model 3 by a small margin. On the one hand, this indicates that such large language models as BERT can generalize pretty well even in the presence of small data. On the other hand, BERT comes with an increase of 160% in the number of parameters and thus significantly higher inference time compared to the winning model. Hence, even with more data and an expected increase in the BERT model performance, the LSTM architecture will show the better performance-runtime tradeoff.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {},
                "metadata": [
                    {
                        "section_title": "IV. RESULTS",
                        "pdf_hash": "",
                        "start": 151,
                        "end": 950,
                        "sentence_offsets": [],
                        "ref_mentions": [],
                        "quote": "The best performing model consists of pre-trained FastText embeddings followed by a stack of bidirectional LSTM layers. This rather simple architecture performs more that 20% better in F 1 compared to a support vector machine baseline with TF-IDF feature engineering and also beats transfer learning with a pretrained BERT model 3 by a small margin. On the one hand, this indicates that such large language models as BERT can generalize pretty well even in the presence of small data. On the other hand, BERT comes with an increase of 160% in the number of parameters and thus significantly higher inference time compared to the winning model. Hence, even with more data and an expected increase in the BERT model performance, the LSTM architecture will show the better performance-runtime tradeoff."
                    }
                ]
            },
            {
                "idx": 14,
                "key": "[253015767 | Frissen et al. | 2022 | Citations: 19]",
                "snippets": "The results indicate that the RF classifier with BERT word embeddings as textual feature achieved the best performance. This illustrates that contextual word embedding representations such as BERT had a superior performance over the non-contextual word embeddings such as FastText and Word-2vec. We also observe that tree-based (Random Forest and Decision Tree) classifiers had a better performance in classifying biased and discriminatory language as compared to the remaining classifiers. Among the textual features, word embedding representations BERT, FastText and ELMo in combination with the RF classifier had the best performance. This was followed by FastText, ELMo and Flair word embeddings in combination with the DT classifier.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {},
                "metadata": [
                    {
                        "section_title": "Results and analysis",
                        "pdf_hash": "",
                        "start": 350,
                        "end": 1088,
                        "sentence_offsets": [
                            {
                                "start": 350,
                                "end": 469
                            },
                            {
                                "start": 470,
                                "end": 645
                            },
                            {
                                "start": 646,
                                "end": 840
                            },
                            {
                                "start": 841,
                                "end": 987
                            },
                            {
                                "start": 988,
                                "end": 1088
                            }
                        ],
                        "ref_mentions": [],
                        "quote": "The results indicate that the RF classifier with BERT word embeddings as textual feature achieved the best performance. This illustrates that contextual word embedding representations such as BERT had a superior performance over the non-contextual word embeddings such as FastText and Word-2vec. We also observe that tree-based (Random Forest and Decision Tree) classifiers had a better performance in classifying biased and discriminatory language as compared to the remaining classifiers. Among the textual features, word embedding representations BERT, FastText and ELMo in combination with the RF classifier had the best performance. This was followed by FastText, ELMo and Flair word embeddings in combination with the DT classifier."
                    }
                ]
            },
            {
                "idx": 15,
                "key": "[253860591 | Melnyk et al. | 2022 | Citations: 1]",
                "snippets": "the BERT model scored better in all of these settings and in classifying unseen data, reaching an F1 score of 0.80 and outperforming FastText by 9.4%.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {},
                "metadata": [
                    {
                        "section_title": "SENTIMENT ANALYSIS WITH BERT",
                        "pdf_hash": "",
                        "start": 1594,
                        "end": 1744,
                        "sentence_offsets": [
                            {
                                "start": 1376,
                                "end": 1744
                            }
                        ],
                        "ref_mentions": [],
                        "quote": "the BERT model scored better in all of these settings and in classifying unseen data, reaching an F1 score of 0.80 and outperforming FastText by 9.4%."
                    }
                ]
            },
            {
                "idx": 16,
                "key": "[260379057 | Ning et al. | 2023 | Citations: 0]",
                "snippets": "We find that the FastText model can do better on the training set and testing set (T+0 day), while the BERT model performs better on the testing set (T+1 day). As 99.89% of test queries on T+0 day have appeared in the training set, it means that the FastText model can be remembered better. On the other hand, as only 57.47% of test queries on T+1 day has exposed in the training set, it means that the BERT model can generalize better on some unseen or low-frequency search query. In other words, in the E-commerce search query classification task, the FastText is better at remembering while the BERT is better at generalization.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {},
                "metadata": [
                    {
                        "section_title": "A Closer Look on FastText and BERT Models",
                        "pdf_hash": "",
                        "start": 1313,
                        "end": 1944,
                        "sentence_offsets": [
                            {
                                "start": 1313,
                                "end": 1472
                            },
                            {
                                "start": 1473,
                                "end": 1603
                            },
                            {
                                "start": 1604,
                                "end": 1794
                            },
                            {
                                "start": 1795,
                                "end": 1944
                            }
                        ],
                        "ref_mentions": [],
                        "quote": "We find that the FastText model can do better on the training set and testing set (T+0 day), while the BERT model performs better on the testing set (T+1 day). As 99.89% of test queries on T+0 day have appeared in the training set, it means that the FastText model can be remembered better. On the other hand, as only 57.47% of test queries on T+1 day has exposed in the training set, it means that the BERT model can generalize better on some unseen or low-frequency search query. In other words, in the E-commerce search query classification task, the FastText is better at remembering while the BERT is better at generalization."
                    }
                ]
            },
            {
                "idx": 17,
                "key": "[265162335 | Wildemann et al. | 2023 | Citations: 0]",
                "snippets": "Overall, BERT achieves the best performance on the F 1 measure in all datasets except for Bank...fastText proves to be the second-best classifier and even outperforms BERT on the Bank dataset. The additional use of pre-trained word embeddings clearly benefits the task and shows an average increase in F 1 -score of 15.3% across all datasets. This suggests a further improvement when using larger domain-specific corpora. Moreover, its fast training and the lack of dependency on GPUs may be advantageous for many practical applications.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {
                    "[209475786 | Zhu et al. | 2019 | Citations: 442]": "Adversarial training, which minimizes the maximal risk for label-preserving input perturbations, has proved to be effective for improving the generalization of language models. In this work, we propose a novel adversarial training algorithm, FreeLB, that promotes higher invariance in the embedding space, by adding adversarial perturbations to word embeddings and minimizing the resultant adversarial risk inside different regions around input samples. To validate the effectiveness of the proposed approach, we apply it to Transformer-based models for natural language understanding and commonsense reasoning tasks. Experiments on the GLUE benchmark show that when applied only to the finetuning stage, it is able to improve the overall test scores of BERT-base model from 78.3 to 79.4, and RoBERTa-large model from 88.5 to 88.8. In addition, the proposed approach achieves state-of-the-art single-model test accuracies of 85.44\\% and 67.75\\% on ARC-Easy and ARC-Challenge. Experiments on CommonsenseQA benchmark further demonstrate that FreeLB can be generalized and boost the performance of RoBERTa-large model on other tasks as well. Code is available at \\url{this https URL .",
                    "[220045835 | Pruksachatkun et al. | 2020 | Citations: 197]": "While pretrained models such as BERT have shown large gains across natural language understanding tasks, their performance can be improved by further training the model on a data-rich intermediate task, before fine-tuning it on a target task. However, it is still poorly understood when and why intermediate-task training is beneficial for a given target task. To investigate this, we perform a large-scale study on the pretrained RoBERTa model with 110 intermediate-target task combinations. We further evaluate all trained models with 25 probing tasks meant to reveal the specific skills that drive transfer. We observe that intermediate tasks requiring high-level inference and reasoning abilities tend to work best. We also observe that target task performance is strongly correlated with higher-level abilities such as coreference resolution. However, we fail to observe more granular correlations between probing and target task performance, highlighting the need for further work on broad-coverage probing benchmarks. We also observe evidence that the forgetting of knowledge learned during pretraining may limit our analysis, highlighting the need for further work on transfer learning methods in these settings."
                },
                "metadata": [
                    {
                        "section_title": "Classifier Performance",
                        "pdf_hash": "",
                        "start": 412,
                        "end": 506,
                        "sentence_offsets": [
                            {
                                "start": 412,
                                "end": 507
                            }
                        ],
                        "ref_mentions": [],
                        "quote": "Overall, BERT achieves the best performance on the F 1 measure in all datasets except for Bank"
                    },
                    {
                        "section_title": "Classifier Performance",
                        "pdf_hash": "",
                        "start": 772,
                        "end": 1213,
                        "sentence_offsets": [
                            {
                                "start": 615,
                                "end": 807
                            },
                            {
                                "start": 807,
                                "end": 970
                            },
                            {
                                "start": 972,
                                "end": 1067
                            },
                            {
                                "start": 1067,
                                "end": 1216
                            }
                        ],
                        "ref_mentions": [
                            "220045835",
                            "209475786"
                        ],
                        "quote": "fastText proves to be the second-best classifier and even outperforms BERT on the Bank dataset. The additional use of pre-trained word embeddings clearly benefits the task and shows an average increase in F 1 -score of 15.3% across all datasets. This suggests a further improvement when using larger domain-specific corpora. Moreover, its fast training and the lack of dependency on GPUs may be advantageous for many practical applications."
                    }
                ]
            },
            {
                "idx": 18,
                "key": "[266566817 | Bo et al. | 2024 | Citations: 2]",
                "snippets": "In contrast, BERT is a pre-trained language model trained on a large-scale corpus, based on the multi-layer Transformer encoder architecture, utilizing attention mechanisms to directly encode word meanings, effectively addressing the polysemy issue based on contextual information (Xinxi, 2021). BERT places greater emphasis on pre-training word meanings, allowing downstream NLP tasks to perform fine-tuning operations based on the specific task's requirements.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {
                    "[239459434 | Xinxi | 2021 | Citations: 5]": "Over the past decades, natural language processing (NLP) has been a hot topic in many fields, e.g., sentiment analysis and news topic classification. As a very powerful language pre-training model, Bidirectional Encoder Representations from Transformers (BERT) has achieved promising results in many language understanding tasks including text classification. However, fine-tune BERT to adapt different text classification task efficiently is a critical problem that needs improvement. In this paper, a general solution is proposed for BERT fine-tuning on single text classification task. Compared with other traditional fine-tune strategies without any pre-training step, the performance of BERT is boosted by pre-training withintask data. Moreover, the proposed solution obtains superior results on six widely-used text classification datasets."
                },
                "metadata": [
                    {
                        "section_title": "A. WORD EMBEDDING",
                        "pdf_hash": "",
                        "start": 1590,
                        "end": 2043,
                        "sentence_offsets": [
                            {
                                "start": 1590,
                                "end": 1876
                            },
                            {
                                "start": 1877,
                                "end": 2043
                            }
                        ],
                        "ref_mentions": [
                            "239459434"
                        ],
                        "quote": "In contrast, BERT is a pre-trained language model trained on a large-scale corpus, based on the multi-layer Transformer encoder architecture, utilizing attention mechanisms to directly encode word meanings, effectively addressing the polysemy issue based on contextual information (Xinxi, 2021). BERT places greater emphasis on pre-training word meanings, allowing downstream NLP tasks to perform fine-tuning operations based on the specific task's requirements."
                    }
                ]
            },
            {
                "idx": 19,
                "key": "[268819482 | Jahan et al. | 2024 | Citations: 6]",
                "snippets": "The comparison reveals that, in the absence of data augmentation, BERT marginally surpasses CNN, showing a slight improvement of 0.9% in accuracy and 1.0% in F1 score...Initially, the impact of data augmentation methods such as WordNet, FastText, and BERT, when applied without BERT-cosine similarity, shows minimal to no improvements in accuracy and F1 scores...Among the various data augmentation methods evaluated, back-translation demonstrated the best performance, showing a 0.5% improvement despite generating a smaller number of augmented sentences compared to WordNet, FastText, and BERT-masking methods.One possible explanation for this is that, although back-translation produces fewer sentences, it results in less label alteration compared to other methods, as observed in the previous table 12.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {},
                "metadata": [
                    {
                        "section_title": "Classification Results",
                        "pdf_hash": "",
                        "start": 504,
                        "end": 670,
                        "sentence_offsets": [
                            {
                                "start": 504,
                                "end": 671
                            }
                        ],
                        "ref_mentions": [],
                        "quote": "The comparison reveals that, in the absence of data augmentation, BERT marginally surpasses CNN, showing a slight improvement of 0.9% in accuracy and 1.0% in F1 score"
                    },
                    {
                        "section_title": "Classification Results",
                        "pdf_hash": "",
                        "start": 1227,
                        "end": 1418,
                        "sentence_offsets": [
                            {
                                "start": 1227,
                                "end": 1419
                            }
                        ],
                        "ref_mentions": [],
                        "quote": "Initially, the impact of data augmentation methods such as WordNet, FastText, and BERT, when applied without BERT-cosine similarity, shows minimal to no improvements in accuracy and F1 scores"
                    },
                    {
                        "section_title": "Classification Results",
                        "pdf_hash": "",
                        "start": 1716,
                        "end": 2160,
                        "sentence_offsets": [
                            {
                                "start": 1716,
                                "end": 1965
                            },
                            {
                                "start": 1965,
                                "end": 2160
                            }
                        ],
                        "ref_mentions": [],
                        "quote": "Among the various data augmentation methods evaluated, back-translation demonstrated the best performance, showing a 0.5% improvement despite generating a smaller number of augmented sentences compared to WordNet, FastText, and BERT-masking methods.One possible explanation for this is that, although back-translation produces fewer sentences, it results in less label alteration compared to other methods, as observed in the previous table 12."
                    }
                ]
            },
            {
                "idx": 20,
                "key": "[269973968 | Xie et al. | 2024 | Citations: 1]",
                "snippets": "When comparing VSM-based methods with Word2Vec-based methods, we equate MLP and FastText due to their similar complexities as shallow neural networks. FastText, however, demonstrates better performance than MLP (0.696 vs 0.670 in Acc and 0.767 vs 0.727 in F 1 ), indicating Word2Vec's superior word representation capabilities...Interestingly, original BERT, despite its wide-ranging successes in natural language processing, does not show a marked advantage over Word2Vec in our analysis and is slightly outperformed by TextCNN. This observation is further evidenced by the improved performance of MLM BERT over the original BERT, suggesting that contextual word embeddings benefit from additional data and training. This leads us to conclude that for specialized domain language tasks like clinical text analysis, pre-training is essential to fully leverage BERT's capabilities.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {},
                "metadata": [
                    {
                        "section_title": "Discussions",
                        "pdf_hash": "",
                        "start": 206,
                        "end": 532,
                        "sentence_offsets": [
                            {
                                "start": 206,
                                "end": 356
                            },
                            {
                                "start": 357,
                                "end": 533
                            }
                        ],
                        "ref_mentions": [],
                        "quote": "When comparing VSM-based methods with Word2Vec-based methods, we equate MLP and FastText due to their similar complexities as shallow neural networks. FastText, however, demonstrates better performance than MLP (0.696 vs 0.670 in Acc and 0.767 vs 0.727 in F 1 ), indicating Word2Vec's superior word representation capabilities"
                    },
                    {
                        "section_title": "Discussions",
                        "pdf_hash": "",
                        "start": 1051,
                        "end": 1603,
                        "sentence_offsets": [
                            {
                                "start": 1051,
                                "end": 1251
                            },
                            {
                                "start": 1252,
                                "end": 1439
                            },
                            {
                                "start": 1440,
                                "end": 1602
                            }
                        ],
                        "ref_mentions": [],
                        "quote": "Interestingly, original BERT, despite its wide-ranging successes in natural language processing, does not show a marked advantage over Word2Vec in our analysis and is slightly outperformed by TextCNN. This observation is further evidenced by the improved performance of MLM BERT over the original BERT, suggesting that contextual word embeddings benefit from additional data and training. This leads us to conclude that for specialized domain language tasks like clinical text analysis, pre-training is essential to fully leverage BERT's capabilities."
                    }
                ]
            },
            {
                "idx": 21,
                "key": "[272359317 | Malik et al. | 2024 | Citations: 1]",
                "snippets": "In the proposed work, three important issues in the field of natural language processing: polysemy, contextual meaning, and out-of-vocabulary terms were addressed. We employed three cutting-edge models to solve these problems: FastText, Elmo, and RoBERTa. \n\nFastText is a text categorization and representation learning library that is intended to be quick and efficient. It handles out-of-vocabulary terms using sub-word information, making it appropriate for NLP jobs. Embeddings from Language Models (ELMO) is a contextualized word representation model that learns word contexts from large-scale text corpora. It captures the contextual meaning of words, making it ideally suited for NLP jobs requiring context awareness. A Robustly Optimized BERT Pre-training Approach (Roberta) is a transformer-based language model that has been refined using large-scale text corpora.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {},
                "metadata": [
                    {
                        "section_title": "Comparative analysis of the proposed model with existing techniques",
                        "pdf_hash": "",
                        "start": 0,
                        "end": 874,
                        "sentence_offsets": [
                            {
                                "start": 0,
                                "end": 163
                            },
                            {
                                "start": 164,
                                "end": 255
                            },
                            {
                                "start": 258,
                                "end": 371
                            },
                            {
                                "start": 372,
                                "end": 470
                            },
                            {
                                "start": 471,
                                "end": 612
                            },
                            {
                                "start": 613,
                                "end": 724
                            },
                            {
                                "start": 725,
                                "end": 874
                            }
                        ],
                        "ref_mentions": [],
                        "quote": "In the proposed work, three important issues in the field of natural language processing: polysemy, contextual meaning, and out-of-vocabulary terms were addressed. We employed three cutting-edge models to solve these problems: FastText, Elmo, and RoBERTa. \n\nFastText is a text categorization and representation learning library that is intended to be quick and efficient. It handles out-of-vocabulary terms using sub-word information, making it appropriate for NLP jobs. Embeddings from Language Models (ELMO) is a contextualized word representation model that learns word contexts from large-scale text corpora. It captures the contextual meaning of words, making it ideally suited for NLP jobs requiring context awareness. A Robustly Optimized BERT Pre-training Approach (Roberta) is a transformer-based language model that has been refined using large-scale text corpora."
                    }
                ]
            },
            {
                "idx": 22,
                "key": "[272449378 | Liu et al. | 2024 | Citations: 1]",
                "snippets": "The Fasttext model performs better, with high speed and second only to the TextRNN-Att model. The loss values for the Fasttext model and the TextRNN-Att model has a loss value of 0.62 and 0.59 respectively, while the TextRNN model with embedded attention mechanism has the best result in all comparisons, which highlights the feature extraction advantage of the attention algorithm, and verifies the feasibility of studying the classification research with the pretrained model based on the attention mechanism. The LERT model retains the original architecture of the BERT, but optimises the training tasks and methods, which provides a good opportunity to study the BERT series of models.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {},
                "metadata": [
                    {
                        "section_title": "IV. ANALYSIS OF EXPERIMENTAL RESULTS",
                        "pdf_hash": "",
                        "start": 0,
                        "end": 689,
                        "sentence_offsets": [
                            {
                                "start": 0,
                                "end": 93
                            },
                            {
                                "start": 94,
                                "end": 511
                            },
                            {
                                "start": 512,
                                "end": 689
                            }
                        ],
                        "ref_mentions": [],
                        "quote": "The Fasttext model performs better, with high speed and second only to the TextRNN-Att model. The loss values for the Fasttext model and the TextRNN-Att model has a loss value of 0.62 and 0.59 respectively, while the TextRNN model with embedded attention mechanism has the best result in all comparisons, which highlights the feature extraction advantage of the attention algorithm, and verifies the feasibility of studying the classification research with the pretrained model based on the attention mechanism. The LERT model retains the original architecture of the BERT, but optimises the training tasks and methods, which provides a good opportunity to study the BERT series of models."
                    }
                ]
            },
            {
                "idx": 23,
                "key": "[274464991 | Mansour et al. | 2024 | Citations: 2]",
                "snippets": "We plot FastText's performance as a function of the number of training sequences in Figure 9. The transformer-based classifier and BERT significantly outperform FastText, but are significantly slower, and require significantly more compute.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {},
                "metadata": [
                    {
                        "section_title": "FineWeb-Edu",
                        "pdf_hash": "",
                        "start": 693,
                        "end": 933,
                        "sentence_offsets": [
                            {
                                "start": 693,
                                "end": 786
                            },
                            {
                                "start": 787,
                                "end": 933
                            }
                        ],
                        "ref_mentions": [],
                        "quote": "We plot FastText's performance as a function of the number of training sequences in Figure 9. The transformer-based classifier and BERT significantly outperform FastText, but are significantly slower, and require significantly more compute."
                    }
                ]
            },
            {
                "idx": 24,
                "key": "[274982292 | Yang et al. | 2024 | Citations: 0]",
                "snippets": "Compared with the single BERT model and fasttext, the accuracy was improved by 0.7% and 0.8%, respectively. Through this model, it is helpful to purify the network environment, maintain the health of network information, and create a positive, civilized, and harmonious online communication space for netizens, driving the development of social networking, information dissemination, and other aspects in a more benign direction.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {},
                "metadata": [
                    {
                        "quote": "Compared with the single BERT model and fasttext, the accuracy was improved by 0.7% and 0.8%, respectively. Through this model, it is helpful to purify the network environment, maintain the health of network information, and create a positive, civilized, and harmonious online communication space for netizens, driving the development of social networking, information dissemination, and other aspects in a more benign direction.",
                        "pdf_hash": "",
                        "section_title": "abstract"
                    }
                ]
            },
            {
                "idx": 25,
                "key": "[275704171 | Sani | 2024 | Citations: 0]",
                "snippets": "In this paper, we propose a novel approach that combines random oversampling with a BERT-base uncased model for essay answer correction. This research explores various scenario of text pre-processing techniques to optimize model accuracy. Using a dataset of essay answers obtained from eighth-grade middle school students in Indonesian language, our approach demonstrates good performance in terms of precision, recall, F1-score and accuracy compared to traditional methods such as Backpropagation Neural Network, Na\u00efve Bayes and Random Forest Classifier using FastText word embedding with Wikipedia 300 vector size pretrained model. The best performance was obtained using the BERT-base uncased model with 2e-5 learning rate and a simplified pre-processing approach.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {},
                "metadata": [
                    {
                        "quote": "In this paper, we propose a novel approach that combines random oversampling with a BERT-base uncased model for essay answer correction. This research explores various scenario of text pre-processing techniques to optimize model accuracy. Using a dataset of essay answers obtained from eighth-grade middle school students in Indonesian language, our approach demonstrates good performance in terms of precision, recall, F1-score and accuracy compared to traditional methods such as Backpropagation Neural Network, Na\u00efve Bayes and Random Forest Classifier using FastText word embedding with Wikipedia 300 vector size pretrained model. The best performance was obtained using the BERT-base uncased model with 2e-5 learning rate and a simplified pre-processing approach.",
                        "pdf_hash": "",
                        "section_title": "abstract"
                    }
                ]
            },
            {
                "idx": 26,
                "key": "[277940080 | Abdelmotaleb et al. | 2025 | Citations: 0]",
                "snippets": "Regarding feature engineering, for Word2Vec and FastText, our proposed PCA approach of combining word vectors using the first principal component shows clear advantages in performance over the traditional approach of taking the average...Additionally, energy consumption analysis revealed that computational efficiency varies significantly among embedding techniques, with TF-IDF being the most resource-efficient and FastText the most computationally de-manding...for the more challenging classification tasks, BERT combined with PCA stood out with the highest performance metrics.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {},
                "metadata": [
                    {
                        "section_title": "Conclusion",
                        "pdf_hash": "",
                        "start": 785,
                        "end": 1020,
                        "sentence_offsets": [
                            {
                                "start": 785,
                                "end": 1021
                            }
                        ],
                        "ref_mentions": [],
                        "quote": "Regarding feature engineering, for Word2Vec and FastText, our proposed PCA approach of combining word vectors using the first principal component shows clear advantages in performance over the traditional approach of taking the average"
                    },
                    {
                        "section_title": "Conclusion",
                        "pdf_hash": "",
                        "start": 1022,
                        "end": 1247,
                        "sentence_offsets": [
                            {
                                "start": 1022,
                                "end": 1247
                            }
                        ],
                        "ref_mentions": [],
                        "quote": "Additionally, energy consumption analysis revealed that computational efficiency varies significantly among embedding techniques, with TF-IDF being the most resource-efficient and FastText the most computationally de-manding"
                    },
                    {
                        "quote": "for the more challenging classification tasks, BERT combined with PCA stood out with the highest performance metrics.",
                        "pdf_hash": "",
                        "section_title": "abstract"
                    }
                ]
            }
        ]
    },
    "cluster": {
        "cost": 0.025644,
        "cot": "I need to organize the quotes into meaningful dimensions that will help answer the query about the comparative effects of FastText-based versus BERT-based classifiers on quality and efficiency of pre-training data filtering for large language models.\n\nLet me plan the dimensions:\n\n1. First, I'll need an \"Introduction/Background\" section to explain what FastText and BERT are and their fundamental differences. This will provide context for the comparison.\n\n2. \"Performance Comparison\" - This dimension will focus on the accuracy, precision, recall, F1 scores, and other quality metrics when comparing FastText and BERT models. This should be a synthesis to meaningfully compare the performance differences.\n\n3. \"Computational Efficiency\" - This dimension will address the computational resources, speed, and training time differences between FastText and BERT. This is important for the efficiency aspect of the query and should be a synthesis to explain the tradeoffs.\n\n4. \"Contextual Understanding Capabilities\" - This dimension will explore how BERT's contextual embeddings differ from FastText's static embeddings and the implications for language understanding. This should be a synthesis to explain these technical differences.\n\n5. \"Domain Adaptation and Generalization\" - This dimension will focus on how each classifier performs across different domains and with unseen data. This should be a synthesis to explain these important differences.\n\n6. \"Practical Applications and Use Cases\" - This section will present specific use cases where one approach might be preferable over the other. A list format works well here to provide distinct examples.\n\nNow, I'll assign quotes to each dimension:",
        "plan": {
            "Introduction and Background (synthesis)": [],
            "Performance Comparison (synthesis)": [
                0,
                2,
                3,
                5,
                6,
                7,
                8,
                9,
                11,
                13,
                14,
                15,
                16,
                17,
                19,
                20,
                23,
                24,
                25,
                26
            ],
            "Computational Efficiency (synthesis)": [
                1,
                10,
                12,
                13,
                17,
                22,
                23,
                26
            ],
            "Contextual Understanding Capabilities (synthesis)": [
                3,
                4,
                7,
                9,
                18,
                21
            ],
            "Domain Adaptation and Generalization (synthesis)": [
                0,
                3,
                5,
                16,
                20
            ],
            "Practical Applications and Use Cases (list)": [
                5,
                11,
                12,
                13,
                17
            ]
        },
        "model": "claude-3-7-sonnet-20250219"
    },
    "summary": {
        "sections": [
            {
                "title": "Introduction and Background",
                "tldr": "Pre-training data filtering is crucial for large language model (LLM) development, with FastText and BERT representing two fundamentally different approaches. FastText uses shallow word embeddings and linear classifiers while BERT leverages deep transformer architecture with contextual understanding. (LLM Memory)",
                "text": "\nPre-training data filtering is a critical step in the development pipeline of large language models (LLMs), directly impacting the quality, capabilities, and biases of the final models. As training datasets continue to grow into the trillions of tokens, efficient and effective filtering methods become increasingly important to maintain quality while managing computational resources. <Model name=\"Anthropic\" version=\"claude-3-7-sonnet-20250219\">\n\nFastText and BERT represent two fundamentally different approaches to text classification that are commonly used for data filtering tasks. FastText, developed by Facebook AI Research, uses shallow word embeddings and bag-of-words representations with a simple linear classifier. It treats words as atomic units and enriches them with subword information, making it particularly efficient for large-scale filtering tasks. <Model name=\"Anthropic\" version=\"claude-3-7-sonnet-20250219\">\n\nIn contrast, BERT (Bidirectional Encoder Representations from Transformers) employs a deep transformer-based architecture that creates contextual word representations based on surrounding text. This allows BERT to capture more nuanced semantic relationships and contextual meaning, potentially leading to more sophisticated filtering decisions. <Model name=\"Anthropic\" version=\"claude-3-7-sonnet-20250219\">\n\nThe choice between these approaches presents significant tradeoffs. FastText offers computational efficiency and speed at the cost of contextual understanding, while BERT provides richer linguistic representations at the expense of computational resources. These tradeoffs become particularly significant when filtering the massive datasets required for modern LLM pre-training. <Model name=\"Anthropic\" version=\"claude-3-7-sonnet-20250219\">",
                "citations": [],
                "format": "synthesis",
                "table": null,
                "model": "claude-3-7-sonnet-20250219"
            },
            {
                "title": "Performance Comparison",
                "tldr": "BERT generally outperforms FastText across most text classification tasks due to its contextual understanding capabilities, though FastText shows competitive performance in specific scenarios involving small datasets or when strong memorization is needed. (15 sources)",
                "text": "\nWhen directly comparing the classification performance of BERT-based and FastText-based models for data filtering tasks, multiple studies consistently show BERT's superior accuracy and F1 scores across diverse domains. BERT clearly outperforms FastText in nearly all measures, with one study noting that FastText sometimes fails to perform better than naive baselines <Paper corpusId=\"216914042\" paperTitle=\"(Breit et al., 2020)\" isShortName></Paper>. This performance gap is particularly pronounced in tasks requiring deep contextual understanding, where BERT's transformer architecture excels.\n\nThe performance advantage of BERT stems primarily from its ability to generate context-dependent word embeddings. Unlike FastText, which produces the same representation for a word regardless of context, BERT can differentiate between multiple meanings of the same word based on surrounding text <Paper corpusId=\"237253665\" paperTitle=\"(Hussain et al., 2021)\" isShortName></Paper>. This capability is especially valuable for morphologically complex languages where word meaning depends heavily on context, with BERT showing superior vectorization compared to FastText for languages like Latvian, Lithuanian, and Russian <Paper corpusId=\"226337596\" paperTitle=\"(Kapociute-Dzikiene et al., 2020)\" isShortName></Paper>.\n\nIn word sense disambiguation tasks, BERT-based models consistently achieve high F1 scores exceeding 90% in most cases, significantly outperforming FastText <Paper corpusId=\"232278102\" paperTitle=\"(Loureiro et al., 2021)\" isShortName></Paper>. Similarly, in software engineering domains, domain-adapted BERT models show approximately 11% higher F1 scores than FastText for issue type prediction tasks <Paper corpusId=\"237485425\" paperTitle=\"(Mosel et al., 2021)\" isShortName></Paper>.\n\nHowever, the performance gap narrows or even reverses in specific scenarios. With small training datasets, FastText coupled with domain-specific word embeddings can perform equally well or better than BERT, even when BERT is pre-trained on domain-specific data <Paper corpusId=\"227231089\" paperTitle=\"(Edwards et al., 2020)\" isShortName></Paper>. For certain financial datasets, FastText outperforms BERT entirely <Paper corpusId=\"265162335\" paperTitle=\"(Wildemann et al., 2023)\" isShortName></Paper> <Paper corpusId=\"209475786\" paperTitle=\"(Zhu et al., 2019)\" isShortName></Paper>.\n\nAn interesting pattern emerges in e-commerce search query classification, where FastText excels at memorization while BERT demonstrates better generalization. One study found that FastText performs better on test queries that appeared in the training set, while BERT handles unseen or low-frequency queries more effectively <Paper corpusId=\"260379057\" paperTitle=\"(Ning et al., 2023)\" isShortName></Paper>. This suggests complementary strengths that could be leveraged in different filtering scenarios.\n\nThe precision-recall tradeoff also differs between models. BERT-based classifiers typically show higher precision (0.92 in one study) but lower recall (0.8), while FastText-based approaches demonstrate superior recall (0.93) <Paper corpusId=\"246608158\" paperTitle=\"(Ivanov et al., 2022)\" isShortName></Paper>. This characteristic makes FastText potentially more valuable for applications where maximizing the extraction of relevant content is prioritized over precision.\n\nFor bias and discrimination detection, random forest classifiers with BERT word embeddings achieved the best performance, outperforming those using non-contextual embeddings like FastText <Paper corpusId=\"253015767\" paperTitle=\"(Frissen et al., 2022)\" isShortName></Paper>. In classification of unseen data, BERT outperformed FastText by 9.4% in F1 score <Paper corpusId=\"253860591\" paperTitle=\"(Melnyk et al., 2022)\" isShortName></Paper>, further highlighting its superior generalization capabilities.\n\nFor essay answer correction, BERT-based approaches demonstrate good performance compared to traditional methods using FastText word embeddings <Paper corpusId=\"275704171\" paperTitle=\"(Sani, 2024)\" isShortName></Paper>. In more challenging classification tasks, BERT combined with dimension reduction techniques like PCA demonstrates the highest performance metrics <Paper corpusId=\"277940080\" paperTitle=\"(Abdelmotaleb et al., 2025)\" isShortName></Paper>.\n\nWhile BERT generally outperforms FastText in accuracy, it's worth noting that in some specialized domains like clinical text analysis, pre-trained BERT models without domain adaptation don't show marked advantages over simpler approaches <Paper corpusId=\"269973968\" paperTitle=\"(Xie et al., 2024)\" isShortName></Paper>, emphasizing the importance of domain adaptation for optimal performance.",
                "citations": [
                    {
                        "id": "(Breit et al., 2020)",
                        "snippets": [
                            "As can be observed, BERT is clearly better than FastText in all measures. In fact, perhaps surprisingly, FastText does not perform better than a naive baseline that retrieves all instances as true",
                            "Interestingly, FastText faces a massive challenge in adapting domains and generalising from the general to the specific domains. However, BERT shows to be much more robust to domain changes."
                        ],
                        "paper": {
                            "corpus_id": 216914042,
                            "title": "WiC-TSV: An Evaluation Benchmark for Target Sense Verification of Words in Context",
                            "authors": [
                                {
                                    "authorId": "1453876854",
                                    "name": "Anna Breit"
                                },
                                {
                                    "authorId": "144688662",
                                    "name": "Artem Revenko"
                                },
                                {
                                    "authorId": "1667035673",
                                    "name": "Kiamehr Rezaee"
                                },
                                {
                                    "authorId": "1717641",
                                    "name": "Mohammad Taher Pilehvar"
                                },
                                {
                                    "authorId": "1387447871",
                                    "name": "Jos\u00e9 Camacho-Collados"
                                }
                            ],
                            "year": 2020,
                            "venue": "Conference of the European Chapter of the Association for Computational Linguistics",
                            "n_citations": 26
                        },
                        "score": 0.7080078125
                    },
                    {
                        "id": "(Hussain et al., 2021)",
                        "snippets": [
                            "BERT outperforms both CNN and LSTM. The reason for the better performance of BERT is that it learns contextualized embeddings in a bidirectional way. In natural language, a word is likely to convey multiple meanings based on the context in which it is used. Word2vec, fasttext, and glove produce the same representations of a word even if it has different meanings in different contexts. BERT, on the other hand, produces context-dependent embeddings of a word."
                        ],
                        "paper": {
                            "corpus_id": 237253665,
                            "title": "Pharmacovigilance with Transformers: A Framework to Detect Adverse Drug Reactions Using BERT Fine-Tuned with FARM",
                            "authors": [
                                {
                                    "authorId": "2113619919",
                                    "name": "Sajid Hussain"
                                },
                                {
                                    "authorId": "1777100",
                                    "name": "H. Afzal"
                                },
                                {
                                    "authorId": "5644298",
                                    "name": "Ramsha Saeed"
                                },
                                {
                                    "authorId": "2740595",
                                    "name": "N. Iltaf"
                                },
                                {
                                    "authorId": "33827057",
                                    "name": "M. Umair"
                                }
                            ],
                            "year": 2021,
                            "venue": "Computational and Mathematical Methods in Medicine",
                            "n_citations": 24
                        },
                        "score": 0.71142578125
                    },
                    {
                        "id": "(Kapociute-Dzikiene et al., 2020)",
                        "snippets": [
                            "As seen from Figure 2, BERT vectorization is a better choice compared to fastText for all morphologically complex languages for all datasets, and this is not surprising. Morphologically complex languages (especially fusional languages) suffer from disambiguation problems, but BERT has mechanisms that are able to vectorize even those words that are written the same but have different meanings, depending on their context, differently. Despite the fact that fastText embeddings are also trained to consider a context around the target word, that context is restricted to only a few words. Despite this, fastText is a suitable vectorization solution for languages (such as English) with strict word order in a sentence. In contrast, BERT is able to consider a much broader context (words, sentences, their order) compared to fastText and is, therefore, more suitable for languages that have a relatively free word order in a sentence (such as Latvian, Lithuanian, and Russian)."
                        ],
                        "paper": {
                            "corpus_id": 226337596,
                            "title": "Intent Detection Problem Solving via Automatic DNN Hyperparameter Optimization",
                            "authors": [
                                {
                                    "authorId": "1403992181",
                                    "name": "J. Kapo\u010di\u016bt\u0117-Dzikien\u0117"
                                },
                                {
                                    "authorId": "3288489",
                                    "name": "K. Balodis"
                                },
                                {
                                    "authorId": "3283640",
                                    "name": "Raivis Skadins"
                                }
                            ],
                            "year": 2020,
                            "venue": "Applied Sciences",
                            "n_citations": 13
                        },
                        "score": 0.689453125
                    },
                    {
                        "id": "(Loureiro et al., 2021)",
                        "snippets": [
                            "We also include two FastText linear classifiers (Joulin et al., 2016)) as baselines: FTX-B (base model without pretrained embeddings) and FTX-C (using pretrained embeddings from Common Crawl). We chose FastText as the baseline given its efficiency and competitive results for sentence classification",
                            "In general, results are high for all Transformerbased models, over 90% in most cases. This reinforces the potential of language models for WSD, both in its light-weight 1NN and in the fine-tuning settings",
                            "Most Frequent Sense (MFS) bias. As expected, macro-F1 results degrade for the purely supervised classification models (FastText and fine-tuned BERT), indicating the inherent sense bias",
                            "In addition to our BERT-based model, we include results for two Fast-Text supervised classifiers (Joulin et al., 2016)) as baselines: a basic one with random initialization (FastText-B) and another initialized with FastText embeddings trained on the Common Crawl (FastText-C)",
                            "We can see that BERT significantly outperforms the FastText static word embedding."
                        ],
                        "paper": {
                            "corpus_id": 232278102,
                            "title": "Analysis and Evaluation of Language Models for Word Sense Disambiguation",
                            "authors": [
                                {
                                    "authorId": "144653901",
                                    "name": "Daniel Loureiro"
                                },
                                {
                                    "authorId": "1667035673",
                                    "name": "Kiamehr Rezaee"
                                },
                                {
                                    "authorId": "1717641",
                                    "name": "Mohammad Taher Pilehvar"
                                },
                                {
                                    "authorId": "1387447871",
                                    "name": "Jos\u00e9 Camacho-Collados"
                                }
                            ],
                            "year": 2021,
                            "venue": "International Conference on Computational Logic",
                            "n_citations": 72
                        },
                        "score": 0.67626953125
                    },
                    {
                        "id": "(Mosel et al., 2021)",
                        "snippets": [
                            "The results show that seBERT and BERToverflow achieve the best performance for the issue type prediction tasks, outperforming both fastText and the general-domain BERT models. The improvement over fastText is very large with an about 11% higher F1 score for the issue type prediction. The violin plot in Figure 5 indicates that the performance improvement in F1 score is due to an improvement of both recall and precision, which means the models reduced both false positives and false negatives in comparison to fastText. The Bayesian signed rank test determined that this improvement of the SE-specific models over the other models is significant."
                        ],
                        "paper": {
                            "corpus_id": 237485425,
                            "title": "On the Validity of Pre-Trained Transformers for Natural Language Processing in the Software Engineering Domain",
                            "authors": [
                                {
                                    "authorId": "2214761830",
                                    "name": "Julian von der Mosel"
                                },
                                {
                                    "authorId": "8058979",
                                    "name": "Alexander Trautsch"
                                },
                                {
                                    "authorId": "3063461",
                                    "name": "Steffen Herbold"
                                }
                            ],
                            "year": 2021,
                            "venue": "IEEE Transactions on Software Engineering",
                            "n_citations": 68
                        },
                        "score": 0.703125
                    },
                    {
                        "id": "(Edwards et al., 2020)",
                        "snippets": [
                            "In settings with small training datasets a simple method like fastText coupled with domain-specific word embeddings performs equally well or better than BERT, even when pre-trained on domain-specific data."
                        ],
                        "paper": {
                            "corpus_id": 227231089,
                            "title": "Go Simple and Pre-Train on Domain-Specific Corpora: On the Role of Training Data for Text Classification",
                            "authors": [
                                {
                                    "authorId": "1383074767",
                                    "name": "A. Edwards"
                                },
                                {
                                    "authorId": "1387447871",
                                    "name": "Jos\u00e9 Camacho-Collados"
                                },
                                {
                                    "authorId": "2750681",
                                    "name": "H\u00e9l\u00e8ne de Ribaupierre"
                                },
                                {
                                    "authorId": "1762890",
                                    "name": "A. Preece"
                                }
                            ],
                            "year": 2020,
                            "venue": "International Conference on Computational Linguistics",
                            "n_citations": 25
                        },
                        "score": 0.9541015625
                    },
                    {
                        "id": "(Wildemann et al., 2023)",
                        "snippets": [
                            "Overall, BERT achieves the best performance on the F 1 measure in all datasets except for Bank",
                            "fastText proves to be the second-best classifier and even outperforms BERT on the Bank dataset. The additional use of pre-trained word embeddings clearly benefits the task and shows an average increase in F 1 -score of 15.3% across all datasets. This suggests a further improvement when using larger domain-specific corpora. Moreover, its fast training and the lack of dependency on GPUs may be advantageous for many practical applications."
                        ],
                        "paper": {
                            "corpus_id": 265162335,
                            "title": "Bridging Qualitative Data Silos: The Potential of Reusing Codings Through Machine Learning Based Cross-Study Code Linking",
                            "authors": [
                                {
                                    "authorId": "3041146",
                                    "name": "Sergej Wildemann"
                                },
                                {
                                    "authorId": "2257974973",
                                    "name": "Claudia Nieder\u00e9e"
                                },
                                {
                                    "authorId": "2098749",
                                    "name": "Erick Elejalde"
                                }
                            ],
                            "year": 2023,
                            "venue": "Social science computer review",
                            "n_citations": 0
                        },
                        "score": 0.71484375
                    },
                    {
                        "id": "(Zhu et al., 2019)",
                        "snippets": [
                            "Adversarial training, which minimizes the maximal risk for label-preserving input perturbations, has proved to be effective for improving the generalization of language models. In this work, we propose a novel adversarial training algorithm, FreeLB, that promotes higher invariance in the embedding space, by adding adversarial perturbations to word embeddings and minimizing the resultant adversarial risk inside different regions around input samples. To validate the effectiveness of the proposed approach, we apply it to Transformer-based models for natural language understanding and commonsense reasoning tasks. Experiments on the GLUE benchmark show that when applied only to the finetuning stage, it is able to improve the overall test scores of BERT-base model from 78.3 to 79.4, and RoBERTa-large model from 88.5 to 88.8. In addition, the proposed approach achieves state-of-the-art single-model test accuracies of 85.44\\% and 67.75\\% on ARC-Easy and ARC-Challenge. Experiments on CommonsenseQA benchmark further demonstrate that FreeLB can be generalized and boost the performance of RoBERTa-large model on other tasks as well. Code is available at \\url{this https URL ."
                        ],
                        "paper": {
                            "corpus_id": 209475786,
                            "title": "FreeLB: Enhanced Adversarial Training for Natural Language Understanding",
                            "authors": [
                                {
                                    "authorId": "1431754650",
                                    "name": "Chen Zhu"
                                },
                                {
                                    "authorId": "145215470",
                                    "name": "Yu Cheng"
                                },
                                {
                                    "authorId": "144702900",
                                    "name": "Zhe Gan"
                                },
                                {
                                    "authorId": "2419809",
                                    "name": "S. Sun"
                                },
                                {
                                    "authorId": "1962083",
                                    "name": "T. Goldstein"
                                },
                                {
                                    "authorId": "46700348",
                                    "name": "Jingjing Liu"
                                }
                            ],
                            "year": 2019,
                            "venue": "International Conference on Learning Representations",
                            "n_citations": 442
                        },
                        "score": 0
                    },
                    {
                        "id": "(Ning et al., 2023)",
                        "snippets": [
                            "We find that the FastText model can do better on the training set and testing set (T+0 day), while the BERT model performs better on the testing set (T+1 day). As 99.89% of test queries on T+0 day have appeared in the training set, it means that the FastText model can be remembered better. On the other hand, as only 57.47% of test queries on T+1 day has exposed in the training set, it means that the BERT model can generalize better on some unseen or low-frequency search query. In other words, in the E-commerce search query classification task, the FastText is better at remembering while the BERT is better at generalization."
                        ],
                        "paper": {
                            "corpus_id": 260379057,
                            "title": "Towards Better Query Classification with Multi-Expert Knowledge Condensation in JD Ads Search",
                            "authors": [
                                {
                                    "authorId": "66266326",
                                    "name": "Kun-Peng Ning"
                                },
                                {
                                    "authorId": "2053434618",
                                    "name": "Ming Pang"
                                },
                                {
                                    "authorId": "2072874946",
                                    "name": "Zheng Fang"
                                },
                                {
                                    "authorId": "2226458237",
                                    "name": "Xue Jiang"
                                },
                                {
                                    "authorId": "2226511841",
                                    "name": "Xi-Wei Zhao"
                                },
                                {
                                    "authorId": "1949218014",
                                    "name": "Changping Peng"
                                },
                                {
                                    "authorId": "2146396439",
                                    "name": "Zhangang Lin"
                                },
                                {
                                    "authorId": "22528137",
                                    "name": "Jinghe Hu"
                                },
                                {
                                    "authorId": "2118926502",
                                    "name": "Jingping Shao"
                                }
                            ],
                            "year": 2023,
                            "venue": "arXiv.org",
                            "n_citations": 0
                        },
                        "score": 0.91552734375
                    },
                    {
                        "id": "(Ivanov et al., 2022)",
                        "snippets": [
                            "Result of the first phase of evaluation is presented in Table 1. As it was expected, more advanced model (BERT) showed better results in terms of F1-score. BERT-based model showed high precision (0.92) and lower recall (0.8). Results of the BERT model is available at https://bit.ly/3oPElMm. However, the values of precision and recall metrics behave differently for the fastText and ELMobased baselines. fastText-based classifier showed better Recall (0.93) comparing with other architectures. This property might be useful in some cases when it is necessary to extract more relevant sentences and text patterns associated with requirements."
                        ],
                        "paper": {
                            "corpus_id": 246608158,
                            "title": "Extracting Software Requirements from Unstructured Documents",
                            "authors": [
                                {
                                    "authorId": "2072422334",
                                    "name": "V. Ivanov"
                                },
                                {
                                    "authorId": "2883654",
                                    "name": "Andrey Sadovykh"
                                },
                                {
                                    "authorId": "3361049",
                                    "name": "Alexandr Naumchev"
                                },
                                {
                                    "authorId": "36000045",
                                    "name": "A. Bagnato"
                                },
                                {
                                    "authorId": "2117465706",
                                    "name": "K. Yakovlev"
                                }
                            ],
                            "year": 2022,
                            "venue": "International Joint Conference on the Analysis of Images, Social Networks and Texts",
                            "n_citations": 11
                        },
                        "score": 0.775390625
                    },
                    {
                        "id": "(Frissen et al., 2022)",
                        "snippets": [
                            "The results indicate that the RF classifier with BERT word embeddings as textual feature achieved the best performance. This illustrates that contextual word embedding representations such as BERT had a superior performance over the non-contextual word embeddings such as FastText and Word-2vec. We also observe that tree-based (Random Forest and Decision Tree) classifiers had a better performance in classifying biased and discriminatory language as compared to the remaining classifiers. Among the textual features, word embedding representations BERT, FastText and ELMo in combination with the RF classifier had the best performance. This was followed by FastText, ELMo and Flair word embeddings in combination with the DT classifier."
                        ],
                        "paper": {
                            "corpus_id": 253015767,
                            "title": "A machine learning approach to recognize bias and discrimination in job advertisements",
                            "authors": [
                                {
                                    "authorId": "2188338310",
                                    "name": "Richard Frissen"
                                },
                                {
                                    "authorId": "2848544",
                                    "name": "K. Adebayo"
                                },
                                {
                                    "authorId": "39418246",
                                    "name": "Rohan Nanda"
                                }
                            ],
                            "year": 2022,
                            "venue": "Ai & Society",
                            "n_citations": 19
                        },
                        "score": 0.77197265625
                    },
                    {
                        "id": "(Melnyk et al., 2022)",
                        "snippets": [
                            "the BERT model scored better in all of these settings and in classifying unseen data, reaching an F1 score of 0.80 and outperforming FastText by 9.4%."
                        ],
                        "paper": {
                            "corpus_id": 253860591,
                            "title": "Sentiment Analysis and Stance Detection on German YouTube Comments on Gender Diversity",
                            "authors": [
                                {
                                    "authorId": "78466463",
                                    "name": "Lidiia Melnyk"
                                },
                                {
                                    "authorId": "2192124742",
                                    "name": "Linda Feld"
                                }
                            ],
                            "year": 2022,
                            "venue": "Journal of Computer-Assisted Linguistic Research",
                            "n_citations": 1
                        },
                        "score": 0.78955078125
                    },
                    {
                        "id": "(Sani, 2024)",
                        "snippets": [
                            "In this paper, we propose a novel approach that combines random oversampling with a BERT-base uncased model for essay answer correction. This research explores various scenario of text pre-processing techniques to optimize model accuracy. Using a dataset of essay answers obtained from eighth-grade middle school students in Indonesian language, our approach demonstrates good performance in terms of precision, recall, F1-score and accuracy compared to traditional methods such as Backpropagation Neural Network, Na\u00efve Bayes and Random Forest Classifier using FastText word embedding with Wikipedia 300 vector size pretrained model. The best performance was obtained using the BERT-base uncased model with 2e-5 learning rate and a simplified pre-processing approach."
                        ],
                        "paper": {
                            "corpus_id": 275704171,
                            "title": "A Random Oversampling and BERT-based Model Approach for Handling Imbalanced Data in Essay Answer Correction",
                            "authors": [
                                {
                                    "authorId": "2341084544",
                                    "name": "Dian Ahkam Sani"
                                }
                            ],
                            "year": 2024,
                            "venue": "Jurnal Infotel",
                            "n_citations": 0
                        },
                        "score": 0.65869140625
                    },
                    {
                        "id": "(Abdelmotaleb et al., 2025)",
                        "snippets": [
                            "Regarding feature engineering, for Word2Vec and FastText, our proposed PCA approach of combining word vectors using the first principal component shows clear advantages in performance over the traditional approach of taking the average",
                            "Additionally, energy consumption analysis revealed that computational efficiency varies significantly among embedding techniques, with TF-IDF being the most resource-efficient and FastText the most computationally de-manding",
                            "for the more challenging classification tasks, BERT combined with PCA stood out with the highest performance metrics."
                        ],
                        "paper": {
                            "corpus_id": 277940080,
                            "title": "Word Embedding Techniques for Classification of Star Ratings",
                            "authors": [
                                {
                                    "authorId": "2233683143",
                                    "name": "Hesham Abdelmotaleb"
                                },
                                {
                                    "authorId": "2342295801",
                                    "name": "Craig McNeile"
                                },
                                {
                                    "authorId": "2273117131",
                                    "name": "Ma\u0142gorzata Wojty\u015b"
                                }
                            ],
                            "year": 2025,
                            "venue": "arXiv.org",
                            "n_citations": 0
                        },
                        "score": 0.67138671875
                    },
                    {
                        "id": "(Xie et al., 2024)",
                        "snippets": [
                            "When comparing VSM-based methods with Word2Vec-based methods, we equate MLP and FastText due to their similar complexities as shallow neural networks. FastText, however, demonstrates better performance than MLP (0.696 vs 0.670 in Acc and 0.767 vs 0.727 in F 1 ), indicating Word2Vec's superior word representation capabilities",
                            "Interestingly, original BERT, despite its wide-ranging successes in natural language processing, does not show a marked advantage over Word2Vec in our analysis and is slightly outperformed by TextCNN. This observation is further evidenced by the improved performance of MLM BERT over the original BERT, suggesting that contextual word embeddings benefit from additional data and training. This leads us to conclude that for specialized domain language tasks like clinical text analysis, pre-training is essential to fully leverage BERT's capabilities."
                        ],
                        "paper": {
                            "corpus_id": 269973968,
                            "title": "Identification of mycoplasma pneumonia in children based on fusion of multi-modal clinical free-text description and structured test data",
                            "authors": [
                                {
                                    "authorId": "2302859526",
                                    "name": "Jingna Xie"
                                },
                                {
                                    "authorId": "2364687744",
                                    "name": "Yingshuo Wang"
                                },
                                {
                                    "authorId": "1576956902",
                                    "name": "Qiuyang Sheng"
                                },
                                {
                                    "authorId": "2283403594",
                                    "name": "Xiaoqing Liu"
                                },
                                {
                                    "authorId": "2336703688",
                                    "name": "Jing Li"
                                },
                                {
                                    "authorId": "47450065",
                                    "name": "Fenglei Sun"
                                },
                                {
                                    "authorId": "2155376102",
                                    "name": "Yuqi Wang"
                                },
                                {
                                    "authorId": "2302784979",
                                    "name": "Shuxian Li"
                                },
                                {
                                    "authorId": "2110531396",
                                    "name": "Yiming Li"
                                },
                                {
                                    "authorId": "2302875109",
                                    "name": "Yizhou Yu"
                                },
                                {
                                    "authorId": "2303601854",
                                    "name": "Gang Yu"
                                }
                            ],
                            "year": 2024,
                            "venue": "Health Informatics Journal",
                            "n_citations": 1
                        },
                        "score": 0.69921875
                    }
                ],
                "format": "synthesis",
                "table": null,
                "model": "claude-3-7-sonnet-20250219"
            },
            {
                "title": "Computational Efficiency",
                "tldr": "FastText significantly outperforms BERT in computational efficiency, requiring fewer resources and faster training/inference times, making it more suitable for real-time applications and resource-constrained environments despite its generally lower accuracy. (7 sources)",
                "text": "\nWhen considering computational efficiency, FastText and BERT represent opposite ends of the spectrum in the tradeoff between performance and resource requirements. FastText consistently demonstrates superior computational efficiency compared to transformer-based models like BERT across multiple studies. Classic word embedding models including FastText learn representations significantly faster than transformer-based models due to their simpler architecture <Paper corpusId=\"219558245\" paperTitle=\"(Hettiarachchi et al., 2020)\" isShortName></Paper>. Even when compared to other traditional word embeddings like Skip-gram, FastText takes slightly more time due to its processing of subword information, but still remains dramatically faster than BERT variants <Paper corpusId=\"219558245\" paperTitle=\"(Hettiarachchi et al., 2020)\" isShortName></Paper>.\n\nThe resource requirements between these approaches differ substantially. FastText has been shown to perform well with minimal computational resources in standard server environments, while BERT-based models demand significantly more computational power <Paper corpusId=\"244895506\" paperTitle=\"(Durairaj et al., 2021)\" isShortName></Paper>. This difference becomes particularly important in real-time applications, where FastText's efficiency enables practical deployment in scenarios where BERT would be prohibitively expensive or slow <Paper corpusId=\"219558245\" paperTitle=\"(Hettiarachchi et al., 2020)\" isShortName></Paper>.\n\nThe efficiency advantage of FastText is so significant that even lightweight BERT variants like DistilBERT, while faster than BERT, still exceed practical time constraints for real-time processing applications <Paper corpusId=\"219558245\" paperTitle=\"(Hettiarachchi et al., 2020)\" isShortName></Paper>. For example, processing tweets from a 2-minute window required approximately 7.2 minutes with DistilBERT, making it unsuitable for real-time analysis <Paper corpusId=\"219558245\" paperTitle=\"(Hettiarachchi et al., 2020)\" isShortName></Paper>.\n\nIn terms of model size and inference speed, studies have shown that BERT-based models can have up to 160% more parameters than competitive FastText-based architectures, resulting in significantly higher inference times <Paper corpusId=\"252901000\" paperTitle=\"(Groger et al., 2022)\" isShortName></Paper>. This performance-runtime tradeoff becomes particularly important in production environments where both accuracy and speed are critical considerations <Paper corpusId=\"252901000\" paperTitle=\"(Groger et al., 2022)\" isShortName></Paper>.\n\nEnergy consumption analysis further highlights this efficiency gap, with FastText being among the most computationally demanding embedding techniques while still requiring far fewer resources than BERT <Paper corpusId=\"277940080\" paperTitle=\"(Abdelmotaleb et al., 2025)\" isShortName></Paper>. Despite transformer-based classifiers and BERT significantly outperforming FastText in accuracy, they are \"significantly slower, and require significantly more compute\" <Paper corpusId=\"274464991\" paperTitle=\"(Mansour et al., 2024)\" isShortName></Paper>.\n\nThe computational efficiency advantage of FastText has made it an attractive candidate for knowledge distillation approaches, where a more powerful but computationally expensive model like BERT or ERNIE serves as a teacher to improve FastText's accuracy while maintaining its computational advantages <Paper corpusId=\"246738423\" paperTitle=\"(Guo et al., 2022)\" isShortName></Paper>. This approach \"guarantees the superiority of its original computational performance\" while significantly improving classification accuracy <Paper corpusId=\"246738423\" paperTitle=\"(Guo et al., 2022)\" isShortName></Paper>.\n\nFor many practical applications, FastText's combination of reasonable performance and computational efficiency makes it an attractive option, particularly when GPU resources are limited or unavailable <Paper corpusId=\"265162335\" paperTitle=\"(Wildemann et al., 2023)\" isShortName></Paper>. Its fast training times and lack of dependency on specialized hardware create significant practical advantages that may outweigh the performance benefits of BERT in resource-constrained environments <Paper corpusId=\"265162335\" paperTitle=\"(Wildemann et al., 2023)\" isShortName></Paper>.",
                "citations": [
                    {
                        "id": "(Hettiarachchi et al., 2020)",
                        "snippets": [
                            "According to the obtained results, classic word embedding models (e.g. Skip-gram and fastText) learn the representations faster than transformer-based models (e.g. BERT and DistilBERT). \n\nComparing fastText and Skip-gram, fastText took more time because it processes subword information. But, incorporation of subwords allows this model to capture connections between modified words. For example, consider the goal-related words found within the top 20 words with high cluster change during a goal score: Skip-gramgoal, goalll, rashyyy, scores fastTextgoalll, goooaaalll, rashford, rashyyy, @marcusrashford, scored, scores fastText captures more modified words than Skip-gram. We could not run a complete evaluation using fastText embeddings, because it requires a manual process since GT keywords only contain the words in actual form. \n\nTransformer-based models took more time than both Skip-gram and fastText due to their complex architecture to learn contextualised word embeddings. DistilBERT is found to be faster than BERT, however, the learning time of DistilBERT is not fast enough for real-time processing because it exceeds the tweet generation time. For example to learn from tweets posted during a 2-minute time window, it took approximately 7.2 minutes. If this model can be further distilled, there is a possibility to achieve the required efficiency to become suitable for real-time processing. However, further distillation can reduce the language understanding capability of the model as there is a 3% reduction in DistilBERT compared to BERT (Sanh et al. 2019)."
                        ],
                        "paper": {
                            "corpus_id": 219558245,
                            "title": "Embed2Detect: temporally clustered embedded words for event detection in social media",
                            "authors": [
                                {
                                    "authorId": "1453653937",
                                    "name": "Hansi Hettiarachchi"
                                },
                                {
                                    "authorId": "1403921831",
                                    "name": "Mariam Adedoyin-Olowe"
                                },
                                {
                                    "authorId": "3181018",
                                    "name": "Jagdev Bhogal"
                                },
                                {
                                    "authorId": "1698684",
                                    "name": "M. Gaber"
                                }
                            ],
                            "year": 2020,
                            "venue": "Machine-mediated learning",
                            "n_citations": 33
                        },
                        "score": 0.7333984375
                    },
                    {
                        "id": "(Durairaj et al., 2021)",
                        "snippets": [
                            "The proposed BERT model outperforms in terms of accuracy and model performance compare to other models. The results of the fastText model showed low accuracy when unigram and bigram methods were used for training the model. The overall model training and data preparation tasks took less time for BERT model in comparison to others. This experiment reveals that the BERT model required more computational resources to train compared with other traditional models. The fastText model performed well with a standard server environment with minimal computational resources compare to other models."
                        ],
                        "paper": {
                            "corpus_id": 244895506,
                            "title": "Transformer based Contextual Model for Sentiment Analysis of Customer Reviews: A Fine-tuned BERT",
                            "authors": [
                                {
                                    "authorId": "90290875",
                                    "name": "A. Durairaj"
                                },
                                {
                                    "authorId": "2048021830",
                                    "name": "Anandan Chinnalagu"
                                }
                            ],
                            "year": 2021,
                            "venue": "International Journal of Advanced Computer Science and Applications",
                            "n_citations": 29
                        },
                        "score": 0.7607421875
                    },
                    {
                        "id": "(Groger et al., 2022)",
                        "snippets": [
                            "The best performing model consists of pre-trained FastText embeddings followed by a stack of bidirectional LSTM layers. This rather simple architecture performs more that 20% better in F 1 compared to a support vector machine baseline with TF-IDF feature engineering and also beats transfer learning with a pretrained BERT model 3 by a small margin. On the one hand, this indicates that such large language models as BERT can generalize pretty well even in the presence of small data. On the other hand, BERT comes with an increase of 160% in the number of parameters and thus significantly higher inference time compared to the winning model. Hence, even with more data and an expected increase in the BERT model performance, the LSTM architecture will show the better performance-runtime tradeoff."
                        ],
                        "paper": {
                            "corpus_id": 252901000,
                            "title": "Assessing Guest Nationality Composition from Hotel Reviews",
                            "authors": [
                                {
                                    "authorId": "2026999646",
                                    "name": "Fabian Gr\u00f6ger"
                                },
                                {
                                    "authorId": "1715783",
                                    "name": "M. Pouly"
                                },
                                {
                                    "authorId": "2187827840",
                                    "name": "Flavia Tinner"
                                },
                                {
                                    "authorId": "39589520",
                                    "name": "Leif Brandes"
                                }
                            ],
                            "year": 2022,
                            "venue": "Swiss Conference on Data Science",
                            "n_citations": 0
                        },
                        "score": 0.76904296875
                    },
                    {
                        "id": "(Abdelmotaleb et al., 2025)",
                        "snippets": [
                            "Regarding feature engineering, for Word2Vec and FastText, our proposed PCA approach of combining word vectors using the first principal component shows clear advantages in performance over the traditional approach of taking the average",
                            "Additionally, energy consumption analysis revealed that computational efficiency varies significantly among embedding techniques, with TF-IDF being the most resource-efficient and FastText the most computationally de-manding",
                            "for the more challenging classification tasks, BERT combined with PCA stood out with the highest performance metrics."
                        ],
                        "paper": {
                            "corpus_id": 277940080,
                            "title": "Word Embedding Techniques for Classification of Star Ratings",
                            "authors": [
                                {
                                    "authorId": "2233683143",
                                    "name": "Hesham Abdelmotaleb"
                                },
                                {
                                    "authorId": "2342295801",
                                    "name": "Craig McNeile"
                                },
                                {
                                    "authorId": "2273117131",
                                    "name": "Ma\u0142gorzata Wojty\u015b"
                                }
                            ],
                            "year": 2025,
                            "venue": "arXiv.org",
                            "n_citations": 0
                        },
                        "score": 0.67138671875
                    },
                    {
                        "id": "(Mansour et al., 2024)",
                        "snippets": [
                            "We plot FastText's performance as a function of the number of training sequences in Figure 9. The transformer-based classifier and BERT significantly outperform FastText, but are significantly slower, and require significantly more compute."
                        ],
                        "paper": {
                            "corpus_id": 274464991,
                            "title": "Measuring Bias of Web-filtered Text Datasets and Bias Propagation Through Training",
                            "authors": [
                                {
                                    "authorId": "2294362620",
                                    "name": "Youssef Mansour"
                                },
                                {
                                    "authorId": "2294361310",
                                    "name": "Reinhard Heckel"
                                }
                            ],
                            "year": 2024,
                            "venue": "arXiv.org",
                            "n_citations": 2
                        },
                        "score": 0.8291015625
                    },
                    {
                        "id": "(Guo et al., 2022)",
                        "snippets": [
                            "In this paper, we use knowledge of the ERNIE model to distill the FastText model; the ERNIE model works as a teacher model to predict the massive online unlabeled data for data enhancement, and then guides the training of the student model of FastText with better computational efficiency. The FastText model is distilled by the ERNIE model in chatbot intention classification. This not only guarantees the superiority of its original computational performance, but also the intention classification accuracy has been significantly improved."
                        ],
                        "paper": {
                            "corpus_id": 246738423,
                            "title": "Application of Knowledge Distillation Based on Transfer Learning of ERNIE Model in Intelligent Dialogue Intention Recognition",
                            "authors": [
                                {
                                    "authorId": "2119112458",
                                    "name": "Shiguang Guo"
                                },
                                {
                                    "authorId": "2117944486",
                                    "name": "Qing Wang"
                                }
                            ],
                            "year": 2022,
                            "venue": "Italian National Conference on Sensors",
                            "n_citations": 8
                        },
                        "score": 0.6806640625
                    },
                    {
                        "id": "(Wildemann et al., 2023)",
                        "snippets": [
                            "Overall, BERT achieves the best performance on the F 1 measure in all datasets except for Bank",
                            "fastText proves to be the second-best classifier and even outperforms BERT on the Bank dataset. The additional use of pre-trained word embeddings clearly benefits the task and shows an average increase in F 1 -score of 15.3% across all datasets. This suggests a further improvement when using larger domain-specific corpora. Moreover, its fast training and the lack of dependency on GPUs may be advantageous for many practical applications."
                        ],
                        "paper": {
                            "corpus_id": 265162335,
                            "title": "Bridging Qualitative Data Silos: The Potential of Reusing Codings Through Machine Learning Based Cross-Study Code Linking",
                            "authors": [
                                {
                                    "authorId": "3041146",
                                    "name": "Sergej Wildemann"
                                },
                                {
                                    "authorId": "2257974973",
                                    "name": "Claudia Nieder\u00e9e"
                                },
                                {
                                    "authorId": "2098749",
                                    "name": "Erick Elejalde"
                                }
                            ],
                            "year": 2023,
                            "venue": "Social science computer review",
                            "n_citations": 0
                        },
                        "score": 0.71484375
                    }
                ],
                "format": "synthesis",
                "table": null,
                "model": "claude-3-7-sonnet-20250219"
            },
            {
                "title": "Contextual Understanding Capabilities",
                "tldr": "BERT's advanced contextual understanding capabilities enable it to effectively handle polysemy and contextual meaning variations, while FastText produces static word embeddings regardless of context. This fundamental difference makes BERT significantly more powerful for languages with complex morphology and free word order. (7 sources)",
                "text": "\nThe most significant advantage of BERT over FastText lies in its sophisticated contextual understanding capabilities. While FastText produces static word embeddings where each word has a single representation regardless of its usage context, BERT generates dynamic, context-dependent embeddings that can represent different meanings of the same word based on surrounding text <Paper corpusId=\"227230531\" paperTitle=\"(Baruah et al., 2020)\" isShortName></Paper> <Paper corpusId=\"237253665\" paperTitle=\"(Hussain et al., 2021)\" isShortName></Paper>.\n\nThis contextual awareness makes BERT particularly effective at handling polysemy\u2014words with multiple potential meanings. Traditional embedding approaches like Word2Vec, GloVe, and FastText fail to capture these semantic nuances since they produce identical representations for a word regardless of its contextual meaning <Paper corpusId=\"237253665\" paperTitle=\"(Hussain et al., 2021)\" isShortName></Paper>. BERT, however, directly encodes word meanings based on contextual information through its transformer architecture and attention mechanisms, effectively addressing the polysemy issue <Paper corpusId=\"266566817\" paperTitle=\"(Bo et al., 2024)\" isShortName></Paper> <Paper corpusId=\"239459434\" paperTitle=\"(Xinxi, 2021)\" isShortName></Paper>.\n\nFor morphologically complex languages, BERT's contextual understanding provides even greater advantages. Languages with fusional morphology often suffer from disambiguation problems that BERT can handle effectively. Unlike FastText, which considers only a limited context of a few surrounding words, BERT processes a much broader context including words, sentences, and their order <Paper corpusId=\"226337596\" paperTitle=\"(Kapociute-Dzikiene et al., 2020)\" isShortName></Paper>. This capability makes BERT significantly more suitable for languages with relatively free word order such as Latvian, Lithuanian, and Russian, while FastText remains adequate primarily for languages with strict word order like English <Paper corpusId=\"226337596\" paperTitle=\"(Kapociute-Dzikiene et al., 2020)\" isShortName></Paper>.\n\nThe fundamental architectural differences between these approaches reflect their distinct capabilities in handling contextual meaning. FastText relies on subword information to address out-of-vocabulary terms, while contextualized models like BERT and its variants (such as RoBERTa) learn word contexts from large-scale text corpora <Paper corpusId=\"272359317\" paperTitle=\"(Malik et al., 2024)\" isShortName></Paper>. This more sophisticated approach to context awareness explains why BERT-based embeddings consistently generate better results than FastText across diverse datasets <Paper corpusId=\"238648863\" paperTitle=\"(Jbene et al., 2021)\" isShortName></Paper>.",
                "citations": [
                    {
                        "id": "(Baruah et al., 2020)",
                        "snippets": [
                            "The word embeddings produced by fastText is static in nature. Each word has a single embedding irrespective of the context in which the word appears. Static embeddings fail to handle polysemy. The embeddings produced by BERT are contextualized embeddings. The same word may have multiple embeddings depending on the context in which it appears."
                        ],
                        "paper": {
                            "corpus_id": 227230531,
                            "title": "IIITG-ADBU at SemEval-2020 Task 12: Comparison of BERT and BiLSTM in Detecting Offensive Language",
                            "authors": [
                                {
                                    "authorId": "48140293",
                                    "name": "Arup Baruah"
                                },
                                {
                                    "authorId": "2006187174",
                                    "name": "K. Das"
                                },
                                {
                                    "authorId": "2568004",
                                    "name": "F. Barbhuiya"
                                },
                                {
                                    "authorId": "144710196",
                                    "name": "K. Dey"
                                }
                            ],
                            "year": 2020,
                            "venue": "International Workshop on Semantic Evaluation",
                            "n_citations": 2
                        },
                        "score": 0.6982421875
                    },
                    {
                        "id": "(Hussain et al., 2021)",
                        "snippets": [
                            "BERT outperforms both CNN and LSTM. The reason for the better performance of BERT is that it learns contextualized embeddings in a bidirectional way. In natural language, a word is likely to convey multiple meanings based on the context in which it is used. Word2vec, fasttext, and glove produce the same representations of a word even if it has different meanings in different contexts. BERT, on the other hand, produces context-dependent embeddings of a word."
                        ],
                        "paper": {
                            "corpus_id": 237253665,
                            "title": "Pharmacovigilance with Transformers: A Framework to Detect Adverse Drug Reactions Using BERT Fine-Tuned with FARM",
                            "authors": [
                                {
                                    "authorId": "2113619919",
                                    "name": "Sajid Hussain"
                                },
                                {
                                    "authorId": "1777100",
                                    "name": "H. Afzal"
                                },
                                {
                                    "authorId": "5644298",
                                    "name": "Ramsha Saeed"
                                },
                                {
                                    "authorId": "2740595",
                                    "name": "N. Iltaf"
                                },
                                {
                                    "authorId": "33827057",
                                    "name": "M. Umair"
                                }
                            ],
                            "year": 2021,
                            "venue": "Computational and Mathematical Methods in Medicine",
                            "n_citations": 24
                        },
                        "score": 0.71142578125
                    },
                    {
                        "id": "(Bo et al., 2024)",
                        "snippets": [
                            "In contrast, BERT is a pre-trained language model trained on a large-scale corpus, based on the multi-layer Transformer encoder architecture, utilizing attention mechanisms to directly encode word meanings, effectively addressing the polysemy issue based on contextual information (Xinxi, 2021). BERT places greater emphasis on pre-training word meanings, allowing downstream NLP tasks to perform fine-tuning operations based on the specific task's requirements."
                        ],
                        "paper": {
                            "corpus_id": 266566817,
                            "title": "Empowering Medical Data Analysis: An Advanced Deep Fusion Model for Sorting Medicine Document",
                            "authors": [
                                {
                                    "authorId": "2276550856",
                                    "name": "Guan Bo"
                                },
                                {
                                    "authorId": "2276574822",
                                    "name": "Shanshan Wang"
                                },
                                {
                                    "authorId": "2276553236",
                                    "name": "Zhang Qing"
                                },
                                {
                                    "authorId": "2276554085",
                                    "name": "Pang Bo"
                                },
                                {
                                    "authorId": "2276746559",
                                    "name": "Zuo Yan"
                                }
                            ],
                            "year": 2024,
                            "venue": "IEEE Access",
                            "n_citations": 2
                        },
                        "score": 0.82958984375
                    },
                    {
                        "id": "(Xinxi, 2021)",
                        "snippets": [
                            "Over the past decades, natural language processing (NLP) has been a hot topic in many fields, e.g., sentiment analysis and news topic classification. As a very powerful language pre-training model, Bidirectional Encoder Representations from Transformers (BERT) has achieved promising results in many language understanding tasks including text classification. However, fine-tune BERT to adapt different text classification task efficiently is a critical problem that needs improvement. In this paper, a general solution is proposed for BERT fine-tuning on single text classification task. Compared with other traditional fine-tune strategies without any pre-training step, the performance of BERT is boosted by pre-training withintask data. Moreover, the proposed solution obtains superior results on six widely-used text classification datasets."
                        ],
                        "paper": {
                            "corpus_id": 239459434,
                            "title": "Single task fine-tune BERT for text classification",
                            "authors": [
                                {
                                    "authorId": "2096537143",
                                    "name": "Z. Xinxi"
                                }
                            ],
                            "year": 2021,
                            "venue": "Other Conferences",
                            "n_citations": 5
                        },
                        "score": 0
                    },
                    {
                        "id": "(Kapociute-Dzikiene et al., 2020)",
                        "snippets": [
                            "As seen from Figure 2, BERT vectorization is a better choice compared to fastText for all morphologically complex languages for all datasets, and this is not surprising. Morphologically complex languages (especially fusional languages) suffer from disambiguation problems, but BERT has mechanisms that are able to vectorize even those words that are written the same but have different meanings, depending on their context, differently. Despite the fact that fastText embeddings are also trained to consider a context around the target word, that context is restricted to only a few words. Despite this, fastText is a suitable vectorization solution for languages (such as English) with strict word order in a sentence. In contrast, BERT is able to consider a much broader context (words, sentences, their order) compared to fastText and is, therefore, more suitable for languages that have a relatively free word order in a sentence (such as Latvian, Lithuanian, and Russian)."
                        ],
                        "paper": {
                            "corpus_id": 226337596,
                            "title": "Intent Detection Problem Solving via Automatic DNN Hyperparameter Optimization",
                            "authors": [
                                {
                                    "authorId": "1403992181",
                                    "name": "J. Kapo\u010di\u016bt\u0117-Dzikien\u0117"
                                },
                                {
                                    "authorId": "3288489",
                                    "name": "K. Balodis"
                                },
                                {
                                    "authorId": "3283640",
                                    "name": "Raivis Skadins"
                                }
                            ],
                            "year": 2020,
                            "venue": "Applied Sciences",
                            "n_citations": 13
                        },
                        "score": 0.689453125
                    },
                    {
                        "id": "(Malik et al., 2024)",
                        "snippets": [
                            "In the proposed work, three important issues in the field of natural language processing: polysemy, contextual meaning, and out-of-vocabulary terms were addressed. We employed three cutting-edge models to solve these problems: FastText, Elmo, and RoBERTa. \n\nFastText is a text categorization and representation learning library that is intended to be quick and efficient. It handles out-of-vocabulary terms using sub-word information, making it appropriate for NLP jobs. Embeddings from Language Models (ELMO) is a contextualized word representation model that learns word contexts from large-scale text corpora. It captures the contextual meaning of words, making it ideally suited for NLP jobs requiring context awareness. A Robustly Optimized BERT Pre-training Approach (Roberta) is a transformer-based language model that has been refined using large-scale text corpora."
                        ],
                        "paper": {
                            "corpus_id": 272359317,
                            "title": "Attention-aware with stacked embedding for sentiment analysis of student feedback through deep learning techniques",
                            "authors": [
                                {
                                    "authorId": "2327353221",
                                    "name": "Shanza Zafar Malik"
                                },
                                {
                                    "authorId": "2294254945",
                                    "name": "Khalid Iqbal"
                                },
                                {
                                    "authorId": "2319267821",
                                    "name": "Muhammad Sharif"
                                },
                                {
                                    "authorId": "2151313189",
                                    "name": "Yaser Ali Shah"
                                },
                                {
                                    "authorId": "2060253868",
                                    "name": "Amaad Khalil"
                                },
                                {
                                    "authorId": "2327345984",
                                    "name": "Muhammad Abeer Irfan"
                                },
                                {
                                    "authorId": "2316178914",
                                    "name": "Joann Rosak-Szyrocka"
                                }
                            ],
                            "year": 2024,
                            "venue": "PeerJ Computer Science",
                            "n_citations": 1
                        },
                        "score": 0.7294921875
                    },
                    {
                        "id": "(Jbene et al., 2021)",
                        "snippets": [
                            "Comparing the results of the model using the different kinds of embeddings, the version with BERT embeddings generates results higher than FastText ones for all the datasets, which can be explained by the difference in dataset size on which the two models were pre-trained on, and most importantly, the capability of BERT in capturing and generating context-dependent word embeddings."
                        ],
                        "paper": {
                            "corpus_id": 238648863,
                            "title": "Deep Neural Network and Boosting Based Hybrid Quality Ranking for e-Commerce Product Search",
                            "authors": [
                                {
                                    "authorId": "1471736056",
                                    "name": "Mourad Jbene"
                                },
                                {
                                    "authorId": "3377410",
                                    "name": "Smail Tigani"
                                },
                                {
                                    "authorId": "1916966",
                                    "name": "Rachid Saadane"
                                },
                                {
                                    "authorId": "2782896",
                                    "name": "Abdellah Chehri"
                                }
                            ],
                            "year": 2021,
                            "venue": "Big Data and Cognitive Computing",
                            "n_citations": 6
                        },
                        "score": 0.84765625
                    }
                ],
                "format": "synthesis",
                "table": null,
                "model": "claude-3-7-sonnet-20250219"
            },
            {
                "title": "Domain Adaptation and Generalization",
                "tldr": "BERT demonstrates superior domain adaptation capabilities compared to FastText, showing more robust performance when transferring between general and specific domains. FastText excels at memorization of seen data while BERT shows stronger generalization to unseen examples, creating complementary strengths for different filtering applications. (5 sources)",
                "text": "\nWhen comparing domain adaptation and generalization capabilities, BERT and FastText show significant differences that affect their suitability for data filtering applications. BERT consistently demonstrates superior robustness when adapting to new domains, while FastText struggles significantly when transferring between general and specific domains <Paper corpusId=\"216914042\" paperTitle=\"(Breit et al., 2020)\" isShortName></Paper>. This domain adaptation gap is substantial enough that in some studies, FastText failed to outperform even naive baselines when applied to new domains <Paper corpusId=\"216914042\" paperTitle=\"(Breit et al., 2020)\" isShortName></Paper>.\n\nThe generalization capabilities of these models reveal complementary strengths. In e-commerce search query classification, FastText exhibits stronger memorization abilities, performing better on test queries that had appeared in the training set. In contrast, BERT shows superior generalization to unseen or low-frequency queries <Paper corpusId=\"260379057\" paperTitle=\"(Ning et al., 2023)\" isShortName></Paper>. This pattern suggests FastText excels at \"remembering\" while BERT excels at \"generalizing,\" which creates different use cases depending on whether the filtering task prioritizes recognizing previously seen patterns or adapting to novel content.\n\nFor languages with complex morphology and relatively free word order (such as Latvian, Lithuanian, and Russian), BERT's generalization capabilities prove particularly valuable. Its ability to process broader contextual information makes it substantially more suitable for these languages compared to FastText, which performs adequately primarily for languages with strict word order like English <Paper corpusId=\"226337596\" paperTitle=\"(Kapociute-Dzikiene et al., 2020)\" isShortName></Paper>.\n\nHowever, the generalization advantage of BERT diminishes in certain scenarios. With small training datasets, FastText coupled with domain-specific word embeddings can perform equally well or better than BERT, even when BERT is pre-trained on domain-specific data <Paper corpusId=\"227231089\" paperTitle=\"(Edwards et al., 2020)\" isShortName></Paper>. This suggests that in data-constrained environments, the simpler FastText approach may offer more reliable generalization.\n\nIn specialized domains like clinical text analysis, pre-trained BERT models without domain adaptation show limited advantages over simpler approaches. Studies have found that in clinical text classification, standard BERT without domain-specific pre-training can be outperformed by simpler approaches like TextCNN and doesn't demonstrate marked advantages over Word2Vec-based methods <Paper corpusId=\"269973968\" paperTitle=\"(Xie et al., 2024)\" isShortName></Paper>. This highlights the critical importance of domain adaptation for BERT to fully leverage its capabilities in specialized fields.\n\nThese findings suggest that optimal pre-training data filtering may involve a hybrid approach\u2014using FastText for efficiency in recognizing common patterns and BERT for its superior generalization to novel or ambiguous content requiring deeper contextual understanding.",
                "citations": [
                    {
                        "id": "(Breit et al., 2020)",
                        "snippets": [
                            "As can be observed, BERT is clearly better than FastText in all measures. In fact, perhaps surprisingly, FastText does not perform better than a naive baseline that retrieves all instances as true",
                            "Interestingly, FastText faces a massive challenge in adapting domains and generalising from the general to the specific domains. However, BERT shows to be much more robust to domain changes."
                        ],
                        "paper": {
                            "corpus_id": 216914042,
                            "title": "WiC-TSV: An Evaluation Benchmark for Target Sense Verification of Words in Context",
                            "authors": [
                                {
                                    "authorId": "1453876854",
                                    "name": "Anna Breit"
                                },
                                {
                                    "authorId": "144688662",
                                    "name": "Artem Revenko"
                                },
                                {
                                    "authorId": "1667035673",
                                    "name": "Kiamehr Rezaee"
                                },
                                {
                                    "authorId": "1717641",
                                    "name": "Mohammad Taher Pilehvar"
                                },
                                {
                                    "authorId": "1387447871",
                                    "name": "Jos\u00e9 Camacho-Collados"
                                }
                            ],
                            "year": 2020,
                            "venue": "Conference of the European Chapter of the Association for Computational Linguistics",
                            "n_citations": 26
                        },
                        "score": 0.7080078125
                    },
                    {
                        "id": "(Ning et al., 2023)",
                        "snippets": [
                            "We find that the FastText model can do better on the training set and testing set (T+0 day), while the BERT model performs better on the testing set (T+1 day). As 99.89% of test queries on T+0 day have appeared in the training set, it means that the FastText model can be remembered better. On the other hand, as only 57.47% of test queries on T+1 day has exposed in the training set, it means that the BERT model can generalize better on some unseen or low-frequency search query. In other words, in the E-commerce search query classification task, the FastText is better at remembering while the BERT is better at generalization."
                        ],
                        "paper": {
                            "corpus_id": 260379057,
                            "title": "Towards Better Query Classification with Multi-Expert Knowledge Condensation in JD Ads Search",
                            "authors": [
                                {
                                    "authorId": "66266326",
                                    "name": "Kun-Peng Ning"
                                },
                                {
                                    "authorId": "2053434618",
                                    "name": "Ming Pang"
                                },
                                {
                                    "authorId": "2072874946",
                                    "name": "Zheng Fang"
                                },
                                {
                                    "authorId": "2226458237",
                                    "name": "Xue Jiang"
                                },
                                {
                                    "authorId": "2226511841",
                                    "name": "Xi-Wei Zhao"
                                },
                                {
                                    "authorId": "1949218014",
                                    "name": "Changping Peng"
                                },
                                {
                                    "authorId": "2146396439",
                                    "name": "Zhangang Lin"
                                },
                                {
                                    "authorId": "22528137",
                                    "name": "Jinghe Hu"
                                },
                                {
                                    "authorId": "2118926502",
                                    "name": "Jingping Shao"
                                }
                            ],
                            "year": 2023,
                            "venue": "arXiv.org",
                            "n_citations": 0
                        },
                        "score": 0.91552734375
                    },
                    {
                        "id": "(Kapociute-Dzikiene et al., 2020)",
                        "snippets": [
                            "As seen from Figure 2, BERT vectorization is a better choice compared to fastText for all morphologically complex languages for all datasets, and this is not surprising. Morphologically complex languages (especially fusional languages) suffer from disambiguation problems, but BERT has mechanisms that are able to vectorize even those words that are written the same but have different meanings, depending on their context, differently. Despite the fact that fastText embeddings are also trained to consider a context around the target word, that context is restricted to only a few words. Despite this, fastText is a suitable vectorization solution for languages (such as English) with strict word order in a sentence. In contrast, BERT is able to consider a much broader context (words, sentences, their order) compared to fastText and is, therefore, more suitable for languages that have a relatively free word order in a sentence (such as Latvian, Lithuanian, and Russian)."
                        ],
                        "paper": {
                            "corpus_id": 226337596,
                            "title": "Intent Detection Problem Solving via Automatic DNN Hyperparameter Optimization",
                            "authors": [
                                {
                                    "authorId": "1403992181",
                                    "name": "J. Kapo\u010di\u016bt\u0117-Dzikien\u0117"
                                },
                                {
                                    "authorId": "3288489",
                                    "name": "K. Balodis"
                                },
                                {
                                    "authorId": "3283640",
                                    "name": "Raivis Skadins"
                                }
                            ],
                            "year": 2020,
                            "venue": "Applied Sciences",
                            "n_citations": 13
                        },
                        "score": 0.689453125
                    },
                    {
                        "id": "(Edwards et al., 2020)",
                        "snippets": [
                            "In settings with small training datasets a simple method like fastText coupled with domain-specific word embeddings performs equally well or better than BERT, even when pre-trained on domain-specific data."
                        ],
                        "paper": {
                            "corpus_id": 227231089,
                            "title": "Go Simple and Pre-Train on Domain-Specific Corpora: On the Role of Training Data for Text Classification",
                            "authors": [
                                {
                                    "authorId": "1383074767",
                                    "name": "A. Edwards"
                                },
                                {
                                    "authorId": "1387447871",
                                    "name": "Jos\u00e9 Camacho-Collados"
                                },
                                {
                                    "authorId": "2750681",
                                    "name": "H\u00e9l\u00e8ne de Ribaupierre"
                                },
                                {
                                    "authorId": "1762890",
                                    "name": "A. Preece"
                                }
                            ],
                            "year": 2020,
                            "venue": "International Conference on Computational Linguistics",
                            "n_citations": 25
                        },
                        "score": 0.9541015625
                    },
                    {
                        "id": "(Xie et al., 2024)",
                        "snippets": [
                            "When comparing VSM-based methods with Word2Vec-based methods, we equate MLP and FastText due to their similar complexities as shallow neural networks. FastText, however, demonstrates better performance than MLP (0.696 vs 0.670 in Acc and 0.767 vs 0.727 in F 1 ), indicating Word2Vec's superior word representation capabilities",
                            "Interestingly, original BERT, despite its wide-ranging successes in natural language processing, does not show a marked advantage over Word2Vec in our analysis and is slightly outperformed by TextCNN. This observation is further evidenced by the improved performance of MLM BERT over the original BERT, suggesting that contextual word embeddings benefit from additional data and training. This leads us to conclude that for specialized domain language tasks like clinical text analysis, pre-training is essential to fully leverage BERT's capabilities."
                        ],
                        "paper": {
                            "corpus_id": 269973968,
                            "title": "Identification of mycoplasma pneumonia in children based on fusion of multi-modal clinical free-text description and structured test data",
                            "authors": [
                                {
                                    "authorId": "2302859526",
                                    "name": "Jingna Xie"
                                },
                                {
                                    "authorId": "2364687744",
                                    "name": "Yingshuo Wang"
                                },
                                {
                                    "authorId": "1576956902",
                                    "name": "Qiuyang Sheng"
                                },
                                {
                                    "authorId": "2283403594",
                                    "name": "Xiaoqing Liu"
                                },
                                {
                                    "authorId": "2336703688",
                                    "name": "Jing Li"
                                },
                                {
                                    "authorId": "47450065",
                                    "name": "Fenglei Sun"
                                },
                                {
                                    "authorId": "2155376102",
                                    "name": "Yuqi Wang"
                                },
                                {
                                    "authorId": "2302784979",
                                    "name": "Shuxian Li"
                                },
                                {
                                    "authorId": "2110531396",
                                    "name": "Yiming Li"
                                },
                                {
                                    "authorId": "2302875109",
                                    "name": "Yizhou Yu"
                                },
                                {
                                    "authorId": "2303601854",
                                    "name": "Gang Yu"
                                }
                            ],
                            "year": 2024,
                            "venue": "Health Informatics Journal",
                            "n_citations": 1
                        },
                        "score": 0.69921875
                    }
                ],
                "format": "synthesis",
                "table": null,
                "model": "claude-3-7-sonnet-20250219"
            },
            {
                "title": "Practical Applications and Use Cases",
                "tldr": "FastText and BERT offer complementary strengths for different data filtering scenarios, with FastText excelling in resource-constrained environments and real-time applications, while BERT delivers superior performance for complex linguistic tasks requiring deep contextual understanding. (5 sources)",
                "text": "\n- **High-volume data filtering with limited computational resources**: FastText provides a practical solution when processing massive datasets with constrained computational resources, offering fast training and no dependency on GPUs while maintaining reasonable performance. <Paper corpusId=\"265162335\" paperTitle=\"(Wildemann et al., 2023)\" isShortName></Paper>\n\n- **Real-time content moderation and filtering**: For applications requiring immediate filtering decisions, FastText's computational efficiency makes it suitable for real-time processing where BERT would be prohibitively slow or resource-intensive. <Model name=\"Anthropic\" version=\"claude-3-7-sonnet-20250219\">\n\n- **Requirements extraction in software engineering**: FastText's superior recall characteristics (0.93 compared to BERT's 0.8) make it particularly valuable for extracting more relevant sentences and text patterns associated with requirements, where comprehensiveness is prioritized over precision. <Paper corpusId=\"246608158\" paperTitle=\"(Ivanov et al., 2022)\" isShortName></Paper>\n\n- **Small dataset scenarios**: In settings with limited training data, FastText coupled with domain-specific word embeddings can perform equally well or better than BERT, even when BERT is pre-trained on domain-specific data. <Paper corpusId=\"227231089\" paperTitle=\"(Edwards et al., 2020)\" isShortName></Paper>\n\n- **Knowledge distillation applications**: FastText can serve as a student model distilled from more powerful but computationally expensive models like BERT or ERNIE, significantly improving classification accuracy while maintaining computational efficiency. This approach is particularly useful for applications like chatbot intention classification. <Paper corpusId=\"246738423\" paperTitle=\"(Guo et al., 2022)\" isShortName></Paper>\n\n- **Production environments with strict performance-runtime constraints**: FastText-based models can offer a better performance-runtime tradeoff in production environments, with one study showing a LSTM architecture with FastText embeddings having 160% fewer parameters than BERT while achieving comparable or slightly better performance. <Paper corpusId=\"252901000\" paperTitle=\"(Groger et al., 2022)\" isShortName></Paper>\n\n- **Financial text analysis**: FastText has demonstrated superior performance over BERT specifically for banking datasets, suggesting particular value in financial domain applications. <Paper corpusId=\"265162335\" paperTitle=\"(Wildemann et al., 2023)\" isShortName></Paper>\n\n- **Hybrid filtering approaches**: Combining FastText for efficient processing of straightforward content with BERT for more ambiguous or context-dependent filtering decisions can optimize both performance and computational efficiency in large-scale data filtering pipelines. <Model name=\"Anthropic\" version=\"claude-3-7-sonnet-20250219\">",
                "citations": [
                    {
                        "id": "(Wildemann et al., 2023)",
                        "snippets": [
                            "Overall, BERT achieves the best performance on the F 1 measure in all datasets except for Bank",
                            "fastText proves to be the second-best classifier and even outperforms BERT on the Bank dataset. The additional use of pre-trained word embeddings clearly benefits the task and shows an average increase in F 1 -score of 15.3% across all datasets. This suggests a further improvement when using larger domain-specific corpora. Moreover, its fast training and the lack of dependency on GPUs may be advantageous for many practical applications."
                        ],
                        "paper": {
                            "corpus_id": 265162335,
                            "title": "Bridging Qualitative Data Silos: The Potential of Reusing Codings Through Machine Learning Based Cross-Study Code Linking",
                            "authors": [
                                {
                                    "authorId": "3041146",
                                    "name": "Sergej Wildemann"
                                },
                                {
                                    "authorId": "2257974973",
                                    "name": "Claudia Nieder\u00e9e"
                                },
                                {
                                    "authorId": "2098749",
                                    "name": "Erick Elejalde"
                                }
                            ],
                            "year": 2023,
                            "venue": "Social science computer review",
                            "n_citations": 0
                        },
                        "score": 0.71484375
                    },
                    {
                        "id": "(Ivanov et al., 2022)",
                        "snippets": [
                            "Result of the first phase of evaluation is presented in Table 1. As it was expected, more advanced model (BERT) showed better results in terms of F1-score. BERT-based model showed high precision (0.92) and lower recall (0.8). Results of the BERT model is available at https://bit.ly/3oPElMm. However, the values of precision and recall metrics behave differently for the fastText and ELMobased baselines. fastText-based classifier showed better Recall (0.93) comparing with other architectures. This property might be useful in some cases when it is necessary to extract more relevant sentences and text patterns associated with requirements."
                        ],
                        "paper": {
                            "corpus_id": 246608158,
                            "title": "Extracting Software Requirements from Unstructured Documents",
                            "authors": [
                                {
                                    "authorId": "2072422334",
                                    "name": "V. Ivanov"
                                },
                                {
                                    "authorId": "2883654",
                                    "name": "Andrey Sadovykh"
                                },
                                {
                                    "authorId": "3361049",
                                    "name": "Alexandr Naumchev"
                                },
                                {
                                    "authorId": "36000045",
                                    "name": "A. Bagnato"
                                },
                                {
                                    "authorId": "2117465706",
                                    "name": "K. Yakovlev"
                                }
                            ],
                            "year": 2022,
                            "venue": "International Joint Conference on the Analysis of Images, Social Networks and Texts",
                            "n_citations": 11
                        },
                        "score": 0.775390625
                    },
                    {
                        "id": "(Edwards et al., 2020)",
                        "snippets": [
                            "In settings with small training datasets a simple method like fastText coupled with domain-specific word embeddings performs equally well or better than BERT, even when pre-trained on domain-specific data."
                        ],
                        "paper": {
                            "corpus_id": 227231089,
                            "title": "Go Simple and Pre-Train on Domain-Specific Corpora: On the Role of Training Data for Text Classification",
                            "authors": [
                                {
                                    "authorId": "1383074767",
                                    "name": "A. Edwards"
                                },
                                {
                                    "authorId": "1387447871",
                                    "name": "Jos\u00e9 Camacho-Collados"
                                },
                                {
                                    "authorId": "2750681",
                                    "name": "H\u00e9l\u00e8ne de Ribaupierre"
                                },
                                {
                                    "authorId": "1762890",
                                    "name": "A. Preece"
                                }
                            ],
                            "year": 2020,
                            "venue": "International Conference on Computational Linguistics",
                            "n_citations": 25
                        },
                        "score": 0.9541015625
                    },
                    {
                        "id": "(Guo et al., 2022)",
                        "snippets": [
                            "In this paper, we use knowledge of the ERNIE model to distill the FastText model; the ERNIE model works as a teacher model to predict the massive online unlabeled data for data enhancement, and then guides the training of the student model of FastText with better computational efficiency. The FastText model is distilled by the ERNIE model in chatbot intention classification. This not only guarantees the superiority of its original computational performance, but also the intention classification accuracy has been significantly improved."
                        ],
                        "paper": {
                            "corpus_id": 246738423,
                            "title": "Application of Knowledge Distillation Based on Transfer Learning of ERNIE Model in Intelligent Dialogue Intention Recognition",
                            "authors": [
                                {
                                    "authorId": "2119112458",
                                    "name": "Shiguang Guo"
                                },
                                {
                                    "authorId": "2117944486",
                                    "name": "Qing Wang"
                                }
                            ],
                            "year": 2022,
                            "venue": "Italian National Conference on Sensors",
                            "n_citations": 8
                        },
                        "score": 0.6806640625
                    },
                    {
                        "id": "(Groger et al., 2022)",
                        "snippets": [
                            "The best performing model consists of pre-trained FastText embeddings followed by a stack of bidirectional LSTM layers. This rather simple architecture performs more that 20% better in F 1 compared to a support vector machine baseline with TF-IDF feature engineering and also beats transfer learning with a pretrained BERT model 3 by a small margin. On the one hand, this indicates that such large language models as BERT can generalize pretty well even in the presence of small data. On the other hand, BERT comes with an increase of 160% in the number of parameters and thus significantly higher inference time compared to the winning model. Hence, even with more data and an expected increase in the BERT model performance, the LSTM architecture will show the better performance-runtime tradeoff."
                        ],
                        "paper": {
                            "corpus_id": 252901000,
                            "title": "Assessing Guest Nationality Composition from Hotel Reviews",
                            "authors": [
                                {
                                    "authorId": "2026999646",
                                    "name": "Fabian Gr\u00f6ger"
                                },
                                {
                                    "authorId": "1715783",
                                    "name": "M. Pouly"
                                },
                                {
                                    "authorId": "2187827840",
                                    "name": "Flavia Tinner"
                                },
                                {
                                    "authorId": "39589520",
                                    "name": "Leif Brandes"
                                }
                            ],
                            "year": 2022,
                            "venue": "Swiss Conference on Data Science",
                            "n_citations": 0
                        },
                        "score": 0.76904296875
                    }
                ],
                "format": "list",
                "table": null,
                "model": "claude-3-7-sonnet-20250219"
            }
        ],
        "cost": 0.136716
    }
}