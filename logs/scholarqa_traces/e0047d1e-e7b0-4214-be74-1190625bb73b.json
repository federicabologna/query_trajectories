{
    "query": "knowledge distillation teacher model typically larger powerful network student model work that teacher model is size matched or smaller more in knowledge the student model for knowledge transfer effective",
    "user_id": "lib_user",
    "task_id": "e0047d1e-e7b0-4214-be74-1190625bb73b",
    "timestamp": "2025-06-24T00:23:51.529074",
    "n_retrieval": 256,
    "n_retrieved": 273,
    "n_candidates": 50,
    "n_rerank": 50,
    "opt_in": true,
    "total_cost": 0.44516700000000003,
    "decomposed_query": {
        "rewritten_query": "Knowledge distillation where teacher model is typically a larger powerful network while student model is size-matched or smaller, focusing on effective knowledge transfer to the student model.",
        "keyword_query": "knowledge distillation teacher model student model size-matched smaller knowledge transfer",
        "search_filters": {
            "fieldsOfStudy": "Computer Science"
        },
        "cost": 0.009939,
        "model": "claude-3-7-sonnet-20250219"
    },
    "candidates": [
        {
            "title": "Improving the Learning of Code Review Successive Tasks with Cross-Task Knowledge Distillation",
            "venue": "Proc. ACM Softw. Eng.",
            "year": 2024,
            "reference_count": 43,
            "citation_count": 10,
            "influential_citation_count": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2402.02063, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "1470757062",
                    "name": "O. Sghaier"
                },
                {
                    "authorId": "9460712",
                    "name": "H. Sahraoui"
                }
            ],
            "abstract": "\n Code review is a fundamental process in software development that plays a pivotal role in ensuring code quality and reducing the likelihood of errors and bugs. However, code review can be complex, subjective, and time-consuming.\n Quality estimation\n ,\n comment generation\n , and\n code refinement\n constitute the three key tasks of this process, and their automation has traditionally been addressed separately in the literature using different approaches. In particular, recent efforts have focused on fine-tuning pre-trained language models to aid in code review tasks, with each task being considered in isolation. We believe that these tasks are interconnected, and their fine-tuning should consider this interconnection. In this paper, we introduce a novel deep-learning architecture, named DISCOREV, which employs cross-task knowledge distillation to address these tasks simultaneously. In our approach, we utilize a cascade of models to enhance both\n comment generation\n and\n code refinement\n models. The fine-tuning of the\n comment generation\n model is guided by the\n code refinement\n model, while the fine-tuning of the\n code refinement\n model is guided by the\n quality estimation\n model. We implement this guidance using two strategies: a feedback-based learning objective and an embedding alignment objective. We evaluate DISCOREV by comparing it to state-of-the-art methods based on independent training and fine-tuning. Our results show that our approach generates better review comments, as measured by the\n BLEU\n score, as well as more accurate\n code refinement\n according to the\n CodeBLEU\n score.\n",
            "corpus_id": 267413258,
            "sentences": [
                {
                    "corpus_id": "267413258",
                    "title": "Improving the Learning of Code Review Successive Tasks with Cross-Task Knowledge Distillation",
                    "text": "Knowledge distillation is a popular technique that was first introduced by Hinton et al. [18] as a method to transfer knowledge from a complex model, also known as the teacher, to a smaller and faster model, called the student. The main goal of knowledge distillation is to transfer the learned knowledge of the teacher model to the student model so that it can achieve comparable or even better performance on a target task. \n\nThe process of knowledge distillation consists of training a teacher model on a large dataset to accomplish a specific task. Then, the student model is trained to mimic the behavior of the teacher model by minimizing the distance between their output distributions on the same dataset, typically using the Kullback-Leibler divergence (KL-divergence) as the distance metric [20,21]. \n\nFor instance, this concept is employed in model compression where the knowledge is transferred from a large and complex neural network (i.e., the teacher) to a smaller and simpler neural network (i.e., the student). The goal of knowledge distillation is to make the student network learn the same function as the teacher network but with fewer parameters. \n\nCross-task knowledge distillation is a recent extension of knowledge distillation that enables the transfer of knowledge from one task to another [22,43,45,47]. Instead of transferring knowledge from a complex model to a smaller one for the same task, cross-task knowledge distillation transfers knowledge from a model trained on one task, referred to as the source task, to a model trained on a different task, referred to as the target task. This allows the target model to improve its performance, even if the two tasks are not directly related.",
                    "score": 0.7882886571772505,
                    "section_title": "Knowledge distillation",
                    "char_start_offset": 12698,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 227
                        },
                        {
                            "start": 228,
                            "end": 425
                        },
                        {
                            "start": 428,
                            "end": 552
                        },
                        {
                            "start": 553,
                            "end": 809
                        },
                        {
                            "start": 812,
                            "end": 1027
                        },
                        {
                            "start": 1028,
                            "end": 1167
                        },
                        {
                            "start": 1170,
                            "end": 1330
                        },
                        {
                            "start": 1331,
                            "end": 1613
                        },
                        {
                            "start": 1614,
                            "end": 1718
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 801,
                            "end": 805,
                            "matchedPaperCorpusId": "37718089"
                        },
                        {
                            "start": 1320,
                            "end": 1323,
                            "matchedPaperCorpusId": "247011924"
                        },
                        {
                            "start": 1323,
                            "end": 1326,
                            "matchedPaperCorpusId": "219965004"
                        },
                        {
                            "start": 1326,
                            "end": 1329,
                            "matchedPaperCorpusId": "209078813"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.994140625
                }
            ],
            "relevance_judgement": 0.994140625,
            "relevance_judgment_input_expanded": "# Title: Improving the Learning of Code Review Successive Tasks with Cross-Task Knowledge Distillation\n# Venue: Proc. ACM Softw. Eng.\n# Authors: O. Sghaier, H. Sahraoui\n## Abstract\n\n Code review is a fundamental process in software development that plays a pivotal role in ensuring code quality and reducing the likelihood of errors and bugs. However, code review can be complex, subjective, and time-consuming.\n Quality estimation\n ,\n comment generation\n , and\n code refinement\n constitute the three key tasks of this process, and their automation has traditionally been addressed separately in the literature using different approaches. In particular, recent efforts have focused on fine-tuning pre-trained language models to aid in code review tasks, with each task being considered in isolation. We believe that these tasks are interconnected, and their fine-tuning should consider this interconnection. In this paper, we introduce a novel deep-learning architecture, named DISCOREV, which employs cross-task knowledge distillation to address these tasks simultaneously. In our approach, we utilize a cascade of models to enhance both\n comment generation\n and\n code refinement\n models. The fine-tuning of the\n comment generation\n model is guided by the\n code refinement\n model, while the fine-tuning of the\n code refinement\n model is guided by the\n quality estimation\n model. We implement this guidance using two strategies: a feedback-based learning objective and an embedding alignment objective. We evaluate DISCOREV by comparing it to state-of-the-art methods based on independent training and fine-tuning. Our results show that our approach generates better review comments, as measured by the\n BLEU\n score, as well as more accurate\n code refinement\n according to the\n CodeBLEU\n score.\n\n## Knowledge distillation\nKnowledge distillation is a popular technique that was first introduced by Hinton et al. [18] as a method to transfer knowledge from a complex model, also known as the teacher, to a smaller and faster model, called the student. The main goal of knowledge distillation is to transfer the learned knowledge of the teacher model to the student model so that it can achieve comparable or even better performance on a target task. \n\nThe process of knowledge distillation consists of training a teacher model on a large dataset to accomplish a specific task. Then, the student model is trained to mimic the behavior of the teacher model by minimizing the distance between their output distributions on the same dataset, typically using the Kullback-Leibler divergence (KL-divergence) as the distance metric [20,21]. \n\nFor instance, this concept is employed in model compression where the knowledge is transferred from a large and complex neural network (i.e., the teacher) to a smaller and simpler neural network (i.e., the student). The goal of knowledge distillation is to make the student network learn the same function as the teacher network but with fewer parameters. \n\nCross-task knowledge distillation is a recent extension of knowledge distillation that enables the transfer of knowledge from one task to another [22,43,45,47]. Instead of transferring knowledge from a complex model to a smaller one for the same task, cross-task knowledge distillation transfers knowledge from a model trained on one task, referred to as the source task, to a model trained on a different task, referred to as the target task. This allows the target model to improve its performance, even if the two tasks are not directly related.",
            "reference_string": "[267413258 | Sghaier et al. | 2024 | Citations: 10]"
        },
        {
            "title": "Counterclockwise block-by-block knowledge distillation for neural network compression",
            "venue": "Scientific Reports",
            "year": 2025,
            "reference_count": 39,
            "citation_count": 0,
            "influential_citation_count": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": "CCBY",
                "disclaimer": "Notice: Paper or abstract available at https://pmc.ncbi.nlm.nih.gov/articles/PMC11965283, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2353357918",
                    "name": "Xiaowei Lan"
                },
                {
                    "authorId": "2224288563",
                    "name": "Yalin Zeng"
                },
                {
                    "authorId": "2353518848",
                    "name": "Xiaoxia Wei"
                },
                {
                    "authorId": "2305091780",
                    "name": "Tian Zhang"
                },
                {
                    "authorId": "2220744175",
                    "name": "Yiwen Wang"
                },
                {
                    "authorId": "2354079428",
                    "name": "Chao Huang"
                },
                {
                    "authorId": "2327826876",
                    "name": "Weikai He"
                }
            ],
            "abstract": "Model compression is a technique for transforming large neural network models into smaller ones. Knowledge distillation (KD) is a crucial model compression technique that involves transferring knowledge from a large teacher model to a lightweight student model. Existing knowledge distillation methods typically facilitate the knowledge transfer from teacher to student models in one or two stages. This paper introduces a novel approach called counterclockwise block-wise knowledge distillation (CBKD) to optimize the knowledge distillation process. The core idea of CBKD aims to mitigate the generation gap between teacher and student models, facilitating the transmission of intermediate-layer knowledge from the teacher model. It divides both teacher and student models into multiple sub-network blocks, and in each stage of knowledge distillation, only the knowledge from one teacher sub-block is transferred to the corresponding position of a student sub-block. Additionally, in the CBKD process, deeper teacher sub-network blocks are assigned higher compression rates. Extensive experiments on tiny-imagenet-200 and CIFAR-10 demonstrate that the proposed CBKD method can enhance the distillation performance of various mainstream knowledge distillation approaches.",
            "corpus_id": 277507254,
            "sentences": [
                {
                    "corpus_id": "277507254",
                    "title": "Counterclockwise block-by-block knowledge distillation for neural network compression",
                    "text": "Knowledge distillation The core concept of knowledge distillation involves transferring \"knowledge\" from a teacher model, which is typically a large and high-performance model, to a student model, which is typically a smaller and lightweight model. This transfer aims to enable the student model to acquire the reasoning and generalization capabilities of the teacher model. At present, knowledge distillation can be divided into four main types based on the type of knowledge transferred: (i) output feature knowledge 23 : output feature knowledge usually refers to the final layer features of the teacher model, mainly including logical unit knowledge and soft target knowledge. The basic idea of output feature knowledge distillation is to enable the student model to learn the final prediction of the teacher model, thereby achieving the same prediction ability as the teacher model. Although the original knowledge distillation was proposed for classification tasks and only included interclass similarity as soft target knowledge, in other tasks such as object detection, the final layer feature output of the network may also contain information related to object localization. (ii) Transfer intermediate feature knowledge 24,25 : This method focuses on extracting features from the network layer of the teacher model to provide guidance for the intermediate layer of the student model. (iii) Knowledge of relational features 26,27 : This approach considers that the essence of learning is not in the output of features, but in the relationships between layers and sample data. It mainly emphasizes providing consistent identity mapping, enabling the student model to better grasp the relational knowledge of the teacher model. (iv) Structural Feature Knowledge 28,29 : This strategy involves conveying the comprehensive knowledge system of the teacher model, encompassing not only the output layer knowledge, intermediate feature knowledge, and relational feature knowledge but also the spatial feature distribution and other aspects of the teacher model knowledge. \n\nThe generation gap between teacher and student models In the context of knowledge impartation and educational processes, disparities in understanding, experience, and language between educators or experts and students can hinder effective information transmission, leading to learning obstacles or misunderstandings. \n\nDuring the process of knowledge distillation, structural differences (i.e., the generation gap) between teacher and student models can result in information loss.",
                    "score": 0.7100617011987036,
                    "section_title": "Related work",
                    "char_start_offset": 4318,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 248
                        },
                        {
                            "start": 249,
                            "end": 374
                        },
                        {
                            "start": 375,
                            "end": 680
                        },
                        {
                            "start": 681,
                            "end": 887
                        },
                        {
                            "start": 888,
                            "end": 1184
                        },
                        {
                            "start": 1185,
                            "end": 1393
                        },
                        {
                            "start": 1394,
                            "end": 1584
                        },
                        {
                            "start": 1585,
                            "end": 1734
                        },
                        {
                            "start": 1735,
                            "end": 2073
                        },
                        {
                            "start": 2076,
                            "end": 2392
                        },
                        {
                            "start": 2395,
                            "end": 2557
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 519,
                            "end": 521,
                            "matchedPaperCorpusId": "272263991"
                        },
                        {
                            "start": 1230,
                            "end": 1233,
                            "matchedPaperCorpusId": "265085593"
                        },
                        {
                            "start": 1233,
                            "end": 1235,
                            "matchedPaperCorpusId": "268744175"
                        },
                        {
                            "start": 1433,
                            "end": 1436,
                            "matchedPaperCorpusId": "266468082"
                        },
                        {
                            "start": 1436,
                            "end": 1438,
                            "matchedPaperCorpusId": "257921696"
                        },
                        {
                            "start": 1772,
                            "end": 1774,
                            "matchedPaperCorpusId": "269660133"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.99072265625
                }
            ],
            "relevance_judgement": 0.99072265625,
            "relevance_judgment_input_expanded": "# Title: Counterclockwise block-by-block knowledge distillation for neural network compression\n# Venue: Scientific Reports\n# Authors: Xiaowei Lan, Yalin Zeng, Xiaoxia Wei, Tian Zhang, Yiwen Wang, Chao Huang, Weikai He\n## Abstract\nModel compression is a technique for transforming large neural network models into smaller ones. Knowledge distillation (KD) is a crucial model compression technique that involves transferring knowledge from a large teacher model to a lightweight student model. Existing knowledge distillation methods typically facilitate the knowledge transfer from teacher to student models in one or two stages. This paper introduces a novel approach called counterclockwise block-wise knowledge distillation (CBKD) to optimize the knowledge distillation process. The core idea of CBKD aims to mitigate the generation gap between teacher and student models, facilitating the transmission of intermediate-layer knowledge from the teacher model. It divides both teacher and student models into multiple sub-network blocks, and in each stage of knowledge distillation, only the knowledge from one teacher sub-block is transferred to the corresponding position of a student sub-block. Additionally, in the CBKD process, deeper teacher sub-network blocks are assigned higher compression rates. Extensive experiments on tiny-imagenet-200 and CIFAR-10 demonstrate that the proposed CBKD method can enhance the distillation performance of various mainstream knowledge distillation approaches.\n## Related work\nKnowledge distillation The core concept of knowledge distillation involves transferring \"knowledge\" from a teacher model, which is typically a large and high-performance model, to a student model, which is typically a smaller and lightweight model. This transfer aims to enable the student model to acquire the reasoning and generalization capabilities of the teacher model. At present, knowledge distillation can be divided into four main types based on the type of knowledge transferred: (i) output feature knowledge 23 : output feature knowledge usually refers to the final layer features of the teacher model, mainly including logical unit knowledge and soft target knowledge. The basic idea of output feature knowledge distillation is to enable the student model to learn the final prediction of the teacher model, thereby achieving the same prediction ability as the teacher model. Although the original knowledge distillation was proposed for classification tasks and only included interclass similarity as soft target knowledge, in other tasks such as object detection, the final layer feature output of the network may also contain information related to object localization. (ii) Transfer intermediate feature knowledge 24,25 : This method focuses on extracting features from the network layer of the teacher model to provide guidance for the intermediate layer of the student model. (iii) Knowledge of relational features 26,27 : This approach considers that the essence of learning is not in the output of features, but in the relationships between layers and sample data. It mainly emphasizes providing consistent identity mapping, enabling the student model to better grasp the relational knowledge of the teacher model. (iv) Structural Feature Knowledge 28,29 : This strategy involves conveying the comprehensive knowledge system of the teacher model, encompassing not only the output layer knowledge, intermediate feature knowledge, and relational feature knowledge but also the spatial feature distribution and other aspects of the teacher model knowledge. \n\nThe generation gap between teacher and student models In the context of knowledge impartation and educational processes, disparities in understanding, experience, and language between educators or experts and students can hinder effective information transmission, leading to learning obstacles or misunderstandings. \n\nDuring the process of knowledge distillation, structural differences (i.e., the generation gap) between teacher and student models can result in information loss.",
            "reference_string": "[277507254 | Lan et al. | 2025 | Citations: 0]"
        },
        {
            "title": "Prototype-guided Cross-task Knowledge Distillation for Large-scale Models",
            "venue": "arXiv.org",
            "year": 2022,
            "reference_count": 59,
            "citation_count": 3,
            "influential_citation_count": 0,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "http://arxiv.org/pdf/2212.13180",
                "status": "GREEN",
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2212.13180, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2158676096",
                    "name": "Deng Li"
                },
                {
                    "authorId": "48352212",
                    "name": "Aming Wu"
                },
                {
                    "authorId": "144622313",
                    "name": "Yahong Han"
                },
                {
                    "authorId": "2149898858",
                    "name": "Qingwen Tian"
                }
            ],
            "abstract": "Recently, large-scale pre-trained models have shown their advantages in many tasks. However, due to the huge computational complexity and storage requirements, it is challenging to apply the large-scale model to real scenes. A common solution is knowledge distillation which regards the large-scale model as a teacher model and helps to train a small student model to obtain a competitive performance. Cross-task Knowledge distillation expands the application scenarios of the large-scale pre-trained model. Existing knowledge distillation works focus on directly mimicking the final prediction or the intermediate layers of the teacher model, which represent the global-level characteristics and are task-specific. To alleviate the constraint of different label spaces, capturing invariant intrinsic local object characteristics (such as the shape characteristics of the leg and tail of the cattle and horse) plays a key role. Considering the complexity and variability of real scene tasks, we propose a Prototype-guided Cross-task Knowledge Distillation (ProC-KD) approach to transfer the intrinsic local-level object knowledge of a large-scale teacher network to various task scenarios. First, to better transfer the generalized knowledge in the teacher model in cross-task scenarios, we propose a prototype learning module to learn from the essential feature representation of objects in the teacher model. Secondly, for diverse downstream tasks, we propose a task-adaptive feature augmentation module to enhance the features of the student model with the learned generalization prototype features and guide the training of the student model to improve its generalization ability. The experimental results on various visual tasks demonstrate the effectiveness of our approach for large-scale model cross-task knowledge distillation scenes.",
            "corpus_id": 255125462,
            "sentences": [
                {
                    "corpus_id": "255125462",
                    "title": "Prototype-guided Cross-task Knowledge Distillation for Large-scale Models",
                    "text": "Knowledge distillation is a model compression technology that transfers the knowledge from a larger deep neural network into a small network. \n\nThe methods of knowledge distillation are mainly divided into response-based knowledge distillation [17], [24], featurebased knowledge distillation [22], [23], and relation-based knowledge distillation [25], [26]. \n\nThe main idea of response-based knowledge distillation is to directly transfer the last output layer neural response of the teacher model. Hinton et al. [17] and Ba et al. [37] propose to shift the knowledge by learning the probabilities distribution via softened labels. However, this method depends on the class probability distribution. The effective method to this is to distill the feature-based or the relation-based knowledge from the teacher model. The goal of feature-based knowledge distillation is to match the intermediate representation of the student model with the teacher model. Fitnets [22] initially introduce intermediate representations learning, in which hints are defined as the outputs of a teacher's hidden layer to improve the student's learning process. Inspired by [22], a variety of feature-based knowledge distillation methods [38]- [40] are proposed. \n\nRelation-based knowledge distillation methods explore the relationships between different layers [25] or data samples [26]. \n\nKD has also been extensively studied in Transformer-based language models [31]- [33]. While previous knowledge distillation methods for Transformer-based models mainly focus on the NLP domain and the task of the teacher model is the same as the student model. Ye et al. [34] deal with a scenario distilling the knowledge from a cross-task teacher. However, it lags in the representation of the invariant intrinsic object and is a two-stage distillation method Different from existing works, in this paper, we explore the scenario of learning the intrinsic local-level features and reusing the knowledge of large-scale models for different downstream tasks in a cross-task manner.",
                    "score": 0.6179391295043728,
                    "section_title": "B. Knowledge Distillation",
                    "char_start_offset": 7561,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 141
                        },
                        {
                            "start": 144,
                            "end": 357
                        },
                        {
                            "start": 360,
                            "end": 498
                        },
                        {
                            "start": 499,
                            "end": 631
                        },
                        {
                            "start": 632,
                            "end": 699
                        },
                        {
                            "start": 700,
                            "end": 816
                        },
                        {
                            "start": 817,
                            "end": 954
                        },
                        {
                            "start": 955,
                            "end": 1139
                        },
                        {
                            "start": 1140,
                            "end": 1240
                        },
                        {
                            "start": 1243,
                            "end": 1366
                        },
                        {
                            "start": 1369,
                            "end": 1454
                        },
                        {
                            "start": 1455,
                            "end": 1628
                        },
                        {
                            "start": 1629,
                            "end": 1716
                        },
                        {
                            "start": 1717,
                            "end": 2048
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 250,
                            "end": 254,
                            "matchedPaperCorpusId": "174802983"
                        },
                        {
                            "start": 292,
                            "end": 296,
                            "matchedPaperCorpusId": "2723173"
                        },
                        {
                            "start": 298,
                            "end": 302,
                            "matchedPaperCorpusId": "30307744"
                        },
                        {
                            "start": 346,
                            "end": 350,
                            "matchedPaperCorpusId": "206596723"
                        },
                        {
                            "start": 352,
                            "end": 356,
                            "matchedPaperCorpusId": "131765296"
                        },
                        {
                            "start": 532,
                            "end": 536,
                            "matchedPaperCorpusId": "11536917"
                        },
                        {
                            "start": 963,
                            "end": 967,
                            "matchedPaperCorpusId": "2723173"
                        },
                        {
                            "start": 1152,
                            "end": 1156,
                            "matchedPaperCorpusId": "2723173"
                        },
                        {
                            "start": 1216,
                            "end": 1220,
                            "matchedPaperCorpusId": "829159"
                        },
                        {
                            "start": 1222,
                            "end": 1226,
                            "matchedPaperCorpusId": "227335337"
                        },
                        {
                            "start": 1340,
                            "end": 1344,
                            "matchedPaperCorpusId": "206596723"
                        },
                        {
                            "start": 1361,
                            "end": 1365,
                            "matchedPaperCorpusId": "131765296"
                        },
                        {
                            "start": 1449,
                            "end": 1453,
                            "matchedPaperCorpusId": "202719327"
                        },
                        {
                            "start": 1639,
                            "end": 1643,
                            "matchedPaperCorpusId": "219965004"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.99072265625
                }
            ],
            "relevance_judgement": 0.99072265625,
            "relevance_judgment_input_expanded": "# Title: Prototype-guided Cross-task Knowledge Distillation for Large-scale Models\n# Venue: arXiv.org\n# Authors: Deng Li, Aming Wu, Yahong Han, Qingwen Tian\n## Abstract\nRecently, large-scale pre-trained models have shown their advantages in many tasks. However, due to the huge computational complexity and storage requirements, it is challenging to apply the large-scale model to real scenes. A common solution is knowledge distillation which regards the large-scale model as a teacher model and helps to train a small student model to obtain a competitive performance. Cross-task Knowledge distillation expands the application scenarios of the large-scale pre-trained model. Existing knowledge distillation works focus on directly mimicking the final prediction or the intermediate layers of the teacher model, which represent the global-level characteristics and are task-specific. To alleviate the constraint of different label spaces, capturing invariant intrinsic local object characteristics (such as the shape characteristics of the leg and tail of the cattle and horse) plays a key role. Considering the complexity and variability of real scene tasks, we propose a Prototype-guided Cross-task Knowledge Distillation (ProC-KD) approach to transfer the intrinsic local-level object knowledge of a large-scale teacher network to various task scenarios. First, to better transfer the generalized knowledge in the teacher model in cross-task scenarios, we propose a prototype learning module to learn from the essential feature representation of objects in the teacher model. Secondly, for diverse downstream tasks, we propose a task-adaptive feature augmentation module to enhance the features of the student model with the learned generalization prototype features and guide the training of the student model to improve its generalization ability. The experimental results on various visual tasks demonstrate the effectiveness of our approach for large-scale model cross-task knowledge distillation scenes.\n## B. Knowledge Distillation\nKnowledge distillation is a model compression technology that transfers the knowledge from a larger deep neural network into a small network. \n\nThe methods of knowledge distillation are mainly divided into response-based knowledge distillation [17], [24], featurebased knowledge distillation [22], [23], and relation-based knowledge distillation [25], [26]. \n\nThe main idea of response-based knowledge distillation is to directly transfer the last output layer neural response of the teacher model. Hinton et al. [17] and Ba et al. [37] propose to shift the knowledge by learning the probabilities distribution via softened labels. However, this method depends on the class probability distribution. The effective method to this is to distill the feature-based or the relation-based knowledge from the teacher model. The goal of feature-based knowledge distillation is to match the intermediate representation of the student model with the teacher model. Fitnets [22] initially introduce intermediate representations learning, in which hints are defined as the outputs of a teacher's hidden layer to improve the student's learning process. Inspired by [22], a variety of feature-based knowledge distillation methods [38]- [40] are proposed. \n\nRelation-based knowledge distillation methods explore the relationships between different layers [25] or data samples [26]. \n\nKD has also been extensively studied in Transformer-based language models [31]- [33]. While previous knowledge distillation methods for Transformer-based models mainly focus on the NLP domain and the task of the teacher model is the same as the student model. Ye et al. [34] deal with a scenario distilling the knowledge from a cross-task teacher. However, it lags in the representation of the invariant intrinsic object and is a two-stage distillation method Different from existing works, in this paper, we explore the scenario of learning the intrinsic local-level features and reusing the knowledge of large-scale models for different downstream tasks in a cross-task manner.",
            "reference_string": "[255125462 | Li et al. | 2022 | Citations: 3]"
        },
        {
            "title": "Knowledge Distillation-based Information Sharing for Online Process Monitoring in Decentralized Manufacturing System",
            "venue": "Journal of Intelligent Manufacturing",
            "year": 2023,
            "reference_count": 62,
            "citation_count": 8,
            "influential_citation_count": 0,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://arxiv.org/pdf/2302.12004",
                "status": "GREEN",
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2302.12004, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2144091000",
                    "name": "Zhangyue Shi"
                },
                {
                    "authorId": "2116210564",
                    "name": "Yuxuan Li"
                },
                {
                    "authorId": "2035538477",
                    "name": "Chenang Liu"
                }
            ],
            "abstract": "In advanced manufacturing, the incorporation of sensing technology provides an opportunity to achieve efficient in-situ process monitoring using machine learning methods. Meanwhile, the advances of information technologies also enable a connected and decentralized environment for manufacturing systems, making different manufacturing units in the system collaborate more closely. In a decentralized manufacturing system, the involved units may fabricate same or similar products and deploy their own machine learning model for online process monitoring. However, due to the possible inconsistency of task progress during the operation, it is also common that some units have more informative data while some have less informative data. Thus, the monitoring performance of machine learning model for each unit may highly vary. Therefore, it is extremely valuable to achieve efficient and secured knowledge sharing among the units in a decentralized manufacturing system for enhancement of poorly performed models. To realize this goal, this paper proposes a novel knowledge distillation-based information sharing (KD-IS) framework, which could distill informative knowledge from well performed models to improve the monitoring performance of poorly performed models. To validate the effectiveness of this method, a real-world case study is conducted in a connected fused filament fabrication (FFF)-based additive manufacturing (AM) platform. The experimental results show that the developed method is very efficient in improving model monitoring performance at poorly performed models, with solid protection on potential data privacy.",
            "corpus_id": 257102399,
            "sentences": [
                {
                    "corpus_id": "257102399",
                    "title": "Knowledge Distillation-based Information Sharing for Online Process Monitoring in Decentralized Manufacturing System",
                    "text": "Knowledge distillation is a model compression technique proposed by Hinton et al. in 2015 [9], which could effectively train a smaller student model by learning from a larger teacher model. As shown in Figure 2, a general knowledge distillation framework consists of three major components: teacher-student network architecture, knowledge, as well as distillation algorithm. The main idea of knowledge distillation is that the student model mimics the teacher model in order to achieve a comparable or even a superior performance [10]. Teacher network usually has more complicated network architecture and learns from the data first. After the training of teacher network, the useful knowledge could be distilled from the teacher network. Hereafter, student network learns the knowledge distilled from teacher network to mimic teacher's behavior together with the training data. The core of knowledge distillation is how to simplify model while maintaining relatively good performance. \n\nOne promising and efficient way to distill knowledge is to mimic the logit output of last layer in the teacher model (i.e., teacher's prediction). Soft target, i.e., the probability that the input data   belongs to each class, is proposed as the logit of teacher network. For a -class classification problem, the soft target of teacher network \u0398  could be calculated by the softmax function as: \n\nWhere   denotes the -th training sample,  is the number of classes,    is logit of last layer for the th class and  denotes temperature factor which could control importance of each target. For example, a higher  produces a softer probability distribution among different classes (assign probability with less difference to each class). Soft target contains informative dark knowledge of teacher network and could enhance performance of student network. Empirically, when the student model is very small compared to the teacher model, lower temperatures work better [9]. This is because a very small model might not be able to capture all the information when we raise the temperature. Therefore, soft target could be seen as knowledge extracted from teacher network and it could be transferred to student network by matching output of two networks. To measure the output similarity between teacher network \u0398  and student network \u0398  , the Kullback Leibler (KL) divergence is often employed, which could be formulated in Eq. ( 2),",
                    "score": 0.7158637893912181,
                    "section_title": "Knowledge distillation",
                    "char_start_offset": 15159,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 189
                        },
                        {
                            "start": 190,
                            "end": 374
                        },
                        {
                            "start": 375,
                            "end": 535
                        },
                        {
                            "start": 536,
                            "end": 633
                        },
                        {
                            "start": 634,
                            "end": 738
                        },
                        {
                            "start": 739,
                            "end": 878
                        },
                        {
                            "start": 879,
                            "end": 985
                        },
                        {
                            "start": 988,
                            "end": 1134
                        },
                        {
                            "start": 1135,
                            "end": 1259
                        },
                        {
                            "start": 1260,
                            "end": 1382
                        },
                        {
                            "start": 1385,
                            "end": 1574
                        },
                        {
                            "start": 1575,
                            "end": 1721
                        },
                        {
                            "start": 1722,
                            "end": 1838
                        },
                        {
                            "start": 1839,
                            "end": 1955
                        },
                        {
                            "start": 1956,
                            "end": 2070
                        },
                        {
                            "start": 2071,
                            "end": 2234
                        },
                        {
                            "start": 2235,
                            "end": 2414
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 530,
                            "end": 534,
                            "matchedPaperCorpusId": "219559263"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.990234375
                }
            ],
            "relevance_judgement": 0.990234375,
            "relevance_judgment_input_expanded": "# Title: Knowledge Distillation-based Information Sharing for Online Process Monitoring in Decentralized Manufacturing System\n# Venue: Journal of Intelligent Manufacturing\n# Authors: Zhangyue Shi, Yuxuan Li, Chenang Liu\n## Abstract\nIn advanced manufacturing, the incorporation of sensing technology provides an opportunity to achieve efficient in-situ process monitoring using machine learning methods. Meanwhile, the advances of information technologies also enable a connected and decentralized environment for manufacturing systems, making different manufacturing units in the system collaborate more closely. In a decentralized manufacturing system, the involved units may fabricate same or similar products and deploy their own machine learning model for online process monitoring. However, due to the possible inconsistency of task progress during the operation, it is also common that some units have more informative data while some have less informative data. Thus, the monitoring performance of machine learning model for each unit may highly vary. Therefore, it is extremely valuable to achieve efficient and secured knowledge sharing among the units in a decentralized manufacturing system for enhancement of poorly performed models. To realize this goal, this paper proposes a novel knowledge distillation-based information sharing (KD-IS) framework, which could distill informative knowledge from well performed models to improve the monitoring performance of poorly performed models. To validate the effectiveness of this method, a real-world case study is conducted in a connected fused filament fabrication (FFF)-based additive manufacturing (AM) platform. The experimental results show that the developed method is very efficient in improving model monitoring performance at poorly performed models, with solid protection on potential data privacy.\n## Knowledge distillation\nKnowledge distillation is a model compression technique proposed by Hinton et al. in 2015 [9], which could effectively train a smaller student model by learning from a larger teacher model. As shown in Figure 2, a general knowledge distillation framework consists of three major components: teacher-student network architecture, knowledge, as well as distillation algorithm. The main idea of knowledge distillation is that the student model mimics the teacher model in order to achieve a comparable or even a superior performance [10]. Teacher network usually has more complicated network architecture and learns from the data first. After the training of teacher network, the useful knowledge could be distilled from the teacher network. Hereafter, student network learns the knowledge distilled from teacher network to mimic teacher's behavior together with the training data. The core of knowledge distillation is how to simplify model while maintaining relatively good performance. \n\nOne promising and efficient way to distill knowledge is to mimic the logit output of last layer in the teacher model (i.e., teacher's prediction). Soft target, i.e., the probability that the input data   belongs to each class, is proposed as the logit of teacher network. For a -class classification problem, the soft target of teacher network \u0398  could be calculated by the softmax function as: \n\nWhere   denotes the -th training sample,  is the number of classes,    is logit of last layer for the th class and  denotes temperature factor which could control importance of each target. For example, a higher  produces a softer probability distribution among different classes (assign probability with less difference to each class). Soft target contains informative dark knowledge of teacher network and could enhance performance of student network. Empirically, when the student model is very small compared to the teacher model, lower temperatures work better [9]. This is because a very small model might not be able to capture all the information when we raise the temperature. Therefore, soft target could be seen as knowledge extracted from teacher network and it could be transferred to student network by matching output of two networks. To measure the output similarity between teacher network \u0398  and student network \u0398  , the Kullback Leibler (KL) divergence is often employed, which could be formulated in Eq. ( 2),",
            "reference_string": "[257102399 | Shi et al. | 2023 | Citations: 8]"
        },
        {
            "title": "Knowledge Distillation via Weighted Ensemble of Teaching Assistants",
            "venue": "International Conference on Big Knowledge",
            "year": 2021,
            "reference_count": 16,
            "citation_count": 4,
            "influential_citation_count": 0,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://arxiv.org/pdf/2206.12005",
                "status": "GREEN",
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2206.12005, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2150123681",
                    "name": "Durga Prasad Ganta"
                },
                {
                    "authorId": "1944682834",
                    "name": "Himel Das Gupta"
                },
                {
                    "authorId": "2278313995",
                    "name": "Victor S. Sheng"
                }
            ],
            "abstract": "Knowledge distillation in machine learning is the process of transferring knowledge from a large model called teacher to a smaller model called student. Knowledge distillation is one of the techniques to compress the large network (teacher) to a smaller network (student) that can be deployed in small devices such as mobile phones. When the network size gap between the teacher and student increases, the performance of the student network decreases. To solve this problem, an intermediate model is employed between the teacher model and the student model known as the teaching assistant model, which in turn bridges the gap between the teacher and the student. In this research, we have shown that using multiple teaching assistant models, the student model (the smaller model) can be further improved. We combined these multiple teaching assistant model using weighted ensemble learning where we have used a differential evaluation optimization algorithm to generate the weight values.",
            "corpus_id": 245974615,
            "sentences": [
                {
                    "corpus_id": "245974615",
                    "title": "Knowledge Distillation via Weighted Ensemble of Teaching Assistants",
                    "text": "Originally proposed by Bucila, Caruana, and Niculescu-Mizil (2006) [6] and popularized by Hinton, Vinyals, and Dean (2015) [2] knowledge distillation compress the knowledge of a large and computational expensive model (often an ensemble of neural networks) to a single computational efficient neural network. The idea of knowledge distillation is to train the small model, the student, on a transfer set with soft targets provided by the large model, the teacher. Knowledge distillation has been commonly used in a number of learning tasks since then. Modeling knowledge transfer between teacher and student has also been done using adversarial methods. Using several teachers was still a good way to improve robustness. Some studies also proposed deep mutual learning which allows an ensemble of student models to learn collaboratively and teach each other during training. \n\nThe main idea of using knowledge distillation is that student network (S) to be trained not only using the true labels information but also observation of how the teacher (T) works with the unseen data provided. Because the teacher model has more more generalization power, the idea is to train the student model is such way that it can mimic the behaviour of that generalization. The teacher network is complex in size being deeper and wider. \n\nLet a t and a s be the logits (the inputs to the final softmax) of the teacher and student network, respectively. In classic supervised learning, the mismatch between output of student network softmax(a s ) and the ground-truth label y r is usually penalized using cross-entropy loss and is given as, \n\nIn knowledge distillation one also tries to match the softened outputs of teacher y t =softmax(a t ) and student y s =softmax(a s ) via Kullback-Leibler divergence loss, \n\nby using a temperature parameter \u03c4 which has an additional control on softening of signal arising from the output of the teacher network. The student network is then trained under the following loss equation which used KD loss and cross-entropy loss, \n\nwhere \u03bb is the parameter used to trade-off between these two losses. This method is used for knowledge distillation from teacher and student models.",
                    "score": 0.803701394549467,
                    "section_title": "B. Knowledge distillation",
                    "char_start_offset": 6434,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 308
                        },
                        {
                            "start": 309,
                            "end": 463
                        },
                        {
                            "start": 464,
                            "end": 551
                        },
                        {
                            "start": 552,
                            "end": 653
                        },
                        {
                            "start": 654,
                            "end": 720
                        },
                        {
                            "start": 721,
                            "end": 874
                        },
                        {
                            "start": 877,
                            "end": 1088
                        },
                        {
                            "start": 1089,
                            "end": 1257
                        },
                        {
                            "start": 1258,
                            "end": 1320
                        },
                        {
                            "start": 1323,
                            "end": 1436
                        },
                        {
                            "start": 1437,
                            "end": 1623
                        },
                        {
                            "start": 1626,
                            "end": 1795
                        },
                        {
                            "start": 1798,
                            "end": 1935
                        },
                        {
                            "start": 1936,
                            "end": 2048
                        },
                        {
                            "start": 2051,
                            "end": 2119
                        },
                        {
                            "start": 2120,
                            "end": 2199
                        }
                    ],
                    "ref_mentions": [],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.98974609375
                }
            ],
            "relevance_judgement": 0.98974609375,
            "relevance_judgment_input_expanded": "# Title: Knowledge Distillation via Weighted Ensemble of Teaching Assistants\n# Venue: International Conference on Big Knowledge\n# Authors: Durga Prasad Ganta, Himel Das Gupta, Victor S. Sheng\n## Abstract\nKnowledge distillation in machine learning is the process of transferring knowledge from a large model called teacher to a smaller model called student. Knowledge distillation is one of the techniques to compress the large network (teacher) to a smaller network (student) that can be deployed in small devices such as mobile phones. When the network size gap between the teacher and student increases, the performance of the student network decreases. To solve this problem, an intermediate model is employed between the teacher model and the student model known as the teaching assistant model, which in turn bridges the gap between the teacher and the student. In this research, we have shown that using multiple teaching assistant models, the student model (the smaller model) can be further improved. We combined these multiple teaching assistant model using weighted ensemble learning where we have used a differential evaluation optimization algorithm to generate the weight values.\n## B. Knowledge distillation\nOriginally proposed by Bucila, Caruana, and Niculescu-Mizil (2006) [6] and popularized by Hinton, Vinyals, and Dean (2015) [2] knowledge distillation compress the knowledge of a large and computational expensive model (often an ensemble of neural networks) to a single computational efficient neural network. The idea of knowledge distillation is to train the small model, the student, on a transfer set with soft targets provided by the large model, the teacher. Knowledge distillation has been commonly used in a number of learning tasks since then. Modeling knowledge transfer between teacher and student has also been done using adversarial methods. Using several teachers was still a good way to improve robustness. Some studies also proposed deep mutual learning which allows an ensemble of student models to learn collaboratively and teach each other during training. \n\nThe main idea of using knowledge distillation is that student network (S) to be trained not only using the true labels information but also observation of how the teacher (T) works with the unseen data provided. Because the teacher model has more more generalization power, the idea is to train the student model is such way that it can mimic the behaviour of that generalization. The teacher network is complex in size being deeper and wider. \n\nLet a t and a s be the logits (the inputs to the final softmax) of the teacher and student network, respectively. In classic supervised learning, the mismatch between output of student network softmax(a s ) and the ground-truth label y r is usually penalized using cross-entropy loss and is given as, \n\nIn knowledge distillation one also tries to match the softened outputs of teacher y t =softmax(a t ) and student y s =softmax(a s ) via Kullback-Leibler divergence loss, \n\nby using a temperature parameter \u03c4 which has an additional control on softening of signal arising from the output of the teacher network. The student network is then trained under the following loss equation which used KD loss and cross-entropy loss, \n\nwhere \u03bb is the parameter used to trade-off between these two losses. This method is used for knowledge distillation from teacher and student models.",
            "reference_string": "[245974615 | Ganta et al. | 2021 | Citations: 4]"
        },
        {
            "title": "A Selective Survey on Versatile Knowledge Distillation Paradigm for Neural Network Models",
            "venue": "arXiv.org",
            "year": 2020,
            "reference_count": 76,
            "citation_count": 3,
            "influential_citation_count": 1,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2011.14554, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "92856269",
                    "name": "J. Ku"
                },
                {
                    "authorId": "31119623",
                    "name": "Jihun Oh"
                },
                {
                    "authorId": "2417832",
                    "name": "Youngyoon Lee"
                },
                {
                    "authorId": "97319268",
                    "name": "Gaurav Pooniwala"
                },
                {
                    "authorId": "2119014436",
                    "name": "Sangjeong Lee"
                }
            ],
            "abstract": "This paper aims to provide a selective survey about knowledge distillation(KD) framework for researchers and practitioners to take advantage of it for developing new optimized models in the deep neural network field. To this end, we give a brief overview of knowledge distillation and some related works including learning using privileged information(LUPI) and generalized distillation(GD). Even though knowledge distillation based on the teacher-student architecture was initially devised as a model compression technique, it has found versatile applications over various frameworks. In this paper, we review the characteristics of knowledge distillation from the hypothesis that the three important ingredients of knowledge distillation are distilled knowledge and loss,teacher-student paradigm, and the distillation process. In addition, we survey the versatility of the knowledge distillation by studying its direct applications and its usage in combination with other deep learning paradigms. Finally we present some future works in knowledge distillation including explainable knowledge distillation where the analytical analysis of the performance gain is studied and the self-supervised learning which is a hot research topic in deep learning community.",
            "corpus_id": 227228204,
            "sentences": [
                {
                    "corpus_id": "227228204",
                    "title": "A Selective Survey on Versatile Knowledge Distillation Paradigm for Neural Network Models",
                    "text": "Knowledge distillation is an effective model compression technique in which a compact model (student) is trained under the supervision of a larger pre-trained model or an ensemble of models (teacher). \n\nKnowledge distillation aims to improve the performance of the student network by providing additional supervision from a teacher network. To the best of our knowledge, exploiting knowledge transfer to compress model was first proposed in C. Bucilu\u01ce et al. [4]. They trained a compressed/ensemble model of strong classifiers with pseudo-labeled data, and reproduced the output of the original larger network. However, the work is limited to shallow models. The idea has been adopted in [5] as knowledge distillation to compress deep and wide networks into shallower ones, where the compressed model mimicked the function learned by the complex model. Hinton et al. [6] popularized the concept of Knowledge Distillation to be extended to more practical uses. The work in [6] proposed knowledge distillation as a more general case of C. Bucilu\u01ce et al. [4] by adopting the concept of temperature parameter at the output of teacher. The student was trained to predict the output and the classification labels. \n\nThe main idea of knowledge distillation approach is to shift knowledge from a large teacher model into a small one by learning the class distributions output via softmax [6]. It has even been observed that the student learns much faster and more reliably if trained using outputs of teacher as soft labels, instead of one-hot-encoded labels. \n\nSince then, a number of knowledge distillation methods have been proposed, each trying to capture and transfer some characteristics of the teacher such as the representation space, decision boundary or intra-data relationship. Despite its simplicity, knowledge distillation demonstrates promising results in various image classification tasks. \n\nKnowledge distillation has proven empirically to be an effective technique for training a compact model [7], [11], [17].",
                    "score": 0.6843745203234013,
                    "section_title": "II. KNOWLEDGE DISTILLATION",
                    "char_start_offset": 2403,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 200
                        },
                        {
                            "start": 203,
                            "end": 340
                        },
                        {
                            "start": 341,
                            "end": 463
                        },
                        {
                            "start": 464,
                            "end": 610
                        },
                        {
                            "start": 611,
                            "end": 658
                        },
                        {
                            "start": 659,
                            "end": 852
                        },
                        {
                            "start": 853,
                            "end": 959
                        },
                        {
                            "start": 960,
                            "end": 1130
                        },
                        {
                            "start": 1131,
                            "end": 1207
                        },
                        {
                            "start": 1210,
                            "end": 1384
                        },
                        {
                            "start": 1385,
                            "end": 1551
                        },
                        {
                            "start": 1554,
                            "end": 1780
                        },
                        {
                            "start": 1781,
                            "end": 1897
                        },
                        {
                            "start": 1900,
                            "end": 2020
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 459,
                            "end": 462,
                            "matchedPaperCorpusId": "11253972"
                        },
                        {
                            "start": 688,
                            "end": 691,
                            "matchedPaperCorpusId": "11536917"
                        },
                        {
                            "start": 867,
                            "end": 870,
                            "matchedPaperCorpusId": "7200347"
                        },
                        {
                            "start": 972,
                            "end": 975,
                            "matchedPaperCorpusId": "7200347"
                        },
                        {
                            "start": 1052,
                            "end": 1055,
                            "matchedPaperCorpusId": "11253972"
                        },
                        {
                            "start": 1380,
                            "end": 1383,
                            "matchedPaperCorpusId": "7200347"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.9892578125
                }
            ],
            "relevance_judgement": 0.9892578125,
            "relevance_judgment_input_expanded": "# Title: A Selective Survey on Versatile Knowledge Distillation Paradigm for Neural Network Models\n# Venue: arXiv.org\n# Authors: J. Ku, Jihun Oh, Youngyoon Lee, Gaurav Pooniwala, Sangjeong Lee\n## Abstract\nThis paper aims to provide a selective survey about knowledge distillation(KD) framework for researchers and practitioners to take advantage of it for developing new optimized models in the deep neural network field. To this end, we give a brief overview of knowledge distillation and some related works including learning using privileged information(LUPI) and generalized distillation(GD). Even though knowledge distillation based on the teacher-student architecture was initially devised as a model compression technique, it has found versatile applications over various frameworks. In this paper, we review the characteristics of knowledge distillation from the hypothesis that the three important ingredients of knowledge distillation are distilled knowledge and loss,teacher-student paradigm, and the distillation process. In addition, we survey the versatility of the knowledge distillation by studying its direct applications and its usage in combination with other deep learning paradigms. Finally we present some future works in knowledge distillation including explainable knowledge distillation where the analytical analysis of the performance gain is studied and the self-supervised learning which is a hot research topic in deep learning community.\n## II. KNOWLEDGE DISTILLATION\nKnowledge distillation is an effective model compression technique in which a compact model (student) is trained under the supervision of a larger pre-trained model or an ensemble of models (teacher). \n\nKnowledge distillation aims to improve the performance of the student network by providing additional supervision from a teacher network. To the best of our knowledge, exploiting knowledge transfer to compress model was first proposed in C. Bucilu\u01ce et al. [4]. They trained a compressed/ensemble model of strong classifiers with pseudo-labeled data, and reproduced the output of the original larger network. However, the work is limited to shallow models. The idea has been adopted in [5] as knowledge distillation to compress deep and wide networks into shallower ones, where the compressed model mimicked the function learned by the complex model. Hinton et al. [6] popularized the concept of Knowledge Distillation to be extended to more practical uses. The work in [6] proposed knowledge distillation as a more general case of C. Bucilu\u01ce et al. [4] by adopting the concept of temperature parameter at the output of teacher. The student was trained to predict the output and the classification labels. \n\nThe main idea of knowledge distillation approach is to shift knowledge from a large teacher model into a small one by learning the class distributions output via softmax [6]. It has even been observed that the student learns much faster and more reliably if trained using outputs of teacher as soft labels, instead of one-hot-encoded labels. \n\nSince then, a number of knowledge distillation methods have been proposed, each trying to capture and transfer some characteristics of the teacher such as the representation space, decision boundary or intra-data relationship. Despite its simplicity, knowledge distillation demonstrates promising results in various image classification tasks. \n\nKnowledge distillation has proven empirically to be an effective technique for training a compact model [7], [11], [17].",
            "reference_string": "[227228204 | Ku et al. | 2020 | Citations: 3]"
        },
        {
            "title": "Feature Alignment and Representation Transfer in Knowledge Distillation for Large Language Models",
            "venue": "arXiv.org",
            "year": 2025,
            "reference_count": 111,
            "citation_count": 0,
            "influential_citation_count": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2504.13825, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2333595030",
                    "name": "Junjie Yang"
                },
                {
                    "authorId": "2312846636",
                    "name": "Jun-Jie Song"
                },
                {
                    "authorId": "2356484215",
                    "name": "Xudong Han"
                },
                {
                    "authorId": "2319608188",
                    "name": "Ziqian Bi"
                },
                {
                    "authorId": "2323695882",
                    "name": "Tianyang Wang"
                },
                {
                    "authorId": "2330158457",
                    "name": "Chia Xin Liang"
                },
                {
                    "authorId": "2330144928",
                    "name": "Xinyuan Song"
                },
                {
                    "authorId": "2327005680",
                    "name": "Yichao Zhang"
                },
                {
                    "authorId": "2319611972",
                    "name": "Qian Niu"
                },
                {
                    "authorId": "2319606006",
                    "name": "Benji Peng"
                },
                {
                    "authorId": "2319648854",
                    "name": "Keyu Chen"
                },
                {
                    "authorId": "2322495607",
                    "name": "Ming Liu"
                }
            ],
            "abstract": "Knowledge distillation (KD) is a technique for transferring knowledge from complex teacher models to simpler student models, significantly enhancing model efficiency and accuracy. It has demonstrated substantial advancements in various applications including image classification, object detection, language modeling, text classification, and sentiment analysis. Recent innovations in KD methods, such as attention-based approaches, block-wise logit distillation, and decoupling distillation, have notably improved student model performance. These techniques focus on stimulus complexity, attention mechanisms, and global information capture to optimize knowledge transfer. In addition, KD has proven effective in compressing large language models while preserving accuracy, reducing computational overhead, and improving inference speed. This survey synthesizes the latest literature, highlighting key findings, contributions, and future directions in knowledge distillation to provide insights for researchers and practitioners on its evolving role in artificial intelligence and machine learning.",
            "corpus_id": 277940190,
            "sentences": [
                {
                    "corpus_id": "277940190",
                    "title": "Feature Alignment and Representation Transfer in Knowledge Distillation for Large Language Models",
                    "text": "Knowledge Distillation (KD) has emerged as a fundamental technique in deep learning, enabling the transfer of knowledge from a complex and pretrained model (the teacher model) to a simpler and smaller model (the student model) (Hinton et al., 2015;Gao, 2023). This approach is effective for model compression, significantly improving their efficiency and making them more suitable for deployment on resourcelimited devices. The core idea behind KD is to distill the dark knowledge embedded in the teacher model into a student model, which enhances student performance (Yu et al., 2024). \n\nRecent studies have focused on enhancing the effectiveness of KD by developing innovative variants, frameworks, and methodologies, including decoupling logit-based and featurebased knowledge distillation methods (Zhang et al., 2024c), correlation-aware knowledge distillation (Zhang et al., 2021), and partial-to-whole knowledge distillation (Abbasi et al., 2020). A common theme among these studies is the emphasis on feature alignment, which is crucial for Knowledge Distillation (KD) (Yu et al., 2024). Aligning the features of teacher and student models enhances the knowledge transfer process, leading to improved model performance. As shown in Figure 1, another important aspect of KD is the decomposition of knowledge into smaller components, which can facilitate more effective knowledge transfer (Zhang et al., 2021). This approach has been explored in various studies, where methods for decomposing teacher knowledge into subnetworks with increasing channel width have been proposed. For example, recent studies have investigated novel strategies for splitting teacher models into smaller, more manageable components to improve student model efficiency (Hou and Fan, 2021;Ruffy and Chahal, 2019). \n\nThe development of a general framework or model for knowledge distillation (KD) remains an active area of research because it offers a unified perspective on diverse  KD techniques and facilitates the systematic analysis and development of new strategies (Wang et al., 2023a). Significant contributions have been made in this area, with innovative frameworks and methodologies proposed to enhance the efficiency and scalability of KD.",
                    "score": 0.6327288982085465,
                    "section_title": "I. INTRODUCTION",
                    "char_start_offset": 18,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 259
                        },
                        {
                            "start": 260,
                            "end": 423
                        },
                        {
                            "start": 424,
                            "end": 586
                        },
                        {
                            "start": 589,
                            "end": 953
                        },
                        {
                            "start": 954,
                            "end": 1094
                        },
                        {
                            "start": 1095,
                            "end": 1226
                        },
                        {
                            "start": 1227,
                            "end": 1415
                        },
                        {
                            "start": 1416,
                            "end": 1582
                        },
                        {
                            "start": 1583,
                            "end": 1795
                        },
                        {
                            "start": 1798,
                            "end": 2074
                        },
                        {
                            "start": 2075,
                            "end": 2232
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 568,
                            "end": 585,
                            "matchedPaperCorpusId": "273811396"
                        },
                        {
                            "start": 931,
                            "end": 952,
                            "matchedPaperCorpusId": "209515527"
                        },
                        {
                            "start": 1076,
                            "end": 1093,
                            "matchedPaperCorpusId": "273811396"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.98876953125
                }
            ],
            "relevance_judgement": 0.98876953125,
            "relevance_judgment_input_expanded": "# Title: Feature Alignment and Representation Transfer in Knowledge Distillation for Large Language Models\n# Venue: arXiv.org\n# Authors: Junjie Yang, Jun-Jie Song, Xudong Han, Ziqian Bi, Tianyang Wang, Chia Xin Liang, Xinyuan Song, Yichao Zhang, Qian Niu, Benji Peng, Keyu Chen, Ming Liu\n## Abstract\nKnowledge distillation (KD) is a technique for transferring knowledge from complex teacher models to simpler student models, significantly enhancing model efficiency and accuracy. It has demonstrated substantial advancements in various applications including image classification, object detection, language modeling, text classification, and sentiment analysis. Recent innovations in KD methods, such as attention-based approaches, block-wise logit distillation, and decoupling distillation, have notably improved student model performance. These techniques focus on stimulus complexity, attention mechanisms, and global information capture to optimize knowledge transfer. In addition, KD has proven effective in compressing large language models while preserving accuracy, reducing computational overhead, and improving inference speed. This survey synthesizes the latest literature, highlighting key findings, contributions, and future directions in knowledge distillation to provide insights for researchers and practitioners on its evolving role in artificial intelligence and machine learning.\n## I. INTRODUCTION\nKnowledge Distillation (KD) has emerged as a fundamental technique in deep learning, enabling the transfer of knowledge from a complex and pretrained model (the teacher model) to a simpler and smaller model (the student model) (Hinton et al., 2015;Gao, 2023). This approach is effective for model compression, significantly improving their efficiency and making them more suitable for deployment on resourcelimited devices. The core idea behind KD is to distill the dark knowledge embedded in the teacher model into a student model, which enhances student performance (Yu et al., 2024). \n\nRecent studies have focused on enhancing the effectiveness of KD by developing innovative variants, frameworks, and methodologies, including decoupling logit-based and featurebased knowledge distillation methods (Zhang et al., 2024c), correlation-aware knowledge distillation (Zhang et al., 2021), and partial-to-whole knowledge distillation (Abbasi et al., 2020). A common theme among these studies is the emphasis on feature alignment, which is crucial for Knowledge Distillation (KD) (Yu et al., 2024). Aligning the features of teacher and student models enhances the knowledge transfer process, leading to improved model performance. As shown in Figure 1, another important aspect of KD is the decomposition of knowledge into smaller components, which can facilitate more effective knowledge transfer (Zhang et al., 2021). This approach has been explored in various studies, where methods for decomposing teacher knowledge into subnetworks with increasing channel width have been proposed. For example, recent studies have investigated novel strategies for splitting teacher models into smaller, more manageable components to improve student model efficiency (Hou and Fan, 2021;Ruffy and Chahal, 2019). \n\nThe development of a general framework or model for knowledge distillation (KD) remains an active area of research because it offers a unified perspective on diverse  KD techniques and facilitates the systematic analysis and development of new strategies (Wang et al., 2023a). Significant contributions have been made in this area, with innovative frameworks and methodologies proposed to enhance the efficiency and scalability of KD.",
            "reference_string": "[277940190 | Yang et al. | 2025 | Citations: 0]"
        },
        {
            "title": "Self-trainable and adaptive sensor intelligence for selective data generation",
            "venue": "Frontiers Artif. Intell.",
            "year": 2025,
            "reference_count": 40,
            "citation_count": 1,
            "influential_citation_count": 0,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://doi.org/10.3389/frai.2024.1403187",
                "status": "GOLD",
                "license": "CCBY",
                "disclaimer": "Notice: Paper or abstract available at https://pmc.ncbi.nlm.nih.gov/articles/PMC11794785, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2126538701",
                    "name": "Arghavan Rezvani"
                },
                {
                    "authorId": "2280107063",
                    "name": "Wenjun Huang"
                },
                {
                    "authorId": "2241550254",
                    "name": "Hanning Chen"
                },
                {
                    "authorId": "2290732265",
                    "name": "Yang Ni"
                },
                {
                    "authorId": "2238651307",
                    "name": "Mohsen Imani"
                }
            ],
            "abstract": "With the increasing integration of machine learning into IoT devices, managing energy consumption and data transmission has become a critical challenge. Many IoT applications depend on complex computations performed on server-side infrastructure, necessitating efficient methods to reduce unnecessary data transmission. One promising solution involves deploying compact machine learning models near sensors, enabling intelligent identification and transmission of only relevant data frames. However, existing near-sensor models lack adaptability, as they require extensive pre-training and are often rigidly configured prior to deployment. This paper proposes a novel framework that fuses online learning, active learning, and knowledge distillation to enable adaptive, resource-efficient near-sensor intelligence. Our approach allows near-sensor models to dynamically fine-tune their parameters post-deployment using online learning, eliminating the need for extensive pre-labeling and training. Through a sequential training and execution process, the framework achieves continuous adaptability without prior knowledge of the deployment environment. To enhance performance while preserving model efficiency, we integrate knowledge distillation, enabling the transfer of critical insights from a larger teacher model to a compact student model. Additionally, active learning reduces the required training data while maintaining competitive performance. We validated our framework on both benchmark data from the MS COCO dataset and in a simulated IoT environment. The results demonstrate significant improvements in energy efficiency and data transmission optimization, highlighting the practical applicability of our method in real-world IoT scenarios.",
            "corpus_id": 275847824,
            "sentences": [
                {
                    "corpus_id": "275847824",
                    "title": "Self-trainable and adaptive sensor intelligence for selective data generation",
                    "text": "Knowledge distillation (KD) is a method to transfer knowledge from a larger network or ensemble of networks (teacher model) to a smaller and less complex model (student model) (Hinton et al., 2015). It can be considered as a way to compress a larger model into a smaller one, making it more efficient and less resource-hungry, which is most effective for deploying models on edge devices (Gou et al., 2021). The intuition behind KD is that supervising the student model with the teacher model helps the student model to mimic the teacher model with comparable accuracy. The distilled knowledge from the teacher model also reveals some underlying patterns in the data, making it easier to learn by a smaller model. The very first idea of knowledge distillation is presented in Bucilu\u01ce et al. (2006), where the student model utilizes the predictions of the teacher model on a large set of pseudo data (unlabeled or synthetic data with the same distribution as the original training data) to get an idea of the function learned by the teacher model. The idea is generalized in Hinton et al. (2015) by formalizing knowledge distillation as a method to supervise a small student by a large teacher model to obtain a competitive performance.",
                    "score": 0.8112526300755384,
                    "section_title": ". Knowledge distillation",
                    "char_start_offset": 9996,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 198
                        },
                        {
                            "start": 199,
                            "end": 407
                        },
                        {
                            "start": 408,
                            "end": 569
                        },
                        {
                            "start": 570,
                            "end": 713
                        },
                        {
                            "start": 714,
                            "end": 1046
                        },
                        {
                            "start": 1047,
                            "end": 1235
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 776,
                            "end": 797,
                            "matchedPaperCorpusId": "11253972"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.98828125
                }
            ],
            "relevance_judgement": 0.98828125,
            "relevance_judgment_input_expanded": "# Title: Self-trainable and adaptive sensor intelligence for selective data generation\n# Venue: Frontiers Artif. Intell.\n# Authors: Arghavan Rezvani, Wenjun Huang, Hanning Chen, Yang Ni, Mohsen Imani\n## Abstract\nWith the increasing integration of machine learning into IoT devices, managing energy consumption and data transmission has become a critical challenge. Many IoT applications depend on complex computations performed on server-side infrastructure, necessitating efficient methods to reduce unnecessary data transmission. One promising solution involves deploying compact machine learning models near sensors, enabling intelligent identification and transmission of only relevant data frames. However, existing near-sensor models lack adaptability, as they require extensive pre-training and are often rigidly configured prior to deployment. This paper proposes a novel framework that fuses online learning, active learning, and knowledge distillation to enable adaptive, resource-efficient near-sensor intelligence. Our approach allows near-sensor models to dynamically fine-tune their parameters post-deployment using online learning, eliminating the need for extensive pre-labeling and training. Through a sequential training and execution process, the framework achieves continuous adaptability without prior knowledge of the deployment environment. To enhance performance while preserving model efficiency, we integrate knowledge distillation, enabling the transfer of critical insights from a larger teacher model to a compact student model. Additionally, active learning reduces the required training data while maintaining competitive performance. We validated our framework on both benchmark data from the MS COCO dataset and in a simulated IoT environment. The results demonstrate significant improvements in energy efficiency and data transmission optimization, highlighting the practical applicability of our method in real-world IoT scenarios.\n## . Knowledge distillation\nKnowledge distillation (KD) is a method to transfer knowledge from a larger network or ensemble of networks (teacher model) to a smaller and less complex model (student model) (Hinton et al., 2015). It can be considered as a way to compress a larger model into a smaller one, making it more efficient and less resource-hungry, which is most effective for deploying models on edge devices (Gou et al., 2021). The intuition behind KD is that supervising the student model with the teacher model helps the student model to mimic the teacher model with comparable accuracy. The distilled knowledge from the teacher model also reveals some underlying patterns in the data, making it easier to learn by a smaller model. The very first idea of knowledge distillation is presented in Bucilu\u01ce et al. (2006), where the student model utilizes the predictions of the teacher model on a large set of pseudo data (unlabeled or synthetic data with the same distribution as the original training data) to get an idea of the function learned by the teacher model. The idea is generalized in Hinton et al. (2015) by formalizing knowledge distillation as a method to supervise a small student by a large teacher model to obtain a competitive performance.",
            "reference_string": "[275847824 | Rezvani et al. | 2025 | Citations: 1]"
        },
        {
            "title": "Condensation of Data and Knowledge for Network Traffic Classification: Techniques, Applications, and Open Issues",
            "venue": "Italian National Conference on Sensors",
            "year": 2025,
            "reference_count": 161,
            "citation_count": 0,
            "influential_citation_count": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": "CCBY",
                "disclaimer": "Notice: Paper or abstract available at https://pmc.ncbi.nlm.nih.gov/articles/PMC12030602, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2282654599",
                    "name": "Changqing Zhao"
                },
                {
                    "authorId": "2150041345",
                    "name": "L. Liao"
                },
                {
                    "authorId": "1653336787",
                    "name": "Guo-Wei Chen"
                },
                {
                    "authorId": "2196703813",
                    "name": "Han-Chieh Chao"
                }
            ],
            "abstract": "The accurate and efficient classification of network traffic, including malicious traffic, is essential for effective network management, cybersecurity, and resource optimization. However, traffic classification methods in modern, complex, and dynamic networks face significant challenges, particularly at the network edge, where resources are limited and issues such as privacy concerns and concept drift arise. Condensation techniques offer a solution by reducing the data size, simplifying complex models, and transferring knowledge from traffic data. This paper explores data and knowledge condensation methods\u2014such as coreset selection, data compression, knowledge distillation, and dataset distillation\u2014within the context of traffic classification tasks. It clarifies the relationship between these techniques and network traffic classification, introducing each method and its typical applications. This paper also outlines potential scenarios for applying each condensation technique, highlighting the associated challenges and open research issues. To the best of our knowledge, this is the first comprehensive summary of condensation techniques specifically tailored for network traffic classification tasks.",
            "corpus_id": 277724285,
            "sentences": [
                {
                    "corpus_id": "277724285",
                    "title": "Condensation of Data and Knowledge for Network Traffic Classification: Techniques, Applications, and Open Issues",
                    "text": "Knowledge distillation is a technique developed to address the challenges of deploying large deep neural networks, commonly used in fields like computer vision and natural language processing, on resource-constrained devices such as mobile phones, embedded systems, and autonomous vehicles. While methods like factorization, pruning, and the compression of convolutional filters can reduce redundancy and lower the storage and transmission cost of large models, these approaches still require the models to be decoded into their full size for execution, maintaining the resource constraints. Knowledge distillation solves this issue by transferring the knowledge from a complex, large model (or an ensemble of models) into a smaller, more efficient model, enabling deployment on resource-limited devices without significant loss of performance. \n\nTypical knowledge distillation involves transferring knowledge from a large model (or an ensemble of models) to a smaller, more efficient model. This process typically includes three key components: one or more pre-trained teacher models, an initial student model, and a training dataset (as shown in Figure 5). The teacher and student models output class prediction probabilities, and the distillation loss is computed by comparing these predictions. Additionally, the loss between the predicted and true class labels is calculated. Both losses are used to train the student model, optimizing it to achieve an accuracy similar to the teacher model. Knowledge distillation aims to reduce the performance gap between the teacher and student models by efficiently determining their structure. This technique, also known as model distillation, is particularly effective for neural networks with complex architectures and is widely applied in fields such as computer vision, visual question answering, natural language processing, and speech recognition, where large models are impractical due to resource constraints [17]. Knowledge in a neural network model can be categorized into response-based, featurebased, and relation-based knowledge, which correspond to information extracted from the output layer, intermediate layers, and relationships between layers, respectively, [95]. Understanding these different types of knowledge, their core concepts, and how they can be combined is crucial for effective model distillation.",
                    "score": 0.611024544852186,
                    "section_title": "Knowledge Distillation",
                    "char_start_offset": 32870,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 290
                        },
                        {
                            "start": 291,
                            "end": 591
                        },
                        {
                            "start": 592,
                            "end": 844
                        },
                        {
                            "start": 847,
                            "end": 991
                        },
                        {
                            "start": 992,
                            "end": 1158
                        },
                        {
                            "start": 1159,
                            "end": 1298
                        },
                        {
                            "start": 1299,
                            "end": 1380
                        },
                        {
                            "start": 1381,
                            "end": 1496
                        },
                        {
                            "start": 1497,
                            "end": 1637
                        },
                        {
                            "start": 1638,
                            "end": 1966
                        },
                        {
                            "start": 1967,
                            "end": 2226
                        },
                        {
                            "start": 2227,
                            "end": 2371
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 1961,
                            "end": 1965,
                            "matchedPaperCorpusId": "219559263"
                        },
                        {
                            "start": 2221,
                            "end": 2225,
                            "matchedPaperCorpusId": "259203401"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.9873046875
                }
            ],
            "relevance_judgement": 0.9873046875,
            "relevance_judgment_input_expanded": "# Title: Condensation of Data and Knowledge for Network Traffic Classification: Techniques, Applications, and Open Issues\n# Venue: Italian National Conference on Sensors\n# Authors: Changqing Zhao, L. Liao, Guo-Wei Chen, Han-Chieh Chao\n## Abstract\nThe accurate and efficient classification of network traffic, including malicious traffic, is essential for effective network management, cybersecurity, and resource optimization. However, traffic classification methods in modern, complex, and dynamic networks face significant challenges, particularly at the network edge, where resources are limited and issues such as privacy concerns and concept drift arise. Condensation techniques offer a solution by reducing the data size, simplifying complex models, and transferring knowledge from traffic data. This paper explores data and knowledge condensation methods\u2014such as coreset selection, data compression, knowledge distillation, and dataset distillation\u2014within the context of traffic classification tasks. It clarifies the relationship between these techniques and network traffic classification, introducing each method and its typical applications. This paper also outlines potential scenarios for applying each condensation technique, highlighting the associated challenges and open research issues. To the best of our knowledge, this is the first comprehensive summary of condensation techniques specifically tailored for network traffic classification tasks.\n## Knowledge Distillation\nKnowledge distillation is a technique developed to address the challenges of deploying large deep neural networks, commonly used in fields like computer vision and natural language processing, on resource-constrained devices such as mobile phones, embedded systems, and autonomous vehicles. While methods like factorization, pruning, and the compression of convolutional filters can reduce redundancy and lower the storage and transmission cost of large models, these approaches still require the models to be decoded into their full size for execution, maintaining the resource constraints. Knowledge distillation solves this issue by transferring the knowledge from a complex, large model (or an ensemble of models) into a smaller, more efficient model, enabling deployment on resource-limited devices without significant loss of performance. \n\nTypical knowledge distillation involves transferring knowledge from a large model (or an ensemble of models) to a smaller, more efficient model. This process typically includes three key components: one or more pre-trained teacher models, an initial student model, and a training dataset (as shown in Figure 5). The teacher and student models output class prediction probabilities, and the distillation loss is computed by comparing these predictions. Additionally, the loss between the predicted and true class labels is calculated. Both losses are used to train the student model, optimizing it to achieve an accuracy similar to the teacher model. Knowledge distillation aims to reduce the performance gap between the teacher and student models by efficiently determining their structure. This technique, also known as model distillation, is particularly effective for neural networks with complex architectures and is widely applied in fields such as computer vision, visual question answering, natural language processing, and speech recognition, where large models are impractical due to resource constraints [17]. Knowledge in a neural network model can be categorized into response-based, featurebased, and relation-based knowledge, which correspond to information extracted from the output layer, intermediate layers, and relationships between layers, respectively, [95]. Understanding these different types of knowledge, their core concepts, and how they can be combined is crucial for effective model distillation.",
            "reference_string": "[277724285 | Zhao et al. | 2025 | Citations: 0]"
        },
        {
            "title": "A Survey on Symbolic Knowledge Distillation of Large Language Models",
            "venue": "IEEE Transactions on Artificial Intelligence",
            "year": 2024,
            "reference_count": 199,
            "citation_count": 7,
            "influential_citation_count": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2408.10210, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2237801390",
                    "name": "Kamal Acharya"
                },
                {
                    "authorId": "2237801384",
                    "name": "Alvaro Velasquez"
                },
                {
                    "authorId": "2261907178",
                    "name": "H. Song"
                }
            ],
            "abstract": "This survey article delves into the emerging and critical area of symbolic knowledge distillation in large language models (LLMs). As LLMs such as generative pretrained transformer-3 (GPT-3) and bidirectional encoder representations from transformers (BERT) continue to expand in scale and complexity, the challenge of effectively harnessing their extensive knowledge becomes paramount. This survey concentrates on the process of distilling the intricate, often implicit knowledge contained within these models into a more symbolic, explicit form. This transformation is crucial for enhancing the interpretability, efficiency, and applicability of LLMs. We categorize the existing research based on methodologies and applications, focusing on how symbolic knowledge distillation can be used to improve the transparency and functionality of smaller, more efficient artificial intelligence (AI) models. The survey discusses the core challenges, including maintaining the depth of knowledge in a comprehensible format, and explores the various approaches and techniques that have been developed in this field. We identify gaps in current research and potential opportunities for future advancements. This survey aims to provide a comprehensive overview of symbolic knowledge distillation in LLMs, spotlighting its significance in the progression toward more accessible and efficient AI systems.",
            "corpus_id": 271227251,
            "sentences": [
                {
                    "corpus_id": "271227251",
                    "title": "A Survey on Symbolic Knowledge Distillation of Large Language Models",
                    "text": "Knowledge distillation is a technique used to transfer knowledge from a larger, more complex model (teacher) to a smaller, simpler model (student) with the goal of retaining much of the teacher model's performance [117]. This process is crucial in scenarios where computational resources are limited or where deployment requires lightweight models. There are various types of traditional knowledge distillation techniques: response-based, feature-based and relation-based and one modern symbolic knowledge distillation, each with its unique approach and area of application: \n\n1) Response-based Knowledge Distillation: Responsebased knowledge distillation involves transferring knowledge from the teacher model's final output layer to the student model, aiming to mimic the teacher's final predictions. This approach is straightforward and has proven effective across various tasks, employing a loss function based on the divergence between the teacher's and student's logits. It's widely applied in model compression and has been adapted for different types of model predictions, including object detection and human pose estimation, where the teacher's output may include additional information like bounding box offsets [118] or heatmaps for landmarks [119]. A key application of responsebased knowledge distillation is in image classification [44], where \"soft targets\" -the probabilities assigned to each class by the teacher model -play a crucial role. These probabilities are adjusted using a temperature factor to control the softness of the targets, allowing the transfer of knowledge from the teacher to the student. The distillation process typically employs the Kullback-Leibler divergence loss to optimize the similarity between the teacher's and student's probability distributions. \n\nThis method is praised for its simplicity and effectiveness, particularly in leveraging knowledge for training. However, its reliance on the final layer's output means it may not fully utilize intermediate-level supervision from the teacher, an aspect crucial for representation learning in deep neural networks. \n\n2) Feature-based Knowledge Distillation: Feature-based knowledge distillation taps into the strength of deep neural networks to learn hierarchical feature representations, a process central to representation learning [120].",
                    "score": 0.6283109573384413,
                    "section_title": "A. Knowledge Distillation",
                    "char_start_offset": 14819,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 220
                        },
                        {
                            "start": 221,
                            "end": 348
                        },
                        {
                            "start": 349,
                            "end": 574
                        },
                        {
                            "start": 577,
                            "end": 802
                        },
                        {
                            "start": 803,
                            "end": 976
                        },
                        {
                            "start": 977,
                            "end": 1261
                        },
                        {
                            "start": 1262,
                            "end": 1458
                        },
                        {
                            "start": 1459,
                            "end": 1626
                        },
                        {
                            "start": 1627,
                            "end": 1796
                        },
                        {
                            "start": 1799,
                            "end": 1910
                        },
                        {
                            "start": 1911,
                            "end": 2111
                        },
                        {
                            "start": 2114,
                            "end": 2337
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 214,
                            "end": 219,
                            "matchedPaperCorpusId": "219559263"
                        },
                        {
                            "start": 1223,
                            "end": 1228,
                            "matchedPaperCorpusId": "29308926"
                        },
                        {
                            "start": 1255,
                            "end": 1260,
                            "matchedPaperCorpusId": "53292120"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.98583984375
                }
            ],
            "relevance_judgement": 0.98583984375,
            "relevance_judgment_input_expanded": "# Title: A Survey on Symbolic Knowledge Distillation of Large Language Models\n# Venue: IEEE Transactions on Artificial Intelligence\n# Authors: Kamal Acharya, Alvaro Velasquez, H. Song\n## Abstract\nThis survey article delves into the emerging and critical area of symbolic knowledge distillation in large language models (LLMs). As LLMs such as generative pretrained transformer-3 (GPT-3) and bidirectional encoder representations from transformers (BERT) continue to expand in scale and complexity, the challenge of effectively harnessing their extensive knowledge becomes paramount. This survey concentrates on the process of distilling the intricate, often implicit knowledge contained within these models into a more symbolic, explicit form. This transformation is crucial for enhancing the interpretability, efficiency, and applicability of LLMs. We categorize the existing research based on methodologies and applications, focusing on how symbolic knowledge distillation can be used to improve the transparency and functionality of smaller, more efficient artificial intelligence (AI) models. The survey discusses the core challenges, including maintaining the depth of knowledge in a comprehensible format, and explores the various approaches and techniques that have been developed in this field. We identify gaps in current research and potential opportunities for future advancements. This survey aims to provide a comprehensive overview of symbolic knowledge distillation in LLMs, spotlighting its significance in the progression toward more accessible and efficient AI systems.\n## A. Knowledge Distillation\nKnowledge distillation is a technique used to transfer knowledge from a larger, more complex model (teacher) to a smaller, simpler model (student) with the goal of retaining much of the teacher model's performance [117]. This process is crucial in scenarios where computational resources are limited or where deployment requires lightweight models. There are various types of traditional knowledge distillation techniques: response-based, feature-based and relation-based and one modern symbolic knowledge distillation, each with its unique approach and area of application: \n\n1) Response-based Knowledge Distillation: Responsebased knowledge distillation involves transferring knowledge from the teacher model's final output layer to the student model, aiming to mimic the teacher's final predictions. This approach is straightforward and has proven effective across various tasks, employing a loss function based on the divergence between the teacher's and student's logits. It's widely applied in model compression and has been adapted for different types of model predictions, including object detection and human pose estimation, where the teacher's output may include additional information like bounding box offsets [118] or heatmaps for landmarks [119]. A key application of responsebased knowledge distillation is in image classification [44], where \"soft targets\" -the probabilities assigned to each class by the teacher model -play a crucial role. These probabilities are adjusted using a temperature factor to control the softness of the targets, allowing the transfer of knowledge from the teacher to the student. The distillation process typically employs the Kullback-Leibler divergence loss to optimize the similarity between the teacher's and student's probability distributions. \n\nThis method is praised for its simplicity and effectiveness, particularly in leveraging knowledge for training. However, its reliance on the final layer's output means it may not fully utilize intermediate-level supervision from the teacher, an aspect crucial for representation learning in deep neural networks. \n\n2) Feature-based Knowledge Distillation: Feature-based knowledge distillation taps into the strength of deep neural networks to learn hierarchical feature representations, a process central to representation learning [120].",
            "reference_string": "[271227251 | Acharya et al. | 2024 | Citations: 7]"
        },
        {
            "title": "Performance Analysis of Deep Neural Network based on Transfer Learning for Pet Classification",
            "venue": "International Journal of Advanced Computer Science and Applications",
            "year": 2021,
            "reference_count": 41,
            "citation_count": 0,
            "influential_citation_count": 0,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://doi.org/10.14569/ijacsa.2021.0120309",
                "status": "GOLD",
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.14569/IJACSA.2021.0120309?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.14569/IJACSA.2021.0120309, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2139643077",
                    "name": "Bhavesh Jaiswal"
                },
                {
                    "authorId": "32136431",
                    "name": "Nagendra Gajjar"
                }
            ],
            "abstract": "Deep learning frameworks have progressed beyond human recognition capabilities and, now it\u2019s the perfect opportunity to optimize them for implementation on the embedded platforms. The present deep learning architectures support learning capabilities, but they lack flexibility for applying learned knowledge on the tasks in other unfamiliar domains. This work tries to fill this gap with the deep neural network-based solution for object detection in unrelated domains with a focus on the reduced footprint of the developed model. Knowledge distillation provides efficient and effective teacher-student learning for a variety of different visual recognition tasks. A lightweight student network can be easily trained under the guidance of the high-capacity teacher networks. The teacher-student architecture implementation on binary classes shows a 20% improvement in accuracy within the same training iterations using the transfer learning approach. The scalability of the student model is tested with binary, ternary and multiclass and their performance is compared on basis of inference speed. The results show that the inference speed does not depend on the number of classes. For similar recognition accuracy, the inference speed of about 50 frames per second or 20ms per image. Thus, this approach can be generalized as per the application requirement with minimal changes, provided the dataset format compatibility.",
            "corpus_id": 234336288,
            "sentences": [
                {
                    "corpus_id": "234336288",
                    "title": "Performance Analysis of Deep Neural Network based on Transfer Learning for Pet Classification",
                    "text": "Knowledge distillation (KD) was introduced by [30] as: \n\n\u2022 Train a large model that performs and generalizes very well. This is called the teacher model. \n\n\u2022 Take all the data you have and compute the predictions of the teacher model. The total dataset with these predictions is called the knowledge, and the predictions themselves are often referred to as soft targets. This is the knowledge distillation step. \n\n\u2022 Use the previously obtained knowledge to train the smaller network, called the student model. IV. TEACHER-STUDENT LEARNING Knowledge distillation starts with training a larger model, the teacher 'T'. As it is trained on a heavier platform (GPU), it achieves high performance. Then a lightweight model known as student 'S' is deployed to learn from 'T'. Now, 'S' is supposed to give comparable performance as 'T' but with less memory and more speed. \n\nTo improve knowledge transfer from teacher to student various types of methods are researched. Assuming a trained 'T' has already eliminated some label errors contained in the ground truth data, the authors in [29] treated the hard label predicted by 'T' as the underlying knowledge. While in [30], the soft label produced by 'T', i.e., the classification probabilities, are focused to provide more information to transfer. In general, knowledge is transferred from the 'T' to 'S' by minimizing a loss function in which the target is the distribution of class probabilities predicted by 'T'. This probability distribution has the correct class at a very high probability (close to '1') with all other class probabilities very close to '0'. As such, it does not provide much information beyond the ground truth labels already provided in the dataset. For this, Hinton [30], introduced the concept of \"softmax temperature\". As it grows, the probability distribution generated by the softmax function becomes softer, providing more information as to which classes 'T' found more like the predicted class. This is the \"dark knowledge\" embedded in the 'T' and transferred to 'S' in the distillation process. The distillation related work can be categorized as below:",
                    "score": 0.6198821220391341,
                    "section_title": "C. Knowledge Distillation",
                    "char_start_offset": 8373,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 54
                        },
                        {
                            "start": 57,
                            "end": 119
                        },
                        {
                            "start": 120,
                            "end": 153
                        },
                        {
                            "start": 156,
                            "end": 234
                        },
                        {
                            "start": 235,
                            "end": 370
                        },
                        {
                            "start": 371,
                            "end": 411
                        },
                        {
                            "start": 414,
                            "end": 509
                        },
                        {
                            "start": 510,
                            "end": 513
                        },
                        {
                            "start": 514,
                            "end": 615
                        },
                        {
                            "start": 616,
                            "end": 691
                        },
                        {
                            "start": 692,
                            "end": 768
                        },
                        {
                            "start": 769,
                            "end": 864
                        },
                        {
                            "start": 867,
                            "end": 961
                        },
                        {
                            "start": 962,
                            "end": 1150
                        },
                        {
                            "start": 1151,
                            "end": 1290
                        },
                        {
                            "start": 1291,
                            "end": 1458
                        },
                        {
                            "start": 1459,
                            "end": 1606
                        },
                        {
                            "start": 1607,
                            "end": 1716
                        },
                        {
                            "start": 1717,
                            "end": 1788
                        },
                        {
                            "start": 1789,
                            "end": 1968
                        },
                        {
                            "start": 1969,
                            "end": 2069
                        },
                        {
                            "start": 2070,
                            "end": 2128
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 1077,
                            "end": 1081,
                            "matchedPaperCorpusId": "11536917"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.984375
                }
            ],
            "relevance_judgement": 0.984375,
            "relevance_judgment_input_expanded": "# Title: Performance Analysis of Deep Neural Network based on Transfer Learning for Pet Classification\n# Venue: International Journal of Advanced Computer Science and Applications\n# Authors: Bhavesh Jaiswal, Nagendra Gajjar\n## Abstract\nDeep learning frameworks have progressed beyond human recognition capabilities and, now it\u2019s the perfect opportunity to optimize them for implementation on the embedded platforms. The present deep learning architectures support learning capabilities, but they lack flexibility for applying learned knowledge on the tasks in other unfamiliar domains. This work tries to fill this gap with the deep neural network-based solution for object detection in unrelated domains with a focus on the reduced footprint of the developed model. Knowledge distillation provides efficient and effective teacher-student learning for a variety of different visual recognition tasks. A lightweight student network can be easily trained under the guidance of the high-capacity teacher networks. The teacher-student architecture implementation on binary classes shows a 20% improvement in accuracy within the same training iterations using the transfer learning approach. The scalability of the student model is tested with binary, ternary and multiclass and their performance is compared on basis of inference speed. The results show that the inference speed does not depend on the number of classes. For similar recognition accuracy, the inference speed of about 50 frames per second or 20ms per image. Thus, this approach can be generalized as per the application requirement with minimal changes, provided the dataset format compatibility.\n## C. Knowledge Distillation\nKnowledge distillation (KD) was introduced by [30] as: \n\n\u2022 Train a large model that performs and generalizes very well. This is called the teacher model. \n\n\u2022 Take all the data you have and compute the predictions of the teacher model. The total dataset with these predictions is called the knowledge, and the predictions themselves are often referred to as soft targets. This is the knowledge distillation step. \n\n\u2022 Use the previously obtained knowledge to train the smaller network, called the student model. IV. TEACHER-STUDENT LEARNING Knowledge distillation starts with training a larger model, the teacher 'T'. As it is trained on a heavier platform (GPU), it achieves high performance. Then a lightweight model known as student 'S' is deployed to learn from 'T'. Now, 'S' is supposed to give comparable performance as 'T' but with less memory and more speed. \n\nTo improve knowledge transfer from teacher to student various types of methods are researched. Assuming a trained 'T' has already eliminated some label errors contained in the ground truth data, the authors in [29] treated the hard label predicted by 'T' as the underlying knowledge. While in [30], the soft label produced by 'T', i.e., the classification probabilities, are focused to provide more information to transfer. In general, knowledge is transferred from the 'T' to 'S' by minimizing a loss function in which the target is the distribution of class probabilities predicted by 'T'. This probability distribution has the correct class at a very high probability (close to '1') with all other class probabilities very close to '0'. As such, it does not provide much information beyond the ground truth labels already provided in the dataset. For this, Hinton [30], introduced the concept of \"softmax temperature\". As it grows, the probability distribution generated by the softmax function becomes softer, providing more information as to which classes 'T' found more like the predicted class. This is the \"dark knowledge\" embedded in the 'T' and transferred to 'S' in the distillation process. The distillation related work can be categorized as below:",
            "reference_string": "[234336288 | Jaiswal et al. | 2021 | Citations: 0]"
        },
        {
            "title": "A Comprehensive Review of Hardware Acceleration Techniques and Convolutional Neural Networks for EEG Signals",
            "venue": "Italian National Conference on Sensors",
            "year": 2024,
            "reference_count": 141,
            "citation_count": 1,
            "influential_citation_count": 0,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://doi.org/10.3390/s24175813",
                "status": "GOLD",
                "license": "CCBY",
                "disclaimer": "Notice: Paper or abstract available at https://pmc.ncbi.nlm.nih.gov/articles/PMC11397884, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2118595916",
                    "name": "Yu Xie"
                },
                {
                    "authorId": "2277106978",
                    "name": "Stefan Oniga"
                }
            ],
            "abstract": "This paper comprehensively reviews hardware acceleration techniques and the deployment of convolutional neural networks (CNNs) for analyzing electroencephalogram (EEG) signals across various application areas, including emotion classification, motor imagery, epilepsy detection, and sleep monitoring. Previous reviews on EEG have mainly focused on software solutions. However, these reviews often overlook key challenges associated with hardware implementation, such as scenarios that require a small size, low power, high security, and high accuracy. This paper discusses the challenges and opportunities of hardware acceleration for wearable EEG devices by focusing on these aspects. Specifically, this review classifies EEG signal features into five groups and discusses hardware implementation solutions for each category in detail, providing insights into the most suitable hardware acceleration strategies for various application scenarios. In addition, it explores the complexity of efficient CNN architectures for EEG signals, including techniques such as pruning, quantization, tensor decomposition, knowledge distillation, and neural architecture search. To the best of our knowledge, this is the first systematic review that combines CNN hardware solutions with EEG signal processing. By providing a comprehensive analysis of current challenges and a roadmap for future research, this paper provides a new perspective on the ongoing development of hardware-accelerated EEG systems.",
            "corpus_id": 272564024,
            "sentences": [
                {
                    "corpus_id": "272564024",
                    "title": "A Comprehensive Review of Hardware Acceleration Techniques and Convolutional Neural Networks for EEG Signals",
                    "text": "Knowledge distillation is a technique that transfers knowledge from large, complex CNNs (teachers) to smaller, more efficient CNNs (students). The student network is trained to mimic the behavior of the teacher, resulting in a compact model with comparable performance. This concept can be traced back to the pioneering work in [101] and has since been extended to the context of deep learning [102]. The core challenge of knowledge distillation revolves around the techniques used to transfer knowledge from the teacher model to the student model, which involves three fundamental components-knowledge, distillation algorithms-and the architecture defining the relationship between the teacher and student models. In this context, knowledge manifests in various forms, including logits, activations, or features extracted from intermediate layers of the teacher model. The distillation algorithms can be categorized as offline, online, or self-distillation. \n\nOffline distillation [101,[103][104][105] extracts knowledge from a pre-trained teacher model and uses the soft label output of the teacher model to train the student network. The authors of [103] used data augmentation to exploit the output distributions of multiple teacher networks. The authors of [101] introduced a tailored distillation approach for quantized models, demonstrating that quantized student networks can closely match the accuracy of full-precision teacher networks while achieving high compression rates and inference acceleration. In contrast, [104] pioneered a data-free technique, training the student network using synthetic data responses from the complex teacher network. Online distillation [106][107][108] occurs during the simultaneous training of both the teacher and student models. It employs online knowledge distillation using the soft-label outputs of the teacher network. Ref. [106] proposed training the student model at different checkpoints of the teacher model until convergence is achieved. Meanwhile, Collaborative Learning Knowledge Distillation (KDCL) [107] dynamically generates high-quality soft targets through an ensemble approach for one-stage online training. Furthermore, as observed in [105,108], knowledge distillation extends its influence to the intermediate layers of the teacher network.",
                    "score": 0.7317625041331279,
                    "section_title": "Knowledge Distillation",
                    "char_start_offset": 46568,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 142
                        },
                        {
                            "start": 143,
                            "end": 269
                        },
                        {
                            "start": 270,
                            "end": 400
                        },
                        {
                            "start": 401,
                            "end": 714
                        },
                        {
                            "start": 715,
                            "end": 869
                        },
                        {
                            "start": 870,
                            "end": 958
                        },
                        {
                            "start": 961,
                            "end": 1136
                        },
                        {
                            "start": 1137,
                            "end": 1246
                        },
                        {
                            "start": 1247,
                            "end": 1512
                        },
                        {
                            "start": 1513,
                            "end": 1658
                        },
                        {
                            "start": 1659,
                            "end": 1774
                        },
                        {
                            "start": 1775,
                            "end": 1868
                        },
                        {
                            "start": 1869,
                            "end": 1992
                        },
                        {
                            "start": 1993,
                            "end": 2170
                        },
                        {
                            "start": 2171,
                            "end": 2305
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 394,
                            "end": 399,
                            "matchedPaperCorpusId": "7200347"
                        },
                        {
                            "start": 987,
                            "end": 992,
                            "matchedPaperCorpusId": "30258763"
                        },
                        {
                            "start": 992,
                            "end": 997,
                            "matchedPaperCorpusId": "159041346"
                        },
                        {
                            "start": 997,
                            "end": 1002,
                            "matchedPaperCorpusId": "182183296"
                        },
                        {
                            "start": 1152,
                            "end": 1157,
                            "matchedPaperCorpusId": "30258763"
                        },
                        {
                            "start": 1526,
                            "end": 1531,
                            "matchedPaperCorpusId": "159041346"
                        },
                        {
                            "start": 1679,
                            "end": 1684,
                            "matchedPaperCorpusId": "125985701"
                        },
                        {
                            "start": 1684,
                            "end": 1689,
                            "matchedPaperCorpusId": "219965421"
                        },
                        {
                            "start": 1689,
                            "end": 1694,
                            "matchedPaperCorpusId": "226841849"
                        },
                        {
                            "start": 1874,
                            "end": 1879,
                            "matchedPaperCorpusId": "125985701"
                        },
                        {
                            "start": 2057,
                            "end": 2062,
                            "matchedPaperCorpusId": "219965421"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.98388671875
                }
            ],
            "relevance_judgement": 0.98388671875,
            "relevance_judgment_input_expanded": "# Title: A Comprehensive Review of Hardware Acceleration Techniques and Convolutional Neural Networks for EEG Signals\n# Venue: Italian National Conference on Sensors\n# Authors: Yu Xie, Stefan Oniga\n## Abstract\nThis paper comprehensively reviews hardware acceleration techniques and the deployment of convolutional neural networks (CNNs) for analyzing electroencephalogram (EEG) signals across various application areas, including emotion classification, motor imagery, epilepsy detection, and sleep monitoring. Previous reviews on EEG have mainly focused on software solutions. However, these reviews often overlook key challenges associated with hardware implementation, such as scenarios that require a small size, low power, high security, and high accuracy. This paper discusses the challenges and opportunities of hardware acceleration for wearable EEG devices by focusing on these aspects. Specifically, this review classifies EEG signal features into five groups and discusses hardware implementation solutions for each category in detail, providing insights into the most suitable hardware acceleration strategies for various application scenarios. In addition, it explores the complexity of efficient CNN architectures for EEG signals, including techniques such as pruning, quantization, tensor decomposition, knowledge distillation, and neural architecture search. To the best of our knowledge, this is the first systematic review that combines CNN hardware solutions with EEG signal processing. By providing a comprehensive analysis of current challenges and a roadmap for future research, this paper provides a new perspective on the ongoing development of hardware-accelerated EEG systems.\n## Knowledge Distillation\nKnowledge distillation is a technique that transfers knowledge from large, complex CNNs (teachers) to smaller, more efficient CNNs (students). The student network is trained to mimic the behavior of the teacher, resulting in a compact model with comparable performance. This concept can be traced back to the pioneering work in [101] and has since been extended to the context of deep learning [102]. The core challenge of knowledge distillation revolves around the techniques used to transfer knowledge from the teacher model to the student model, which involves three fundamental components-knowledge, distillation algorithms-and the architecture defining the relationship between the teacher and student models. In this context, knowledge manifests in various forms, including logits, activations, or features extracted from intermediate layers of the teacher model. The distillation algorithms can be categorized as offline, online, or self-distillation. \n\nOffline distillation [101,[103][104][105] extracts knowledge from a pre-trained teacher model and uses the soft label output of the teacher model to train the student network. The authors of [103] used data augmentation to exploit the output distributions of multiple teacher networks. The authors of [101] introduced a tailored distillation approach for quantized models, demonstrating that quantized student networks can closely match the accuracy of full-precision teacher networks while achieving high compression rates and inference acceleration. In contrast, [104] pioneered a data-free technique, training the student network using synthetic data responses from the complex teacher network. Online distillation [106][107][108] occurs during the simultaneous training of both the teacher and student models. It employs online knowledge distillation using the soft-label outputs of the teacher network. Ref. [106] proposed training the student model at different checkpoints of the teacher model until convergence is achieved. Meanwhile, Collaborative Learning Knowledge Distillation (KDCL) [107] dynamically generates high-quality soft targets through an ensemble approach for one-stage online training. Furthermore, as observed in [105,108], knowledge distillation extends its influence to the intermediate layers of the teacher network.",
            "reference_string": "[272564024 | Xie et al. | 2024 | Citations: 1]"
        },
        {
            "title": "A Lightweight Fault Diagnosis Method Based on Knowledge Distillation Under Time-Varying Rotational Speeds",
            "venue": "IEEE Access",
            "year": 2025,
            "reference_count": 23,
            "citation_count": 0,
            "influential_citation_count": 0,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://doi.org/10.1109/access.2024.3431228",
                "status": "GOLD",
                "license": "CCBYNCND",
                "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.1109/ACCESS.2024.3431228?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1109/ACCESS.2024.3431228, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2304217576",
                    "name": "Xiwang Yang"
                },
                {
                    "authorId": "2304301205",
                    "name": "Yarong Wang"
                },
                {
                    "authorId": "2289791638",
                    "name": "Lele Gao"
                },
                {
                    "authorId": "2119194967",
                    "name": "Jia Luo"
                },
                {
                    "authorId": "2294094731",
                    "name": "Licheng Jing"
                },
                {
                    "authorId": "2294320327",
                    "name": "Jinying Huang"
                },
                {
                    "authorId": "2312885379",
                    "name": "Guangpu Liu"
                },
                {
                    "authorId": "2312892885",
                    "name": "Chenfeng Yang"
                }
            ],
            "abstract": "Intelligent fault diagnosis models for engineering must meet real-time requirements while satisfying high diagnosis rates. However, collecting a large amount of fault information of different working conditions in the engineering environment is difficult, and the lack of fault data is a problem that has been difficult to solve. This paper presents a lightweight fault diagnosis model for wind turbine gearboxes based on transfer learning and knowledge distillation. The model is designed to be lightweight and can be used under time-varying rotational speeds. First, TL-ResPConv based on transfer learning and partial convolution was designed as a teacher network model to train a lightweight student network model for fault diagnosis based on TL-ResPConv-KD. Then, the student model obtained through knowledge distillation training is used as the distilled fault diagnosis model. Finally, the performance of the different models is validated and compared on a laboratory simulated planetary gearbox dataset. The experiments prove that the proposed model outperforms other models in fault recognition, achieving a balance between lightweight and high diagnostic accuracy. This makes it a practical network with higher diagnostic accuracy for fault diagnosis under time-varying rotational speeds, meeting the demand for lightweight deployment of edge equipment.",
            "corpus_id": 271356843,
            "sentences": [
                {
                    "corpus_id": "271356843",
                    "title": "A Lightweight Fault Diagnosis Method Based on Knowledge Distillation Under Time-Varying Rotational Speeds",
                    "text": "The core concept of knowledge distillation is that the deep knowledge accumulated in the complex pre-trained teacher network model is distilled and injected into the more streamlined student model, which is able to inherit and mimic the predictive performance of the teacher model through knowledge distillation, thus reducing the model complexity while maintaining similar predictive effects. Knowledge distillation aims to optimize the balance between model size and performance through knowledge transfer to achieve effective model compression and knowledge transfer. The principle of knowledge distillation is shown in Figure 2. In this paper, we adopt the knowledge transfer mechanism, which is a framework in which the student network learns by simulating the output of the teacher network. In this framework, the student network is trained under the guidance and supervision of the teacher network, which draws on the a priori knowledge and generalization ability of the pre-trained teacher network, and helps to improve the learning efficiency and effectiveness. \n\nOffline distillation follows the classical ''teacher-student'' paradigm, so that the student network, in the process of imitating the teacher network, not only learns the decision-making behavior of the teacher network on the training samples, but also passes on the deep patterns and abstract knowledge contained in the teacher network, thus realizing the effective transfer and compression of knowledge, and the degree of fit of the student network to the output of the teacher network is quantified by the loss function. The degree of fitting of the teacher network output results by the loss function can be expressed as follows: \n\nwhere L task denotes the learning performance of the student network on the original task, Ls is the output vector of the student network and Lt is the output vector of the teacher network, which L KD is used to quantify the difference between the student network and the teacher network at the output level. In order to integrate the loss of the student network's own learning task with the loss of imitating the teacher network's output, a balance coefficient is introduced, which \u03bb is used to regulate the relative weights of the two in the total loss function.",
                    "score": 0.7213240739902158,
                    "section_title": "C. KNOWLEDGE DISTILLATION",
                    "char_start_offset": 10854,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 393
                        },
                        {
                            "start": 394,
                            "end": 570
                        },
                        {
                            "start": 571,
                            "end": 796
                        },
                        {
                            "start": 797,
                            "end": 1070
                        },
                        {
                            "start": 1073,
                            "end": 1596
                        },
                        {
                            "start": 1597,
                            "end": 1706
                        },
                        {
                            "start": 1709,
                            "end": 2017
                        },
                        {
                            "start": 2018,
                            "end": 2273
                        }
                    ],
                    "ref_mentions": [],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.98388671875
                }
            ],
            "relevance_judgement": 0.98388671875,
            "relevance_judgment_input_expanded": "# Title: A Lightweight Fault Diagnosis Method Based on Knowledge Distillation Under Time-Varying Rotational Speeds\n# Venue: IEEE Access\n# Authors: Xiwang Yang, Yarong Wang, Lele Gao, Jia Luo, Licheng Jing, Jinying Huang, Guangpu Liu, Chenfeng Yang\n## Abstract\nIntelligent fault diagnosis models for engineering must meet real-time requirements while satisfying high diagnosis rates. However, collecting a large amount of fault information of different working conditions in the engineering environment is difficult, and the lack of fault data is a problem that has been difficult to solve. This paper presents a lightweight fault diagnosis model for wind turbine gearboxes based on transfer learning and knowledge distillation. The model is designed to be lightweight and can be used under time-varying rotational speeds. First, TL-ResPConv based on transfer learning and partial convolution was designed as a teacher network model to train a lightweight student network model for fault diagnosis based on TL-ResPConv-KD. Then, the student model obtained through knowledge distillation training is used as the distilled fault diagnosis model. Finally, the performance of the different models is validated and compared on a laboratory simulated planetary gearbox dataset. The experiments prove that the proposed model outperforms other models in fault recognition, achieving a balance between lightweight and high diagnostic accuracy. This makes it a practical network with higher diagnostic accuracy for fault diagnosis under time-varying rotational speeds, meeting the demand for lightweight deployment of edge equipment.\n## C. KNOWLEDGE DISTILLATION\nThe core concept of knowledge distillation is that the deep knowledge accumulated in the complex pre-trained teacher network model is distilled and injected into the more streamlined student model, which is able to inherit and mimic the predictive performance of the teacher model through knowledge distillation, thus reducing the model complexity while maintaining similar predictive effects. Knowledge distillation aims to optimize the balance between model size and performance through knowledge transfer to achieve effective model compression and knowledge transfer. The principle of knowledge distillation is shown in Figure 2. In this paper, we adopt the knowledge transfer mechanism, which is a framework in which the student network learns by simulating the output of the teacher network. In this framework, the student network is trained under the guidance and supervision of the teacher network, which draws on the a priori knowledge and generalization ability of the pre-trained teacher network, and helps to improve the learning efficiency and effectiveness. \n\nOffline distillation follows the classical ''teacher-student'' paradigm, so that the student network, in the process of imitating the teacher network, not only learns the decision-making behavior of the teacher network on the training samples, but also passes on the deep patterns and abstract knowledge contained in the teacher network, thus realizing the effective transfer and compression of knowledge, and the degree of fit of the student network to the output of the teacher network is quantified by the loss function. The degree of fitting of the teacher network output results by the loss function can be expressed as follows: \n\nwhere L task denotes the learning performance of the student network on the original task, Ls is the output vector of the student network and Lt is the output vector of the teacher network, which L KD is used to quantify the difference between the student network and the teacher network at the output level. In order to integrate the loss of the student network's own learning task with the loss of imitating the teacher network's output, a balance coefficient is introduced, which \u03bb is used to regulate the relative weights of the two in the total loss function.",
            "reference_string": "[271356843 | Yang et al. | 2025 | Citations: 0]"
        },
        {
            "title": "Breast Cancer Histopathology Images Classification Through Multi-View Augmented Contrastive Learning and Pre-Learning Knowledge Distillation",
            "venue": "IEEE Access",
            "year": 2024,
            "reference_count": 42,
            "citation_count": 5,
            "influential_citation_count": 0,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://ieeexplore.ieee.org/ielx7/6287639/6514899/10436650.pdf",
                "status": "GOLD",
                "license": "CCBYNCND",
                "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.1109/ACCESS.2024.3366185?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1109/ACCESS.2024.3366185, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2284255154",
                    "name": "Jialong Si"
                },
                {
                    "authorId": "2284258056",
                    "name": "Wei Jia"
                },
                {
                    "authorId": "2284308867",
                    "name": "Haifeng Jiang"
                }
            ],
            "abstract": "Breast cancer is one of the most common malignant tumors in women, and accurate classification of benign and malignant breast cancer histopathology images is critical for diagnosis and treatment. Knowledge distillation techniques have recently received considerable attention for enabling cost-effective image classification of breast cancer histopathology images. However, there are still challenges, including the inefficient extraction of global and local features from breast cancer histopathology images by the teacher network, and the over-prioritization of feature fusion within the same encoder layer, which tends to neglect valuable information from adjacent encoder layers during knowledge extraction. To address these challenges, we propose a method that combines multi-view augmented contrastive learning and pre-learning knowledge distillation. Multi-view augmented contrastive learning improves the ability of the teacher network to extract global and local features. Pre-learning knowledge distillation merges features from adjacent encoder layers of the teacher network, reducing feature loss between these layers. Empirical experiments demonstrate the effectiveness of our proposed method in classifying histopathology images of benign and malignant breast cancer, achieving excellent results on the BreakHis dataset. These results confirm our method as an effective tool for the classification of benign and malignant breast cancer histopathology images.",
            "corpus_id": 267688311,
            "sentences": [
                {
                    "corpus_id": "267688311",
                    "title": "Breast Cancer Histopathology Images Classification Through Multi-View Augmented Contrastive Learning and Pre-Learning Knowledge Distillation",
                    "text": "Knowledge distillation is a commonly used technique in deep learning, as proposed by Hinton et al. in 2015 [17]. It is used to transfer knowledge from a large model to a smaller one, resulting in model compression and acceleration. The goal of knowledge distillation is to improve the accuracy and generalizability of the student network by transferring knowledge from the teacher network. Auxiliary information during training is usually obtained from the output of the teacher network. Such techniques have been applied in various domains, including natural language processing, computer vision, and speech recognition. An important use case is the deployment of deep learning models on mobile devices, which often have limited computational resources and require smaller, lightweight models. Knowledge distillation typically involves a larger teacher network and a smaller student network. The teacher network is usually trained on large amounts of data and therefore has higher accuracy and more comprehensive knowledge. Compared to the teacher network, the student network is trained on less data and has fewer parameters. Nevertheless, some student networks can surpass the accuracy of the teacher network. In practice, many variations and improved distillation techniques are used. For example, different objective functions, loss functions, and regularization techniques can be used. In addition, merging knowledge from multiple teacher networks into student networks can also be beneficial. To achieve better performance, these techniques can be optimized for specific tasks and data [18]. \n\nKnowledge can be transferred using the output of the last layer via response-based knowledge distillation, which is relatively simple and does not require additional complex computation or design. However, it overlooks the output information of the middle layer, which limits the ability of the student network to fully capture the multi-level features of the teacher network. In addition to using the output of the last layer, the output of the middle layer can also facilitate effective student learning. Consequently, the combination of response-based and feature-based knowledge distillation has become a popular research area [19]. Fitnets [20] propose to use feature maps as output to match the feature maps of teachers and students for better learning. Zagoruyko and Komodakis [21] were inspired by the attention mechanism and proposed Attention Transfer (AT), replacing the original feature maps with attention maps for effective knowledge transfer.",
                    "score": 0.6565598663708867,
                    "section_title": "II. RELATED WORK A. KNOWLEDGE DISTILLATION",
                    "char_start_offset": 7051,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 112
                        },
                        {
                            "start": 113,
                            "end": 231
                        },
                        {
                            "start": 232,
                            "end": 389
                        },
                        {
                            "start": 390,
                            "end": 487
                        },
                        {
                            "start": 488,
                            "end": 621
                        },
                        {
                            "start": 622,
                            "end": 794
                        },
                        {
                            "start": 795,
                            "end": 892
                        },
                        {
                            "start": 893,
                            "end": 1024
                        },
                        {
                            "start": 1025,
                            "end": 1127
                        },
                        {
                            "start": 1128,
                            "end": 1212
                        },
                        {
                            "start": 1213,
                            "end": 1288
                        },
                        {
                            "start": 1289,
                            "end": 1391
                        },
                        {
                            "start": 1392,
                            "end": 1499
                        },
                        {
                            "start": 1500,
                            "end": 1598
                        },
                        {
                            "start": 1601,
                            "end": 1797
                        },
                        {
                            "start": 1798,
                            "end": 1977
                        },
                        {
                            "start": 1978,
                            "end": 2107
                        },
                        {
                            "start": 2108,
                            "end": 2237
                        },
                        {
                            "start": 2238,
                            "end": 2360
                        },
                        {
                            "start": 2361,
                            "end": 2558
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 1593,
                            "end": 1597,
                            "matchedPaperCorpusId": "219559263"
                        },
                        {
                            "start": 2232,
                            "end": 2236,
                            "matchedPaperCorpusId": "232232777"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.9833984375
                }
            ],
            "relevance_judgement": 0.9833984375,
            "relevance_judgment_input_expanded": "# Title: Breast Cancer Histopathology Images Classification Through Multi-View Augmented Contrastive Learning and Pre-Learning Knowledge Distillation\n# Venue: IEEE Access\n# Authors: Jialong Si, Wei Jia, Haifeng Jiang\n## Abstract\nBreast cancer is one of the most common malignant tumors in women, and accurate classification of benign and malignant breast cancer histopathology images is critical for diagnosis and treatment. Knowledge distillation techniques have recently received considerable attention for enabling cost-effective image classification of breast cancer histopathology images. However, there are still challenges, including the inefficient extraction of global and local features from breast cancer histopathology images by the teacher network, and the over-prioritization of feature fusion within the same encoder layer, which tends to neglect valuable information from adjacent encoder layers during knowledge extraction. To address these challenges, we propose a method that combines multi-view augmented contrastive learning and pre-learning knowledge distillation. Multi-view augmented contrastive learning improves the ability of the teacher network to extract global and local features. Pre-learning knowledge distillation merges features from adjacent encoder layers of the teacher network, reducing feature loss between these layers. Empirical experiments demonstrate the effectiveness of our proposed method in classifying histopathology images of benign and malignant breast cancer, achieving excellent results on the BreakHis dataset. These results confirm our method as an effective tool for the classification of benign and malignant breast cancer histopathology images.\n## II. RELATED WORK A. KNOWLEDGE DISTILLATION\nKnowledge distillation is a commonly used technique in deep learning, as proposed by Hinton et al. in 2015 [17]. It is used to transfer knowledge from a large model to a smaller one, resulting in model compression and acceleration. The goal of knowledge distillation is to improve the accuracy and generalizability of the student network by transferring knowledge from the teacher network. Auxiliary information during training is usually obtained from the output of the teacher network. Such techniques have been applied in various domains, including natural language processing, computer vision, and speech recognition. An important use case is the deployment of deep learning models on mobile devices, which often have limited computational resources and require smaller, lightweight models. Knowledge distillation typically involves a larger teacher network and a smaller student network. The teacher network is usually trained on large amounts of data and therefore has higher accuracy and more comprehensive knowledge. Compared to the teacher network, the student network is trained on less data and has fewer parameters. Nevertheless, some student networks can surpass the accuracy of the teacher network. In practice, many variations and improved distillation techniques are used. For example, different objective functions, loss functions, and regularization techniques can be used. In addition, merging knowledge from multiple teacher networks into student networks can also be beneficial. To achieve better performance, these techniques can be optimized for specific tasks and data [18]. \n\nKnowledge can be transferred using the output of the last layer via response-based knowledge distillation, which is relatively simple and does not require additional complex computation or design. However, it overlooks the output information of the middle layer, which limits the ability of the student network to fully capture the multi-level features of the teacher network. In addition to using the output of the last layer, the output of the middle layer can also facilitate effective student learning. Consequently, the combination of response-based and feature-based knowledge distillation has become a popular research area [19]. Fitnets [20] propose to use feature maps as output to match the feature maps of teachers and students for better learning. Zagoruyko and Komodakis [21] were inspired by the attention mechanism and proposed Attention Transfer (AT), replacing the original feature maps with attention maps for effective knowledge transfer.",
            "reference_string": "[267688311 | Si et al. | 2024 | Citations: 5]"
        },
        {
            "title": "Transfer learning and self-distillation for automated detection of schizophrenia using single-channel EEG and scalogram images.",
            "venue": "Physical and Engineering Sciences in Medicine",
            "year": 2024,
            "reference_count": 42,
            "citation_count": 0,
            "influential_citation_count": 0,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://www.researchsquare.com/article/rs-3276496/latest.pdf",
                "status": "GREEN",
                "license": "CCBY",
                "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.1007/s13246-024-01420-1?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1007/s13246-024-01420-1, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2298021598",
                    "name": "Mohammadreza Mostafavi"
                },
                {
                    "authorId": "2291228937",
                    "name": "Seok-bum Ko"
                },
                {
                    "authorId": "1704308",
                    "name": "S. B. Shokouhi"
                },
                {
                    "authorId": "2264990315",
                    "name": "Ahmad Ayatollahi"
                }
            ],
            "abstract": "Schizophrenia (SZ) has been acknowledged as a highly intricate mental disorder for a long time. In fact, individuals with SZ experience a blurred line between fantasy and reality, leading to a lack of awareness about their condition, which can pose significant challenges during the treatment process. Due to the importance of the issue, timely diagnosis of this illness can not only assist patients and their families in managing the condition but also enable early intervention, which may help prevent its advancement. EEG is a widely utilized technique for investigating mental disorders like SZ due to its non-invasive nature, affordability, and wide accessibility. In this study, our main goal is to develop an optimized system that can achieve automatic diagnosis of SZ with minimal input information. To optimize the system, we adopted a strategy of using single-channel EEG signals and integrated knowledge distillation and transfer learning techniques into the model. This approach was designed to improve the performance and efficiency of our proposed method for SZ diagnosis. Additionally, to leverage the pre-trained models effectively, we converted the EEG signals into images using Continuous Wavelet Transform (CWT). This transformation allowed us to harness the capabilities of pre-trained models in the image domain, enabling automatic SZ detection with enhanced efficiency. To achieve a more robust estimate of the model's performance, we employed fivefold cross-validation. The accuracy achieved from the 5-s records of the EEG signal, along with the combination of self-distillation and VGG16 for the P4 channel, is 97.81. This indicates a high level of accuracy in diagnosing SZ using the proposed method.",
            "corpus_id": 269327338,
            "sentences": [
                {
                    "corpus_id": "269327338",
                    "title": "Transfer learning and self-distillation for automated detection of schizophrenia using single-channel EEG and scalogram images.",
                    "text": "Knowledge Distillation denotes the procedure of transferring knowledge from a sophisticated model to a more straightforward one. It involves training smaller models to achieve similar accuracy as larger models by leveraging the knowledge gained from the larger models. Within the context of knowledge distillation, the term \"teacher network\" is used to describe the larger model, whereas the \"student network\" refers to the smaller network. The fundamental concept behind Knowledge Distillation is to train a smaller and less complex model to imitate the behavior and generalization capabilities of a larger and more complex model. The process of knowledge distillation involves transferring knowledge from the teacher network to the student network by optimizing a loss function [32]. Figure 6 [33] illustrates a typical framework for knowledge distillation, where a teacher-student relationship is established.",
                    "score": 0.8343212112949687,
                    "section_title": "Knowledge distillation",
                    "char_start_offset": 17184,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 128
                        },
                        {
                            "start": 129,
                            "end": 268
                        },
                        {
                            "start": 269,
                            "end": 440
                        },
                        {
                            "start": 441,
                            "end": 631
                        },
                        {
                            "start": 632,
                            "end": 785
                        },
                        {
                            "start": 786,
                            "end": 912
                        }
                    ],
                    "ref_mentions": [],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.98291015625
                }
            ],
            "relevance_judgement": 0.98291015625,
            "relevance_judgment_input_expanded": "# Title: Transfer learning and self-distillation for automated detection of schizophrenia using single-channel EEG and scalogram images.\n# Venue: Physical and Engineering Sciences in Medicine\n# Authors: Mohammadreza Mostafavi, Seok-bum Ko, S. B. Shokouhi, Ahmad Ayatollahi\n## Abstract\nSchizophrenia (SZ) has been acknowledged as a highly intricate mental disorder for a long time. In fact, individuals with SZ experience a blurred line between fantasy and reality, leading to a lack of awareness about their condition, which can pose significant challenges during the treatment process. Due to the importance of the issue, timely diagnosis of this illness can not only assist patients and their families in managing the condition but also enable early intervention, which may help prevent its advancement. EEG is a widely utilized technique for investigating mental disorders like SZ due to its non-invasive nature, affordability, and wide accessibility. In this study, our main goal is to develop an optimized system that can achieve automatic diagnosis of SZ with minimal input information. To optimize the system, we adopted a strategy of using single-channel EEG signals and integrated knowledge distillation and transfer learning techniques into the model. This approach was designed to improve the performance and efficiency of our proposed method for SZ diagnosis. Additionally, to leverage the pre-trained models effectively, we converted the EEG signals into images using Continuous Wavelet Transform (CWT). This transformation allowed us to harness the capabilities of pre-trained models in the image domain, enabling automatic SZ detection with enhanced efficiency. To achieve a more robust estimate of the model's performance, we employed fivefold cross-validation. The accuracy achieved from the 5-s records of the EEG signal, along with the combination of self-distillation and VGG16 for the P4 channel, is 97.81. This indicates a high level of accuracy in diagnosing SZ using the proposed method.\n## Knowledge distillation\nKnowledge Distillation denotes the procedure of transferring knowledge from a sophisticated model to a more straightforward one. It involves training smaller models to achieve similar accuracy as larger models by leveraging the knowledge gained from the larger models. Within the context of knowledge distillation, the term \"teacher network\" is used to describe the larger model, whereas the \"student network\" refers to the smaller network. The fundamental concept behind Knowledge Distillation is to train a smaller and less complex model to imitate the behavior and generalization capabilities of a larger and more complex model. The process of knowledge distillation involves transferring knowledge from the teacher network to the student network by optimizing a loss function [32]. Figure 6 [33] illustrates a typical framework for knowledge distillation, where a teacher-student relationship is established.",
            "reference_string": "[269327338 | Mostafavi et al. | 2024 | Citations: 0]"
        },
        {
            "title": "From Efficient Multimodal Models to World Models: A Survey",
            "venue": "arXiv.org",
            "year": 2024,
            "reference_count": 121,
            "citation_count": 5,
            "influential_citation_count": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2407.00118, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2276937443",
                    "name": "Xinji Mai"
                },
                {
                    "authorId": "2261831274",
                    "name": "Zeng Tao"
                },
                {
                    "authorId": "2261891655",
                    "name": "Junxiong Lin"
                },
                {
                    "authorId": "2276807843",
                    "name": "Haoran Wang"
                },
                {
                    "authorId": "2276969811",
                    "name": "Yang Chang"
                },
                {
                    "authorId": "2212014366",
                    "name": "Yanlan Kang"
                },
                {
                    "authorId": "2276879376",
                    "name": "Yan Wang"
                },
                {
                    "authorId": "2276819302",
                    "name": "Wenqiang Zhang"
                }
            ],
            "abstract": "Multimodal Large Models (MLMs) are becoming a significant research focus, combining powerful large language models with multimodal learning to perform complex tasks across different data modalities. This review explores the latest developments and challenges in MLMs, emphasizing their potential in achieving artificial general intelligence and as a pathway to world models. We provide an overview of key techniques such as Multimodal Chain of Thought (M-COT), Multimodal Instruction Tuning (M-IT), and Multimodal In-Context Learning (M-ICL). Additionally, we discuss both the fundamental and specific technologies of multimodal models, highlighting their applications, input/output modalities, and design characteristics. Despite significant advancements, the development of a unified multimodal model remains elusive. We discuss the integration of 3D generation and embodied intelligence to enhance world simulation capabilities and propose incorporating external rule systems for improved reasoning and decision-making. Finally, we outline future research directions to address these challenges and advance the field.",
            "corpus_id": 270870796,
            "sentences": [
                {
                    "corpus_id": "270870796",
                    "title": "From Efficient Multimodal Models to World Models: A Survey",
                    "text": "Knowledge Distillation (KD) is a model compression technique that transfers knowledge from a complex model (called the teacher model) to a smaller model (called the student model).This allows the student model to maintain high computational efficiency while achieving the performance of the teacher model.Knowledge distillation was first proposed by Bucilu\u01ce et al., who trained compressed models with pseudodata classifiers to replicate the original classifier's outputs [14].KD can be divided into homomorphic KD and heteromorphic KD.\n\nHomomorphic KD means the student and teacher models have similar or identical structures.In this approach, the student model learns by mimicking the teacher model's outputs (e.g., logits, feature layer outputs).Common homomorphic KD methods include logit-level distillation, feature-level distillation, and module-level distillation.For instance, TinyViT [46] applies distillation during pre-training, storing logits from a large teacher model on hardware to achieve memory and computational efficiency when transferring knowledge to a smaller student Transformer.DeiT-Tiny [47] adopts patchlevel distillation, training a small student model to match the pre-trained teacher model's patch structure, then optimizing with decomposed manifold matching loss to reduce computational costs.Module-level methods like m2mKD [48] separate the teacher module from a pre-trained unified model, combining student modules with modular models, and using a shared meta-model for composition, enabling student modules to mimic teacher module behavior.Feature-level distillation methods like MiniViT [49] combine weights from consecutive Transformer blocks for cross-layer weight sharing, introducing transformations to enhance learning.\n\nHeteromorphic KD refers to student and teacher models with different structures.In this approach, the student model learns by mimicking the teacher model's outputs or intermediate features, despite different architectures.Heteromorphic KD enhances the student model's adaptability, enabling it to learn useful information from the teacher model.Heteromorphic KD includes soft label distillation, where the student model trains by mimicking the teacher model's soft label outputs.",
                    "score": 0.6117979809528357,
                    "section_title": "C. Knowledge Distillation",
                    "char_start_offset": 25157,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 180
                        },
                        {
                            "start": 180,
                            "end": 305
                        },
                        {
                            "start": 305,
                            "end": 476
                        },
                        {
                            "start": 476,
                            "end": 535
                        },
                        {
                            "start": 537,
                            "end": 626
                        },
                        {
                            "start": 626,
                            "end": 748
                        },
                        {
                            "start": 748,
                            "end": 870
                        },
                        {
                            "start": 870,
                            "end": 1101
                        },
                        {
                            "start": 1101,
                            "end": 1322
                        },
                        {
                            "start": 1322,
                            "end": 1573
                        },
                        {
                            "start": 1573,
                            "end": 1758
                        },
                        {
                            "start": 1760,
                            "end": 1840
                        },
                        {
                            "start": 1840,
                            "end": 1982
                        },
                        {
                            "start": 1982,
                            "end": 2105
                        },
                        {
                            "start": 2105,
                            "end": 2239
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 471,
                            "end": 475,
                            "matchedPaperCorpusId": "219559263"
                        },
                        {
                            "start": 892,
                            "end": 896,
                            "matchedPaperCorpusId": "250920355"
                        },
                        {
                            "start": 1111,
                            "end": 1115,
                            "matchedPaperCorpusId": "247230104"
                        },
                        {
                            "start": 1621,
                            "end": 1625,
                            "matchedPaperCorpusId": "248177918"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.9814453125
                }
            ],
            "relevance_judgement": 0.9814453125,
            "relevance_judgment_input_expanded": "# Title: From Efficient Multimodal Models to World Models: A Survey\n# Venue: arXiv.org\n# Authors: Xinji Mai, Zeng Tao, Junxiong Lin, Haoran Wang, Yang Chang, Yanlan Kang, Yan Wang, Wenqiang Zhang\n## Abstract\nMultimodal Large Models (MLMs) are becoming a significant research focus, combining powerful large language models with multimodal learning to perform complex tasks across different data modalities. This review explores the latest developments and challenges in MLMs, emphasizing their potential in achieving artificial general intelligence and as a pathway to world models. We provide an overview of key techniques such as Multimodal Chain of Thought (M-COT), Multimodal Instruction Tuning (M-IT), and Multimodal In-Context Learning (M-ICL). Additionally, we discuss both the fundamental and specific technologies of multimodal models, highlighting their applications, input/output modalities, and design characteristics. Despite significant advancements, the development of a unified multimodal model remains elusive. We discuss the integration of 3D generation and embodied intelligence to enhance world simulation capabilities and propose incorporating external rule systems for improved reasoning and decision-making. Finally, we outline future research directions to address these challenges and advance the field.\n## C. Knowledge Distillation\nKnowledge Distillation (KD) is a model compression technique that transfers knowledge from a complex model (called the teacher model) to a smaller model (called the student model).This allows the student model to maintain high computational efficiency while achieving the performance of the teacher model.Knowledge distillation was first proposed by Bucilu\u01ce et al., who trained compressed models with pseudodata classifiers to replicate the original classifier's outputs [14].KD can be divided into homomorphic KD and heteromorphic KD.\n\nHomomorphic KD means the student and teacher models have similar or identical structures.In this approach, the student model learns by mimicking the teacher model's outputs (e.g., logits, feature layer outputs).Common homomorphic KD methods include logit-level distillation, feature-level distillation, and module-level distillation.For instance, TinyViT [46] applies distillation during pre-training, storing logits from a large teacher model on hardware to achieve memory and computational efficiency when transferring knowledge to a smaller student Transformer.DeiT-Tiny [47] adopts patchlevel distillation, training a small student model to match the pre-trained teacher model's patch structure, then optimizing with decomposed manifold matching loss to reduce computational costs.Module-level methods like m2mKD [48] separate the teacher module from a pre-trained unified model, combining student modules with modular models, and using a shared meta-model for composition, enabling student modules to mimic teacher module behavior.Feature-level distillation methods like MiniViT [49] combine weights from consecutive Transformer blocks for cross-layer weight sharing, introducing transformations to enhance learning.\n\nHeteromorphic KD refers to student and teacher models with different structures.In this approach, the student model learns by mimicking the teacher model's outputs or intermediate features, despite different architectures.Heteromorphic KD enhances the student model's adaptability, enabling it to learn useful information from the teacher model.Heteromorphic KD includes soft label distillation, where the student model trains by mimicking the teacher model's soft label outputs.",
            "reference_string": "[270870796 | Mai et al. | 2024 | Citations: 5]"
        },
        {
            "title": "Distilling Wisdom: A Review on Optimizing Learning From Massive Language Models",
            "venue": "IEEE Access",
            "year": 2025,
            "reference_count": 205,
            "citation_count": 0,
            "influential_citation_count": 0,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://doi.org/10.1109/access.2025.3554586",
                "status": "GOLD",
                "license": "CCBY",
                "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.1109/ACCESS.2025.3554586?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1109/ACCESS.2025.3554586, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2326763710",
                    "name": "Dingzong Zhang"
                },
                {
                    "authorId": "2345374431",
                    "name": "Devi Listiyani"
                },
                {
                    "authorId": "2302811064",
                    "name": "Priyanka Singh"
                },
                {
                    "authorId": "2309099465",
                    "name": "Manoranjan Mohanty"
                }
            ],
            "abstract": "In the era of Large Language Models (LLMs), Knowledge Distillation (KD) enables the transfer of capabilities from proprietary LLMs to open-source models. This survey provides a detailed discussion of the basic principles, algorithms, and implementation methods of knowledge distillation. It explores KD\u2019s impact on LLMs, emphasizing its utility in model compression, performance enhancement, and self-improvement. Through the analysis of practical examples such as DistilBERT, TinyBERT, and MobileBERT, the paper demonstrates how knowledge distillation can markedly enhance the efficiency and applicability of large language models in real-world scenarios. The discussion encompasses the varied applications of KD across multiple domains, including industrial systems, embedded systems, Natural Language Processing (NLP), multi-modal processing, and vertical domains, such as medicine, law, science, finance, and materials science. This survey outlines current KD methodologies and future research directions, highlighting its role in advancing AI technologies and fostering innovation across different sectors.",
            "corpus_id": 277398866,
            "sentences": [
                {
                    "corpus_id": "277398866",
                    "title": "Distilling Wisdom: A Review on Optimizing Learning From Massive Language Models",
                    "text": "Knowledge distillation is a technique used to compress and transfer knowledge from a large, complex model(the teacher) to a smaller, simpler model(the student). This process involves the student model learning to mimic the behavior of the teacher model, thereby achieving similar performance with fewer parameters and reduced computational requirements [33], [34]. The primary objective of knowledge distillation is to retain the accuracy and capabilities of the large model while significantly reducing its size and inference time. \n\nThe core techniques of knowledge distillation revolve around three main approaches: soft targets, feature matching, and attention transfer. Instead of using hard labels for training, the student model is trained on the soft targets provided by the teacher model, which include the probability distribution of the output classes, helping the student model capture the teacher model's learned knowledge more effectively [33], [34]. The term ''soft target'' denotes the probability distribution from a teacher model that provides confidence scores for various answers, helping the student model learn more efficiently than just using the correct answer [35], [36]. Feature matching involves using the intermediate features (activations) of the teacher model to guide the learning process of the student model, allowing the student to learn richer and more nuanced features [34], [37]. Attention transfer involves transferring the attention maps of the teacher model to the student model, highlighting important areas in the input data that the teacher model focuses on, thus helping the student learn where to pay attention [37]. By guiding a smaller model to concentrate on the significant aspects of the data identified by a larger model, this technique contributes to improved accuracy in predictions [38], [39]. \n\nThe current research of KD focuses on several key areas, including cross-modal knowledge distillation, selfdistillation, online knowledge distillation, and distillation for robustness and generalization. Cross-modal KD extends distillation techniques to scenarios where the teacher and student models operate on different modalities, such as distilling knowledge from a vision model to a language model [40], [41].",
                    "score": 0.6886778172324168,
                    "section_title": "B. OVERVIEW OF KNOWLEDGE DISTILLATION",
                    "char_start_offset": 21468,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 160
                        },
                        {
                            "start": 161,
                            "end": 364
                        },
                        {
                            "start": 365,
                            "end": 532
                        },
                        {
                            "start": 535,
                            "end": 674
                        },
                        {
                            "start": 675,
                            "end": 964
                        },
                        {
                            "start": 965,
                            "end": 1196
                        },
                        {
                            "start": 1197,
                            "end": 1416
                        },
                        {
                            "start": 1417,
                            "end": 1661
                        },
                        {
                            "start": 1662,
                            "end": 1847
                        },
                        {
                            "start": 1850,
                            "end": 2053
                        },
                        {
                            "start": 2054,
                            "end": 2264
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 1191,
                            "end": 1195,
                            "matchedPaperCorpusId": "252846553"
                        },
                        {
                            "start": 1836,
                            "end": 1840,
                            "matchedPaperCorpusId": "222831725"
                        },
                        {
                            "start": 1842,
                            "end": 1846,
                            "matchedPaperCorpusId": "252995794"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.98095703125
                }
            ],
            "relevance_judgement": 0.98095703125,
            "relevance_judgment_input_expanded": "# Title: Distilling Wisdom: A Review on Optimizing Learning From Massive Language Models\n# Venue: IEEE Access\n# Authors: Dingzong Zhang, Devi Listiyani, Priyanka Singh, Manoranjan Mohanty\n## Abstract\nIn the era of Large Language Models (LLMs), Knowledge Distillation (KD) enables the transfer of capabilities from proprietary LLMs to open-source models. This survey provides a detailed discussion of the basic principles, algorithms, and implementation methods of knowledge distillation. It explores KD\u2019s impact on LLMs, emphasizing its utility in model compression, performance enhancement, and self-improvement. Through the analysis of practical examples such as DistilBERT, TinyBERT, and MobileBERT, the paper demonstrates how knowledge distillation can markedly enhance the efficiency and applicability of large language models in real-world scenarios. The discussion encompasses the varied applications of KD across multiple domains, including industrial systems, embedded systems, Natural Language Processing (NLP), multi-modal processing, and vertical domains, such as medicine, law, science, finance, and materials science. This survey outlines current KD methodologies and future research directions, highlighting its role in advancing AI technologies and fostering innovation across different sectors.\n## B. OVERVIEW OF KNOWLEDGE DISTILLATION\nKnowledge distillation is a technique used to compress and transfer knowledge from a large, complex model(the teacher) to a smaller, simpler model(the student). This process involves the student model learning to mimic the behavior of the teacher model, thereby achieving similar performance with fewer parameters and reduced computational requirements [33], [34]. The primary objective of knowledge distillation is to retain the accuracy and capabilities of the large model while significantly reducing its size and inference time. \n\nThe core techniques of knowledge distillation revolve around three main approaches: soft targets, feature matching, and attention transfer. Instead of using hard labels for training, the student model is trained on the soft targets provided by the teacher model, which include the probability distribution of the output classes, helping the student model capture the teacher model's learned knowledge more effectively [33], [34]. The term ''soft target'' denotes the probability distribution from a teacher model that provides confidence scores for various answers, helping the student model learn more efficiently than just using the correct answer [35], [36]. Feature matching involves using the intermediate features (activations) of the teacher model to guide the learning process of the student model, allowing the student to learn richer and more nuanced features [34], [37]. Attention transfer involves transferring the attention maps of the teacher model to the student model, highlighting important areas in the input data that the teacher model focuses on, thus helping the student learn where to pay attention [37]. By guiding a smaller model to concentrate on the significant aspects of the data identified by a larger model, this technique contributes to improved accuracy in predictions [38], [39]. \n\nThe current research of KD focuses on several key areas, including cross-modal knowledge distillation, selfdistillation, online knowledge distillation, and distillation for robustness and generalization. Cross-modal KD extends distillation techniques to scenarios where the teacher and student models operate on different modalities, such as distilling knowledge from a vision model to a language model [40], [41].",
            "reference_string": "[277398866 | Zhang et al. | 2025 | Citations: 0]"
        },
        {
            "title": "Function-Consistent Feature Distillation",
            "venue": "International Conference on Learning Representations",
            "year": 2023,
            "reference_count": 43,
            "citation_count": 19,
            "influential_citation_count": 5,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "http://arxiv.org/pdf/2304.11832",
                "status": "CLOSED",
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2304.11832, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2152509045",
                    "name": "Dongyang Liu"
                },
                {
                    "authorId": "1693589",
                    "name": "Meina Kan"
                },
                {
                    "authorId": "145455919",
                    "name": "S. Shan"
                },
                {
                    "authorId": "46772547",
                    "name": "Xilin Chen"
                }
            ],
            "abstract": "Feature distillation makes the student mimic the intermediate features of the teacher. Nearly all existing feature-distillation methods use L2 distance or its slight variants as the distance metric between teacher and student features. However, while L2 distance is isotropic w.r.t. all dimensions, the neural network's operation on different dimensions is usually anisotropic, i.e., perturbations with the same 2-norm but in different dimensions of intermediate features lead to changes in the final output with largely different magnitude. Considering this, we argue that the similarity between teacher and student features should not be measured merely based on their appearance (i.e., L2 distance), but should, more importantly, be measured by their difference in function, namely how later layers of the network will read, decode, and process them. Therefore, we propose Function-Consistent Feature Distillation (FCFD), which explicitly optimizes the functional similarity between teacher and student features. The core idea of FCFD is to make teacher and student features not only numerically similar, but more importantly produce similar outputs when fed to the later part of the same network. With FCFD, the student mimics the teacher more faithfully and learns more from the teacher. Extensive experiments on image classification and object detection demonstrate the superiority of FCFD to existing methods. Furthermore, we can combine FCFD with many existing methods to obtain even higher accuracy. Our codes are available at https://github.com/LiuDongyang6/FCFD.",
            "corpus_id": 258298441,
            "sentences": [
                {
                    "corpus_id": "258298441",
                    "title": "Function-Consistent Feature Distillation",
                    "text": "Deep neural networks (DNNs) have demonstrated great power on a variety of tasks. However, the high performance of DNN models is often accompanied by large storage and computational costs, which barricade their deployment on edge devices. A common solution to this problem is Knowledge Distillation (KD), whose central idea is to transfer the knowledge from a strong teacher model to a compact student model, in hopes that the additional guidance could raise the performance of the student (Gou et al., 2021;Wang & Yoon, 2021). According to the type of carrier for knowledge transfer, existing KD methods can be roughly divided into the following two categories: \n\n1.1 KNOWLEDGE DISTILLATION BASED ON FINAL OUTPUT Hinton et al. (2015) first clarifies the concept of knowledge distillation. In their method, softened probability distribution output by the teacher is used as the guidance to the student. Following works (Ding et al., 2019;Wen et al., 2019) step further to explore the trade-off between soft logits and hard task label. Zhao et al. (2022) propose to decouple target-class and non-target-class knowledge transfer, and Chen et al. (2022) make the student to reuse the teacher's classifier. Park et al. (2019) claims that the relationship among representations of different samples implies important information, and they used this relationship as the knowledge carrier. Recently, some works introduce contrastive learning to knowledge distillation. Specifically, Tian et al. (2020) propose to identify if a pair of teacher and student representations are congruent (same input provided to teacher and student) or not, and SSKD (Xu et al., 2020) introduce an additional self-supervision task to the training process. Whereas all the aforementioned methods employ a fixed teacher model during distillation, Zhang et al. (2018b) propose online knowledge distillation, where both the large and the small models are randomly initialized and learn mutually from each other during training.",
                    "score": 0.6652891699309288,
                    "section_title": "INTRODUCTION",
                    "char_start_offset": 15,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 80
                        },
                        {
                            "start": 81,
                            "end": 237
                        },
                        {
                            "start": 238,
                            "end": 526
                        },
                        {
                            "start": 527,
                            "end": 661
                        },
                        {
                            "start": 664,
                            "end": 788
                        },
                        {
                            "start": 789,
                            "end": 901
                        },
                        {
                            "start": 902,
                            "end": 1033
                        },
                        {
                            "start": 1034,
                            "end": 1201
                        },
                        {
                            "start": 1202,
                            "end": 1381
                        },
                        {
                            "start": 1382,
                            "end": 1460
                        },
                        {
                            "start": 1461,
                            "end": 1727
                        },
                        {
                            "start": 1728,
                            "end": 1995
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 489,
                            "end": 507,
                            "matchedPaperCorpusId": "219559263"
                        },
                        {
                            "start": 1202,
                            "end": 1220,
                            "matchedPaperCorpusId": "131765296"
                        },
                        {
                            "start": 1817,
                            "end": 1837,
                            "matchedPaperCorpusId": "26071966"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.98095703125
                }
            ],
            "relevance_judgement": 0.98095703125,
            "relevance_judgment_input_expanded": "# Title: Function-Consistent Feature Distillation\n# Venue: International Conference on Learning Representations\n# Authors: Dongyang Liu, Meina Kan, S. Shan, Xilin Chen\n## Abstract\nFeature distillation makes the student mimic the intermediate features of the teacher. Nearly all existing feature-distillation methods use L2 distance or its slight variants as the distance metric between teacher and student features. However, while L2 distance is isotropic w.r.t. all dimensions, the neural network's operation on different dimensions is usually anisotropic, i.e., perturbations with the same 2-norm but in different dimensions of intermediate features lead to changes in the final output with largely different magnitude. Considering this, we argue that the similarity between teacher and student features should not be measured merely based on their appearance (i.e., L2 distance), but should, more importantly, be measured by their difference in function, namely how later layers of the network will read, decode, and process them. Therefore, we propose Function-Consistent Feature Distillation (FCFD), which explicitly optimizes the functional similarity between teacher and student features. The core idea of FCFD is to make teacher and student features not only numerically similar, but more importantly produce similar outputs when fed to the later part of the same network. With FCFD, the student mimics the teacher more faithfully and learns more from the teacher. Extensive experiments on image classification and object detection demonstrate the superiority of FCFD to existing methods. Furthermore, we can combine FCFD with many existing methods to obtain even higher accuracy. Our codes are available at https://github.com/LiuDongyang6/FCFD.\n## INTRODUCTION\nDeep neural networks (DNNs) have demonstrated great power on a variety of tasks. However, the high performance of DNN models is often accompanied by large storage and computational costs, which barricade their deployment on edge devices. A common solution to this problem is Knowledge Distillation (KD), whose central idea is to transfer the knowledge from a strong teacher model to a compact student model, in hopes that the additional guidance could raise the performance of the student (Gou et al., 2021;Wang & Yoon, 2021). According to the type of carrier for knowledge transfer, existing KD methods can be roughly divided into the following two categories: \n\n1.1 KNOWLEDGE DISTILLATION BASED ON FINAL OUTPUT Hinton et al. (2015) first clarifies the concept of knowledge distillation. In their method, softened probability distribution output by the teacher is used as the guidance to the student. Following works (Ding et al., 2019;Wen et al., 2019) step further to explore the trade-off between soft logits and hard task label. Zhao et al. (2022) propose to decouple target-class and non-target-class knowledge transfer, and Chen et al. (2022) make the student to reuse the teacher's classifier. Park et al. (2019) claims that the relationship among representations of different samples implies important information, and they used this relationship as the knowledge carrier. Recently, some works introduce contrastive learning to knowledge distillation. Specifically, Tian et al. (2020) propose to identify if a pair of teacher and student representations are congruent (same input provided to teacher and student) or not, and SSKD (Xu et al., 2020) introduce an additional self-supervision task to the training process. Whereas all the aforementioned methods employ a fixed teacher model during distillation, Zhang et al. (2018b) propose online knowledge distillation, where both the large and the small models are randomly initialized and learn mutually from each other during training.",
            "reference_string": "[258298441 | Liu et al. | 2023 | Citations: 19]"
        },
        {
            "title": "Noisy Self-Knowledge Distillation for Text Summarization",
            "venue": "North American Chapter of the Association for Computational Linguistics",
            "year": 2020,
            "reference_count": 64,
            "citation_count": 44,
            "influential_citation_count": 4,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://aclanthology.org/2021.naacl-main.56.pdf",
                "status": "HYBRID",
                "license": "CCBY",
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2009.07032, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "39798499",
                    "name": "Yang Liu"
                },
                {
                    "authorId": "2072819533",
                    "name": "S. Shen"
                },
                {
                    "authorId": "1747893",
                    "name": "Mirella Lapata"
                }
            ],
            "abstract": "In this paper we apply self-knowledge distillation to text summarization which we argue can alleviate problems with maximum-likelihood training on single reference and noisy datasets. Instead of relying on one-hot annotation labels, our student summarization model is trained with guidance from a teacher which generates smoothed labels to help regularize training. Furthermore, to better model uncertainty during training, we introduce multiple noise signals for both teacher and student models. We demonstrate experimentally on three benchmarks that our framework boosts the performance of both pretrained and non-pretrained summarizers achieving state-of-the-art results.",
            "corpus_id": 221703021,
            "sentences": [
                {
                    "corpus_id": "221703021",
                    "title": "Noisy Self-Knowledge Distillation for Text Summarization",
                    "text": "Knowledge Distillation refers to a class of methods for training a new smaller student network by learning from a teacher network (in addition to learning from the training data). It is generally assumed that the teacher has been previously trained, and the parameters for the student are estimated by matching the student's predictions to the teacher.\n\nLet T and S denote teacher and student models, respectively. Let f T and f S be functions of the teacher and student. The models are typically neural networks and function f can be in principle defined using the output of any network layer (e.g., a hidden or softmax layer). Knowledge distillation methods are commonly expressed as minimizing 694 an objective function over training set X :\n\nwhere l() is a loss function that penalizes the difference between the teacher and the student. Specific instantiations of this general framework include minimizing the teacher/student difference based on output logits, intermediate hidden representations, attention maps, and derivatives of the loss to the input (Ba and Caruana, 2014;Romero et al., 2014;Zagoruyko and Komodakis, 2017;. Other work integrates an ensemble of teachers in order to improve the student (Urban et al., 2016), trains a succession of students (Furlanello et al., 2018), introduces a \"teacher assistant\" for better knowledge transfer (Mirzadeh et al., 2019), and regularizes multi-task agents (Parisotto et al., 2015;Teh et al., 2017) in reinforcement learning. Compared to direct training, knowledge distillation provides a more stable training process which leads to better performing student models (Hinton et al., 2015;Phuong and Lampert, 2019). Recent work (Furlanello et al., 2018;Hahn and Choi, 2019) also sheds light on leveraging knowledge distillation for training a highperforming student model with the same size as the teacher (see the discussion in the next section).\n\nKnowledge distillation has been also shown to improve results for various NLP tasks.  use it to transfer knowledge from BERT to smaller models, helping them approach or exceed the quality of much larger pretrained neural networks. Aside from distilling large models into smaller ones (Kim and Rush, 2016;",
                    "score": 0.748707938567696,
                    "section_title": "Knowledge Distillation",
                    "char_start_offset": 6953,
                    "sentence_offsets": [],
                    "ref_mentions": [
                        {
                            "start": 1060,
                            "end": 1082,
                            "matchedPaperCorpusId": "11536917"
                        },
                        {
                            "start": 1102,
                            "end": 1132,
                            "matchedPaperCorpusId": "829159"
                        },
                        {
                            "start": 1266,
                            "end": 1291,
                            "matchedPaperCorpusId": "4110009"
                        },
                        {
                            "start": 1439,
                            "end": 1456,
                            "matchedPaperCorpusId": "31009408"
                        },
                        {
                            "start": 1645,
                            "end": 1670,
                            "matchedPaperCorpusId": "174800711"
                        },
                        {
                            "start": 1684,
                            "end": 1709,
                            "matchedPaperCorpusId": "4110009"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.98046875
                }
            ],
            "relevance_judgement": 0.98046875,
            "relevance_judgment_input_expanded": "# Title: Noisy Self-Knowledge Distillation for Text Summarization\n# Venue: North American Chapter of the Association for Computational Linguistics\n# Authors: Yang Liu, S. Shen, Mirella Lapata\n## Abstract\nIn this paper we apply self-knowledge distillation to text summarization which we argue can alleviate problems with maximum-likelihood training on single reference and noisy datasets. Instead of relying on one-hot annotation labels, our student summarization model is trained with guidance from a teacher which generates smoothed labels to help regularize training. Furthermore, to better model uncertainty during training, we introduce multiple noise signals for both teacher and student models. We demonstrate experimentally on three benchmarks that our framework boosts the performance of both pretrained and non-pretrained summarizers achieving state-of-the-art results.\n## Knowledge Distillation\nKnowledge Distillation refers to a class of methods for training a new smaller student network by learning from a teacher network (in addition to learning from the training data). It is generally assumed that the teacher has been previously trained, and the parameters for the student are estimated by matching the student's predictions to the teacher.\n\nLet T and S denote teacher and student models, respectively. Let f T and f S be functions of the teacher and student. The models are typically neural networks and function f can be in principle defined using the output of any network layer (e.g., a hidden or softmax layer). Knowledge distillation methods are commonly expressed as minimizing 694 an objective function over training set X :\n\nwhere l() is a loss function that penalizes the difference between the teacher and the student. Specific instantiations of this general framework include minimizing the teacher/student difference based on output logits, intermediate hidden representations, attention maps, and derivatives of the loss to the input (Ba and Caruana, 2014;Romero et al., 2014;Zagoruyko and Komodakis, 2017;. Other work integrates an ensemble of teachers in order to improve the student (Urban et al., 2016), trains a succession of students (Furlanello et al., 2018), introduces a \"teacher assistant\" for better knowledge transfer (Mirzadeh et al., 2019), and regularizes multi-task agents (Parisotto et al., 2015;Teh et al., 2017) in reinforcement learning. Compared to direct training, knowledge distillation provides a more stable training process which leads to better performing student models (Hinton et al., 2015;Phuong and Lampert, 2019). Recent work (Furlanello et al., 2018;Hahn and Choi, 2019) also sheds light on leveraging knowledge distillation for training a highperforming student model with the same size as the teacher (see the discussion in the next section).\n\nKnowledge distillation has been also shown to improve results for various NLP tasks.  use it to transfer knowledge from BERT to smaller models, helping them approach or exceed the quality of much larger pretrained neural networks. Aside from distilling large models into smaller ones (Kim and Rush, 2016;",
            "reference_string": "[221703021 | Liu et al. | 2020 | Citations: 44]"
        },
        {
            "title": "Online Distilling from Checkpoints for Neural Machine Translation",
            "venue": "North American Chapter of the Association for Computational Linguistics",
            "year": 2019,
            "reference_count": 33,
            "citation_count": 32,
            "influential_citation_count": 3,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://aclanthology.org/N19-1192, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "134085586",
                    "name": "Hao-Ran Wei"
                },
                {
                    "authorId": "2046010",
                    "name": "Shujian Huang"
                },
                {
                    "authorId": "2115256564",
                    "name": "Ran Wang"
                },
                {
                    "authorId": "3035069",
                    "name": "Xinyu Dai"
                },
                {
                    "authorId": "1838162",
                    "name": "Jiajun Chen"
                }
            ],
            "abstract": "Current predominant neural machine translation (NMT) models often have a deep structure with large amounts of parameters, making these models hard to train and easily suffering from over-fitting. A common practice is to utilize a validation set to evaluate the training process and select the best checkpoint. Average and ensemble techniques on checkpoints can lead to further performance improvement. However, as these methods do not affect the training process, the system performance is restricted to the checkpoints generated in original training procedure. In contrast, we propose an online knowledge distillation method. Our method on-the-fly generates a teacher model from checkpoints, guiding the training process to obtain better performance. Experiments on several datasets and language pairs show steady improvement over a strong self-attention-based baseline system. We also provide analysis on data-limited setting against over-fitting. Furthermore, our method leads to an improvement in a machine reading experiment as well.",
            "corpus_id": 174800781,
            "sentences": [
                {
                    "corpus_id": "174800781",
                    "title": "Online Distilling from Checkpoints for Neural Machine Translation",
                    "text": "Knowledge distillation is a class of methods which transfers knowledge from a pre-trained teacher model T , to a student model S. The teacher model can be a model with large capacity (Bucila et al., 2006) or an ensemble of several models (Hinton et al., 2015). In knowledge distillation, the student model learns to match the predictions of the teacher model. Concretely, assuming that we learn a classification model (parameterized by \u03b8) on a set of training samples in the form of (x, y) with |V| classes. Instead of minimizing the cross-entropy loss between one-hot label y and model's output probability p(y|x; \u03b8), knowledge distillation uses the teacher model's distribution q(\u2022|x) as \"soft targets\" and optimizes the loss: \n\nwhere \u03b8 T parameterizes the teacher model and p(\u2022|x) is the distribution of the student model. Kim and Rush (2016) proposed that, as the loss of NMT model (Equation 4) can be factored into minimizing cross-entropy loss between the target words and word-level probabilities of the NMT model for every position at target side, knowledge distillation on multi-class classification can be naturally applied. They defined word-level knowledge distillation (W-KD) on a sentence as: \n\n\u2022 log p(y j = k|y <j , H(x); \u03b8), where V is the target vocabulary. They further proposed sequence-level knowledge distillation (S-KD), which optimizes the student model by matching the predictions of the teacher model in the probability distribution over the space of all possible target sequences: \n\nwhere \u03c4 is the space of target side sentences. As summing over exponential numbers of samples here is intractable, they proposed to train student model on samples generated by teacher model as an approximation. \n\n3 Online Distillation from Checkpoints",
                    "score": 0.7030594447815925,
                    "section_title": "Knowledge Distillation in Neural Machine Translation",
                    "char_start_offset": 5201,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 260
                        },
                        {
                            "start": 261,
                            "end": 359
                        },
                        {
                            "start": 360,
                            "end": 507
                        },
                        {
                            "start": 508,
                            "end": 728
                        },
                        {
                            "start": 731,
                            "end": 825
                        },
                        {
                            "start": 826,
                            "end": 1134
                        },
                        {
                            "start": 1135,
                            "end": 1206
                        },
                        {
                            "start": 1209,
                            "end": 1275
                        },
                        {
                            "start": 1276,
                            "end": 1507
                        },
                        {
                            "start": 1510,
                            "end": 1556
                        },
                        {
                            "start": 1557,
                            "end": 1720
                        },
                        {
                            "start": 1723,
                            "end": 1761
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 183,
                            "end": 204,
                            "matchedPaperCorpusId": "11253972"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.98046875
                }
            ],
            "relevance_judgement": 0.98046875,
            "relevance_judgment_input_expanded": "# Title: Online Distilling from Checkpoints for Neural Machine Translation\n# Venue: North American Chapter of the Association for Computational Linguistics\n# Authors: Hao-Ran Wei, Shujian Huang, Ran Wang, Xinyu Dai, Jiajun Chen\n## Abstract\nCurrent predominant neural machine translation (NMT) models often have a deep structure with large amounts of parameters, making these models hard to train and easily suffering from over-fitting. A common practice is to utilize a validation set to evaluate the training process and select the best checkpoint. Average and ensemble techniques on checkpoints can lead to further performance improvement. However, as these methods do not affect the training process, the system performance is restricted to the checkpoints generated in original training procedure. In contrast, we propose an online knowledge distillation method. Our method on-the-fly generates a teacher model from checkpoints, guiding the training process to obtain better performance. Experiments on several datasets and language pairs show steady improvement over a strong self-attention-based baseline system. We also provide analysis on data-limited setting against over-fitting. Furthermore, our method leads to an improvement in a machine reading experiment as well.\n## Knowledge Distillation in Neural Machine Translation\nKnowledge distillation is a class of methods which transfers knowledge from a pre-trained teacher model T , to a student model S. The teacher model can be a model with large capacity (Bucila et al., 2006) or an ensemble of several models (Hinton et al., 2015). In knowledge distillation, the student model learns to match the predictions of the teacher model. Concretely, assuming that we learn a classification model (parameterized by \u03b8) on a set of training samples in the form of (x, y) with |V| classes. Instead of minimizing the cross-entropy loss between one-hot label y and model's output probability p(y|x; \u03b8), knowledge distillation uses the teacher model's distribution q(\u2022|x) as \"soft targets\" and optimizes the loss: \n\nwhere \u03b8 T parameterizes the teacher model and p(\u2022|x) is the distribution of the student model. Kim and Rush (2016) proposed that, as the loss of NMT model (Equation 4) can be factored into minimizing cross-entropy loss between the target words and word-level probabilities of the NMT model for every position at target side, knowledge distillation on multi-class classification can be naturally applied. They defined word-level knowledge distillation (W-KD) on a sentence as: \n\n\u2022 log p(y j = k|y <j , H(x); \u03b8), where V is the target vocabulary. They further proposed sequence-level knowledge distillation (S-KD), which optimizes the student model by matching the predictions of the teacher model in the probability distribution over the space of all possible target sequences: \n\nwhere \u03c4 is the space of target side sentences. As summing over exponential numbers of samples here is intractable, they proposed to train student model on samples generated by teacher model as an approximation. \n\n3 Online Distillation from Checkpoints",
            "reference_string": "[174800781 | Wei et al. | 2019 | Citations: 32]"
        },
        {
            "title": "Multistage feature fusion knowledge distillation",
            "venue": "Scientific Reports",
            "year": 2024,
            "reference_count": 40,
            "citation_count": 1,
            "influential_citation_count": 0,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://www.nature.com/articles/s41598-024-64041-4.pdf",
                "status": "GOLD",
                "license": "CCBY",
                "disclaimer": "Notice: Paper or abstract available at https://pmc.ncbi.nlm.nih.gov/articles/PMC11166915, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2307186661",
                    "name": "Gang Li"
                },
                {
                    "authorId": "2307436738",
                    "name": "Kun Wang"
                },
                {
                    "authorId": "2305765306",
                    "name": "Pengfei Lv"
                },
                {
                    "authorId": "2305752302",
                    "name": "Pan He"
                },
                {
                    "authorId": "2287501699",
                    "name": "Zheng Zhou"
                },
                {
                    "authorId": "2817613",
                    "name": "Chuanyun Xu"
                }
            ],
            "abstract": "Generally, the recognition performance of lightweight models is often lower than that of large models. Knowledge distillation, by teaching a student model using a teacher model, can further enhance the recognition accuracy of lightweight models. In this paper, we approach knowledge distillation from the perspective of intermediate feature-level knowledge distillation. We combine a cross-stage feature fusion symmetric framework, an attention mechanism to enhance the fused features, and a contrastive loss function for teacher and student models at the same stage to comprehensively implement a multistage feature fusion knowledge distillation method. This approach addresses the problem of significant differences in the intermediate feature distributions between teacher and student models, making it difficult to effectively learn implicit knowledge and thus improving the recognition accuracy of the student model. Compared to existing knowledge distillation methods, our method performs at a superior level. On the CIFAR100 dataset, it boosts the recognition accuracy of ResNet20 from 69.06% to 71.34%, and on the TinyImagenet dataset, it increases the recognition accuracy of ResNet18 from 66.54% to 68.03%, demonstrating the effectiveness and generalizability of our approach. Furthermore, there is room for further optimization of the overall distillation structure and feature extraction methods in this approach, which requires further research and exploration.",
            "corpus_id": 270389751,
            "sentences": [
                {
                    "corpus_id": "270389751",
                    "title": "Multistage feature fusion knowledge distillation",
                    "text": "Knowledge distillation (KD), as initially proposed by Hinton et al. 10 , aims to supervise the training convergence of a student network, a smaller model, with a teacher network, a larger model.This method controls the extent of knowledge transfer between two networks using a temperature parameter, T, to control the transfer of soft-label dark knowledge.This approach has given rise to variations, including intermediate feature layer distillation and multistage soft label distillation.\n\nIn the context of intermediate feature layer distillation, the challenge of inconsistent multistage feature knowledge distribution is a critical issue.FitNet 11 employs squared distance constraints to measure the similarity of intermediate layer features between teacher and student networks.AT 15 uses multi-layer attention maps to extract features between the teacher network and the student network, and builds a knowledge transfer mechanism between the two.CC 16 proposed a correlation congruence method to reduce the correlation consistency distribution between teachers and students across multiple sample instances, and improve the distribution consistency between student models and teacher models in the classification output of multiple instances.AB 17 proposed a method for knowledge transfer by extracting the activation boundaries formed by hidden neurons, which enables students to learn the separation boundaries between different activation regions formed by each neuron in the teacher, thereby reducing the differences between the student network and the teacher network.FT 18 proposed two convolutional modules, the reader and the translator, which are used to extract the feature information of teachers and the translator to extract the feature information of students.Through distillation training, the differences between the two modules are reduced, achieving the imitation and learning of the teacher network by www.nature.com/scientificreports/ the student network.NST 19 proposed a new KT loss function to minimize the maximum average difference in neuron feature distribution between the teacher model and the student model, significantly improving the performance of the student model.CRD 20 is a knowledge distillation method based on contrastive learning, which preserves mutual information between teachers and students by optimizing the distillation loss function.OFD 21 uses a novel distance function and edge residual function to distill essential information between teacher and student networks.ReviewKD 22 utilizes a multilevel composite knowledge approach to transfer dark knowledge at the feature level, achieving state-of-the-art performance.",
                    "score": 0.6285981404521053,
                    "section_title": "Knowledge distillation",
                    "char_start_offset": 2815,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 194
                        },
                        {
                            "start": 194,
                            "end": 356
                        },
                        {
                            "start": 356,
                            "end": 489
                        },
                        {
                            "start": 491,
                            "end": 642
                        },
                        {
                            "start": 642,
                            "end": 783
                        },
                        {
                            "start": 783,
                            "end": 952
                        },
                        {
                            "start": 952,
                            "end": 1248
                        },
                        {
                            "start": 1248,
                            "end": 1579
                        },
                        {
                            "start": 1579,
                            "end": 1780
                        },
                        {
                            "start": 1780,
                            "end": 1981
                        },
                        {
                            "start": 1981,
                            "end": 2204
                        },
                        {
                            "start": 2204,
                            "end": 2387
                        },
                        {
                            "start": 2387,
                            "end": 2522
                        },
                        {
                            "start": 2522,
                            "end": 2673
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 68,
                            "end": 70,
                            "matchedPaperCorpusId": "7200347"
                        },
                        {
                            "start": 955,
                            "end": 957,
                            "matchedPaperCorpusId": "102483463"
                        },
                        {
                            "start": 1251,
                            "end": 1253,
                            "matchedPaperCorpusId": "53213211"
                        },
                        {
                            "start": 1582,
                            "end": 1584,
                            "matchedPaperCorpusId": "3608236"
                        },
                        {
                            "start": 2208,
                            "end": 2210,
                            "matchedPaperCorpusId": "204838340"
                        },
                        {
                            "start": 2391,
                            "end": 2393,
                            "matchedPaperCorpusId": "102483181"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.98046875
                }
            ],
            "relevance_judgement": 0.98046875,
            "relevance_judgment_input_expanded": "# Title: Multistage feature fusion knowledge distillation\n# Venue: Scientific Reports\n# Authors: Gang Li, Kun Wang, Pengfei Lv, Pan He, Zheng Zhou, Chuanyun Xu\n## Abstract\nGenerally, the recognition performance of lightweight models is often lower than that of large models. Knowledge distillation, by teaching a student model using a teacher model, can further enhance the recognition accuracy of lightweight models. In this paper, we approach knowledge distillation from the perspective of intermediate feature-level knowledge distillation. We combine a cross-stage feature fusion symmetric framework, an attention mechanism to enhance the fused features, and a contrastive loss function for teacher and student models at the same stage to comprehensively implement a multistage feature fusion knowledge distillation method. This approach addresses the problem of significant differences in the intermediate feature distributions between teacher and student models, making it difficult to effectively learn implicit knowledge and thus improving the recognition accuracy of the student model. Compared to existing knowledge distillation methods, our method performs at a superior level. On the CIFAR100 dataset, it boosts the recognition accuracy of ResNet20 from 69.06% to 71.34%, and on the TinyImagenet dataset, it increases the recognition accuracy of ResNet18 from 66.54% to 68.03%, demonstrating the effectiveness and generalizability of our approach. Furthermore, there is room for further optimization of the overall distillation structure and feature extraction methods in this approach, which requires further research and exploration.\n## Knowledge distillation\nKnowledge distillation (KD), as initially proposed by Hinton et al. 10 , aims to supervise the training convergence of a student network, a smaller model, with a teacher network, a larger model.This method controls the extent of knowledge transfer between two networks using a temperature parameter, T, to control the transfer of soft-label dark knowledge.This approach has given rise to variations, including intermediate feature layer distillation and multistage soft label distillation.\n\nIn the context of intermediate feature layer distillation, the challenge of inconsistent multistage feature knowledge distribution is a critical issue.FitNet 11 employs squared distance constraints to measure the similarity of intermediate layer features between teacher and student networks.AT 15 uses multi-layer attention maps to extract features between the teacher network and the student network, and builds a knowledge transfer mechanism between the two.CC 16 proposed a correlation congruence method to reduce the correlation consistency distribution between teachers and students across multiple sample instances, and improve the distribution consistency between student models and teacher models in the classification output of multiple instances.AB 17 proposed a method for knowledge transfer by extracting the activation boundaries formed by hidden neurons, which enables students to learn the separation boundaries between different activation regions formed by each neuron in the teacher, thereby reducing the differences between the student network and the teacher network.FT 18 proposed two convolutional modules, the reader and the translator, which are used to extract the feature information of teachers and the translator to extract the feature information of students.Through distillation training, the differences between the two modules are reduced, achieving the imitation and learning of the teacher network by www.nature.com/scientificreports/ the student network.NST 19 proposed a new KT loss function to minimize the maximum average difference in neuron feature distribution between the teacher model and the student model, significantly improving the performance of the student model.CRD 20 is a knowledge distillation method based on contrastive learning, which preserves mutual information between teachers and students by optimizing the distillation loss function.OFD 21 uses a novel distance function and edge residual function to distill essential information between teacher and student networks.ReviewKD 22 utilizes a multilevel composite knowledge approach to transfer dark knowledge at the feature level, achieving state-of-the-art performance.",
            "reference_string": "[270389751 | Li et al. | 2024 | Citations: 1]"
        },
        {
            "title": "Synthetic data generation method for data-free knowledge distillation in regression neural networks",
            "venue": "Expert systems with applications",
            "year": 2023,
            "reference_count": 51,
            "citation_count": 7,
            "influential_citation_count": 0,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://arxiv.org/pdf/2301.04338",
                "status": "GREEN",
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2301.04338, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2163011403",
                    "name": "Tianxun Zhou"
                },
                {
                    "authorId": "4287306",
                    "name": "K. Chiam"
                }
            ],
            "abstract": null,
            "corpus_id": 255595916,
            "sentences": [
                {
                    "corpus_id": "255595916",
                    "title": "Synthetic data generation method for data-free knowledge distillation in regression neural networks",
                    "text": "As neural networks become increasingly large in number of parameters, the deployment of such models faces a difficult challenge for applications such as mobile devices and embedded systems due to limitations in computational resources and memory [Cheng et al., 2018, Deng et al., 2020]. To address such problems, model compression through knowledge distillation has become an active area of research in recent years [Liu et al., 2022, Wang et al., 2022]. Knowledge distillation is the technique where knowledge learned by a larger teacher model is transferred to a smaller student model [Gou et al., 2021, Hinton et al., 2015, Wang and Yoon, 2022]. The main idea is that the student model mimics the teacher model to achieve a similar or even a superior performance. \n\nVarious methods of knowledge distillation define and focus on different forms of knowledge. Following the nomenclature in [Gou et al., 2021], these can be largely grouped as response-based knowledge, feature-based knowledge, and relation-based knowledge. For response-based knowledge, outputs of the teacher model are used to supervise the training of the student model. For example, [Hinton et al., 2015] uses soft targets from the logits output of the teacher model to train the student. For featurebased knowledge, outputs of intermediate layers, or feature maps learned by the teacher model can be to supervise the training of the student model. For example, [Romero et al., 2014] trains the student model to match the feature activations of the teacher model. For relationship-based knowledge, the relationships between different layers or data samples are used. For example, [Yim et al., 2017] uses the inner products between features from two layers to represent the relationship between different layers, while [Chen et al., 2021] trains the student model to learn to preserve the similarity of samples' feature embeddings in the intermediate layers of the teacher models.",
                    "score": 0.6294524537223987,
                    "section_title": "Knowledge distillation",
                    "char_start_offset": 4099,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 286
                        },
                        {
                            "start": 287,
                            "end": 454
                        },
                        {
                            "start": 455,
                            "end": 648
                        },
                        {
                            "start": 649,
                            "end": 766
                        },
                        {
                            "start": 769,
                            "end": 860
                        },
                        {
                            "start": 861,
                            "end": 1023
                        },
                        {
                            "start": 1024,
                            "end": 1139
                        },
                        {
                            "start": 1140,
                            "end": 1258
                        },
                        {
                            "start": 1259,
                            "end": 1418
                        },
                        {
                            "start": 1419,
                            "end": 1533
                        },
                        {
                            "start": 1534,
                            "end": 1636
                        },
                        {
                            "start": 1637,
                            "end": 1949
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 265,
                            "end": 285,
                            "matchedPaperCorpusId": "215799572"
                        },
                        {
                            "start": 416,
                            "end": 433,
                            "matchedPaperCorpusId": "227228186"
                        },
                        {
                            "start": 587,
                            "end": 604,
                            "matchedPaperCorpusId": "219559263"
                        },
                        {
                            "start": 625,
                            "end": 647,
                            "matchedPaperCorpusId": "215745611"
                        },
                        {
                            "start": 891,
                            "end": 909,
                            "matchedPaperCorpusId": "219559263"
                        },
                        {
                            "start": 1650,
                            "end": 1668,
                            "matchedPaperCorpusId": "206596723"
                        },
                        {
                            "start": 1788,
                            "end": 1807,
                            "matchedPaperCorpusId": "56356408"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.97998046875
                }
            ],
            "relevance_judgement": 0.97998046875,
            "relevance_judgment_input_expanded": "# Title: Synthetic data generation method for data-free knowledge distillation in regression neural networks\n# Venue: Expert systems with applications\n# Authors: Tianxun Zhou, K. Chiam\n## Abstract\nNone\n## Knowledge distillation\nAs neural networks become increasingly large in number of parameters, the deployment of such models faces a difficult challenge for applications such as mobile devices and embedded systems due to limitations in computational resources and memory [Cheng et al., 2018, Deng et al., 2020]. To address such problems, model compression through knowledge distillation has become an active area of research in recent years [Liu et al., 2022, Wang et al., 2022]. Knowledge distillation is the technique where knowledge learned by a larger teacher model is transferred to a smaller student model [Gou et al., 2021, Hinton et al., 2015, Wang and Yoon, 2022]. The main idea is that the student model mimics the teacher model to achieve a similar or even a superior performance. \n\nVarious methods of knowledge distillation define and focus on different forms of knowledge. Following the nomenclature in [Gou et al., 2021], these can be largely grouped as response-based knowledge, feature-based knowledge, and relation-based knowledge. For response-based knowledge, outputs of the teacher model are used to supervise the training of the student model. For example, [Hinton et al., 2015] uses soft targets from the logits output of the teacher model to train the student. For featurebased knowledge, outputs of intermediate layers, or feature maps learned by the teacher model can be to supervise the training of the student model. For example, [Romero et al., 2014] trains the student model to match the feature activations of the teacher model. For relationship-based knowledge, the relationships between different layers or data samples are used. For example, [Yim et al., 2017] uses the inner products between features from two layers to represent the relationship between different layers, while [Chen et al., 2021] trains the student model to learn to preserve the similarity of samples' feature embeddings in the intermediate layers of the teacher models.",
            "reference_string": "[255595916 | Zhou et al. | 2023 | Citations: 7]"
        },
        {
            "title": "Improve Cross-Architecture Generalization on Dataset Distillation",
            "venue": "arXiv.org",
            "year": 2024,
            "reference_count": 30,
            "citation_count": 4,
            "influential_citation_count": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2402.13007, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2284824964",
                    "name": "Binglin Zhou"
                },
                {
                    "authorId": "2284822738",
                    "name": "Linhao Zhong"
                },
                {
                    "authorId": "2284825420",
                    "name": "Wentao Chen"
                }
            ],
            "abstract": "Dataset distillation, a pragmatic approach in machine learning, aims to create a smaller synthetic dataset from a larger existing dataset. However, existing distillation methods primarily adopt a model-based paradigm, where the synthetic dataset inherits model-specific biases, limiting its generalizability to alternative models. In response to this constraint, we propose a novel methodology termed\"model pool\". This approach involves selecting models from a diverse model pool based on a specific probability distribution during the data distillation process. Additionally, we integrate our model pool with the established knowledge distillation approach and apply knowledge distillation to the test process of the distilled dataset. Our experimental results validate the effectiveness of the model pool approach across a range of existing models while testing, demonstrating superior performance compared to existing methodologies.",
            "corpus_id": 267759723,
            "sentences": [
                {
                    "corpus_id": "267759723",
                    "title": "Improve Cross-Architecture Generalization on Dataset Distillation",
                    "text": "Knowledge distillation is a method to transfer knowledge from a large model to a small model. It's first proposed in this paper [12]. The large model is called the teacher model and the small model is called the student model. The student model is trained to mimic the output of the teacher model. The student model is usually a shallow neural network with fewer parameters and faster inference speed. The teacher model is usually a deep neural network with more parameters and slower inference speed. The student model is trained on the same dataset as the teacher model to minimize the difference between the output of the student model and the output of the teacher model (i.e. maintain the knowledge as much as possible). \n\nAfter knowledge distillation was proposed, a lot of work has been put into exploring further applications in diverse fields, such as speech recognition, image recognition, and natural language processing. It proved that knowledge distillation is a great success in these various fields [11]. \n\nHowever, there are some arguments against knowledge distillation. A paper [22] said that, while knowledge distillation improves the generalization of student models, there often remains a surprisingly large discrepancy between the predictive distributions of the teacher and the student, even in cases when the student can perfectly match the teacher.",
                    "score": 0.8812893508811146,
                    "section_title": "Knowledge Distillation",
                    "char_start_offset": 6172,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 93
                        },
                        {
                            "start": 94,
                            "end": 133
                        },
                        {
                            "start": 134,
                            "end": 226
                        },
                        {
                            "start": 227,
                            "end": 297
                        },
                        {
                            "start": 298,
                            "end": 401
                        },
                        {
                            "start": 402,
                            "end": 501
                        },
                        {
                            "start": 502,
                            "end": 680
                        },
                        {
                            "start": 681,
                            "end": 725
                        },
                        {
                            "start": 728,
                            "end": 932
                        },
                        {
                            "start": 933,
                            "end": 1019
                        },
                        {
                            "start": 1022,
                            "end": 1087
                        },
                        {
                            "start": 1088,
                            "end": 1373
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 1014,
                            "end": 1018,
                            "matchedPaperCorpusId": "219559263"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.9794921875
                }
            ],
            "relevance_judgement": 0.9794921875,
            "relevance_judgment_input_expanded": "# Title: Improve Cross-Architecture Generalization on Dataset Distillation\n# Venue: arXiv.org\n# Authors: Binglin Zhou, Linhao Zhong, Wentao Chen\n## Abstract\nDataset distillation, a pragmatic approach in machine learning, aims to create a smaller synthetic dataset from a larger existing dataset. However, existing distillation methods primarily adopt a model-based paradigm, where the synthetic dataset inherits model-specific biases, limiting its generalizability to alternative models. In response to this constraint, we propose a novel methodology termed\"model pool\". This approach involves selecting models from a diverse model pool based on a specific probability distribution during the data distillation process. Additionally, we integrate our model pool with the established knowledge distillation approach and apply knowledge distillation to the test process of the distilled dataset. Our experimental results validate the effectiveness of the model pool approach across a range of existing models while testing, demonstrating superior performance compared to existing methodologies.\n## Knowledge Distillation\nKnowledge distillation is a method to transfer knowledge from a large model to a small model. It's first proposed in this paper [12]. The large model is called the teacher model and the small model is called the student model. The student model is trained to mimic the output of the teacher model. The student model is usually a shallow neural network with fewer parameters and faster inference speed. The teacher model is usually a deep neural network with more parameters and slower inference speed. The student model is trained on the same dataset as the teacher model to minimize the difference between the output of the student model and the output of the teacher model (i.e. maintain the knowledge as much as possible). \n\nAfter knowledge distillation was proposed, a lot of work has been put into exploring further applications in diverse fields, such as speech recognition, image recognition, and natural language processing. It proved that knowledge distillation is a great success in these various fields [11]. \n\nHowever, there are some arguments against knowledge distillation. A paper [22] said that, while knowledge distillation improves the generalization of student models, there often remains a surprisingly large discrepancy between the predictive distributions of the teacher and the student, even in cases when the student can perfectly match the teacher.",
            "reference_string": "[267759723 | Zhou et al. | 2024 | Citations: 4]"
        },
        {
            "title": "Online cross-layer knowledge distillation on graph neural networks with deep supervision",
            "venue": "Neural computing & applications (Print)",
            "year": 2022,
            "reference_count": 56,
            "citation_count": 3,
            "influential_citation_count": 1,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": "CLOSED",
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2210.13743, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2143286047",
                    "name": "Jiongyu Guo"
                },
                {
                    "authorId": "1684692",
                    "name": "Defang Chen"
                },
                {
                    "authorId": "2144350104",
                    "name": "Can Wang"
                }
            ],
            "abstract": "Graph neural networks (GNNs) have become one of the most popular research topics in both academia and industry communities for their strong ability in handling irregular graph data. However, large-scale datasets are posing great challenges for deploying GNNs in edge devices with limited resources and model compression techniques have drawn considerable research attention. Existing model compression techniques such as knowledge distillation mainly focus on convolutional neural networks. Only limited attempts have been made recently for distilling knowledge from GNNs in an offline manner. As the performance of the teacher model does not necessarily improve as the number of layers increases in GNNs, selecting an appropriate teacher model will require substantial efforts. To address these challenges, we propose a novel online knowledge distillation framework called Alignahead++ in this paper. Alignahead++ transfers structure and feature information in a student layer to the previous layer of another simultaneously trained student model in an alternating training procedure. Meanwhile, to avoid over-smoothing problem in GNNs, deep supervision is employed in Alignahead++ by adding an auxiliary classifier in each intermediate layer to prevent the collapse of the node feature embeddings. Experimental results on four datasets including PPI, Cora, PubMed and CiteSeer demonstrate that the student performance is consistently boosted in our collaborative training framework without the supervision of a pre-trained teacher model and its effectiveness can generally be improved by increasing the number of students.",
            "corpus_id": 253107469,
            "sentences": [
                {
                    "corpus_id": "253107469",
                    "title": "Online cross-layer knowledge distillation on graph neural networks with deep supervision",
                    "text": "Knowledge distillation is one of the most popular techniques in model compression [7], which utilizes a powerful but cumbersome teacher model to boost a compressed student model. Hinton et al. [8] first proposed the concept of knowledge distillation, trying to transfer the soft targets, i.e., the teacher model predictions to the student models. It's believed that the soft targets can capture the relationship among classes and apply regularization during the training. In addition to soft targets, feature maps in the intermediate layers are another form of knowledge [9][10][11][12]. Romero et al. [9] first put forward FitNet to reduce the distance of feature embeddings between teacher models and student models. Agoruyko et al. [10] considered the attention mechanism and extracted attention maps instead of features. SemCKD [11] utilizes the attention mechanism to automatically assign adaptive intermediate features for student model. SimKD [13] achieves incredible effect just by reusing the teacher's classifier. \n\nHowever, training a large teacher model in advance is time-consuming and requires high computing resources. In a worse case, we can't even get a teacher model. The online knowledge distillation [22][23][24] is proposed to address these issues, where a group of student models are trained simultaneously by aggregating and aligning their outputs. Compared with the offline knowledge distillation mentioned above, online knowledge distillation can still boost the performance of the student model when reducing the training costs. Deep mutual learning [22] trained a group of student models collaboratively by learning the soft targets from each other. OKDDip [24] introduced multiple auxiliary peers and one team leader, proposing a novel two-level online knowledge distillation framework.",
                    "score": 0.6098076783686592,
                    "section_title": "Knowledge Distillation",
                    "char_start_offset": 6016,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 178
                        },
                        {
                            "start": 179,
                            "end": 346
                        },
                        {
                            "start": 347,
                            "end": 471
                        },
                        {
                            "start": 472,
                            "end": 587
                        },
                        {
                            "start": 588,
                            "end": 718
                        },
                        {
                            "start": 719,
                            "end": 824
                        },
                        {
                            "start": 825,
                            "end": 943
                        },
                        {
                            "start": 944,
                            "end": 1023
                        },
                        {
                            "start": 1026,
                            "end": 1133
                        },
                        {
                            "start": 1134,
                            "end": 1185
                        },
                        {
                            "start": 1186,
                            "end": 1371
                        },
                        {
                            "start": 1372,
                            "end": 1554
                        },
                        {
                            "start": 1555,
                            "end": 1676
                        },
                        {
                            "start": 1677,
                            "end": 1814
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 82,
                            "end": 85,
                            "matchedPaperCorpusId": "11253972"
                        },
                        {
                            "start": 571,
                            "end": 574,
                            "matchedPaperCorpusId": "2723173"
                        },
                        {
                            "start": 574,
                            "end": 578,
                            "matchedPaperCorpusId": "829159"
                        },
                        {
                            "start": 578,
                            "end": 582,
                            "matchedPaperCorpusId": "227335337"
                        },
                        {
                            "start": 582,
                            "end": 586,
                            "matchedPaperCorpusId": "245650711"
                        },
                        {
                            "start": 602,
                            "end": 605,
                            "matchedPaperCorpusId": "2723173"
                        },
                        {
                            "start": 735,
                            "end": 739,
                            "matchedPaperCorpusId": "829159"
                        },
                        {
                            "start": 832,
                            "end": 836,
                            "matchedPaperCorpusId": "227335337"
                        },
                        {
                            "start": 950,
                            "end": 954,
                            "matchedPaperCorpusId": "247762862"
                        },
                        {
                            "start": 1220,
                            "end": 1224,
                            "matchedPaperCorpusId": "26071966"
                        },
                        {
                            "start": 1224,
                            "end": 1228,
                            "matchedPaperCorpusId": "2331610"
                        },
                        {
                            "start": 1228,
                            "end": 1232,
                            "matchedPaperCorpusId": "208526905"
                        },
                        {
                            "start": 1576,
                            "end": 1580,
                            "matchedPaperCorpusId": "26071966"
                        },
                        {
                            "start": 1684,
                            "end": 1688,
                            "matchedPaperCorpusId": "208526905"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.9794921875
                }
            ],
            "relevance_judgement": 0.9794921875,
            "relevance_judgment_input_expanded": "# Title: Online cross-layer knowledge distillation on graph neural networks with deep supervision\n# Venue: Neural computing & applications (Print)\n# Authors: Jiongyu Guo, Defang Chen, Can Wang\n## Abstract\nGraph neural networks (GNNs) have become one of the most popular research topics in both academia and industry communities for their strong ability in handling irregular graph data. However, large-scale datasets are posing great challenges for deploying GNNs in edge devices with limited resources and model compression techniques have drawn considerable research attention. Existing model compression techniques such as knowledge distillation mainly focus on convolutional neural networks. Only limited attempts have been made recently for distilling knowledge from GNNs in an offline manner. As the performance of the teacher model does not necessarily improve as the number of layers increases in GNNs, selecting an appropriate teacher model will require substantial efforts. To address these challenges, we propose a novel online knowledge distillation framework called Alignahead++ in this paper. Alignahead++ transfers structure and feature information in a student layer to the previous layer of another simultaneously trained student model in an alternating training procedure. Meanwhile, to avoid over-smoothing problem in GNNs, deep supervision is employed in Alignahead++ by adding an auxiliary classifier in each intermediate layer to prevent the collapse of the node feature embeddings. Experimental results on four datasets including PPI, Cora, PubMed and CiteSeer demonstrate that the student performance is consistently boosted in our collaborative training framework without the supervision of a pre-trained teacher model and its effectiveness can generally be improved by increasing the number of students.\n## Knowledge Distillation\nKnowledge distillation is one of the most popular techniques in model compression [7], which utilizes a powerful but cumbersome teacher model to boost a compressed student model. Hinton et al. [8] first proposed the concept of knowledge distillation, trying to transfer the soft targets, i.e., the teacher model predictions to the student models. It's believed that the soft targets can capture the relationship among classes and apply regularization during the training. In addition to soft targets, feature maps in the intermediate layers are another form of knowledge [9][10][11][12]. Romero et al. [9] first put forward FitNet to reduce the distance of feature embeddings between teacher models and student models. Agoruyko et al. [10] considered the attention mechanism and extracted attention maps instead of features. SemCKD [11] utilizes the attention mechanism to automatically assign adaptive intermediate features for student model. SimKD [13] achieves incredible effect just by reusing the teacher's classifier. \n\nHowever, training a large teacher model in advance is time-consuming and requires high computing resources. In a worse case, we can't even get a teacher model. The online knowledge distillation [22][23][24] is proposed to address these issues, where a group of student models are trained simultaneously by aggregating and aligning their outputs. Compared with the offline knowledge distillation mentioned above, online knowledge distillation can still boost the performance of the student model when reducing the training costs. Deep mutual learning [22] trained a group of student models collaboratively by learning the soft targets from each other. OKDDip [24] introduced multiple auxiliary peers and one team leader, proposing a novel two-level online knowledge distillation framework.",
            "reference_string": "[253107469 | Guo et al. | 2022 | Citations: 3]"
        },
        {
            "title": "Dataset Distillation: A Comprehensive Review",
            "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence",
            "year": 2023,
            "reference_count": 188,
            "citation_count": 130,
            "influential_citation_count": 6,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://arxiv.org/pdf/2301.07014",
                "status": "GREEN",
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2301.07014, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2049052456",
                    "name": "Ruonan Yu"
                },
                {
                    "authorId": "50152011",
                    "name": "Songhua Liu"
                },
                {
                    "authorId": "48631088",
                    "name": "Xinchao Wang"
                }
            ],
            "abstract": "Recent success of deep learning is largely attributed to the sheer amount of data used for training deep neural networks. Despite the unprecedented success, the massive data, unfortunately, significantly increases the burden on storage and transmission and further gives rise to a cumbersome model training process. Besides, relying on the raw data for training per se yields concerns about privacy and copyright. To alleviate these shortcomings, dataset distillation (DD), also known as dataset condensation (DC), was introduced and has recently attracted much research attention in the community. Given an original dataset, DD aims to derive a much smaller dataset containing synthetic samples, based on which the trained models yield performance comparable with those trained on the original dataset. In this paper, we give a comprehensive review and summary of recent advances in DD and its application. We first introduce the task formally and propose an overall algorithmic framework followed by all existing DD methods. Next, we provide a systematic taxonomy of current methodologies in this area, and discuss their theoretical interconnections. We also present current challenges in DD through extensive empirical studies and envision possible directions for future works.",
            "corpus_id": 255942245,
            "sentences": [
                {
                    "corpus_id": "255942245",
                    "title": "Dataset Distillation: A Comprehensive Review",
                    "text": "Knowledge distillation (KD) [37], [38], [39], [40] aims to transfer knowledge from a large teacher network to a smaller student network, such that the student network can preserve the performance of the teacher with reduced computational overhead. The seminal work by Hinton et al. [37] leads the student to mimic the outputs of the teacher, which can represent knowledge acquired by the teacher network. Afterward, improvements of KD have focused on four aspects: representations of knowledge, teacherstudent architectures, distillation algorithms, and distillation schemes. First, knowledge can be represented by model response/output [37], [41], features [38], [42], [43], and relation [44], [45], [46]. Second, teacher-student architectures refer to the network architectures of teacher and student models, which determines the quality of knowledge acquisition and distillation from teacher to student [40]. Third, distillation algorithms determine the ways of knowledge transfer. A simple and typical way is to match the knowledge captured by the teacher and student models directly [37], [38]. Beyond that, many different algorithms are proposed to handle more complex settings, such as adversarial distillation [47], attention-based distillation [39], and data-free distillation [48], [49]. Finally, distillation schemes control training configurations of teacher and student, and there are offline- [37], [38], online- [50], and self-distillation [51]. As for application, KD is widely used in ensemble learning [52] and model compression [38], [53], [54]. \n\nThe concept of DD is inspired by KD [18]. Specifically, DD aims at a lightweight dataset, while KD aims at a lightweight model. In this view, DD and KD are only conceptually related but technically orthogonal. It is worth noting that, similar to DD, recent data-free KD methods [48], [49], [55] are also concerned with the generation of synthetic training samples since original training datasets are unavailable. Their differences are two-fold.",
                    "score": 0.7780427429888754,
                    "section_title": "Knowledge Distillation",
                    "char_start_offset": 5802,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 247
                        },
                        {
                            "start": 248,
                            "end": 404
                        },
                        {
                            "start": 405,
                            "end": 575
                        },
                        {
                            "start": 576,
                            "end": 706
                        },
                        {
                            "start": 707,
                            "end": 911
                        },
                        {
                            "start": 912,
                            "end": 984
                        },
                        {
                            "start": 985,
                            "end": 1099
                        },
                        {
                            "start": 1100,
                            "end": 1297
                        },
                        {
                            "start": 1298,
                            "end": 1460
                        },
                        {
                            "start": 1461,
                            "end": 1564
                        },
                        {
                            "start": 1567,
                            "end": 1608
                        },
                        {
                            "start": 1609,
                            "end": 1694
                        },
                        {
                            "start": 1695,
                            "end": 1776
                        },
                        {
                            "start": 1777,
                            "end": 1980
                        },
                        {
                            "start": 1981,
                            "end": 2012
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 46,
                            "end": 50,
                            "matchedPaperCorpusId": "219559263"
                        },
                        {
                            "start": 643,
                            "end": 647,
                            "matchedPaperCorpusId": "212908749"
                        },
                        {
                            "start": 664,
                            "end": 668,
                            "matchedPaperCorpusId": "221559239"
                        },
                        {
                            "start": 670,
                            "end": 674,
                            "matchedPaperCorpusId": "227232038"
                        },
                        {
                            "start": 689,
                            "end": 693,
                            "matchedPaperCorpusId": "26021416"
                        },
                        {
                            "start": 695,
                            "end": 699,
                            "matchedPaperCorpusId": "131765296"
                        },
                        {
                            "start": 701,
                            "end": 705,
                            "matchedPaperCorpusId": "198160865"
                        },
                        {
                            "start": 906,
                            "end": 910,
                            "matchedPaperCorpusId": "219559263"
                        },
                        {
                            "start": 1218,
                            "end": 1222,
                            "matchedPaperCorpusId": "53976534"
                        },
                        {
                            "start": 1292,
                            "end": 1296,
                            "matchedPaperCorpusId": "261514306"
                        },
                        {
                            "start": 1427,
                            "end": 1431,
                            "matchedPaperCorpusId": "24982157"
                        },
                        {
                            "start": 1455,
                            "end": 1459,
                            "matchedPaperCorpusId": "159041406"
                        },
                        {
                            "start": 1520,
                            "end": 1524,
                            "matchedPaperCorpusId": "7350432"
                        },
                        {
                            "start": 1553,
                            "end": 1557,
                            "matchedPaperCorpusId": "11536917"
                        },
                        {
                            "start": 1851,
                            "end": 1855,
                            "matchedPaperCorpusId": "261514306"
                        },
                        {
                            "start": 1857,
                            "end": 1861,
                            "matchedPaperCorpusId": "159041346"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.97802734375
                }
            ],
            "relevance_judgement": 0.97802734375,
            "relevance_judgment_input_expanded": "# Title: Dataset Distillation: A Comprehensive Review\n# Venue: IEEE Transactions on Pattern Analysis and Machine Intelligence\n# Authors: Ruonan Yu, Songhua Liu, Xinchao Wang\n## Abstract\nRecent success of deep learning is largely attributed to the sheer amount of data used for training deep neural networks. Despite the unprecedented success, the massive data, unfortunately, significantly increases the burden on storage and transmission and further gives rise to a cumbersome model training process. Besides, relying on the raw data for training per se yields concerns about privacy and copyright. To alleviate these shortcomings, dataset distillation (DD), also known as dataset condensation (DC), was introduced and has recently attracted much research attention in the community. Given an original dataset, DD aims to derive a much smaller dataset containing synthetic samples, based on which the trained models yield performance comparable with those trained on the original dataset. In this paper, we give a comprehensive review and summary of recent advances in DD and its application. We first introduce the task formally and propose an overall algorithmic framework followed by all existing DD methods. Next, we provide a systematic taxonomy of current methodologies in this area, and discuss their theoretical interconnections. We also present current challenges in DD through extensive empirical studies and envision possible directions for future works.\n## Knowledge Distillation\nKnowledge distillation (KD) [37], [38], [39], [40] aims to transfer knowledge from a large teacher network to a smaller student network, such that the student network can preserve the performance of the teacher with reduced computational overhead. The seminal work by Hinton et al. [37] leads the student to mimic the outputs of the teacher, which can represent knowledge acquired by the teacher network. Afterward, improvements of KD have focused on four aspects: representations of knowledge, teacherstudent architectures, distillation algorithms, and distillation schemes. First, knowledge can be represented by model response/output [37], [41], features [38], [42], [43], and relation [44], [45], [46]. Second, teacher-student architectures refer to the network architectures of teacher and student models, which determines the quality of knowledge acquisition and distillation from teacher to student [40]. Third, distillation algorithms determine the ways of knowledge transfer. A simple and typical way is to match the knowledge captured by the teacher and student models directly [37], [38]. Beyond that, many different algorithms are proposed to handle more complex settings, such as adversarial distillation [47], attention-based distillation [39], and data-free distillation [48], [49]. Finally, distillation schemes control training configurations of teacher and student, and there are offline- [37], [38], online- [50], and self-distillation [51]. As for application, KD is widely used in ensemble learning [52] and model compression [38], [53], [54]. \n\nThe concept of DD is inspired by KD [18]. Specifically, DD aims at a lightweight dataset, while KD aims at a lightweight model. In this view, DD and KD are only conceptually related but technically orthogonal. It is worth noting that, similar to DD, recent data-free KD methods [48], [49], [55] are also concerned with the generation of synthetic training samples since original training datasets are unavailable. Their differences are two-fold.",
            "reference_string": "[255942245 | Yu et al. | 2023 | Citations: 130]"
        },
        {
            "title": "Exploring Graph-based Knowledge: Multi-Level Feature Distillation via Channels Relational Graph",
            "venue": "arXiv.org",
            "year": 2024,
            "reference_count": 54,
            "citation_count": 0,
            "influential_citation_count": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2405.08547, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2301262030",
                    "name": "Zhiwei Wang"
                },
                {
                    "authorId": "2301263964",
                    "name": "Jun Huang"
                },
                {
                    "authorId": "2301265454",
                    "name": "Longhua Ma"
                },
                {
                    "authorId": "2301254673",
                    "name": "Chengyu Wu"
                },
                {
                    "authorId": "2301265433",
                    "name": "Hongyu Ma"
                }
            ],
            "abstract": "In visual tasks, large teacher models capture essential features and deep information, enhancing performance. However, distilling this information into smaller student models often leads to performance loss due to structural differences and capacity limitations. To tackle this, we propose a distillation framework based on graph knowledge, including a multi-level feature alignment strategy and an attention-guided mechanism to provide a targeted learning trajectory for the student model. We emphasize spectral embedding (SE) as a key technique in our distillation process, which merges the student's feature space with the relational knowledge and structural complexities similar to the teacher network. This method captures the teacher's understanding in a graph-based representation, enabling the student model to more accurately mimic the complex structural dependencies present in the teacher model. Compared to methods that focus only on specific distillation areas, our strategy not only considers key features within the teacher model but also endeavors to capture the relationships and interactions among feature sets, encoding these complex pieces of information into a graph structure to understand and utilize the dynamic relationships among these pieces of information from a global perspective. Experiments show that our method outperforms previous feature distillation methods on the CIFAR-100, MS-COCO, and Pascal VOC datasets, proving its efficiency and applicability.",
            "corpus_id": 269761365,
            "sentences": [
                {
                    "corpus_id": "269761365",
                    "title": "Exploring Graph-based Knowledge: Multi-Level Feature Distillation via Channels Relational Graph",
                    "text": "In the field of deep learning, the impressive success of large neural networks has come at the cost of increased computational complexity, which poses significant challenges for deployment in resourceconstrained environments.While these heavyweight models, often referred to as teacher networks, set state-of-the-art benchmarks on various tasks, their practical applicability is limited by their demanding requirements for memory, processing power, and energy.Knowledge Distillation (KD) [16] emerges as a promising solution to address this dichotomy by transferring the knowledge from a cumbersome model to a more compact and efficient student networks.\n\nThe quintessence of knowledge distillation lies in its ability to encapsulate the representational power of a larger model into a smaller one without incurring a substantial loss in performance.Pioneered by Hinton et al. [16], the process involves training a smaller model to mimic the behavior of the pre-trained larger model by softening the output logits [19], thus leveraging the rich information embedded in the output distributions of the teacher network.\n\nRecent advances in KD techniques have extended beyond the mere replication of output distributions.Contemporary works explore the distillation of intermediate representations [1,49,31], attention mechanisms [50,20], and even the inculcation of adversarial robustness from teacher to student [33,55,37].The underlying hypothesis is that the intermediate layers of a neural network embody a wealth of information about the data manifold that, when transferred effectively, can endow the student with nuanced understanding akin to its teacher.\n\nLarge and complex teacher models often capture a wealth of features and deep information, crucial for enhancing task performance [7].However, attempting to directly distill this rich information into smaller capacity student models often results in suboptimal performance reproduction due to the student model's capacity limitations.The student model may struggle to process the complex information present in the teacher model, leading to ineffective distillation.Moreover, simply mimicking all features of the teacher model overlooks the differences in relationships and structures between the information, particularly when there is a significant gap between the teacher and student models [12].",
                    "score": 0.6471252973423388,
                    "section_title": "Introduction",
                    "char_start_offset": 15,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 225
                        },
                        {
                            "start": 225,
                            "end": 460
                        },
                        {
                            "start": 460,
                            "end": 654
                        },
                        {
                            "start": 656,
                            "end": 850
                        },
                        {
                            "start": 850,
                            "end": 1117
                        },
                        {
                            "start": 1119,
                            "end": 1218
                        },
                        {
                            "start": 1218,
                            "end": 1421
                        },
                        {
                            "start": 1421,
                            "end": 1659
                        },
                        {
                            "start": 1661,
                            "end": 1794
                        },
                        {
                            "start": 1794,
                            "end": 1994
                        },
                        {
                            "start": 1994,
                            "end": 2126
                        },
                        {
                            "start": 2126,
                            "end": 2359
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 1014,
                            "end": 1018,
                            "matchedPaperCorpusId": "260933721"
                        },
                        {
                            "start": 1294,
                            "end": 1297,
                            "matchedPaperCorpusId": "2723173"
                        },
                        {
                            "start": 1297,
                            "end": 1300,
                            "matchedPaperCorpusId": "206596723"
                        },
                        {
                            "start": 1300,
                            "end": 1303,
                            "matchedPaperCorpusId": "198185886"
                        },
                        {
                            "start": 1330,
                            "end": 1333,
                            "matchedPaperCorpusId": "125950115"
                        },
                        {
                            "start": 1414,
                            "end": 1417,
                            "matchedPaperCorpusId": "237194985"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.97802734375
                }
            ],
            "relevance_judgement": 0.97802734375,
            "relevance_judgment_input_expanded": "# Title: Exploring Graph-based Knowledge: Multi-Level Feature Distillation via Channels Relational Graph\n# Venue: arXiv.org\n# Authors: Zhiwei Wang, Jun Huang, Longhua Ma, Chengyu Wu, Hongyu Ma\n## Abstract\nIn visual tasks, large teacher models capture essential features and deep information, enhancing performance. However, distilling this information into smaller student models often leads to performance loss due to structural differences and capacity limitations. To tackle this, we propose a distillation framework based on graph knowledge, including a multi-level feature alignment strategy and an attention-guided mechanism to provide a targeted learning trajectory for the student model. We emphasize spectral embedding (SE) as a key technique in our distillation process, which merges the student's feature space with the relational knowledge and structural complexities similar to the teacher network. This method captures the teacher's understanding in a graph-based representation, enabling the student model to more accurately mimic the complex structural dependencies present in the teacher model. Compared to methods that focus only on specific distillation areas, our strategy not only considers key features within the teacher model but also endeavors to capture the relationships and interactions among feature sets, encoding these complex pieces of information into a graph structure to understand and utilize the dynamic relationships among these pieces of information from a global perspective. Experiments show that our method outperforms previous feature distillation methods on the CIFAR-100, MS-COCO, and Pascal VOC datasets, proving its efficiency and applicability.\n## Introduction\nIn the field of deep learning, the impressive success of large neural networks has come at the cost of increased computational complexity, which poses significant challenges for deployment in resourceconstrained environments.While these heavyweight models, often referred to as teacher networks, set state-of-the-art benchmarks on various tasks, their practical applicability is limited by their demanding requirements for memory, processing power, and energy.Knowledge Distillation (KD) [16] emerges as a promising solution to address this dichotomy by transferring the knowledge from a cumbersome model to a more compact and efficient student networks.\n\nThe quintessence of knowledge distillation lies in its ability to encapsulate the representational power of a larger model into a smaller one without incurring a substantial loss in performance.Pioneered by Hinton et al. [16], the process involves training a smaller model to mimic the behavior of the pre-trained larger model by softening the output logits [19], thus leveraging the rich information embedded in the output distributions of the teacher network.\n\nRecent advances in KD techniques have extended beyond the mere replication of output distributions.Contemporary works explore the distillation of intermediate representations [1,49,31], attention mechanisms [50,20], and even the inculcation of adversarial robustness from teacher to student [33,55,37].The underlying hypothesis is that the intermediate layers of a neural network embody a wealth of information about the data manifold that, when transferred effectively, can endow the student with nuanced understanding akin to its teacher.\n\nLarge and complex teacher models often capture a wealth of features and deep information, crucial for enhancing task performance [7].However, attempting to directly distill this rich information into smaller capacity student models often results in suboptimal performance reproduction due to the student model's capacity limitations.The student model may struggle to process the complex information present in the teacher model, leading to ineffective distillation.Moreover, simply mimicking all features of the teacher model overlooks the differences in relationships and structures between the information, particularly when there is a significant gap between the teacher and student models [12].",
            "reference_string": "[269761365 | Wang et al. | 2024 | Citations: 0]"
        },
        {
            "title": "Deep Learning-Based Eye Gaze Estimation for Automotive Applications Using Knowledge Distillation",
            "venue": "IEEE Access",
            "year": 2023,
            "reference_count": 39,
            "citation_count": 1,
            "influential_citation_count": 0,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://ieeexplore.ieee.org/ielx7/6287639/6514899/10287539.pdf",
                "status": "GOLD",
                "license": "CCBYNCND",
                "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.1109/ACCESS.2023.3325134?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1109/ACCESS.2023.3325134, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2188876785",
                    "name": "Ioan Lucan Or\u0103\u0219an"
                },
                {
                    "authorId": "2044357384",
                    "name": "Adrian Bublea"
                },
                {
                    "authorId": "32693028",
                    "name": "C. C\u0103leanu"
                }
            ],
            "abstract": "Deep neural networks are currently applied in multiple domains, especially in the automotive industry. The main reason for this is related to the more complex challenges found in the field of signal processing, especially when the tasks involve image and video data types. Using conventional/statistical algorithms to deal with these high-complexity challenges is no longer a viable approach. Therefore, the involvement of artificial intelligence solutions like deep neural networks has significantly increased. In recent years, numerous architectures have been developed with the aim of maximizing performance. However, their size and computation requirements have increased at the same time. For this reason, special attention is currently being paid to the optimization of deep neural networks while trying to maintain (almost) the same performance. In this work, we aim to tackle the problem of eye gaze estimation considered within the automotive framework. Our proposal uses a knowledge distillation concept applied to a custom CNN architecture, called the teacher model. Based on this, several CNN student models are derived using layerwise and widthwise compression techniques. Furthermore, they are evaluated with respect to certain performance metrics, e.g. neural network size and inference time. In the experimental results, we propose certain compression methods which can address specific user requirements like model size, accuracy, and inference time. Finally, the student models are evaluated using an EdgeAI embedded device (STM32H747I-DISCO) in terms of accuracy, memory utilization, MACC complexity, and inference time. The combination of layerwise and widthwise compression results as the optimal method to derive student models with a good trade-off between the above-mentioned metrics. Using knowledge distillation, the accuracy can be improved by up to 9.5% over the conventional training procedure.",
            "corpus_id": 264323617,
            "sentences": [
                {
                    "corpus_id": "264323617",
                    "title": "Deep Learning-Based Eye Gaze Estimation for Automotive Applications Using Knowledge Distillation",
                    "text": "The compression of a DNN is common practice for obtaining a lightweight network for low-cost and resource-scarce hardware devices. Knowledge distillation is a particular method that involves training to transfer knowledge from a larger network to a different one that has a significantly smaller size. Bucilua et al. [31] in their important paper successfully demonstrated for the first time that the knowledge acquired by a large ensemble of models can be transferred to a single small model. The aim of using this method is to train the student network so that it can reproduce the performance of the teacher network, but with fewer resources. \n\nThe process of applying knowledge distillation consists of the following main stages: (1) definition of teacher and student networks, (2) training of the teacher network, and (3) training of the student network with knowledge transfer from the teacher network. They are detailed below.",
                    "score": 0.6490433136974237,
                    "section_title": "III. METHODOLOGY A. KNOWLEDGE DISTILLATION",
                    "char_start_offset": 22290,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 130
                        },
                        {
                            "start": 131,
                            "end": 301
                        },
                        {
                            "start": 302,
                            "end": 493
                        },
                        {
                            "start": 494,
                            "end": 645
                        },
                        {
                            "start": 648,
                            "end": 908
                        },
                        {
                            "start": 909,
                            "end": 933
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 317,
                            "end": 321,
                            "matchedPaperCorpusId": "11253972"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.9775390625
                }
            ],
            "relevance_judgement": 0.9775390625,
            "relevance_judgment_input_expanded": "# Title: Deep Learning-Based Eye Gaze Estimation for Automotive Applications Using Knowledge Distillation\n# Venue: IEEE Access\n# Authors: Ioan Lucan Or\u0103\u0219an, Adrian Bublea, C. C\u0103leanu\n## Abstract\nDeep neural networks are currently applied in multiple domains, especially in the automotive industry. The main reason for this is related to the more complex challenges found in the field of signal processing, especially when the tasks involve image and video data types. Using conventional/statistical algorithms to deal with these high-complexity challenges is no longer a viable approach. Therefore, the involvement of artificial intelligence solutions like deep neural networks has significantly increased. In recent years, numerous architectures have been developed with the aim of maximizing performance. However, their size and computation requirements have increased at the same time. For this reason, special attention is currently being paid to the optimization of deep neural networks while trying to maintain (almost) the same performance. In this work, we aim to tackle the problem of eye gaze estimation considered within the automotive framework. Our proposal uses a knowledge distillation concept applied to a custom CNN architecture, called the teacher model. Based on this, several CNN student models are derived using layerwise and widthwise compression techniques. Furthermore, they are evaluated with respect to certain performance metrics, e.g. neural network size and inference time. In the experimental results, we propose certain compression methods which can address specific user requirements like model size, accuracy, and inference time. Finally, the student models are evaluated using an EdgeAI embedded device (STM32H747I-DISCO) in terms of accuracy, memory utilization, MACC complexity, and inference time. The combination of layerwise and widthwise compression results as the optimal method to derive student models with a good trade-off between the above-mentioned metrics. Using knowledge distillation, the accuracy can be improved by up to 9.5% over the conventional training procedure.\n## III. METHODOLOGY A. KNOWLEDGE DISTILLATION\nThe compression of a DNN is common practice for obtaining a lightweight network for low-cost and resource-scarce hardware devices. Knowledge distillation is a particular method that involves training to transfer knowledge from a larger network to a different one that has a significantly smaller size. Bucilua et al. [31] in their important paper successfully demonstrated for the first time that the knowledge acquired by a large ensemble of models can be transferred to a single small model. The aim of using this method is to train the student network so that it can reproduce the performance of the teacher network, but with fewer resources. \n\nThe process of applying knowledge distillation consists of the following main stages: (1) definition of teacher and student networks, (2) training of the teacher network, and (3) training of the student network with knowledge transfer from the teacher network. They are detailed below.",
            "reference_string": "[264323617 | Orasan et al. | 2023 | Citations: 1]"
        },
        {
            "title": "Large\u2010scale knowledge distillation with elastic heterogeneous computing resources",
            "venue": "Concurrency and Computation",
            "year": 2022,
            "reference_count": 44,
            "citation_count": 6,
            "influential_citation_count": 0,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://arxiv.org/pdf/2207.06667",
                "status": "GREEN",
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2207.06667, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2118971193",
                    "name": "Ji Liu"
                },
                {
                    "authorId": "9532787",
                    "name": "Daxiang Dong"
                },
                {
                    "authorId": "2108249583",
                    "name": "Xi Wang"
                },
                {
                    "authorId": "2140532375",
                    "name": "An Qin"
                },
                {
                    "authorId": "2155445773",
                    "name": "Xingjian Li"
                },
                {
                    "authorId": "144255847",
                    "name": "P. Valduriez"
                },
                {
                    "authorId": "1721158",
                    "name": "D. Dou"
                },
                {
                    "authorId": "3046102",
                    "name": "Dianhai Yu"
                }
            ],
            "abstract": "Although more layers and more parameters generally improve the accuracy of the models, such big models generally have high computational complexity and require big memory, which exceed the capacity of small devices for inference and incurs long training time. In addition, it is difficult to afford long training time and inference time of big models even in high performance servers, as well. As an efficient approach to compress a large deep model (a teacher model) to a compact model (a student model), knowledge distillation emerges as a promising approach to deal with the big models. Existing knowledge distillation methods cannot exploit the elastic available computing resources and correspond to low efficiency. In this paper, we propose an Elastic Deep Learning framework for knowledge Distillation, that is, EDL\u2010Dist. The advantages of EDL\u2010Dist are threefold. First, the inference and the training process is separated. Second, elastic available computing resources can be utilized to improve the efficiency. Third, fault\u2010tolerance of the training and inference processes is supported. We take extensive experimentation to show that the throughput of EDL\u2010Dist is up to 3.125 times faster than the baseline method (online knowledge distillation) while the accuracy is similar or higher.",
            "corpus_id": 250526328,
            "sentences": [
                {
                    "corpus_id": "250526328",
                    "title": "Large\u2010scale knowledge distillation with elastic heterogeneous computing resources",
                    "text": "In this section, we present the background of elastic deep learning distillation, i.e., knowledge distillation. Then, we introduce the methods for distributed and decentralized training and elastic computing resources. \n\nModel compression based on knowledge transferring was first proposed in 7 in order to compress an ensemble of models or a large model to a compact model. The ensemble of models or the large model take much storage space and require a long time for inference while the compact model requires relatively small storage space and a short time for inference. Knowledge distillation is based on the popular machine learning Softmax function and a temperature 8 , which is defined in Formula 1. \n\n, where is the output from the output layer of a teacher neural network; is a temperature, which indicates the impact of the output from the teacher model. The Softmax output layer computes the probability that the input data corresponds to each class with the corresponding computed logit. The probability is related to a temperature T, i.e., which represents the impact of the distilled knowledge of the teacher model on the student model. A higher temperature corresponds to a weaker impact. As shown in Figure 1, during the training of knowledge distillation, two neural networks are used: teacher model and student model. The student model is trained based on the combination of two loss values. The first loss value is calculated from a soft prediction, which contains the probability of each class calculated from the teacher model. The soft prediction is calculated using Formula 1. The other loss value corresponds to a hard prediction, which is the ground truth label from the training data. \n\nTwo types of work are also related to this paper, i.e., mutual knowledge distillation and model pruning. When there is no available teacher model, the teacher and student can be trained at the same time, i.e., an ensemble of students can learn collaboratively and teach each other throughout the training process 16,17 , which is the mutual knowledge distillation 9 . Note that the mutual knowledge distillation is also denoted \"online knowledge distillation\" in 9 . However, we use the \"online knowledge distillation\" to represent the knowledge distillation with the teacher model and the student model deployed in the same GPU card during the training process of knowledge distillation in this paper.",
                    "score": 0.6116170616945228,
                    "section_title": "RELATED WORK",
                    "char_start_offset": 3556,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 111
                        },
                        {
                            "start": 112,
                            "end": 218
                        },
                        {
                            "start": 221,
                            "end": 374
                        },
                        {
                            "start": 375,
                            "end": 574
                        },
                        {
                            "start": 575,
                            "end": 708
                        },
                        {
                            "start": 711,
                            "end": 866
                        },
                        {
                            "start": 867,
                            "end": 1001
                        },
                        {
                            "start": 1002,
                            "end": 1152
                        },
                        {
                            "start": 1153,
                            "end": 1205
                        },
                        {
                            "start": 1206,
                            "end": 1337
                        },
                        {
                            "start": 1338,
                            "end": 1411
                        },
                        {
                            "start": 1412,
                            "end": 1550
                        },
                        {
                            "start": 1551,
                            "end": 1601
                        },
                        {
                            "start": 1602,
                            "end": 1712
                        },
                        {
                            "start": 1715,
                            "end": 1819
                        },
                        {
                            "start": 1820,
                            "end": 2082
                        },
                        {
                            "start": 2083,
                            "end": 2181
                        },
                        {
                            "start": 2182,
                            "end": 2417
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 293,
                            "end": 294,
                            "matchedPaperCorpusId": "11253972"
                        },
                        {
                            "start": 674,
                            "end": 675,
                            "matchedPaperCorpusId": "7200347"
                        },
                        {
                            "start": 2028,
                            "end": 2031,
                            "matchedPaperCorpusId": "26071966"
                        },
                        {
                            "start": 2031,
                            "end": 2033,
                            "matchedPaperCorpusId": "208526905"
                        },
                        {
                            "start": 2079,
                            "end": 2080,
                            "matchedPaperCorpusId": "219559263"
                        },
                        {
                            "start": 2178,
                            "end": 2179,
                            "matchedPaperCorpusId": "219559263"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.9775390625
                }
            ],
            "relevance_judgement": 0.9775390625,
            "relevance_judgment_input_expanded": "# Title: Large\u2010scale knowledge distillation with elastic heterogeneous computing resources\n# Venue: Concurrency and Computation\n# Authors: Ji Liu, Daxiang Dong, Xi Wang, An Qin, Xingjian Li, P. Valduriez, D. Dou, Dianhai Yu\n## Abstract\nAlthough more layers and more parameters generally improve the accuracy of the models, such big models generally have high computational complexity and require big memory, which exceed the capacity of small devices for inference and incurs long training time. In addition, it is difficult to afford long training time and inference time of big models even in high performance servers, as well. As an efficient approach to compress a large deep model (a teacher model) to a compact model (a student model), knowledge distillation emerges as a promising approach to deal with the big models. Existing knowledge distillation methods cannot exploit the elastic available computing resources and correspond to low efficiency. In this paper, we propose an Elastic Deep Learning framework for knowledge Distillation, that is, EDL\u2010Dist. The advantages of EDL\u2010Dist are threefold. First, the inference and the training process is separated. Second, elastic available computing resources can be utilized to improve the efficiency. Third, fault\u2010tolerance of the training and inference processes is supported. We take extensive experimentation to show that the throughput of EDL\u2010Dist is up to 3.125 times faster than the baseline method (online knowledge distillation) while the accuracy is similar or higher.\n## RELATED WORK\nIn this section, we present the background of elastic deep learning distillation, i.e., knowledge distillation. Then, we introduce the methods for distributed and decentralized training and elastic computing resources. \n\nModel compression based on knowledge transferring was first proposed in 7 in order to compress an ensemble of models or a large model to a compact model. The ensemble of models or the large model take much storage space and require a long time for inference while the compact model requires relatively small storage space and a short time for inference. Knowledge distillation is based on the popular machine learning Softmax function and a temperature 8 , which is defined in Formula 1. \n\n, where is the output from the output layer of a teacher neural network; is a temperature, which indicates the impact of the output from the teacher model. The Softmax output layer computes the probability that the input data corresponds to each class with the corresponding computed logit. The probability is related to a temperature T, i.e., which represents the impact of the distilled knowledge of the teacher model on the student model. A higher temperature corresponds to a weaker impact. As shown in Figure 1, during the training of knowledge distillation, two neural networks are used: teacher model and student model. The student model is trained based on the combination of two loss values. The first loss value is calculated from a soft prediction, which contains the probability of each class calculated from the teacher model. The soft prediction is calculated using Formula 1. The other loss value corresponds to a hard prediction, which is the ground truth label from the training data. \n\nTwo types of work are also related to this paper, i.e., mutual knowledge distillation and model pruning. When there is no available teacher model, the teacher and student can be trained at the same time, i.e., an ensemble of students can learn collaboratively and teach each other throughout the training process 16,17 , which is the mutual knowledge distillation 9 . Note that the mutual knowledge distillation is also denoted \"online knowledge distillation\" in 9 . However, we use the \"online knowledge distillation\" to represent the knowledge distillation with the teacher model and the student model deployed in the same GPU card during the training process of knowledge distillation in this paper.",
            "reference_string": "[250526328 | Liu et al. | 2022 | Citations: 6]"
        },
        {
            "title": "Forest Fire Object Detection Analysis Based on Knowledge Distillation",
            "venue": "Fire",
            "year": 2023,
            "reference_count": 48,
            "citation_count": 6,
            "influential_citation_count": 0,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://www.mdpi.com/2571-6255/6/12/446/pdf?version=1700635738",
                "status": "GOLD",
                "license": "CCBY",
                "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.3390/fire6120446?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.3390/fire6120446, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2268047558",
                    "name": "Jinzhou Xie"
                },
                {
                    "authorId": "2268031861",
                    "name": "Hongmin Zhao"
                }
            ],
            "abstract": "This paper investigates the application of the YOLOv7 object detection model combined with knowledge distillation techniques in forest fire detection. As an advanced object detection model, YOLOv7 boasts efficient real-time detection capabilities. However, its performance may be constrained in resource-limited environments. To address this challenge, this research proposes a novel approach: considering that deep neural networks undergo multi-layer mapping from the input to the output space, we define the knowledge propagation between layers by evaluating the dot product of features extracted from two different layers. To this end, we utilize the Flow of Solution Procedure (FSP) matrix based on the Gram matrix and redesign the distillation loss using the Pearson correlation coefficient, presenting a new knowledge distillation method termed ILKDG (Intermediate Layer Knowledge Distillation with Gram Matrix-based Feature Flow). Compared with the classical knowledge distillation algorithm, KD, ILKDG achieved a significant performance improvement on a self-created forest fire detection dataset. Specifically, without altering the student network\u2019s parameters or network layers, mAP@0.5 improved by 2.9%, and mAP@0.5:0.95 increased by 2.7%. These results indicate that the proposed ILKDG method effectively enhances the accuracy and performance of forest fire detection without introducing additional parameters. The ILKDG method, based on the Gram matrix and Pearson correlation coefficient, presents a novel knowledge distillation approach, providing a fresh avenue for future research. Researchers can further optimize and refine this method to achieve superior results in fire detection.",
            "corpus_id": 265384964,
            "sentences": [
                {
                    "corpus_id": "265384964",
                    "title": "Forest Fire Object Detection Analysis Based on Knowledge Distillation",
                    "text": "Knowledge distillation [34] involves the process of transferring knowledge from a sizable, intricate model (teacher model) to a more compact, effective model (student model) [35]. It involves training the student network to mimic the teacher network's output and emulate its internal representations or decision-making process. This technique is used to enhance the performance of smaller models, making them approximate the behavior of larger models while reducing computational costs and memory requirements. In the process of knowledge distillation, logits are used as a basis for comparing the outputs of the student model with those of the teacher model. By examining logits, the model can measure the certainty or confidence of its predictions before applying the softmax function to obtain the final probability. The standard cross-entropy loss depends on the predicted probabilities and ground truth labels. Additionally, the loss function is extended to include the standard cross-entropy loss between the predictions of the student model and the ground truth labels, as well as an additional loss term that measures the discrepancy between the softened probabilities (obtained through a higher temperature softmax) of the teacher model's predictions and the corresponding predictions of the student model. This additional term ensures that the student model not only learns to predict the correct labels but also aims to replicate the softened outputs of the teacher model, effectively transferring its knowledge to the student model. By jointly minimizing these two loss terms, the student model can learn to generalize better and imitate the behavior of the more complex teacher model. The calculation of the probability for the class is as follows: \n\nHere, T represents the \"temperature\" of knowledge distillation. When T = 1, it corresponds to a normalized exponential function. With the increase in the temperature parameter T, the softmax function's probability distribution becomes smoother, thereby conveying more nuanced particulars about the interrelation of different categories according to the teacher model. This information, referred to as \"dark knowledge\" by Hinton, is what we aim to impart to the student model in distillation. To compute the loss function for the teacher's soft targets, we use the same T value to calculate the softmax function based on the student logits. This kind of loss is frequently called \"distillation loss.\"",
                    "score": 0.6086358299316963,
                    "section_title": "Knowledge Distillation",
                    "char_start_offset": 9617,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 179
                        },
                        {
                            "start": 180,
                            "end": 327
                        },
                        {
                            "start": 328,
                            "end": 510
                        },
                        {
                            "start": 511,
                            "end": 659
                        },
                        {
                            "start": 660,
                            "end": 819
                        },
                        {
                            "start": 820,
                            "end": 915
                        },
                        {
                            "start": 916,
                            "end": 1315
                        },
                        {
                            "start": 1316,
                            "end": 1544
                        },
                        {
                            "start": 1545,
                            "end": 1697
                        },
                        {
                            "start": 1698,
                            "end": 1761
                        },
                        {
                            "start": 1764,
                            "end": 1827
                        },
                        {
                            "start": 1828,
                            "end": 1892
                        },
                        {
                            "start": 1893,
                            "end": 2131
                        },
                        {
                            "start": 2132,
                            "end": 2255
                        },
                        {
                            "start": 2256,
                            "end": 2403
                        },
                        {
                            "start": 2404,
                            "end": 2463
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 23,
                            "end": 27,
                            "matchedPaperCorpusId": "11253972"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.9775390625
                }
            ],
            "relevance_judgement": 0.9775390625,
            "relevance_judgment_input_expanded": "# Title: Forest Fire Object Detection Analysis Based on Knowledge Distillation\n# Venue: Fire\n# Authors: Jinzhou Xie, Hongmin Zhao\n## Abstract\nThis paper investigates the application of the YOLOv7 object detection model combined with knowledge distillation techniques in forest fire detection. As an advanced object detection model, YOLOv7 boasts efficient real-time detection capabilities. However, its performance may be constrained in resource-limited environments. To address this challenge, this research proposes a novel approach: considering that deep neural networks undergo multi-layer mapping from the input to the output space, we define the knowledge propagation between layers by evaluating the dot product of features extracted from two different layers. To this end, we utilize the Flow of Solution Procedure (FSP) matrix based on the Gram matrix and redesign the distillation loss using the Pearson correlation coefficient, presenting a new knowledge distillation method termed ILKDG (Intermediate Layer Knowledge Distillation with Gram Matrix-based Feature Flow). Compared with the classical knowledge distillation algorithm, KD, ILKDG achieved a significant performance improvement on a self-created forest fire detection dataset. Specifically, without altering the student network\u2019s parameters or network layers, mAP@0.5 improved by 2.9%, and mAP@0.5:0.95 increased by 2.7%. These results indicate that the proposed ILKDG method effectively enhances the accuracy and performance of forest fire detection without introducing additional parameters. The ILKDG method, based on the Gram matrix and Pearson correlation coefficient, presents a novel knowledge distillation approach, providing a fresh avenue for future research. Researchers can further optimize and refine this method to achieve superior results in fire detection.\n## Knowledge Distillation\nKnowledge distillation [34] involves the process of transferring knowledge from a sizable, intricate model (teacher model) to a more compact, effective model (student model) [35]. It involves training the student network to mimic the teacher network's output and emulate its internal representations or decision-making process. This technique is used to enhance the performance of smaller models, making them approximate the behavior of larger models while reducing computational costs and memory requirements. In the process of knowledge distillation, logits are used as a basis for comparing the outputs of the student model with those of the teacher model. By examining logits, the model can measure the certainty or confidence of its predictions before applying the softmax function to obtain the final probability. The standard cross-entropy loss depends on the predicted probabilities and ground truth labels. Additionally, the loss function is extended to include the standard cross-entropy loss between the predictions of the student model and the ground truth labels, as well as an additional loss term that measures the discrepancy between the softened probabilities (obtained through a higher temperature softmax) of the teacher model's predictions and the corresponding predictions of the student model. This additional term ensures that the student model not only learns to predict the correct labels but also aims to replicate the softened outputs of the teacher model, effectively transferring its knowledge to the student model. By jointly minimizing these two loss terms, the student model can learn to generalize better and imitate the behavior of the more complex teacher model. The calculation of the probability for the class is as follows: \n\nHere, T represents the \"temperature\" of knowledge distillation. When T = 1, it corresponds to a normalized exponential function. With the increase in the temperature parameter T, the softmax function's probability distribution becomes smoother, thereby conveying more nuanced particulars about the interrelation of different categories according to the teacher model. This information, referred to as \"dark knowledge\" by Hinton, is what we aim to impart to the student model in distillation. To compute the loss function for the teacher's soft targets, we use the same T value to calculate the softmax function based on the student logits. This kind of loss is frequently called \"distillation loss.\"",
            "reference_string": "[265384964 | Xie et al. | 2023 | Citations: 6]"
        },
        {
            "title": "Scalable Collaborative Learning via Representation Sharing",
            "venue": "arXiv.org",
            "year": 2022,
            "reference_count": 69,
            "citation_count": 3,
            "influential_citation_count": 0,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "http://arxiv.org/pdf/2211.10943",
                "status": "GREEN",
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2211.10943, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2183082271",
                    "name": "Fr'ed'eric Berdoz"
                },
                {
                    "authorId": "2034349211",
                    "name": "Abhishek Singh"
                },
                {
                    "authorId": "2456863",
                    "name": "Martin Jaggi"
                },
                {
                    "authorId": "2070747078",
                    "name": "Ramesh Raskar"
                }
            ],
            "abstract": "Privacy-preserving machine learning has become a key conundrum for multi-party artificial intelligence. Federated learning (FL) and Split Learning (SL) are two frameworks that enable collaborative learning while keeping the data private (on device). In FL, each data holder trains a model locally and releases it to a central server for aggregation. In SL, the clients must release individual cut-layer activations (smashed data) to the server and wait for its response (during both inference and back propagation). While relevant in several settings, both of these schemes have a high communication cost, rely on server-level computation algorithms and do not allow for tunable levels of collaboration. In this work, we present a novel approach for privacy-preserving machine learning, where the clients collaborate via online knowledge distillation using a contrastive loss (contrastive w.r.t. the labels). The goal is to ensure that the participants learn similar features on similar classes without sharing their input data. To do so, each client releases averaged last hidden layer activations of similar labels to a central server that only acts as a relay (i.e., is not involved in the training or aggregation of the models). Then, the clients download these last layer activations (feature representations) of the ensemble of users and distill their knowledge in their personal model using a contrastive objective. For cross-device applications (i.e., small local datasets and limited computational capacity), this approach increases the utility of the models compared to independent learning and other federated knowledge distillation (FD) schemes, is communication efficient and is scalable with the number of clients. We prove theoretically that our framework is well-posed, and we benchmark its performance against standard FD and FL on various datasets using different model architectures.",
            "corpus_id": 253734490,
            "sentences": [
                {
                    "corpus_id": "253734490",
                    "title": "Scalable Collaborative Learning via Representation Sharing",
                    "text": "The concept of knowledge distillation (KD) originated in Bucila et al. [8] as a way of compressing models, and was later generalized by Hinton et al. [19] (see Gou et al. [14] for an overview of the field). The standard use case for KD is that of a Teacher-Student (or offline) configuration, in which the teacher model (usually a large and well-trained model) transfers its knowledge to the student model (usually a smaller model) by sharing its last layer activations on a given transfer dataset (see Fig. 1a). The knowledge is then distilled into the student model using a divergence loss between the teacher and student models outputs (response-based KD) or intermediate layers (feature-based KD) on the transfer dataset. Traditional KD schemes use a transfer set that is similar (or identical) to the teacher training dataset, but some recent work has focused on data-free (or zero-shot) KD. This can be achieved either by looking at some of the teacher model statistics to generate synthetic transfer data [35,39,6], or by training a GAN in parallel [37,10,2]. It has also been shown that positive results can be obtained using mismatched or random unlabeled data for the distillation [27,40]. A key concept for our framework is the idea of online KD (or co-distillation [3], see Fig. 1b), where each model is treated as both a teacher and a student, meaning that the KD is performed synchronously with the training of each model (rather than after the training of the teacher model) [15,57,45]. Finally, ensemble KD (see Fig. 1c) refers to the setup where the knowledge is distilled from an ensemble of teacher (offline) or teacher-student (online) models. \n\nCollaborative Learning via Knowledge Distillation A growing body of literature has recently investigated ways of using online knowledge distillation (KD) [8,19] for collaborative learning in order to alleviate the need of sharing model updates (FL) or individual smashed data (SL).",
                    "score": 0.665603142072542,
                    "section_title": "Knowledge Distillation",
                    "char_start_offset": 6216,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 206
                        },
                        {
                            "start": 207,
                            "end": 512
                        },
                        {
                            "start": 513,
                            "end": 725
                        },
                        {
                            "start": 726,
                            "end": 896
                        },
                        {
                            "start": 897,
                            "end": 1066
                        },
                        {
                            "start": 1067,
                            "end": 1199
                        },
                        {
                            "start": 1200,
                            "end": 1501
                        },
                        {
                            "start": 1502,
                            "end": 1663
                        },
                        {
                            "start": 1666,
                            "end": 1947
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 71,
                            "end": 74,
                            "matchedPaperCorpusId": "11253972"
                        },
                        {
                            "start": 171,
                            "end": 175,
                            "matchedPaperCorpusId": "219559263"
                        },
                        {
                            "start": 1016,
                            "end": 1019,
                            "matchedPaperCorpusId": "159041346"
                        },
                        {
                            "start": 1056,
                            "end": 1060,
                            "matchedPaperCorpusId": "162183830"
                        },
                        {
                            "start": 1060,
                            "end": 1063,
                            "matchedPaperCorpusId": "91183944"
                        },
                        {
                            "start": 1063,
                            "end": 1065,
                            "matchedPaperCorpusId": "209500810"
                        },
                        {
                            "start": 1195,
                            "end": 1198,
                            "matchedPaperCorpusId": "227013462"
                        },
                        {
                            "start": 1490,
                            "end": 1494,
                            "matchedPaperCorpusId": "219965421"
                        },
                        {
                            "start": 1494,
                            "end": 1497,
                            "matchedPaperCorpusId": "26071966"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.97705078125
                }
            ],
            "relevance_judgement": 0.97705078125,
            "relevance_judgment_input_expanded": "# Title: Scalable Collaborative Learning via Representation Sharing\n# Venue: arXiv.org\n# Authors: Fr'ed'eric Berdoz, Abhishek Singh, Martin Jaggi, Ramesh Raskar\n## Abstract\nPrivacy-preserving machine learning has become a key conundrum for multi-party artificial intelligence. Federated learning (FL) and Split Learning (SL) are two frameworks that enable collaborative learning while keeping the data private (on device). In FL, each data holder trains a model locally and releases it to a central server for aggregation. In SL, the clients must release individual cut-layer activations (smashed data) to the server and wait for its response (during both inference and back propagation). While relevant in several settings, both of these schemes have a high communication cost, rely on server-level computation algorithms and do not allow for tunable levels of collaboration. In this work, we present a novel approach for privacy-preserving machine learning, where the clients collaborate via online knowledge distillation using a contrastive loss (contrastive w.r.t. the labels). The goal is to ensure that the participants learn similar features on similar classes without sharing their input data. To do so, each client releases averaged last hidden layer activations of similar labels to a central server that only acts as a relay (i.e., is not involved in the training or aggregation of the models). Then, the clients download these last layer activations (feature representations) of the ensemble of users and distill their knowledge in their personal model using a contrastive objective. For cross-device applications (i.e., small local datasets and limited computational capacity), this approach increases the utility of the models compared to independent learning and other federated knowledge distillation (FD) schemes, is communication efficient and is scalable with the number of clients. We prove theoretically that our framework is well-posed, and we benchmark its performance against standard FD and FL on various datasets using different model architectures.\n## Knowledge Distillation\nThe concept of knowledge distillation (KD) originated in Bucila et al. [8] as a way of compressing models, and was later generalized by Hinton et al. [19] (see Gou et al. [14] for an overview of the field). The standard use case for KD is that of a Teacher-Student (or offline) configuration, in which the teacher model (usually a large and well-trained model) transfers its knowledge to the student model (usually a smaller model) by sharing its last layer activations on a given transfer dataset (see Fig. 1a). The knowledge is then distilled into the student model using a divergence loss between the teacher and student models outputs (response-based KD) or intermediate layers (feature-based KD) on the transfer dataset. Traditional KD schemes use a transfer set that is similar (or identical) to the teacher training dataset, but some recent work has focused on data-free (or zero-shot) KD. This can be achieved either by looking at some of the teacher model statistics to generate synthetic transfer data [35,39,6], or by training a GAN in parallel [37,10,2]. It has also been shown that positive results can be obtained using mismatched or random unlabeled data for the distillation [27,40]. A key concept for our framework is the idea of online KD (or co-distillation [3], see Fig. 1b), where each model is treated as both a teacher and a student, meaning that the KD is performed synchronously with the training of each model (rather than after the training of the teacher model) [15,57,45]. Finally, ensemble KD (see Fig. 1c) refers to the setup where the knowledge is distilled from an ensemble of teacher (offline) or teacher-student (online) models. \n\nCollaborative Learning via Knowledge Distillation A growing body of literature has recently investigated ways of using online knowledge distillation (KD) [8,19] for collaborative learning in order to alleviate the need of sharing model updates (FL) or individual smashed data (SL).",
            "reference_string": "[253734490 | Berdoz et al. | 2022 | Citations: 3]"
        },
        {
            "title": "A Survey on Green Deep Learning",
            "venue": "arXiv.org",
            "year": 2021,
            "reference_count": 393,
            "citation_count": 84,
            "influential_citation_count": 3,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2111.05193, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "47883405",
                    "name": "Jingjing Xu"
                },
                {
                    "authorId": "150341221",
                    "name": "Wangchunshu Zhou"
                },
                {
                    "authorId": "2068057294",
                    "name": "Zhiyi Fu"
                },
                {
                    "authorId": null,
                    "name": "Hao Zhou"
                },
                {
                    "authorId": "2151530622",
                    "name": "Lei Li"
                }
            ],
            "abstract": "In recent years, larger and deeper models are springing up and continuously pushing state-of-the-art (SOTA) results across various fields like natural language processing (NLP) and computer vision (CV). However, despite promising results, it needs to be noted that the computations required by SOTA models have been increased at an exponential rate. Massive computations not only have a surprisingly large carbon footprint but also have negative effects on research inclusiveness and deployment on real-world applications. Green deep learning is an increasingly hot research field that appeals to researchers to pay attention to energy usage and carbon emission during model training and inference. The target is to yield novel results with lightweight and efficient technologies. Many technologies can be used to achieve this goal, like model compression and knowledge distillation. This paper focuses on presenting a systematic review of the development of Green deep learning technologies. We classify these approaches into four categories: (1) compact networks, (2) energy-efficient training strategies, (3) energy-efficient inference approaches, and (4) efficient data usage. For each category, we discuss the progress that has been achieved and the unresolved challenges.",
            "corpus_id": 243861089,
            "sentences": [
                {
                    "corpus_id": "243861089",
                    "title": "A Survey on Green Deep Learning",
                    "text": "The idea of knowledge distillation (KD) is exploiting the knowledge inside a large trained \"teacher\" model to help the training of a \"student\" model (Bucila et al., 2006;Ba & Caruana, 2014;Hinton et al., 2015). In this way, we can use a smaller student model to distill a trained model as a replacement for inference.\n\nThe traditional KD solution is to minimize the difference between the output produced by the teacher model and that produced by the student model. Formally, given a labeled dataset D of N samples D = {(x 1 , y 1 ) , . . . , (x N , y N )}, we can write the loss function of the student network during the process of knowledge distillation as follows:\n\nwhere \u03b1 is a hyper-parameter to control the relative importance of the two terms; \u03b8 T and \u03b8 S are the parameters of teacher T and student S, respectively. L T refers to the task-specific loss and L KD refers to the knowledge distillation loss which measures the similarity of the student and the teacher.\n\nIn general, KD exploits the knowledge from the teacher model to help train the student model by minimizing the discrepancy between the knowledge in the teacher model and that in the student model. According to the source of teacher knowledge, we can classify KD approaches into three categories: logits-based KD, feature-based KD, and relation-based KD. According to teacher types, we also can classify KD approaches into three categories: KD with static teacher, KD with multiple teachers, and KD with dynamic teacher.\n\nLogits-based KD focuses on the output class distribution of the teacher model, also referred as \"soft labels\". This is the vanilla form of knowledge distillation (Hinton et al., 2015;Ba & Caruana, 2014). Soft targets generated by the teacher model provide much more information than hard targets. Therefore, training the student model to fit soft targets can help the student model generalize better like the teacher model.\n\nFeature-based KD exploits intermediate features to teach the student model, which is believed to be important for representation learning (Bengio et al., 2013a). The simplest solution is to minimize",
                    "score": 0.6153421276992278,
                    "section_title": "Knowledge Distillation",
                    "char_start_offset": 86515,
                    "sentence_offsets": [],
                    "ref_mentions": [
                        {
                            "start": 149,
                            "end": 170,
                            "matchedPaperCorpusId": "11253972"
                        },
                        {
                            "start": 170,
                            "end": 189,
                            "matchedPaperCorpusId": "11536917"
                        },
                        {
                            "start": 1680,
                            "end": 1699,
                            "matchedPaperCorpusId": "11536917"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.9765625
                }
            ],
            "relevance_judgement": 0.9765625,
            "relevance_judgment_input_expanded": "# Title: A Survey on Green Deep Learning\n# Venue: arXiv.org\n# Authors: Jingjing Xu, Wangchunshu Zhou, Zhiyi Fu, Hao Zhou, Lei Li\n## Abstract\nIn recent years, larger and deeper models are springing up and continuously pushing state-of-the-art (SOTA) results across various fields like natural language processing (NLP) and computer vision (CV). However, despite promising results, it needs to be noted that the computations required by SOTA models have been increased at an exponential rate. Massive computations not only have a surprisingly large carbon footprint but also have negative effects on research inclusiveness and deployment on real-world applications. Green deep learning is an increasingly hot research field that appeals to researchers to pay attention to energy usage and carbon emission during model training and inference. The target is to yield novel results with lightweight and efficient technologies. Many technologies can be used to achieve this goal, like model compression and knowledge distillation. This paper focuses on presenting a systematic review of the development of Green deep learning technologies. We classify these approaches into four categories: (1) compact networks, (2) energy-efficient training strategies, (3) energy-efficient inference approaches, and (4) efficient data usage. For each category, we discuss the progress that has been achieved and the unresolved challenges.\n## Knowledge Distillation\nThe idea of knowledge distillation (KD) is exploiting the knowledge inside a large trained \"teacher\" model to help the training of a \"student\" model (Bucila et al., 2006;Ba & Caruana, 2014;Hinton et al., 2015). In this way, we can use a smaller student model to distill a trained model as a replacement for inference.\n\nThe traditional KD solution is to minimize the difference between the output produced by the teacher model and that produced by the student model. Formally, given a labeled dataset D of N samples D = {(x 1 , y 1 ) , . . . , (x N , y N )}, we can write the loss function of the student network during the process of knowledge distillation as follows:\n\nwhere \u03b1 is a hyper-parameter to control the relative importance of the two terms; \u03b8 T and \u03b8 S are the parameters of teacher T and student S, respectively. L T refers to the task-specific loss and L KD refers to the knowledge distillation loss which measures the similarity of the student and the teacher.\n\nIn general, KD exploits the knowledge from the teacher model to help train the student model by minimizing the discrepancy between the knowledge in the teacher model and that in the student model. According to the source of teacher knowledge, we can classify KD approaches into three categories: logits-based KD, feature-based KD, and relation-based KD. According to teacher types, we also can classify KD approaches into three categories: KD with static teacher, KD with multiple teachers, and KD with dynamic teacher.\n\nLogits-based KD focuses on the output class distribution of the teacher model, also referred as \"soft labels\". This is the vanilla form of knowledge distillation (Hinton et al., 2015;Ba & Caruana, 2014). Soft targets generated by the teacher model provide much more information than hard targets. Therefore, training the student model to fit soft targets can help the student model generalize better like the teacher model.\n\nFeature-based KD exploits intermediate features to teach the student model, which is believed to be important for representation learning (Bengio et al., 2013a). The simplest solution is to minimize",
            "reference_string": "[243861089 | Xu et al. | 2021 | Citations: 84]"
        },
        {
            "title": "A Machine Learning-Oriented Survey on Tiny Machine Learning",
            "venue": "IEEE Access",
            "year": 2023,
            "reference_count": 176,
            "citation_count": 43,
            "influential_citation_count": 1,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://ieeexplore.ieee.org/ielx7/6287639/6514899/10433185.pdf",
                "status": "GOLD",
                "license": "CCBY",
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2309.11932, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2135267479",
                    "name": "Luigi Capogrosso"
                },
                {
                    "authorId": "1396330675",
                    "name": "Federico Cunico"
                },
                {
                    "authorId": "1780197",
                    "name": "D. Cheng"
                },
                {
                    "authorId": "2243336023",
                    "name": "Franco Fummi"
                },
                {
                    "authorId": "2238815087",
                    "name": "Marco Cristani"
                }
            ],
            "abstract": "The emergence of Tiny Machine Learning (TinyML) has positively revolutionized the field of Artificial Intelligence by promoting the joint design of resource-constrained IoT hardware devices and their learning-based software architectures. TinyML carries an essential role within the fourth and fifth industrial revolutions in helping societies, economies, and individuals employ effective AI-infused computing technologies (e.g., smart cities, automotive, and medical robotics). Given its multidisciplinary nature, the field of TinyML has been approached from many different angles: this comprehensive survey wishes to provide an up-to-date overview focused on all the learning algorithms within TinyML-based solutions. The survey is based on the Preferred Reporting Items for Systematic Reviews and Meta-Analyses (PRISMA) methodological flow, allowing for a systematic and complete literature survey. In particular, firstly, we will examine the three different workflows for implementing a TinyML-based system, i.e., ML-oriented, HW-oriented, and co-design. Secondly, we propose a taxonomy that covers the learning panorama under the TinyML lens, examining in detail the different families of model optimization and design, as well as the state-of-the-art learning techniques. Thirdly, this survey will present the distinct features of hardware devices and software tools that represent the current state-of-the-art for TinyML intelligent edge applications. Finally, we discuss the challenges and future directions.",
            "corpus_id": 262084420,
            "sentences": [
                {
                    "corpus_id": "262084420",
                    "title": "A Machine Learning-Oriented Survey on Tiny Machine Learning",
                    "text": "This technique transfers knowledge from a large, complex model (teacher) to a smaller, simpler model (student) [84]. This process is important for various reasons, such as reducing computational demands or enhancing model performance on specific tasks. Knowledge types, distillation strategies, and teacher-student architectures are vital factors in student learning during knowledge distillation. The subsequent paragraphs introduce the key categories of knowledge types and distillation strategies. \n\nThe extraction of knowledge from teachers and its utilization for training student networks can be classified into three categories: response-based, feature-based, and relationbased. Specifically, response-based knowledge distillation involves mimicking the final predictions of the teacher model by capturing the neural response in the last output layer [87]. Feature-based knowledge expands upon this approach by using both the outputs of the last layer and intermediate layers to train thinner networks [81]. Finally, relation-based knowledge takes a step further by exploring the relationships between different layers or data samples in addition to the outputs of specific layers in the teacher model [86]. \n\nThe distillation schemes are also crucial for the student learning process. Depending on the training strategy, the following three different categories are presented: offline distillation, online distillation, self-distillation. Offline distillation is a two-stage strategy, where the teacher model is first trained on a set of training samples, and then the trained teacher model is used to guide the student model by extracting intermediate features or logits [80]. On the other hand, online distillation is an end-to-end approach where both the teacher and student models are updated simultaneously, making it suitable when the teacher model is not significantly larger or higher performing [85]. Finally, self-distillation is a special case of online distillation where the teacher and student networks have the same architecture [79]. \n\nIn general, knowledge distillation is used to achieve a good trade-off between small model size and an acceptable accuracy [88]. For this reason, it is widely adopted in several fields where existing models are well-performing but unable to be deployed ''as they are'' in resource-constrained hardware.",
                    "score": 0.7547895383867367,
                    "section_title": "3) Knowledge Distillation",
                    "char_start_offset": 28218,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 116
                        },
                        {
                            "start": 117,
                            "end": 252
                        },
                        {
                            "start": 253,
                            "end": 397
                        },
                        {
                            "start": 398,
                            "end": 500
                        },
                        {
                            "start": 503,
                            "end": 685
                        },
                        {
                            "start": 686,
                            "end": 863
                        },
                        {
                            "start": 864,
                            "end": 1014
                        },
                        {
                            "start": 1015,
                            "end": 1214
                        },
                        {
                            "start": 1217,
                            "end": 1292
                        },
                        {
                            "start": 1293,
                            "end": 1446
                        },
                        {
                            "start": 1447,
                            "end": 1685
                        },
                        {
                            "start": 1686,
                            "end": 1917
                        },
                        {
                            "start": 1918,
                            "end": 2057
                        },
                        {
                            "start": 2060,
                            "end": 2188
                        },
                        {
                            "start": 2189,
                            "end": 2362
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 111,
                            "end": 115,
                            "matchedPaperCorpusId": "219559263"
                        },
                        {
                            "start": 858,
                            "end": 862,
                            "matchedPaperCorpusId": "232104927"
                        },
                        {
                            "start": 1009,
                            "end": 1013,
                            "matchedPaperCorpusId": "235613518"
                        },
                        {
                            "start": 1209,
                            "end": 1213,
                            "matchedPaperCorpusId": "239486869"
                        },
                        {
                            "start": 1680,
                            "end": 1684,
                            "matchedPaperCorpusId": "198179767"
                        },
                        {
                            "start": 1912,
                            "end": 1916,
                            "matchedPaperCorpusId": "224914013"
                        },
                        {
                            "start": 2052,
                            "end": 2056,
                            "matchedPaperCorpusId": "214727822"
                        },
                        {
                            "start": 2183,
                            "end": 2187,
                            "matchedPaperCorpusId": "255266316"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.97607421875
                }
            ],
            "relevance_judgement": 0.97607421875,
            "relevance_judgment_input_expanded": "# Title: A Machine Learning-Oriented Survey on Tiny Machine Learning\n# Venue: IEEE Access\n# Authors: Luigi Capogrosso, Federico Cunico, D. Cheng, Franco Fummi, Marco Cristani\n## Abstract\nThe emergence of Tiny Machine Learning (TinyML) has positively revolutionized the field of Artificial Intelligence by promoting the joint design of resource-constrained IoT hardware devices and their learning-based software architectures. TinyML carries an essential role within the fourth and fifth industrial revolutions in helping societies, economies, and individuals employ effective AI-infused computing technologies (e.g., smart cities, automotive, and medical robotics). Given its multidisciplinary nature, the field of TinyML has been approached from many different angles: this comprehensive survey wishes to provide an up-to-date overview focused on all the learning algorithms within TinyML-based solutions. The survey is based on the Preferred Reporting Items for Systematic Reviews and Meta-Analyses (PRISMA) methodological flow, allowing for a systematic and complete literature survey. In particular, firstly, we will examine the three different workflows for implementing a TinyML-based system, i.e., ML-oriented, HW-oriented, and co-design. Secondly, we propose a taxonomy that covers the learning panorama under the TinyML lens, examining in detail the different families of model optimization and design, as well as the state-of-the-art learning techniques. Thirdly, this survey will present the distinct features of hardware devices and software tools that represent the current state-of-the-art for TinyML intelligent edge applications. Finally, we discuss the challenges and future directions.\n## 3) Knowledge Distillation\nThis technique transfers knowledge from a large, complex model (teacher) to a smaller, simpler model (student) [84]. This process is important for various reasons, such as reducing computational demands or enhancing model performance on specific tasks. Knowledge types, distillation strategies, and teacher-student architectures are vital factors in student learning during knowledge distillation. The subsequent paragraphs introduce the key categories of knowledge types and distillation strategies. \n\nThe extraction of knowledge from teachers and its utilization for training student networks can be classified into three categories: response-based, feature-based, and relationbased. Specifically, response-based knowledge distillation involves mimicking the final predictions of the teacher model by capturing the neural response in the last output layer [87]. Feature-based knowledge expands upon this approach by using both the outputs of the last layer and intermediate layers to train thinner networks [81]. Finally, relation-based knowledge takes a step further by exploring the relationships between different layers or data samples in addition to the outputs of specific layers in the teacher model [86]. \n\nThe distillation schemes are also crucial for the student learning process. Depending on the training strategy, the following three different categories are presented: offline distillation, online distillation, self-distillation. Offline distillation is a two-stage strategy, where the teacher model is first trained on a set of training samples, and then the trained teacher model is used to guide the student model by extracting intermediate features or logits [80]. On the other hand, online distillation is an end-to-end approach where both the teacher and student models are updated simultaneously, making it suitable when the teacher model is not significantly larger or higher performing [85]. Finally, self-distillation is a special case of online distillation where the teacher and student networks have the same architecture [79]. \n\nIn general, knowledge distillation is used to achieve a good trade-off between small model size and an acceptable accuracy [88]. For this reason, it is widely adopted in several fields where existing models are well-performing but unable to be deployed ''as they are'' in resource-constrained hardware.",
            "reference_string": "[262084420 | Capogrosso et al. | 2023 | Citations: 43]"
        },
        {
            "title": "Quantifying Knowledge Distillation Using Partial Information Decomposition",
            "venue": "arXiv.org",
            "year": 2024,
            "reference_count": 51,
            "citation_count": 1,
            "influential_citation_count": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2411.07483, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2091816108",
                    "name": "Pasan Dissanayake"
                },
                {
                    "authorId": "2190039006",
                    "name": "Faisal Hamman"
                },
                {
                    "authorId": "2209644696",
                    "name": "Barproda Halder"
                },
                {
                    "authorId": "2226897111",
                    "name": "Ilia Sucholutsky"
                },
                {
                    "authorId": "2261249944",
                    "name": "Qiuyi Zhang"
                },
                {
                    "authorId": "14579807",
                    "name": "Sanghamitra Dutta"
                }
            ],
            "abstract": "Knowledge distillation deploys complex machine learning models in resource-constrained environments by training a smaller student model to emulate internal representations of a complex teacher model. However, the teacher's representations can also encode nuisance or additional information not relevant to the downstream task. Distilling such irrelevant information can actually impede the performance of a capacity-limited student model. This observation motivates our primary question: What are the information-theoretic limits of knowledge distillation? To this end, we leverage Partial Information Decomposition to quantify and explain the transferred knowledge and knowledge left to distill for a downstream task. We theoretically demonstrate that the task-relevant transferred knowledge is succinctly captured by the measure of redundant information about the task between the teacher and student. We propose a novel multi-level optimization to incorporate redundant information as a regularizer, leading to our framework of Redundant Information Distillation (RID). RID leads to more resilient and effective distillation under nuisance teachers as it succinctly quantifies task-relevant knowledge rather than simply aligning student and teacher representations.",
            "corpus_id": 273969804,
            "sentences": [
                {
                    "corpus_id": "273969804",
                    "title": "Quantifying Knowledge Distillation Using Partial Information Decomposition",
                    "text": "Modern-day machine learning requires large amounts of compute for both training and inference. Knowledge distillation [1,2] can be used to compress a complex machine learning model (the teacher) by distilling it into a relatively simpler model (the student). The term \"distillation\" in this context means obtaining some assistance from the teacher while training the student so that the student performs much better than when trained alone (see Figure 1). In its earliest forms, knowledge distillation involved the student trying to match the output logits of the teacher [1]. More advanced methods focus on distilling multiple intermediate representations of the teacher to the corresponding layers of the student [2][3][4][5]. We also refer the reader to [6,7] for surveys. \n\nInformation theory has been instrumental in both designing [3,4] and explaining [8,9] knowledge distillation techniques. However, less attention has been given to characterizing the fundamental limits of the process from an information-theoretical perspective. Our goal is to bridge this gap by first introducing new measures to quantify the \"transferred knowledge\" and \"knowledge to distill\" for a teacher and a student model given a target downstream task. We bring in an emerging body of work called Partial Information Decomposition (PID) [10][11][12] to explain knowledge distillation. We define the knowledge to distill using the PID measure of \"unique\" information about the task that is available only with the teacher but not the student. As it follows, the transferred knowledge is succinctly quantified by the measure of \"redundant\" information that is common between the teacher and student. \n\nWe propose a multi-level optimization that maximizes redundant information (transferred knowledge) as a regularizer for more effective distillation. While PID has been explored in a few avenues of machine learning, Figure 1: Knowledge Distillation: The teacher (a complex model) assists the student (usually a substantially simpler model) during their training. The learned student can perform much better than an independently trained student without distillation with a similar training setup (i.e., hyperparameters and data). The teacher may or may not have been trained for the same task as the student.",
                    "score": 0.6342960030505805,
                    "section_title": "Introduction",
                    "char_start_offset": 15,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 94
                        },
                        {
                            "start": 95,
                            "end": 258
                        },
                        {
                            "start": 259,
                            "end": 455
                        },
                        {
                            "start": 456,
                            "end": 576
                        },
                        {
                            "start": 577,
                            "end": 728
                        },
                        {
                            "start": 729,
                            "end": 775
                        },
                        {
                            "start": 778,
                            "end": 898
                        },
                        {
                            "start": 899,
                            "end": 1038
                        },
                        {
                            "start": 1039,
                            "end": 1236
                        },
                        {
                            "start": 1237,
                            "end": 1368
                        },
                        {
                            "start": 1369,
                            "end": 1525
                        },
                        {
                            "start": 1526,
                            "end": 1681
                        },
                        {
                            "start": 1684,
                            "end": 1832
                        },
                        {
                            "start": 1833,
                            "end": 2045
                        },
                        {
                            "start": 2046,
                            "end": 2212
                        },
                        {
                            "start": 2213,
                            "end": 2291
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 121,
                            "end": 123,
                            "matchedPaperCorpusId": "2723173"
                        },
                        {
                            "start": 715,
                            "end": 718,
                            "matchedPaperCorpusId": "2723173"
                        },
                        {
                            "start": 718,
                            "end": 721,
                            "matchedPaperCorpusId": "118649278"
                        },
                        {
                            "start": 721,
                            "end": 724,
                            "matchedPaperCorpusId": "204838340"
                        },
                        {
                            "start": 724,
                            "end": 727,
                            "matchedPaperCorpusId": "252693152"
                        },
                        {
                            "start": 757,
                            "end": 760,
                            "matchedPaperCorpusId": "219559263"
                        },
                        {
                            "start": 837,
                            "end": 840,
                            "matchedPaperCorpusId": "118649278"
                        },
                        {
                            "start": 840,
                            "end": 842,
                            "matchedPaperCorpusId": "204838340"
                        },
                        {
                            "start": 858,
                            "end": 861,
                            "matchedPaperCorpusId": "251643827"
                        },
                        {
                            "start": 861,
                            "end": 863,
                            "matchedPaperCorpusId": "252846591"
                        },
                        {
                            "start": 1325,
                            "end": 1329,
                            "matchedPaperCorpusId": "1031742"
                        },
                        {
                            "start": 1329,
                            "end": 1333,
                            "matchedPaperCorpusId": "10901107"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.97607421875
                }
            ],
            "relevance_judgement": 0.97607421875,
            "relevance_judgment_input_expanded": "# Title: Quantifying Knowledge Distillation Using Partial Information Decomposition\n# Venue: arXiv.org\n# Authors: Pasan Dissanayake, Faisal Hamman, Barproda Halder, Ilia Sucholutsky, Qiuyi Zhang, Sanghamitra Dutta\n## Abstract\nKnowledge distillation deploys complex machine learning models in resource-constrained environments by training a smaller student model to emulate internal representations of a complex teacher model. However, the teacher's representations can also encode nuisance or additional information not relevant to the downstream task. Distilling such irrelevant information can actually impede the performance of a capacity-limited student model. This observation motivates our primary question: What are the information-theoretic limits of knowledge distillation? To this end, we leverage Partial Information Decomposition to quantify and explain the transferred knowledge and knowledge left to distill for a downstream task. We theoretically demonstrate that the task-relevant transferred knowledge is succinctly captured by the measure of redundant information about the task between the teacher and student. We propose a novel multi-level optimization to incorporate redundant information as a regularizer, leading to our framework of Redundant Information Distillation (RID). RID leads to more resilient and effective distillation under nuisance teachers as it succinctly quantifies task-relevant knowledge rather than simply aligning student and teacher representations.\n## Introduction\nModern-day machine learning requires large amounts of compute for both training and inference. Knowledge distillation [1,2] can be used to compress a complex machine learning model (the teacher) by distilling it into a relatively simpler model (the student). The term \"distillation\" in this context means obtaining some assistance from the teacher while training the student so that the student performs much better than when trained alone (see Figure 1). In its earliest forms, knowledge distillation involved the student trying to match the output logits of the teacher [1]. More advanced methods focus on distilling multiple intermediate representations of the teacher to the corresponding layers of the student [2][3][4][5]. We also refer the reader to [6,7] for surveys. \n\nInformation theory has been instrumental in both designing [3,4] and explaining [8,9] knowledge distillation techniques. However, less attention has been given to characterizing the fundamental limits of the process from an information-theoretical perspective. Our goal is to bridge this gap by first introducing new measures to quantify the \"transferred knowledge\" and \"knowledge to distill\" for a teacher and a student model given a target downstream task. We bring in an emerging body of work called Partial Information Decomposition (PID) [10][11][12] to explain knowledge distillation. We define the knowledge to distill using the PID measure of \"unique\" information about the task that is available only with the teacher but not the student. As it follows, the transferred knowledge is succinctly quantified by the measure of \"redundant\" information that is common between the teacher and student. \n\nWe propose a multi-level optimization that maximizes redundant information (transferred knowledge) as a regularizer for more effective distillation. While PID has been explored in a few avenues of machine learning, Figure 1: Knowledge Distillation: The teacher (a complex model) assists the student (usually a substantially simpler model) during their training. The learned student can perform much better than an independently trained student without distillation with a similar training setup (i.e., hyperparameters and data). The teacher may or may not have been trained for the same task as the student.",
            "reference_string": "[273969804 | Dissanayake et al. | 2024 | Citations: 1]"
        },
        {
            "title": "Semi-Online Knowledge Distillation",
            "venue": "British Machine Vision Conference",
            "year": 2021,
            "reference_count": 45,
            "citation_count": 5,
            "influential_citation_count": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2111.11747, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2118809537",
                    "name": "Zhiqiang Liu"
                },
                {
                    "authorId": "2130679830",
                    "name": "Yanxia Liu"
                },
                {
                    "authorId": "2150607873",
                    "name": "Chengkai Huang"
                }
            ],
            "abstract": "Knowledge distillation is an effective and stable method for model compression via knowledge transfer. Conventional knowledge distillation (KD) is to transfer knowledge from a large and well pre-trained teacher network to a small student network, which is a one-way process. Recently, deep mutual learning (DML) has been proposed to help student networks learn collaboratively and simultaneously. However, to the best of our knowledge, KD and DML have never been jointly explored in a unified framework to solve the knowledge distillation problem. In this paper, we investigate that the teacher model supports more trustworthy supervision signals in KD, while the student captures more similar behaviors from the teacher in DML. Based on these observations, we first propose to combine KD with DML in a unified framework. Furthermore, we propose a Semi-Online Knowledge Distillation (SOKD) method that effectively improves the performance of the student and the teacher. In this method, we introduce the peer-teaching training fashion in DML in order to alleviate the student's imitation difficulty, and also leverage the supervision signals provided by the well-trained teacher in KD. Besides, we also show our framework can be easily extended to feature-based distillation methods. Extensive experiments on CIFAR-100 and ImageNet datasets demonstrate the proposed method achieves state-of-the-art performance.",
            "corpus_id": 244488325,
            "sentences": [
                {
                    "corpus_id": "244488325",
                    "title": "Semi-Online Knowledge Distillation",
                    "text": "Knowledge distillation is an effective and stable method for model compression via knowledge transfer. Conventional knowledge distillation (KD) is to transfer knowledge from a large and well pre-trained teacher network to a small student network, which is a one-way process. Recently, deep mutual learning (DML) has been proposed to help student networks learn collaboratively and simultaneously. However, to the best of our knowledge, KD and DML have never been jointly explored in a unified framework to solve the knowledge distillation problem. In this paper, we investigate that the teacher model supports more trustworthy supervision signals in KD, while the student captures more similar behaviors from the teacher in DML. Based on these observations, we first propose to combine KD with DML in a unified framework. Furthermore, we propose a Semi-Online Knowledge Distillation (SOKD) method that effectively improves the performance of the student and the teacher. In this method, we introduce the peer-teaching training fashion in DML in order to alleviate the student's imitation difficulty, and also leverage the supervision signals provided by the well-trained teacher in KD. Besides, we also show our framework can be easily extended to feature-based distillation methods. Extensive experiments on CIFAR-100 and ImageNet datasets demonstrate the proposed method achieves state-of-the-art performance.",
                    "score": 0.6189571569114636,
                    "section_title": "abstract",
                    "char_start_offset": 0,
                    "sentence_offsets": [],
                    "ref_mentions": [],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.97607421875
                }
            ],
            "relevance_judgement": 0.97607421875,
            "relevance_judgment_input_expanded": "# Title: Semi-Online Knowledge Distillation\n# Venue: British Machine Vision Conference\n# Authors: Zhiqiang Liu, Yanxia Liu, Chengkai Huang\n## Abstract\nKnowledge distillation is an effective and stable method for model compression via knowledge transfer. Conventional knowledge distillation (KD) is to transfer knowledge from a large and well pre-trained teacher network to a small student network, which is a one-way process. Recently, deep mutual learning (DML) has been proposed to help student networks learn collaboratively and simultaneously. However, to the best of our knowledge, KD and DML have never been jointly explored in a unified framework to solve the knowledge distillation problem. In this paper, we investigate that the teacher model supports more trustworthy supervision signals in KD, while the student captures more similar behaviors from the teacher in DML. Based on these observations, we first propose to combine KD with DML in a unified framework. Furthermore, we propose a Semi-Online Knowledge Distillation (SOKD) method that effectively improves the performance of the student and the teacher. In this method, we introduce the peer-teaching training fashion in DML in order to alleviate the student's imitation difficulty, and also leverage the supervision signals provided by the well-trained teacher in KD. Besides, we also show our framework can be easily extended to feature-based distillation methods. Extensive experiments on CIFAR-100 and ImageNet datasets demonstrate the proposed method achieves state-of-the-art performance.\n",
            "reference_string": "[244488325 | Liu et al. | 2021 | Citations: 5]"
        },
        {
            "title": "Informed Learning by Wide Neural Networks: Convergence, Generalization and Sampling Complexity",
            "venue": "International Conference on Machine Learning",
            "year": 2022,
            "reference_count": 101,
            "citation_count": 3,
            "influential_citation_count": 0,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "http://arxiv.org/pdf/2207.00751",
                "status": "GREEN",
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2207.00751, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "51284511",
                    "name": "Jianyi Yang"
                },
                {
                    "authorId": "2075290953",
                    "name": "Shaolei Ren"
                }
            ],
            "abstract": "By integrating domain knowledge with labeled samples, informed machine learning has been emerging to improve the learning performance for a wide range of applications. Nonetheless, rigorous understanding of the role of injected domain knowledge has been under-explored. In this paper, we consider an informed deep neural network (DNN) with over-parameterization and domain knowledge integrated into its training objective function, and study how and why domain knowledge benefits the performance. Concretely, we quantitatively demonstrate the two benefits of domain knowledge in informed learning - regularizing the label-based supervision and supplementing the labeled samples - and reveal the trade-off between label and knowledge imperfectness in the bound of the population risk. Based on the theoretical analysis, we propose a generalized informed training objective to better exploit the benefits of knowledge and balance the label and knowledge imperfectness, which is validated by the population risk bound. Our analysis on sampling complexity sheds lights on how to choose the hyper-parameters for informed learning, and further justifies the advantages of knowledge informed learning.",
            "corpus_id": 250264223,
            "sentences": [
                {
                    "corpus_id": "250264223",
                    "title": "Informed Learning by Wide Neural Networks: Convergence, Generalization and Sampling Complexity",
                    "text": "Knowledge distillation (Hinton et al., 2014;Furlanello et al., 2018;Phuong & Lampert, 2019;Allen-Zhu & Li, 2020) is an important technique to transfer prior knowledge from a pre-trained neural network (a.k.a. teacher network) to another network (a.k.a. student network), with the same or different architectures. Typically, given an (possibly unlabled) input, knowledge distillation is performed by matching the output of the student network with the output of the teacher network. In addition, labeled samples can also be included to introduce a label-based loss. Thus, by formulating g(X) as the output of the teacher network, knowledge distillation can be viewed as a particular instance of informed machine learning, where the knowledge comes from a teacher network and is usually assumed to be perfect.",
                    "score": 0.7284699649743361,
                    "section_title": "F.4. Knowledge distillation and transfer",
                    "char_start_offset": 71442,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 208
                        },
                        {
                            "start": 209,
                            "end": 252
                        },
                        {
                            "start": 253,
                            "end": 312
                        },
                        {
                            "start": 313,
                            "end": 481
                        },
                        {
                            "start": 482,
                            "end": 564
                        },
                        {
                            "start": 565,
                            "end": 807
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 23,
                            "end": 44,
                            "matchedPaperCorpusId": "7200347"
                        },
                        {
                            "start": 44,
                            "end": 68,
                            "matchedPaperCorpusId": "4110009"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.9755859375
                }
            ],
            "relevance_judgement": 0.9755859375,
            "relevance_judgment_input_expanded": "# Title: Informed Learning by Wide Neural Networks: Convergence, Generalization and Sampling Complexity\n# Venue: International Conference on Machine Learning\n# Authors: Jianyi Yang, Shaolei Ren\n## Abstract\nBy integrating domain knowledge with labeled samples, informed machine learning has been emerging to improve the learning performance for a wide range of applications. Nonetheless, rigorous understanding of the role of injected domain knowledge has been under-explored. In this paper, we consider an informed deep neural network (DNN) with over-parameterization and domain knowledge integrated into its training objective function, and study how and why domain knowledge benefits the performance. Concretely, we quantitatively demonstrate the two benefits of domain knowledge in informed learning - regularizing the label-based supervision and supplementing the labeled samples - and reveal the trade-off between label and knowledge imperfectness in the bound of the population risk. Based on the theoretical analysis, we propose a generalized informed training objective to better exploit the benefits of knowledge and balance the label and knowledge imperfectness, which is validated by the population risk bound. Our analysis on sampling complexity sheds lights on how to choose the hyper-parameters for informed learning, and further justifies the advantages of knowledge informed learning.\n## F.4. Knowledge distillation and transfer\nKnowledge distillation (Hinton et al., 2014;Furlanello et al., 2018;Phuong & Lampert, 2019;Allen-Zhu & Li, 2020) is an important technique to transfer prior knowledge from a pre-trained neural network (a.k.a. teacher network) to another network (a.k.a. student network), with the same or different architectures. Typically, given an (possibly unlabled) input, knowledge distillation is performed by matching the output of the student network with the output of the teacher network. In addition, labeled samples can also be included to introduce a label-based loss. Thus, by formulating g(X) as the output of the teacher network, knowledge distillation can be viewed as a particular instance of informed machine learning, where the knowledge comes from a teacher network and is usually assumed to be perfect.",
            "reference_string": "[250264223 | Yang et al. | 2022 | Citations: 3]"
        },
        {
            "title": "Dual-Head Knowledge Distillation: Enhancing Logits Utilization with an Auxiliary Head",
            "venue": "arXiv.org",
            "year": 2024,
            "reference_count": 51,
            "citation_count": 1,
            "influential_citation_count": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2411.08937, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2330846022",
                    "name": "Peng-Hui Yang"
                },
                {
                    "authorId": "2123355687",
                    "name": "Chen-Chen Zong"
                },
                {
                    "authorId": "2316442907",
                    "name": "Shengjun Huang"
                },
                {
                    "authorId": "2269837261",
                    "name": "Lei Feng"
                },
                {
                    "authorId": "2346984226",
                    "name": "Bo An"
                }
            ],
            "abstract": "Traditional knowledge distillation focuses on aligning the student's predicted probabilities with both ground-truth labels and the teacher's predicted probabilities. However, the transition to predicted probabilities from logits would obscure certain indispensable information. To address this issue, it is intuitive to additionally introduce a logit-level loss function as a supplement to the widely used probability-level loss function, for exploiting the latent information of logits. Unfortunately, we empirically find that the amalgamation of the newly introduced logit-level loss and the previous probability-level loss will lead to performance degeneration, even trailing behind the performance of employing either loss in isolation. We attribute this phenomenon to the collapse of the classification head, which is verified by our theoretical analysis based on the neural collapse theory. Specifically, the gradients of the two loss functions exhibit contradictions in the linear classifier yet display no such conflict within the backbone. Drawing from the theoretical analysis, we propose a novel method called dual-head knowledge distillation, which partitions the linear classifier into two classification heads responsible for different losses, thereby preserving the beneficial effects of both losses on the backbone while eliminating adverse influences on the classification head. Extensive experiments validate that our method can effectively exploit the information inside the logits and achieve superior performance against state-of-the-art counterparts. Our code is available at: https://github.com/penghui-yang/DHKD.",
            "corpus_id": 274023578,
            "sentences": [
                {
                    "corpus_id": "274023578",
                    "title": "Dual-Head Knowledge Distillation: Enhancing Logits Utilization with an Auxiliary Head",
                    "text": "2.1 KNOWLEDGE DISTILLATION Knowledge distillation (KD) (Hinton et al., 2015) aims to transfer knowledge from a large teacher network to a small student network. Existing works can be roughly divided into two groups: featurebased methods and logit-based methods. \n\nFeature-based methods focus on distilling knowledge from intermediate feature layers. FitNet (Romero et al., 2015) is the first approach to distill knowledge from intermediate features by measuring the distance between feature maps. RKD (Park et al., 2019) utilizes the relations among instances to guide the training process of the student model. CRD (Tian et al., 2019) incorporates contrastive learning into knowledge distillation. OFD (Heo et al., 2019) contains a new distance function to distill significant information between the teacher and student using marginal ReLU. \n\nReviewKD (Chen et al., 2021) proposes a review mechanism that uses multiple layers in the teacher to supervise one layer in the student. Other papers (Passalis & Tefas, 2018;Kim et al., 2018;Koratana et al., 2019;Li, 2022;Liu et al., 2023;Wang et al., 2023;Roy Miles & Deng, 2024;Miles & Mikolajczyk, 2024) enforce various criteria based on features. Most feature-based methods can attain superior performance, yet involving considerably high computational and storage costs. \n\nLogit-based methods mainly concentrate on distilling knowledge from logits and softmax scores after logits. DML (Zhang et al., 2018b) introduces a mutual learning method to train both teachers and students simultaneously. DKD (Zhao et al., 2022) proposes a novel logit-based method to reformulate the classical KD loss into two parts and achieves state-of-the-art performance by adjusting weights for these two parts. DIST (Huang et al., 2022) relaxes the exact matching in previous KL divergence loss with a correlation-based loss and performs better when the discrepancy between the teacher and the student is large.",
                    "score": 0.6231989142715317,
                    "section_title": "RELATED WORK",
                    "char_start_offset": 4677,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 160
                        },
                        {
                            "start": 161,
                            "end": 261
                        },
                        {
                            "start": 264,
                            "end": 349
                        },
                        {
                            "start": 350,
                            "end": 496
                        },
                        {
                            "start": 497,
                            "end": 611
                        },
                        {
                            "start": 612,
                            "end": 698
                        },
                        {
                            "start": 699,
                            "end": 842
                        },
                        {
                            "start": 845,
                            "end": 981
                        },
                        {
                            "start": 982,
                            "end": 1195
                        },
                        {
                            "start": 1196,
                            "end": 1320
                        },
                        {
                            "start": 1323,
                            "end": 1430
                        },
                        {
                            "start": 1431,
                            "end": 1544
                        },
                        {
                            "start": 1545,
                            "end": 1740
                        },
                        {
                            "start": 1741,
                            "end": 1941
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 501,
                            "end": 520,
                            "matchedPaperCorpusId": "131765296"
                        },
                        {
                            "start": 616,
                            "end": 635,
                            "matchedPaperCorpusId": "204838340"
                        },
                        {
                            "start": 703,
                            "end": 721,
                            "matchedPaperCorpusId": "102483181"
                        },
                        {
                            "start": 854,
                            "end": 873,
                            "matchedPaperCorpusId": "233296935"
                        },
                        {
                            "start": 995,
                            "end": 1019,
                            "matchedPaperCorpusId": "52012952"
                        },
                        {
                            "start": 1036,
                            "end": 1058,
                            "matchedPaperCorpusId": "172133986"
                        },
                        {
                            "start": 1102,
                            "end": 1125,
                            "matchedPaperCorpusId": "268358422"
                        },
                        {
                            "start": 1125,
                            "end": 1151,
                            "matchedPaperCorpusId": "257632008"
                        },
                        {
                            "start": 1435,
                            "end": 1456,
                            "matchedPaperCorpusId": "26071966"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.9755859375
                }
            ],
            "relevance_judgement": 0.9755859375,
            "relevance_judgment_input_expanded": "# Title: Dual-Head Knowledge Distillation: Enhancing Logits Utilization with an Auxiliary Head\n# Venue: arXiv.org\n# Authors: Peng-Hui Yang, Chen-Chen Zong, Shengjun Huang, Lei Feng, Bo An\n## Abstract\nTraditional knowledge distillation focuses on aligning the student's predicted probabilities with both ground-truth labels and the teacher's predicted probabilities. However, the transition to predicted probabilities from logits would obscure certain indispensable information. To address this issue, it is intuitive to additionally introduce a logit-level loss function as a supplement to the widely used probability-level loss function, for exploiting the latent information of logits. Unfortunately, we empirically find that the amalgamation of the newly introduced logit-level loss and the previous probability-level loss will lead to performance degeneration, even trailing behind the performance of employing either loss in isolation. We attribute this phenomenon to the collapse of the classification head, which is verified by our theoretical analysis based on the neural collapse theory. Specifically, the gradients of the two loss functions exhibit contradictions in the linear classifier yet display no such conflict within the backbone. Drawing from the theoretical analysis, we propose a novel method called dual-head knowledge distillation, which partitions the linear classifier into two classification heads responsible for different losses, thereby preserving the beneficial effects of both losses on the backbone while eliminating adverse influences on the classification head. Extensive experiments validate that our method can effectively exploit the information inside the logits and achieve superior performance against state-of-the-art counterparts. Our code is available at: https://github.com/penghui-yang/DHKD.\n## RELATED WORK\n2.1 KNOWLEDGE DISTILLATION Knowledge distillation (KD) (Hinton et al., 2015) aims to transfer knowledge from a large teacher network to a small student network. Existing works can be roughly divided into two groups: featurebased methods and logit-based methods. \n\nFeature-based methods focus on distilling knowledge from intermediate feature layers. FitNet (Romero et al., 2015) is the first approach to distill knowledge from intermediate features by measuring the distance between feature maps. RKD (Park et al., 2019) utilizes the relations among instances to guide the training process of the student model. CRD (Tian et al., 2019) incorporates contrastive learning into knowledge distillation. OFD (Heo et al., 2019) contains a new distance function to distill significant information between the teacher and student using marginal ReLU. \n\nReviewKD (Chen et al., 2021) proposes a review mechanism that uses multiple layers in the teacher to supervise one layer in the student. Other papers (Passalis & Tefas, 2018;Kim et al., 2018;Koratana et al., 2019;Li, 2022;Liu et al., 2023;Wang et al., 2023;Roy Miles & Deng, 2024;Miles & Mikolajczyk, 2024) enforce various criteria based on features. Most feature-based methods can attain superior performance, yet involving considerably high computational and storage costs. \n\nLogit-based methods mainly concentrate on distilling knowledge from logits and softmax scores after logits. DML (Zhang et al., 2018b) introduces a mutual learning method to train both teachers and students simultaneously. DKD (Zhao et al., 2022) proposes a novel logit-based method to reformulate the classical KD loss into two parts and achieves state-of-the-art performance by adjusting weights for these two parts. DIST (Huang et al., 2022) relaxes the exact matching in previous KL divergence loss with a correlation-based loss and performs better when the discrepancy between the teacher and the student is large.",
            "reference_string": "[274023578 | Yang et al. | 2024 | Citations: 1]"
        },
        {
            "title": "Prototypes Sampling Mechanism for Class Incremental Learning",
            "venue": "IEEE Access",
            "year": 2023,
            "reference_count": 46,
            "citation_count": 3,
            "influential_citation_count": 0,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://ieeexplore.ieee.org/ielx7/6287639/6514899/10201868.pdf",
                "status": "GOLD",
                "license": "CCBYNCND",
                "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.1109/ACCESS.2023.3301123?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1109/ACCESS.2023.3301123, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2213939405",
                    "name": "Zhe Tao"
                },
                {
                    "authorId": "50178505",
                    "name": "Shucheng Huang"
                },
                {
                    "authorId": "2155613761",
                    "name": "Gang Wang"
                }
            ],
            "abstract": "Incremental learning aims to alleviate the catastrophic forgetting problem of deep neural networks during learning sequential data stream. This problem is even more challenging when old data is unavailable, since learning system can only be trained under the supervision of current data. To address this problem, we proposed a prototype sampling mechanism based on K-means clustering method. On the one hand, we proposed to use K-means clustering to pick out class-representative prototypes for each old class. During incremental stages, prototypes and deep features from current data are trained together to maintain the distinction and balance between old and new classes. On the other hand, we proposed to attach a mask to the loss function based on the cosine similarity between the prototypes and the current data. Which further enhances the discrimination between old and new classes compared to naive knowledge distillation schemes. Extensive experiments conducted on three benchmark datasets including CIFAR100, Tiny-ImageNet and vggface2 verified the effectiveness and advantages of our proposed method. Specifically, we improved class incremental performance by 1.6%, 1.2% and 1.7% on three datasets respectively.",
            "corpus_id": 260599456,
            "sentences": [
                {
                    "corpus_id": "260599456",
                    "title": "Prototypes Sampling Mechanism for Class Incremental Learning",
                    "text": "Knowledge Distillation is a widely used technique in model compression [36], [37] and transfer learning [38], which typically involves transferring knowledge from a large and complex model (often referred to as the teacher model) to a smaller and simpler model (often referred to as the student model). The goal of knowledge distillation is to improve the performance of the student model by leveraging the knowledge contained in the teacher model, while still maintaining the efficiency and simplicity of the student model. It has also been widely applied in the field of incremental learning. A large number of works have used knowledge distillation to transfer knowledge from old models to new models, thus reducing catastrophic forgetting of old classes.",
                    "score": 0.7347564859304607,
                    "section_title": "B. KNOWLEDGE DISTILLATION",
                    "char_start_offset": 10129,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 302
                        },
                        {
                            "start": 303,
                            "end": 524
                        },
                        {
                            "start": 525,
                            "end": 594
                        },
                        {
                            "start": 595,
                            "end": 758
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 71,
                            "end": 75,
                            "matchedPaperCorpusId": "32588614"
                        },
                        {
                            "start": 77,
                            "end": 81,
                            "matchedPaperCorpusId": "11253972"
                        },
                        {
                            "start": 104,
                            "end": 108,
                            "matchedPaperCorpusId": "206596723"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.974609375
                }
            ],
            "relevance_judgement": 0.974609375,
            "relevance_judgment_input_expanded": "# Title: Prototypes Sampling Mechanism for Class Incremental Learning\n# Venue: IEEE Access\n# Authors: Zhe Tao, Shucheng Huang, Gang Wang\n## Abstract\nIncremental learning aims to alleviate the catastrophic forgetting problem of deep neural networks during learning sequential data stream. This problem is even more challenging when old data is unavailable, since learning system can only be trained under the supervision of current data. To address this problem, we proposed a prototype sampling mechanism based on K-means clustering method. On the one hand, we proposed to use K-means clustering to pick out class-representative prototypes for each old class. During incremental stages, prototypes and deep features from current data are trained together to maintain the distinction and balance between old and new classes. On the other hand, we proposed to attach a mask to the loss function based on the cosine similarity between the prototypes and the current data. Which further enhances the discrimination between old and new classes compared to naive knowledge distillation schemes. Extensive experiments conducted on three benchmark datasets including CIFAR100, Tiny-ImageNet and vggface2 verified the effectiveness and advantages of our proposed method. Specifically, we improved class incremental performance by 1.6%, 1.2% and 1.7% on three datasets respectively.\n## B. KNOWLEDGE DISTILLATION\nKnowledge Distillation is a widely used technique in model compression [36], [37] and transfer learning [38], which typically involves transferring knowledge from a large and complex model (often referred to as the teacher model) to a smaller and simpler model (often referred to as the student model). The goal of knowledge distillation is to improve the performance of the student model by leveraging the knowledge contained in the teacher model, while still maintaining the efficiency and simplicity of the student model. It has also been widely applied in the field of incremental learning. A large number of works have used knowledge distillation to transfer knowledge from old models to new models, thus reducing catastrophic forgetting of old classes.",
            "reference_string": "[260599456 | Tao et al. | 2023 | Citations: 3]"
        },
        {
            "title": "LightTS: Lightweight Time Series Classification with Adaptive Ensemble Distillation",
            "venue": "Proc. ACM Manag. Data",
            "year": 2023,
            "reference_count": 72,
            "citation_count": 54,
            "influential_citation_count": 1,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://arxiv.org/pdf/2302.12721",
                "status": "GREEN",
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2302.12721, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2056081421",
                    "name": "David Campos"
                },
                {
                    "authorId": "2194662156",
                    "name": "Miao Zhang"
                },
                {
                    "authorId": "37606919",
                    "name": "B. Yang"
                },
                {
                    "authorId": "19170161",
                    "name": "Tung Kieu"
                },
                {
                    "authorId": "2637615",
                    "name": "Chenjuan Guo"
                },
                {
                    "authorId": "145917501",
                    "name": "Christian S. Jensen"
                }
            ],
            "abstract": "Due to the sweeping digitalization of processes, increasingly vast amounts of time series data are being produced. Accurate classification of such time series facilitates decision making in multiple domains. State-of-the-art classification accuracy is often achieved by ensemble learning where results are synthesized from multiple base models. This characteristic implies that ensemble learning needs substantial computing resources, preventing their use in resource-limited environments, such as in edge devices. To extend the applicability of ensemble learning, we propose the LightTS framework that compresses large ensembles into lightweight models while ensuring competitive accuracy. First, we propose adaptive ensemble distillation that assigns adaptive weights to different base models such that their varying classification capabilities contribute purposefully to the training of the lightweight model. Second, we propose means of identifying Pareto optimal settings w.r.t. model accuracy and model size, thus enabling users with a space budget to select the most accurate lightweight model. We report on experiments using 128 real-world time series sets and different types of base models that justify key decisions in the design of LightTS and provide evidence that LightTS is able to outperform competitors.",
            "corpus_id": 257205990,
            "sentences": [
                {
                    "corpus_id": "257205990",
                    "title": "LightTS: Lightweight Time Series Classification with Adaptive Ensemble Distillation",
                    "text": "Knowledge Distillation (KD) [25] aims to transfer knowledge from a teacher model to a student model, where the teacher is often a larger model with higher discriminative capacity than the student. In classification, the knowledge is represented by a probability distribution over classes produced by the teacher model. Let  and  represent the class distributions from the teacher and the student, respectively. Then, knowledge distillation is formalized in Equation 1, where  \u2208 [0, 1] is a hyper-parameter [54]. \n\nSpecifically, the loss function is computed over two components that are weighted by . The first component is the cross-entropy (CE) between the student class probabilities  and the ground truth label , which provides supervision from the ground truth labels. The second component represents the distance between the teacher and student distributions  and , e.g., Kullback-Leibler (KL) divergence, to encourage a student to mimic the behavior of a more powerful teacher. They both contribute to training an accurate student. \n\nWhen a group of base models is available as the teachers, their average-ensemble,  = 1/ \u00d7   , where   is the class distribution returned by the -th base model, is typically considered as the knowledge source [17]. Figure 5 shows an example of the knowledge distillation with an ensemble consisting of three base models.",
                    "score": 0.661406088576282,
                    "section_title": "Knowledge Distillation",
                    "char_start_offset": 13449,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 196
                        },
                        {
                            "start": 197,
                            "end": 318
                        },
                        {
                            "start": 319,
                            "end": 410
                        },
                        {
                            "start": 411,
                            "end": 511
                        },
                        {
                            "start": 514,
                            "end": 600
                        },
                        {
                            "start": 601,
                            "end": 773
                        },
                        {
                            "start": 774,
                            "end": 984
                        },
                        {
                            "start": 985,
                            "end": 1038
                        },
                        {
                            "start": 1041,
                            "end": 1254
                        },
                        {
                            "start": 1255,
                            "end": 1360
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 28,
                            "end": 32,
                            "matchedPaperCorpusId": "7200347"
                        },
                        {
                            "start": 506,
                            "end": 510,
                            "matchedPaperCorpusId": "228376532"
                        },
                        {
                            "start": 1249,
                            "end": 1253,
                            "matchedPaperCorpusId": "227276362"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.974609375
                }
            ],
            "relevance_judgement": 0.974609375,
            "relevance_judgment_input_expanded": "# Title: LightTS: Lightweight Time Series Classification with Adaptive Ensemble Distillation\n# Venue: Proc. ACM Manag. Data\n# Authors: David Campos, Miao Zhang, B. Yang, Tung Kieu, Chenjuan Guo, Christian S. Jensen\n## Abstract\nDue to the sweeping digitalization of processes, increasingly vast amounts of time series data are being produced. Accurate classification of such time series facilitates decision making in multiple domains. State-of-the-art classification accuracy is often achieved by ensemble learning where results are synthesized from multiple base models. This characteristic implies that ensemble learning needs substantial computing resources, preventing their use in resource-limited environments, such as in edge devices. To extend the applicability of ensemble learning, we propose the LightTS framework that compresses large ensembles into lightweight models while ensuring competitive accuracy. First, we propose adaptive ensemble distillation that assigns adaptive weights to different base models such that their varying classification capabilities contribute purposefully to the training of the lightweight model. Second, we propose means of identifying Pareto optimal settings w.r.t. model accuracy and model size, thus enabling users with a space budget to select the most accurate lightweight model. We report on experiments using 128 real-world time series sets and different types of base models that justify key decisions in the design of LightTS and provide evidence that LightTS is able to outperform competitors.\n## Knowledge Distillation\nKnowledge Distillation (KD) [25] aims to transfer knowledge from a teacher model to a student model, where the teacher is often a larger model with higher discriminative capacity than the student. In classification, the knowledge is represented by a probability distribution over classes produced by the teacher model. Let  and  represent the class distributions from the teacher and the student, respectively. Then, knowledge distillation is formalized in Equation 1, where  \u2208 [0, 1] is a hyper-parameter [54]. \n\nSpecifically, the loss function is computed over two components that are weighted by . The first component is the cross-entropy (CE) between the student class probabilities  and the ground truth label , which provides supervision from the ground truth labels. The second component represents the distance between the teacher and student distributions  and , e.g., Kullback-Leibler (KL) divergence, to encourage a student to mimic the behavior of a more powerful teacher. They both contribute to training an accurate student. \n\nWhen a group of base models is available as the teachers, their average-ensemble,  = 1/ \u00d7   , where   is the class distribution returned by the -th base model, is typically considered as the knowledge source [17]. Figure 5 shows an example of the knowledge distillation with an ensemble consisting of three base models.",
            "reference_string": "[257205990 | Campos et al. | 2023 | Citations: 54]"
        },
        {
            "title": "Self-Knowledge Distillation via Progressive Associative Learning",
            "venue": "Electronics",
            "year": 2024,
            "reference_count": 49,
            "citation_count": 3,
            "influential_citation_count": 0,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://www.mdpi.com/2079-9292/13/11/2062/pdf?version=1716627360",
                "status": "GOLD",
                "license": "CCBY",
                "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.3390/electronics13112062?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.3390/electronics13112062, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2292247795",
                    "name": "Haoran Zhao"
                },
                {
                    "authorId": "2297863565",
                    "name": "Yanxian Bi"
                },
                {
                    "authorId": "2304028661",
                    "name": "Shuwen Tian"
                },
                {
                    "authorId": "2237771070",
                    "name": "Jian Wang"
                },
                {
                    "authorId": "40075749",
                    "name": "Peiying Zhang"
                },
                {
                    "authorId": "2292213958",
                    "name": "Zhaopeng Deng"
                },
                {
                    "authorId": "2301512529",
                    "name": "Kai Liu"
                }
            ],
            "abstract": "As a specific form of knowledge distillation (KD), self-knowledge distillation enables a student network to progressively distill its own knowledge without relying on a pretrained, complex teacher network; however, recent studies of self-KD have discovered that additional dark knowledge captured by auxiliary architecture or data augmentation could create better soft targets for enhancing the network but at the cost of significantly more computations and/or parameters. Moreover, most existing self-KD methods extract the soft label as a supervisory signal from individual input samples, which overlooks the knowledge of relationships among categories. Inspired by human associative learning, we propose a simple yet effective self-KD method named associative learning for self-distillation (ALSD), which progressively distills richer knowledge regarding the relationships between categories across independent samples. Specifically, in the process of distillation, the propagation of knowledge is weighted based on the intersample relationship between associated samples generated in different minibatches, which are progressively estimated with the current network. In this way, our ALSD framework achieves knowledge ensembling progressively across multiple samples using a single network, resulting in minimal computational and memory overhead compared to existing ensembling methods. Extensive experiments demonstrate that our ALSD method consistently boosts the classification performance of various architectures on multiple datasets. Notably, ALSD pushes forward the self-KD performance to 80.10% on CIFAR-100, which exceeds the standard backpropagation by 4.81%. Furthermore, we observe that the proposed method shows comparable performance with the state-of-the-art knowledge distillation methods without the pretrained teacher network.",
            "corpus_id": 270077283,
            "sentences": [
                {
                    "corpus_id": "270077283",
                    "title": "Self-Knowledge Distillation via Progressive Associative Learning",
                    "text": "In this section, we first briefly introduce the most related works of knowledge distillation.Then we specifically review recent self-distillation works.\n\nKnowledge distillation is a widely used paradigm for model compression, which transfers knowledge from a complex teacher model to a compact student model.To be specific, the teacher network has high accuracy and huge parameters, while the student network is not as accurate as the teacher network but has fewer parameters.Through knowledge distillation, we hope that the student network can approach or exceed the teacher network as much as possible.In this way, we obtain a compact student network with a similar prediction effect as the teacher network.Ba et al. [30] first proposed a method that uses the teacher's logits before the softmax as the regression target to train the student network, which completes the imitation of the teacher network by forcing the student network to mimic the teacher network's logits.Hinton et al. [12] first proposed to use the soft outputs of the pretrained teacher network as dark knowledge to supervise the training of the student network.They introduced a temperature hyperparameter T and formulated the problem as \"knowledge distillation\".The student network is forced to learn the soft targets of the teacher network, which are obtained through using a high temperature T on the softmax inputs.In the process of knowledge transfer, soft targets often contain richer information than one-hot targets.Romero et al. [13] extended the knowledge distillation method proposed by Hinton et al.In their method, the student network can be deeper and narrower than the teacher network and improve the performance by learning the outputs of the teacher network and the features of the middle layer.All the above methods are offline distillation methods [31,32], which need a pretrained teacher network.\n\nIn contrast to these methods, online knowledge distillation trains the student network under the supervision of a teacher from scratch.For example, Zhang et al. [33] proposed a mutual learning method, which uses multiple neural networks.Zhao et al. [9] proposed a collaborative training method, which uses both an expert teacher and a from-scratch teacher to supervise the student.To reduce the computational cost, Zhou et al. [34] proposed to employ two different networks which share some low parameters and train separately.",
                    "score": 0.6444066895478471,
                    "section_title": "Related Work",
                    "char_start_offset": 5661,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 93
                        },
                        {
                            "start": 93,
                            "end": 152
                        },
                        {
                            "start": 154,
                            "end": 308
                        },
                        {
                            "start": 308,
                            "end": 476
                        },
                        {
                            "start": 476,
                            "end": 604
                        },
                        {
                            "start": 604,
                            "end": 709
                        },
                        {
                            "start": 709,
                            "end": 975
                        },
                        {
                            "start": 975,
                            "end": 1134
                        },
                        {
                            "start": 1134,
                            "end": 1236
                        },
                        {
                            "start": 1236,
                            "end": 1392
                        },
                        {
                            "start": 1392,
                            "end": 1497
                        },
                        {
                            "start": 1497,
                            "end": 1584
                        },
                        {
                            "start": 1584,
                            "end": 1785
                        },
                        {
                            "start": 1785,
                            "end": 1889
                        },
                        {
                            "start": 1891,
                            "end": 2026
                        },
                        {
                            "start": 2026,
                            "end": 2128
                        },
                        {
                            "start": 2128,
                            "end": 2272
                        },
                        {
                            "start": 2272,
                            "end": 2418
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 719,
                            "end": 723,
                            "matchedPaperCorpusId": "11536917"
                        },
                        {
                            "start": 989,
                            "end": 993,
                            "matchedPaperCorpusId": "7200347"
                        },
                        {
                            "start": 1840,
                            "end": 1844,
                            "matchedPaperCorpusId": "247476179"
                        },
                        {
                            "start": 1844,
                            "end": 1847,
                            "matchedPaperCorpusId": "258888057"
                        },
                        {
                            "start": 2052,
                            "end": 2056,
                            "matchedPaperCorpusId": "26071966"
                        },
                        {
                            "start": 2140,
                            "end": 2143,
                            "matchedPaperCorpusId": "198179767"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.9736328125
                }
            ],
            "relevance_judgement": 0.9736328125,
            "relevance_judgment_input_expanded": "# Title: Self-Knowledge Distillation via Progressive Associative Learning\n# Venue: Electronics\n# Authors: Haoran Zhao, Yanxian Bi, Shuwen Tian, Jian Wang, Peiying Zhang, Zhaopeng Deng, Kai Liu\n## Abstract\nAs a specific form of knowledge distillation (KD), self-knowledge distillation enables a student network to progressively distill its own knowledge without relying on a pretrained, complex teacher network; however, recent studies of self-KD have discovered that additional dark knowledge captured by auxiliary architecture or data augmentation could create better soft targets for enhancing the network but at the cost of significantly more computations and/or parameters. Moreover, most existing self-KD methods extract the soft label as a supervisory signal from individual input samples, which overlooks the knowledge of relationships among categories. Inspired by human associative learning, we propose a simple yet effective self-KD method named associative learning for self-distillation (ALSD), which progressively distills richer knowledge regarding the relationships between categories across independent samples. Specifically, in the process of distillation, the propagation of knowledge is weighted based on the intersample relationship between associated samples generated in different minibatches, which are progressively estimated with the current network. In this way, our ALSD framework achieves knowledge ensembling progressively across multiple samples using a single network, resulting in minimal computational and memory overhead compared to existing ensembling methods. Extensive experiments demonstrate that our ALSD method consistently boosts the classification performance of various architectures on multiple datasets. Notably, ALSD pushes forward the self-KD performance to 80.10% on CIFAR-100, which exceeds the standard backpropagation by 4.81%. Furthermore, we observe that the proposed method shows comparable performance with the state-of-the-art knowledge distillation methods without the pretrained teacher network.\n## Related Work\nIn this section, we first briefly introduce the most related works of knowledge distillation.Then we specifically review recent self-distillation works.\n\nKnowledge distillation is a widely used paradigm for model compression, which transfers knowledge from a complex teacher model to a compact student model.To be specific, the teacher network has high accuracy and huge parameters, while the student network is not as accurate as the teacher network but has fewer parameters.Through knowledge distillation, we hope that the student network can approach or exceed the teacher network as much as possible.In this way, we obtain a compact student network with a similar prediction effect as the teacher network.Ba et al. [30] first proposed a method that uses the teacher's logits before the softmax as the regression target to train the student network, which completes the imitation of the teacher network by forcing the student network to mimic the teacher network's logits.Hinton et al. [12] first proposed to use the soft outputs of the pretrained teacher network as dark knowledge to supervise the training of the student network.They introduced a temperature hyperparameter T and formulated the problem as \"knowledge distillation\".The student network is forced to learn the soft targets of the teacher network, which are obtained through using a high temperature T on the softmax inputs.In the process of knowledge transfer, soft targets often contain richer information than one-hot targets.Romero et al. [13] extended the knowledge distillation method proposed by Hinton et al.In their method, the student network can be deeper and narrower than the teacher network and improve the performance by learning the outputs of the teacher network and the features of the middle layer.All the above methods are offline distillation methods [31,32], which need a pretrained teacher network.\n\nIn contrast to these methods, online knowledge distillation trains the student network under the supervision of a teacher from scratch.For example, Zhang et al. [33] proposed a mutual learning method, which uses multiple neural networks.Zhao et al. [9] proposed a collaborative training method, which uses both an expert teacher and a from-scratch teacher to supervise the student.To reduce the computational cost, Zhou et al. [34] proposed to employ two different networks which share some low parameters and train separately.",
            "reference_string": "[270077283 | Zhao et al. | 2024 | Citations: 3]"
        },
        {
            "title": "Semi-Supervised Domain Generalizable Person Re-Identification",
            "venue": "arXiv.org",
            "year": 2021,
            "reference_count": 58,
            "citation_count": 17,
            "influential_citation_count": 1,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2108.05045, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "144486617",
                    "name": "Lingxiao He"
                },
                {
                    "authorId": "2117994406",
                    "name": "Wu Liu"
                },
                {
                    "authorId": "143932869",
                    "name": "Jian Liang"
                },
                {
                    "authorId": "84005711",
                    "name": "Kecheng Zheng"
                },
                {
                    "authorId": "71425456",
                    "name": "Xingyu Liao"
                },
                {
                    "authorId": "2112624683",
                    "name": "Peng Cheng"
                },
                {
                    "authorId": "144025741",
                    "name": "Tao Mei"
                }
            ],
            "abstract": "Existing person re-identification (re-id) methods are stuck when deployed to a new unseen scenario despite the success in cross-camera person matching. Recent efforts have been substantially devoted to domain adaptive person re-id where extensive unlabeled data in the new scenario are utilized in a transductive learning manner. However, for each scenario, it is required to first collect enough data and then train such a domain adaptive re-id model, thus restricting their practical application. Instead, we aim to explore multiple labeled datasets to learn generalized domain-invariant representations for person re-id, which is expected universally effective for each new-coming re-id scenario. To pursue practicability in real-world systems, we collect all the person re-id datasets (20 datasets) in this field and select the three most frequently used datasets (i.e., Market1501, DukeMTMC, and MSMT17) as unseen target domains. In addition, we develop DataHunter that collects over 300K+ weak annotated images named YouTube-Human from YouTube street-view videos, which joins 17 remaining full labeled datasets to form multiple source domains. On such a large and challenging benchmark called FastHuman (~440K+ labeled images), we further propose a simple yet effective Semi-Supervised Knowledge Distillation (SSKD) framework. SSKD effectively exploits the weakly annotated data by assigning soft pseudo labels to YouTube-Human to improve models' generalization ability. Experiments on several protocols verify the effectiveness of the proposed SSKD framework on domain generalizable person re-id, which is even comparable to supervised learning on the target domains. Lastly, but most importantly, we hope the proposed benchmark FastHuman could bring the next development of domain generalizable person re-id algorithms.",
            "corpus_id": 236976374,
            "sentences": [
                {
                    "corpus_id": "236976374",
                    "title": "Semi-Supervised Domain Generalizable Person Re-Identification",
                    "text": "The learning of a small network from a large network is later formally popularized as vanilla knowledge distillation (KD) [17] where a small student model is generally supervised by a large teacher model. The main idea is that the student model mimics the teacher model to achieve competitive or even superior performance. Particularly, knowledge is transferred from the teacher model to the student by minimizing a loss function in which the target is the distribution of class probabilities predicted by the teacher model. \n\nExisting knowledge distillation methods can be grouped into the three categories. The response-based Knowledge knowledge distillation uses the logits of a large deep model as the teacher knowledge [17], [21], [33]. The activations, neurons or features of intermediate layers also can be adopted as the feature-based knowledge distillation [1], [16], [23] to instruct students in the learning of the model. The relationship-based knowledge distillation [24], [28], [39], [45] further explores the relationships between different activations, neurons or pairs of samples. KD, as a quite popular technique, has been widely applied in model compression, object detection, and various feature learning tasks. Motivated by KD, we provide a feasible solution to combine labeled and unlabeled data to improve the generalization ability of person re-id models.",
                    "score": 0.6357717054282868,
                    "section_title": "Knowledge Distillation",
                    "char_start_offset": 7516,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 204
                        },
                        {
                            "start": 205,
                            "end": 322
                        },
                        {
                            "start": 323,
                            "end": 524
                        },
                        {
                            "start": 527,
                            "end": 608
                        },
                        {
                            "start": 609,
                            "end": 741
                        },
                        {
                            "start": 742,
                            "end": 932
                        },
                        {
                            "start": 933,
                            "end": 1096
                        },
                        {
                            "start": 1097,
                            "end": 1230
                        },
                        {
                            "start": 1231,
                            "end": 1378
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 730,
                            "end": 734,
                            "matchedPaperCorpusId": "3608236"
                        },
                        {
                            "start": 736,
                            "end": 740,
                            "matchedPaperCorpusId": "212908749"
                        },
                        {
                            "start": 866,
                            "end": 869,
                            "matchedPaperCorpusId": "118649278"
                        },
                        {
                            "start": 871,
                            "end": 875,
                            "matchedPaperCorpusId": "53213211"
                        },
                        {
                            "start": 985,
                            "end": 989,
                            "matchedPaperCorpusId": "198185886"
                        },
                        {
                            "start": 991,
                            "end": 995,
                            "matchedPaperCorpusId": "198179476"
                        },
                        {
                            "start": 997,
                            "end": 1001,
                            "matchedPaperCorpusId": "102351826"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.9736328125
                }
            ],
            "relevance_judgement": 0.9736328125,
            "relevance_judgment_input_expanded": "# Title: Semi-Supervised Domain Generalizable Person Re-Identification\n# Venue: arXiv.org\n# Authors: Lingxiao He, Wu Liu, Jian Liang, Kecheng Zheng, Xingyu Liao, Peng Cheng, Tao Mei\n## Abstract\nExisting person re-identification (re-id) methods are stuck when deployed to a new unseen scenario despite the success in cross-camera person matching. Recent efforts have been substantially devoted to domain adaptive person re-id where extensive unlabeled data in the new scenario are utilized in a transductive learning manner. However, for each scenario, it is required to first collect enough data and then train such a domain adaptive re-id model, thus restricting their practical application. Instead, we aim to explore multiple labeled datasets to learn generalized domain-invariant representations for person re-id, which is expected universally effective for each new-coming re-id scenario. To pursue practicability in real-world systems, we collect all the person re-id datasets (20 datasets) in this field and select the three most frequently used datasets (i.e., Market1501, DukeMTMC, and MSMT17) as unseen target domains. In addition, we develop DataHunter that collects over 300K+ weak annotated images named YouTube-Human from YouTube street-view videos, which joins 17 remaining full labeled datasets to form multiple source domains. On such a large and challenging benchmark called FastHuman (~440K+ labeled images), we further propose a simple yet effective Semi-Supervised Knowledge Distillation (SSKD) framework. SSKD effectively exploits the weakly annotated data by assigning soft pseudo labels to YouTube-Human to improve models' generalization ability. Experiments on several protocols verify the effectiveness of the proposed SSKD framework on domain generalizable person re-id, which is even comparable to supervised learning on the target domains. Lastly, but most importantly, we hope the proposed benchmark FastHuman could bring the next development of domain generalizable person re-id algorithms.\n## Knowledge Distillation\nThe learning of a small network from a large network is later formally popularized as vanilla knowledge distillation (KD) [17] where a small student model is generally supervised by a large teacher model. The main idea is that the student model mimics the teacher model to achieve competitive or even superior performance. Particularly, knowledge is transferred from the teacher model to the student by minimizing a loss function in which the target is the distribution of class probabilities predicted by the teacher model. \n\nExisting knowledge distillation methods can be grouped into the three categories. The response-based Knowledge knowledge distillation uses the logits of a large deep model as the teacher knowledge [17], [21], [33]. The activations, neurons or features of intermediate layers also can be adopted as the feature-based knowledge distillation [1], [16], [23] to instruct students in the learning of the model. The relationship-based knowledge distillation [24], [28], [39], [45] further explores the relationships between different activations, neurons or pairs of samples. KD, as a quite popular technique, has been widely applied in model compression, object detection, and various feature learning tasks. Motivated by KD, we provide a feasible solution to combine labeled and unlabeled data to improve the generalization ability of person re-id models.",
            "reference_string": "[236976374 | He et al. | 2021 | Citations: 17]"
        },
        {
            "title": "Maximizing discrimination capability of knowledge distillation with energy function",
            "venue": "Knowledge-Based Systems",
            "year": 2023,
            "reference_count": 57,
            "citation_count": 4,
            "influential_citation_count": 0,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://arxiv.org/pdf/2311.14334",
                "status": "GREEN",
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2311.14334, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2268350036",
                    "name": "Seonghak Kim"
                },
                {
                    "authorId": "2156910329",
                    "name": "Gyeongdo Ham"
                },
                {
                    "authorId": "2268370058",
                    "name": "Suin Lee"
                },
                {
                    "authorId": "2268310103",
                    "name": "Donggon Jang"
                },
                {
                    "authorId": "2145154407",
                    "name": "Daeshik Kim"
                }
            ],
            "abstract": null,
            "corpus_id": 267657497,
            "sentences": [
                {
                    "corpus_id": "267657497",
                    "title": "Maximizing discrimination capability of knowledge distillation with energy function",
                    "text": "Knowledge distillation (KD) is a technique used to enhance the performance of lightweight student networks by leveraging the dark knowledge embedded in large teacher networks. Over the years, KD methods have evolved to narrow the performance gap between student and teacher models by utilizing both final predictions, known as logits-based distillation [10,15,16,11,17], and intermediate features, known as features-based distillation [18,19,20,21,22,23,24,12,25,26,27,28]. \n\nPrevious works on logits-based distillations include the following: DML [15] proposed a mutual learning strategy for collaboratively teaching and learning between student and teacher models; TAKD [16] introduced a multi-step method with an intermediate-size network (i.e., assistant network) to bridge the gap between teachers and students; DKD [11] decomposed the soft-label distillation loss into two components: target class knowledge distillation (TCKD) and non-target class knowledge distillation (NCKD), enabling each part to independently harness its effectiveness; Multi KD [17] proposed multi-level prediction alignment, containing instance, batch, and class levels, and prediction augmentation. While these approaches emphasize effective knowledge transfer, they do not consider dividing entire datasets or provide mechanisms to distinguish and transfer knowledge from specific samples. \n\nFitNet [18] was groundbreaking as it leveraged not only the final outputs but also intermediate representations. Since the introduction of FitNet, various feature-based KD methods have emerged as follows: AT [22] prompted the student to mimic the attention map of the teacher network; PKT [19] employed various kernels to estimate the probability distributions, employing different divergence metrics for distillation; RKD [20] focused on transferring the mutual relations of data samples; CRD [21] framed the objective as contrastive learning for distillation; VID [23] took a different approach by maximizing mutual information; OFD [24] introduced a novel loss function incorporating teacher transform and a new distance function; Review KD [12] introduced a review mechanism that leverages past features for guiding current ones through residual learning.",
                    "score": 0.6128478648924719,
                    "section_title": "Knowledge Distillation",
                    "char_start_offset": 3764,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 175
                        },
                        {
                            "start": 176,
                            "end": 473
                        },
                        {
                            "start": 476,
                            "end": 1180
                        },
                        {
                            "start": 1181,
                            "end": 1372
                        },
                        {
                            "start": 1375,
                            "end": 1487
                        },
                        {
                            "start": 1488,
                            "end": 2234
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 363,
                            "end": 366,
                            "matchedPaperCorpusId": "247476179"
                        },
                        {
                            "start": 366,
                            "end": 369,
                            "matchedPaperCorpusId": "260933721"
                        },
                        {
                            "start": 439,
                            "end": 442,
                            "matchedPaperCorpusId": "219169868"
                        },
                        {
                            "start": 442,
                            "end": 445,
                            "matchedPaperCorpusId": "131765296"
                        },
                        {
                            "start": 451,
                            "end": 454,
                            "matchedPaperCorpusId": "118649278"
                        },
                        {
                            "start": 454,
                            "end": 457,
                            "matchedPaperCorpusId": "102483181"
                        },
                        {
                            "start": 457,
                            "end": 460,
                            "matchedPaperCorpusId": "233296935"
                        },
                        {
                            "start": 460,
                            "end": 463,
                            "matchedPaperCorpusId": "258298441"
                        },
                        {
                            "start": 463,
                            "end": 466,
                            "matchedPaperCorpusId": "258309453"
                        },
                        {
                            "start": 466,
                            "end": 469,
                            "matchedPaperCorpusId": "269167845"
                        },
                        {
                            "start": 469,
                            "end": 472,
                            "matchedPaperCorpusId": "269206209"
                        },
                        {
                            "start": 821,
                            "end": 825,
                            "matchedPaperCorpusId": "247476179"
                        },
                        {
                            "start": 1058,
                            "end": 1062,
                            "matchedPaperCorpusId": "260933721"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.9736328125
                }
            ],
            "relevance_judgement": 0.9736328125,
            "relevance_judgment_input_expanded": "# Title: Maximizing discrimination capability of knowledge distillation with energy function\n# Venue: Knowledge-Based Systems\n# Authors: Seonghak Kim, Gyeongdo Ham, Suin Lee, Donggon Jang, Daeshik Kim\n## Abstract\nNone\n## Knowledge Distillation\nKnowledge distillation (KD) is a technique used to enhance the performance of lightweight student networks by leveraging the dark knowledge embedded in large teacher networks. Over the years, KD methods have evolved to narrow the performance gap between student and teacher models by utilizing both final predictions, known as logits-based distillation [10,15,16,11,17], and intermediate features, known as features-based distillation [18,19,20,21,22,23,24,12,25,26,27,28]. \n\nPrevious works on logits-based distillations include the following: DML [15] proposed a mutual learning strategy for collaboratively teaching and learning between student and teacher models; TAKD [16] introduced a multi-step method with an intermediate-size network (i.e., assistant network) to bridge the gap between teachers and students; DKD [11] decomposed the soft-label distillation loss into two components: target class knowledge distillation (TCKD) and non-target class knowledge distillation (NCKD), enabling each part to independently harness its effectiveness; Multi KD [17] proposed multi-level prediction alignment, containing instance, batch, and class levels, and prediction augmentation. While these approaches emphasize effective knowledge transfer, they do not consider dividing entire datasets or provide mechanisms to distinguish and transfer knowledge from specific samples. \n\nFitNet [18] was groundbreaking as it leveraged not only the final outputs but also intermediate representations. Since the introduction of FitNet, various feature-based KD methods have emerged as follows: AT [22] prompted the student to mimic the attention map of the teacher network; PKT [19] employed various kernels to estimate the probability distributions, employing different divergence metrics for distillation; RKD [20] focused on transferring the mutual relations of data samples; CRD [21] framed the objective as contrastive learning for distillation; VID [23] took a different approach by maximizing mutual information; OFD [24] introduced a novel loss function incorporating teacher transform and a new distance function; Review KD [12] introduced a review mechanism that leverages past features for guiding current ones through residual learning.",
            "reference_string": "[267657497 | Kim et al. | 2023 | Citations: 4]"
        },
        {
            "title": "Efficiency Optimization of Large-Scale Language Models Based on Deep Learning in Natural Language Processing Tasks",
            "venue": "2024 IEEE 2nd International Conference on Sensors, Electronics and Computer Engineering (ICSECE)",
            "year": 2024,
            "reference_count": 41,
            "citation_count": 20,
            "influential_citation_count": 0,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://arxiv.org/pdf/2405.11704",
                "status": "GREEN",
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2405.11704, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2298916636",
                    "name": "Taiyuan Mei"
                },
                {
                    "authorId": "2298918720",
                    "name": "Yun Zi"
                },
                {
                    "authorId": "2222987403",
                    "name": "X. Cheng"
                },
                {
                    "authorId": "2297725659",
                    "name": "Zijun Gao"
                },
                {
                    "authorId": "2297735971",
                    "name": "Qi Wang"
                },
                {
                    "authorId": "2302372513",
                    "name": "Haowei Yang"
                }
            ],
            "abstract": "The internal structure and operation mechanism of large-scale language models are analyzed theoretically, especially how Transformer and its derivative architectures can restrict computing efficiency while capturing long-term dependencies. Further, we dig deep into the efficiency bottleneck of the training phase, and evaluate in detail the contribution of adaptive optimization algorithms (such as AdamW), massively parallel computing techniques, and mixed precision training strategies to accelerate convergence and reduce memory footprint. By analyzing the mathematical principles and implementation details of these algorithms, we reveal how they effectively improve training efficiency in practice. In terms of model deployment and inference optimization, this paper systematically reviews the latest advances in model compression techniques, focusing on strategies such as quantification, pruning, and knowledge distillation. By comparing the theoretical frameworks of these techniques and their effects in different application scenarios, we demonstrate their ability to significantly reduce model size and inference delay while maintaining model prediction accuracy. In addition, this paper critically examines the limitations of current efficiency optimization methods, such as the increased risk of overfitting, the control of performance loss after compression, and the problem of algorithm generality, and proposes some prospects for future research. In conclusion, this study provides a comprehensive theoretical framework for understanding the efficiency optimization of large-scale language models.",
            "corpus_id": 269921267,
            "sentences": [
                {
                    "corpus_id": "269921267",
                    "title": "Efficiency Optimization of Large-Scale Language Models Based on Deep Learning in Natural Language Processing Tasks",
                    "text": "Knowledge distillation, a core strategy in modern machine learning, focuses on solving the problem of balance between model size and computational efficiency.The core idea is to effectively transfer the deep knowledge and experience accumulated in large-scale, complex models (often referred to as \"teacher models\") to \"student models\" with smaller numbers of participants and lower computational requirements.\n\nSince then, this technology has rapidly attracted widespread attention from academia and industry, and has been expanded and deepened in several ways: Early stage (2015-2018) : Initial research has focused on simplifying network structure, reducing model volume and computational requirements, while maintaining the predictive performance of the model.In this period, the basic framework of knowledge distillation and the design of loss function were established.Technical deepening (2019-2021) : Researchers began to explore more refined distillation methods, including multi-teacher distillation, feature-stage distillation, relational distillation, etc.\n\nIn the field of deep learning, model knowledge lies in the configuration of parameters it learns through training, which guides the model on how best to extract features from input data and make predictions.Large networks, thanks to their large number of parameters and complex structure design, can capture deeper feature associations on large-scale data sets, showing superior learning ability and generalization performance.However, this advantage is often difficult to play directly in resource-constrained real-world application scenarios, because they have high requirements for computing resources and storage space.Knowledge distillation technology is born to solve this contradiction, it focuses not only on the final classification or regression results of the model output, but also on the teaching of the teacher model's confidence distribution (i.e., soft label) that each sample belongs to various categories.By designing a specific training mechanism, the student model tries to imitate the soft decision-making process while learning the real label, so that it can also \"inherit\" the decision logic and deep understanding of the data of the teacher model under limited parameters.This process not only involves the traditional cross-entropy loss, but also introduces the distillation loss that reflects the difference in the predicted probability distribution.",
                    "score": 0.725340674115057,
                    "section_title": "B. Knowledge distillation",
                    "char_start_offset": 12089,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 158
                        },
                        {
                            "start": 158,
                            "end": 410
                        },
                        {
                            "start": 412,
                            "end": 764
                        },
                        {
                            "start": 764,
                            "end": 875
                        },
                        {
                            "start": 875,
                            "end": 1068
                        },
                        {
                            "start": 1070,
                            "end": 1277
                        },
                        {
                            "start": 1277,
                            "end": 1497
                        },
                        {
                            "start": 1497,
                            "end": 1693
                        },
                        {
                            "start": 1693,
                            "end": 1993
                        },
                        {
                            "start": 1993,
                            "end": 2266
                        },
                        {
                            "start": 2266,
                            "end": 2446
                        }
                    ],
                    "ref_mentions": [],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.97314453125
                }
            ],
            "relevance_judgement": 0.97314453125,
            "relevance_judgment_input_expanded": "# Title: Efficiency Optimization of Large-Scale Language Models Based on Deep Learning in Natural Language Processing Tasks\n# Venue: 2024 IEEE 2nd International Conference on Sensors, Electronics and Computer Engineering (ICSECE)\n# Authors: Taiyuan Mei, Yun Zi, X. Cheng, Zijun Gao, Qi Wang, Haowei Yang\n## Abstract\nThe internal structure and operation mechanism of large-scale language models are analyzed theoretically, especially how Transformer and its derivative architectures can restrict computing efficiency while capturing long-term dependencies. Further, we dig deep into the efficiency bottleneck of the training phase, and evaluate in detail the contribution of adaptive optimization algorithms (such as AdamW), massively parallel computing techniques, and mixed precision training strategies to accelerate convergence and reduce memory footprint. By analyzing the mathematical principles and implementation details of these algorithms, we reveal how they effectively improve training efficiency in practice. In terms of model deployment and inference optimization, this paper systematically reviews the latest advances in model compression techniques, focusing on strategies such as quantification, pruning, and knowledge distillation. By comparing the theoretical frameworks of these techniques and their effects in different application scenarios, we demonstrate their ability to significantly reduce model size and inference delay while maintaining model prediction accuracy. In addition, this paper critically examines the limitations of current efficiency optimization methods, such as the increased risk of overfitting, the control of performance loss after compression, and the problem of algorithm generality, and proposes some prospects for future research. In conclusion, this study provides a comprehensive theoretical framework for understanding the efficiency optimization of large-scale language models.\n## B. Knowledge distillation\nKnowledge distillation, a core strategy in modern machine learning, focuses on solving the problem of balance between model size and computational efficiency.The core idea is to effectively transfer the deep knowledge and experience accumulated in large-scale, complex models (often referred to as \"teacher models\") to \"student models\" with smaller numbers of participants and lower computational requirements.\n\nSince then, this technology has rapidly attracted widespread attention from academia and industry, and has been expanded and deepened in several ways: Early stage (2015-2018) : Initial research has focused on simplifying network structure, reducing model volume and computational requirements, while maintaining the predictive performance of the model.In this period, the basic framework of knowledge distillation and the design of loss function were established.Technical deepening (2019-2021) : Researchers began to explore more refined distillation methods, including multi-teacher distillation, feature-stage distillation, relational distillation, etc.\n\nIn the field of deep learning, model knowledge lies in the configuration of parameters it learns through training, which guides the model on how best to extract features from input data and make predictions.Large networks, thanks to their large number of parameters and complex structure design, can capture deeper feature associations on large-scale data sets, showing superior learning ability and generalization performance.However, this advantage is often difficult to play directly in resource-constrained real-world application scenarios, because they have high requirements for computing resources and storage space.Knowledge distillation technology is born to solve this contradiction, it focuses not only on the final classification or regression results of the model output, but also on the teaching of the teacher model's confidence distribution (i.e., soft label) that each sample belongs to various categories.By designing a specific training mechanism, the student model tries to imitate the soft decision-making process while learning the real label, so that it can also \"inherit\" the decision logic and deep understanding of the data of the teacher model under limited parameters.This process not only involves the traditional cross-entropy loss, but also introduces the distillation loss that reflects the difference in the predicted probability distribution.",
            "reference_string": "[269921267 | Mei et al. | 2024 | Citations: 20]"
        },
        {
            "title": "Explainability-Driven Leaf Disease Classification Using Adversarial Training and Knowledge Distillation",
            "venue": "International Conference on Agents and Artificial Intelligence",
            "year": 2023,
            "reference_count": 41,
            "citation_count": 1,
            "influential_citation_count": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2401.00334, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2219976776",
                    "name": "Sebastian-Vasile Echim"
                },
                {
                    "authorId": "2279431357",
                    "name": "Iulian-Marius Taiatu"
                },
                {
                    "authorId": "3046903",
                    "name": "Dumitru-Clementin Cercel"
                },
                {
                    "authorId": "2261270760",
                    "name": "Florin-Catalin Pop"
                }
            ],
            "abstract": "This work focuses on plant leaf disease classification and explores three crucial aspects: adversarial training, model explainability, and model compression. The models' robustness against adversarial attacks is enhanced through adversarial training, ensuring accurate classification even in the presence of threats. Leveraging explainability techniques, we gain insights into the model's decision-making process, improving trust and transparency. Additionally, we explore model compression techniques to optimize computational efficiency while maintaining classification performance. Through our experiments, we determine that on a benchmark dataset, the robustness can be the price of the classification accuracy with performance reductions of 3%-20% for regular tests and gains of 50%-70% for adversarial attack tests. We also demonstrate that a student model can be 15-25 times more computationally efficient for a slight performance reduction, distilling the knowledge of more complex models.",
            "corpus_id": 266693464,
            "sentences": [
                {
                    "corpus_id": "266693464",
                    "title": "Explainability-Driven Leaf Disease Classification Using Adversarial Training and Knowledge Distillation",
                    "text": "A practical method for compressing models is knowledge distillation, which facilitates information transfer from a large teacher network to a small student network. Initially introduced by Bucila et al. (Bucilu\u01ce et al., 2006) and later generalized by Hinton et al. (Hinton et al., 2015), this method has gained popularity across multiple machine learning applications. Unlike conventional training, knowledge distillation involves learning the student network to emulate the outputs of the teacher model, typically represented as probability distributions over classes. By minimizing the similarity loss between the student's predictions and the teacher's probabilities, the student network can assimilate the knowledge from the teacher, resulting in superior performance compared to training from",
                    "score": 0.7210586230963048,
                    "section_title": "Knowledge Distillation",
                    "char_start_offset": 10668,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 164
                        },
                        {
                            "start": 165,
                            "end": 368
                        },
                        {
                            "start": 369,
                            "end": 569
                        },
                        {
                            "start": 570,
                            "end": 797
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 203,
                            "end": 225,
                            "matchedPaperCorpusId": "11253972"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.97314453125
                }
            ],
            "relevance_judgement": 0.97314453125,
            "relevance_judgment_input_expanded": "# Title: Explainability-Driven Leaf Disease Classification Using Adversarial Training and Knowledge Distillation\n# Venue: International Conference on Agents and Artificial Intelligence\n# Authors: Sebastian-Vasile Echim, Iulian-Marius Taiatu, Dumitru-Clementin Cercel, Florin-Catalin Pop\n## Abstract\nThis work focuses on plant leaf disease classification and explores three crucial aspects: adversarial training, model explainability, and model compression. The models' robustness against adversarial attacks is enhanced through adversarial training, ensuring accurate classification even in the presence of threats. Leveraging explainability techniques, we gain insights into the model's decision-making process, improving trust and transparency. Additionally, we explore model compression techniques to optimize computational efficiency while maintaining classification performance. Through our experiments, we determine that on a benchmark dataset, the robustness can be the price of the classification accuracy with performance reductions of 3%-20% for regular tests and gains of 50%-70% for adversarial attack tests. We also demonstrate that a student model can be 15-25 times more computationally efficient for a slight performance reduction, distilling the knowledge of more complex models.\n## Knowledge Distillation\nA practical method for compressing models is knowledge distillation, which facilitates information transfer from a large teacher network to a small student network. Initially introduced by Bucila et al. (Bucilu\u01ce et al., 2006) and later generalized by Hinton et al. (Hinton et al., 2015), this method has gained popularity across multiple machine learning applications. Unlike conventional training, knowledge distillation involves learning the student network to emulate the outputs of the teacher model, typically represented as probability distributions over classes. By minimizing the similarity loss between the student's predictions and the teacher's probabilities, the student network can assimilate the knowledge from the teacher, resulting in superior performance compared to training from",
            "reference_string": "[266693464 | Echim et al. | 2023 | Citations: 1]"
        },
        {
            "title": "Distilling a Powerful Student Model via Online Knowledge Distillation",
            "venue": "IEEE Transactions on Neural Networks and Learning Systems",
            "year": 2021,
            "reference_count": 65,
            "citation_count": 47,
            "influential_citation_count": 0,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://arxiv.org/pdf/2103.14473",
                "status": "GREEN",
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2103.14473, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "145904937",
                    "name": "Shaojie Li"
                },
                {
                    "authorId": "49352079",
                    "name": "Mingbao Lin"
                },
                {
                    "authorId": "2152543905",
                    "name": "Yan Wang"
                },
                {
                    "authorId": "1835006",
                    "name": "Feiyue Huang"
                },
                {
                    "authorId": "47096329",
                    "name": "Yongjian Wu"
                },
                {
                    "authorId": "40161651",
                    "name": "Yonghong Tian"
                },
                {
                    "authorId": "144082425",
                    "name": "Ling Shao"
                },
                {
                    "authorId": "1572139630",
                    "name": "Rongrong Ji"
                }
            ],
            "abstract": "Existing online knowledge distillation approaches either adopt the student with the best performance or construct an ensemble model for better holistic performance. However, the former strategy ignores other students\u2019 information, while the latter increases the computational complexity during deployment. In this article, we propose a novel method for online knowledge distillation, termed feature fusion and self-distillation (FFSD), which comprises two key components: FFSD, toward solving the above problems in a unified framework. Different from previous works, where all students are treated equally, the proposed FFSD splits them into a leader student set and a common student set. Then, the feature fusion module converts the concatenation of feature maps from all common students into a fused feature map. The fused representation is used to assist the learning of the leader student. To enable the leader student to absorb more diverse information, we design an enhancement strategy to increase the diversity among students. Besides, a self-distillation module is adopted to convert the feature map of deeper layers into a shallower one. Then, the shallower layers are encouraged to mimic the transformed feature maps of the deeper layers, which helps the students to generalize better. After training, we simply adopt the leader student, which achieves superior performance, over the common students, without increasing the storage or inference cost. Extensive experiments on CIFAR-100 and ImageNet demonstrate the superiority of our FFSD over existing works. The code is available at https://github.com/SJLeo/FFSD.",
            "corpus_id": 232380330,
            "sentences": [
                {
                    "corpus_id": "232380330",
                    "title": "Distilling a Powerful Student Model via Online Knowledge Distillation",
                    "text": "Traditional Knowledge Distillation. Traditional distillation works transfer knowledge from a cumbersome teacher model to a light-weight student model. As such, a large-scale model has to be trained in advance, based on which various knowledge definitions and transfer strategies are proposed to boost the performance of the student model. The pioneering work [7] performs knowledge representation of a teacher model using the softmax output layer, which converts the logit into a soft probability with a temperature parameter. Following this, a large number of works proposed new forms of knowledge, such as output logits [20], [21], intermediate feature maps [8], [22], [23], attention maps [9], second-order statistics [24], contrastive features [25], [26], or structured knowledge [27], [28], [29]. Another group of methods focus on transfer strategies so as to enable the student model to inherit knowledge from the teacher model. An intuitive solution is to use the Kullback-Leibler divergence or p -loss when the knowledge falls on the soft logit [7], [30] or intermediate representation [8], [9]. Beyond that, Wang et al. [31] utilized the adversarial training scheme in generative adversarial networks (GANs) [32] to transfer knowledge. Jang et al. [33] considered meta-learning to selectively transfer knowledge. In [34], a reinforcement learning based architecture-aware distillation was proposed to pass the structural knowledge to the student. Recently, there are some works [35], [36], [37], [38] that used knowledge distillation for GAN compression to ensure effective compression. The surveys [39], [40] summarized the development of knowledge distillation in recent years. \n\nOnline Knowledge Distillation. Online knowledge distillation has emerged as an alternative that eliminates the dependency on the teacher model. It builds knowledge distillation based on a collection of student models that collaborate through simultaneous training. To this end, Zhang et al. [10] proposed a deep mutual learning strategy where pair-wise students are encouraged to learn from each other by a mimicry loss based on the Kullback-Leibler divergence.",
                    "score": 0.6269751652211206,
                    "section_title": "II. RELATED WORK",
                    "char_start_offset": 5383,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 35
                        },
                        {
                            "start": 36,
                            "end": 150
                        },
                        {
                            "start": 151,
                            "end": 338
                        },
                        {
                            "start": 339,
                            "end": 526
                        },
                        {
                            "start": 527,
                            "end": 801
                        },
                        {
                            "start": 802,
                            "end": 934
                        },
                        {
                            "start": 935,
                            "end": 1103
                        },
                        {
                            "start": 1104,
                            "end": 1244
                        },
                        {
                            "start": 1245,
                            "end": 1321
                        },
                        {
                            "start": 1322,
                            "end": 1455
                        },
                        {
                            "start": 1456,
                            "end": 1595
                        },
                        {
                            "start": 1596,
                            "end": 1688
                        },
                        {
                            "start": 1691,
                            "end": 1721
                        },
                        {
                            "start": 1722,
                            "end": 1834
                        },
                        {
                            "start": 1835,
                            "end": 1955
                        },
                        {
                            "start": 1956,
                            "end": 2152
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 622,
                            "end": 626,
                            "matchedPaperCorpusId": "219962714"
                        },
                        {
                            "start": 628,
                            "end": 632,
                            "matchedPaperCorpusId": "235306324"
                        },
                        {
                            "start": 665,
                            "end": 669,
                            "matchedPaperCorpusId": "233296935"
                        },
                        {
                            "start": 671,
                            "end": 675,
                            "matchedPaperCorpusId": "227335337"
                        },
                        {
                            "start": 692,
                            "end": 695,
                            "matchedPaperCorpusId": "829159"
                        },
                        {
                            "start": 721,
                            "end": 725,
                            "matchedPaperCorpusId": "206596723"
                        },
                        {
                            "start": 784,
                            "end": 788,
                            "matchedPaperCorpusId": "131765296"
                        },
                        {
                            "start": 790,
                            "end": 794,
                            "matchedPaperCorpusId": "198185886"
                        },
                        {
                            "start": 796,
                            "end": 800,
                            "matchedPaperCorpusId": "218487294"
                        },
                        {
                            "start": 1058,
                            "end": 1062,
                            "matchedPaperCorpusId": "14659675"
                        },
                        {
                            "start": 1099,
                            "end": 1102,
                            "matchedPaperCorpusId": "829159"
                        },
                        {
                            "start": 1129,
                            "end": 1133,
                            "matchedPaperCorpusId": "53976534"
                        },
                        {
                            "start": 1217,
                            "end": 1221,
                            "matchedPaperCorpusId": "10319744"
                        },
                        {
                            "start": 1257,
                            "end": 1261,
                            "matchedPaperCorpusId": "155092628"
                        },
                        {
                            "start": 1325,
                            "end": 1329,
                            "matchedPaperCorpusId": "208175624"
                        },
                        {
                            "start": 1487,
                            "end": 1491,
                            "matchedPaperCorpusId": "213175568"
                        },
                        {
                            "start": 1499,
                            "end": 1503,
                            "matchedPaperCorpusId": "232135115"
                        },
                        {
                            "start": 1505,
                            "end": 1509,
                            "matchedPaperCorpusId": "233033467"
                        },
                        {
                            "start": 1608,
                            "end": 1612,
                            "matchedPaperCorpusId": "219559263"
                        },
                        {
                            "start": 1614,
                            "end": 1618,
                            "matchedPaperCorpusId": "215745611"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.97265625
                }
            ],
            "relevance_judgement": 0.97265625,
            "relevance_judgment_input_expanded": "# Title: Distilling a Powerful Student Model via Online Knowledge Distillation\n# Venue: IEEE Transactions on Neural Networks and Learning Systems\n# Authors: Shaojie Li, Mingbao Lin, Yan Wang, Feiyue Huang, Yongjian Wu, Yonghong Tian, Ling Shao, Rongrong Ji\n## Abstract\nExisting online knowledge distillation approaches either adopt the student with the best performance or construct an ensemble model for better holistic performance. However, the former strategy ignores other students\u2019 information, while the latter increases the computational complexity during deployment. In this article, we propose a novel method for online knowledge distillation, termed feature fusion and self-distillation (FFSD), which comprises two key components: FFSD, toward solving the above problems in a unified framework. Different from previous works, where all students are treated equally, the proposed FFSD splits them into a leader student set and a common student set. Then, the feature fusion module converts the concatenation of feature maps from all common students into a fused feature map. The fused representation is used to assist the learning of the leader student. To enable the leader student to absorb more diverse information, we design an enhancement strategy to increase the diversity among students. Besides, a self-distillation module is adopted to convert the feature map of deeper layers into a shallower one. Then, the shallower layers are encouraged to mimic the transformed feature maps of the deeper layers, which helps the students to generalize better. After training, we simply adopt the leader student, which achieves superior performance, over the common students, without increasing the storage or inference cost. Extensive experiments on CIFAR-100 and ImageNet demonstrate the superiority of our FFSD over existing works. The code is available at https://github.com/SJLeo/FFSD.\n## II. RELATED WORK\nTraditional Knowledge Distillation. Traditional distillation works transfer knowledge from a cumbersome teacher model to a light-weight student model. As such, a large-scale model has to be trained in advance, based on which various knowledge definitions and transfer strategies are proposed to boost the performance of the student model. The pioneering work [7] performs knowledge representation of a teacher model using the softmax output layer, which converts the logit into a soft probability with a temperature parameter. Following this, a large number of works proposed new forms of knowledge, such as output logits [20], [21], intermediate feature maps [8], [22], [23], attention maps [9], second-order statistics [24], contrastive features [25], [26], or structured knowledge [27], [28], [29]. Another group of methods focus on transfer strategies so as to enable the student model to inherit knowledge from the teacher model. An intuitive solution is to use the Kullback-Leibler divergence or p -loss when the knowledge falls on the soft logit [7], [30] or intermediate representation [8], [9]. Beyond that, Wang et al. [31] utilized the adversarial training scheme in generative adversarial networks (GANs) [32] to transfer knowledge. Jang et al. [33] considered meta-learning to selectively transfer knowledge. In [34], a reinforcement learning based architecture-aware distillation was proposed to pass the structural knowledge to the student. Recently, there are some works [35], [36], [37], [38] that used knowledge distillation for GAN compression to ensure effective compression. The surveys [39], [40] summarized the development of knowledge distillation in recent years. \n\nOnline Knowledge Distillation. Online knowledge distillation has emerged as an alternative that eliminates the dependency on the teacher model. It builds knowledge distillation based on a collection of student models that collaborate through simultaneous training. To this end, Zhang et al. [10] proposed a deep mutual learning strategy where pair-wise students are encouraged to learn from each other by a mimicry loss based on the Kullback-Leibler divergence.",
            "reference_string": "[232380330 | Li et al. | 2021 | Citations: 47]"
        },
        {
            "title": "Teacher-student collaborative knowledge distillation for image classification",
            "venue": "Applied intelligence (Boston)",
            "year": 2022,
            "reference_count": 47,
            "citation_count": 44,
            "influential_citation_count": 1,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://link.springer.com/content/pdf/10.1007/s10489-022-03486-4.pdf",
                "status": "HYBRID",
                "license": "CCBY",
                "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.1007/s10489-022-03486-4?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1007/s10489-022-03486-4, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2817613",
                    "name": "Chuanyun Xu"
                },
                {
                    "authorId": "2164051968",
                    "name": "Wenjian Gao"
                },
                {
                    "authorId": "2164318078",
                    "name": "Tian Li"
                },
                {
                    "authorId": "2164821600",
                    "name": "Nanlan Bai"
                },
                {
                    "authorId": "2155121570",
                    "name": "Gang Li"
                },
                {
                    "authorId": "2145954082",
                    "name": "Yang Zhang"
                }
            ],
            "abstract": "A single model usually cannot learn all the appropriate features with limited data, thus leading to poor performance when test data are used. To improve model performance, we propose a teacher-student collaborative knowledge distillation (TSKD) method based on knowledge distillation and self-distillation. The method consists of two parts: learning in the teacher network and self-teaching in the student network. Learning in the teacher network allows the student network to use knowledge from the teacher network. Self-teaching in the student network is to build a multi-exit network based on self-distillation and provide deep features as supervised information for training. In the inference stage, we use ensembles to vote on the classification results of multiple sub-models in the student network. The experimental results demonstrate the superior performance of our method compared with a traditional knowledge distillation method and a self-distillation-based multi-exit network.",
            "corpus_id": 248683566,
            "sentences": [
                {
                    "corpus_id": "248683566",
                    "title": "Teacher-student collaborative knowledge distillation for image classification",
                    "text": "Knowledge distillation (KD), an important method of model compression [3][4][5], is effective in transferring \"dark knowledge\" from a larger model to a smaller model, allowing the smaller model to approximate the performance level achieved by the larger model [6][7][8]. This concept was first proposed in [9], but then was not explicitly explained. In 2014, [10] proposed an approach that enables a student network to learn the soft targets output by a teacher network and defined the method as knowledge distillation. However, conventional knowledge distillation methods only learn the output of the teacher network, which leads to the loss of intermediate layer knowledge. Later approaches attempted to exploit the information contained in middle model layers by designing different knowledge representations rather than just using the output information [11][12][13][14][15][16][17]. For example, [11] proposed an approach in which the student network simulates not only the output of the teacher network but also the hidden layer characteristics of the teacher network. [12] used attention transfer mechanisms to significantly improve its performance by forcing the student network to mimic the attention map of the powerful teacher network. Although the above algorithms utilized knowledge from the teacher network, they only consider the output of a specific layer of the teacher network. The relational knowledge distillation (RKD) approach proposed by [15] can transfer the structured relationships associated with the output results obtained by the teacher network to the student network, which alleviates the above problem. The correlations among different categories of probabilities may contain useful information to regularize a learning problem, and [16] found that the generation gap between teacher and student representation of mutual information can be minimized through contrastive representation distillation. Based on an adversarial-based learning strategy as a supervisor to guide and optimize lightweight student networks and recover knowledge from teacher networks, [18] recently proposed a knowledge distillation method for one-stage object detection . [19] constructed a compressed model to learn low-dimensional spatial information from potential representations of teacher networks. Most studies have focused on the representation of feature knowledge or methods of maximizing the transfer of teacher network feature knowledge while ignoring the potential capabilities of student networks.",
                    "score": 0.6210257238007683,
                    "section_title": "Knowledge distillation",
                    "char_start_offset": 4620,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 270
                        },
                        {
                            "start": 271,
                            "end": 349
                        },
                        {
                            "start": 350,
                            "end": 519
                        },
                        {
                            "start": 520,
                            "end": 675
                        },
                        {
                            "start": 676,
                            "end": 887
                        },
                        {
                            "start": 888,
                            "end": 1074
                        },
                        {
                            "start": 1075,
                            "end": 1246
                        },
                        {
                            "start": 1247,
                            "end": 1395
                        },
                        {
                            "start": 1396,
                            "end": 1634
                        },
                        {
                            "start": 1635,
                            "end": 1930
                        },
                        {
                            "start": 1931,
                            "end": 2178
                        },
                        {
                            "start": 2179,
                            "end": 2311
                        },
                        {
                            "start": 2312,
                            "end": 2518
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 70,
                            "end": 73,
                            "matchedPaperCorpusId": "167217261"
                        },
                        {
                            "start": 73,
                            "end": 76,
                            "matchedPaperCorpusId": "32588614"
                        },
                        {
                            "start": 76,
                            "end": 79,
                            "matchedPaperCorpusId": "222310537"
                        },
                        {
                            "start": 260,
                            "end": 263,
                            "matchedPaperCorpusId": "206596723"
                        },
                        {
                            "start": 263,
                            "end": 266,
                            "matchedPaperCorpusId": "8451212"
                        },
                        {
                            "start": 266,
                            "end": 269,
                            "matchedPaperCorpusId": "219559263"
                        },
                        {
                            "start": 306,
                            "end": 309,
                            "matchedPaperCorpusId": "11253972"
                        },
                        {
                            "start": 862,
                            "end": 866,
                            "matchedPaperCorpusId": "829159"
                        },
                        {
                            "start": 866,
                            "end": 870,
                            "matchedPaperCorpusId": "198179476"
                        },
                        {
                            "start": 870,
                            "end": 874,
                            "matchedPaperCorpusId": "118649278"
                        },
                        {
                            "start": 874,
                            "end": 878,
                            "matchedPaperCorpusId": "131765296"
                        },
                        {
                            "start": 878,
                            "end": 882,
                            "matchedPaperCorpusId": "204838340"
                        },
                        {
                            "start": 882,
                            "end": 886,
                            "matchedPaperCorpusId": "53213211"
                        },
                        {
                            "start": 1075,
                            "end": 1079,
                            "matchedPaperCorpusId": "829159"
                        },
                        {
                            "start": 1461,
                            "end": 1465,
                            "matchedPaperCorpusId": "131765296"
                        },
                        {
                            "start": 1765,
                            "end": 1769,
                            "matchedPaperCorpusId": "204838340"
                        },
                        {
                            "start": 2091,
                            "end": 2095,
                            "matchedPaperCorpusId": "237734820"
                        },
                        {
                            "start": 2179,
                            "end": 2183,
                            "matchedPaperCorpusId": "225257208"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.97265625
                }
            ],
            "relevance_judgement": 0.97265625,
            "relevance_judgment_input_expanded": "# Title: Teacher-student collaborative knowledge distillation for image classification\n# Venue: Applied intelligence (Boston)\n# Authors: Chuanyun Xu, Wenjian Gao, Tian Li, Nanlan Bai, Gang Li, Yang Zhang\n## Abstract\nA single model usually cannot learn all the appropriate features with limited data, thus leading to poor performance when test data are used. To improve model performance, we propose a teacher-student collaborative knowledge distillation (TSKD) method based on knowledge distillation and self-distillation. The method consists of two parts: learning in the teacher network and self-teaching in the student network. Learning in the teacher network allows the student network to use knowledge from the teacher network. Self-teaching in the student network is to build a multi-exit network based on self-distillation and provide deep features as supervised information for training. In the inference stage, we use ensembles to vote on the classification results of multiple sub-models in the student network. The experimental results demonstrate the superior performance of our method compared with a traditional knowledge distillation method and a self-distillation-based multi-exit network.\n## Knowledge distillation\nKnowledge distillation (KD), an important method of model compression [3][4][5], is effective in transferring \"dark knowledge\" from a larger model to a smaller model, allowing the smaller model to approximate the performance level achieved by the larger model [6][7][8]. This concept was first proposed in [9], but then was not explicitly explained. In 2014, [10] proposed an approach that enables a student network to learn the soft targets output by a teacher network and defined the method as knowledge distillation. However, conventional knowledge distillation methods only learn the output of the teacher network, which leads to the loss of intermediate layer knowledge. Later approaches attempted to exploit the information contained in middle model layers by designing different knowledge representations rather than just using the output information [11][12][13][14][15][16][17]. For example, [11] proposed an approach in which the student network simulates not only the output of the teacher network but also the hidden layer characteristics of the teacher network. [12] used attention transfer mechanisms to significantly improve its performance by forcing the student network to mimic the attention map of the powerful teacher network. Although the above algorithms utilized knowledge from the teacher network, they only consider the output of a specific layer of the teacher network. The relational knowledge distillation (RKD) approach proposed by [15] can transfer the structured relationships associated with the output results obtained by the teacher network to the student network, which alleviates the above problem. The correlations among different categories of probabilities may contain useful information to regularize a learning problem, and [16] found that the generation gap between teacher and student representation of mutual information can be minimized through contrastive representation distillation. Based on an adversarial-based learning strategy as a supervisor to guide and optimize lightweight student networks and recover knowledge from teacher networks, [18] recently proposed a knowledge distillation method for one-stage object detection . [19] constructed a compressed model to learn low-dimensional spatial information from potential representations of teacher networks. Most studies have focused on the representation of feature knowledge or methods of maximizing the transfer of teacher network feature knowledge while ignoring the potential capabilities of student networks.",
            "reference_string": "[248683566 | Xu et al. | 2022 | Citations: 44]"
        },
        {
            "title": "PURSUhInT: In Search of Informative Hint Points Based on Layer Clustering for Knowledge Distillation",
            "venue": "Expert systems with applications",
            "year": 2021,
            "reference_count": 79,
            "citation_count": 7,
            "influential_citation_count": 0,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "http://arxiv.org/pdf/2103.00053",
                "status": "GREEN",
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2103.00053, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "72263298",
                    "name": "Reyhan Kevser Keser"
                },
                {
                    "authorId": "27583231",
                    "name": "Aydin Ayanzadeh"
                },
                {
                    "authorId": "12717766",
                    "name": "O. A. Aghdam"
                },
                {
                    "authorId": "2051713853",
                    "name": "\u00c7aglar Kilcioglu"
                },
                {
                    "authorId": "2021714",
                    "name": "B. U. T\u00f6reyin"
                },
                {
                    "authorId": "2877453",
                    "name": "N. K. Ure"
                }
            ],
            "abstract": null,
            "corpus_id": 232076059,
            "sentences": [
                {
                    "corpus_id": "232076059",
                    "title": "PURSUhInT: In Search of Informative Hint Points Based on Layer Clustering for Knowledge Distillation",
                    "text": "Knowledge distillation (KD) is the process of transferring knowledge between networks, where one usually aims to transfer the knowledge of a big network (teacher) to a smaller/more compact network (student). KD is mostly known due to Hinton's work [16], while it was first proposed by [3]. The most well known form of KD uses the combination of soft targets produced by the teacher model and labels as the target in its objective function. The methods that uses soft targets without hints, are known as output (logit) distillation [14]. \n\nSoft targets are calculated as in (1), by passing training batches through teacher model and using softmax output layer with a higher temperature value (T > 1): \n\nwhere p, z and T show the softened class probability, logit and temperature values, respectively. For instance, i, j \u2208 {0, ..., 99} for a classification task with 100 classes, such as CIFAR-100. The same temperature value is used for generating both softened logits in teacher and student models, and higher T results in a softer class probability distribution. The loss function to train the student network is evaluated in (2) as follows: \n\nwhere \u03bb is the trade-off between logit distillation loss (L logit ) and classification loss (L cls ), y represents the label, p S and p T are the softened logits of student and teacher networks, respectively. Temperature is set to 1 in L cls . Cross-entropy and Kullback-Leibler losses are generaly used for L cls and L logit , respectively. A recent study [10] improves KD, where it introduces Spherical Knowledge Distillation (SKD) that projects all logits of the teacher and the student on a sphere by normalization. In SKD, logits are scaled into a unit vector and multiplied with the average teacher norm in order to recover its norm to the original level. [69] introduces a novel distillation approach by reformulating the classical logit distillation. Furthermore, [47] proposes a logit distillation approach in order to train a transformer student using lightweight teacher models with different architectures.",
                    "score": 0.6144021180252159,
                    "section_title": "Knowledge Distillation",
                    "char_start_offset": 6036,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 207
                        },
                        {
                            "start": 208,
                            "end": 289
                        },
                        {
                            "start": 290,
                            "end": 439
                        },
                        {
                            "start": 440,
                            "end": 536
                        },
                        {
                            "start": 539,
                            "end": 699
                        },
                        {
                            "start": 702,
                            "end": 799
                        },
                        {
                            "start": 800,
                            "end": 896
                        },
                        {
                            "start": 897,
                            "end": 1063
                        },
                        {
                            "start": 1064,
                            "end": 1142
                        },
                        {
                            "start": 1145,
                            "end": 1353
                        },
                        {
                            "start": 1354,
                            "end": 1388
                        },
                        {
                            "start": 1389,
                            "end": 1486
                        },
                        {
                            "start": 1487,
                            "end": 1664
                        },
                        {
                            "start": 1665,
                            "end": 1806
                        },
                        {
                            "start": 1807,
                            "end": 1903
                        },
                        {
                            "start": 1904,
                            "end": 2063
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 285,
                            "end": 288,
                            "matchedPaperCorpusId": "11253972"
                        },
                        {
                            "start": 531,
                            "end": 535,
                            "matchedPaperCorpusId": "102483181"
                        },
                        {
                            "start": 573,
                            "end": 576,
                            "matchedPaperCorpusId": "118649278"
                        },
                        {
                            "start": 1807,
                            "end": 1811,
                            "matchedPaperCorpusId": "247476179"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.97265625
                }
            ],
            "relevance_judgement": 0.97265625,
            "relevance_judgment_input_expanded": "# Title: PURSUhInT: In Search of Informative Hint Points Based on Layer Clustering for Knowledge Distillation\n# Venue: Expert systems with applications\n# Authors: Reyhan Kevser Keser, Aydin Ayanzadeh, O. A. Aghdam, \u00c7aglar Kilcioglu, B. U. T\u00f6reyin, N. K. Ure\n## Abstract\nNone\n## Knowledge Distillation\nKnowledge distillation (KD) is the process of transferring knowledge between networks, where one usually aims to transfer the knowledge of a big network (teacher) to a smaller/more compact network (student). KD is mostly known due to Hinton's work [16], while it was first proposed by [3]. The most well known form of KD uses the combination of soft targets produced by the teacher model and labels as the target in its objective function. The methods that uses soft targets without hints, are known as output (logit) distillation [14]. \n\nSoft targets are calculated as in (1), by passing training batches through teacher model and using softmax output layer with a higher temperature value (T > 1): \n\nwhere p, z and T show the softened class probability, logit and temperature values, respectively. For instance, i, j \u2208 {0, ..., 99} for a classification task with 100 classes, such as CIFAR-100. The same temperature value is used for generating both softened logits in teacher and student models, and higher T results in a softer class probability distribution. The loss function to train the student network is evaluated in (2) as follows: \n\nwhere \u03bb is the trade-off between logit distillation loss (L logit ) and classification loss (L cls ), y represents the label, p S and p T are the softened logits of student and teacher networks, respectively. Temperature is set to 1 in L cls . Cross-entropy and Kullback-Leibler losses are generaly used for L cls and L logit , respectively. A recent study [10] improves KD, where it introduces Spherical Knowledge Distillation (SKD) that projects all logits of the teacher and the student on a sphere by normalization. In SKD, logits are scaled into a unit vector and multiplied with the average teacher norm in order to recover its norm to the original level. [69] introduces a novel distillation approach by reformulating the classical logit distillation. Furthermore, [47] proposes a logit distillation approach in order to train a transformer student using lightweight teacher models with different architectures.",
            "reference_string": "[232076059 | Keser et al. | 2021 | Citations: 7]"
        },
        {
            "title": "SNN-PAR: Energy Efficient Pedestrian Attribute Recognition via Spiking Neural Networks",
            "venue": "arXiv.org",
            "year": 2024,
            "reference_count": 75,
            "citation_count": 0,
            "influential_citation_count": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2410.07857, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2316447852",
                    "name": "Haiyang Wang"
                },
                {
                    "authorId": "2299062895",
                    "name": "Qian Zhu"
                },
                {
                    "authorId": "2325151513",
                    "name": "Mowen She"
                },
                {
                    "authorId": "2325202742",
                    "name": "Yabo Li"
                },
                {
                    "authorId": "2325294395",
                    "name": "Haoyu Song"
                },
                {
                    "authorId": "2325504629",
                    "name": "Minghe Xu"
                },
                {
                    "authorId": "2325835807",
                    "name": "Xiao Wang"
                }
            ],
            "abstract": "Artificial neural network based Pedestrian Attribute Recognition (PAR) has been widely studied in recent years, despite many progresses, however, the energy consumption is still high. To address this issue, in this paper, we propose a Spiking Neural Network (SNN) based framework for energy-efficient attribute recognition. Specifically, we first adopt a spiking tokenizer module to transform the given pedestrian image into spiking feature representations. Then, the output will be fed into the spiking Transformer backbone networks for energy-efficient feature extraction. We feed the enhanced spiking features into a set of feed-forward networks for pedestrian attribute recognition. In addition to the widely used binary cross-entropy loss function, we also exploit knowledge distillation from the artificial neural network to the spiking Transformer network for more accurate attribute recognition. Extensive experiments on three widely used PAR benchmark datasets fully validated the effectiveness of our proposed SNN-PAR framework. The source code of this paper is released on \\url{https://github.com/Event-AHU/OpenPAR}.",
            "corpus_id": 273233176,
            "sentences": [
                {
                    "corpus_id": "273233176",
                    "title": "SNN-PAR: Energy Efficient Pedestrian Attribute Recognition via Spiking Neural Networks",
                    "text": "Knowledge Distillation is a technique for model compression that facilitates a smaller student model in learning from a larger teacher model. The student acquires knowledge by imitating various aspects of the teacher, such as its intermediate features [41], prediction logits [22], or activation boundaries [20]]. This approach was originally put forth by Hinton et al. [21] to supervise students based on the hard and soft label's output by the teacher, and nowadays there is a lot of work on using distillation for knowledge transfer to help the model get better performance. Earlier knowledge distillation (KD) techniques can be classified into three distinct categories: distillation from logits, distillation from features, and distillation based on attention. In terms of logit distillation, DIST [24] employs the Pearson correlation coefficient in place of KL divergence, combining both inter-and intra-class relationships. SRRL [62] ensures that the logits output from the teacher and the features of the student, after the teacher's linear layer, are identical. WSLD [75] examines soft labels and assigns varying weights to them based on the bias-variance trade-off. In addition to logit distillation, Several studies [5,43,64,66] concentrate on transferring knowledge through intermediate features. FitNet [41] directly distills semantic information from these intermediate features. AT [68] shifts the attention from feature maps to the student model. RKD [40] extracts relationships from the feature maps. MGD [65] masks the features of the student model, compelling it to replicate the features of the teacher model. To our knowledge, AT [68] is the sole knowledge distillation method that focuses on transferring attention, defining the attention map as a spatial representation that highlights the areas of the input that the model concentrates on the most. Wang et al. propose the HDETrack [56] which employs a hierarchical knowledge distillation strategy to augment the student tracking network from multi-modal or multi-view teacher network. In this paper, we employ both logits and intermediate features for knowledge distillation, believing that the integration of these two methods can greatly enhance knowledge transfer and improve the effectiveness of the student model. 3 Our Proposed Approach",
                    "score": 0.6383681608915899,
                    "section_title": "Knowledge Distillation",
                    "char_start_offset": 8288,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 141
                        },
                        {
                            "start": 142,
                            "end": 313
                        },
                        {
                            "start": 314,
                            "end": 577
                        },
                        {
                            "start": 578,
                            "end": 765
                        },
                        {
                            "start": 766,
                            "end": 930
                        },
                        {
                            "start": 931,
                            "end": 1070
                        },
                        {
                            "start": 1071,
                            "end": 1175
                        },
                        {
                            "start": 1176,
                            "end": 1308
                        },
                        {
                            "start": 1309,
                            "end": 1393
                        },
                        {
                            "start": 1394,
                            "end": 1462
                        },
                        {
                            "start": 1463,
                            "end": 1517
                        },
                        {
                            "start": 1518,
                            "end": 1629
                        },
                        {
                            "start": 1630,
                            "end": 1872
                        },
                        {
                            "start": 1873,
                            "end": 2059
                        },
                        {
                            "start": 2060,
                            "end": 2293
                        },
                        {
                            "start": 2294,
                            "end": 2317
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 307,
                            "end": 311,
                            "matchedPaperCorpusId": "53213211"
                        },
                        {
                            "start": 803,
                            "end": 807,
                            "matchedPaperCorpusId": "248986690"
                        },
                        {
                            "start": 936,
                            "end": 940,
                            "matchedPaperCorpusId": "235613564"
                        },
                        {
                            "start": 1227,
                            "end": 1230,
                            "matchedPaperCorpusId": "250279758"
                        },
                        {
                            "start": 1230,
                            "end": 1233,
                            "matchedPaperCorpusId": "236882796"
                        },
                        {
                            "start": 1233,
                            "end": 1236,
                            "matchedPaperCorpusId": "244488341"
                        },
                        {
                            "start": 1467,
                            "end": 1471,
                            "matchedPaperCorpusId": "131765296"
                        },
                        {
                            "start": 1522,
                            "end": 1526,
                            "matchedPaperCorpusId": "248506080"
                        },
                        {
                            "start": 1906,
                            "end": 1910,
                            "matchedPaperCorpusId": "262822525"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.97216796875
                }
            ],
            "relevance_judgement": 0.97216796875,
            "relevance_judgment_input_expanded": "# Title: SNN-PAR: Energy Efficient Pedestrian Attribute Recognition via Spiking Neural Networks\n# Venue: arXiv.org\n# Authors: Haiyang Wang, Qian Zhu, Mowen She, Yabo Li, Haoyu Song, Minghe Xu, Xiao Wang\n## Abstract\nArtificial neural network based Pedestrian Attribute Recognition (PAR) has been widely studied in recent years, despite many progresses, however, the energy consumption is still high. To address this issue, in this paper, we propose a Spiking Neural Network (SNN) based framework for energy-efficient attribute recognition. Specifically, we first adopt a spiking tokenizer module to transform the given pedestrian image into spiking feature representations. Then, the output will be fed into the spiking Transformer backbone networks for energy-efficient feature extraction. We feed the enhanced spiking features into a set of feed-forward networks for pedestrian attribute recognition. In addition to the widely used binary cross-entropy loss function, we also exploit knowledge distillation from the artificial neural network to the spiking Transformer network for more accurate attribute recognition. Extensive experiments on three widely used PAR benchmark datasets fully validated the effectiveness of our proposed SNN-PAR framework. The source code of this paper is released on \\url{https://github.com/Event-AHU/OpenPAR}.\n## Knowledge Distillation\nKnowledge Distillation is a technique for model compression that facilitates a smaller student model in learning from a larger teacher model. The student acquires knowledge by imitating various aspects of the teacher, such as its intermediate features [41], prediction logits [22], or activation boundaries [20]]. This approach was originally put forth by Hinton et al. [21] to supervise students based on the hard and soft label's output by the teacher, and nowadays there is a lot of work on using distillation for knowledge transfer to help the model get better performance. Earlier knowledge distillation (KD) techniques can be classified into three distinct categories: distillation from logits, distillation from features, and distillation based on attention. In terms of logit distillation, DIST [24] employs the Pearson correlation coefficient in place of KL divergence, combining both inter-and intra-class relationships. SRRL [62] ensures that the logits output from the teacher and the features of the student, after the teacher's linear layer, are identical. WSLD [75] examines soft labels and assigns varying weights to them based on the bias-variance trade-off. In addition to logit distillation, Several studies [5,43,64,66] concentrate on transferring knowledge through intermediate features. FitNet [41] directly distills semantic information from these intermediate features. AT [68] shifts the attention from feature maps to the student model. RKD [40] extracts relationships from the feature maps. MGD [65] masks the features of the student model, compelling it to replicate the features of the teacher model. To our knowledge, AT [68] is the sole knowledge distillation method that focuses on transferring attention, defining the attention map as a spatial representation that highlights the areas of the input that the model concentrates on the most. Wang et al. propose the HDETrack [56] which employs a hierarchical knowledge distillation strategy to augment the student tracking network from multi-modal or multi-view teacher network. In this paper, we employ both logits and intermediate features for knowledge distillation, believing that the integration of these two methods can greatly enhance knowledge transfer and improve the effectiveness of the student model. 3 Our Proposed Approach",
            "reference_string": "[273233176 | Wang et al. | 2024 | Citations: 0]"
        },
        {
            "title": "Joint Embedding of 2D and 3D Networks for Medical Image Anomaly Detection",
            "venue": "arXiv.org",
            "year": 2022,
            "reference_count": 37,
            "citation_count": 1,
            "influential_citation_count": 0,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "http://arxiv.org/pdf/2212.10939",
                "status": "GREEN",
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2212.10939, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2052278844",
                    "name": "In-Joo Kang"
                },
                {
                    "authorId": "2115954336",
                    "name": "Jinah Park"
                }
            ],
            "abstract": "Obtaining ground truth data in medical imaging has difficulties due to the fact that it requires a lot of annotating time from the experts in the field. Also, when trained with supervised learning, it detects only the cases included in the labels. In real practice, we want to also open to other possibilities than the named cases while examining the medical images. As a solution, the need for anomaly detection that can detect and localize abnormalities by learning the normal characteristics using only normal images is emerging. With medical image data, we can design either 2D or 3D networks of self-supervised learning for anomaly detection task. Although 3D networks, which learns 3D structures of the human body, show good performance in 3D medical image anomaly detection, they cannot be stacked in deeper layers due to memory problems. While 2D networks have advantage in feature detection, they lack 3D context information. In this paper, we develop a method for combining the strength of the 3D network and the strength of the 2D network through joint embedding. We also propose the pretask of self-supervised learning to make it possible for the networks to learn efficiently. Through the experiments, we show that the proposed method achieves better performance in both classification and segmentation tasks compared to the SoTA method.",
            "corpus_id": 254926873,
            "sentences": [
                {
                    "corpus_id": "254926873",
                    "title": "Joint Embedding of 2D and 3D Networks for Medical Image Anomaly Detection",
                    "text": "Knowledge Distillation. Knowledge Distillation aims to transfer knowledge of pretrained huge networks to relatively light networks. It is firstly devised by Hinton et al [17], with the idea that wide and deep networks can extract the key point features well. Therefore, if the huge network, coined as teacher network, can distillate their feature extract performances to small networks, coined as student networks, it can be said as memory-efficient networks. Initially, knowledge distillation models use two loss function, student loss and distillation loss. Student loss refers that general cross entropy loss between student network's prediction results and labels. Distillation loss refers that comparing the results between teacher networks and student networks to distillate the teacher's knowledge to student [18]. Some paper use the knowledge distillation for model compression [19]. Knowledge distillation is viewed as function matching, and when knowledge distillation is applied with the function matching method proposed in this paper, the student reaches the teacher's accuracy. The accuracy of the teacher and the student is the same, but the model size of the student is small, so it can be thought of as a model compression method. By applying knowledge distillation to a model that people lightly use this latest model, we get a generalized model. Distillation for model compression highlights two methods. First, augmentation with the same input image of teacher and student should be applied. The second emphasizes the incredibly long learning times. According to the results of this paper [19], it can be confirmed that the performance of knowledge distillation takes a extremely long time to train. \n\nJoint Embedding. Multi-modal learning is a method of using data types with different characteristics rather than one type of data. The difficulty of learning multi-modal data is that they have different representations according to their modalities. For example, audio is a 1D signal wave form, an image can be represented as a 2D or 3D array and the text has embedding data corresponding to the word [20]. The method of learning embedding vectors to match different representations is called joint embedding. The most important aspect of cross-modality research is learning a common subspace in which different types of modalities can be directly compared.",
                    "score": 0.6161897345747356,
                    "section_title": "Representation Learning",
                    "char_start_offset": 6502,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 23
                        },
                        {
                            "start": 24,
                            "end": 131
                        },
                        {
                            "start": 132,
                            "end": 258
                        },
                        {
                            "start": 259,
                            "end": 459
                        },
                        {
                            "start": 460,
                            "end": 559
                        },
                        {
                            "start": 560,
                            "end": 668
                        },
                        {
                            "start": 669,
                            "end": 821
                        },
                        {
                            "start": 822,
                            "end": 891
                        },
                        {
                            "start": 892,
                            "end": 1091
                        },
                        {
                            "start": 1092,
                            "end": 1247
                        },
                        {
                            "start": 1248,
                            "end": 1364
                        },
                        {
                            "start": 1365,
                            "end": 1423
                        },
                        {
                            "start": 1424,
                            "end": 1511
                        },
                        {
                            "start": 1512,
                            "end": 1569
                        },
                        {
                            "start": 1570,
                            "end": 1719
                        },
                        {
                            "start": 1722,
                            "end": 1738
                        },
                        {
                            "start": 1739,
                            "end": 1852
                        },
                        {
                            "start": 1853,
                            "end": 1971
                        },
                        {
                            "start": 1972,
                            "end": 2128
                        },
                        {
                            "start": 2129,
                            "end": 2231
                        },
                        {
                            "start": 2232,
                            "end": 2379
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 2123,
                            "end": 2127,
                            "matchedPaperCorpusId": "239615993"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.97216796875
                }
            ],
            "relevance_judgement": 0.97216796875,
            "relevance_judgment_input_expanded": "# Title: Joint Embedding of 2D and 3D Networks for Medical Image Anomaly Detection\n# Venue: arXiv.org\n# Authors: In-Joo Kang, Jinah Park\n## Abstract\nObtaining ground truth data in medical imaging has difficulties due to the fact that it requires a lot of annotating time from the experts in the field. Also, when trained with supervised learning, it detects only the cases included in the labels. In real practice, we want to also open to other possibilities than the named cases while examining the medical images. As a solution, the need for anomaly detection that can detect and localize abnormalities by learning the normal characteristics using only normal images is emerging. With medical image data, we can design either 2D or 3D networks of self-supervised learning for anomaly detection task. Although 3D networks, which learns 3D structures of the human body, show good performance in 3D medical image anomaly detection, they cannot be stacked in deeper layers due to memory problems. While 2D networks have advantage in feature detection, they lack 3D context information. In this paper, we develop a method for combining the strength of the 3D network and the strength of the 2D network through joint embedding. We also propose the pretask of self-supervised learning to make it possible for the networks to learn efficiently. Through the experiments, we show that the proposed method achieves better performance in both classification and segmentation tasks compared to the SoTA method.\n## Representation Learning\nKnowledge Distillation. Knowledge Distillation aims to transfer knowledge of pretrained huge networks to relatively light networks. It is firstly devised by Hinton et al [17], with the idea that wide and deep networks can extract the key point features well. Therefore, if the huge network, coined as teacher network, can distillate their feature extract performances to small networks, coined as student networks, it can be said as memory-efficient networks. Initially, knowledge distillation models use two loss function, student loss and distillation loss. Student loss refers that general cross entropy loss between student network's prediction results and labels. Distillation loss refers that comparing the results between teacher networks and student networks to distillate the teacher's knowledge to student [18]. Some paper use the knowledge distillation for model compression [19]. Knowledge distillation is viewed as function matching, and when knowledge distillation is applied with the function matching method proposed in this paper, the student reaches the teacher's accuracy. The accuracy of the teacher and the student is the same, but the model size of the student is small, so it can be thought of as a model compression method. By applying knowledge distillation to a model that people lightly use this latest model, we get a generalized model. Distillation for model compression highlights two methods. First, augmentation with the same input image of teacher and student should be applied. The second emphasizes the incredibly long learning times. According to the results of this paper [19], it can be confirmed that the performance of knowledge distillation takes a extremely long time to train. \n\nJoint Embedding. Multi-modal learning is a method of using data types with different characteristics rather than one type of data. The difficulty of learning multi-modal data is that they have different representations according to their modalities. For example, audio is a 1D signal wave form, an image can be represented as a 2D or 3D array and the text has embedding data corresponding to the word [20]. The method of learning embedding vectors to match different representations is called joint embedding. The most important aspect of cross-modality research is learning a common subspace in which different types of modalities can be directly compared.",
            "reference_string": "[254926873 | Kang et al. | 2022 | Citations: 1]"
        },
        {
            "title": "Emotions Beyond Words: Non-Speech Audio Emotion Recognition With Edge Computing",
            "venue": "arXiv.org",
            "year": 2023,
            "reference_count": 43,
            "citation_count": 3,
            "influential_citation_count": 0,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "http://arxiv.org/pdf/2305.00725",
                "status": "CLOSED",
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2305.00725, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2105891149",
                    "name": "Ibrahim Malik"
                },
                {
                    "authorId": "24040678",
                    "name": "S. Latif"
                },
                {
                    "authorId": "1483726395",
                    "name": "Sanaullah Manzoor"
                },
                {
                    "authorId": "145474282",
                    "name": "Muhammad Usama"
                },
                {
                    "authorId": "1734917",
                    "name": "Junaid Qadir"
                },
                {
                    "authorId": "1770270",
                    "name": "R. Jurdak"
                }
            ],
            "abstract": "Non-speech emotion recognition has a wide range of applications including healthcare, crime control and rescue, and entertainment, to name a few. Providing these applications using edge computing has great potential, however, recent studies are focused on speech-emotion recognition using complex architectures. In this paper, a non-speech-based emotion recognition system is proposed, which can rely on edge computing to analyse emotions conveyed through non-speech expressions like screaming and crying. In particular, we explore knowledge distillation to design a computationally efficient system that can be deployed on edge devices with limited resources without degrading the performance significantly. We comprehensively evaluate our proposed framework using two publicly available datasets and highlight its effectiveness by comparing the results with the well-known MobileNet model. Our results demonstrate the feasibility and effectiveness of using edge computing for non-speech emotion detection, which can potentially improve applications that rely on emotion detection in communication networks. To the best of our knowledge, this is the first work on an edge-computing-based framework for detecting emotions in non-speech audio, offering promising directions for future research.",
            "corpus_id": 258426697,
            "sentences": [
                {
                    "corpus_id": "258426697",
                    "title": "Emotions Beyond Words: Non-Speech Audio Emotion Recognition With Edge Computing",
                    "text": "The process of knowledge distillation as the name suggests is the method of transferring knowledge from a larger computationally expensive model to a relatively smaller model. The larger and smaller models are called the teacher and student models respectively. Thus, knowledge distillation consists of three principal components: (1) knowledge; (2) distillation algorithm; and (3) teacherstudent architecture. While there are now multiple methods of distillation algorithms we selected the response-based algorithm. As shown in Figure 3, the hypothesis is that the student model will learn to mimic the predictions of the teacher model. This can be achieved by using a loss function, termed the distillation loss, that captures the difference between the logits of the student and the teacher model respectively. As this loss minimizes overtraining, the student model will improve at making the same predictions as the teacher. In the offline training scheme, the teacher model is first trained and the weights are then frozen. Next, we train the student model using the distillation loss and the logits from the teacher model as targets. Following is the equation of the distillation loss. \n\nwhere: L d : the loss function for knowledge distillation \u03b1: a hyperparameter that controls the trade-off between the classification loss and the distillation loss T : the temperature hyperparameter used to soften the logits (outputs of the last layer before softmax) of the teacher and student models KL: the Kullback-Leibler divergence, a measure of how different two probability distributions are softmax: a function that converts the logits to probabilities f (T, x): the logits of the teacher model for input x g(T, x): the logits of the student model for input x 2) Teacher Model: Generally, for the teacher model a larger and deeper network is chosen so that it performs well on the task at hand. We chose ResNet18 [38] as our teacher model. ResNet18 contains 18 residual blocks stacked together which alleviates the degradation and vanishing gradient problem. Figure 4 shows a single layer, where the outputs of the previous layer are added to the outputs of the next layer, and Figure 5 depicts the complete architecture of ResNet18 used for the teacher model.",
                    "score": 0.6362803001654813,
                    "section_title": "1) Knowledge Distillation:",
                    "char_start_offset": 16396,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 175
                        },
                        {
                            "start": 176,
                            "end": 261
                        },
                        {
                            "start": 262,
                            "end": 410
                        },
                        {
                            "start": 411,
                            "end": 516
                        },
                        {
                            "start": 517,
                            "end": 637
                        },
                        {
                            "start": 638,
                            "end": 813
                        },
                        {
                            "start": 814,
                            "end": 928
                        },
                        {
                            "start": 929,
                            "end": 1028
                        },
                        {
                            "start": 1029,
                            "end": 1139
                        },
                        {
                            "start": 1140,
                            "end": 1191
                        },
                        {
                            "start": 1194,
                            "end": 1897
                        },
                        {
                            "start": 1898,
                            "end": 1942
                        },
                        {
                            "start": 1943,
                            "end": 2263
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 1916,
                            "end": 1920,
                            "matchedPaperCorpusId": "206594692"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.9716796875
                }
            ],
            "relevance_judgement": 0.9716796875,
            "relevance_judgment_input_expanded": "# Title: Emotions Beyond Words: Non-Speech Audio Emotion Recognition With Edge Computing\n# Venue: arXiv.org\n# Authors: Ibrahim Malik, S. Latif, Sanaullah Manzoor, Muhammad Usama, Junaid Qadir, R. Jurdak\n## Abstract\nNon-speech emotion recognition has a wide range of applications including healthcare, crime control and rescue, and entertainment, to name a few. Providing these applications using edge computing has great potential, however, recent studies are focused on speech-emotion recognition using complex architectures. In this paper, a non-speech-based emotion recognition system is proposed, which can rely on edge computing to analyse emotions conveyed through non-speech expressions like screaming and crying. In particular, we explore knowledge distillation to design a computationally efficient system that can be deployed on edge devices with limited resources without degrading the performance significantly. We comprehensively evaluate our proposed framework using two publicly available datasets and highlight its effectiveness by comparing the results with the well-known MobileNet model. Our results demonstrate the feasibility and effectiveness of using edge computing for non-speech emotion detection, which can potentially improve applications that rely on emotion detection in communication networks. To the best of our knowledge, this is the first work on an edge-computing-based framework for detecting emotions in non-speech audio, offering promising directions for future research.\n## 1) Knowledge Distillation:\nThe process of knowledge distillation as the name suggests is the method of transferring knowledge from a larger computationally expensive model to a relatively smaller model. The larger and smaller models are called the teacher and student models respectively. Thus, knowledge distillation consists of three principal components: (1) knowledge; (2) distillation algorithm; and (3) teacherstudent architecture. While there are now multiple methods of distillation algorithms we selected the response-based algorithm. As shown in Figure 3, the hypothesis is that the student model will learn to mimic the predictions of the teacher model. This can be achieved by using a loss function, termed the distillation loss, that captures the difference between the logits of the student and the teacher model respectively. As this loss minimizes overtraining, the student model will improve at making the same predictions as the teacher. In the offline training scheme, the teacher model is first trained and the weights are then frozen. Next, we train the student model using the distillation loss and the logits from the teacher model as targets. Following is the equation of the distillation loss. \n\nwhere: L d : the loss function for knowledge distillation \u03b1: a hyperparameter that controls the trade-off between the classification loss and the distillation loss T : the temperature hyperparameter used to soften the logits (outputs of the last layer before softmax) of the teacher and student models KL: the Kullback-Leibler divergence, a measure of how different two probability distributions are softmax: a function that converts the logits to probabilities f (T, x): the logits of the teacher model for input x g(T, x): the logits of the student model for input x 2) Teacher Model: Generally, for the teacher model a larger and deeper network is chosen so that it performs well on the task at hand. We chose ResNet18 [38] as our teacher model. ResNet18 contains 18 residual blocks stacked together which alleviates the degradation and vanishing gradient problem. Figure 4 shows a single layer, where the outputs of the previous layer are added to the outputs of the next layer, and Figure 5 depicts the complete architecture of ResNet18 used for the teacher model.",
            "reference_string": "[258426697 | Malik et al. | 2023 | Citations: 3]"
        },
        {
            "title": "ToXCL: A Unified Framework for Toxic Speech Detection and Explanation",
            "venue": "North American Chapter of the Association for Computational Linguistics",
            "year": 2024,
            "reference_count": 52,
            "citation_count": 6,
            "influential_citation_count": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2403.16685, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2240194981",
                    "name": "Nhat M. Hoang"
                },
                {
                    "authorId": "2060491855",
                    "name": "Do Xuan Long"
                },
                {
                    "authorId": "2140149071",
                    "name": "Duc Anh Do"
                },
                {
                    "authorId": "2217555512",
                    "name": "Duc Anh Vu"
                },
                {
                    "authorId": "26336902",
                    "name": "Anh Tuan Luu"
                }
            ],
            "abstract": "The proliferation of online toxic speech is a pertinent problem posing threats to demographic groups. While explicit toxic speech contains offensive lexical signals, implicit one consists of coded or indirect language. Therefore, it is crucial for models not only to detect implicit toxic speech but also to explain its toxicity. This draws a unique need for unified frameworks that can effectively detect and explain implicit toxic speech. Prior works mainly formulated the task of toxic speech detection and explanation as a text generation problem. Nonetheless, models trained using this strategy can be prone to suffer from the consequent error propagation problem. Moreover, our experiments reveal that the detection results of such models are much lower than those that focus only on the detection task. To bridge these gaps, we introduce ToXCL, a unified framework for the detection and explanation of implicit toxic speech. Our model consists of three modules: a (i) Target Group Generator to generate the targeted demographic group(s) of a given post; an (ii) Encoder-Decoder Model in which the encoder focuses on detecting implicit toxic speech and is boosted by a (iii) Teacher Classifier via knowledge distillation, and the decoder generates the necessary explanation. ToXCL achieves new state-of-the-art effectiveness, and outperforms baselines significantly.",
            "corpus_id": 268681103,
            "sentences": [
                {
                    "corpus_id": "268681103",
                    "title": "ToXCL: A Unified Framework for Toxic Speech Detection and Explanation",
                    "text": "Knowledge distillation (Hinton et al., 2015) is a technique that enables a smaller student model to learn from a larger teacher model by transferring knowledge.It has proven effective in improving performance, reducing computational requirements, and increasing efficiency in the field of Computer Vision (Gou et al., 2021).Recently, researchers have explored applying knowledge distillation in Natural Language Processing.For example, Fu et al. (2020) used a contrastive approach to align the intermediate layer outputs of the teacher and student models.Turc et al. (2019) extensively studied the interaction between pre-training, distillation, and fine-tuning, demonstrating the effectiveness of pre-trained distillation in tasks like sentiment analysis.Additionally, Clark et al. (2019) trained a multitasking network by ensembling multiple single-task teachers.In our work, we distill the knowledge from a teacher classifier to our model's classifier (the student classifier), optimizing the Kullback-Leibler distance (Csisz\u00e1r, 1975) between soft labels.",
                    "score": 0.6661235107530565,
                    "section_title": "Knowledge Distillation",
                    "char_start_offset": 6794,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 160
                        },
                        {
                            "start": 160,
                            "end": 324
                        },
                        {
                            "start": 324,
                            "end": 423
                        },
                        {
                            "start": 423,
                            "end": 555
                        },
                        {
                            "start": 555,
                            "end": 756
                        },
                        {
                            "start": 756,
                            "end": 865
                        },
                        {
                            "start": 865,
                            "end": 1058
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 305,
                            "end": 323,
                            "matchedPaperCorpusId": "219559263"
                        },
                        {
                            "start": 770,
                            "end": 789,
                            "matchedPaperCorpusId": "85464175"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.97119140625
                }
            ],
            "relevance_judgement": 0.97119140625,
            "relevance_judgment_input_expanded": "# Title: ToXCL: A Unified Framework for Toxic Speech Detection and Explanation\n# Venue: North American Chapter of the Association for Computational Linguistics\n# Authors: Nhat M. Hoang, Do Xuan Long, Duc Anh Do, Duc Anh Vu, Anh Tuan Luu\n## Abstract\nThe proliferation of online toxic speech is a pertinent problem posing threats to demographic groups. While explicit toxic speech contains offensive lexical signals, implicit one consists of coded or indirect language. Therefore, it is crucial for models not only to detect implicit toxic speech but also to explain its toxicity. This draws a unique need for unified frameworks that can effectively detect and explain implicit toxic speech. Prior works mainly formulated the task of toxic speech detection and explanation as a text generation problem. Nonetheless, models trained using this strategy can be prone to suffer from the consequent error propagation problem. Moreover, our experiments reveal that the detection results of such models are much lower than those that focus only on the detection task. To bridge these gaps, we introduce ToXCL, a unified framework for the detection and explanation of implicit toxic speech. Our model consists of three modules: a (i) Target Group Generator to generate the targeted demographic group(s) of a given post; an (ii) Encoder-Decoder Model in which the encoder focuses on detecting implicit toxic speech and is boosted by a (iii) Teacher Classifier via knowledge distillation, and the decoder generates the necessary explanation. ToXCL achieves new state-of-the-art effectiveness, and outperforms baselines significantly.\n## Knowledge Distillation\nKnowledge distillation (Hinton et al., 2015) is a technique that enables a smaller student model to learn from a larger teacher model by transferring knowledge.It has proven effective in improving performance, reducing computational requirements, and increasing efficiency in the field of Computer Vision (Gou et al., 2021).Recently, researchers have explored applying knowledge distillation in Natural Language Processing.For example, Fu et al. (2020) used a contrastive approach to align the intermediate layer outputs of the teacher and student models.Turc et al. (2019) extensively studied the interaction between pre-training, distillation, and fine-tuning, demonstrating the effectiveness of pre-trained distillation in tasks like sentiment analysis.Additionally, Clark et al. (2019) trained a multitasking network by ensembling multiple single-task teachers.In our work, we distill the knowledge from a teacher classifier to our model's classifier (the student classifier), optimizing the Kullback-Leibler distance (Csisz\u00e1r, 1975) between soft labels.",
            "reference_string": "[268681103 | Hoang et al. | 2024 | Citations: 6]"
        }
    ],
    "retrieved": [
        {
            "corpus_id": "228372613",
            "title": "Improving Task-Agnostic BERT Distillation with Layer Mapping Search",
            "text": "Knowledge Distillation (Hinton et al., 2015) (KD) aims to transfer the knowledge of a large teacher network T to a small student network S. The knowledge transfer can be done by making student model mimic the behaviors of teacher model, which is formulated as minimizing the following objective:\n\nwhere L is a loss function, X denotes the dataset, T (x) and S(x) mean the behavior functions of teacher and student networks on data sample x respectively.Original KD defines the softened output as the behavior.",
            "score": 0.9024730407013375,
            "section_title": "Knowledge Distillation",
            "char_start_offset": 10073,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 295
                },
                {
                    "start": 297,
                    "end": 453
                },
                {
                    "start": 453,
                    "end": 509
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.86962890625
        },
        {
            "corpus_id": "267759723",
            "title": "Improve Cross-Architecture Generalization on Dataset Distillation",
            "text": "Knowledge distillation is a method to transfer knowledge from a large model to a small model. It's first proposed in this paper [12]. The large model is called the teacher model and the small model is called the student model. The student model is trained to mimic the output of the teacher model. The student model is usually a shallow neural network with fewer parameters and faster inference speed. The teacher model is usually a deep neural network with more parameters and slower inference speed. The student model is trained on the same dataset as the teacher model to minimize the difference between the output of the student model and the output of the teacher model (i.e. maintain the knowledge as much as possible). \n\nAfter knowledge distillation was proposed, a lot of work has been put into exploring further applications in diverse fields, such as speech recognition, image recognition, and natural language processing. It proved that knowledge distillation is a great success in these various fields [11]. \n\nHowever, there are some arguments against knowledge distillation. A paper [22] said that, while knowledge distillation improves the generalization of student models, there often remains a surprisingly large discrepancy between the predictive distributions of the teacher and the student, even in cases when the student can perfectly match the teacher.",
            "score": 0.8812893508811146,
            "section_title": "Knowledge Distillation",
            "char_start_offset": 6172,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 93
                },
                {
                    "start": 94,
                    "end": 133
                },
                {
                    "start": 134,
                    "end": 226
                },
                {
                    "start": 227,
                    "end": 297
                },
                {
                    "start": 298,
                    "end": 401
                },
                {
                    "start": 402,
                    "end": 501
                },
                {
                    "start": 502,
                    "end": 680
                },
                {
                    "start": 681,
                    "end": 725
                },
                {
                    "start": 728,
                    "end": 932
                },
                {
                    "start": 933,
                    "end": 1019
                },
                {
                    "start": 1022,
                    "end": 1087
                },
                {
                    "start": 1088,
                    "end": 1373
                }
            ],
            "ref_mentions": [
                {
                    "start": 1014,
                    "end": 1018,
                    "matchedPaperCorpusId": "219559263"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.9794921875
        },
        {
            "corpus_id": "227228204",
            "title": "A Selective Survey on Versatile Knowledge Distillation Paradigm for Neural Network Models",
            "text": "In the perspective of a Single Teacher-Single Student learning paradigm, knowledge distillation is a simple way to transfer knowledge of a teacher to improve the performance of small deep learning model called a student. More specifically, knowledge distillation refers to the method that helps the training process of a small network (student) under the supervision of a large network (teacher). The additional supervision about the relative probabilities of secondary class and relational information between data points at the output of Teacher network can be useful in increasing the efficacy of the Student network. \n\nWe can downsize a student network regardless of the structural difference between teacher and student. Allowing this architectural flexibility, knowledge distillation is emerging as a next generation approach of network compression. However, too excessive gap of the capacity between a teacher network and a student network is a critical obstacle for knowledge transfer performance. This is empirically shown and analyzed in [17]. \n\n2) Multi-Step Learning Seyed-Iman Mirzadeh et al. [17] showed that the student network performance degrades when the gap between student and teacher networks is large. So, given a fixed student network, one cannot employ an arbitrarily large teacher network; in other words, a teacher network can effectively transfer its knowledge to student networks having up to a certain capacity. \n\nTo alleviate this shortcoming, multi-step knowledge distillation was introduced. It is a new distillation framework called Teacher Assistant Knowledge Distillation (TAKD), which introduces intermediate-sized network known as teacher assistants (TAs) between the teacher and the student to fill in the gap. TA models are distilled from the teacher, and the student is then only distilled from the TAs. Through extensive empirical evaluations and a theoretical justification, they showed that introducing intermediate TA networks improve the distillation performance and concluded that the size (capacity) gap difference between a teacher-TA and a TA-student is important. \n\nFig. 1. A teacher assistant network fills the gap between student and teacher networks [17]. \n\nFig. 1 shows the overall knowledge distillation structure incorporating teacher assistant. \n\n3) Multiple-Teacher Learning Shan You et al. [15] proposed a new method which uses multiple teacher networks to train a thin and deep student network. Fig. 2 shows overall knowledge distillation incorporating multiple teachers.",
            "score": 0.8586144857761961,
            "section_title": "1) Single Teacher-Single Student",
            "char_start_offset": 8203,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 220
                },
                {
                    "start": 221,
                    "end": 396
                },
                {
                    "start": 397,
                    "end": 620
                },
                {
                    "start": 623,
                    "end": 725
                },
                {
                    "start": 726,
                    "end": 855
                },
                {
                    "start": 856,
                    "end": 1005
                },
                {
                    "start": 1006,
                    "end": 1053
                },
                {
                    "start": 1056,
                    "end": 1223
                },
                {
                    "start": 1224,
                    "end": 1440
                },
                {
                    "start": 1443,
                    "end": 1523
                },
                {
                    "start": 1524,
                    "end": 1748
                },
                {
                    "start": 1749,
                    "end": 1843
                },
                {
                    "start": 1844,
                    "end": 2113
                },
                {
                    "start": 2116,
                    "end": 2123
                },
                {
                    "start": 2124,
                    "end": 2208
                },
                {
                    "start": 2211,
                    "end": 2301
                },
                {
                    "start": 2304,
                    "end": 2454
                },
                {
                    "start": 2455,
                    "end": 2531
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.82568359375
        },
        {
            "corpus_id": "269327338",
            "title": "Transfer learning and self-distillation for automated detection of schizophrenia using single-channel EEG and scalogram images.",
            "text": "Knowledge Distillation denotes the procedure of transferring knowledge from a sophisticated model to a more straightforward one. It involves training smaller models to achieve similar accuracy as larger models by leveraging the knowledge gained from the larger models. Within the context of knowledge distillation, the term \"teacher network\" is used to describe the larger model, whereas the \"student network\" refers to the smaller network. The fundamental concept behind Knowledge Distillation is to train a smaller and less complex model to imitate the behavior and generalization capabilities of a larger and more complex model. The process of knowledge distillation involves transferring knowledge from the teacher network to the student network by optimizing a loss function [32]. Figure 6 [33] illustrates a typical framework for knowledge distillation, where a teacher-student relationship is established.",
            "score": 0.8343212112949687,
            "section_title": "Knowledge distillation",
            "char_start_offset": 17184,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 128
                },
                {
                    "start": 129,
                    "end": 268
                },
                {
                    "start": 269,
                    "end": 440
                },
                {
                    "start": 441,
                    "end": 631
                },
                {
                    "start": 632,
                    "end": 785
                },
                {
                    "start": 786,
                    "end": 912
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.98291015625
        },
        {
            "corpus_id": "270764542",
            "title": "Aligning Teacher with Student Preferences for Tailored Training Data Generation",
            "text": "Knowledge Distillation Knowledge distillation is a widely used technique to transfer knowledge from a large teacher model to a smaller student model [18].In this study, we focus on scenarios where",
            "score": 0.824857936626565,
            "section_title": "Preliminaries",
            "char_start_offset": 4875,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 154
                },
                {
                    "start": 154,
                    "end": 196
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.52490234375
        },
        {
            "corpus_id": "277349784",
            "title": "Neuroplasticity in Artificial Intelligence - An Overview and Inspirations on Drop In & Out Learning",
            "text": "b) Student-teacher model: Student-teacher models on the other hand belong to the more applicable attempts to reduce model size while keeping high performance. The idea was proposed by Hinton et al. [46], where a well-trained large model serves as the \"teacher model\" and a less complex model acts as the \"student model\". There are two main directions in the student-teacher framework. The first is knowledge distillation, in which the teacher transfers its knowledge to the student; Hu et al. [47] conducted a survey on this approach. The second is reverse knowledge distillation, where the student model transfers its acquired knowledge back to the teacher to enhance its capabilities [48]. These two approaches constitute the \"learning-to-teach\" paradigm [49]. \n\nThe knowledge distillation process can be interpreted in relation to the concepts of neuroplasticity, as it can be likened to the identification and dynamic removal of useless knowledge from neurons in large-scale teacher models. This results in smaller-scale student models with concentrated and selectively retained knowledge. It demonstrates the advantages of neural networks of different sizes, through which distilled knowledge flows. In contrast, the reverse process differs, involving the student model providing additional information to the teacher for verification.",
            "score": 0.8204014227842671,
            "section_title": "A. Duality of Small and Large Networks",
            "char_start_offset": 10864,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 158
                },
                {
                    "start": 159,
                    "end": 320
                },
                {
                    "start": 321,
                    "end": 384
                },
                {
                    "start": 385,
                    "end": 534
                },
                {
                    "start": 535,
                    "end": 691
                },
                {
                    "start": 692,
                    "end": 762
                },
                {
                    "start": 765,
                    "end": 994
                },
                {
                    "start": 995,
                    "end": 1093
                },
                {
                    "start": 1094,
                    "end": 1204
                },
                {
                    "start": 1205,
                    "end": 1340
                }
            ],
            "ref_mentions": [
                {
                    "start": 686,
                    "end": 690,
                    "matchedPaperCorpusId": "259991069"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.8212890625
        },
        {
            "corpus_id": "227012681",
            "title": "Privileged Knowledge Distillation for Online Action Detection",
            "text": "Knowledge Distillation. Ordinarily, knowledge distillation is an effective technique that can transfer the knowledge from teachers, i.e., large complex models, to students, i.e., small simple models. After Hinton et al. [16] addressed knowledge distillation by training the student with the soft target provided by the teacher, knowledge distillation has been widely adopted in a variety of learning tasks. [3,2,7,15,38,21] let the student imitate the output logits of the teacher; [27,32,28,1,11,37] were focused on minimizing the representations between intermediate layers of the student and teacher. Besides, some studies introduced the idea of progressive training. TAKD [23] and DGKD [30] introduced medium size models, called teacher assistant, to bridge the architecture gap between the teacher and student models.",
            "score": 0.8187734934916997,
            "section_title": "Related Work",
            "char_start_offset": 6357,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 23
                },
                {
                    "start": 24,
                    "end": 199
                },
                {
                    "start": 200,
                    "end": 406
                },
                {
                    "start": 407,
                    "end": 603
                },
                {
                    "start": 604,
                    "end": 670
                },
                {
                    "start": 671,
                    "end": 822
                }
            ],
            "ref_mentions": [
                {
                    "start": 407,
                    "end": 410,
                    "matchedPaperCorpusId": "11536917"
                },
                {
                    "start": 410,
                    "end": 412,
                    "matchedPaperCorpusId": "118649278"
                },
                {
                    "start": 412,
                    "end": 414,
                    "matchedPaperCorpusId": "203642130"
                },
                {
                    "start": 414,
                    "end": 417,
                    "matchedPaperCorpusId": "204907215"
                },
                {
                    "start": 417,
                    "end": 420,
                    "matchedPaperCorpusId": "54436113"
                },
                {
                    "start": 420,
                    "end": 423,
                    "matchedPaperCorpusId": "69629714"
                },
                {
                    "start": 482,
                    "end": 486,
                    "matchedPaperCorpusId": "2723173"
                },
                {
                    "start": 486,
                    "end": 489,
                    "matchedPaperCorpusId": "198179476"
                },
                {
                    "start": 489,
                    "end": 492,
                    "matchedPaperCorpusId": "54447578"
                },
                {
                    "start": 492,
                    "end": 494,
                    "matchedPaperCorpusId": "203953149"
                },
                {
                    "start": 497,
                    "end": 500,
                    "matchedPaperCorpusId": "221970924"
                },
                {
                    "start": 676,
                    "end": 680,
                    "matchedPaperCorpusId": "212908749"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.94384765625
        },
        {
            "corpus_id": "275847824",
            "title": "Self-trainable and adaptive sensor intelligence for selective data generation",
            "text": "Knowledge distillation (KD) is a method to transfer knowledge from a larger network or ensemble of networks (teacher model) to a smaller and less complex model (student model) (Hinton et al., 2015). It can be considered as a way to compress a larger model into a smaller one, making it more efficient and less resource-hungry, which is most effective for deploying models on edge devices (Gou et al., 2021). The intuition behind KD is that supervising the student model with the teacher model helps the student model to mimic the teacher model with comparable accuracy. The distilled knowledge from the teacher model also reveals some underlying patterns in the data, making it easier to learn by a smaller model. The very first idea of knowledge distillation is presented in Bucilu\u01ce et al. (2006), where the student model utilizes the predictions of the teacher model on a large set of pseudo data (unlabeled or synthetic data with the same distribution as the original training data) to get an idea of the function learned by the teacher model. The idea is generalized in Hinton et al. (2015) by formalizing knowledge distillation as a method to supervise a small student by a large teacher model to obtain a competitive performance.",
            "score": 0.8112526300755384,
            "section_title": ". Knowledge distillation",
            "char_start_offset": 9996,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 198
                },
                {
                    "start": 199,
                    "end": 407
                },
                {
                    "start": 408,
                    "end": 569
                },
                {
                    "start": 570,
                    "end": 713
                },
                {
                    "start": 714,
                    "end": 1046
                },
                {
                    "start": 1047,
                    "end": 1235
                }
            ],
            "ref_mentions": [
                {
                    "start": 776,
                    "end": 797,
                    "matchedPaperCorpusId": "11253972"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.98828125
        },
        {
            "corpus_id": "245974615",
            "title": "Knowledge Distillation via Weighted Ensemble of Teaching Assistants",
            "text": "Originally proposed by Bucila, Caruana, and Niculescu-Mizil (2006) [6] and popularized by Hinton, Vinyals, and Dean (2015) [2] knowledge distillation compress the knowledge of a large and computational expensive model (often an ensemble of neural networks) to a single computational efficient neural network. The idea of knowledge distillation is to train the small model, the student, on a transfer set with soft targets provided by the large model, the teacher. Knowledge distillation has been commonly used in a number of learning tasks since then. Modeling knowledge transfer between teacher and student has also been done using adversarial methods. Using several teachers was still a good way to improve robustness. Some studies also proposed deep mutual learning which allows an ensemble of student models to learn collaboratively and teach each other during training. \n\nThe main idea of using knowledge distillation is that student network (S) to be trained not only using the true labels information but also observation of how the teacher (T) works with the unseen data provided. Because the teacher model has more more generalization power, the idea is to train the student model is such way that it can mimic the behaviour of that generalization. The teacher network is complex in size being deeper and wider. \n\nLet a t and a s be the logits (the inputs to the final softmax) of the teacher and student network, respectively. In classic supervised learning, the mismatch between output of student network softmax(a s ) and the ground-truth label y r is usually penalized using cross-entropy loss and is given as, \n\nIn knowledge distillation one also tries to match the softened outputs of teacher y t =softmax(a t ) and student y s =softmax(a s ) via Kullback-Leibler divergence loss, \n\nby using a temperature parameter \u03c4 which has an additional control on softening of signal arising from the output of the teacher network. The student network is then trained under the following loss equation which used KD loss and cross-entropy loss, \n\nwhere \u03bb is the parameter used to trade-off between these two losses. This method is used for knowledge distillation from teacher and student models.",
            "score": 0.803701394549467,
            "section_title": "B. Knowledge distillation",
            "char_start_offset": 6434,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 308
                },
                {
                    "start": 309,
                    "end": 463
                },
                {
                    "start": 464,
                    "end": 551
                },
                {
                    "start": 552,
                    "end": 653
                },
                {
                    "start": 654,
                    "end": 720
                },
                {
                    "start": 721,
                    "end": 874
                },
                {
                    "start": 877,
                    "end": 1088
                },
                {
                    "start": 1089,
                    "end": 1257
                },
                {
                    "start": 1258,
                    "end": 1320
                },
                {
                    "start": 1323,
                    "end": 1436
                },
                {
                    "start": 1437,
                    "end": 1623
                },
                {
                    "start": 1626,
                    "end": 1795
                },
                {
                    "start": 1798,
                    "end": 1935
                },
                {
                    "start": 1936,
                    "end": 2048
                },
                {
                    "start": 2051,
                    "end": 2119
                },
                {
                    "start": 2120,
                    "end": 2199
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.98974609375
        },
        {
            "corpus_id": "237571632",
            "title": "Learning Versatile Convolution Filters for Efficient Visual Recognition",
            "text": "Knowledge Distillation. Knowledge distillation, also known as teacher-student framework, aims to transfer knowledge from teacher models to student models, where student models usually enjoy a lighter architecture than that of teacher models. Hinton et al. [22] first proposed the concept of knowledge distillation by introducing the teacher's softened output. Romero et al. [52] further distilled the knowledge underlying the features of the teacher model to the student model. Chen et al. [6] proposed to distill the student network without provided data. However, the major aim of knowledge distillation is to improve the performance of student models rather than designing compact networks. \n\nIn summary, pruning methods remove the unimportant weights or filters, while the proposed versatile filters generate subfilters from the intrinsic filters. Quantization methods represent the weights or activations in low-bit values, while our method operates at the filter level. Compared with the related works on extracting filters during inference, e.g., FSNet [75] enforces weight sharing across nearby filters and Savarese et al. [55] shares weights across layers, the proposed versatile filters generate subfilters by handdesigned or learnable patterns.",
            "score": 0.7967756481834244,
            "section_title": "Model Compression",
            "char_start_offset": 10165,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 23
                },
                {
                    "start": 24,
                    "end": 241
                },
                {
                    "start": 242,
                    "end": 359
                },
                {
                    "start": 360,
                    "end": 477
                },
                {
                    "start": 478,
                    "end": 556
                },
                {
                    "start": 557,
                    "end": 693
                },
                {
                    "start": 696,
                    "end": 851
                },
                {
                    "start": 852,
                    "end": 975
                },
                {
                    "start": 976,
                    "end": 1255
                }
            ],
            "ref_mentions": [
                {
                    "start": 490,
                    "end": 493,
                    "matchedPaperCorpusId": "91183944"
                },
                {
                    "start": 1131,
                    "end": 1135,
                    "matchedPaperCorpusId": "53408116"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.96240234375
        },
        {
            "corpus_id": "252907522",
            "title": "Learning Generalizable Models for Vehicle Routing Problems via Knowledge Distillation",
            "text": "Knowledge distillation [44] is a kind of teacher-student training paradigm that aims at transferring knowledge from a (group of) complex teacher model(s) \u03b8 T , to a succinct student model \u03b8 S . The recent research findings reveal that knowledge distillation is not only able to effectively learn a lighter student network from larger teacher network(s) [45,46], but also has potential to improve the generalization [47,48] even over its teacher(s) [49][50][51]. Typically, the teacher models are pre-trained, and the student model compares the output of teacher model(s) with its own and considers it as a supervisory signal. Formally, the student network is trained with the goal of minimizing a weighted combination of the distillation loss L KD and the original task loss L Task as follows, \n\nwhere \u03b1 \u2208 [0, 1]. The L KD with respect to one or more teachers (N T \u2265 1) is generally formulated as, \n\nwhere x is the training data from X and \u03c6(\u2022) measures the statistical distance between the teacher and the student, such as the Kullback-Leibler divergence KL(p \u03b8 T p \u03b8 S ) = i p \u03b8 T (log p \u03b8 T \u2212 log p \u03b8 S ).",
            "score": 0.7956375318673659,
            "section_title": "Knowledge distillation",
            "char_start_offset": 8395,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 193
                },
                {
                    "start": 194,
                    "end": 461
                },
                {
                    "start": 462,
                    "end": 625
                },
                {
                    "start": 626,
                    "end": 793
                },
                {
                    "start": 796,
                    "end": 813
                },
                {
                    "start": 814,
                    "end": 897
                },
                {
                    "start": 900,
                    "end": 1108
                }
            ],
            "ref_mentions": [
                {
                    "start": 353,
                    "end": 357,
                    "matchedPaperCorpusId": "202719327"
                },
                {
                    "start": 357,
                    "end": 360,
                    "matchedPaperCorpusId": "214802887"
                },
                {
                    "start": 415,
                    "end": 419,
                    "matchedPaperCorpusId": "235390933"
                },
                {
                    "start": 419,
                    "end": 422,
                    "matchedPaperCorpusId": "211096976"
                },
                {
                    "start": 452,
                    "end": 456,
                    "matchedPaperCorpusId": "4110009"
                },
                {
                    "start": 456,
                    "end": 460,
                    "matchedPaperCorpusId": "3619097"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.90283203125
        },
        {
            "corpus_id": "267413258",
            "title": "Improving the Learning of Code Review Successive Tasks with Cross-Task Knowledge Distillation",
            "text": "Knowledge distillation is a popular technique that was first introduced by Hinton et al. [18] as a method to transfer knowledge from a complex model, also known as the teacher, to a smaller and faster model, called the student. The main goal of knowledge distillation is to transfer the learned knowledge of the teacher model to the student model so that it can achieve comparable or even better performance on a target task. \n\nThe process of knowledge distillation consists of training a teacher model on a large dataset to accomplish a specific task. Then, the student model is trained to mimic the behavior of the teacher model by minimizing the distance between their output distributions on the same dataset, typically using the Kullback-Leibler divergence (KL-divergence) as the distance metric [20,21]. \n\nFor instance, this concept is employed in model compression where the knowledge is transferred from a large and complex neural network (i.e., the teacher) to a smaller and simpler neural network (i.e., the student). The goal of knowledge distillation is to make the student network learn the same function as the teacher network but with fewer parameters. \n\nCross-task knowledge distillation is a recent extension of knowledge distillation that enables the transfer of knowledge from one task to another [22,43,45,47]. Instead of transferring knowledge from a complex model to a smaller one for the same task, cross-task knowledge distillation transfers knowledge from a model trained on one task, referred to as the source task, to a model trained on a different task, referred to as the target task. This allows the target model to improve its performance, even if the two tasks are not directly related.",
            "score": 0.7882886571772505,
            "section_title": "Knowledge distillation",
            "char_start_offset": 12698,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 227
                },
                {
                    "start": 228,
                    "end": 425
                },
                {
                    "start": 428,
                    "end": 552
                },
                {
                    "start": 553,
                    "end": 809
                },
                {
                    "start": 812,
                    "end": 1027
                },
                {
                    "start": 1028,
                    "end": 1167
                },
                {
                    "start": 1170,
                    "end": 1330
                },
                {
                    "start": 1331,
                    "end": 1613
                },
                {
                    "start": 1614,
                    "end": 1718
                }
            ],
            "ref_mentions": [
                {
                    "start": 801,
                    "end": 805,
                    "matchedPaperCorpusId": "37718089"
                },
                {
                    "start": 1320,
                    "end": 1323,
                    "matchedPaperCorpusId": "247011924"
                },
                {
                    "start": 1323,
                    "end": 1326,
                    "matchedPaperCorpusId": "219965004"
                },
                {
                    "start": 1326,
                    "end": 1329,
                    "matchedPaperCorpusId": "209078813"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.994140625
        },
        {
            "corpus_id": "219559263",
            "title": "Knowledge Distillation: A Survey",
            "text": "To improve the efficacy of knowledge transfer, the relationships between the model complexity and existing distillation schemes or other novel distillation schemes (Sun et al. 2021) should be further investigated. \n\nCurrently, most KD methods focus on new types of knowledge or distillation loss functions, leaving the design of the teacher-student architectures poorly investigated (Nowak and Corso 2018;Crowley et al. 2018;Kang et al. 2020;Liu et al. 2019i;Ashok et al. 2018;Liu et al. 2019a). In fact, apart from the knowledge and distillation algorithms, the relationship between the structures of the teacher and the student also significantly influences the performance of knowledge distillation. For example, on one hand, some recent works find that the student model can learn little from some teacher models due to the model capacity gap between the teacher model and the student model (Zhang et al. 2019b;Kang et al. 2020); On the other hand, from some early theoretical analysis on the capacity of neural networks, shallow networks are capable of learning the same representation as deep neural networks (Ba and Caruana 2014). Therefore, the design of an effective student model or construction of a proper teacher model are still challenging problems in knowledge distillation. \n\nDespite a huge number of the knowledge distillation methods and applications, the understanding of knowledge distillation including theoretical explanations and empiri-cal evaluations remains insufficient (Lopez-Paz et al. 2016;Phuong and Lampert 2019a;Cho and Hariharan 2019). For example, distillation can be viewed as a form of learning with privileged information (Lopez-Paz et al. 2016). The assumption of linear teacher and student models enables the study of the theoretical explanations of characteristics of the student learning via distillation (Phuong and Lampert 2019a). Furthermore, some empirical evaluations and analysis on the efficacy of knowledge distillation were performed by Cho and Hariharan (2019). However, a deep understanding of generalizability of knowledge distillation, especially how to measure the quality of knowledge or the quality of the teacher-student architecture, is still very difficult to attain.",
            "score": 0.7825864187250223,
            "section_title": "Challenges",
            "char_start_offset": 82695,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 213
                },
                {
                    "start": 216,
                    "end": 495
                },
                {
                    "start": 496,
                    "end": 702
                },
                {
                    "start": 703,
                    "end": 1137
                },
                {
                    "start": 1138,
                    "end": 1289
                },
                {
                    "start": 1292,
                    "end": 1569
                },
                {
                    "start": 1570,
                    "end": 1684
                },
                {
                    "start": 1685,
                    "end": 1874
                },
                {
                    "start": 1875,
                    "end": 2013
                },
                {
                    "start": 2014,
                    "end": 2228
                }
            ],
            "ref_mentions": [
                {
                    "start": 405,
                    "end": 425,
                    "matchedPaperCorpusId": "23316647"
                },
                {
                    "start": 425,
                    "end": 442,
                    "matchedPaperCorpusId": "208513309"
                },
                {
                    "start": 442,
                    "end": 459,
                    "matchedPaperCorpusId": "208175624"
                },
                {
                    "start": 459,
                    "end": 477,
                    "matchedPaperCorpusId": "13352766"
                },
                {
                    "start": 477,
                    "end": 493,
                    "matchedPaperCorpusId": "69629714"
                },
                {
                    "start": 895,
                    "end": 915,
                    "matchedPaperCorpusId": "159041406"
                },
                {
                    "start": 915,
                    "end": 932,
                    "matchedPaperCorpusId": "208513309"
                },
                {
                    "start": 1497,
                    "end": 1520,
                    "matchedPaperCorpusId": "8125776"
                },
                {
                    "start": 1520,
                    "end": 1545,
                    "matchedPaperCorpusId": "174800711"
                },
                {
                    "start": 1545,
                    "end": 1568,
                    "matchedPaperCorpusId": "203642130"
                },
                {
                    "start": 1660,
                    "end": 1683,
                    "matchedPaperCorpusId": "8125776"
                },
                {
                    "start": 1847,
                    "end": 1873,
                    "matchedPaperCorpusId": "174800711"
                },
                {
                    "start": 1988,
                    "end": 2012,
                    "matchedPaperCorpusId": "203642130"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.80810546875
        },
        {
            "corpus_id": "253397743",
            "title": "Bridging Fairness and Environmental Sustainability in Natural Language Processing",
            "text": "The underlying idea of knowledge distillation (KD; Bucilu\u01ce et al., 2006;Hinton et al., 2015) is to transfer knowledge from a (typically big, pre-trained, and highly regularized) teacher model to a (typically much smaller and untrained) student network. It has been shown that a student network which can learn from the teacher's knowledge is likely to perform better than a small model trained without a teacher's guidance. The knowledge transfer happens through effective supervision from the teacher, e.g., via comparing output probabilities (e.g., Hinton et al., 2015), comparing the intermediate features (e.g., Ji et al., 2021), and initializing the student's layers from the teacher's layers.",
            "score": 0.7787500503667737,
            "section_title": "Knowledge Distillation",
            "char_start_offset": 11042,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 252
                },
                {
                    "start": 253,
                    "end": 423
                },
                {
                    "start": 424,
                    "end": 698
                }
            ],
            "ref_mentions": [
                {
                    "start": 51,
                    "end": 72,
                    "matchedPaperCorpusId": "11253972"
                },
                {
                    "start": 616,
                    "end": 632,
                    "matchedPaperCorpusId": "231839582"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.89697265625
        },
        {
            "corpus_id": "255942245",
            "title": "Dataset Distillation: A Comprehensive Review",
            "text": "Knowledge distillation (KD) [37], [38], [39], [40] aims to transfer knowledge from a large teacher network to a smaller student network, such that the student network can preserve the performance of the teacher with reduced computational overhead. The seminal work by Hinton et al. [37] leads the student to mimic the outputs of the teacher, which can represent knowledge acquired by the teacher network. Afterward, improvements of KD have focused on four aspects: representations of knowledge, teacherstudent architectures, distillation algorithms, and distillation schemes. First, knowledge can be represented by model response/output [37], [41], features [38], [42], [43], and relation [44], [45], [46]. Second, teacher-student architectures refer to the network architectures of teacher and student models, which determines the quality of knowledge acquisition and distillation from teacher to student [40]. Third, distillation algorithms determine the ways of knowledge transfer. A simple and typical way is to match the knowledge captured by the teacher and student models directly [37], [38]. Beyond that, many different algorithms are proposed to handle more complex settings, such as adversarial distillation [47], attention-based distillation [39], and data-free distillation [48], [49]. Finally, distillation schemes control training configurations of teacher and student, and there are offline- [37], [38], online- [50], and self-distillation [51]. As for application, KD is widely used in ensemble learning [52] and model compression [38], [53], [54]. \n\nThe concept of DD is inspired by KD [18]. Specifically, DD aims at a lightweight dataset, while KD aims at a lightweight model. In this view, DD and KD are only conceptually related but technically orthogonal. It is worth noting that, similar to DD, recent data-free KD methods [48], [49], [55] are also concerned with the generation of synthetic training samples since original training datasets are unavailable. Their differences are two-fold.",
            "score": 0.7780427429888754,
            "section_title": "Knowledge Distillation",
            "char_start_offset": 5802,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 247
                },
                {
                    "start": 248,
                    "end": 404
                },
                {
                    "start": 405,
                    "end": 575
                },
                {
                    "start": 576,
                    "end": 706
                },
                {
                    "start": 707,
                    "end": 911
                },
                {
                    "start": 912,
                    "end": 984
                },
                {
                    "start": 985,
                    "end": 1099
                },
                {
                    "start": 1100,
                    "end": 1297
                },
                {
                    "start": 1298,
                    "end": 1460
                },
                {
                    "start": 1461,
                    "end": 1564
                },
                {
                    "start": 1567,
                    "end": 1608
                },
                {
                    "start": 1609,
                    "end": 1694
                },
                {
                    "start": 1695,
                    "end": 1776
                },
                {
                    "start": 1777,
                    "end": 1980
                },
                {
                    "start": 1981,
                    "end": 2012
                }
            ],
            "ref_mentions": [
                {
                    "start": 46,
                    "end": 50,
                    "matchedPaperCorpusId": "219559263"
                },
                {
                    "start": 643,
                    "end": 647,
                    "matchedPaperCorpusId": "212908749"
                },
                {
                    "start": 664,
                    "end": 668,
                    "matchedPaperCorpusId": "221559239"
                },
                {
                    "start": 670,
                    "end": 674,
                    "matchedPaperCorpusId": "227232038"
                },
                {
                    "start": 689,
                    "end": 693,
                    "matchedPaperCorpusId": "26021416"
                },
                {
                    "start": 695,
                    "end": 699,
                    "matchedPaperCorpusId": "131765296"
                },
                {
                    "start": 701,
                    "end": 705,
                    "matchedPaperCorpusId": "198160865"
                },
                {
                    "start": 906,
                    "end": 910,
                    "matchedPaperCorpusId": "219559263"
                },
                {
                    "start": 1218,
                    "end": 1222,
                    "matchedPaperCorpusId": "53976534"
                },
                {
                    "start": 1292,
                    "end": 1296,
                    "matchedPaperCorpusId": "261514306"
                },
                {
                    "start": 1427,
                    "end": 1431,
                    "matchedPaperCorpusId": "24982157"
                },
                {
                    "start": 1455,
                    "end": 1459,
                    "matchedPaperCorpusId": "159041406"
                },
                {
                    "start": 1520,
                    "end": 1524,
                    "matchedPaperCorpusId": "7350432"
                },
                {
                    "start": 1553,
                    "end": 1557,
                    "matchedPaperCorpusId": "11536917"
                },
                {
                    "start": 1851,
                    "end": 1855,
                    "matchedPaperCorpusId": "261514306"
                },
                {
                    "start": 1857,
                    "end": 1861,
                    "matchedPaperCorpusId": "159041346"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.97802734375
        },
        {
            "corpus_id": "268060109",
            "title": "Advanced hybrid LSTM-transformer architecture for real-time multi-task prediction in engineering systems",
            "text": "Knowledge distillation involves training a smaller, more compact model (the student) using the knowledge gained by a larger, more complex model (the teacher). The primary aim is to transfer the essence of the teacher model's knowledge to the student, ensuring that the student achieves comparable performance with reduced computational overhead.",
            "score": 0.7675797732579703,
            "section_title": "Concept of distillation",
            "char_start_offset": 11680,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 158
                },
                {
                    "start": 159,
                    "end": 345
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.85546875
        },
        {
            "corpus_id": "251107546",
            "title": "Self-Supervision and Self-Distillation with Multilayer Feature Contrast for Supervision Collapse in Few-Shot Remote Sensing Scene Classification",
            "text": "Knowledge distillation [50] can transfer the knowledge learned in one network to another. The structures of the two networks can be the same or different. Knowledge distillation usually trains a selected network as a teacher first and then uses the output of the teacher network and labels to train a network as a student, so it is also called \"teacherstudent learning\". Knowledge distillation can reduce the volume of the network and maintain a performance level close to that of the original network. Two networks with different performances can also be combined through knowledge distillation. Han [51] showed that for a given neural network, reducing the network weight by more than 85% through knowledge distillation would not significantly damage the performance of the neural network. According to the object of distillation, knowledge distillation has many forms. KD [50] realized knowledge transfer by minimizing the difference between teachers' and students' classified output labels. Fitness [52] extracted features from the middle layer of the teacher network to guide the student to learn useful knowledge, which can make the student network deeper and narrower than the teacher network. In [53], knowledge was transferred to students in the form of an attention map. IRG [54] constrained the similarity of multiple samples and proposed to constrain the instance relationship graph of students and teachers.",
            "score": 0.7667305225353238,
            "section_title": "Knowledge Distillation",
            "char_start_offset": 14701,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 89
                },
                {
                    "start": 90,
                    "end": 154
                },
                {
                    "start": 155,
                    "end": 370
                },
                {
                    "start": 371,
                    "end": 502
                },
                {
                    "start": 503,
                    "end": 596
                },
                {
                    "start": 597,
                    "end": 791
                },
                {
                    "start": 792,
                    "end": 871
                },
                {
                    "start": 872,
                    "end": 994
                },
                {
                    "start": 995,
                    "end": 1200
                },
                {
                    "start": 1201,
                    "end": 1280
                },
                {
                    "start": 1281,
                    "end": 1420
                }
            ],
            "ref_mentions": [
                {
                    "start": 1204,
                    "end": 1208,
                    "matchedPaperCorpusId": "829159"
                },
                {
                    "start": 1285,
                    "end": 1289,
                    "matchedPaperCorpusId": "198185886"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.96875
        },
        {
            "corpus_id": "259228825",
            "title": "Research on Safety Helmet Detection Algorithm Based on Improved YOLOv5s",
            "text": "Knowledge distillation is a technique utilized to extract the knowledge of a large teacher model and condense it into a small student model. It can be understood as a large teacher neural network teaching his knowledge to a small student network [35][36][37]. \n\nThe process is transferred from the teacher network to the student network. The teacher network is generally bloated; therefore, the teacher network provides knowledge to the student network. The student network is a relatively small network and can thus obtain a lightweight network model. Knowledge distillation adopts the teacher-student mode. In this mode, the teacher is the output party of \"knowledge\", and the student is the receiver of \"knowledge\" [38]. \n\nThe teacher has a strong learning ability and can transfer the learned knowledge to the student model with a lower learning ability, so as to improve the generalization ability of the student model. The complicated and cumbersome but easy-to-use teacher model has no upper limit; it is purely a tutor, and in reality, a simple and flexible student model is deployed. The knowledge distillation process is shown in Figure 9   First, distill a deeper teacher network with a better extraction ability to obtain a logit, and distill it at T temperature. Then, use the classification prediction probability distribution in the Softmax layer to obtain soft targets. At the same temperature T, the logits in the student network are distilled, and then the category prediction probability distribution in Softmax is used to obtain the loss function L soft . Its expression is: \n\nwhere C j is the true label value of the j-th class. Finally, L hard and L soft are weighted and summed to obtain the final loss function L. This loss function can prevent the wrong information from the teacher network from being transmitted to the student network by comparing it with the real label. In this study, the improved YOLOv5s model was used as the teacher network, and the YOLOv5s model with the large target detection layer removed by structural pruning was used as the student model for knowledge distillation to obtain the final model and reduce the amount of calculation and parameters of the improved network model.",
            "score": 0.7645791631337158,
            "section_title": "Knowledge Distillation",
            "char_start_offset": 38607,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 140
                },
                {
                    "start": 141,
                    "end": 259
                },
                {
                    "start": 262,
                    "end": 337
                },
                {
                    "start": 338,
                    "end": 453
                },
                {
                    "start": 454,
                    "end": 552
                },
                {
                    "start": 553,
                    "end": 608
                },
                {
                    "start": 609,
                    "end": 723
                },
                {
                    "start": 726,
                    "end": 924
                },
                {
                    "start": 925,
                    "end": 1092
                },
                {
                    "start": 1093,
                    "end": 1275
                },
                {
                    "start": 1276,
                    "end": 1385
                },
                {
                    "start": 1386,
                    "end": 1575
                },
                {
                    "start": 1576,
                    "end": 1594
                },
                {
                    "start": 1597,
                    "end": 1649
                },
                {
                    "start": 1650,
                    "end": 1898
                },
                {
                    "start": 1899,
                    "end": 2229
                }
            ],
            "ref_mentions": [
                {
                    "start": 246,
                    "end": 250,
                    "matchedPaperCorpusId": "250957345"
                },
                {
                    "start": 250,
                    "end": 254,
                    "matchedPaperCorpusId": "231969372"
                },
                {
                    "start": 254,
                    "end": 258,
                    "matchedPaperCorpusId": "244283694"
                },
                {
                    "start": 718,
                    "end": 722,
                    "matchedPaperCorpusId": "219559263"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.9501953125
        },
        {
            "corpus_id": "240419662",
            "title": "Rethinking the Knowledge Distillation From the Perspective of Model Calibration",
            "text": "Knowledge distillation (Hinton, Vinyals, and Dean 2015) is widely used to compress the deep neural networks, which employs the soft probabilities of a lager teacher network to guide the training of smaller student network (Cho and Hariharan 2019). The applications of knowledge distillation seem to be evident, such as model compression, transfer learning, domain adaption, incremental learning. Many follow-ups work endeavor to improve the performance of knowledge distillation, using different strategies. Despite sustainable efforts have been made, many studies (Mirzadeh et al. 2020) suggest that: more accurate teacher models do not necessarily make for better teachers. The small model does not imitate the large model very well, when the performance gap between the large and small models is large. Researchers attributed the phenomena to the mismatch of abilities between the teacher and student. What characterizes this phenomenon? Are the teacher models over-confident? In our study, we aim to answer these questions.",
            "score": 0.7601084126896938,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 247
                },
                {
                    "start": 248,
                    "end": 395
                },
                {
                    "start": 396,
                    "end": 507
                },
                {
                    "start": 508,
                    "end": 675
                },
                {
                    "start": 676,
                    "end": 805
                },
                {
                    "start": 806,
                    "end": 904
                },
                {
                    "start": 905,
                    "end": 940
                },
                {
                    "start": 941,
                    "end": 979
                },
                {
                    "start": 980,
                    "end": 1027
                }
            ],
            "ref_mentions": [
                {
                    "start": 222,
                    "end": 246,
                    "matchedPaperCorpusId": "203642130"
                },
                {
                    "start": 565,
                    "end": 587,
                    "matchedPaperCorpusId": "212908749"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.79931640625
        },
        {
            "corpus_id": "262084420",
            "title": "A Machine Learning-Oriented Survey on Tiny Machine Learning",
            "text": "This technique transfers knowledge from a large, complex model (teacher) to a smaller, simpler model (student) [84]. This process is important for various reasons, such as reducing computational demands or enhancing model performance on specific tasks. Knowledge types, distillation strategies, and teacher-student architectures are vital factors in student learning during knowledge distillation. The subsequent paragraphs introduce the key categories of knowledge types and distillation strategies. \n\nThe extraction of knowledge from teachers and its utilization for training student networks can be classified into three categories: response-based, feature-based, and relationbased. Specifically, response-based knowledge distillation involves mimicking the final predictions of the teacher model by capturing the neural response in the last output layer [87]. Feature-based knowledge expands upon this approach by using both the outputs of the last layer and intermediate layers to train thinner networks [81]. Finally, relation-based knowledge takes a step further by exploring the relationships between different layers or data samples in addition to the outputs of specific layers in the teacher model [86]. \n\nThe distillation schemes are also crucial for the student learning process. Depending on the training strategy, the following three different categories are presented: offline distillation, online distillation, self-distillation. Offline distillation is a two-stage strategy, where the teacher model is first trained on a set of training samples, and then the trained teacher model is used to guide the student model by extracting intermediate features or logits [80]. On the other hand, online distillation is an end-to-end approach where both the teacher and student models are updated simultaneously, making it suitable when the teacher model is not significantly larger or higher performing [85]. Finally, self-distillation is a special case of online distillation where the teacher and student networks have the same architecture [79]. \n\nIn general, knowledge distillation is used to achieve a good trade-off between small model size and an acceptable accuracy [88]. For this reason, it is widely adopted in several fields where existing models are well-performing but unable to be deployed ''as they are'' in resource-constrained hardware.",
            "score": 0.7547895383867367,
            "section_title": "3) Knowledge Distillation",
            "char_start_offset": 28218,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 116
                },
                {
                    "start": 117,
                    "end": 252
                },
                {
                    "start": 253,
                    "end": 397
                },
                {
                    "start": 398,
                    "end": 500
                },
                {
                    "start": 503,
                    "end": 685
                },
                {
                    "start": 686,
                    "end": 863
                },
                {
                    "start": 864,
                    "end": 1014
                },
                {
                    "start": 1015,
                    "end": 1214
                },
                {
                    "start": 1217,
                    "end": 1292
                },
                {
                    "start": 1293,
                    "end": 1446
                },
                {
                    "start": 1447,
                    "end": 1685
                },
                {
                    "start": 1686,
                    "end": 1917
                },
                {
                    "start": 1918,
                    "end": 2057
                },
                {
                    "start": 2060,
                    "end": 2188
                },
                {
                    "start": 2189,
                    "end": 2362
                }
            ],
            "ref_mentions": [
                {
                    "start": 111,
                    "end": 115,
                    "matchedPaperCorpusId": "219559263"
                },
                {
                    "start": 858,
                    "end": 862,
                    "matchedPaperCorpusId": "232104927"
                },
                {
                    "start": 1009,
                    "end": 1013,
                    "matchedPaperCorpusId": "235613518"
                },
                {
                    "start": 1209,
                    "end": 1213,
                    "matchedPaperCorpusId": "239486869"
                },
                {
                    "start": 1680,
                    "end": 1684,
                    "matchedPaperCorpusId": "198179767"
                },
                {
                    "start": 1912,
                    "end": 1916,
                    "matchedPaperCorpusId": "224914013"
                },
                {
                    "start": 2052,
                    "end": 2056,
                    "matchedPaperCorpusId": "214727822"
                },
                {
                    "start": 2183,
                    "end": 2187,
                    "matchedPaperCorpusId": "255266316"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.97607421875
        },
        {
            "corpus_id": "221703021",
            "title": "Noisy Self-Knowledge Distillation for Text Summarization",
            "text": "Knowledge Distillation refers to a class of methods for training a new smaller student network by learning from a teacher network (in addition to learning from the training data). It is generally assumed that the teacher has been previously trained, and the parameters for the student are estimated by matching the student's predictions to the teacher.\n\nLet T and S denote teacher and student models, respectively. Let f T and f S be functions of the teacher and student. The models are typically neural networks and function f can be in principle defined using the output of any network layer (e.g., a hidden or softmax layer). Knowledge distillation methods are commonly expressed as minimizing 694 an objective function over training set X :\n\nwhere l() is a loss function that penalizes the difference between the teacher and the student. Specific instantiations of this general framework include minimizing the teacher/student difference based on output logits, intermediate hidden representations, attention maps, and derivatives of the loss to the input (Ba and Caruana, 2014;Romero et al., 2014;Zagoruyko and Komodakis, 2017;. Other work integrates an ensemble of teachers in order to improve the student (Urban et al., 2016), trains a succession of students (Furlanello et al., 2018), introduces a \"teacher assistant\" for better knowledge transfer (Mirzadeh et al., 2019), and regularizes multi-task agents (Parisotto et al., 2015;Teh et al., 2017) in reinforcement learning. Compared to direct training, knowledge distillation provides a more stable training process which leads to better performing student models (Hinton et al., 2015;Phuong and Lampert, 2019). Recent work (Furlanello et al., 2018;Hahn and Choi, 2019) also sheds light on leveraging knowledge distillation for training a highperforming student model with the same size as the teacher (see the discussion in the next section).\n\nKnowledge distillation has been also shown to improve results for various NLP tasks.  use it to transfer knowledge from BERT to smaller models, helping them approach or exceed the quality of much larger pretrained neural networks. Aside from distilling large models into smaller ones (Kim and Rush, 2016;",
            "score": 0.748707938567696,
            "section_title": "Knowledge Distillation",
            "char_start_offset": 6953,
            "sentence_offsets": [],
            "ref_mentions": [
                {
                    "start": 1060,
                    "end": 1082,
                    "matchedPaperCorpusId": "11536917"
                },
                {
                    "start": 1102,
                    "end": 1132,
                    "matchedPaperCorpusId": "829159"
                },
                {
                    "start": 1266,
                    "end": 1291,
                    "matchedPaperCorpusId": "4110009"
                },
                {
                    "start": 1439,
                    "end": 1456,
                    "matchedPaperCorpusId": "31009408"
                },
                {
                    "start": 1645,
                    "end": 1670,
                    "matchedPaperCorpusId": "174800711"
                },
                {
                    "start": 1684,
                    "end": 1709,
                    "matchedPaperCorpusId": "4110009"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.98046875
        },
        {
            "corpus_id": "260494397",
            "title": "NCL++: Nested Collaborative Learning for Long-Tailed Visual Recognition",
            "text": "Knowledge distillation is a prevalent technology in knowledge transferring. One typical manner of knowledge distillation is teacher-student learning [45,46], which transfers knowledge from a large teacher model to a small student model. Current methods in knowledge distillation can be divided into three categories: offline distillation [45,47,48,49,50], online distillation [26,24,51,52,51,53] and self-distillation [54,55,56,57,58]. Early methods [45,48] often adopt an offline learning strategy, which transfers the knowledge from a pretrained teacher model to a student model. Most works [45,48,59] distill the knowledge from the output distributions, while some works achieve the knowledge transferring by matching feature representations [47] or attention maps [50]. The offline distillation is very popular in the early stage. However, the offline way only considers transferring the knowledge from the teacher to the student, and therefore, the teacher normally should be a more complex high-capacity model than the student. In recent years, knowledge distillation has been extended to an online way [26,24,51,52,51,60], where the whole knowledge distillation is conducted in an one-phase and end-to-end training scheme. For example, in Deep Mutual Learning [26], any one model can be a student and can distill knowledge from all other models. Zhu et al. [25] propose a multi-branch architecture with treating each branch as a student to further reduce computational cost. Compared with offline distillation, online distillation is more efficient by taking the learning in an one-phase end-to-end scheme. For self-distillation [54,55,56,57,58], it can be regarded as a special case in online distillation, where the teacher and the student refer to the same network. In other words, self-distillation means that the model distills the knowledge from itself. For example, Zhang et al. [55] divide the network into several sections according to their depth, and allow the low sections to distill the knowledge from high sections.",
            "score": 0.7438604834192852,
            "section_title": "Knowledge distillation.",
            "char_start_offset": 12792,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 75
                },
                {
                    "start": 76,
                    "end": 236
                },
                {
                    "start": 237,
                    "end": 435
                },
                {
                    "start": 436,
                    "end": 581
                },
                {
                    "start": 582,
                    "end": 773
                },
                {
                    "start": 774,
                    "end": 834
                },
                {
                    "start": 835,
                    "end": 1033
                },
                {
                    "start": 1034,
                    "end": 1229
                },
                {
                    "start": 1230,
                    "end": 1352
                },
                {
                    "start": 1353,
                    "end": 1481
                },
                {
                    "start": 1482,
                    "end": 1613
                },
                {
                    "start": 1614,
                    "end": 1775
                },
                {
                    "start": 1776,
                    "end": 1866
                },
                {
                    "start": 1867,
                    "end": 2036
                }
            ],
            "ref_mentions": [
                {
                    "start": 418,
                    "end": 422,
                    "matchedPaperCorpusId": "219558831"
                },
                {
                    "start": 1636,
                    "end": 1640,
                    "matchedPaperCorpusId": "219558831"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.95849609375
        },
        {
            "corpus_id": "231925118",
            "title": "Learning Student-Friendly Teacher Networks for Knowledge Distillation",
            "text": "Several recent knowledge distillation methods focus on the strategy of knowledge distillation. Born again network (BAN) [27] presents the effectiveness of sequential knowledge distillation via the networks with an identical architecture. A curriculum learning method [28] employs the optimization trajectory of a teacher model to train students. Collaborative learning approaches [4,5,6] attempt to learn multiple models with distillation jointly, but their concept is not well-suited for asymmetric teacher-student relationship, which may lead to suboptimal convergence of student models. \n\nThe model capacity gap between a teacher and a student is addressed in [2,29,3]. TAKD [3] employs an extra network to reduce model capacity gap between teacher and student models, where a teacher transfers knowledge to a student via a teaching assistant network with an intermediate size. \n\nAn early stopping technique for training teacher networks is proposed to obtain better transferable representations and a neural architecture search is employed to identify a student model with the optimal size [2]. Our work proposes a novel student-friendly learning technique of a teacher network to facilitate knowledge distillation.",
            "score": 0.7434922369939614,
            "section_title": "How to distill",
            "char_start_offset": 5931,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 94
                },
                {
                    "start": 95,
                    "end": 237
                },
                {
                    "start": 238,
                    "end": 345
                },
                {
                    "start": 346,
                    "end": 589
                },
                {
                    "start": 592,
                    "end": 672
                },
                {
                    "start": 673,
                    "end": 880
                },
                {
                    "start": 883,
                    "end": 1098
                },
                {
                    "start": 1099,
                    "end": 1219
                }
            ],
            "ref_mentions": [
                {
                    "start": 120,
                    "end": 124,
                    "matchedPaperCorpusId": "4110009"
                },
                {
                    "start": 267,
                    "end": 271,
                    "matchedPaperCorpusId": "125985701"
                },
                {
                    "start": 380,
                    "end": 383,
                    "matchedPaperCorpusId": "26071966"
                },
                {
                    "start": 383,
                    "end": 385,
                    "matchedPaperCorpusId": "219965421"
                },
                {
                    "start": 663,
                    "end": 666,
                    "matchedPaperCorpusId": "208513309"
                },
                {
                    "start": 666,
                    "end": 669,
                    "matchedPaperCorpusId": "203642130"
                },
                {
                    "start": 669,
                    "end": 671,
                    "matchedPaperCorpusId": "60440652"
                },
                {
                    "start": 678,
                    "end": 681,
                    "matchedPaperCorpusId": "60440652"
                },
                {
                    "start": 1094,
                    "end": 1097,
                    "matchedPaperCorpusId": "208513309"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.765625
        },
        {
            "corpus_id": "231925118",
            "title": "Learning Student-Friendly Teacher Networks for Knowledge Distillation",
            "text": "Since Hinton et al. [1] introduce the basic concept of knowledge distillation, where the dark knowledge in teacher models is given by the temperature-scaled representations of the softmax function, various kinds of information have been employed as the sources of knowledge for distillation from teachers to students. FitNets [18] distills intermediate features of a teacher network, where the student network transforms the intermediate features using guided layers and then calculates the difference between the guided layers and the intermediate features of teacher network. The position of distillation is shifted to the layers before the ReLU operations in [19], which also proposes the novel activation function and the partial L 2 loss function for effective knowledge transfer. Zagoruyko and Komodakis [20] argue importance of attention and propose an attention transfer (AT) method from teachers to students while Kim et al. [21] compute the factor information of the teacher representations using an autoencoder, which is decoded by students for knowledge transfer. Relational knowledge distillation (RKD) [22] introduces a technique to transfer relational information such as distances and angles of features. \n\nCRD [23] maximizes mutual information between a teacher and a student via contrastive learning. \n\nThere exist a couple of methods to perform knowledge distillation without teacher models. For example, ONE [24] distills knowledge from an ensemble of multiple students while BYOT [25] transfers knowledge from deeper layers to shallower ones. Besides, SSKD [26] distills self-supervised features of teachers to students for transferring richer knowledge.",
            "score": 0.7396818686928184,
            "section_title": "What to distill",
            "char_start_offset": 4237,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 317
                },
                {
                    "start": 318,
                    "end": 577
                },
                {
                    "start": 578,
                    "end": 785
                },
                {
                    "start": 786,
                    "end": 1075
                },
                {
                    "start": 1076,
                    "end": 1220
                },
                {
                    "start": 1223,
                    "end": 1318
                },
                {
                    "start": 1321,
                    "end": 1410
                },
                {
                    "start": 1411,
                    "end": 1563
                },
                {
                    "start": 1564,
                    "end": 1675
                }
            ],
            "ref_mentions": [
                {
                    "start": 20,
                    "end": 23,
                    "matchedPaperCorpusId": "7200347"
                },
                {
                    "start": 662,
                    "end": 666,
                    "matchedPaperCorpusId": "102483181"
                },
                {
                    "start": 1116,
                    "end": 1120,
                    "matchedPaperCorpusId": "131765296"
                },
                {
                    "start": 1227,
                    "end": 1231,
                    "matchedPaperCorpusId": "204838340"
                },
                {
                    "start": 1428,
                    "end": 1432,
                    "matchedPaperCorpusId": "48352434"
                },
                {
                    "start": 1501,
                    "end": 1505,
                    "matchedPaperCorpusId": "159041406"
                },
                {
                    "start": 1578,
                    "end": 1582,
                    "matchedPaperCorpusId": "219636179"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.9365234375
        },
        {
            "corpus_id": "273963558",
            "title": "Enhancing Predictive Maintenance in Mining Mobile Machinery through a TinyML-enabled Hierarchical Inference Network",
            "text": "Knowledge distillation transfers knowledge from a large, complex model (the teacher) to a smaller, simpler model (the student) [33]. The process takes into account two main factors: the type of knowledge and the distillation scheme. Knowledge types include response-based, where the student mimics the teacher's final predictions; feature-based, where the student learns from the teacher's intermediate layers; and relation-based, where the student learns from the relationships between layers or samples [21]. Distillation schemes include offline distillation, where the teacher is trained first and then guides the student; online distillation, where both models train simultaneously; and self-distillation, where the teacher and student share the same architecture [21], [34]. This method creates lightweight models that can maintain or even surpass the performance of larger models. Figure 1.c illustrates the knowledge distillation process, showing the transfer of knowledge from a teacher model to a student model.",
            "score": 0.7358238348647546,
            "section_title": "3) Knowledge Distillation",
            "char_start_offset": 11001,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 132
                },
                {
                    "start": 133,
                    "end": 232
                },
                {
                    "start": 233,
                    "end": 510
                },
                {
                    "start": 511,
                    "end": 779
                },
                {
                    "start": 780,
                    "end": 886
                },
                {
                    "start": 887,
                    "end": 1020
                }
            ],
            "ref_mentions": [
                {
                    "start": 127,
                    "end": 131,
                    "matchedPaperCorpusId": "264109973"
                },
                {
                    "start": 505,
                    "end": 509,
                    "matchedPaperCorpusId": "248507674"
                },
                {
                    "start": 768,
                    "end": 772,
                    "matchedPaperCorpusId": "248507674"
                },
                {
                    "start": 774,
                    "end": 778,
                    "matchedPaperCorpusId": "219559263"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.9677734375
        },
        {
            "corpus_id": "268820185",
            "title": "A Comprehensive Review of Knowledge Distillation in Computer Vision",
            "text": "A powerful machine learning approach called knowledge distillation enables the transfer of knowledge from a large and complex model to a more compact and computationally efficient model.The idea of knowledge distillation was first introduced by Bucila et al. [12] in 2006, and since then it has gained significant attention from researchers and practitioners in the field of machine learning.\n\nTraining a smaller model, known as a student model, to do the same tasks as the bigger model, known as a teacher model, is the fundamental aim of knowledge distillation.To achieve cutting-edge performance, the teacher model is often trained on a specific task and dataset.On the other hand, the student model is smaller and more computationally effective, making it suitable for deployment on devices with constrained resources.Transferring knowledge from the teacher model to the student model allows us to improve the performance of the student model without appreciably raising its complexity.The student model is trained using the same task and dataset as the teacher model throughout the knowledge distillation process (figure 2).The student model is trained according to the teacher model's predictions, which provide the student model additional knowledge and help it perform better.Knowledge transfer techniques include soft target training, attention transfer, and feature mimicry [52].\n\nFig 2 : The generic teacher-student framework for knowledge distillation [50] In the distillation loss, the lightweight student model duplicates the output produced by the teacher model by using the loss function.The weighted total contains the cross-entropy loss between the student's output and the real labels, as well as the cross-entropy loss between the student's output and the teacher's output (both after temperature scaling).\n\nwhere   is the student model output,  \u210e is the teacher model output, t is the true labels,  is a hyperparameter that regulates the weight given to each term, and T is the temperature parameter used for temperature scaling.The use of temperature scaling to soften soft logits produced by the softmax function is another important feature of knowledge distillation.This is accomplished by dividing the logits by a temperature parameter T, followed by the softmax function.The temperature parameter governs the probability distribution's \"softness,\" with higher temperatures resulting in softer distributions.",
            "score": 0.7348385975264169,
            "section_title": "INTRODUCTION TO KNOWLEDGE DISTILLATION",
            "char_start_offset": 12659,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 186
                },
                {
                    "start": 186,
                    "end": 392
                },
                {
                    "start": 394,
                    "end": 563
                },
                {
                    "start": 563,
                    "end": 666
                },
                {
                    "start": 666,
                    "end": 822
                },
                {
                    "start": 822,
                    "end": 990
                },
                {
                    "start": 990,
                    "end": 1129
                },
                {
                    "start": 1129,
                    "end": 1284
                },
                {
                    "start": 1284,
                    "end": 1389
                },
                {
                    "start": 1391,
                    "end": 1604
                },
                {
                    "start": 1604,
                    "end": 1826
                },
                {
                    "start": 1828,
                    "end": 2050
                },
                {
                    "start": 2050,
                    "end": 2191
                },
                {
                    "start": 2191,
                    "end": 2298
                },
                {
                    "start": 2298,
                    "end": 2434
                }
            ],
            "ref_mentions": [
                {
                    "start": 1384,
                    "end": 1388,
                    "matchedPaperCorpusId": "219559263"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.9541015625
        },
        {
            "corpus_id": "260599456",
            "title": "Prototypes Sampling Mechanism for Class Incremental Learning",
            "text": "Knowledge Distillation is a widely used technique in model compression [36], [37] and transfer learning [38], which typically involves transferring knowledge from a large and complex model (often referred to as the teacher model) to a smaller and simpler model (often referred to as the student model). The goal of knowledge distillation is to improve the performance of the student model by leveraging the knowledge contained in the teacher model, while still maintaining the efficiency and simplicity of the student model. It has also been widely applied in the field of incremental learning. A large number of works have used knowledge distillation to transfer knowledge from old models to new models, thus reducing catastrophic forgetting of old classes.",
            "score": 0.7347564859304607,
            "section_title": "B. KNOWLEDGE DISTILLATION",
            "char_start_offset": 10129,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 302
                },
                {
                    "start": 303,
                    "end": 524
                },
                {
                    "start": 525,
                    "end": 594
                },
                {
                    "start": 595,
                    "end": 758
                }
            ],
            "ref_mentions": [
                {
                    "start": 71,
                    "end": 75,
                    "matchedPaperCorpusId": "32588614"
                },
                {
                    "start": 77,
                    "end": 81,
                    "matchedPaperCorpusId": "11253972"
                },
                {
                    "start": 104,
                    "end": 108,
                    "matchedPaperCorpusId": "206596723"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.974609375
        },
        {
            "corpus_id": "275920740",
            "title": "On Accelerating Edge AI: Optimizing Resource-Constrained Environments",
            "text": "Distillation is a model compression technique that transfers knowledge from a large, complex model (teacher) to a smaller, efficient model (student) with minimal loss in performance, reducing resource costs and enabling deployment in resource-constrained environments Hinton et al. [2015]. Fig. 4 referees to distillation components and approaches. \n\nThe two essential steps in knowledge distillation (KD) are knowledge elicitation and student distillation Xu et al. [2024b]. The initial step involves extracting knowledge from the teacher model. For instance, the teacher can be given input data to produce the corresponding output logits Hinton et al. [2015]. White-box methods, like feature extraction, provide direct access to the intermediate activations of the teacher model. In contrast, black-box techniques only utilize the output logits from both the teacher and student models Xu et al. [2024b]. Furthermore, in the context of foundation models, other knowledge elicitation strategies can be employed; the teacher might label a dataset for training the student, generate synthetic data through input expansion (creating new inputs based on initial seed data), or use data curation, where teacher feedback is applied to refine the dataset. After eliciting the knowledge, the student model is trained to mimic or approximate the teacher's knowledge. This training typically involves a loss function that minimizes the divergence between student and teacher logits but can also incorporate supervised fine-tuning (which maximizes the likelihood of sequences generated by the teacher) and reinforcement learning, where teacher feedback enhances the student's performance. These strategies aim to narrow the gap between the outputs of the student and teacher, facilitating effective knowledge transfer Xu et al. [2024b]. Authors propose that students deployed on small or expensive-to-label datasets may benefit from this kind of transfer learning, as a large teacher model in one modality may incorporate the knowledge that improves the performance of the student in another modality Gupta et al. [2015].",
            "score": 0.7328023863807692,
            "section_title": "Knowledge Distillation",
            "char_start_offset": 13268,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 289
                },
                {
                    "start": 290,
                    "end": 348
                },
                {
                    "start": 351,
                    "end": 475
                },
                {
                    "start": 476,
                    "end": 546
                },
                {
                    "start": 547,
                    "end": 661
                },
                {
                    "start": 662,
                    "end": 781
                },
                {
                    "start": 782,
                    "end": 906
                },
                {
                    "start": 907,
                    "end": 1249
                },
                {
                    "start": 1250,
                    "end": 1358
                },
                {
                    "start": 1359,
                    "end": 1678
                },
                {
                    "start": 1679,
                    "end": 1826
                },
                {
                    "start": 1827,
                    "end": 2111
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.96435546875
        },
        {
            "corpus_id": "272564024",
            "title": "A Comprehensive Review of Hardware Acceleration Techniques and Convolutional Neural Networks for EEG Signals",
            "text": "Knowledge distillation is a technique that transfers knowledge from large, complex CNNs (teachers) to smaller, more efficient CNNs (students). The student network is trained to mimic the behavior of the teacher, resulting in a compact model with comparable performance. This concept can be traced back to the pioneering work in [101] and has since been extended to the context of deep learning [102]. The core challenge of knowledge distillation revolves around the techniques used to transfer knowledge from the teacher model to the student model, which involves three fundamental components-knowledge, distillation algorithms-and the architecture defining the relationship between the teacher and student models. In this context, knowledge manifests in various forms, including logits, activations, or features extracted from intermediate layers of the teacher model. The distillation algorithms can be categorized as offline, online, or self-distillation. \n\nOffline distillation [101,[103][104][105] extracts knowledge from a pre-trained teacher model and uses the soft label output of the teacher model to train the student network. The authors of [103] used data augmentation to exploit the output distributions of multiple teacher networks. The authors of [101] introduced a tailored distillation approach for quantized models, demonstrating that quantized student networks can closely match the accuracy of full-precision teacher networks while achieving high compression rates and inference acceleration. In contrast, [104] pioneered a data-free technique, training the student network using synthetic data responses from the complex teacher network. Online distillation [106][107][108] occurs during the simultaneous training of both the teacher and student models. It employs online knowledge distillation using the soft-label outputs of the teacher network. Ref. [106] proposed training the student model at different checkpoints of the teacher model until convergence is achieved. Meanwhile, Collaborative Learning Knowledge Distillation (KDCL) [107] dynamically generates high-quality soft targets through an ensemble approach for one-stage online training. Furthermore, as observed in [105,108], knowledge distillation extends its influence to the intermediate layers of the teacher network.",
            "score": 0.7317625041331279,
            "section_title": "Knowledge Distillation",
            "char_start_offset": 46568,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 142
                },
                {
                    "start": 143,
                    "end": 269
                },
                {
                    "start": 270,
                    "end": 400
                },
                {
                    "start": 401,
                    "end": 714
                },
                {
                    "start": 715,
                    "end": 869
                },
                {
                    "start": 870,
                    "end": 958
                },
                {
                    "start": 961,
                    "end": 1136
                },
                {
                    "start": 1137,
                    "end": 1246
                },
                {
                    "start": 1247,
                    "end": 1512
                },
                {
                    "start": 1513,
                    "end": 1658
                },
                {
                    "start": 1659,
                    "end": 1774
                },
                {
                    "start": 1775,
                    "end": 1868
                },
                {
                    "start": 1869,
                    "end": 1992
                },
                {
                    "start": 1993,
                    "end": 2170
                },
                {
                    "start": 2171,
                    "end": 2305
                }
            ],
            "ref_mentions": [
                {
                    "start": 394,
                    "end": 399,
                    "matchedPaperCorpusId": "7200347"
                },
                {
                    "start": 987,
                    "end": 992,
                    "matchedPaperCorpusId": "30258763"
                },
                {
                    "start": 992,
                    "end": 997,
                    "matchedPaperCorpusId": "159041346"
                },
                {
                    "start": 997,
                    "end": 1002,
                    "matchedPaperCorpusId": "182183296"
                },
                {
                    "start": 1152,
                    "end": 1157,
                    "matchedPaperCorpusId": "30258763"
                },
                {
                    "start": 1526,
                    "end": 1531,
                    "matchedPaperCorpusId": "159041346"
                },
                {
                    "start": 1679,
                    "end": 1684,
                    "matchedPaperCorpusId": "125985701"
                },
                {
                    "start": 1684,
                    "end": 1689,
                    "matchedPaperCorpusId": "219965421"
                },
                {
                    "start": 1689,
                    "end": 1694,
                    "matchedPaperCorpusId": "226841849"
                },
                {
                    "start": 1874,
                    "end": 1879,
                    "matchedPaperCorpusId": "125985701"
                },
                {
                    "start": 2057,
                    "end": 2062,
                    "matchedPaperCorpusId": "219965421"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.98388671875
        },
        {
            "corpus_id": "250264223",
            "title": "Informed Learning by Wide Neural Networks: Convergence, Generalization and Sampling Complexity",
            "text": "Knowledge distillation (Hinton et al., 2014;Furlanello et al., 2018;Phuong & Lampert, 2019;Allen-Zhu & Li, 2020) is an important technique to transfer prior knowledge from a pre-trained neural network (a.k.a. teacher network) to another network (a.k.a. student network), with the same or different architectures. Typically, given an (possibly unlabled) input, knowledge distillation is performed by matching the output of the student network with the output of the teacher network. In addition, labeled samples can also be included to introduce a label-based loss. Thus, by formulating g(X) as the output of the teacher network, knowledge distillation can be viewed as a particular instance of informed machine learning, where the knowledge comes from a teacher network and is usually assumed to be perfect.",
            "score": 0.7284699649743361,
            "section_title": "F.4. Knowledge distillation and transfer",
            "char_start_offset": 71442,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 208
                },
                {
                    "start": 209,
                    "end": 252
                },
                {
                    "start": 253,
                    "end": 312
                },
                {
                    "start": 313,
                    "end": 481
                },
                {
                    "start": 482,
                    "end": 564
                },
                {
                    "start": 565,
                    "end": 807
                }
            ],
            "ref_mentions": [
                {
                    "start": 23,
                    "end": 44,
                    "matchedPaperCorpusId": "7200347"
                },
                {
                    "start": 44,
                    "end": 68,
                    "matchedPaperCorpusId": "4110009"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.9755859375
        },
        {
            "corpus_id": "269921267",
            "title": "Efficiency Optimization of Large-Scale Language Models Based on Deep Learning in Natural Language Processing Tasks",
            "text": "Knowledge distillation, a core strategy in modern machine learning, focuses on solving the problem of balance between model size and computational efficiency.The core idea is to effectively transfer the deep knowledge and experience accumulated in large-scale, complex models (often referred to as \"teacher models\") to \"student models\" with smaller numbers of participants and lower computational requirements.\n\nSince then, this technology has rapidly attracted widespread attention from academia and industry, and has been expanded and deepened in several ways: Early stage (2015-2018) : Initial research has focused on simplifying network structure, reducing model volume and computational requirements, while maintaining the predictive performance of the model.In this period, the basic framework of knowledge distillation and the design of loss function were established.Technical deepening (2019-2021) : Researchers began to explore more refined distillation methods, including multi-teacher distillation, feature-stage distillation, relational distillation, etc.\n\nIn the field of deep learning, model knowledge lies in the configuration of parameters it learns through training, which guides the model on how best to extract features from input data and make predictions.Large networks, thanks to their large number of parameters and complex structure design, can capture deeper feature associations on large-scale data sets, showing superior learning ability and generalization performance.However, this advantage is often difficult to play directly in resource-constrained real-world application scenarios, because they have high requirements for computing resources and storage space.Knowledge distillation technology is born to solve this contradiction, it focuses not only on the final classification or regression results of the model output, but also on the teaching of the teacher model's confidence distribution (i.e., soft label) that each sample belongs to various categories.By designing a specific training mechanism, the student model tries to imitate the soft decision-making process while learning the real label, so that it can also \"inherit\" the decision logic and deep understanding of the data of the teacher model under limited parameters.This process not only involves the traditional cross-entropy loss, but also introduces the distillation loss that reflects the difference in the predicted probability distribution.",
            "score": 0.725340674115057,
            "section_title": "B. Knowledge distillation",
            "char_start_offset": 12089,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 158
                },
                {
                    "start": 158,
                    "end": 410
                },
                {
                    "start": 412,
                    "end": 764
                },
                {
                    "start": 764,
                    "end": 875
                },
                {
                    "start": 875,
                    "end": 1068
                },
                {
                    "start": 1070,
                    "end": 1277
                },
                {
                    "start": 1277,
                    "end": 1497
                },
                {
                    "start": 1497,
                    "end": 1693
                },
                {
                    "start": 1693,
                    "end": 1993
                },
                {
                    "start": 1993,
                    "end": 2266
                },
                {
                    "start": 2266,
                    "end": 2446
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.97314453125
        },
        {
            "corpus_id": "231986112",
            "title": "Exploring Knowledge Distillation of a Deep Neural Network for Multi-Script identification",
            "text": "Earlier papers used ensemble methods for model compression [8,9]. Distillation of knowledge from a teacher network and transferring it to a student network to mimic the teacher network is a basic fundamental concept of knowledge distillation. The first proposed concept of knowledge distillation [7] introduces the concept of compressing the knowledge of a more in-depth or larger model to a single computational efficient neural network. It has introduced the concept of dark knowledge transfer from a deep teacher network to a smaller student network by taking the softmax of the results of the teacher network with a specific temperature value and calculating loss between it and the predicted outputs of the student network. They validated their findings by running on MNIST dataset and, JFT dataset by google and other speech recognition tasks. Since then, knowledge distillation has progressed a lot, and adversarial methods [17,18] also have utilized for modelling knowledge transfer between teacher and student. After this study, extensive research has conducted on knowledge distillation. In the paper [11] has introduced the transfer of a hidden activation output and other has proposed transferring attention information as knowledge [20]. \n\nArticle [19] has briefly described the advantages and efficiency of the knowledge distillation. It has described importance of knowledge transfer from teacher to student model using distilled knowledge. They have compared two student deep neural networks trained with teacher network and without teacher model with same size.They have proposed a method of transferring the distilled knowledge between two layers to shows three important points. The student model is more efficient than the original model and it also outperform the original model which is trained from scratch. The student network understand the flow of solving the problem and it start learning with good initial weights. It can learnt and optimized faster than original or normal deep neural network.This paper proves that, the student model reports better efficiency than a normal network without a teacher model.They have compared various knowledge transfer techniques with a normal network without any teacher model for knowledge transfer.They have learned their model with two main condition. First, the teacher model must pretrained with some different dataset and second condition is the teacher model is shallower or deeper than the student model. Their approach contains two step training.",
            "score": 0.7252455939571296,
            "section_title": "Related Study",
            "char_start_offset": 4015,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 65
                },
                {
                    "start": 66,
                    "end": 242
                },
                {
                    "start": 243,
                    "end": 438
                },
                {
                    "start": 439,
                    "end": 728
                },
                {
                    "start": 729,
                    "end": 849
                },
                {
                    "start": 850,
                    "end": 1019
                },
                {
                    "start": 1020,
                    "end": 1097
                },
                {
                    "start": 1098,
                    "end": 1250
                },
                {
                    "start": 1253,
                    "end": 1348
                },
                {
                    "start": 1349,
                    "end": 1455
                },
                {
                    "start": 1456,
                    "end": 1697
                },
                {
                    "start": 1698,
                    "end": 1830
                },
                {
                    "start": 1831,
                    "end": 1942
                },
                {
                    "start": 1943,
                    "end": 2318
                },
                {
                    "start": 2319,
                    "end": 2476
                },
                {
                    "start": 2477,
                    "end": 2519
                }
            ],
            "ref_mentions": [
                {
                    "start": 59,
                    "end": 62,
                    "matchedPaperCorpusId": "9433631"
                },
                {
                    "start": 62,
                    "end": 64,
                    "matchedPaperCorpusId": "125985701"
                },
                {
                    "start": 931,
                    "end": 935,
                    "matchedPaperCorpusId": "53976534"
                },
                {
                    "start": 1261,
                    "end": 1265,
                    "matchedPaperCorpusId": "206596723"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.95263671875
        },
        {
            "corpus_id": "271356843",
            "title": "A Lightweight Fault Diagnosis Method Based on Knowledge Distillation Under Time-Varying Rotational Speeds",
            "text": "The core concept of knowledge distillation is that the deep knowledge accumulated in the complex pre-trained teacher network model is distilled and injected into the more streamlined student model, which is able to inherit and mimic the predictive performance of the teacher model through knowledge distillation, thus reducing the model complexity while maintaining similar predictive effects. Knowledge distillation aims to optimize the balance between model size and performance through knowledge transfer to achieve effective model compression and knowledge transfer. The principle of knowledge distillation is shown in Figure 2. In this paper, we adopt the knowledge transfer mechanism, which is a framework in which the student network learns by simulating the output of the teacher network. In this framework, the student network is trained under the guidance and supervision of the teacher network, which draws on the a priori knowledge and generalization ability of the pre-trained teacher network, and helps to improve the learning efficiency and effectiveness. \n\nOffline distillation follows the classical ''teacher-student'' paradigm, so that the student network, in the process of imitating the teacher network, not only learns the decision-making behavior of the teacher network on the training samples, but also passes on the deep patterns and abstract knowledge contained in the teacher network, thus realizing the effective transfer and compression of knowledge, and the degree of fit of the student network to the output of the teacher network is quantified by the loss function. The degree of fitting of the teacher network output results by the loss function can be expressed as follows: \n\nwhere L task denotes the learning performance of the student network on the original task, Ls is the output vector of the student network and Lt is the output vector of the teacher network, which L KD is used to quantify the difference between the student network and the teacher network at the output level. In order to integrate the loss of the student network's own learning task with the loss of imitating the teacher network's output, a balance coefficient is introduced, which \u03bb is used to regulate the relative weights of the two in the total loss function.",
            "score": 0.7213240739902158,
            "section_title": "C. KNOWLEDGE DISTILLATION",
            "char_start_offset": 10854,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 393
                },
                {
                    "start": 394,
                    "end": 570
                },
                {
                    "start": 571,
                    "end": 796
                },
                {
                    "start": 797,
                    "end": 1070
                },
                {
                    "start": 1073,
                    "end": 1596
                },
                {
                    "start": 1597,
                    "end": 1706
                },
                {
                    "start": 1709,
                    "end": 2017
                },
                {
                    "start": 2018,
                    "end": 2273
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.98388671875
        },
        {
            "corpus_id": "266693464",
            "title": "Explainability-Driven Leaf Disease Classification Using Adversarial Training and Knowledge Distillation",
            "text": "A practical method for compressing models is knowledge distillation, which facilitates information transfer from a large teacher network to a small student network. Initially introduced by Bucila et al. (Bucilu\u01ce et al., 2006) and later generalized by Hinton et al. (Hinton et al., 2015), this method has gained popularity across multiple machine learning applications. Unlike conventional training, knowledge distillation involves learning the student network to emulate the outputs of the teacher model, typically represented as probability distributions over classes. By minimizing the similarity loss between the student's predictions and the teacher's probabilities, the student network can assimilate the knowledge from the teacher, resulting in superior performance compared to training from",
            "score": 0.7210586230963048,
            "section_title": "Knowledge Distillation",
            "char_start_offset": 10668,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 164
                },
                {
                    "start": 165,
                    "end": 368
                },
                {
                    "start": 369,
                    "end": 569
                },
                {
                    "start": 570,
                    "end": 797
                }
            ],
            "ref_mentions": [
                {
                    "start": 203,
                    "end": 225,
                    "matchedPaperCorpusId": "11253972"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.97314453125
        },
        {
            "corpus_id": "272986634",
            "title": "FLINT: Learning-based Flow Estimation and Temporal Interpolation for Scientific Ensemble Visualization",
            "text": "Student-Teacher Learning. In machine learning, techniques have been developed under the name of teacher-student architecture. One approach is knowledge distillation [42] to transfer the knowledge (parameters) learnt by a larger model (teacher model) and transfer it to a smaller model (student model). A separate strand of research uses the concept of privileged information [43], where the teacher provides the student during training with additional \"privileged\" information for knowledge transfer. Both approaches have been unified [44]. The learning can be offline, where student networks learn the knowledge from pre-trained teacher networks, or online, where student and teacher networks are simultaneously trained, so that the whole knowledge learning process can be end-toend trainable [45]. We have implemented an online studentteacher model and instead of a fully separate teacher network, we employ only one additional block that corresponds to the teacher network, akin to Huang et al. [8].",
            "score": 0.719648288195761,
            "section_title": "Machine learning-based upscaling & super-resolution.",
            "char_start_offset": 12587,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 25
                },
                {
                    "start": 26,
                    "end": 125
                },
                {
                    "start": 126,
                    "end": 301
                },
                {
                    "start": 302,
                    "end": 500
                },
                {
                    "start": 501,
                    "end": 540
                },
                {
                    "start": 541,
                    "end": 799
                },
                {
                    "start": 800,
                    "end": 1002
                }
            ],
            "ref_mentions": [
                {
                    "start": 375,
                    "end": 379,
                    "matchedPaperCorpusId": "12874183"
                },
                {
                    "start": 535,
                    "end": 539,
                    "matchedPaperCorpusId": "8125776"
                },
                {
                    "start": 998,
                    "end": 1001,
                    "matchedPaperCorpusId": "244499996"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.75146484375
        },
        {
            "corpus_id": "258762638",
            "title": "Cross-modality Data Augmentation for End-to-End Sign Language Translation",
            "text": "Knowledge Distillation (KD) (Hinton et al., 2015) is a powerful technique for model compression and transfer learning, wherein a smaller student model is trained to mimic the behavior of a larger, more complex teacher model or an ensemble of models, allowing the student to learn a more gen-",
            "score": 0.7162580773612821,
            "section_title": "Knowledge Distillation",
            "char_start_offset": 21283,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 291
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.748046875
        },
        {
            "corpus_id": "257102399",
            "title": "Knowledge Distillation-based Information Sharing for Online Process Monitoring in Decentralized Manufacturing System",
            "text": "Knowledge distillation is a model compression technique proposed by Hinton et al. in 2015 [9], which could effectively train a smaller student model by learning from a larger teacher model. As shown in Figure 2, a general knowledge distillation framework consists of three major components: teacher-student network architecture, knowledge, as well as distillation algorithm. The main idea of knowledge distillation is that the student model mimics the teacher model in order to achieve a comparable or even a superior performance [10]. Teacher network usually has more complicated network architecture and learns from the data first. After the training of teacher network, the useful knowledge could be distilled from the teacher network. Hereafter, student network learns the knowledge distilled from teacher network to mimic teacher's behavior together with the training data. The core of knowledge distillation is how to simplify model while maintaining relatively good performance. \n\nOne promising and efficient way to distill knowledge is to mimic the logit output of last layer in the teacher model (i.e., teacher's prediction). Soft target, i.e., the probability that the input data   belongs to each class, is proposed as the logit of teacher network. For a -class classification problem, the soft target of teacher network \u0398  could be calculated by the softmax function as: \n\nWhere   denotes the -th training sample,  is the number of classes,    is logit of last layer for the th class and  denotes temperature factor which could control importance of each target. For example, a higher  produces a softer probability distribution among different classes (assign probability with less difference to each class). Soft target contains informative dark knowledge of teacher network and could enhance performance of student network. Empirically, when the student model is very small compared to the teacher model, lower temperatures work better [9]. This is because a very small model might not be able to capture all the information when we raise the temperature. Therefore, soft target could be seen as knowledge extracted from teacher network and it could be transferred to student network by matching output of two networks. To measure the output similarity between teacher network \u0398  and student network \u0398  , the Kullback Leibler (KL) divergence is often employed, which could be formulated in Eq. ( 2),",
            "score": 0.7158637893912181,
            "section_title": "Knowledge distillation",
            "char_start_offset": 15159,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 189
                },
                {
                    "start": 190,
                    "end": 374
                },
                {
                    "start": 375,
                    "end": 535
                },
                {
                    "start": 536,
                    "end": 633
                },
                {
                    "start": 634,
                    "end": 738
                },
                {
                    "start": 739,
                    "end": 878
                },
                {
                    "start": 879,
                    "end": 985
                },
                {
                    "start": 988,
                    "end": 1134
                },
                {
                    "start": 1135,
                    "end": 1259
                },
                {
                    "start": 1260,
                    "end": 1382
                },
                {
                    "start": 1385,
                    "end": 1574
                },
                {
                    "start": 1575,
                    "end": 1721
                },
                {
                    "start": 1722,
                    "end": 1838
                },
                {
                    "start": 1839,
                    "end": 1955
                },
                {
                    "start": 1956,
                    "end": 2070
                },
                {
                    "start": 2071,
                    "end": 2234
                },
                {
                    "start": 2235,
                    "end": 2414
                }
            ],
            "ref_mentions": [
                {
                    "start": 530,
                    "end": 534,
                    "matchedPaperCorpusId": "219559263"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.990234375
        },
        {
            "corpus_id": "232352737",
            "title": "Student Network Learning via Evolutionary Knowledge Distillation",
            "text": "Knowledge distillation provides an effective way to transfer knowledge via teacher-student learning, where most existing distillation approaches apply a fixed pre-trained model as teacher to supervise the learning of student network. This manner usually brings in a big capability gap between teacher and student networks during learning. Recent researches have observed that a small teacher-student capability gap can facilitate knowledge transfer. Inspired by that, we propose an evolutionary knowledge distillation approach to improve the transfer effectiveness of teacher knowledge. Instead of a fixed pre-trained teacher, an evolutionary teacher is learned online and consistently transfers intermediate knowledge to supervise student network learning on-the-fly. To enhance intermediate knowledge representation and mimicking, several simple guided modules are introduced between corresponding teacher-student blocks. In this way, the student can simultaneously obtain rich internal knowledge and capture its growth process, leading to effective student network learning. Extensive experiments clearly demonstrate the effectiveness of our approach as well as good adaptability in the low-resolution and few-sample scenarios.",
            "score": 0.7154128819652757,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.88818359375
        },
        {
            "corpus_id": "270371436",
            "title": "DeviceBERT: Applied Transfer Learning With Targeted Annotations and Vocabulary Enrichment to Identify Medical Device and Component Terminology in FDA Recall Summaries",
            "text": "Transfer learning is a widely adopted technique in machine learning that utilizes pre-trained model weights from models trained on large-scale datasets to fine-tune the model on smaller, downstream tasks (Zhuang et al. [2021]).This approach takes advantage of the knowledge captured by the pre-trained model on the larger dataset and adapts it to the specific requirements of the target task, thereby reducing the need for extensive retraining and improving performance.\n\nKnowledge distillation is a related technique that involves training a smaller student model to approximate the output of a pre-trained teacher model (Hinton et al. [2015]; Tian et al. [2022], Beyer et al. [2022]).The teacher model, typically a larger and more complex model, serves as a reference for the student model, providing guidance on how to map inputs to outputs.Through this process, the student model learns to mimic the behavior of the teacher model, capturing its knowledge and expertise.The key advantage of knowledge distillation lies in its ability to retain the performance of the teacher model while significantly reducing computational requirements, making it an attractive approach for deploying models in resource-constrained environments.",
            "score": 0.712848404664405,
            "section_title": "Related Work",
            "char_start_offset": 3476,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 227
                },
                {
                    "start": 227,
                    "end": 470
                },
                {
                    "start": 472,
                    "end": 686
                },
                {
                    "start": 686,
                    "end": 844
                },
                {
                    "start": 844,
                    "end": 973
                },
                {
                    "start": 973,
                    "end": 1232
                }
            ],
            "ref_mentions": [
                {
                    "start": 204,
                    "end": 225,
                    "matchedPaperCorpusId": "207847753"
                },
                {
                    "start": 645,
                    "end": 663,
                    "matchedPaperCorpusId": "204838340"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.93212890625
        },
        {
            "corpus_id": "219530513",
            "title": "ResKD: Residual-Guided Knowledge Distillation",
            "text": "We categorize knowledge distillation methods in terms of the number of stages. Traditionally, knowledge distillation is a two-stage method, in which a teacher network is trained first, arXiv:2006.04719v4 [cs.CV] 9 Mar 2021 and then a student network is trained under the guidance of the teacher network. Bucil\u0203 et al. [24] pioneered the idea of transferring the knowledge from a cumbersome model to a small model. Hinton et al. [18] popularized this idea by the concept of knowledge distillation (KD), in which a student neural network is trained with the benefit of the soft targets provided by teacher networks. Compared to traditional onehot labels, the output from a teacher network contains more information about the fine-grained distribution of data, which helps the student achieve better performance. Recently, many works have focused on improving the information propagation way or putting strictness to the distillation process via optimization [25], [26], [27], [19], [28], [29], [30], [31], [32], [33], [34], [35], [36], [37] to teach the student better. For example, Peng et al. [19] proposed that a student network should not only focus on mimicking from a teacher at an instance level, but also imitating the embedding space of a teacher so that the student can possess intra-class compactness and inter-class separability. In addition, the effect of different teachers is also researched [38], [39], [40]. For example, Sau et al. [39] proposed an approach to simulate the effect of multiple teachers by injecting noise to the training data and perturbing the logit outputs of a teacher. In such a way, the perturbed outputs not only simulate the setting of multiple teachers but also result in noise in the softmax layer, thus regularizing the distillation loss. With the help of many teachers, the student is improved a lot. Kang et al. [40] used Neural Architecture Search (NAS) to acquire knowledge for both the architecture and the parameters of the student network from different teachers.",
            "score": 0.7128044543337013,
            "section_title": "A. Knowledge Distillation",
            "char_start_offset": 4359,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 78
                },
                {
                    "start": 79,
                    "end": 203
                },
                {
                    "start": 204,
                    "end": 303
                },
                {
                    "start": 304,
                    "end": 413
                },
                {
                    "start": 414,
                    "end": 613
                },
                {
                    "start": 614,
                    "end": 809
                },
                {
                    "start": 810,
                    "end": 1067
                },
                {
                    "start": 1068,
                    "end": 1339
                },
                {
                    "start": 1340,
                    "end": 1422
                },
                {
                    "start": 1423,
                    "end": 1603
                },
                {
                    "start": 1604,
                    "end": 1779
                },
                {
                    "start": 1780,
                    "end": 1842
                },
                {
                    "start": 1843,
                    "end": 2011
                }
            ],
            "ref_mentions": [
                {
                    "start": 318,
                    "end": 322,
                    "matchedPaperCorpusId": "11253972"
                },
                {
                    "start": 956,
                    "end": 960,
                    "matchedPaperCorpusId": "2723173"
                },
                {
                    "start": 962,
                    "end": 966,
                    "matchedPaperCorpusId": "201805763"
                },
                {
                    "start": 974,
                    "end": 978,
                    "matchedPaperCorpusId": "102483463"
                },
                {
                    "start": 980,
                    "end": 984,
                    "matchedPaperCorpusId": "198179476"
                },
                {
                    "start": 986,
                    "end": 990,
                    "matchedPaperCorpusId": "206596723"
                },
                {
                    "start": 992,
                    "end": 996,
                    "matchedPaperCorpusId": "131765296"
                },
                {
                    "start": 998,
                    "end": 1002,
                    "matchedPaperCorpusId": "219962714"
                },
                {
                    "start": 1004,
                    "end": 1008,
                    "matchedPaperCorpusId": "118649278"
                },
                {
                    "start": 1016,
                    "end": 1020,
                    "matchedPaperCorpusId": "52012952"
                },
                {
                    "start": 1028,
                    "end": 1032,
                    "matchedPaperCorpusId": "53213211"
                },
                {
                    "start": 1093,
                    "end": 1097,
                    "matchedPaperCorpusId": "102483463"
                },
                {
                    "start": 1405,
                    "end": 1409,
                    "matchedPaperCorpusId": "11536917"
                },
                {
                    "start": 1417,
                    "end": 1421,
                    "matchedPaperCorpusId": "208513309"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.92333984375
        },
        {
            "corpus_id": "257513218",
            "title": "Model Compression for Deep Neural Networks: A Survey",
            "text": "As shown in Figure 5, knowledge distillation is a teacher-student architecture [106][107][108].The teacher network is a complex pre-trained network, and the student network is a simple small network.The teacher network provides the student network with prior knowledge so that the student network achieves similar performance to that of the teacher network.Deploying deep models in mobile devices is challenging due to the limited processing power and memory of these devices.To address these issues, Bucilu\u0203 et al. [109] first proposed model compression to transfer information from a large model to train a small model without significant accuracy degradation.Henceforth, the training of small models by large models was called knowledge distillation [108,110,111].Chen et al. [112] posited that feature embedding from deep neural networks could convey complementary information and, thus, proposed a novel knowledge-distilling strategy to improve its performance.The main idea of knowledge distillation was that the student model imitated the teacher model to achieve competitive, or even superior, performance.The key focus was how to transfer knowledge from a large teacher model to a small student model.\n\nIn the process of knowledge distillation, knowledge types, distillation strategies, and teacher-student architectures have played key roles in the student learning process.The activations, neurons, and features of the middle layer were available as knowledge to guide the learning of the student model [113][114][115][116][117]. The relationship between different activations, neurons, and features contained the rich knowledge learned by the teacher model [118][119][120][121][122]. As shown in Figure 6, three methods of knowledge distillation were introduced.These three distillation methods are described in detail in the following sections.",
            "score": 0.7108575547340397,
            "section_title": "Knowledge Distillation",
            "char_start_offset": 26253,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 95
                },
                {
                    "start": 95,
                    "end": 199
                },
                {
                    "start": 199,
                    "end": 357
                },
                {
                    "start": 357,
                    "end": 476
                },
                {
                    "start": 476,
                    "end": 662
                },
                {
                    "start": 662,
                    "end": 767
                },
                {
                    "start": 767,
                    "end": 966
                },
                {
                    "start": 966,
                    "end": 1114
                },
                {
                    "start": 1114,
                    "end": 1210
                },
                {
                    "start": 1212,
                    "end": 1384
                },
                {
                    "start": 1384,
                    "end": 1774
                },
                {
                    "start": 1774,
                    "end": 1857
                }
            ],
            "ref_mentions": [
                {
                    "start": 84,
                    "end": 89,
                    "matchedPaperCorpusId": "236159551"
                },
                {
                    "start": 516,
                    "end": 521,
                    "matchedPaperCorpusId": "11253972"
                },
                {
                    "start": 758,
                    "end": 762,
                    "matchedPaperCorpusId": "11536917"
                },
                {
                    "start": 762,
                    "end": 766,
                    "matchedPaperCorpusId": "16550689"
                },
                {
                    "start": 779,
                    "end": 784,
                    "matchedPaperCorpusId": "52927917"
                },
                {
                    "start": 1524,
                    "end": 1529,
                    "matchedPaperCorpusId": "118649278"
                },
                {
                    "start": 1529,
                    "end": 1534,
                    "matchedPaperCorpusId": "53213211"
                },
                {
                    "start": 1534,
                    "end": 1539,
                    "matchedPaperCorpusId": "829159"
                },
                {
                    "start": 1669,
                    "end": 1674,
                    "matchedPaperCorpusId": "206596723"
                },
                {
                    "start": 1674,
                    "end": 1679,
                    "matchedPaperCorpusId": "195847947"
                },
                {
                    "start": 1679,
                    "end": 1684,
                    "matchedPaperCorpusId": "198185886"
                },
                {
                    "start": 1684,
                    "end": 1689,
                    "matchedPaperCorpusId": "198179476"
                },
                {
                    "start": 1689,
                    "end": 1694,
                    "matchedPaperCorpusId": "102351826"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.96728515625
        },
        {
            "corpus_id": "277507254",
            "title": "Counterclockwise block-by-block knowledge distillation for neural network compression",
            "text": "Knowledge distillation The core concept of knowledge distillation involves transferring \"knowledge\" from a teacher model, which is typically a large and high-performance model, to a student model, which is typically a smaller and lightweight model. This transfer aims to enable the student model to acquire the reasoning and generalization capabilities of the teacher model. At present, knowledge distillation can be divided into four main types based on the type of knowledge transferred: (i) output feature knowledge 23 : output feature knowledge usually refers to the final layer features of the teacher model, mainly including logical unit knowledge and soft target knowledge. The basic idea of output feature knowledge distillation is to enable the student model to learn the final prediction of the teacher model, thereby achieving the same prediction ability as the teacher model. Although the original knowledge distillation was proposed for classification tasks and only included interclass similarity as soft target knowledge, in other tasks such as object detection, the final layer feature output of the network may also contain information related to object localization. (ii) Transfer intermediate feature knowledge 24,25 : This method focuses on extracting features from the network layer of the teacher model to provide guidance for the intermediate layer of the student model. (iii) Knowledge of relational features 26,27 : This approach considers that the essence of learning is not in the output of features, but in the relationships between layers and sample data. It mainly emphasizes providing consistent identity mapping, enabling the student model to better grasp the relational knowledge of the teacher model. (iv) Structural Feature Knowledge 28,29 : This strategy involves conveying the comprehensive knowledge system of the teacher model, encompassing not only the output layer knowledge, intermediate feature knowledge, and relational feature knowledge but also the spatial feature distribution and other aspects of the teacher model knowledge. \n\nThe generation gap between teacher and student models In the context of knowledge impartation and educational processes, disparities in understanding, experience, and language between educators or experts and students can hinder effective information transmission, leading to learning obstacles or misunderstandings. \n\nDuring the process of knowledge distillation, structural differences (i.e., the generation gap) between teacher and student models can result in information loss.",
            "score": 0.7100617011987036,
            "section_title": "Related work",
            "char_start_offset": 4318,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 248
                },
                {
                    "start": 249,
                    "end": 374
                },
                {
                    "start": 375,
                    "end": 680
                },
                {
                    "start": 681,
                    "end": 887
                },
                {
                    "start": 888,
                    "end": 1184
                },
                {
                    "start": 1185,
                    "end": 1393
                },
                {
                    "start": 1394,
                    "end": 1584
                },
                {
                    "start": 1585,
                    "end": 1734
                },
                {
                    "start": 1735,
                    "end": 2073
                },
                {
                    "start": 2076,
                    "end": 2392
                },
                {
                    "start": 2395,
                    "end": 2557
                }
            ],
            "ref_mentions": [
                {
                    "start": 519,
                    "end": 521,
                    "matchedPaperCorpusId": "272263991"
                },
                {
                    "start": 1230,
                    "end": 1233,
                    "matchedPaperCorpusId": "265085593"
                },
                {
                    "start": 1233,
                    "end": 1235,
                    "matchedPaperCorpusId": "268744175"
                },
                {
                    "start": 1433,
                    "end": 1436,
                    "matchedPaperCorpusId": "266468082"
                },
                {
                    "start": 1436,
                    "end": 1438,
                    "matchedPaperCorpusId": "257921696"
                },
                {
                    "start": 1772,
                    "end": 1774,
                    "matchedPaperCorpusId": "269660133"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.99072265625
        },
        {
            "corpus_id": "222124879",
            "title": "Online Knowledge Distillation via Multi-branch Diversity Enhancement",
            "text": "Traditional knowledge distillation methods have two stages that require a pretrained teacher model to provide soft output for distillation. Different from above complex training methods, several works adopts collaboratively training strategy. Simultaneously training a group of student models based on each other's predictions is an effective single-stage distillation method, which can be a good substitute for pretrained teacher models. Some methods [16,18] solve this problem. The online knowledge distillation was completed through mutual instruction between two peers [16]. However, the lack of a high-capacity teacher model will decrease the distillation efficiency. In [17,40], each student model learns from the average of the predictions generated by a group of students and obtains a better teacher model effect. ONE found that simply averaging the results would reduce the diversity among students, affecting the training of branch-based models. ONE generates the importance score corresponding to each student through the gate module. By assigning different importance score to each branch, a high-capacity teacher model is constructed, which can leverage knowledge from training data more effectively. OKDDip [19] proposed the concept of two-level distillation. The ensemble results of auxiliary peer networks were distilled into the group leader. The diversified peer network plays a key role in improving distillation performance.",
            "score": 0.7068134675825605,
            "section_title": "Online Knowledge Distillation",
            "char_start_offset": 7086,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 139
                },
                {
                    "start": 140,
                    "end": 242
                },
                {
                    "start": 243,
                    "end": 438
                },
                {
                    "start": 439,
                    "end": 479
                },
                {
                    "start": 480,
                    "end": 578
                },
                {
                    "start": 579,
                    "end": 672
                },
                {
                    "start": 673,
                    "end": 822
                },
                {
                    "start": 823,
                    "end": 956
                },
                {
                    "start": 957,
                    "end": 1046
                },
                {
                    "start": 1047,
                    "end": 1214
                },
                {
                    "start": 1215,
                    "end": 1274
                },
                {
                    "start": 1275,
                    "end": 1360
                },
                {
                    "start": 1361,
                    "end": 1445
                }
            ],
            "ref_mentions": [
                {
                    "start": 452,
                    "end": 456,
                    "matchedPaperCorpusId": "26071966"
                },
                {
                    "start": 573,
                    "end": 577,
                    "matchedPaperCorpusId": "26071966"
                },
                {
                    "start": 676,
                    "end": 680,
                    "matchedPaperCorpusId": "48352434"
                },
                {
                    "start": 680,
                    "end": 683,
                    "matchedPaperCorpusId": "44119099"
                },
                {
                    "start": 1222,
                    "end": 1226,
                    "matchedPaperCorpusId": "208526905"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.7451171875
        },
        {
            "corpus_id": "53488567",
            "title": "Structured Pruning for Efficient ConvNets via Incremental Regularization",
            "text": "(4) Knowledge distillation transfers the learned knowledge from a large teacher model (or ensemble of models) to a small student model, which is pioneered by [37], [38] and refined by Hinton et al. [39]. Ever since, various definitions of knowledge such as attention [40] and metric structure [41] have been proposed to transfer the network expressiveness.",
            "score": 0.7060652012508405,
            "section_title": "II. RELATED WORK",
            "char_start_offset": 6856,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 203
                },
                {
                    "start": 204,
                    "end": 356
                }
            ],
            "ref_mentions": [
                {
                    "start": 158,
                    "end": 162,
                    "matchedPaperCorpusId": "11253972"
                },
                {
                    "start": 164,
                    "end": 168,
                    "matchedPaperCorpusId": "11536917"
                },
                {
                    "start": 293,
                    "end": 297,
                    "matchedPaperCorpusId": "19207026"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.88232421875
        },
        {
            "corpus_id": "272753515",
            "title": "Improving Cone-Beam CT Image Quality with Knowledge Distillation-Enhanced Diffusion Model in Imbalanced Data Settings",
            "text": "Knowledge distillation [7] is a model compression technique that involves transferring insights from a complex teacher model to a simpler student model. The primary objective is to distill valuable knowledge and generalization capabilities embedded in the teacher model, delivering them to a more compact and computationally efficient student model. Typically, a large and high-performing neural network serves as the teacher model, acting as the initial source of information. The student model, often smaller in size and computational complexity, is trained to replicate not only the output predictions of the teacher model but also its internal representations and ac-quired knowledge.",
            "score": 0.7042923458289072,
            "section_title": "Knowledge distillation-based self-training",
            "char_start_offset": 5148,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 152
                },
                {
                    "start": 153,
                    "end": 349
                },
                {
                    "start": 350,
                    "end": 477
                },
                {
                    "start": 478,
                    "end": 688
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.94091796875
        },
        {
            "corpus_id": "174800781",
            "title": "Online Distilling from Checkpoints for Neural Machine Translation",
            "text": "Knowledge distillation is a class of methods which transfers knowledge from a pre-trained teacher model T , to a student model S. The teacher model can be a model with large capacity (Bucila et al., 2006) or an ensemble of several models (Hinton et al., 2015). In knowledge distillation, the student model learns to match the predictions of the teacher model. Concretely, assuming that we learn a classification model (parameterized by \u03b8) on a set of training samples in the form of (x, y) with |V| classes. Instead of minimizing the cross-entropy loss between one-hot label y and model's output probability p(y|x; \u03b8), knowledge distillation uses the teacher model's distribution q(\u2022|x) as \"soft targets\" and optimizes the loss: \n\nwhere \u03b8 T parameterizes the teacher model and p(\u2022|x) is the distribution of the student model. Kim and Rush (2016) proposed that, as the loss of NMT model (Equation 4) can be factored into minimizing cross-entropy loss between the target words and word-level probabilities of the NMT model for every position at target side, knowledge distillation on multi-class classification can be naturally applied. They defined word-level knowledge distillation (W-KD) on a sentence as: \n\n\u2022 log p(y j = k|y <j , H(x); \u03b8), where V is the target vocabulary. They further proposed sequence-level knowledge distillation (S-KD), which optimizes the student model by matching the predictions of the teacher model in the probability distribution over the space of all possible target sequences: \n\nwhere \u03c4 is the space of target side sentences. As summing over exponential numbers of samples here is intractable, they proposed to train student model on samples generated by teacher model as an approximation. \n\n3 Online Distillation from Checkpoints",
            "score": 0.7030594447815925,
            "section_title": "Knowledge Distillation in Neural Machine Translation",
            "char_start_offset": 5201,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 260
                },
                {
                    "start": 261,
                    "end": 359
                },
                {
                    "start": 360,
                    "end": 507
                },
                {
                    "start": 508,
                    "end": 728
                },
                {
                    "start": 731,
                    "end": 825
                },
                {
                    "start": 826,
                    "end": 1134
                },
                {
                    "start": 1135,
                    "end": 1206
                },
                {
                    "start": 1209,
                    "end": 1275
                },
                {
                    "start": 1276,
                    "end": 1507
                },
                {
                    "start": 1510,
                    "end": 1556
                },
                {
                    "start": 1557,
                    "end": 1720
                },
                {
                    "start": 1723,
                    "end": 1761
                }
            ],
            "ref_mentions": [
                {
                    "start": 183,
                    "end": 204,
                    "matchedPaperCorpusId": "11253972"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.98046875
        },
        {
            "corpus_id": "236782270",
            "title": "Adaptive Teacher Finetune: Towards high-performance knowledge distillation through adaptive fine-tuning",
            "text": "Knowledge distillation [1] is a well-known technique to learn compact deep neural network models with competitive accuracy, where a smaller network (student) is trained to simulate the representations of a larger one (teacher) with higher accuracy. The popularity of knowledge distillation is mainly due to its simplicity and generality; it is straightforward to learn a student model based on a teacher and there is no restriction about the network architectures of both models. The main goal of most approaches is how to transfer dark knowledge to student models effectively, given predefifined or pretrained teacher networks. Parameter quantification or character recognition, pruning, and knowledge distillation (KD) are some well-known methods in this subject [1]. KD defines the class probability of the teacher system as the aim that the tiny contactors strives to replicate by transferring the knowledge of big pre-training networks (or integration of tiny networks) as a teacher network. Students can enhance their performance by matching their predictions with the teacher's predictions. There is no unique teacher-student function in provided to users refinement. Starting with the initial training, all systems learn at the same time by teaching each other. It uses ground truth labels to train traditional cross-entropy loss and imitation loss to learn from peers. The result obtained by the network trained in this online distillation method is not only better than the network trained only by cross-entropy loss, but it performs better than a network taught in the typical offline distillation method through an also before the teacher network Hinton et al. developed many prospective knowledge distillation approaches to promote the optimization process of distilling by using diverse \"information,\" such as intermediate representation, in [2], inter-layer flow [3], pay attention to figure [5], structure Relationship [6] and activation similarity [7]. While these 's system well in compressed deployment models, they often use a two-stage training approach, which involves before the a highly capable previous teachers before transferring knowledge to a compact student model, which takes more time and money.",
            "score": 0.7022082240603666,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 248
                },
                {
                    "start": 249,
                    "end": 479
                },
                {
                    "start": 480,
                    "end": 628
                },
                {
                    "start": 629,
                    "end": 769
                },
                {
                    "start": 770,
                    "end": 996
                },
                {
                    "start": 997,
                    "end": 1097
                },
                {
                    "start": 1098,
                    "end": 1174
                },
                {
                    "start": 1175,
                    "end": 1269
                },
                {
                    "start": 1270,
                    "end": 1377
                },
                {
                    "start": 1378,
                    "end": 1969
                },
                {
                    "start": 1970,
                    "end": 2227
                }
            ],
            "ref_mentions": [
                {
                    "start": 1907,
                    "end": 1910,
                    "matchedPaperCorpusId": "206596723"
                },
                {
                    "start": 1935,
                    "end": 1938,
                    "matchedPaperCorpusId": "198185886"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.90771484375
        },
        {
            "corpus_id": "268707594",
            "title": "From Toxic to Trustworthy: Using Self-Distillation and Semi-supervised Methods to Refine Neural Networks",
            "text": "Knowledge distillation, as shown in Fig. 3 is a technique that aims to transfer the knowledge from a large, overparameterized teacher model to a smaller, more compact student model. This process is often used as a compression approach and can lead to improved performance, as well as enable high compression and rapid acceleration. By transferring the knowledge from the teacher model to the student model, the student model can achieve comparable performance with the teacher model while being more efficient in terms of memory and computation. The basic idea behind knowledge distillation is to train the student model to mimic the outputs of the teacher model, such as the probability vectors, while minimizing the difference between the student's predictions and the teacher's predictions. This allows the student model to learn from the teacher's expertise, improving its performance and generalization ability on the target task. However, traditional knowledge distillation methods suffer from low efficiency in knowledge transfer and challenges in designing and training appropriate teacher models. To address these issues, self-distillation (Zhang et al. 2019) has been proposed as a novel one-step framework, focusing directly on training the student model. This approach not only reduces training time significantly but also achieves higher accuracy, making it a promising alternative to traditional knowledge distillation methods.",
            "score": 0.7021068725841562,
            "section_title": "Knowledge Distillation",
            "char_start_offset": 6190,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 181
                },
                {
                    "start": 182,
                    "end": 331
                },
                {
                    "start": 332,
                    "end": 545
                },
                {
                    "start": 546,
                    "end": 793
                },
                {
                    "start": 794,
                    "end": 935
                },
                {
                    "start": 936,
                    "end": 1105
                },
                {
                    "start": 1106,
                    "end": 1266
                },
                {
                    "start": 1267,
                    "end": 1441
                }
            ],
            "ref_mentions": [
                {
                    "start": 1149,
                    "end": 1167,
                    "matchedPaperCorpusId": "159041406"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.9482421875
        },
        {
            "corpus_id": "237250417",
            "title": "BERT Learns to Teach: Knowledge Distillation with Meta Learning",
            "text": "Knowledge Distillation Recently, many attempts have been made to accelerate large neural networks (Xu et al., , 2021bXu & McAuley, 2022). Knowledge distillation is a prominent method for training compact networks to achieve comparable performance to a deep network. Hinton et al. (2015b) first introduced the idea of knowledge distillation to exploit the \"dark knowledge\" (i.e., soft label distribution) from a large teacher model as additional supervision for training a smaller student model. Since its introduction, several works (Romero et al., 2015;Zagoruyko & Komodakis, 2017;Tung & Mori, 2019;Park et al., 2019;Sun et al., 2019;Jiao et al., 2019) have investigated methods that align different latent representations between the student and teacher models for better knowledge transfer. In the context of knowledge distillation, MetaDistil shares some common ideas with the line of work that utilizes a sequence of intermediate teacher models to make the teacher network better adapt to the capacity of the student model throughout the training process, including teacher assistant knowledge distillation (TAKD) (Mirzadeh et al., 2020) and route constraint optimization (RCO) . However, the intermediate teachers are heuristically selected independently of the training process and the evolution of the teacher network is discrete. In contrast, MetaDistil employs meta learning to make the teacher model adapt to the current state of the student model and provide a continuously evolving meta-teacher that can better teach the student. Concurrently, Park et al. (2021) and Shi et al. (2021) propose to update the teacher model jointly with the student model with task specific objectives (e.g., cross-entropy loss) during the KD process and add constraints to keep student and teacher similar to each other. Their approaches makes the teacher model aware of the student model by constraining the teacher model's capacity. However, the teacher models in their methods are still not optimized for knowledge transfer. In addition, Zhang et al. (2018) introduced deep mutual learning where multiple models learn collaboratively and teach each other throughout the training process. While it is focused on a different setting where different models have approximately the same capacity and are learned from scratch, it also",
            "score": 0.7004433388914115,
            "section_title": "Related Work",
            "char_start_offset": 9555,
            "sentence_offsets": [],
            "ref_mentions": [
                {
                    "start": 582,
                    "end": 600,
                    "matchedPaperCorpusId": "198179476"
                },
                {
                    "start": 600,
                    "end": 618,
                    "matchedPaperCorpusId": "131765296"
                },
                {
                    "start": 618,
                    "end": 635,
                    "matchedPaperCorpusId": "201670719"
                },
                {
                    "start": 1119,
                    "end": 1142,
                    "matchedPaperCorpusId": "212908749"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.92236328125
        },
        {
            "corpus_id": "271488200",
            "title": "A Review of Recent Hardware and Software Advances in GPU-Accelerated Edge-Computing Single-Board Computers (SBCs) for Computer Vision",
            "text": "Knowledge distillation involves transferring knowledge from a large, cumbersome model (the teacher) to a smaller, more efficient model (the student). Techniques such as Hinton's Knowledge Distillation [84] and Born-Again Networks [85] aid in this knowledge transfer. Recent advancements incorporate attention mechanisms and multi-stage distillation to enhance the student model performance and accuracy. Knowledge distillation is pivotal for transferring knowledge from large, complex models to smaller, efficient ones, facilitating deployment on resource-constrained edge-computing devices. Various approaches have evolved over time, enabling the compression of complex models without significant performance loss. Early techniques focused on transferring knowledge from a large, well-trained teacher model to a smaller student model, typically involving mimicking the teacher's behaviour through soft labels or intermediate representations. Notable early approaches include the following: Recent advancements in knowledge distillation have focused on integrating selfdistillation techniques and exploring the synergy between different distillation approaches. These developments aim to enhance the scalability and adaptability of knowledge distillation methods, enabling the efficient deployment of compact and accurate CV models in diverse edge-computing applications.",
            "score": 0.699292589608859,
            "section_title": "Knowledge Distillation",
            "char_start_offset": 43009,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 149
                },
                {
                    "start": 150,
                    "end": 266
                },
                {
                    "start": 267,
                    "end": 403
                },
                {
                    "start": 404,
                    "end": 591
                },
                {
                    "start": 592,
                    "end": 715
                },
                {
                    "start": 716,
                    "end": 942
                },
                {
                    "start": 943,
                    "end": 1161
                },
                {
                    "start": 1162,
                    "end": 1371
                }
            ],
            "ref_mentions": [
                {
                    "start": 201,
                    "end": 205,
                    "matchedPaperCorpusId": "220364494"
                },
                {
                    "start": 230,
                    "end": 234,
                    "matchedPaperCorpusId": "238203535"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.96630859375
        },
        {
            "corpus_id": "237855678",
            "title": "Virtual to Real-World Transfer Learning: A Systematic Review",
            "text": "Knowledge distillation is a process where large high dimensionality networks are distilled into smaller networks that are smaller but efficient. In this case, the larger network (the teacher) generates training data for the training of the smaller network (the student) [11]. For example, complex visuals that contain high dimensional input are further merged into simpler forms when training the new model.",
            "score": 0.6973161090739934,
            "section_title": "Knowledge Distillation",
            "char_start_offset": 11736,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 144
                },
                {
                    "start": 145,
                    "end": 275
                },
                {
                    "start": 276,
                    "end": 407
                }
            ],
            "ref_mentions": [
                {
                    "start": 270,
                    "end": 274,
                    "matchedPaperCorpusId": "219559263"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.78076171875
        },
        {
            "corpus_id": "236782048",
            "title": "Research on the Theory and Application of Deep Interactive Learning",
            "text": "Knowledge distillation is defined as refining a larger neural network model into a smaller model [1]. The larger neural network is like a teacher network, while the smaller neural network can be regarded as a student network. In addition to probability distribution, other studies also try to extract various features to students. In training the model efficiency of a small student network, KD can achieve the same effect as some model compression methods, such as pruning [2] and quantification. Knowledge distillation is like a teacher instilling knowledge into students without getting feedback from students, which is obviously not conducive to students' learning. Therefore, some scholars have proposed a method of Deep Mutual Learning (DML) [11]. By interacting the characteristics of the output layer of the teacher network with the student network, kullback-leibler is used to measure the degree of interaction. In this framework, the boundary between each student network and the teacher network is no longer obvious, and students are more inclined to learn from each other. One advantage of this method is that it can flexibly apply any different network architecture. But the fly in the ointment is that this method can only exchange limited information, because it does not make full use of the rich information of teacher model.Another classical online distillation method is the on-the-fly native ensemble (ONE). It focus on improving the performance of student network using the gate of branch logic. But trying to tranfer the knowledge from teacher model to student model is focal point of this method.The shortcoming of the ONE is that it can only train a single architecture because of the branch gate logic. Our method is based on DML, and some improvements have been made.",
            "score": 0.6934502871393419,
            "section_title": "Knowledge Distillation (KD)",
            "char_start_offset": 5271,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 101
                },
                {
                    "start": 102,
                    "end": 225
                },
                {
                    "start": 226,
                    "end": 330
                },
                {
                    "start": 331,
                    "end": 497
                },
                {
                    "start": 498,
                    "end": 669
                },
                {
                    "start": 670,
                    "end": 753
                },
                {
                    "start": 754,
                    "end": 920
                },
                {
                    "start": 921,
                    "end": 1084
                },
                {
                    "start": 1085,
                    "end": 1179
                },
                {
                    "start": 1180,
                    "end": 1427
                },
                {
                    "start": 1428,
                    "end": 1516
                },
                {
                    "start": 1517,
                    "end": 1727
                },
                {
                    "start": 1728,
                    "end": 1793
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.88037109375
        },
        {
            "corpus_id": "277940190",
            "title": "Feature Alignment and Representation Transfer in Knowledge Distillation for Large Language Models",
            "text": "The concept of knowledge distillation (KD) has gained significant attention in recent years, with numerous studies focusing on improving the effectiveness of this technique (Ku et al., 2020;Zhang et al., 2021;Gao, 2023). The architecture of the knowledge distillation framework can be visualized, as shown in Figure 2, which illustrates the key components and their interactions in the distillation process. This framework typically involves a teacher model, which is pretrained on a large dataset, and a student model that learns from the teacher's output. This process allows the student to mimic the teacher's behavior and transfer knowledge in a more efficient manner, especially for applications where computational resources are limited. \n\nKnowledge distillation (KD) involves transferring knowledge from a pre-trained teacher model to a student model, typically with a smaller capacity, to enhance student performance (Ruffy and Chahal, 2019). This is typically achieved by optimizing a combined loss function that blends hard-target supervision with a softened probability distribution from the teacher model. \n\n(1) where L CE is the standard cross-entropy loss, L KL is the Kullback-Leibler divergence (Hinton et al., 2015), T is the temperature, \u03bb balances the two terms, and \u03c3(\u2022) denotes the softmax function. \n\nThis process has been shown to be highly effective in various applications including image classification, natural language processing, and speech recognition. Recent studies have introduced new variants of knowledge distillation (KD), such as teaching-assistant distillation, curriculum distillation, mask distillation, and decoupling distillation, which aim to improve the performance of KD by introducing additional components or modifying the learning process (Yu et al., 2024;Zhang et al., 2024c). Furthermore, novel frameworks, such as partial-to-whole knowledge distillation (PWKD) and Correlation-Aware Knowledge Distillation (CAKD) have been proposed to improve the efficacy of KD by decomposing knowledge and prioritizing influential distillation components Fig. 2: Foundations of Knowledge Distillation.",
            "score": 0.6928133413313504,
            "section_title": "II. BACKGROUND AND FOUNDATIONS",
            "char_start_offset": 3801,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 220
                },
                {
                    "start": 221,
                    "end": 407
                },
                {
                    "start": 408,
                    "end": 557
                },
                {
                    "start": 558,
                    "end": 743
                },
                {
                    "start": 746,
                    "end": 950
                },
                {
                    "start": 951,
                    "end": 1117
                },
                {
                    "start": 1120,
                    "end": 1320
                },
                {
                    "start": 1323,
                    "end": 1482
                },
                {
                    "start": 1483,
                    "end": 1825
                },
                {
                    "start": 1826,
                    "end": 2137
                }
            ],
            "ref_mentions": [
                {
                    "start": 1787,
                    "end": 1804,
                    "matchedPaperCorpusId": "273811396"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.9375
        },
        {
            "corpus_id": "277443344",
            "title": "Multi-Band NIR Colorization via Dual-Teacher Color and Structure Distillation",
            "text": "Knowledge distillation [32] is a technique used to transfer knowledge from one network (teacher) to another (student). The conventional model training approach is considered less efficient because the model needs to create and learn ways to solve problems without any underlying knowledge. \n\nTo achieve more effective model training, a method has been developed where a teacher is employed, similar to training a person, by providing more data to instruct and train the model [33]. The teacher network provides its own prelearned knowledge to the student network, which is trained by imitating the transferred knowledge. The knowledge distillation technique has the advantage of improving the performance of a model without increasing the number of parameters and complexity of the network. Therefore, this technology is widely used in neural network compression to enable network that typically require significant computational power on large servers to run efficiently on smartphones, where computational resources are limited.",
            "score": 0.692802852492286,
            "section_title": "IV. THE PROPOSED METHOD A. UNDERSTANDING THE KNOWLEDGE DISTILLATION NETWORK",
            "char_start_offset": 12368,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 118
                },
                {
                    "start": 119,
                    "end": 289
                },
                {
                    "start": 292,
                    "end": 481
                },
                {
                    "start": 482,
                    "end": 620
                },
                {
                    "start": 621,
                    "end": 790
                },
                {
                    "start": 791,
                    "end": 1030
                }
            ],
            "ref_mentions": [
                {
                    "start": 476,
                    "end": 480,
                    "matchedPaperCorpusId": "102483181"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.93115234375
        },
        {
            "corpus_id": "258741280",
            "title": "A survey of the vision transformers and their CNN-transformer based variants",
            "text": "This category enlists those ViT architectures that utilize a knowledge transfer (knowledge distillation) approach. It involves conveying knowledge from a larger network to a smaller network, much like a teacher imparting knowledge to a student (Kanwal et al. 2023;Habib et al. 2023). The teacher model is usually a complex model with ample learning capability, while the student model is simpler. The basic idea behind knowledge distillation is to facilitate the student model in acquiring and incorporating the distinctive features of the teacher model. This can be particularly useful for tasks where computational resources are limited, as the smaller ViT model can be deployed more efficiently than the larger one.",
            "score": 0.6923823842041339,
            "section_title": "Knowledge transfer-based approaches",
            "char_start_offset": 22727,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.88330078125
        },
        {
            "corpus_id": "244119160",
            "title": "Combining Curriculum Learning and Knowledge Distillation for Dialogue Generation",
            "text": "Knowledge distillation describes a class of methods for the knowledge transfer from teacher network to student network. In our model, the student network S \u03b8 is trained over the same architecture but different parameters as teacher model T \u03b8 . The teacher has previously been trained, and we freeze its parameters when training the student network. \n\nWe transfer the knowledge from teacher to student by minimizing the similarity distance between the output of student network and the soft label generated by the teacher network. We use crossentropy loss to measure the two logits as (Romero et al., 2015). To further improve the sequence-tosequence student model, hard-assigned labels are also utilized. The final student network is trained to optimize the following compound objective: \n\nwhere H refers to the cross-entropy and V is a parameter to indicate the temperature of distillation. Later, we will use the method of the model level curriculum learning to process \u03bb in section 2.5. Note that the first term in Equation ( 3) corresponds to the traditional cross-entropy between the softmax layer's output of a (student) network and word distribution in response Y , whereas the second term is to learn from the softened output of the teacher network to strengthen its supervision for the student. \n\nIn the teacher model, we train it by using all the dataset with original order, while in the student model, the training starts from the step that consists of examples with the lowest difficulty. After that, data in the next step is aggregated to the current training dataset.",
            "score": 0.6923323838994426,
            "section_title": "Output Knowledge Distillation",
            "char_start_offset": 9984,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 119
                },
                {
                    "start": 120,
                    "end": 243
                },
                {
                    "start": 244,
                    "end": 348
                },
                {
                    "start": 351,
                    "end": 529
                },
                {
                    "start": 530,
                    "end": 606
                },
                {
                    "start": 607,
                    "end": 704
                },
                {
                    "start": 705,
                    "end": 787
                },
                {
                    "start": 790,
                    "end": 891
                },
                {
                    "start": 892,
                    "end": 989
                },
                {
                    "start": 990,
                    "end": 1303
                },
                {
                    "start": 1306,
                    "end": 1501
                },
                {
                    "start": 1502,
                    "end": 1582
                }
            ],
            "ref_mentions": [
                {
                    "start": 584,
                    "end": 605,
                    "matchedPaperCorpusId": "2723173"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.86328125
        },
        {
            "corpus_id": "213158848",
            "title": "Look One and More: Distilling Hybrid Order Relational Knowledge for Cross-Resolution Image Recognition",
            "text": "With the improvement of model performance, the number of model parameters is also increasing. It often needs to occupy huge memory resources and consume lots of time, leading to the issue of model compression. Knowledge distillation (Hinton, Vinyals, and Dean 2015;Romero et al. 2015;Liu et al. 2019;Park et al. 2019;Xue et al. 2019) provides a feasible way to address that issue. Simply speaking, its idea is using a large parameter model (teacher) to supervise the learning of a small parameter model (student). In practice, the student model learns the output behaviors of the teacher model that has been pre-trained on some target dataset. Although the student model can't reach the accuracy of the teacher model, it still is very powerful and yet efficient. There are two key factors of knowledge distillation: what knowledge has been learned and how to effectively transfer knowledge from teacher model to student model. \n\nTraditional knowledge distillation tends to the direct transmission of instance information, and the teacher and student often input in pairs, losing the structural information Figure 2: The framework of our approach. By focusing on the structural relational knowledge in various orders, our approach step-wisely transfers knowledge from a complex teacher to a light teacher with an assistant by cross-structure distillation and and cross-resolution distillation. about original data. Some recent approaches (Yu et al. 2019;Liu et al. 2019) attempt to pay more attention to the structural relationship of the output rather than the output itself, so when training student model, they try to mimic the same relationship structures as teachers. These approaches can retain higher-order attribute information in transfer learning. Intuitively, more effective information can be obtained by focusing on higher-order relational knowledge, which inspires the original intention of our new distillation approach. \n\n3 Proposed Approach",
            "score": 0.690555136989087,
            "section_title": "Knowledge Distillation and Transfer",
            "char_start_offset": 6716,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 93
                },
                {
                    "start": 94,
                    "end": 209
                },
                {
                    "start": 210,
                    "end": 380
                },
                {
                    "start": 381,
                    "end": 513
                },
                {
                    "start": 514,
                    "end": 643
                },
                {
                    "start": 644,
                    "end": 762
                },
                {
                    "start": 763,
                    "end": 926
                },
                {
                    "start": 929,
                    "end": 1146
                },
                {
                    "start": 1147,
                    "end": 1392
                },
                {
                    "start": 1393,
                    "end": 1413
                },
                {
                    "start": 1414,
                    "end": 1671
                },
                {
                    "start": 1672,
                    "end": 1756
                },
                {
                    "start": 1757,
                    "end": 1934
                },
                {
                    "start": 1937,
                    "end": 1956
                }
            ],
            "ref_mentions": [
                {
                    "start": 265,
                    "end": 284,
                    "matchedPaperCorpusId": "52012952"
                },
                {
                    "start": 284,
                    "end": 300,
                    "matchedPaperCorpusId": "198185886"
                },
                {
                    "start": 300,
                    "end": 317,
                    "matchedPaperCorpusId": "131765296"
                },
                {
                    "start": 317,
                    "end": 333,
                    "matchedPaperCorpusId": "198905931"
                },
                {
                    "start": 1437,
                    "end": 1453,
                    "matchedPaperCorpusId": "102351826"
                },
                {
                    "start": 1453,
                    "end": 1469,
                    "matchedPaperCorpusId": "198185886"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.9609375
        },
        {
            "corpus_id": "258564799",
            "title": "DynamicKD: An Effective Knowledge Distillation via Dynamic Entropy Correction-Based Distillation for Gap Optimizing",
            "text": "Therefore, this paper focuses on knowledge distillation-based model compression. \n\nIn the knowledge distillation, how the teacher network better guides the student network training has become vital research. Several researchers have studied various knowledge used in the distillation process. \n\nRomero et al. applied the middle layer features of the teacher network as knowledge to guide the student network learning [26]. Komodakis et al. transferred attention from the large-scale teacher network to the student network [27]. Damiano et al. viewed the knowledge transfer between teacher and student networks as maximizing the mutual information between teacher and student networks [28]. Zagoruyko et al. used the attention mechanism as a learnable knowledge, and they used the teacher network's attention knowledge to guide the student network's training. In neural networks, the convolutional layers map one feature to another. Yim et al. treated the mapping processing of features between layers as knowledge and used the FSP matrix to describe this knowledge so that the student network could imitate it [29]. The rich and varied knowledge exchange between the teacher and student networks helps the student network training. \n\nBut, these methods do not focus on the performance gap between the teacher and student networks, which may affect student network learning. \n\nSeveral works have studied the performance gap and proposed corresponding improvements. Cho et al. found that the underperformance teacher with early-stop training benefits the student [30]. Mirzadeh et al. found that when the gap between the teacher network and the student network is large, the student trained by a lower-performance lightweight teacher network performs better than the one taught by a higher-performance large-scale teacher network [16]. For this reason, he utilized a medium-sized neural network (called Teacher Assistant) to help the student network cross the large performance gap. Both methods mentioned above show that reducing the performance gap can improve distillation performance. \n\nHowever, these static methods do not correct the performance gap further during the distillation. The performance gap keeps changing with the performance improvement of the student network, so these strategies may still hinder the student network from imitating the high-performance teacher network. \n\nVarious knowledge distillation algorithms are available to continuously update the knowledge applied for the student network training during the distillation.",
            "score": 0.6891534827118627,
            "section_title": "Introduction",
            "char_start_offset": 2212,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 80
                },
                {
                    "start": 83,
                    "end": 207
                },
                {
                    "start": 208,
                    "end": 292
                },
                {
                    "start": 295,
                    "end": 422
                },
                {
                    "start": 423,
                    "end": 527
                },
                {
                    "start": 528,
                    "end": 689
                },
                {
                    "start": 690,
                    "end": 858
                },
                {
                    "start": 859,
                    "end": 931
                },
                {
                    "start": 932,
                    "end": 1115
                },
                {
                    "start": 1116,
                    "end": 1231
                },
                {
                    "start": 1234,
                    "end": 1373
                },
                {
                    "start": 1376,
                    "end": 1463
                },
                {
                    "start": 1464,
                    "end": 1566
                },
                {
                    "start": 1567,
                    "end": 1833
                },
                {
                    "start": 1834,
                    "end": 1980
                },
                {
                    "start": 1981,
                    "end": 2086
                },
                {
                    "start": 2089,
                    "end": 2186
                },
                {
                    "start": 2187,
                    "end": 2388
                },
                {
                    "start": 2391,
                    "end": 2549
                }
            ],
            "ref_mentions": [
                {
                    "start": 522,
                    "end": 526,
                    "matchedPaperCorpusId": "829159"
                },
                {
                    "start": 684,
                    "end": 688,
                    "matchedPaperCorpusId": "118649278"
                },
                {
                    "start": 1561,
                    "end": 1565,
                    "matchedPaperCorpusId": "203642130"
                },
                {
                    "start": 1828,
                    "end": 1832,
                    "matchedPaperCorpusId": "212908749"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.8837890625
        },
        {
            "corpus_id": "277398866",
            "title": "Distilling Wisdom: A Review on Optimizing Learning From Massive Language Models",
            "text": "Knowledge distillation is a technique used to compress and transfer knowledge from a large, complex model(the teacher) to a smaller, simpler model(the student). This process involves the student model learning to mimic the behavior of the teacher model, thereby achieving similar performance with fewer parameters and reduced computational requirements [33], [34]. The primary objective of knowledge distillation is to retain the accuracy and capabilities of the large model while significantly reducing its size and inference time. \n\nThe core techniques of knowledge distillation revolve around three main approaches: soft targets, feature matching, and attention transfer. Instead of using hard labels for training, the student model is trained on the soft targets provided by the teacher model, which include the probability distribution of the output classes, helping the student model capture the teacher model's learned knowledge more effectively [33], [34]. The term ''soft target'' denotes the probability distribution from a teacher model that provides confidence scores for various answers, helping the student model learn more efficiently than just using the correct answer [35], [36]. Feature matching involves using the intermediate features (activations) of the teacher model to guide the learning process of the student model, allowing the student to learn richer and more nuanced features [34], [37]. Attention transfer involves transferring the attention maps of the teacher model to the student model, highlighting important areas in the input data that the teacher model focuses on, thus helping the student learn where to pay attention [37]. By guiding a smaller model to concentrate on the significant aspects of the data identified by a larger model, this technique contributes to improved accuracy in predictions [38], [39]. \n\nThe current research of KD focuses on several key areas, including cross-modal knowledge distillation, selfdistillation, online knowledge distillation, and distillation for robustness and generalization. Cross-modal KD extends distillation techniques to scenarios where the teacher and student models operate on different modalities, such as distilling knowledge from a vision model to a language model [40], [41].",
            "score": 0.6886778172324168,
            "section_title": "B. OVERVIEW OF KNOWLEDGE DISTILLATION",
            "char_start_offset": 21468,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 160
                },
                {
                    "start": 161,
                    "end": 364
                },
                {
                    "start": 365,
                    "end": 532
                },
                {
                    "start": 535,
                    "end": 674
                },
                {
                    "start": 675,
                    "end": 964
                },
                {
                    "start": 965,
                    "end": 1196
                },
                {
                    "start": 1197,
                    "end": 1416
                },
                {
                    "start": 1417,
                    "end": 1661
                },
                {
                    "start": 1662,
                    "end": 1847
                },
                {
                    "start": 1850,
                    "end": 2053
                },
                {
                    "start": 2054,
                    "end": 2264
                }
            ],
            "ref_mentions": [
                {
                    "start": 1191,
                    "end": 1195,
                    "matchedPaperCorpusId": "252846553"
                },
                {
                    "start": 1836,
                    "end": 1840,
                    "matchedPaperCorpusId": "222831725"
                },
                {
                    "start": 1842,
                    "end": 1846,
                    "matchedPaperCorpusId": "252995794"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.98095703125
        },
        {
            "corpus_id": "270878760",
            "title": "Survey on Knowledge Distillation for Large Language Models: Methods, Evaluation, and Application",
            "text": "The feature-based knowledge distillation methods [46,122] extract knowledge from the embedding space, transformer layers, and prediction layers, allowing the student model to learn various aspects of the teacher model comprehensively.For instance, Sun et al. [122] proposed a patient knowledge distillation (PKD) method aimed at compressing a large-scale teacher model into an equally effective lightweight student model.They proposed two distinct distillation strategies: 1) PKD-Last: The student model learns from the last  layers of the teacher model, based on the assumption that the top layers contain the most informative knowledge.2) PKD-Skip: The student learns from every -layer of the teacher, suggesting that the lower layers also contain essential information that should be gradually transferred during distillation.Experiments conducted on seven datasets across four tasks-sentiment classification, paraphrase similarity matching, natural language inference, and machine reading comprehension-showed that the PKD method outperformed standard knowledge distillation methods.It achieved superior performance and better generalization, significantly enhancing training efficiency and reducing storage requirements while maintaining accuracy comparable to the original large-scale model.MetaDistill [168] offers a simple and efficient alternative to traditional KD methods by keeping the teacher model fixed during training.Within the meta-learning framework, teacher networks enhance knowledge transfer to student networks by distilling feedback on student performance.Additionally, a pilot update mechanism is introduced to improve the alignment between internal learners and meta-learners, focusing on enhancing internal learners' performance.Extensive experiments have validated the effectiveness and versatility of this method across text and image classification tasks.Furthermore, experiments on the GLUE benchmark have shown that MetaDistill significantly outperforms traditional knowledge distillation, achieving state-of-the-art performance compression.AD-KD [155] addresses two key limitations of existing knowledge distillation methods.First, student models often merely mimic the teacher's behavior without developing their own reasoning capabilities.Second, these methods typically focus on transferring knowledge specific to complex models while neglecting data-specific knowledge.",
            "score": 0.6886682773586494,
            "section_title": "Hint-based KD.",
            "char_start_offset": 16483,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 234
                },
                {
                    "start": 234,
                    "end": 421
                },
                {
                    "start": 421,
                    "end": 638
                },
                {
                    "start": 638,
                    "end": 829
                },
                {
                    "start": 829,
                    "end": 1087
                },
                {
                    "start": 1087,
                    "end": 1297
                },
                {
                    "start": 1297,
                    "end": 1434
                },
                {
                    "start": 1434,
                    "end": 1580
                },
                {
                    "start": 1580,
                    "end": 1756
                },
                {
                    "start": 1756,
                    "end": 1885
                },
                {
                    "start": 1885,
                    "end": 2073
                },
                {
                    "start": 2073,
                    "end": 2158
                },
                {
                    "start": 2158,
                    "end": 2274
                },
                {
                    "start": 2274,
                    "end": 2406
                }
            ],
            "ref_mentions": [
                {
                    "start": 49,
                    "end": 53,
                    "matchedPaperCorpusId": "215415863"
                },
                {
                    "start": 53,
                    "end": 57,
                    "matchedPaperCorpusId": "201670719"
                },
                {
                    "start": 259,
                    "end": 264,
                    "matchedPaperCorpusId": "201670719"
                },
                {
                    "start": 1309,
                    "end": 1314,
                    "matchedPaperCorpusId": "237250417"
                },
                {
                    "start": 2079,
                    "end": 2084,
                    "matchedPaperCorpusId": "258740796"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.912109375
        },
        {
            "corpus_id": "252110723",
            "title": "On the effectiveness of compact biomedical transformers",
            "text": "Knowledge distillation (Hinton et al., 2015) is the process of transferring knowledge from a larger model called \"teacher\" to a smaller one called \"student\" using the larger model's outputs as soft labels. Distillation can be done in a task-specific way where the pre-trained model is first fine-tuned on a task and then the student attempts to imitate the teacher network. This is an effective method, however, fine-tuning of a pre-trained model can be computationally expensive. Task-agnostic distillation, on the other hand, allows the student to mimic the teacher by looking at its masked language predictions or intermediate representations. The student can subsequently be directly fine-tuned on the final task (Wang et al., 2020;Yao et al., 2021). \n\nDistilBERT is a well-known example of a compressed model that uses knowledge distillation to transfer the knowledge within the BERT base model to a much smaller student network which is about 40% smaller and 60% faster. It uses a triple loss which is a linear combination of language modeling, distillation and cosine-distance losses.",
            "score": 0.68531017278142,
            "section_title": "Knowledge Distillation",
            "char_start_offset": 13990,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 205
                },
                {
                    "start": 206,
                    "end": 373
                },
                {
                    "start": 374,
                    "end": 480
                },
                {
                    "start": 481,
                    "end": 646
                },
                {
                    "start": 647,
                    "end": 754
                },
                {
                    "start": 757,
                    "end": 976
                },
                {
                    "start": 977,
                    "end": 1091
                }
            ],
            "ref_mentions": [
                {
                    "start": 717,
                    "end": 736,
                    "matchedPaperCorpusId": "211296536"
                },
                {
                    "start": 736,
                    "end": 753,
                    "matchedPaperCorpusId": "235652233"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.95849609375
        },
        {
            "corpus_id": "263671901",
            "title": "Talking Models: Distill Pre-trained Knowledge to Downstream Models via Interactive Communication",
            "text": "Many recent breakthroughs in machine learning have been enabled by the pre-trained foundation models. By scaling up model parameters, training data, and computation resources, foundation models have significantly advanced the state-of-the-art in many applications. However, it is still an open question of how to use these models to perform downstream tasks efficiently. Knowledge distillation (KD) has been explored to tackle this challenge. KD transfers knowledge from a large teacher model to a smaller student model. While KD has been successful in improving student model performance, recent research has discovered that a powerful teacher does not necessarily lead to a powerful student, due to their huge capacity gap. In addition, the potential distribution shifts between the pre-training data and downstream tasks can make knowledge transfer in KD sub-optimal for improving downstream task performance. In this paper, we extend KD with an interactive communication process to help students of downstream tasks learn effectively from pre-trained foundation models. Our design is inspired by the way humans learn from teachers who can explain knowledge in a way that meets the students' needs. Specifically, we let each model (i.e., student and teacher) train two components: (1) an encoder encoding the model's hidden states to a message and (2) a decoder decoding any messages to its own hidden states. With encoder and decoder, not only can the teacher transfer rich information by encoding its hidden states, but also the student can send messages with information of downstream tasks to the teacher. Therefore, knowledge passing from teacher to student can be tailored to the student's capacity and downstream tasks' distributions. We conducted experiments on benchmark datasets to show that our communication mechanism outperforms state-of-the-art distillation techniques.",
            "score": 0.6848751096673928,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.9287109375
        },
        {
            "corpus_id": "227228204",
            "title": "A Selective Survey on Versatile Knowledge Distillation Paradigm for Neural Network Models",
            "text": "Knowledge distillation is an effective model compression technique in which a compact model (student) is trained under the supervision of a larger pre-trained model or an ensemble of models (teacher). \n\nKnowledge distillation aims to improve the performance of the student network by providing additional supervision from a teacher network. To the best of our knowledge, exploiting knowledge transfer to compress model was first proposed in C. Bucilu\u01ce et al. [4]. They trained a compressed/ensemble model of strong classifiers with pseudo-labeled data, and reproduced the output of the original larger network. However, the work is limited to shallow models. The idea has been adopted in [5] as knowledge distillation to compress deep and wide networks into shallower ones, where the compressed model mimicked the function learned by the complex model. Hinton et al. [6] popularized the concept of Knowledge Distillation to be extended to more practical uses. The work in [6] proposed knowledge distillation as a more general case of C. Bucilu\u01ce et al. [4] by adopting the concept of temperature parameter at the output of teacher. The student was trained to predict the output and the classification labels. \n\nThe main idea of knowledge distillation approach is to shift knowledge from a large teacher model into a small one by learning the class distributions output via softmax [6]. It has even been observed that the student learns much faster and more reliably if trained using outputs of teacher as soft labels, instead of one-hot-encoded labels. \n\nSince then, a number of knowledge distillation methods have been proposed, each trying to capture and transfer some characteristics of the teacher such as the representation space, decision boundary or intra-data relationship. Despite its simplicity, knowledge distillation demonstrates promising results in various image classification tasks. \n\nKnowledge distillation has proven empirically to be an effective technique for training a compact model [7], [11], [17].",
            "score": 0.6843745203234013,
            "section_title": "II. KNOWLEDGE DISTILLATION",
            "char_start_offset": 2403,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 200
                },
                {
                    "start": 203,
                    "end": 340
                },
                {
                    "start": 341,
                    "end": 463
                },
                {
                    "start": 464,
                    "end": 610
                },
                {
                    "start": 611,
                    "end": 658
                },
                {
                    "start": 659,
                    "end": 852
                },
                {
                    "start": 853,
                    "end": 959
                },
                {
                    "start": 960,
                    "end": 1130
                },
                {
                    "start": 1131,
                    "end": 1207
                },
                {
                    "start": 1210,
                    "end": 1384
                },
                {
                    "start": 1385,
                    "end": 1551
                },
                {
                    "start": 1554,
                    "end": 1780
                },
                {
                    "start": 1781,
                    "end": 1897
                },
                {
                    "start": 1900,
                    "end": 2020
                }
            ],
            "ref_mentions": [
                {
                    "start": 459,
                    "end": 462,
                    "matchedPaperCorpusId": "11253972"
                },
                {
                    "start": 688,
                    "end": 691,
                    "matchedPaperCorpusId": "11536917"
                },
                {
                    "start": 867,
                    "end": 870,
                    "matchedPaperCorpusId": "7200347"
                },
                {
                    "start": 972,
                    "end": 975,
                    "matchedPaperCorpusId": "7200347"
                },
                {
                    "start": 1052,
                    "end": 1055,
                    "matchedPaperCorpusId": "11253972"
                },
                {
                    "start": 1380,
                    "end": 1383,
                    "matchedPaperCorpusId": "7200347"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.9892578125
        },
        {
            "corpus_id": "256460933",
            "title": "Calibrating Student Models for Emotion-related Tasks",
            "text": "Knowledge Distillation: Knowledge distillation (KD) is an efficient method broadly used for transferring knowledge from a teacher network to a student network. In the knowledge distillation setting, a student model is trained to obtain the knowledge of a deeper or more complex teacher model and can therefore estimate the capacity of the powerful teacher model by incorporating the extra knowledge. KD was first introduced as an approach to compress large networks into smaller networks (Ba and Caruana, 2014;Bucilu\u01ce et al., 2006) for computational efficiency. The advances of KD, however, go beyond model compression. Zhang and Sabuncu (2020) empirically explained the reason behind the enhanced performance of self-distillation and proposed a framework that employs instance-specific regularization for teacher predictions. Phuong and Lampert (2019) examined the impact of distillation on student models by analyzing linear and deep linear classifiers. Unlike previous works, we are interested in analyzing the impact of knowledge distillation on the calibration of the models. Thus, we examine the calibration of large-scale pre-trained models through knowledge distillation. We further analyze the impact of dataset shift on calibration for all these settings. We evaluate the predictive uncertainty on both in-domain and out-of-domain test sets from known and unknown distributions on emotion-related datasets. \n\nMixup: Mixup (Zhang et al., 2018) was first proposed to improve the generalization of deep neural networks in computer vision. Since then, many studies have explored mixup in natural language processing tasks (Guo et al., 2019;Guo, 2020;Chen et al., 2020;Yin et al., 2021;Kong et al., 2020;Liang et al., 2021). Liang et al. (2021) proposed a data-agnostic distillation framework that leverages mixup to confer the student model with better generalization ability. Kong et al. (2020) examined BERT calibration using mixup by generating augmented samples based on a cosine distance of extracted features.",
            "score": 0.684195705848321,
            "section_title": "Related Work",
            "char_start_offset": 7664,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 159
                },
                {
                    "start": 160,
                    "end": 399
                },
                {
                    "start": 400,
                    "end": 561
                },
                {
                    "start": 562,
                    "end": 619
                },
                {
                    "start": 620,
                    "end": 826
                },
                {
                    "start": 827,
                    "end": 955
                },
                {
                    "start": 956,
                    "end": 1080
                },
                {
                    "start": 1081,
                    "end": 1179
                },
                {
                    "start": 1180,
                    "end": 1265
                },
                {
                    "start": 1266,
                    "end": 1416
                },
                {
                    "start": 1419,
                    "end": 1545
                },
                {
                    "start": 1546,
                    "end": 1729
                },
                {
                    "start": 1730,
                    "end": 1882
                },
                {
                    "start": 1883,
                    "end": 2021
                }
            ],
            "ref_mentions": [
                {
                    "start": 488,
                    "end": 510,
                    "matchedPaperCorpusId": "11536917"
                },
                {
                    "start": 510,
                    "end": 531,
                    "matchedPaperCorpusId": "11253972"
                },
                {
                    "start": 1432,
                    "end": 1452,
                    "matchedPaperCorpusId": "3162051"
                },
                {
                    "start": 1646,
                    "end": 1656,
                    "matchedPaperCorpusId": "212928508"
                },
                {
                    "start": 1656,
                    "end": 1674,
                    "matchedPaperCorpusId": "216553182"
                },
                {
                    "start": 1674,
                    "end": 1691,
                    "matchedPaperCorpusId": "236477688"
                },
                {
                    "start": 1709,
                    "end": 1728,
                    "matchedPaperCorpusId": "226226888"
                },
                {
                    "start": 1730,
                    "end": 1749,
                    "matchedPaperCorpusId": "226226888"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.95556640625
        },
        {
            "corpus_id": "251180994",
            "title": "Language Bias-Driven Self-Knowledge Distillation with Generalization Uncertainty for Reducing Language Bias in Visual Question Answering",
            "text": "In recent years, knowledge distillation [45][46][47][48] has been widely used in deep learning to transfer knowledge between different models. Hinton et al. [13] used knowledge distillation for model compression; that is, moving knowledge from powerful but complex models (teacher models) to simple models (student models). By minimizing the Kullback-Leibler (KL) divergence loss of the categorical output probability, the student can imitate the output of the teacher model. In addition, some new knowledge transfer goals have been proposed, such as intermediate feature maps [49], attention maps [50], second-order statistics [46], contrastive features [51,52] or structured knowledge [53][54][55]. \n\nHowever, these methods require a distinction between the roles of the teacher and the student and are typically distilled offline. Online knowledge distillation is a knowledge distillation based on a series of student (generally two) models by eliminating cumbersome teacher models. Based on the Kullback-Leibler divergence, Zhang et al. [16] proposed a technique for deep mutual learning (DML) in which pair-wise students learn from each other using a mimicry loss. By adding distillation loss after updating enough steps, codistillation [15] (similar to DML) enables student networks to sustain their diversity for a longer time. However, KL divergence alone cannot capture the learning degree of student models. The authors of this paper put forward the notion of generalization uncertainty as a way for the model to learn unbiased knowledge.",
            "score": 0.6835457951276005,
            "section_title": "Knowledge Distillation",
            "char_start_offset": 7758,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 142
                },
                {
                    "start": 143,
                    "end": 323
                },
                {
                    "start": 324,
                    "end": 475
                },
                {
                    "start": 476,
                    "end": 700
                },
                {
                    "start": 703,
                    "end": 833
                },
                {
                    "start": 834,
                    "end": 985
                },
                {
                    "start": 986,
                    "end": 1169
                },
                {
                    "start": 1170,
                    "end": 1334
                },
                {
                    "start": 1335,
                    "end": 1417
                },
                {
                    "start": 1418,
                    "end": 1548
                }
            ],
            "ref_mentions": [
                {
                    "start": 40,
                    "end": 44,
                    "matchedPaperCorpusId": "213331938"
                },
                {
                    "start": 44,
                    "end": 48,
                    "matchedPaperCorpusId": "206596723"
                },
                {
                    "start": 48,
                    "end": 52,
                    "matchedPaperCorpusId": "236927674"
                },
                {
                    "start": 52,
                    "end": 56,
                    "matchedPaperCorpusId": "250461062"
                },
                {
                    "start": 157,
                    "end": 161,
                    "matchedPaperCorpusId": "7200347"
                },
                {
                    "start": 598,
                    "end": 602,
                    "matchedPaperCorpusId": "829159"
                },
                {
                    "start": 628,
                    "end": 632,
                    "matchedPaperCorpusId": "206596723"
                },
                {
                    "start": 655,
                    "end": 659,
                    "matchedPaperCorpusId": "204838340"
                },
                {
                    "start": 659,
                    "end": 662,
                    "matchedPaperCorpusId": "219636179"
                },
                {
                    "start": 687,
                    "end": 691,
                    "matchedPaperCorpusId": "131765296"
                },
                {
                    "start": 691,
                    "end": 695,
                    "matchedPaperCorpusId": "198185886"
                },
                {
                    "start": 695,
                    "end": 699,
                    "matchedPaperCorpusId": "218487294"
                },
                {
                    "start": 1041,
                    "end": 1045,
                    "matchedPaperCorpusId": "26071966"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.94921875
        },
        {
            "corpus_id": "219559263",
            "title": "Knowledge Distillation: A Survey",
            "text": "Different teacher architectures can provide their own useful knowledge for a student network. The multiple teacher networks can be individually and integrally used for distillation during the period of training a student network. In a typical teacher-student framework, the teacher usually has a large model or an ensemble of large models. To transfer knowledge from multiple teachers, the simplest way is to use the averaged response from all teachers as the supervision signal (Hinton et al. 2015) Yuan et al. 2021). A generic framework for multi-teacher distillation is shown in Fig. 11. Multiple teacher networks have turned out to be effective for training student model usually using logits and feature representation as the knowledge. In addition to the averaged logits from all teachers, You et al. (2017) further incorporated features from the intermediate layers in order to encourage the dissimilarity among different training samples. To utilize both logits and intermediate features, Chen et al. (2019b) used two teacher networks, in which one teacher transfers response-based knowledge to the student and the other teacher transfers feature-based knowledge to the student. Fukuda et al. (2017) randomly selected one teacher from the pool of teacher networks at each iteration. To transfer featurebased knowledge from multiple teachers, additional teacher branches are added to the student networks to mimic the intermediate features of teachers (Park and Kwak 2020;Asif et al. 2020). Born again networks address multiple teachers in a step-by-step manner, i.e., the student at the t step is used as the teacher of the student at the t + 1 step (Furlanello et al. 2018),  and similar ideas can be found in Yang et al. (2019a). To efficiently perform knowledge transfer and explore the power of multiple teachers, several alternative methods have been proposed to simulate multiple teachers by adding different types of noise to a given teacher (Sau and Balasubramanian 2016) or by using stochastic blocks and skip connections (Lee et al. 2019c). Using multiple teacher models with feature ensembles, knowledge amalgamation is designed in (Shen et al. 2019a;Luo et al. 2019;Shen et al. 2019b;Luo et al. 2020).",
            "score": 0.6832599740372125,
            "section_title": "Multi-teacher Distillation",
            "char_start_offset": 38164,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 93
                },
                {
                    "start": 94,
                    "end": 229
                },
                {
                    "start": 230,
                    "end": 339
                },
                {
                    "start": 340,
                    "end": 518
                },
                {
                    "start": 519,
                    "end": 590
                },
                {
                    "start": 591,
                    "end": 741
                },
                {
                    "start": 742,
                    "end": 946
                },
                {
                    "start": 947,
                    "end": 1186
                },
                {
                    "start": 1187,
                    "end": 1290
                },
                {
                    "start": 1291,
                    "end": 1497
                },
                {
                    "start": 1498,
                    "end": 1739
                },
                {
                    "start": 1740,
                    "end": 2058
                },
                {
                    "start": 2059,
                    "end": 2221
                }
            ],
            "ref_mentions": [
                {
                    "start": 500,
                    "end": 517,
                    "matchedPaperCorpusId": "228376532"
                },
                {
                    "start": 796,
                    "end": 813,
                    "matchedPaperCorpusId": "26021416"
                },
                {
                    "start": 1459,
                    "end": 1479,
                    "matchedPaperCorpusId": "220378802"
                },
                {
                    "start": 1479,
                    "end": 1496,
                    "matchedPaperCorpusId": "202660953"
                },
                {
                    "start": 1719,
                    "end": 1738,
                    "matchedPaperCorpusId": "21668571"
                },
                {
                    "start": 2039,
                    "end": 2057,
                    "matchedPaperCorpusId": "210952293"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.771484375
        },
        {
            "corpus_id": "273186993",
            "title": "Gap Preserving Distillation by Building Bidirectional Mappings with A Dynamic Teacher",
            "text": "Knowledge distillation. Knowledge distillation (KD) (Hinton et al., 2015) transfers knowledge from a teacher to a smaller student model. Methods improve this by focusing on logits or intermediate features (Sun et al., 2024;Jin et al., 2023;Zhao et al., 2022a;Li et al., 2023;Passalis et al., 2021;Tian et al., 2019;Zagoruyko & Komodakis, 2017a;Heo et al., 2019a;Chen et al., 2021b;Heo et al., 2019b;Kim et al., 2018). Standard methods prioritize fully converged teachers with high performance, yet the performance gap can hinder knowledge transfer (Wang et al., 2022;Gao et al., 2020;Cho & Hariharan, 2019;Yuan et al., 2019). Strategies to address this include using intermediatestage teachers (Cho & Hariharan, 2019;Zhao et al., 2022b), pre-training student-friendly teacher model (Yang et al., 2019a;Park et al., 2021;Dong et al., 2024), introducing intermediate-sized assistant teachers (Mirzadeh et al., 2020;Son et al., 2021) or introducing auxiliary networks (Gao et al., 2021). These methods often rely on specially designed and pre-trained intermediate models. Feature-based methods like DTSKD (Li et al., 2024) and DiffKD (Huang et al., 2023) focus on bridging semantic gaps or denoising features. SCKD (Zhu & Wang, 2021) optimizes transfer using gradient similarity. Recent works refine soft labels (Yuan et al., 2024;Rao et al., 2023) or student's output entropy (Zhu et al., 2024a) to enhance knowledge transfer. In contrast, our GPD constructs a trainable dynamic teacher based on the student model, maintaining an appropriate accuracy gap throughout distillation for effective knowledge transfer. Reparameterization.",
            "score": 0.6832108025739984,
            "section_title": "RELATED WORK",
            "char_start_offset": 6141,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 23
                },
                {
                    "start": 24,
                    "end": 136
                },
                {
                    "start": 137,
                    "end": 417
                },
                {
                    "start": 418,
                    "end": 625
                },
                {
                    "start": 626,
                    "end": 984
                },
                {
                    "start": 985,
                    "end": 1068
                },
                {
                    "start": 1069,
                    "end": 1206
                },
                {
                    "start": 1207,
                    "end": 1276
                },
                {
                    "start": 1277,
                    "end": 1424
                },
                {
                    "start": 1425,
                    "end": 1610
                },
                {
                    "start": 1611,
                    "end": 1630
                }
            ],
            "ref_mentions": [
                {
                    "start": 205,
                    "end": 223,
                    "matchedPaperCorpusId": "268247468"
                },
                {
                    "start": 223,
                    "end": 240,
                    "matchedPaperCorpusId": "260933721"
                },
                {
                    "start": 240,
                    "end": 259,
                    "matchedPaperCorpusId": "247476179"
                },
                {
                    "start": 259,
                    "end": 275,
                    "matchedPaperCorpusId": "254069919"
                },
                {
                    "start": 275,
                    "end": 297,
                    "matchedPaperCorpusId": "219169868"
                },
                {
                    "start": 315,
                    "end": 344,
                    "matchedPaperCorpusId": "829159"
                },
                {
                    "start": 344,
                    "end": 362,
                    "matchedPaperCorpusId": "258309453"
                },
                {
                    "start": 362,
                    "end": 381,
                    "matchedPaperCorpusId": "233296935"
                },
                {
                    "start": 381,
                    "end": 399,
                    "matchedPaperCorpusId": "53213211"
                },
                {
                    "start": 399,
                    "end": 416,
                    "matchedPaperCorpusId": "3608236"
                },
                {
                    "start": 548,
                    "end": 567,
                    "matchedPaperCorpusId": "252846591"
                },
                {
                    "start": 584,
                    "end": 606,
                    "matchedPaperCorpusId": "203642130"
                },
                {
                    "start": 694,
                    "end": 717,
                    "matchedPaperCorpusId": "203642130"
                },
                {
                    "start": 717,
                    "end": 736,
                    "matchedPaperCorpusId": "198179767"
                },
                {
                    "start": 782,
                    "end": 802,
                    "matchedPaperCorpusId": "54986302"
                },
                {
                    "start": 802,
                    "end": 820,
                    "matchedPaperCorpusId": "231925118"
                },
                {
                    "start": 820,
                    "end": 838,
                    "matchedPaperCorpusId": "249642077"
                },
                {
                    "start": 890,
                    "end": 913,
                    "matchedPaperCorpusId": "212908749"
                },
                {
                    "start": 913,
                    "end": 930,
                    "matchedPaperCorpusId": "221802641"
                },
                {
                    "start": 965,
                    "end": 983,
                    "matchedPaperCorpusId": "229400079"
                },
                {
                    "start": 1102,
                    "end": 1119,
                    "matchedPaperCorpusId": "268439162"
                },
                {
                    "start": 1131,
                    "end": 1151,
                    "matchedPaperCorpusId": "258888057"
                },
                {
                    "start": 1212,
                    "end": 1230,
                    "matchedPaperCorpusId": "244680427"
                },
                {
                    "start": 1328,
                    "end": 1345,
                    "matchedPaperCorpusId": "249209818"
                },
                {
                    "start": 1374,
                    "end": 1393,
                    "matchedPaperCorpusId": "258564799"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.806640625
        },
        {
            "corpus_id": "251460030",
            "title": "Improving Deep Mutual Learning via Knowledge Distillation",
            "text": "Knowledge distillation in [8] is one of the most popular knowledge transfer methods today, and it uses a teacher-student framework. The basic idea of this method is that a pretrained teacher network (i.e., a cumbersome network or the biggest network) using certain hyperparameters are then used to train an untrained student network (i.e., small network) for the purpose of transferring knowledge. This process uses a distillation knowledge equation where a temperature (T) is involved and can be varied to obtain a soft probability output from a class C image which can be calculated as: \n\nSuppose the teacher network is marked as G t and the student network is G s , then the distillation loss can be defined as: \n\nAs a result, the student loss function contained in Figure 2 is minimized during the training process based on ( 6) and (7) as: \n\nwhere \u03bb is a balancing value between the two losses. The main purpose of the teacherstudent framework is to force the student output probability to imitate or match the pre-trained teacher network's probability output.",
            "score": 0.6832042794244063,
            "section_title": "DML and KD",
            "char_start_offset": 10613,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 131
                },
                {
                    "start": 132,
                    "end": 397
                },
                {
                    "start": 398,
                    "end": 588
                },
                {
                    "start": 591,
                    "end": 714
                },
                {
                    "start": 717,
                    "end": 844
                },
                {
                    "start": 847,
                    "end": 899
                },
                {
                    "start": 900,
                    "end": 1065
                }
            ],
            "ref_mentions": [
                {
                    "start": 837,
                    "end": 840,
                    "matchedPaperCorpusId": "206594692"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.927734375
        },
        {
            "corpus_id": "268060109",
            "title": "Advanced hybrid LSTM-transformer architecture for real-time multi-task prediction in engineering systems",
            "text": "Knowledge distillation is a technique where a compact model (student) is trained to mimic the behavior of a larger, more complex model (teacher). This allows the student model to inherit the teacher's capabilities without incurring the computational overhead. The schematic can be referred to Fig. 2: \n\nRationale behind distillation Concept. In many engineering scenarios, deploying gargantuan models is infeasible due to resource constraints. However, these large models often possess superior performance. Knowledge distillation bridges this gap, enabling smaller models to emulate the performance of their larger counterparts.",
            "score": 0.682104614627752,
            "section_title": "Knowledge distillation",
            "char_start_offset": 21974,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 145
                },
                {
                    "start": 146,
                    "end": 259
                },
                {
                    "start": 260,
                    "end": 300
                },
                {
                    "start": 303,
                    "end": 341
                },
                {
                    "start": 342,
                    "end": 443
                },
                {
                    "start": 444,
                    "end": 507
                },
                {
                    "start": 508,
                    "end": 629
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.94140625
        },
        {
            "corpus_id": "251719675",
            "title": "Multi-Granularity Distillation Scheme Towards Lightweight Semi-Supervised Semantic Segmentation",
            "text": "Knowledge Distillation [13,16] is a knowledge transfer technique that optimizes a lightweight student model with effective information transfer and supervision of a larger teacher model or ensembles. Besides the knowledge transfer in the outputs, the feature maps [28,35] in the intermediate layers of networks are used to improve the performance of the student networks. Moreover, some methods attempt to transfer the attention concepts [47][48][49] of feature maps from each channel in intermediate layers. Some approaches try to exploit the knowledge distillation on the dense prediction tasks [27,38,46,50]. In addition, [54] indicates that multiple teacher networks can provide more effective information for training a lightweight and ascendant student network. In this paper, we design a deeper and a wider model to play as the complementary teachers and provide multi-granularity auxiliary supervision.",
            "score": 0.6820256980295517,
            "section_title": "Knowledge Distillation",
            "char_start_offset": 7740,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 199
                },
                {
                    "start": 200,
                    "end": 371
                },
                {
                    "start": 372,
                    "end": 508
                },
                {
                    "start": 509,
                    "end": 611
                },
                {
                    "start": 612,
                    "end": 767
                },
                {
                    "start": 768,
                    "end": 910
                }
            ],
            "ref_mentions": [
                {
                    "start": 23,
                    "end": 27,
                    "matchedPaperCorpusId": "219559263"
                },
                {
                    "start": 264,
                    "end": 268,
                    "matchedPaperCorpusId": "208109903"
                },
                {
                    "start": 442,
                    "end": 446,
                    "matchedPaperCorpusId": "201809759"
                },
                {
                    "start": 597,
                    "end": 601,
                    "matchedPaperCorpusId": "73729180"
                },
                {
                    "start": 601,
                    "end": 604,
                    "matchedPaperCorpusId": "236882796"
                },
                {
                    "start": 604,
                    "end": 607,
                    "matchedPaperCorpusId": "226292143"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.85205078125
        },
        {
            "corpus_id": "253121300",
            "title": "KD-MVS: Knowledge Distillation Based Self-supervised Learning for Multi-view Stereo",
            "text": "Knowledge distillation [13] aims to transfer knowledge from a teacher model to a student model, so that a powerful and lightweight student model can be obtained. [25,35,29,34,26] consider knowledge at feature space and transfer it to the student model's feature space. Born-Again Networks (BAN) [8] trains a student model similarly parameterized as the teacher model and makes the trained student be a teacher model in a new round. The self-training scheme [37] generates distillation labels for unlabeled data and trains the student model with these labels. Probabilistic knowledge transfer (PKT) [28,27] trains the student model via matching the probability distribution of the teacher model. Since labeled data are not required to minimize the difference of probability distribution, PKT can also be applied to unsupervised learning. In this work, we are inspired by PKT and offline distillation [30,47,15,24,20] and propose to transfer the response-based knowledge [10] by forcing the predicted probability distribution of the student model to be similar to the probability distribution of the teacher model in an offline manner.",
            "score": 0.6816439309622613,
            "section_title": "Knowledge Distillation",
            "char_start_offset": 5194,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 161
                },
                {
                    "start": 162,
                    "end": 268
                },
                {
                    "start": 269,
                    "end": 431
                },
                {
                    "start": 432,
                    "end": 558
                },
                {
                    "start": 559,
                    "end": 694
                },
                {
                    "start": 695,
                    "end": 836
                },
                {
                    "start": 837,
                    "end": 1133
                }
            ],
            "ref_mentions": [
                {
                    "start": 162,
                    "end": 166,
                    "matchedPaperCorpusId": "131765296"
                },
                {
                    "start": 166,
                    "end": 169,
                    "matchedPaperCorpusId": "198179476"
                },
                {
                    "start": 169,
                    "end": 172,
                    "matchedPaperCorpusId": "102483463"
                },
                {
                    "start": 175,
                    "end": 178,
                    "matchedPaperCorpusId": "52012952"
                },
                {
                    "start": 295,
                    "end": 298,
                    "matchedPaperCorpusId": "4110009"
                },
                {
                    "start": 457,
                    "end": 461,
                    "matchedPaperCorpusId": "207853355"
                },
                {
                    "start": 598,
                    "end": 602,
                    "matchedPaperCorpusId": "219169868"
                },
                {
                    "start": 602,
                    "end": 605,
                    "matchedPaperCorpusId": "52012952"
                },
                {
                    "start": 909,
                    "end": 912,
                    "matchedPaperCorpusId": "212908749"
                },
                {
                    "start": 969,
                    "end": 973,
                    "matchedPaperCorpusId": "219559263"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.73974609375
        },
        {
            "corpus_id": "263828861",
            "title": "OpenIncrement: A Unified Framework for Open Set Recognition and Deep Class-Incremental Learning",
            "text": "In the context of deep learning, knowledge distillation (KD) was first known in [16] and [33] that enables to transfer the knowledge learned by a larger model (teacher) to a smaller one (student). There are three types of knowledge that can be transferred, namely response-based knowledge, feature-based knowledge, and relation-based knowledge. \n\nResponse-based knowledge distillation is to directly align the predictions between students and teachers. The divergence between the logit layers of teachers and students is encouraged to be minimized. In [16], the logit layer of the teacher network is adapted as soft targets when computing the softmax loss so that the student can be expected to give the same outputs as the teacher. \n\nIn feature-based knowledge distillation, not only the last logit layer but also the intermediate layers of the teachers are utilized to transfer knowledge. The students can therefore output similar features as the teachers. In [33], the neural activations of the first layers in the teacher model are directly used to match the student model. Kim et. al. proposed to extract the factor maps of teachers' and students' layers using convolutional modules and let the student factor mimic the teacher factor [22]. Similarly, Passban et. al. proposed a combinatorial technique that can merge the features of multiple layers in the teacher model using the atten-tion mechanism [29]. \n\nUnlike the previous two categories that align the layer outputs between the teachers and students, relation-based knowledge distillation tends to preserve the relationships among the features in different layers or instances [28][39] [38]. Park et. al. proposed the relational knowledge distillation that transfers the instance relations between teachers and students [28]. Tung et. al. proposed the similarity-preserving knowledge distillation, in which the similarities between the instance features can be transferred to the students [39].",
            "score": 0.6813022769905115,
            "section_title": "Knowledge Distillation",
            "char_start_offset": 6993,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 196
                },
                {
                    "start": 197,
                    "end": 344
                },
                {
                    "start": 347,
                    "end": 452
                },
                {
                    "start": 453,
                    "end": 548
                },
                {
                    "start": 549,
                    "end": 732
                },
                {
                    "start": 735,
                    "end": 890
                },
                {
                    "start": 891,
                    "end": 958
                },
                {
                    "start": 959,
                    "end": 1077
                },
                {
                    "start": 1078,
                    "end": 1085
                },
                {
                    "start": 1086,
                    "end": 1245
                },
                {
                    "start": 1246,
                    "end": 1268
                },
                {
                    "start": 1269,
                    "end": 1412
                },
                {
                    "start": 1415,
                    "end": 1654
                },
                {
                    "start": 1655,
                    "end": 1663
                },
                {
                    "start": 1664,
                    "end": 1788
                },
                {
                    "start": 1789,
                    "end": 1797
                },
                {
                    "start": 1798,
                    "end": 1957
                }
            ],
            "ref_mentions": [
                {
                    "start": 1640,
                    "end": 1644,
                    "matchedPaperCorpusId": "131765296"
                },
                {
                    "start": 1783,
                    "end": 1787,
                    "matchedPaperCorpusId": "131765296"
                },
                {
                    "start": 1952,
                    "end": 1956,
                    "matchedPaperCorpusId": "198179476"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.96044921875
        },
        {
            "corpus_id": "270702669",
            "title": "Continual Learning with Diffusion-based Generative Replay for Industrial Streaming Data",
            "text": "Knowledge distillation facilitates knowledge transfer from a cumbersome model to a smaller one.Offline distillation methods typically involve training a large and complex teacher model to perform well on a specific task, and training a simpler student model to closely imitate the output of the teacher model on the same task [11].Online distillation methods simultaneously update the teacher and the student and employ end-to-end training [12].Self-distillation methods distill knowledge internally, where the student and teacher originate from the same networks [13].Our DSG is categorized as an offline distillation method.",
            "score": 0.6804575205542568,
            "section_title": "C. Knowledge Distillation",
            "char_start_offset": 6435,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 95
                },
                {
                    "start": 95,
                    "end": 331
                },
                {
                    "start": 331,
                    "end": 445
                },
                {
                    "start": 445,
                    "end": 569
                },
                {
                    "start": 569,
                    "end": 626
                }
            ],
            "ref_mentions": [
                {
                    "start": 440,
                    "end": 444,
                    "matchedPaperCorpusId": "26071966"
                },
                {
                    "start": 564,
                    "end": 568,
                    "matchedPaperCorpusId": "159041406"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.87939453125
        },
        {
            "corpus_id": "236986986",
            "title": "A Survey on Vision Transformer",
            "text": "Knowledge distillation aims to train student networks by transferring knowledge from large teacher networks [207], [208], [209]. Compared with teacher networks, student networks usually have thinner and shallower architectures, which are easier to be deployed on resource-limited resources. Both the output and intermediate features of neural networks can also be used to transfer effective information from teachers to students. Focused on transformer models, Mukherjee et al. [210] used the pre-trained BERT [10] as a teacher to guide the training of small models, leveraging large amounts of unlabeled data. Wang et al. [211] train the student networks to mimic the output of self-attention layers in the pre-trained teacher models. The dot-product between values is introduced as a new form of knowledge for guiding students. A teacher's assistant [212] is also introduced in [211], reducing the gap between large pre-trained transformer models and compact  [196] design Q-BERT [197] Quantization 12 --Q8BERT [198] 12 TinyBERT [ [46] design different objective functions to transfer knowledge from teachers to students. For example, the outputs of student models' embedding layers imitate those of teachers via MSE losses. For the vision transformer, Jia et al. [213] proposed a finegrained manifold distillation method, which excavates effective knowledge through the relationship between images and the divided patches.",
            "score": 0.680014107596042,
            "section_title": "Knowledge Distillation",
            "char_start_offset": 78352,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 128
                },
                {
                    "start": 129,
                    "end": 290
                },
                {
                    "start": 291,
                    "end": 429
                },
                {
                    "start": 430,
                    "end": 610
                },
                {
                    "start": 611,
                    "end": 735
                },
                {
                    "start": 736,
                    "end": 829
                },
                {
                    "start": 830,
                    "end": 1123
                },
                {
                    "start": 1124,
                    "end": 1226
                },
                {
                    "start": 1227,
                    "end": 1425
                }
            ],
            "ref_mentions": [
                {
                    "start": 115,
                    "end": 120,
                    "matchedPaperCorpusId": "11253972"
                },
                {
                    "start": 478,
                    "end": 483,
                    "matchedPaperCorpusId": "218502458"
                },
                {
                    "start": 510,
                    "end": 514,
                    "matchedPaperCorpusId": "226096901"
                },
                {
                    "start": 852,
                    "end": 857,
                    "matchedPaperCorpusId": "212908749"
                },
                {
                    "start": 962,
                    "end": 967,
                    "matchedPaperCorpusId": "211066200"
                },
                {
                    "start": 982,
                    "end": 987,
                    "matchedPaperCorpusId": "202565587"
                },
                {
                    "start": 1033,
                    "end": 1037,
                    "matchedPaperCorpusId": "202719327"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.5947265625
        },
        {
            "corpus_id": "208526905",
            "title": "Online Knowledge Distillation with Diverse Peers",
            "text": "Knowledge Distillation. KD provides a succinct but effective solution for compressing a pre-trained large teacher model into a smaller student model by steering the student predictions towards teacher predictions (Bucilua, Caruana, and Niculescu-Mizil 2006;Ba and Caruana 2014;Hinton, Vinyals, and Dean 2015;Polino, Pascanu, and Alistarh 2018). Compared to hard ground-truth labels, fine-grained class information in soft predictions helps the small model to reach flatter local minima, which results in more robust performance and improves generalization ability (Pereyra et al. 2017;Keskar et al. 2017). Several recent works attempt to further improve the performance with new formulations of teacher-learned knowledge (Yim et al. 2017;Chen, Zhang, and Dong 2018;Ahn et al. 2019).\n\nOnline Knowledge Distillation. Instead of two-stage knowledge transfer, recent work focus on more economic online knowledge distillation without a pre-trained teacher model. Simultaneously training a group of student models by learning from peers' predictions is an effective substitute for teacher-absent knowledge distillation. Some approaches use individual networks with each one corresponding to a student model Anil et al. 2018), while some others ask all student models to share the same early blocks to further reduce the training cost (Song and Chai 2018; Lan, Zhu, and Gong 2018). The main difference in these approaches is the way that each student model learns from others. In ), each student model learns from the simple average of predictions of all other group members and requires complex asynchronous updating among different networks. A similar variants called codistillation investigated the potential benefits for online distillation in the distributed learning (Anil et al. 2018). In (Lan, Zhu, and Gong 2018), all student models share the same target distribution by averaging predictions of all the members with weights learned by a fully connected layer. Unfortunately, simply treating each peer to be equally important or forcing all the members to learn from the same targets would hurt the diversity among students, which limits the effectiveness of within-group knowledge transfer.\n\nSelf-Attention. Attention was introduced in natural language processing for encoding each word with others which are most",
            "score": 0.6799805151247492,
            "section_title": "Related Work",
            "char_start_offset": 4075,
            "sentence_offsets": [],
            "ref_mentions": [
                {
                    "start": 213,
                    "end": 257,
                    "matchedPaperCorpusId": "11253972"
                },
                {
                    "start": 257,
                    "end": 277,
                    "matchedPaperCorpusId": "11536917"
                },
                {
                    "start": 308,
                    "end": 343,
                    "matchedPaperCorpusId": "3323727"
                },
                {
                    "start": 585,
                    "end": 604,
                    "matchedPaperCorpusId": "5834589"
                },
                {
                    "start": 721,
                    "end": 738,
                    "matchedPaperCorpusId": "206596723"
                },
                {
                    "start": 738,
                    "end": 765,
                    "matchedPaperCorpusId": "49542008"
                },
                {
                    "start": 765,
                    "end": 781,
                    "matchedPaperCorpusId": "118649278"
                },
                {
                    "start": 1201,
                    "end": 1218,
                    "matchedPaperCorpusId": "2331610"
                },
                {
                    "start": 1766,
                    "end": 1784,
                    "matchedPaperCorpusId": "2331610"
                },
                {
                    "start": 1789,
                    "end": 1814,
                    "matchedPaperCorpusId": "48352434"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.91796875
        },
        {
            "corpus_id": "271956980",
            "title": "Bring the Power of Diffusion Model to Defect Detection",
            "text": "Large-scale deep models have achieved remarkable success, but their computational complexity and massive storage requirements make real-time deployment a challenge, especially on resource-constrained devices such as video surveillance and self-driving cars. \n\nKnowledge distillation is a classical approach of model compression and acceleration that effectively learns small student models from large teacher models [18]. In knowledge distillation, small student models are usually supervised with the assistance of large teacher models, allowing the student models to mimic the teacher models for a competitive or even superior performance. Knowledge distillation is similar to the way humans learn, and the key issue in distillation is how to better transfer knowledge from large teacher models to small student models. Inspired by this, recent methods to knowledge distillation have been extended to teacher-student learning [19], mutual learning [20], self-learning [21] and so on. Most of the extensions to knowledge distillation focus on compressing deep neural networks. The generated lightweight student networks can be easily deployed in applications such as visual recognition, speech recognition and natural language processing (NLP). \n\nIn addition, the gap between the capacity of the teacher model and the student model affects the effectiveness of knowledge distillation, so the capacity of the teacher model needs to be controlled [22]. We propose to build powerful teacher model without changing the capacity (width and depth) of the student model. This approach allows for optimal performance transfer between teacher and student.",
            "score": 0.6790326430542091,
            "section_title": "B. Knowledge Distillation",
            "char_start_offset": 7189,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 257
                },
                {
                    "start": 260,
                    "end": 421
                },
                {
                    "start": 422,
                    "end": 641
                },
                {
                    "start": 642,
                    "end": 821
                },
                {
                    "start": 822,
                    "end": 985
                },
                {
                    "start": 986,
                    "end": 1077
                },
                {
                    "start": 1078,
                    "end": 1245
                },
                {
                    "start": 1248,
                    "end": 1451
                },
                {
                    "start": 1452,
                    "end": 1564
                },
                {
                    "start": 1565,
                    "end": 1647
                }
            ],
            "ref_mentions": [
                {
                    "start": 416,
                    "end": 420,
                    "matchedPaperCorpusId": "7200347"
                },
                {
                    "start": 928,
                    "end": 932,
                    "matchedPaperCorpusId": "259697005"
                },
                {
                    "start": 950,
                    "end": 954,
                    "matchedPaperCorpusId": "249145972"
                },
                {
                    "start": 970,
                    "end": 974,
                    "matchedPaperCorpusId": "251196646"
                },
                {
                    "start": 1446,
                    "end": 1450,
                    "matchedPaperCorpusId": "212908749"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.9619140625
        },
        {
            "corpus_id": "208106643",
            "title": "Acceleration of Target Detection Based on Forced Knowledge Distillation",
            "text": "Knowledge distillation refers to making small models fit large models by transferring large amounts of data, so that small models can learn function mapping similar to large models. Knowledge distillation was first proposed by Hinton in 2015. And then Romero proposed FitNets [6]. By adding loss into the middle layer, the network is divided into two parts while training, so that the output of the middle layer of the teacher network and the student network is as close as possible. Then the FSP method proposed by Yim et al. [11] transfers the relativity between feature maps as knowledge to student networks. Zagoruyko et al. [12] transfers the attention of feature maps as knowledge to student networks.",
            "score": 0.6763552863328411,
            "section_title": "Knowledge Distillation",
            "char_start_offset": 2440,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 181
                },
                {
                    "start": 182,
                    "end": 242
                },
                {
                    "start": 243,
                    "end": 280
                },
                {
                    "start": 281,
                    "end": 483
                },
                {
                    "start": 484,
                    "end": 611
                },
                {
                    "start": 612,
                    "end": 707
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.94140625
        },
        {
            "corpus_id": "251741056",
            "title": "Design Automation for Fast, Lightweight, and Effective Deep Learning Models: A Survey",
            "text": "Knowledge distillation (KD) is extended from knowledge transfer (KT) [143] by Ba and Caruana [144] to compress a cumbersome network (teacher) into a smaller and simpler network (student). This is done by making the student model mimic the function learned by the teacher model in order to achieve a competitive accuracy. It is later formally popularized by Hinton et al. [21] as a student-teacher paradigm, where the knowledge is transferred from the teacher to the student by minimizing the difference between the logits (features before the final softmax) of the teacher and student. In many situations, the performance of the teacher is almost perfect with a very high classification probability for the correct class and flat probabilities for the other classes. Therefore, the teacher is not able to provide much more information than the ground truth labels. Hinton et al. [21] introduce the concept of softmax temperature to transfer knowledge, which can better deliver the information of which classes the teacher find similar to the correct class. Formally, given the logits of the teacher model, the classification probability p i of the class i is: \n\nwhere \u03c4 is the temperature parameter. It controls how soft the labels from the teacher are. The soft labels together with the ground truth labels are used to supervise a compact student model. \n\nVanilla knowledge distillation mostly focuses on transferring knowledge to a student model with a fixed small architecture, which is manually designed in advance. However, different teachers and tasks favour different student architectures, and hand-crafted architectures are prone to be suboptimal. Considering these limitations, there is a growing trend to automate the architecture design of a student model [145], [146], [147], [148], [149], [150], [151]. The ground-truth labels are combined with the distillation labels to guide the automatic design process. AKDNet [145] proposes to search optimal student architectures for distilling a given teacher by RL-based NAS. It adopts the efficient search space of [48] and designs a KD-guided reward with a teacher network.",
            "score": 0.676131956098998,
            "section_title": "B. Automated Knowledge Distillation",
            "char_start_offset": 84556,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 187
                },
                {
                    "start": 188,
                    "end": 320
                },
                {
                    "start": 321,
                    "end": 585
                },
                {
                    "start": 586,
                    "end": 766
                },
                {
                    "start": 767,
                    "end": 864
                },
                {
                    "start": 865,
                    "end": 1056
                },
                {
                    "start": 1057,
                    "end": 1159
                },
                {
                    "start": 1162,
                    "end": 1199
                },
                {
                    "start": 1200,
                    "end": 1253
                },
                {
                    "start": 1254,
                    "end": 1354
                },
                {
                    "start": 1357,
                    "end": 1519
                },
                {
                    "start": 1520,
                    "end": 1656
                },
                {
                    "start": 1657,
                    "end": 1816
                },
                {
                    "start": 1817,
                    "end": 1921
                },
                {
                    "start": 1922,
                    "end": 2031
                },
                {
                    "start": 2032,
                    "end": 2131
                }
            ],
            "ref_mentions": [
                {
                    "start": 69,
                    "end": 74,
                    "matchedPaperCorpusId": "11253972"
                },
                {
                    "start": 93,
                    "end": 98,
                    "matchedPaperCorpusId": "11536917"
                },
                {
                    "start": 371,
                    "end": 375,
                    "matchedPaperCorpusId": "7200347"
                },
                {
                    "start": 879,
                    "end": 883,
                    "matchedPaperCorpusId": "7200347"
                },
                {
                    "start": 1768,
                    "end": 1773,
                    "matchedPaperCorpusId": "208175624"
                },
                {
                    "start": 1782,
                    "end": 1787,
                    "matchedPaperCorpusId": "235428017"
                },
                {
                    "start": 1810,
                    "end": 1815,
                    "matchedPaperCorpusId": "233877900"
                },
                {
                    "start": 1929,
                    "end": 1934,
                    "matchedPaperCorpusId": "208175624"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.90771484375
        },
        {
            "corpus_id": "234032397",
            "title": "Robust CNN Compression Framework for Security-Sensitive Embedded Systems",
            "text": "The main idea of the knowledge distillation [21] is to transfer the knowledge of a trained teacher network to a student network by training the student network using the input and the SoftMax output of the teacher. In the early stage, it is usually applied for model compression and achieved by transferring the knowledge of an over-parameterized teacher model to a smaller student model. Bucila et al. [25] primarily used this strategy with unlabeled synthesized data to transfer the knowledge of a large ensemble teacher. Hinton et al. [21] formally defined the knowledge distillation loss with temperature and showed that distillation is effective for transferring knowledge with the original training dataset. \n\nDistillation also can be used as a defense to adversarial examples. The defensive distillation [22] achieves adversarial robustness by applying distillation on student and teacher models which have the same structure. However, it has been shown that the defense can be easily broken [4]. \n\nMany methods have been proposed to improve the effectiveness of distillation. Distillation with boundary support samples [26] tries to improve the generalization performance of a student model by conducting the distillation with the adversarial examples near the decision boundary. Distillation with teacher assistant [27] fills the gap between student and teacher models by using intermediate models called teacher assistants.",
            "score": 0.6759336605944212,
            "section_title": "Knowledge Distillation",
            "char_start_offset": 8543,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 214
                },
                {
                    "start": 215,
                    "end": 388
                },
                {
                    "start": 389,
                    "end": 523
                },
                {
                    "start": 524,
                    "end": 713
                },
                {
                    "start": 716,
                    "end": 783
                },
                {
                    "start": 784,
                    "end": 933
                },
                {
                    "start": 934,
                    "end": 1003
                },
                {
                    "start": 1006,
                    "end": 1083
                },
                {
                    "start": 1084,
                    "end": 1287
                },
                {
                    "start": 1288,
                    "end": 1433
                }
            ],
            "ref_mentions": [
                {
                    "start": 811,
                    "end": 815,
                    "matchedPaperCorpusId": "2672720"
                },
                {
                    "start": 999,
                    "end": 1002,
                    "matchedPaperCorpusId": "2893830"
                },
                {
                    "start": 1127,
                    "end": 1131,
                    "matchedPaperCorpusId": "21679091"
                },
                {
                    "start": 1324,
                    "end": 1328,
                    "matchedPaperCorpusId": "212908749"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.935546875
        },
        {
            "corpus_id": "258187531",
            "title": "Deep Collective Knowledge Distillation",
            "text": "Knowledge distillation [8] is an effective method for compressing a heavy teacher model to a lighter student model. The main idea of knowledge distillation is that a teacher model with higher capacity and better performance distills the softened output distribution into a student model as knowledge. Hinton et al. [8] explained that softened outputs with higher entropy than hard labels provide much richer information. In previous studies [1,8,17], assigning the probabilities to other classes, which leads to increased entropy, was effective in generalizing a network. These probabilities provided valuable information about the correlation between other classes and Dubey et al. [4] showed that maximum entropy training with these correlation information is effective. Many studies [7,10,[14][15][16]18,21,24] on knowledge distillation have focused on efficiently transferring teacher's knowledge to students. We took a step further by adopting a different approach to knowledge enrichment. If a student model learns from only a teacher's soft targets, then the student will only imitate the teacher. However, if the knowledge of peer students is additionally given, it can help a student model outperform the student model learned only from a teacher. Our work focuses on creating a student model that can have rich representation by training not only with the knowledge provided by the teacher but also with additional knowledge of various correlations between classes provided by the peer students. As a method for generating additional knowledge, we propose gathering knowledge from multiple students. \n\nMutual learning methods [2,5,12,26] aim to train powerful student models using ensembled knowledge of multiple untrained students without a pretrained teacher model. These methods train multistudent models with knowledge generated on-the-fly from students' logits, and this online knowledge is often generated to better represent the ground truth or soft targets of the teacher model. However, we propose an approach for generating additional online knowledge containing diverse correlation information from multistudent models, not similar to the ground truth or the teacher's soft targets. Since the teacher model learned with the supervised learning manner becomes overconfident [20], it may overlook other correlation information. Thus, we believe that the additional knowledge, including correlational information between classes, can assist the teacher model.",
            "score": 0.6759226483032117,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 115
                },
                {
                    "start": 116,
                    "end": 300
                },
                {
                    "start": 301,
                    "end": 420
                },
                {
                    "start": 421,
                    "end": 571
                },
                {
                    "start": 572,
                    "end": 772
                },
                {
                    "start": 773,
                    "end": 913
                },
                {
                    "start": 914,
                    "end": 994
                },
                {
                    "start": 995,
                    "end": 1104
                },
                {
                    "start": 1105,
                    "end": 1256
                },
                {
                    "start": 1257,
                    "end": 1505
                },
                {
                    "start": 1506,
                    "end": 1609
                },
                {
                    "start": 1612,
                    "end": 1777
                },
                {
                    "start": 1778,
                    "end": 1996
                },
                {
                    "start": 1997,
                    "end": 2203
                },
                {
                    "start": 2204,
                    "end": 2346
                },
                {
                    "start": 2347,
                    "end": 2477
                }
            ],
            "ref_mentions": [
                {
                    "start": 441,
                    "end": 444,
                    "matchedPaperCorpusId": "11253972"
                },
                {
                    "start": 446,
                    "end": 449,
                    "matchedPaperCorpusId": "9545399"
                },
                {
                    "start": 789,
                    "end": 792,
                    "matchedPaperCorpusId": "3608236"
                },
                {
                    "start": 792,
                    "end": 796,
                    "matchedPaperCorpusId": "131765296"
                },
                {
                    "start": 796,
                    "end": 800,
                    "matchedPaperCorpusId": "102483463"
                },
                {
                    "start": 800,
                    "end": 804,
                    "matchedPaperCorpusId": "233296935"
                },
                {
                    "start": 804,
                    "end": 807,
                    "matchedPaperCorpusId": "2723173"
                },
                {
                    "start": 807,
                    "end": 810,
                    "matchedPaperCorpusId": "204838340"
                },
                {
                    "start": 810,
                    "end": 813,
                    "matchedPaperCorpusId": "829159"
                },
                {
                    "start": 1636,
                    "end": 1639,
                    "matchedPaperCorpusId": "208526905"
                },
                {
                    "start": 1639,
                    "end": 1641,
                    "matchedPaperCorpusId": "219965421"
                },
                {
                    "start": 1641,
                    "end": 1644,
                    "matchedPaperCorpusId": "48352434"
                },
                {
                    "start": 1644,
                    "end": 1647,
                    "matchedPaperCorpusId": "26071966"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.9638671875
        },
        {
            "corpus_id": "270045106",
            "title": "Retro: Reusing teacher projection head for efficient embedding distillation on Lightweight Models via Self-supervised Learning",
            "text": "Knowledge distillation is a powerful technique used for transferring knowledge from a large, complex model (known as the teacher) to a smaller, simpler model (known as the student) to improve its performance. \n\nThe idea of knowledge distillation was first proposed by [15], which transfers knowledge from a large teacher to a smaller student by minimizing the Kulback-Leibler (KL) divergence between the outputs of the two models. Attention Transfer (AT) is introduced to transfer the spatial attention of the teacher to the student by minimizing the mean squared error (MSE) between the feature maps of the two models. This method guides the student to focus on relevant regions of the input image, improving its performance on small datasets.= FitNets [23] is another method of knowledge distillation that transfers knowledge from the intermediate layers of a deep and thin teacher to a deeper but thinner student. The intermediate layers learned by the teacher are treated as hints, and the student is trained to mimic them using mean squared error loss. Relation Knowledge Distillation (RKD) [20] is a method that transfers the mutual relationship between the samples in a batch from the teacher to the student. RKD uses distance-wise and angle-wise distillation loss to transfer the relationship between the samples to the student.",
            "score": 0.675406974876918,
            "section_title": "Knowledge Distillation",
            "char_start_offset": 6547,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 208
                },
                {
                    "start": 211,
                    "end": 430
                },
                {
                    "start": 431,
                    "end": 619
                },
                {
                    "start": 620,
                    "end": 745
                },
                {
                    "start": 746,
                    "end": 916
                },
                {
                    "start": 917,
                    "end": 1057
                },
                {
                    "start": 1058,
                    "end": 1215
                },
                {
                    "start": 1216,
                    "end": 1336
                }
            ],
            "ref_mentions": [
                {
                    "start": 1096,
                    "end": 1100,
                    "matchedPaperCorpusId": "131765296"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.9599609375
        },
        {
            "corpus_id": "249244040",
            "title": "Unsupervised Black-Box Model Domain Adaptation for Brain Tumor Segmentation",
            "text": "Knowledge distillation is proposed to transfer knowledge learned by a teacher model to a student model. Typically, the teacher model has larger backbones with more parameters, while the student one is typically a more compact model. Therefore, it is possible to efficiently compact a model with little sacrifice of performance. The conventional solution used a distillation loss function to enforce the consistency between the outputs of teacher and student models with the same input sample (Hinton et al., 2015). Essentially, the knowledge distillation is an adaptive label smoothing regularization (Szegedy et al., 2016). Kim et al. (2020) showed that the previous prediction can teach the network with a self-knowledge distillation scheme, which can be potentially used for semi-supervised learning. A recent work (Samuli and Timo, 2017) assembled the prediction along with the training as a teacher model prediction. Rather than using the average teacher model predictions, Tarvainen and Valpola (2017) used the averaged previous model parameters as a teacher model.",
            "score": 0.6727829006958109,
            "section_title": "Knowledge Distillation",
            "char_start_offset": 8838,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 103
                },
                {
                    "start": 104,
                    "end": 232
                },
                {
                    "start": 233,
                    "end": 327
                },
                {
                    "start": 328,
                    "end": 514
                },
                {
                    "start": 515,
                    "end": 624
                },
                {
                    "start": 625,
                    "end": 803
                },
                {
                    "start": 804,
                    "end": 921
                },
                {
                    "start": 922,
                    "end": 1071
                }
            ],
            "ref_mentions": [
                {
                    "start": 601,
                    "end": 623,
                    "matchedPaperCorpusId": "206593880"
                },
                {
                    "start": 818,
                    "end": 841,
                    "matchedPaperCorpusId": "13123084"
                },
                {
                    "start": 979,
                    "end": 1007,
                    "matchedPaperCorpusId": "263861232"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.9150390625
        },
        {
            "corpus_id": "271334394",
            "title": "Generalizing Teacher Networks for Effective Knowledge Distillation Across Student Architectures",
            "text": "Knowledge Distillation (KD) [10] is a technique that involves training a compact neural network, referred to as the \"student\" model, to approximate the decision-making capabilities of a more complex one known as the \"teacher.\"Typically, the student model is trained to match the logit scores or softmax probabilities of the teacher model [19].However, alternative approaches, such as employing activation maps or attention scores, have also been explored [22] for this purpose.Recently Zhao et al. [23] identified that the skewness of the predictive distribution of complex teachers towards target classes limits the transfer of useful knowledge from non-target classes.Introducing Decoupled Knowledge Distillation (DKD), they separate knowledge transfer from target and non-target classes.DKD dynamically adjusts the weighting of both learning objectives to ensure that the impact of non-target class-related information is not overlooked.",
            "score": 0.6725717194922761,
            "section_title": "Knowledge Distillation (KD)",
            "char_start_offset": 4653,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 226
                },
                {
                    "start": 226,
                    "end": 343
                },
                {
                    "start": 343,
                    "end": 477
                },
                {
                    "start": 477,
                    "end": 670
                },
                {
                    "start": 670,
                    "end": 790
                },
                {
                    "start": 790,
                    "end": 940
                }
            ],
            "ref_mentions": [
                {
                    "start": 455,
                    "end": 459,
                    "matchedPaperCorpusId": "829159"
                },
                {
                    "start": 498,
                    "end": 502,
                    "matchedPaperCorpusId": "247476179"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.94873046875
        },
        {
            "corpus_id": "258489662",
            "title": "PyNET-Q\u00d7Q: An Efficient PyNET Variant for Q\u00d7Q Bayer Pattern Demosaicing in CMOS Image Sensors",
            "text": "In knowledge distillation [23], a pre-trained, complex teacher model provides a distilled output (soft labels) to a smaller student model, transferring the teacher's knowledge. Although it seems intuitive that the student model should learn better from a stronger model, Cho and Hariharan [10] demonstrated that a highly accurate teacher is not always the best choice. They revealed that a less-trained network might be a more suitable teacher when the student network has limited capacity. This suggests that the teacher's knowledge should align with the student's capabilities. Furthermore, Rezagholizadeh et al. [51] proposed progressive distillation for natural language processing (NLP) and classification tasks to minimize the capability gap between the student and teacher. Progressive distillation gradually distills knowledge from a smoother teacher to a fully-trained teacher, allowing the student to learn from a teacher at the appropriate level. \n\nThere are various knowledge distillation methods. Feature distillation [52,67] transfers the teacher model's intermediate feature maps to a student model, while online distillation [5,8,19,11,70] trains both the teacher and student models simultaneously. Adaptive distillation [16,45] enables the student network to learn from multiple teachers adaptively. Du et al. [16] update the weights of knowledge distillation losses and feature losses during training based on teachers' gradients. Liu et al. [45] transfer features from multiple teachers and adaptively adjust weights between teachers' soft targets. \n\nHowever, in generative tasks that produce an image (such as super-resolution and demosaicing), directly applying distillation techniques is challenging since they do not have a notion of soft labels. Consequently, many super-resolution networks distill teacher networks' features [17,22,50] using an additional tool like a regressor to address dimension mismatches between the teacher and student. Gao et al. [17] transfer the first-order statistical map (e.g., average, maximum, or minimum value) of intermediate features, while FAKD [22] proposed spatial affinity of features-based distillation to utilize rich high-dimensional statistical information.",
            "score": 0.6725172110602808,
            "section_title": "Knowledge Distillation",
            "char_start_offset": 9780,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 176
                },
                {
                    "start": 177,
                    "end": 368
                },
                {
                    "start": 369,
                    "end": 490
                },
                {
                    "start": 491,
                    "end": 579
                },
                {
                    "start": 580,
                    "end": 780
                },
                {
                    "start": 781,
                    "end": 957
                },
                {
                    "start": 960,
                    "end": 1009
                },
                {
                    "start": 1010,
                    "end": 1214
                },
                {
                    "start": 1215,
                    "end": 1316
                },
                {
                    "start": 1317,
                    "end": 1448
                },
                {
                    "start": 1449,
                    "end": 1567
                },
                {
                    "start": 1570,
                    "end": 1769
                },
                {
                    "start": 1770,
                    "end": 1967
                },
                {
                    "start": 1968,
                    "end": 2224
                }
            ],
            "ref_mentions": [
                {
                    "start": 26,
                    "end": 30,
                    "matchedPaperCorpusId": "7200347"
                },
                {
                    "start": 289,
                    "end": 293,
                    "matchedPaperCorpusId": "203642130"
                },
                {
                    "start": 1149,
                    "end": 1152,
                    "matchedPaperCorpusId": "209319166"
                },
                {
                    "start": 1152,
                    "end": 1155,
                    "matchedPaperCorpusId": "48352434"
                },
                {
                    "start": 1237,
                    "end": 1241,
                    "matchedPaperCorpusId": "227276362"
                },
                {
                    "start": 1241,
                    "end": 1244,
                    "matchedPaperCorpusId": "224818016"
                },
                {
                    "start": 1327,
                    "end": 1331,
                    "matchedPaperCorpusId": "227276362"
                },
                {
                    "start": 1460,
                    "end": 1464,
                    "matchedPaperCorpusId": "224818016"
                },
                {
                    "start": 1854,
                    "end": 1857,
                    "matchedPaperCorpusId": "224858624"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.8427734375
        },
        {
            "corpus_id": "231925118",
            "title": "Learning Student-Friendly Teacher Networks for Knowledge Distillation",
            "text": "Knowledge distillation [1] is a well-known technique to learn compact deep neural network models with competitive accuracy, where a smaller network (student) is trained to simulate the representations of a larger one (teacher). The popularity of knowledge distillation is mainly due to its simplicity and generality; it is straightforward to learn a student model based on a teacher and there is no restriction about the network architectures of both models. The main goal of most approaches is how to transfer dark knowledge to student models effectively, given predefined and pretrained teacher networks. \n\nAlthough knowledge distillation is a promising and convenient method, it sometimes fails to achieve satisfactory performance in terms of accuracy. This is partly because the model capacity of a student is too limited compared to that of a teacher and knowledge distillation algorithms are suboptimal [2,3]. In addition to this reason, we claim that the consistency of teacher and student features is critical to knowledge transfer and the inappropriate representation learning of a teacher often leads to the suboptimality of knowledge distillation. \n\nWe are interested in making a teacher network hold better transferable knowledge by providing the teacher with a snapshot of the student model at the time of its training. We take advantage of the typical structures of convolutional neural networks with multiple blocks and make the representations of each block in teachers easy to be transferred to students. The proposed approach aims to train  teacher models friendly to students for facilitating knowledge distillation; we call the teacher model trained by this strategy student-friendly teacher network (SFTN). SFTN is deployed in arbitrary distillation algorithms easily due to its generality for training models and transferring knowledge. \n\nSFTN is partly related to collaborative learning methods [4,5,6], which may suffer from the correlation between the models trained jointly and fail to fully exploit knowledge in teacher models. On the other hand, SFTN is free from the limitation since it performs knowledge transfer from a teacher to a student in one direction via a two-stage learning procedure-student-aware training of teacher network followed by knowledge distillation from a teacher to a student. Although the structure of a teacher network depends on target student models, it is sufficiently generic to be adopted by students with various architectures.",
            "score": 0.6719950673728285,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 227
                },
                {
                    "start": 228,
                    "end": 458
                },
                {
                    "start": 459,
                    "end": 606
                },
                {
                    "start": 609,
                    "end": 755
                },
                {
                    "start": 756,
                    "end": 915
                },
                {
                    "start": 916,
                    "end": 1158
                },
                {
                    "start": 1161,
                    "end": 1332
                },
                {
                    "start": 1333,
                    "end": 1521
                },
                {
                    "start": 1522,
                    "end": 1727
                },
                {
                    "start": 1728,
                    "end": 1858
                },
                {
                    "start": 1861,
                    "end": 2054
                },
                {
                    "start": 2055,
                    "end": 2329
                },
                {
                    "start": 2330,
                    "end": 2488
                }
            ],
            "ref_mentions": [
                {
                    "start": 23,
                    "end": 26,
                    "matchedPaperCorpusId": "7200347"
                },
                {
                    "start": 909,
                    "end": 912,
                    "matchedPaperCorpusId": "208513309"
                },
                {
                    "start": 912,
                    "end": 914,
                    "matchedPaperCorpusId": "60440652"
                },
                {
                    "start": 1918,
                    "end": 1921,
                    "matchedPaperCorpusId": "26071966"
                },
                {
                    "start": 1921,
                    "end": 1923,
                    "matchedPaperCorpusId": "219965421"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.9375
        },
        {
            "corpus_id": "267036722",
            "title": "Research on Real-Time Detection of Maize Seedling Navigation Line Based on Improved YOLOv5s Lightweighting Technology",
            "text": "The technique of knowledge distillation is a widely adopted approach for compressing models, which differs from pruning and quantization in model compression. Essentially, knowledge distillation involves training a compact network model to emulate the knowledge extracted from a pre-trained larger network. This training methodology is commonly referred to as \"teacher-student\", where the larger network is the \"teacher network\", while the smaller network is the \"student network\". The goal of knowledge distillation is to enable the student network to achieve comparable, if not better, accuracy than the larger network while having fewer parameters and a smaller scale. By distilling the model, the problem of slow speed and high memory consumption is resolved, while also enhancing model accuracy. The distillation process can be observed in Figure 11. \n\nAgriculture 2024, 14, x FOR PEER REVIEW 11 of 28 \n\nThe technique of knowledge distillation is a widely adopted approach for compressing models, which differs from pruning and quantization in model compression. Essentially, knowledge distillation involves training a compact network model to emulate the knowledge extracted from a pre-trained larger network. This training methodology is commonly referred to as \"teacher-student\", where the larger network is the \"teacher network\", while the smaller network is the \"student network\". The goal of knowledge distillation is to enable the student network to achieve comparable, if not better, accuracy than the larger network while having fewer parameters and a smaller scale. By distilling the model, the problem of slow speed and high memory consumption is resolved, while also enhancing model accuracy. The distillation process can be observed in Figure 11. This article utilizes the YOLOv5m model as the teacher model. Firstly, a deeper and more capable teacher network is trained using the data to extract features. Then, the teacher network outputs the logits function, which is distilled at a temperature of T. The class probability distribution obtained by applying the softmax layer is used as soft targets. At the same time, the student network outputs logits that are distilled at the same temperature T, and knowledge distillation is performed. This is a commonly used method for model compression, which differs from pruning and quantization.",
            "score": 0.6714598281945641,
            "section_title": "Knowledge Distillation",
            "char_start_offset": 32175,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 158
                },
                {
                    "start": 159,
                    "end": 306
                },
                {
                    "start": 307,
                    "end": 481
                },
                {
                    "start": 482,
                    "end": 671
                },
                {
                    "start": 672,
                    "end": 800
                },
                {
                    "start": 801,
                    "end": 855
                },
                {
                    "start": 858,
                    "end": 906
                },
                {
                    "start": 909,
                    "end": 1067
                },
                {
                    "start": 1068,
                    "end": 1215
                },
                {
                    "start": 1216,
                    "end": 1390
                },
                {
                    "start": 1391,
                    "end": 1580
                },
                {
                    "start": 1581,
                    "end": 1709
                },
                {
                    "start": 1710,
                    "end": 1764
                },
                {
                    "start": 1765,
                    "end": 1826
                },
                {
                    "start": 1827,
                    "end": 1924
                },
                {
                    "start": 1925,
                    "end": 2120
                },
                {
                    "start": 2121,
                    "end": 2260
                },
                {
                    "start": 2261,
                    "end": 2359
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.96435546875
        },
        {
            "corpus_id": "219559263",
            "title": "Knowledge Distillation: A Survey",
            "text": "These predicted probabilities are defined by the feature representations of the training model. They reflect the similarities of data in feature embedding space. Yun et al. proposed class-wise self-knowledge distillation to match the output distributions of the training model between intra-class samples and augmented samples within the same source with the same model (Yun et al. 2020). In addition, the self-distillation proposed by Lee et al. (2019a) is adopted for data augmentation and the self-knowledge of augmentation is distilled into the model itself. Self distillation is also adopted to optimize deep models (the teacher or student networks) with the same architecture one by one (Furlanello et al. 2018;Bagherinezhad et al. 2018). Each network distills the knowledge of the previous network using a teacher-student optimization. \n\nBesides, offline, online and self distillation can also be intuitively understood from the perspective of human beings teacher-student learning. Offline distillation means the knowledgeable teacher teaches a student knowledge; online distillation means both teacher and student study together with each other; self-distillation means student learn knowledge by oneself. Moreover, just like the human beings learning, these three kinds of distillation can be combined to complement each other due to their own advantages. For example, both self-distillation and online distillation are properly integrated via the multiple knowledge transfer framework (Sun et al. 2021). In knowledge distillation, the teacher-student architecture is a generic carrier to form the knowledge transfer. In other words, the quality of knowledge acquisition and distillation from teacher to student is also determined by how to design the teacher and student networks. In terms of the habits of human beings learning, we hope that a student can find a right teacher. Thus, to well finish capturing and distilling knowledge in knowledge distillation, how to select or design proper structures of teacher and student is very important but difficult problem. Recently, the model setups of teacher and student are almost pre-fixed with unvaried sizes and structures during distillation, so as to easily cause the model capacity gap.",
            "score": 0.671257749172204,
            "section_title": "Self-Distillation",
            "char_start_offset": 27181,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 95
                },
                {
                    "start": 96,
                    "end": 161
                },
                {
                    "start": 162,
                    "end": 388
                },
                {
                    "start": 389,
                    "end": 562
                },
                {
                    "start": 563,
                    "end": 744
                },
                {
                    "start": 745,
                    "end": 842
                },
                {
                    "start": 845,
                    "end": 989
                },
                {
                    "start": 990,
                    "end": 1214
                },
                {
                    "start": 1215,
                    "end": 1365
                },
                {
                    "start": 1366,
                    "end": 1514
                },
                {
                    "start": 1515,
                    "end": 1627
                },
                {
                    "start": 1628,
                    "end": 1791
                },
                {
                    "start": 1792,
                    "end": 1889
                },
                {
                    "start": 1890,
                    "end": 2078
                },
                {
                    "start": 2079,
                    "end": 2251
                }
            ],
            "ref_mentions": [
                {
                    "start": 370,
                    "end": 386,
                    "matchedPaperCorpusId": "214727822"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.90869140625
        },
        {
            "corpus_id": "272968875",
            "title": "Student-Oriented Teacher Knowledge Refinement for Knowledge Distillation",
            "text": "Knowledge distillation has become widely recognized for its ability to transfer knowledge from a large teacher network to a compact and more streamlined student network. Traditional knowledge distillation methods primarily follow a teacher-oriented paradigm that imposes the task of learning the teacher's complex knowledge onto the student network. However, significant disparities in model capacity and architectural design hinder the student's comprehension of the complex knowledge imparted by the teacher, resulting in sub-optimal performance. This paper introduces a novel perspective emphasizing student-oriented and refining the teacher's knowledge to better align with the student's needs, thereby improving knowledge transfer effectiveness. Specifically, we present the Student-Oriented Knowledge Distillation (SoKD), which incorporates a learnable feature augmentation strategy during training to refine the teacher's knowledge of the student dynamically. Furthermore, we deploy the Distinctive Area Detection Module (DAM) to identify areas of mutual interest between the teacher and student, concentrating knowledge transfer within these critical areas to avoid transferring irrelevant information. This customized module ensures a more focused and effective knowledge distillation process. Our approach, functioning as a plug-in, could be integrated with various knowledge distillation methods. Extensive experimental results demonstrate the efficacy and generalizability of our method.",
            "score": 0.6712161083905006,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.9033203125
        },
        {
            "corpus_id": "269137377",
            "title": "Federated Distillation: A Survey",
            "text": "These methods focus on transferring knowledge from the middle layers of the teacher model to guide the training of the student model, helping the student model learn crucial representations and features at an intermediate level.\n\n3) Parameter Knowledge -In this category, partially trained parameters or network modules of the teacher model are directly used as knowledge during distillation training.This method is often combined with other distillation approaches to enhance knowledge transfer [63].Specifically, PESF-KD proposes an efficient approach to transferring knowledge from the teacher network to the student network by updating specific parameters of the pre-trained teacher model [64].FSKD compresses the pre-trained teacher model to obtain a student model and adjusts the feature dimensions using 1 \u00d7 1 convolutions between layers, allowing the student model to achieve performance comparable to traditional fine-tuned distillation methods with a small number of samples [65].Similarly, IAKD [66] and NGFSKD [67] assist distillation by replacing modules of the teacher model and the student model.The distillation loss for individual module replacement is expressed as:\n\nwhere L CE represents the calculation of cross-entropy, and M T o and M S o are the outputs of the corresponding modules in the teacher and student networks.Recently, SAKD proposes merging teacher and student networks of the same style system into a multi-path network.\n\nDuring training, a different teacher network module is dynamically selected to replace the corresponding student network module for each sample, resulting in a student model with superior performance [68].These methods leverage parameter knowledge from the teacher model to guide the training of the student model, either by updating specific parameters or by replacing modules.This enables the student model to learn from the partially trained teacher model, improving its performance and reducing its complexity.",
            "score": 0.6710542751840005,
            "section_title": "Soft targets Teacher model Loss",
            "char_start_offset": 14697,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 228
                },
                {
                    "start": 230,
                    "end": 401
                },
                {
                    "start": 401,
                    "end": 501
                },
                {
                    "start": 501,
                    "end": 698
                },
                {
                    "start": 698,
                    "end": 990
                },
                {
                    "start": 990,
                    "end": 1111
                },
                {
                    "start": 1111,
                    "end": 1183
                },
                {
                    "start": 1185,
                    "end": 1342
                },
                {
                    "start": 1342,
                    "end": 1454
                },
                {
                    "start": 1456,
                    "end": 1661
                },
                {
                    "start": 1661,
                    "end": 1834
                },
                {
                    "start": 1834,
                    "end": 1970
                }
            ],
            "ref_mentions": [
                {
                    "start": 496,
                    "end": 500,
                    "matchedPaperCorpusId": "216035835"
                },
                {
                    "start": 693,
                    "end": 697,
                    "matchedPaperCorpusId": "249209818"
                },
                {
                    "start": 1006,
                    "end": 1010,
                    "matchedPaperCorpusId": "234805083"
                },
                {
                    "start": 1022,
                    "end": 1026,
                    "matchedPaperCorpusId": "228063796"
                },
                {
                    "start": 1656,
                    "end": 1660,
                    "matchedPaperCorpusId": "248503415"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.73779296875
        },
        {
            "corpus_id": "273662110",
            "title": "Multi-Level Feature Distillation of Joint Teachers Trained on Distinct Image Datasets",
            "text": "Knowledge distillation. The goal of Knowledge Distillation (KD) is to transfer knowledge from a large teacher model to a small student model, such that the obtained student model mimics the behavior of the larger model [4,5,15,17,31,34]. The work of [15] defines the knowledge in the form of the output of the large model (logits). \n\nThe basic form of knowledge distillation consists in training the small student model to reproduce the logits of the large teacher model. This can be done either in the unsupervised [1,22] or supervised [26] scenario. In the supervised scenario, with ground-truth labels available at training time, Hinton et al. [15] show that significant improvements can be obtained by minimizing an objective function that takes into account the cross-entropy between the logits of the two models, but also another term that enforces the student model to predict ground-truth labels. Other works [3,39,51] consider knowledge at the feature level, employing distillation by matching feature distributions as well as logit outputs between the teacher and student models. Romero et al. penalize structural differences in relations. Li [33] reuses channel-wise and layer-wise meaningful features within the student to provide teacher-like knowledge without an additional model, in a teacher-free feature distillation framework. Liu et al. [29] propose a two-stage knowledge distillation method, which relies on a simple Feature Transform module consisting of two linear layers. Our proposed multilevel feature distillation method considers knowledge both at the logits level and feature level, but, unlike other approaches, it uses multiple teachers, each of them trained on a distinct dataset. Usually, knowledge distillation is applied for model compression [15], where the teacher model has a much larger capacity and memory footprint w.r.t. the student model. Our method allows student architectures to be identical to those of the individual teachers. Moreover, our method is very flexible, since it can combine teachers with distinct architectures.",
            "score": 0.6696035963990248,
            "section_title": "Related Work",
            "char_start_offset": 3719,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 23
                },
                {
                    "start": 24,
                    "end": 237
                },
                {
                    "start": 238,
                    "end": 331
                },
                {
                    "start": 334,
                    "end": 471
                },
                {
                    "start": 472,
                    "end": 551
                },
                {
                    "start": 552,
                    "end": 904
                },
                {
                    "start": 905,
                    "end": 1089
                },
                {
                    "start": 1090,
                    "end": 1149
                },
                {
                    "start": 1150,
                    "end": 1344
                },
                {
                    "start": 1345,
                    "end": 1494
                },
                {
                    "start": 1495,
                    "end": 1711
                },
                {
                    "start": 1712,
                    "end": 1880
                },
                {
                    "start": 1881,
                    "end": 1973
                },
                {
                    "start": 1974,
                    "end": 2071
                }
            ],
            "ref_mentions": [
                {
                    "start": 219,
                    "end": 222,
                    "matchedPaperCorpusId": "212633769"
                },
                {
                    "start": 222,
                    "end": 224,
                    "matchedPaperCorpusId": "203642130"
                },
                {
                    "start": 233,
                    "end": 236,
                    "matchedPaperCorpusId": "212908749"
                },
                {
                    "start": 516,
                    "end": 519,
                    "matchedPaperCorpusId": "251800257"
                },
                {
                    "start": 519,
                    "end": 522,
                    "matchedPaperCorpusId": "267022515"
                },
                {
                    "start": 537,
                    "end": 541,
                    "matchedPaperCorpusId": "257771519"
                },
                {
                    "start": 917,
                    "end": 920,
                    "matchedPaperCorpusId": "233296935"
                },
                {
                    "start": 920,
                    "end": 923,
                    "matchedPaperCorpusId": "2723173"
                },
                {
                    "start": 923,
                    "end": 925,
                    "matchedPaperCorpusId": "221559239"
                },
                {
                    "start": 1153,
                    "end": 1157,
                    "matchedPaperCorpusId": "253270253"
                },
                {
                    "start": 1356,
                    "end": 1360,
                    "matchedPaperCorpusId": "258841675"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.970703125
        },
        {
            "corpus_id": "219559263",
            "title": "Knowledge Distillation: A Survey",
            "text": "Inspired by the success of neural architecture search (or NAS), the performances of small neural networks have been further improved by searching for a global structure based on efficient meta operations or blocks (Wu et al. 2019;Tan et al. 2019;Tan and Le 2019;Radosavovic et al. 2020). Furthermore, the idea of dynamically searching for a knowledge transfer regime also appears in knowledge distillation, e.g., automatically removing redundant layers in a data-driven way using reinforcement learning (Ashok et al. 2018), and searching for optimal student networks given the teacher networks (Liu et al. 2019i;Xie et al. 2020;Gu and Tresp 2020). \n\nMost previous works focus on designing either the structures of teacher and student models or the knowledge transfer scheme between them. To make a small student model well match a large teacher model for improving knowledge distillation performance, the adaptive teacher-student learning architecture is necessary. Recently, the idea of a neural architecture search in knowledge distillation, i.e., a joint search of student structure and knowledge transfer under the guidance of the teacher model, will be an interesting subject of future study.",
            "score": 0.6690796632940484,
            "section_title": "Self-Distillation",
            "char_start_offset": 32595,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 287
                },
                {
                    "start": 288,
                    "end": 647
                },
                {
                    "start": 650,
                    "end": 787
                },
                {
                    "start": 788,
                    "end": 965
                },
                {
                    "start": 966,
                    "end": 1197
                }
            ],
            "ref_mentions": [
                {
                    "start": 214,
                    "end": 230,
                    "matchedPaperCorpusId": "54461508"
                },
                {
                    "start": 262,
                    "end": 285,
                    "matchedPaperCorpusId": "214714446"
                },
                {
                    "start": 503,
                    "end": 522,
                    "matchedPaperCorpusId": "13352766"
                },
                {
                    "start": 594,
                    "end": 612,
                    "matchedPaperCorpusId": "208175624"
                },
                {
                    "start": 612,
                    "end": 628,
                    "matchedPaperCorpusId": "207853355"
                },
                {
                    "start": 628,
                    "end": 646,
                    "matchedPaperCorpusId": "211004051"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.88330078125
        },
        {
            "corpus_id": "278327499",
            "title": "A Survey of Slow Thinking-based Reasoning LLMs using Reinforced Learning and Inference-time Scaling Law",
            "text": "Knowledge Distillation is a technique to transfer knowledge from a large, complex model (the teacher) to a smaller, efficient model (the student) [132]. The goal is to compress the teacher's expertise into the student while retaining performance. Two primary approaches exist: Model Distillation and Data Distillation. \n\nModel Distillation. Model Distillation directly transfers knowledge via the teacher's outputs. The student learns by mimicking the teacher's softened probability distribution (soft labels) over classes, which contains richer information than hard labels (one-hot vectors). A temperature-scaled softmax is used to smooth the outputs, and the student's loss combines both the teacher's guidance and ground-truth labels \n\nwhere   and   are the teacher's and student's softened probabilities for class ,   and   are logits from the teacher and student, and  is the temperature. \n\nThen, the loss function is computed as: \n\nwhere KL( \u2225 ) is the Kullback-Leibler divergence between teacher and student distributions; CE(,  \u2032 ) is cross-entropy loss between student predictions  \u2032 = softmax(  ) and true labels ;  is the weight balancing distillation and cross-entropy losses;  2 compensates for gradient scaling caused by temperature. Moreover,   ,   is logits from teacher and student;  is the emperature ( > 1 smooths probabilities,  = 1 recovers standard softmax);  is the distillation weight (typically 0.5); ,  is the teacher/student softened probabilities;  is the ground-truth labels. \n\nData Distillation. Data Distillation generates synthetic data that encapsulates the teacher's knowledge for student training. Synthetic data  syn ,  syn is generated such that training the student on it mimics training on the original dataset with the teacher. Methods include gradient matching or leveraging the teacher to label synthetic samples. \n\nMinimize the difference between student gradients on synthetic data and teacher gradients on real data: \n\nwhere L syn is the loss on synthetic data ( syn ,  syn ); L real is loss on real data (,  );  is the model parameters. Moreover,  syn ,  syn is the synthetic inputs and labels (learned via optimization);  is the parameters of the student model.",
            "score": 0.6685416801175373,
            "section_title": "Knowledge Distillation",
            "char_start_offset": 16249,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 152
                },
                {
                    "start": 153,
                    "end": 246
                },
                {
                    "start": 247,
                    "end": 318
                },
                {
                    "start": 321,
                    "end": 340
                },
                {
                    "start": 341,
                    "end": 415
                },
                {
                    "start": 416,
                    "end": 593
                },
                {
                    "start": 594,
                    "end": 737
                },
                {
                    "start": 740,
                    "end": 894
                },
                {
                    "start": 897,
                    "end": 936
                },
                {
                    "start": 939,
                    "end": 1248
                },
                {
                    "start": 1249,
                    "end": 1505
                },
                {
                    "start": 1508,
                    "end": 1526
                },
                {
                    "start": 1527,
                    "end": 1633
                },
                {
                    "start": 1634,
                    "end": 1768
                },
                {
                    "start": 1769,
                    "end": 1856
                },
                {
                    "start": 1859,
                    "end": 1962
                },
                {
                    "start": 1965,
                    "end": 2083
                },
                {
                    "start": 2084,
                    "end": 2209
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.9521484375
        },
        {
            "corpus_id": "257757091",
            "title": "Mixed-Type Wafer Classification For Low Memory Devices Using Knowledge Distillation",
            "text": "where T (x) and S(x) are the feature maps of the intermediate layers of the teacher and student model. The transformation \u03a6 T and \u03a6 S are applied when the feature map has different shapes. \n\nRelation-Based Knowledge Distillation: Relation-based knowledge distillation further explores the relationships between the different layers in the teacher and student model. In general, the distillation loss of relation-based knowledge based on the relations of feature maps can be formulated as \n\nwhere T (x) and S(x) are feature maps from different layers in teacher and student models, respectively. Pairs of feature maps are picked from both teacher and student models, T (x) and \u0164 (x) from the teacher and \u015c(x), \u0160(x) from the student. \u03a8 T and \u03a8 S are the similarity functions for the pair of feature maps sampled and L R is the correlation function between the teacher and student network.",
            "score": 0.6681938003408194,
            "section_title": "Knowledge Distillation",
            "char_start_offset": 8229,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 102
                },
                {
                    "start": 103,
                    "end": 188
                },
                {
                    "start": 191,
                    "end": 365
                },
                {
                    "start": 366,
                    "end": 487
                },
                {
                    "start": 490,
                    "end": 594
                },
                {
                    "start": 595,
                    "end": 731
                },
                {
                    "start": 732,
                    "end": 886
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.798828125
        },
        {
            "corpus_id": "212747830",
            "title": "Pre-trained models for natural language processing: A survey",
            "text": "Knowledge distillation (KD) [146] is a compression technique in which a small model calledstudent model is trained to reproduce the behaviors of a large model calledteacher model. Here the teacher model can be an ensemble of many models and usually well pre-trained. Different to model compression, distillation techniques learn a small student model from a fixed teacher model through some optimization objectives, while compression techniques aiming at searching a sparser architecture.\n\nGenerally, distillation mechanisms can be divided into three types: (1) distillation from soft target probabilities, (2) distillation from other knowledge, and (3) distillation to other structures.\n\n(1) Distillation from soft target probabilities. Ref. [143] showed that making the student approximate the teacher model can transfer knowledge from teacher to student. A common method is approximating the logits of the teacher model. DistilBERT [91] trained the student model with a distillation loss over the soft target probabilities of the teacher as\n\nwhere t i and s i are the probabilities estimated by the teacher model and the student, respectively. Distillation from soft target probabilities can also be used in task-specific models, such as information retrieval [155], and sequence labeling [156].\n\n(2) Distillation from other knowledge. Distillation from soft target probabilities regards the teacher model as a black box and only focus on its outputs. Moreover, decomposing the teacher model and distilling more knowledge can bring improvement to the student model. a) The desing of this table is borrowed from [94,151].\n\nb) The averaged score on 8 tasks (without WNLI) of GLUE benchmark (see Sect. 7.1). Here MNLI-m and MNLI-mm are regarded as two different tasks. \"dev\" indicates the result is on dev set. \"ensemble\" indicates the result is from the ensemble model. c) \"L MLM \", \"L NSP \", and \"L SOP \" indicate pre-training objective (see Sect. 3.1 and Table 1).\"L Task \" means task-specific loss. \"HAWQ\", \"GWQ\", \"DQ\", and \"QAT",
            "score": 0.6677600755243049,
            "section_title": "Knowledge distillation",
            "char_start_offset": 50542,
            "sentence_offsets": [],
            "ref_mentions": [
                {
                    "start": 743,
                    "end": 748,
                    "matchedPaperCorpusId": "11253972"
                },
                {
                    "start": 1292,
                    "end": 1297,
                    "matchedPaperCorpusId": "202122780"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.96875
        },
        {
            "corpus_id": "231839582",
            "title": "Show, Attend and Distill: Knowledge Distillation via Attention-based Feature Matching",
            "text": "Knowledge distillation is the technique for transferring knowledge from a source neural network to a target neural network (Hinton, Vinyals, and Dean 2015). The source network, referred to as a teacher, indicates a large network that is highly regularized via pre-training, and the target network, referred to as a student, is a smaller network for a specific task. The pre-trained teacher directly informs the student of the solution and intermediate process of a problem, and this informative supervision enables fast and effective learning of the student. Based on knowledge distillation, recent studies have shown significant improvements in model compression (Hinton, Vinyals, and Dean 2015;Romero et al. 2014;Yim et al. 2017;Tian, Krishnan, and Isola 2019), crossdomain transfer learning (Orbes-Arteainst et al. 2019;Asami et al. 2017), and continual learning (Li and Hoiem 2017;Hou et al. 2018).\n\nFor the success of knowledge distillation, various distillation methods were introduced. Starting from transferring output probability distributions of the teacher (Hinton, Vinyals, and Dean 2015), intermediate features representations (Romero et al. 2014) and their variants (Zagoruyko and Komodakis 2016a;Park et al. 2019;Tian, Krishnan, and Isola 2019) are investigated to identify what knowledge of the teacher helps to build a better student. However, most studies manually links the teacher and student features and perform distillation through the links individually. This manual link selection does not consider the similarity between the teacher and student features, so there is a risk of forcing an incorrect intermediate process to the student. Furthermore, the link selection has a limitation on fully utilizing the whole knowledge of the teacher by choosing a few of all possible links.\n\nTo compensate for the limitation, Jang et al. (Jang et al. 2019) apply a meta-networks, \"learning to transfer (L2T)\", automatically determining the links. In more details, the metanetwork consists of individual gates for all possible links, and each gate determines whether distillation through the link contributes to decreasing the classification loss of the student. Their results prove that knowledge distillation with the identified links provides better performance than those with manually selected",
            "score": 0.6673809838535079,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [],
            "ref_mentions": [
                {
                    "start": 715,
                    "end": 731,
                    "matchedPaperCorpusId": "206596723"
                },
                {
                    "start": 823,
                    "end": 841,
                    "matchedPaperCorpusId": "206742843"
                },
                {
                    "start": 885,
                    "end": 901,
                    "matchedPaperCorpusId": "52959234"
                },
                {
                    "start": 1211,
                    "end": 1228,
                    "matchedPaperCorpusId": "131765296"
                },
                {
                    "start": 1852,
                    "end": 1869,
                    "matchedPaperCorpusId": "155092628"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.9521484375
        },
        {
            "corpus_id": "253116921",
            "title": "Semi-Supervised Learning Based on Reference Model for Low-resource TTS",
            "text": "Knowledge distillation (KD) [34] can make student modle get the information from the teacher model. Its success is usually attributed to the privileged information of similarity between the class distribution of the teacher model and the student model. It was first proposed by Hinton et al. [34] transfer knowledge from large teacher networks to smaller student networks. It works by training students to predict target classification labels and imitate teachers' class probabilities because these features contain additional information about how teachers generalize [34].\n\nLiu et al. [35] tried the method of the teacher-student model for resolving the problem of exposure bias. There is an existing problem of exposure bias of autoregressive, due to the unmatched training and inference phase. This problem cloud leads to an unpredictable error for the model during the inference and accumulates the error frame by frame along the time axis.",
            "score": 0.6673629598608666,
            "section_title": "A. Knowledge Distillation",
            "char_start_offset": 4599,
            "sentence_offsets": [],
            "ref_mentions": [
                {
                    "start": 587,
                    "end": 591,
                    "matchedPaperCorpusId": "207847470"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.8603515625
        },
        {
            "corpus_id": "237091534",
            "title": "Online Multi-Granularity Distillation for GAN Compression",
            "text": "Knowledge Distillation (KD) [19] is a fundamental compression technique, where a smaller student model is optimized under the effective information transfer and supervision of a larger teacher model or ensembles [6]. Hinton [19] performed knowledge distillation via minimizing the distance between the output distribution statistics between student and teacher network. In this way, the student network attempts to learn dark knowledge [19] that contains the similarities between different classes, which can not be provided by the ground truth labels. Romero et al. [41] further took advantage of the concepts of feature maps from the intermediate layers to enhance the performance of the student network. Zhou et al. [65] presented that each channel of the feature map corresponds to a visual pattern, so they focus on transferring attention concepts [53,54,55] of feature map from each channel in intermediate layers. \n\nMoreover, You et al. [59] revealed that multiple teacher networks can provide more comprehensive knowledge for learning a more effective student network. MEAL [44] compressed large and complex trained ensembles into a sin-gle network, which employs an adversarial-based learning strategy to guide the pre-defined student network to transfer knowledge from teacher models. Offline knowledge distillation requires a pre-trained teacher model in the stage of optimizing, while online KD simultaneously optimizes the teacher and student network or just a group of student peers [51]. Anil et al. [2] trained two networks with the identical architecture parallelly and these two networks play the role of student and teacher iteratively. In this paper, we employ the multi-granularity based online distillation scheme, which aims to learn an effective student model from complementary structure of the teacher generators and the knowledge from diverse layers.",
            "score": 0.6669367712508201,
            "section_title": "Knowledge Distillation",
            "char_start_offset": 6919,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 216
                },
                {
                    "start": 217,
                    "end": 369
                },
                {
                    "start": 370,
                    "end": 552
                },
                {
                    "start": 553,
                    "end": 706
                },
                {
                    "start": 707,
                    "end": 920
                },
                {
                    "start": 923,
                    "end": 1076
                },
                {
                    "start": 1077,
                    "end": 1294
                },
                {
                    "start": 1295,
                    "end": 1502
                },
                {
                    "start": 1503,
                    "end": 1655
                },
                {
                    "start": 1656,
                    "end": 1877
                }
            ],
            "ref_mentions": [
                {
                    "start": 28,
                    "end": 32,
                    "matchedPaperCorpusId": "7200347"
                },
                {
                    "start": 212,
                    "end": 215,
                    "matchedPaperCorpusId": "11253972"
                },
                {
                    "start": 224,
                    "end": 228,
                    "matchedPaperCorpusId": "7200347"
                },
                {
                    "start": 436,
                    "end": 440,
                    "matchedPaperCorpusId": "7200347"
                },
                {
                    "start": 853,
                    "end": 857,
                    "matchedPaperCorpusId": "56594619"
                },
                {
                    "start": 857,
                    "end": 860,
                    "matchedPaperCorpusId": "201809759"
                },
                {
                    "start": 860,
                    "end": 863,
                    "matchedPaperCorpusId": "39802142"
                },
                {
                    "start": 944,
                    "end": 948,
                    "matchedPaperCorpusId": "26021416"
                },
                {
                    "start": 1082,
                    "end": 1086,
                    "matchedPaperCorpusId": "54447578"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.94482421875
        },
        {
            "corpus_id": "232428032",
            "title": "Fixing the Teacher-Student Knowledge Discrepancy in Distillation",
            "text": "Training a small student network with the guidance of a larger teacher network is an effective way to promote the performance of the student. Despite the different types, the guided knowledge used to distill is always kept unchanged for different teacher and student pairs in previous knowledge distillation methods. However, we find that teacher and student models with different networks or trained from different initialization could have distinct feature representations among different channels. (e.g. the high activated channel for different categories). We name this incongruous representation of channels as teacher-student knowledge discrepancy in the distillation process. Ignoring the knowledge discrepancy problem of teacher and student models will make the learning of student from teacher more difficult. To solve this problem, in this paper, we propose a novel student-dependent distillation method, knowledge consistent distillation, which makes teacher's knowledge more consistent with the student and provides the best suitable knowledge to different student networks for distillation. Extensive experiments on different datasets (CIFAR100, ImageNet, COCO) and tasks (image classification, object detection) reveal the widely existing knowledge discrepancy problem between teachers and students and demonstrate the effectiveness of our proposed method. Our method is very flexible that can be easily combined with other state-of-the-art approaches.",
            "score": 0.6664811553722977,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.7626953125
        },
        {
            "corpus_id": "268681103",
            "title": "ToXCL: A Unified Framework for Toxic Speech Detection and Explanation",
            "text": "Knowledge distillation (Hinton et al., 2015) is a technique that enables a smaller student model to learn from a larger teacher model by transferring knowledge.It has proven effective in improving performance, reducing computational requirements, and increasing efficiency in the field of Computer Vision (Gou et al., 2021).Recently, researchers have explored applying knowledge distillation in Natural Language Processing.For example, Fu et al. (2020) used a contrastive approach to align the intermediate layer outputs of the teacher and student models.Turc et al. (2019) extensively studied the interaction between pre-training, distillation, and fine-tuning, demonstrating the effectiveness of pre-trained distillation in tasks like sentiment analysis.Additionally, Clark et al. (2019) trained a multitasking network by ensembling multiple single-task teachers.In our work, we distill the knowledge from a teacher classifier to our model's classifier (the student classifier), optimizing the Kullback-Leibler distance (Csisz\u00e1r, 1975) between soft labels.",
            "score": 0.6661235107530565,
            "section_title": "Knowledge Distillation",
            "char_start_offset": 6794,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 160
                },
                {
                    "start": 160,
                    "end": 324
                },
                {
                    "start": 324,
                    "end": 423
                },
                {
                    "start": 423,
                    "end": 555
                },
                {
                    "start": 555,
                    "end": 756
                },
                {
                    "start": 756,
                    "end": 865
                },
                {
                    "start": 865,
                    "end": 1058
                }
            ],
            "ref_mentions": [
                {
                    "start": 305,
                    "end": 323,
                    "matchedPaperCorpusId": "219559263"
                },
                {
                    "start": 770,
                    "end": 789,
                    "matchedPaperCorpusId": "85464175"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.97119140625
        },
        {
            "corpus_id": "235489777",
            "title": "Knowledge Distillation via Instance-level Sequence Learning",
            "text": "Since then, researchers attempt to explore variants of knowledge distillation by using more supervised information from the teacher network. In [12], Romero et al. introduce a new metric of intermediate features between the teacher and student networks and add a regressor to match different size of teacher's and student's outputs. Zagoruyko et al. [13] propose to use the activation-based and gradient-based spatial attention maps from intermediate layers as the supervise information. Yim et al. [21] propose to use the flow of solution procedure (FSP) that is generated by computing the Gram matrix of features between layers to transfer knowledge. And the students imitate the process of solving problems by the teachers in FSP method. \n\nDifferent from the above methods, several researches adopts multiple teachers to supervise the student network's training. Shan et al. [22] conduct distillation by combining the knowledge of intermediate representations from multiple teacher networks. Shen et al. [23] extend this idea by learning a compact student model which is capable of handling the super task from multiple teachers. \n\nMoreover, knowledge distillation can be used by combining with the conventional DNNs compression and acceleration approaches. In [24], Mishra et al. propose a novel method to combine network quantization with knowledge distillation by jointly training a teacher network (full-precision) and a student network (low-precision) from scratch based on knowledge distillation. Besides, Zhou et al. [25] also propose a similar framework that the student network and the teacher network sharing lower layers and training simultaneously. Recently, researchers study knowledge distillation from another perspective rather than model compression. Born-again-network [26] optimizes the same network in generations by training the students parameterized identically to their teachers. Yang et al. [27] optimize deep networks in many generations and a few networks with the same architecture are optimized one by one. A similar idea has been proposed to extract useful information from earlier epochs in the same generation [28]. \n\nAs shown above, knowledge distillation typically transfers knowledge via feeding the sequence of random mini-batches sampled uniformly from the training data. In contrast, the student network in our proposed SLKD method is gradually guided using samples ordered in a meaningful sequence.",
            "score": 0.6659530998741057,
            "section_title": "II. RELATED WORK",
            "char_start_offset": 6489,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 140
                },
                {
                    "start": 141,
                    "end": 332
                },
                {
                    "start": 333,
                    "end": 487
                },
                {
                    "start": 488,
                    "end": 652
                },
                {
                    "start": 653,
                    "end": 740
                },
                {
                    "start": 743,
                    "end": 865
                },
                {
                    "start": 866,
                    "end": 994
                },
                {
                    "start": 995,
                    "end": 1132
                },
                {
                    "start": 1135,
                    "end": 1260
                },
                {
                    "start": 1261,
                    "end": 1505
                },
                {
                    "start": 1506,
                    "end": 1663
                },
                {
                    "start": 1664,
                    "end": 1770
                },
                {
                    "start": 1771,
                    "end": 1906
                },
                {
                    "start": 1907,
                    "end": 2038
                },
                {
                    "start": 2039,
                    "end": 2150
                },
                {
                    "start": 2153,
                    "end": 2311
                },
                {
                    "start": 2312,
                    "end": 2440
                }
            ],
            "ref_mentions": [
                {
                    "start": 144,
                    "end": 148,
                    "matchedPaperCorpusId": "2723173"
                },
                {
                    "start": 499,
                    "end": 503,
                    "matchedPaperCorpusId": "206596723"
                },
                {
                    "start": 878,
                    "end": 882,
                    "matchedPaperCorpusId": "26021416"
                },
                {
                    "start": 1790,
                    "end": 1794,
                    "matchedPaperCorpusId": "4110009"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.9677734375
        },
        {
            "corpus_id": "219559263",
            "title": "Knowledge Distillation: A Survey",
            "text": "Note that 'Section' is abbreviated as 'Sec.' in this figure (Cho and Hariharan 2019). Empirical results show that a larger model may not be a better teacher because of model capacity gap (Mirzadeh et al. 2020). Experiments also show that distillation adversely affects the student learning. The empirical evaluation of different forms of knowledge distillation about knowledge, distillation and mutual affection between teacher and student is not covered by Cho and Hariharan (2019). Knowledge distillation has also been explored for label smoothing, for assessing the accuracy of the teacher and for obtaining a prior for the optimal output layer geometry (Tang et al. 2020). \n\nKnowledge distillation for model compression is similar to the way in which human beings learn. Inspired by this, recent knowledge distillation methods have extended to teacher-student learning (Hinton et al. 2015), mutual learning (Zhang et al. 2018b), assistant teaching (Mirzadeh et al. 2020), lifelong learning (Zhai et al. 2019), and self-learning (Yuan et al. 2020). Most of the extensions of knowledge distillation concentrate on compressing deep neural networks. The resulting lightweight student networks can be easily deployed in applications such as visual recognition, speech recognition, and natural language processing (NLP). Furthermore, the knowledge transfer from one model to another in knowledge distillation can be extended to other tasks, such as adversarial attacks (Papernot et al. 2016), data augmentation (Lee et al. 2019a;Gordon and Duh 2019), data privacy and security (Wang et al. 2019a). Motivated by knowledge distillation for model compression, the idea of knowledge transfer has been further applied in compressing the training data, i.e., dataset distillation, which transfers the knowledge from a large dataset into a small dataset to reduce the training loads of deep models (Wang et al. 2018c;Bohdal et al. 2020). \n\nIn this paper, we present a comprehensive survey on knowledge distillation.",
            "score": 0.6656050904740245,
            "section_title": "Knowledge Transfer",
            "char_start_offset": 5554,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 85
                },
                {
                    "start": 86,
                    "end": 210
                },
                {
                    "start": 211,
                    "end": 290
                },
                {
                    "start": 291,
                    "end": 483
                },
                {
                    "start": 484,
                    "end": 676
                },
                {
                    "start": 679,
                    "end": 774
                },
                {
                    "start": 775,
                    "end": 1051
                },
                {
                    "start": 1052,
                    "end": 1149
                },
                {
                    "start": 1150,
                    "end": 1318
                },
                {
                    "start": 1319,
                    "end": 1595
                },
                {
                    "start": 1596,
                    "end": 1928
                },
                {
                    "start": 1931,
                    "end": 2006
                }
            ],
            "ref_mentions": [
                {
                    "start": 60,
                    "end": 84,
                    "matchedPaperCorpusId": "203642130"
                },
                {
                    "start": 187,
                    "end": 209,
                    "matchedPaperCorpusId": "212908749"
                },
                {
                    "start": 458,
                    "end": 482,
                    "matchedPaperCorpusId": "203642130"
                },
                {
                    "start": 911,
                    "end": 930,
                    "matchedPaperCorpusId": "26071966"
                },
                {
                    "start": 952,
                    "end": 974,
                    "matchedPaperCorpusId": "212908749"
                },
                {
                    "start": 994,
                    "end": 1012,
                    "matchedPaperCorpusId": "198229709"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.7568359375
        },
        {
            "corpus_id": "253734490",
            "title": "Scalable Collaborative Learning via Representation Sharing",
            "text": "The concept of knowledge distillation (KD) originated in Bucila et al. [8] as a way of compressing models, and was later generalized by Hinton et al. [19] (see Gou et al. [14] for an overview of the field). The standard use case for KD is that of a Teacher-Student (or offline) configuration, in which the teacher model (usually a large and well-trained model) transfers its knowledge to the student model (usually a smaller model) by sharing its last layer activations on a given transfer dataset (see Fig. 1a). The knowledge is then distilled into the student model using a divergence loss between the teacher and student models outputs (response-based KD) or intermediate layers (feature-based KD) on the transfer dataset. Traditional KD schemes use a transfer set that is similar (or identical) to the teacher training dataset, but some recent work has focused on data-free (or zero-shot) KD. This can be achieved either by looking at some of the teacher model statistics to generate synthetic transfer data [35,39,6], or by training a GAN in parallel [37,10,2]. It has also been shown that positive results can be obtained using mismatched or random unlabeled data for the distillation [27,40]. A key concept for our framework is the idea of online KD (or co-distillation [3], see Fig. 1b), where each model is treated as both a teacher and a student, meaning that the KD is performed synchronously with the training of each model (rather than after the training of the teacher model) [15,57,45]. Finally, ensemble KD (see Fig. 1c) refers to the setup where the knowledge is distilled from an ensemble of teacher (offline) or teacher-student (online) models. \n\nCollaborative Learning via Knowledge Distillation A growing body of literature has recently investigated ways of using online knowledge distillation (KD) [8,19] for collaborative learning in order to alleviate the need of sharing model updates (FL) or individual smashed data (SL).",
            "score": 0.665603142072542,
            "section_title": "Knowledge Distillation",
            "char_start_offset": 6216,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 206
                },
                {
                    "start": 207,
                    "end": 512
                },
                {
                    "start": 513,
                    "end": 725
                },
                {
                    "start": 726,
                    "end": 896
                },
                {
                    "start": 897,
                    "end": 1066
                },
                {
                    "start": 1067,
                    "end": 1199
                },
                {
                    "start": 1200,
                    "end": 1501
                },
                {
                    "start": 1502,
                    "end": 1663
                },
                {
                    "start": 1666,
                    "end": 1947
                }
            ],
            "ref_mentions": [
                {
                    "start": 71,
                    "end": 74,
                    "matchedPaperCorpusId": "11253972"
                },
                {
                    "start": 171,
                    "end": 175,
                    "matchedPaperCorpusId": "219559263"
                },
                {
                    "start": 1016,
                    "end": 1019,
                    "matchedPaperCorpusId": "159041346"
                },
                {
                    "start": 1056,
                    "end": 1060,
                    "matchedPaperCorpusId": "162183830"
                },
                {
                    "start": 1060,
                    "end": 1063,
                    "matchedPaperCorpusId": "91183944"
                },
                {
                    "start": 1063,
                    "end": 1065,
                    "matchedPaperCorpusId": "209500810"
                },
                {
                    "start": 1195,
                    "end": 1198,
                    "matchedPaperCorpusId": "227013462"
                },
                {
                    "start": 1490,
                    "end": 1494,
                    "matchedPaperCorpusId": "219965421"
                },
                {
                    "start": 1494,
                    "end": 1497,
                    "matchedPaperCorpusId": "26071966"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.97705078125
        },
        {
            "corpus_id": "258298441",
            "title": "Function-Consistent Feature Distillation",
            "text": "Deep neural networks (DNNs) have demonstrated great power on a variety of tasks. However, the high performance of DNN models is often accompanied by large storage and computational costs, which barricade their deployment on edge devices. A common solution to this problem is Knowledge Distillation (KD), whose central idea is to transfer the knowledge from a strong teacher model to a compact student model, in hopes that the additional guidance could raise the performance of the student (Gou et al., 2021;Wang & Yoon, 2021). According to the type of carrier for knowledge transfer, existing KD methods can be roughly divided into the following two categories: \n\n1.1 KNOWLEDGE DISTILLATION BASED ON FINAL OUTPUT Hinton et al. (2015) first clarifies the concept of knowledge distillation. In their method, softened probability distribution output by the teacher is used as the guidance to the student. Following works (Ding et al., 2019;Wen et al., 2019) step further to explore the trade-off between soft logits and hard task label. Zhao et al. (2022) propose to decouple target-class and non-target-class knowledge transfer, and Chen et al. (2022) make the student to reuse the teacher's classifier. Park et al. (2019) claims that the relationship among representations of different samples implies important information, and they used this relationship as the knowledge carrier. Recently, some works introduce contrastive learning to knowledge distillation. Specifically, Tian et al. (2020) propose to identify if a pair of teacher and student representations are congruent (same input provided to teacher and student) or not, and SSKD (Xu et al., 2020) introduce an additional self-supervision task to the training process. Whereas all the aforementioned methods employ a fixed teacher model during distillation, Zhang et al. (2018b) propose online knowledge distillation, where both the large and the small models are randomly initialized and learn mutually from each other during training.",
            "score": 0.6652891699309288,
            "section_title": "INTRODUCTION",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 80
                },
                {
                    "start": 81,
                    "end": 237
                },
                {
                    "start": 238,
                    "end": 526
                },
                {
                    "start": 527,
                    "end": 661
                },
                {
                    "start": 664,
                    "end": 788
                },
                {
                    "start": 789,
                    "end": 901
                },
                {
                    "start": 902,
                    "end": 1033
                },
                {
                    "start": 1034,
                    "end": 1201
                },
                {
                    "start": 1202,
                    "end": 1381
                },
                {
                    "start": 1382,
                    "end": 1460
                },
                {
                    "start": 1461,
                    "end": 1727
                },
                {
                    "start": 1728,
                    "end": 1995
                }
            ],
            "ref_mentions": [
                {
                    "start": 489,
                    "end": 507,
                    "matchedPaperCorpusId": "219559263"
                },
                {
                    "start": 1202,
                    "end": 1220,
                    "matchedPaperCorpusId": "131765296"
                },
                {
                    "start": 1817,
                    "end": 1837,
                    "matchedPaperCorpusId": "26071966"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.98095703125
        },
        {
            "corpus_id": "252421036",
            "title": "Improved Knowledge Distillationwith Dynamic Network Pruning",
            "text": "Knowledge Distillation is a simple way to have compact deep learning models. In this method, a large (i.e. cumbersome) network or an ensemble model is trained, first. This model is called the \"teacher\", which typically produces accurate predictions. Then, a smaller network, called the \"student\", is trained using the guidance coming from the teacher model. This guidance is obtained using \"temperature softmax\" applied on the logits of the teacher. The goal is to provide a better training for the student model than using only the labels from the dataset. Trained in this way, the final student network was shown to produce comparable results to the teacher's [18]. \n\nSimilar ideas to knowledge distillation has been explored before. Bucilua et al. approach the idea of knowledge transfer from a different point of view [42]. Instead of training a neural network on an original small set, they use an ensemble of base-level classifiers to label a large unlabeled dataset and then train the network on this much larger dataset. Ba and Caruana propose using L2 loss on the logits to mimic the teacher network [43]. \n\nFitNets [18] use knowledge distillation to yield deep and thin student networks that perform on par with or better than the teacher. They achieve this by training some student layers using the teacher's supervision for better initialization. Luo et al. show that using L2 loss to match the features of top hidden layers from both teacher and student is effective [19]. Yim et al. distill knowledge from the teacher by generating a matrix from feature maps at each layer [44]. Then, they transfer the knowledge from teacher to student, which has the same depth as the teacher, by applying L2 loss to these matrices.",
            "score": 0.6643323550946472,
            "section_title": "Knowledge Distillation",
            "char_start_offset": 14682,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 76
                },
                {
                    "start": 77,
                    "end": 106
                },
                {
                    "start": 107,
                    "end": 166
                },
                {
                    "start": 167,
                    "end": 249
                },
                {
                    "start": 250,
                    "end": 357
                },
                {
                    "start": 358,
                    "end": 449
                },
                {
                    "start": 450,
                    "end": 557
                },
                {
                    "start": 558,
                    "end": 667
                },
                {
                    "start": 670,
                    "end": 735
                },
                {
                    "start": 736,
                    "end": 827
                },
                {
                    "start": 828,
                    "end": 1028
                },
                {
                    "start": 1029,
                    "end": 1114
                },
                {
                    "start": 1117,
                    "end": 1249
                },
                {
                    "start": 1250,
                    "end": 1358
                },
                {
                    "start": 1359,
                    "end": 1485
                },
                {
                    "start": 1486,
                    "end": 1592
                },
                {
                    "start": 1593,
                    "end": 1731
                }
            ],
            "ref_mentions": [
                {
                    "start": 662,
                    "end": 666,
                    "matchedPaperCorpusId": "2723173"
                },
                {
                    "start": 822,
                    "end": 826,
                    "matchedPaperCorpusId": "11253972"
                },
                {
                    "start": 1109,
                    "end": 1113,
                    "matchedPaperCorpusId": "11536917"
                },
                {
                    "start": 1125,
                    "end": 1129,
                    "matchedPaperCorpusId": "2723173"
                },
                {
                    "start": 1480,
                    "end": 1484,
                    "matchedPaperCorpusId": "13621822"
                },
                {
                    "start": 1587,
                    "end": 1591,
                    "matchedPaperCorpusId": "206596723"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.94580078125
        },
        {
            "corpus_id": "264941356",
            "title": "Incorporating Natural Language Processing into Virtual Assistants: An Intelligent Assessment Strategy for Enhancing Language Comprehension",
            "text": "Given the imperative of modest model sizes for lowlatency applications, a direct distillation from extensively www.ijacsa.thesai.org large teacher models to considerably smaller student models might hinder the effective transfer of the teacher's expertise. As a remedy, a two-stage teacher assistance setup was devised for the distillation process, as illustrated in Fig. 1. This strategy aimed to strike a balance between knowledge transfer and model size reduction. In the initial stage, the immense teacher model was compressed into an intermediate-sized model. Subsequently, the final student model was trained using this intermediate model as a guide. This approach ensured that the transfer of knowledge from the teacher to the student was well-optimized, despite the significant size reduction. Drawing inspiration from the teacher's pretraining methodology, a distillation process was initiated from a randomly initialized student model. Convergence in training signaled a seamless transition to the deployment of the Stage 2 teacher model, thus continuing the distillation process. Importantly, the data employed for distillation in both stages remained consistent with the data utilized for teacher pretraining in their respective stages. Within the intermediate student/teacher pairing, a balanced blend of categorical crossentropy (MLM loss) and soft cross-entropy was applied, with equal weighting. Remarkably, experimentation indicated no substantial benefits from incorporating the attention and hidden layer outputs of the teacher model. Transitioning to the final student model, a dual-stage process was undertaken. First, the intermediate model underwent further pretraining, exclusively using Stage 2 data and without teacher involvement. Subsequently, a distillation procedure was executed to seamlessly transfer knowledge to the compact final student model. During this distillation phase, techniques mirrored those employed in the initial distillation, with the addition of hidden-layer output matching. In essence, the approach mirrors the core principles of the process outlined in the source paper. The process ensures effective knowledge transfer while mitigating the challenges arising from substantial model size reductions.",
            "score": 0.6637481253015061,
            "section_title": "D. Distillation",
            "char_start_offset": 34882,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 132
                },
                {
                    "start": 133,
                    "end": 256
                },
                {
                    "start": 257,
                    "end": 374
                },
                {
                    "start": 375,
                    "end": 467
                },
                {
                    "start": 468,
                    "end": 564
                },
                {
                    "start": 565,
                    "end": 656
                },
                {
                    "start": 657,
                    "end": 801
                },
                {
                    "start": 802,
                    "end": 945
                },
                {
                    "start": 946,
                    "end": 1090
                },
                {
                    "start": 1091,
                    "end": 1248
                },
                {
                    "start": 1249,
                    "end": 1411
                },
                {
                    "start": 1412,
                    "end": 1553
                },
                {
                    "start": 1554,
                    "end": 1632
                },
                {
                    "start": 1633,
                    "end": 1757
                },
                {
                    "start": 1758,
                    "end": 1878
                },
                {
                    "start": 1879,
                    "end": 2025
                },
                {
                    "start": 2026,
                    "end": 2123
                },
                {
                    "start": 2124,
                    "end": 2252
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.61865234375
        },
        {
            "corpus_id": "260181893",
            "title": "Focal Channel Knowledge Distillation for Multi-Modality Action Recognition",
            "text": "Knowledge distillation aims to transfer knowledge from a large model to a lighter model, which can reduce the model size but retain considerable performance. Managing the flow of knowledge transfer in students' training process is the essential technique because the teacher network is the source of knowledge. The logits of a teacher DNN model can be defined as the first type of knowledge. Hinton et al. [19] adopt a temperature coefficient to soften the predicted logits output as a soft target and transfer the prediction knowledge by minimizing the KL divergence between the logits of the student and teacher, thus effectively improving the performance of the student model. To learn more featurebased knowledge, some works [20], [21] explored the feature representations from intermediate layers of the teacher as the second type of knowledge and transferred them to the student. FitNets [20] directly match the feature activations of the teacher and the student by minimizing the Euclidean distance of the feature maps. AT [21] proposed to transfer feature attention knowledge by minimizing the L2 distance of the spatial attention maps between the teacher and student. Furthermore, several recent works [12], [13], [14], [22] also explored the channel knowledge representations and transferred them to the student. HMAT [22] proposed a hierarchical multi-attention transfer framework for knowledge distillation, which utilizes position-based, channel-based, and activation-based attention to transfer knowledge at different levels of deep representations. The knowledge can also be defined by the probability distributions of the representations. Shu et al. [12] proposed a channel-wise knowledge distillation method to minimize the KL divergence between the channel-wise probability maps of the student and teacher networks, allowing the distillation process to focus more on the salient regions of each channel. To learn to diversity properties of features, ICKD [13] proposed to align the feature diversity and homology between teacher and student networks by minimizing the L2 distance between interchannel correlations. To transfer the full relation of both the channel and spatial knowledge, CSC [14] minimized the L2 distance of the channel and spatial correlation matrices between the student and teacher.",
            "score": 0.6635266126755981,
            "section_title": "II. RELATED WORK A. KNOWLEDGE DISTILLATION",
            "char_start_offset": 7751,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 157
                },
                {
                    "start": 158,
                    "end": 310
                },
                {
                    "start": 311,
                    "end": 391
                },
                {
                    "start": 392,
                    "end": 679
                },
                {
                    "start": 680,
                    "end": 885
                },
                {
                    "start": 886,
                    "end": 1026
                },
                {
                    "start": 1027,
                    "end": 1176
                },
                {
                    "start": 1177,
                    "end": 1322
                },
                {
                    "start": 1323,
                    "end": 1563
                },
                {
                    "start": 1564,
                    "end": 1654
                },
                {
                    "start": 1655,
                    "end": 1921
                },
                {
                    "start": 1922,
                    "end": 2132
                },
                {
                    "start": 2133,
                    "end": 2321
                }
            ],
            "ref_mentions": [
                {
                    "start": 406,
                    "end": 410,
                    "matchedPaperCorpusId": "7200347"
                },
                {
                    "start": 729,
                    "end": 733,
                    "matchedPaperCorpusId": "2723173"
                },
                {
                    "start": 735,
                    "end": 739,
                    "matchedPaperCorpusId": "829159"
                },
                {
                    "start": 894,
                    "end": 898,
                    "matchedPaperCorpusId": "2723173"
                },
                {
                    "start": 1030,
                    "end": 1034,
                    "matchedPaperCorpusId": "829159"
                },
                {
                    "start": 1211,
                    "end": 1215,
                    "matchedPaperCorpusId": "236882796"
                },
                {
                    "start": 1217,
                    "end": 1221,
                    "matchedPaperCorpusId": "244101388"
                },
                {
                    "start": 1223,
                    "end": 1227,
                    "matchedPaperCorpusId": "221235995"
                },
                {
                    "start": 1229,
                    "end": 1233,
                    "matchedPaperCorpusId": "252995794"
                },
                {
                    "start": 1328,
                    "end": 1332,
                    "matchedPaperCorpusId": "252995794"
                },
                {
                    "start": 1666,
                    "end": 1670,
                    "matchedPaperCorpusId": "236882796"
                },
                {
                    "start": 1973,
                    "end": 1977,
                    "matchedPaperCorpusId": "244101388"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.96875
        },
        {
            "corpus_id": "125985701",
            "title": "Knowledge Distillation via Route Constrained Optimization",
            "text": "of models could be transferred to the other single model. Then Hinton et al. [10] further claimed that knowledge distillation (KD) could transfer distilled knowledge to student network efficiently. By increasing the temperature, the logits (the inputs to the final softmax) contain richer information than one-hot labels. Afterward, [14] proposed to learn the curriculum from data by a network called Mentor-Net. [18] adopted a method to learn from noisy labels.\n\nLearning Representation from Hint. Hint-based learning is often used for open-set classification such as face recognition and person Re-identification. FitNet [24] firstly introduced more supervision by exploiting intermediatelevel feature maps from the hidden layers of teacher to guide training process of student. Afterward, Zagoruyko et al. [30] proposed the method to transfer attention maps from teacher to student. Yim et al. [29] defined the distilled knowledge from teacher network as the flow of the solution process (FSP), which is calculated by the inner product between feature maps from two selected layers.\n\nPrevious knowledge transfer methods only supervise student with converged teacher, thus fail to capture the knowledge during teacher's training process. Our work differs from existing approaches in that we supervise student with the knowledge transferred from teacher's training trajectory.",
            "score": 0.6635218844218758,
            "section_title": "Related Work",
            "char_start_offset": 5767,
            "sentence_offsets": [],
            "ref_mentions": [
                {
                    "start": 333,
                    "end": 337,
                    "matchedPaperCorpusId": "51876228"
                },
                {
                    "start": 413,
                    "end": 417,
                    "matchedPaperCorpusId": "14659675"
                },
                {
                    "start": 897,
                    "end": 901,
                    "matchedPaperCorpusId": "206596723"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.8779296875
        },
        {
            "corpus_id": "278602421",
            "title": "DCSNet: A Lightweight Knowledge Distillation-Based Model with Explainable AI for Lung Cancer Diagnosis from Histopathological Images",
            "text": "KD is widely used in recent studies, where smaller student models are trained to mimic the behavior of larger teacher models. Hinton et al. [18], showed the student learns to mimic the output logits of the teacher models. Fukuda et al. [19] proposed an ensemble KD approach, where a smaller student network is trained using a set of teacher models, such as VGG or LSTM. The student model simultaneously learns from the outputs of multiple teachers. Yoon et al. [20] introduced a similarity-preserving knowledge distillation, where a small student network is trained to replicate the output of an extensive teacher network while maintaining important similarities between data. \n\nThese methods transfer knowledge from a large teacher to a smaller student model. However, if the student is significantly less complex it may fail to capture and replicate the teacher's intricate outputs. This issue is addressed in the multi-step KD method. Mirzadeh et al. [21] utilized the Teacher Assistant Knowledge Distillation (TAKD) approach. TAKD utilizes an intermediate model, called a Teacher Assistant (TA), to bridge the gap between a large teacher model and a small student model. Training a student using the TA gives better performance as the TA is closer in size to the student and makes the transfer of knowledge effective. Saleknia et al. [22] also employed midsize assistant networks that bridge the computational gap between the teacher and student models. They use three variants of EfficientNetV2 to build the teacher, teacher assistant, and student networks. Their approach improved the student model performance on the Standford-40 dataset from 94.75% to 96.30%. A disadvantage of this method is that computational complexity increases because of the multi-step distillation process However, transferring knowledge using only output logits has certain limitations, as it solely focuses on the output distribution while completely ignoring the intermediate representations learned at various layers. Featurebased distillation strategies address this limitation by transferring knowledge from intermediate layers. Remero et al. [23] used both the output logits and the intermediate representations of the teacher model to guide the training process of the student. Additionally, they introduced extra parameters to map the student's hidden layers to the prediction of the teacher's hidden layers.",
            "score": 0.6625888430112172,
            "section_title": "Knowledge Distillation",
            "char_start_offset": 6891,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 125
                },
                {
                    "start": 126,
                    "end": 221
                },
                {
                    "start": 222,
                    "end": 369
                },
                {
                    "start": 370,
                    "end": 448
                },
                {
                    "start": 449,
                    "end": 676
                },
                {
                    "start": 679,
                    "end": 760
                },
                {
                    "start": 761,
                    "end": 884
                },
                {
                    "start": 885,
                    "end": 937
                },
                {
                    "start": 938,
                    "end": 1029
                },
                {
                    "start": 1030,
                    "end": 1174
                },
                {
                    "start": 1175,
                    "end": 1321
                },
                {
                    "start": 1322,
                    "end": 1457
                },
                {
                    "start": 1458,
                    "end": 1562
                },
                {
                    "start": 1563,
                    "end": 1667
                },
                {
                    "start": 1668,
                    "end": 2003
                },
                {
                    "start": 2004,
                    "end": 2116
                },
                {
                    "start": 2117,
                    "end": 2267
                },
                {
                    "start": 2268,
                    "end": 2399
                }
            ],
            "ref_mentions": [
                {
                    "start": 461,
                    "end": 465,
                    "matchedPaperCorpusId": "229376103"
                },
                {
                    "start": 954,
                    "end": 958,
                    "matchedPaperCorpusId": "212908749"
                },
                {
                    "start": 1338,
                    "end": 1342,
                    "matchedPaperCorpusId": "268714642"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.88916015625
        },
        {
            "corpus_id": "264405606",
            "title": "DistillCSE: Distilled Contrastive Learning for Sentence Embeddings",
            "text": "Knowledge distillation (Hinton et al., 2015) involves training a compact model, often referred to as a student model, to mimic the behavior and knowledge of a larger, more complex model known as the teacher model. It has been successfully applied to various tasks, such as language modeling (Zhuang et al., 2021), text classification (Heinzerling and Strube, 2018;Chia et al., 2019), named entity recognition (Zhou et al., 2021), machine translation (Tan et al., 2019), language generation (Melas-Kyriazi et al., 2019). \n\nTeacher model knowledge guide students in multiple ways during the distillation. Its predictions or soft targets, attention weights, or hidden representations, could all be used to guide the training of the student model. Consequently, the student is provided with stronger training signals from the teacher and achieves even superior performance. For example, Zhuang et al. (2021) directly mimics the output logits on vocabulary while Jiao et al. (2020) utilizes both hidden representations and attention matrix. \n\nDifferent from those studies, we employ knowledge distillation as an element of our self-training framework. Therefore, we focus on the most fundamental and general form of distillation which only minimize the cross entropy of prediction logits distribution between teacher and students. We use the homogeneous structure model for both the student and teacher model for distillation. Our research mainly focuses on the output logit distribution from the teacher instead of a special distillation framework. Therefore, our method is generic for more advanced distillation technologies.",
            "score": 0.6623101804818599,
            "section_title": "Knowledge Distillation",
            "char_start_offset": 25385,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 213
                },
                {
                    "start": 214,
                    "end": 519
                },
                {
                    "start": 522,
                    "end": 602
                },
                {
                    "start": 603,
                    "end": 743
                },
                {
                    "start": 744,
                    "end": 869
                },
                {
                    "start": 870,
                    "end": 1035
                },
                {
                    "start": 1038,
                    "end": 1146
                },
                {
                    "start": 1147,
                    "end": 1325
                },
                {
                    "start": 1326,
                    "end": 1421
                },
                {
                    "start": 1422,
                    "end": 1544
                },
                {
                    "start": 1545,
                    "end": 1622
                }
            ],
            "ref_mentions": [
                {
                    "start": 23,
                    "end": 44,
                    "matchedPaperCorpusId": "7200347"
                },
                {
                    "start": 291,
                    "end": 312,
                    "matchedPaperCorpusId": "236164690"
                },
                {
                    "start": 334,
                    "end": 364,
                    "matchedPaperCorpusId": "21697629"
                },
                {
                    "start": 490,
                    "end": 518,
                    "matchedPaperCorpusId": "208050764"
                },
                {
                    "start": 883,
                    "end": 903,
                    "matchedPaperCorpusId": "236164690"
                },
                {
                    "start": 958,
                    "end": 976,
                    "matchedPaperCorpusId": "202719327"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.95947265625
        },
        {
            "corpus_id": "257205990",
            "title": "LightTS: Lightweight Time Series Classification with Adaptive Ensemble Distillation",
            "text": "Knowledge Distillation (KD) [25] aims to transfer knowledge from a teacher model to a student model, where the teacher is often a larger model with higher discriminative capacity than the student. In classification, the knowledge is represented by a probability distribution over classes produced by the teacher model. Let  and  represent the class distributions from the teacher and the student, respectively. Then, knowledge distillation is formalized in Equation 1, where  \u2208 [0, 1] is a hyper-parameter [54]. \n\nSpecifically, the loss function is computed over two components that are weighted by . The first component is the cross-entropy (CE) between the student class probabilities  and the ground truth label , which provides supervision from the ground truth labels. The second component represents the distance between the teacher and student distributions  and , e.g., Kullback-Leibler (KL) divergence, to encourage a student to mimic the behavior of a more powerful teacher. They both contribute to training an accurate student. \n\nWhen a group of base models is available as the teachers, their average-ensemble,  = 1/ \u00d7   , where   is the class distribution returned by the -th base model, is typically considered as the knowledge source [17]. Figure 5 shows an example of the knowledge distillation with an ensemble consisting of three base models.",
            "score": 0.661406088576282,
            "section_title": "Knowledge Distillation",
            "char_start_offset": 13449,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 196
                },
                {
                    "start": 197,
                    "end": 318
                },
                {
                    "start": 319,
                    "end": 410
                },
                {
                    "start": 411,
                    "end": 511
                },
                {
                    "start": 514,
                    "end": 600
                },
                {
                    "start": 601,
                    "end": 773
                },
                {
                    "start": 774,
                    "end": 984
                },
                {
                    "start": 985,
                    "end": 1038
                },
                {
                    "start": 1041,
                    "end": 1254
                },
                {
                    "start": 1255,
                    "end": 1360
                }
            ],
            "ref_mentions": [
                {
                    "start": 28,
                    "end": 32,
                    "matchedPaperCorpusId": "7200347"
                },
                {
                    "start": 506,
                    "end": 510,
                    "matchedPaperCorpusId": "228376532"
                },
                {
                    "start": 1249,
                    "end": 1253,
                    "matchedPaperCorpusId": "227276362"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.974609375
        },
        {
            "corpus_id": "253097876",
            "title": "Hard Gate Knowledge Distillation - Leverage Calibration for Robust and Reliable Language Model",
            "text": "In recent years, the deep learning community has achieved marked performance gains across a variety of tasks (Brown et al., 2020;Devlin et al., 2018). In the meantime, some deep learning models have become excessively large, limiting their applicability in some scenarios. To cope with the issue, Hinton et al. (2015) proposed knowledge distillation (KD), in which knowledge of a large network, called a teacher network, is transferred to a relatively small model, called a student model. \n\nThe benefits of KD have been widely witnessed across multiple domains (Romero et al., 2015;Jiao et al., 2020). Recently, it has been observed that KD can be used in both reducing model size and improving model generalization (Tang et al., 2021;Furlanello et al., 2018). Hinton et al. (2015) argue that a distribution, defined by a teacher, holds inter-class relations, commonly referred to as the dark knowledge, and that such distribution brings a meaningful supervision to a student. Therefore, a large body of research in KD has viewed a teacher as a source of knowledge and has focused on finding a meaningful knowledge to be transferred (Romero et al., 2015;Bul\u00f2 et al., 2016;Park et al., 2019;Yuan et al., 2020;Kim et al., 2021). \n\nIn this work, we focus on when to distill knowledge of a teacher. This is a central question to ask, as a model can benefit from the adaptive control of supervision between ground truth and a teacher; When a model is trained to increase the predictive score of a prediction, a one-hot encoded supervision, without incorporating teacher model, sends a direct signal in increasing the score (M\u00fcller et al., 2019). In another case, when a model is trained to learn knowledge of a teacher, a teacher's output without fusing a ground truth sends more direct signal in minimizing the knowledge gap between the student and the teacher. However, the question of \"when\" has not been answered. For this reason, previous works choose to learn from both of the supervisions. \n\nWe give an answer to the question from the perspective of model calibration.",
            "score": 0.6601604308497149,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 150
                },
                {
                    "start": 151,
                    "end": 272
                },
                {
                    "start": 273,
                    "end": 488
                },
                {
                    "start": 491,
                    "end": 601
                },
                {
                    "start": 602,
                    "end": 760
                },
                {
                    "start": 761,
                    "end": 976
                },
                {
                    "start": 977,
                    "end": 1226
                },
                {
                    "start": 1229,
                    "end": 1294
                },
                {
                    "start": 1295,
                    "end": 1640
                },
                {
                    "start": 1641,
                    "end": 1857
                },
                {
                    "start": 1858,
                    "end": 1912
                },
                {
                    "start": 1913,
                    "end": 1991
                },
                {
                    "start": 1994,
                    "end": 2070
                }
            ],
            "ref_mentions": [
                {
                    "start": 109,
                    "end": 129,
                    "matchedPaperCorpusId": "218971783"
                },
                {
                    "start": 561,
                    "end": 582,
                    "matchedPaperCorpusId": "2723173"
                },
                {
                    "start": 582,
                    "end": 600,
                    "matchedPaperCorpusId": "202719327"
                },
                {
                    "start": 735,
                    "end": 759,
                    "matchedPaperCorpusId": "4110009"
                },
                {
                    "start": 1133,
                    "end": 1154,
                    "matchedPaperCorpusId": "2723173"
                },
                {
                    "start": 1154,
                    "end": 1172,
                    "matchedPaperCorpusId": "15776580"
                },
                {
                    "start": 1172,
                    "end": 1190,
                    "matchedPaperCorpusId": "131765296"
                },
                {
                    "start": 1190,
                    "end": 1208,
                    "matchedPaperCorpusId": "219962714"
                },
                {
                    "start": 1208,
                    "end": 1225,
                    "matchedPaperCorpusId": "233714221"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.966796875
        },
        {
            "corpus_id": "257308816",
            "title": "A Survey on Deep-Learning-Based Real-Time SAR Ship Detection",
            "text": "Hinton put forward the concept of knowledge distillation for the first time in distilling the knowledge in a neural network. And he introduced the soft targets of teacher to induce the training of students' network. The knowledge distillation is classified into three categories, they are logits transfer, teacher assistant, and domain adaptation [109]. \n\nGenerally speaking, the teacher model has strong ability and performance, while the student model is compact. The knowledge distillation methods transfer the generalization ability of the teacher model to the compact student model to improve its performance with less complexity. The basic idea of knowledge distillation is to transfer the dark knowledge in the complex teacher model to the simple student model. These methods match or outperform the teacher's performance, while requiring notably fewer parameters and multiplications [110], [111], [112], as shown in Fig. 7. \n\nThe parameter T represents temperature. Generally, T is 1. When T is larger, a softer probability distribution will be obtained. There are two loss functions. The first loss function requires that the student model and the teacher model use the same T when calculating the softmax layer. The second loss function requires the student model T to be taken as 1, and the loss function is the weighted average of the two objective functions. Soft prediction carries more and more useful information than hard prediction. The knowledge distillation can get a lightweight CNN model with high accuracy [113]. \n\nThe softmax function is formulated as follows:",
            "score": 0.6596012341311479,
            "section_title": "C. Knowledge Distillation",
            "char_start_offset": 14030,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 124
                },
                {
                    "start": 125,
                    "end": 215
                },
                {
                    "start": 216,
                    "end": 353
                },
                {
                    "start": 356,
                    "end": 465
                },
                {
                    "start": 466,
                    "end": 635
                },
                {
                    "start": 636,
                    "end": 768
                },
                {
                    "start": 769,
                    "end": 931
                },
                {
                    "start": 934,
                    "end": 973
                },
                {
                    "start": 974,
                    "end": 992
                },
                {
                    "start": 993,
                    "end": 1062
                },
                {
                    "start": 1063,
                    "end": 1092
                },
                {
                    "start": 1093,
                    "end": 1221
                },
                {
                    "start": 1222,
                    "end": 1371
                },
                {
                    "start": 1372,
                    "end": 1450
                },
                {
                    "start": 1451,
                    "end": 1535
                },
                {
                    "start": 1538,
                    "end": 1584
                }
            ],
            "ref_mentions": [
                {
                    "start": 891,
                    "end": 896,
                    "matchedPaperCorpusId": "203642130"
                },
                {
                    "start": 898,
                    "end": 903,
                    "matchedPaperCorpusId": "48352434"
                },
                {
                    "start": 905,
                    "end": 910,
                    "matchedPaperCorpusId": "2723173"
                },
                {
                    "start": 1529,
                    "end": 1534,
                    "matchedPaperCorpusId": "102483463"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.95947265625
        },
        {
            "corpus_id": "237940335",
            "title": "Consistency Training of Multi-exit Architectures for Sensor Data",
            "text": "Distillation is a general-purpose technique for transferring knowledge between models to achieve smaller model with smaller size and less computational load [4,12]. It aims to capture and transfer information from a large model (or ensemble), termed as a teacher for supervising a relatively small model named a student with the goal of achieving compression for efficient inference. In knowledge distillation, the teacher is generally a fixed pretrained network learned with a massive amount of high-quality data. In contrast, the student is a low capacity network that is guided to mimic the output of the teacher on a certain task. The rich supervisory signal from the privileged teacher model enables a compact student network to learn important aspects of the input that otherwise could not be possible while solely minimizing a supervised objective. Hinton et al. [12] proposed to use softened class probabilities from the teacher to provide extra supervision, which acts as targets for the student model to optimize. \n\nIn the classical distillation framework, the teacher network   produces logits   (or unnormalized probabilities), which are a pre-activation vector before applying the softmax function (). In the case of ensembles or having multiple teachers, their outputs can simply be averaged. Similarly, a student network   , which has possibly different architecture and set of weights, produces an output analogous to the teacher model. The student network is trained to match its output as closely as possible to the teacher on the same input together, along with minimizing cross-entropy loss on the actual class labels . The loss applied to the logits from   and   is temperature-scaled with a hyper-parameter  > 1 to soften the predictions. This step is crucial to mitigate the over-confidence of the networks as, generally, they put excessive probability mass on the top predicted class and too little on the rest. Thus, the scaling operation produces a more informative training signal from the teacher network as the difference between the largest and smallest output values is reduced. The complete learning objective that is minimized to penalize the difference between these models can be specified as: \n\nDue to its conceptual simplicity, distillation is also adopted for improving the training of multiexit architectures [31] through treating later (generally last) exits in the network as a teacher and earlier exits as students.",
            "score": 0.6595701680432832,
            "section_title": "Knowledge Distillation",
            "char_start_offset": 8623,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 164
                },
                {
                    "start": 165,
                    "end": 383
                },
                {
                    "start": 384,
                    "end": 514
                },
                {
                    "start": 515,
                    "end": 634
                },
                {
                    "start": 635,
                    "end": 855
                },
                {
                    "start": 856,
                    "end": 1023
                },
                {
                    "start": 1026,
                    "end": 1214
                },
                {
                    "start": 1215,
                    "end": 1306
                },
                {
                    "start": 1307,
                    "end": 1452
                },
                {
                    "start": 1453,
                    "end": 1639
                },
                {
                    "start": 1640,
                    "end": 1760
                },
                {
                    "start": 1761,
                    "end": 1935
                },
                {
                    "start": 1936,
                    "end": 2109
                },
                {
                    "start": 2110,
                    "end": 2228
                },
                {
                    "start": 2231,
                    "end": 2457
                }
            ],
            "ref_mentions": [
                {
                    "start": 157,
                    "end": 160,
                    "matchedPaperCorpusId": "11253972"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.923828125
        },
        {
            "corpus_id": "258715315",
            "title": "Tailoring Instructions to Student\u2019s Learning Levels Boosts Knowledge Distillation",
            "text": "We further discuss some design choices of current methods, including the initialization state of the teacher and the updating order of the teacher and student models. Following (Guo et al., 2022), we apply the entropy gap to evaluate these design choices.\n\nF.1 Impact of the Teacher's initial state While vanilla distillation and meta distillation employ a two-stage training approach, online distillation and LGTM employ a one-stage joint training strategy for the teacher and student models. The key difference is whether to involve fine-tuning the teacher network on target task. In this study, we investigate the impact of the teacher network's state on the student network.\n\nA teacher network initialized in the same state as the student network can maintain the student network's progress at all times, but its capabilities may be relatively weak. In contrast, a converged teacher network has superior performance but also a larger gap, which can prevent the student network from gaining knowledge effectively.\n\nAs show in fig. 5, a lower initial confidence gap between the teacher model and the student model leads to more efficient knowledge transfer. When the initial ability gap is relatively high, it takes more iterations for the student model to catch up to the fine-tuned teacher model. In contrast, when the initial ability gap is lower, a teacher model initialized at the same state as the student model is able to transfer knowledge to the student more quickly. Specifically, in the early stages, the teacher model focuses more on self-evolution than knowledge transfer, causing the entropy gap to increase. Then, the teacher model shifts its focus towards knowledge transfer, resulting in an increasing and then decreasing trend in the entropy gap.",
            "score": 0.6568638609564491,
            "section_title": "F Analysis",
            "char_start_offset": 31755,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.6796875
        },
        {
            "corpus_id": "267688311",
            "title": "Breast Cancer Histopathology Images Classification Through Multi-View Augmented Contrastive Learning and Pre-Learning Knowledge Distillation",
            "text": "Knowledge distillation is a commonly used technique in deep learning, as proposed by Hinton et al. in 2015 [17]. It is used to transfer knowledge from a large model to a smaller one, resulting in model compression and acceleration. The goal of knowledge distillation is to improve the accuracy and generalizability of the student network by transferring knowledge from the teacher network. Auxiliary information during training is usually obtained from the output of the teacher network. Such techniques have been applied in various domains, including natural language processing, computer vision, and speech recognition. An important use case is the deployment of deep learning models on mobile devices, which often have limited computational resources and require smaller, lightweight models. Knowledge distillation typically involves a larger teacher network and a smaller student network. The teacher network is usually trained on large amounts of data and therefore has higher accuracy and more comprehensive knowledge. Compared to the teacher network, the student network is trained on less data and has fewer parameters. Nevertheless, some student networks can surpass the accuracy of the teacher network. In practice, many variations and improved distillation techniques are used. For example, different objective functions, loss functions, and regularization techniques can be used. In addition, merging knowledge from multiple teacher networks into student networks can also be beneficial. To achieve better performance, these techniques can be optimized for specific tasks and data [18]. \n\nKnowledge can be transferred using the output of the last layer via response-based knowledge distillation, which is relatively simple and does not require additional complex computation or design. However, it overlooks the output information of the middle layer, which limits the ability of the student network to fully capture the multi-level features of the teacher network. In addition to using the output of the last layer, the output of the middle layer can also facilitate effective student learning. Consequently, the combination of response-based and feature-based knowledge distillation has become a popular research area [19]. Fitnets [20] propose to use feature maps as output to match the feature maps of teachers and students for better learning. Zagoruyko and Komodakis [21] were inspired by the attention mechanism and proposed Attention Transfer (AT), replacing the original feature maps with attention maps for effective knowledge transfer.",
            "score": 0.6565598663708867,
            "section_title": "II. RELATED WORK A. KNOWLEDGE DISTILLATION",
            "char_start_offset": 7051,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 112
                },
                {
                    "start": 113,
                    "end": 231
                },
                {
                    "start": 232,
                    "end": 389
                },
                {
                    "start": 390,
                    "end": 487
                },
                {
                    "start": 488,
                    "end": 621
                },
                {
                    "start": 622,
                    "end": 794
                },
                {
                    "start": 795,
                    "end": 892
                },
                {
                    "start": 893,
                    "end": 1024
                },
                {
                    "start": 1025,
                    "end": 1127
                },
                {
                    "start": 1128,
                    "end": 1212
                },
                {
                    "start": 1213,
                    "end": 1288
                },
                {
                    "start": 1289,
                    "end": 1391
                },
                {
                    "start": 1392,
                    "end": 1499
                },
                {
                    "start": 1500,
                    "end": 1598
                },
                {
                    "start": 1601,
                    "end": 1797
                },
                {
                    "start": 1798,
                    "end": 1977
                },
                {
                    "start": 1978,
                    "end": 2107
                },
                {
                    "start": 2108,
                    "end": 2237
                },
                {
                    "start": 2238,
                    "end": 2360
                },
                {
                    "start": 2361,
                    "end": 2558
                }
            ],
            "ref_mentions": [
                {
                    "start": 1593,
                    "end": 1597,
                    "matchedPaperCorpusId": "219559263"
                },
                {
                    "start": 2232,
                    "end": 2236,
                    "matchedPaperCorpusId": "232232777"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.9833984375
        },
        {
            "corpus_id": "231925118",
            "title": "Learning Student-Friendly Teacher Networks for Knowledge Distillation",
            "text": "Although our teacher network obtained from the student-aware training procedure is specialized for a specific student model, it is also effective to transfer knowledge to the students models with substantially different architectures. Table 7 shows that the benefit of our method is also preserved well as long as the student branch has similar capacity to the student models, where the model capacity is defined by the achievable accuracy via independent training without distillation. In addition, it presents that larger students branches are often effective to enhance distillation performance while smaller student branches are not always helpful. In summary, these results imply that a teacher network in SFTN trained for a specific architecture of student network has the potential to transfer its knowledge to other types of student networks.",
            "score": 0.6555912156541311,
            "section_title": "Versatility of SFTN",
            "char_start_offset": 21082,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 234
                },
                {
                    "start": 235,
                    "end": 486
                },
                {
                    "start": 487,
                    "end": 652
                },
                {
                    "start": 653,
                    "end": 850
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.77490234375
        },
        {
            "corpus_id": "239016062",
            "title": "FrugalScore: Learning Cheaper, Lighter and Faster Evaluation Metrics for Automatic Text Generation",
            "text": "Knowledge distillation (KD) (Hinton et al., 2015) is the process of transferring knowledge from a large teacher model to a smaller student model to accomplish model compression (Bucilu\u01ce et al., 2006). It was originally proposed in the domain of computer vision and speech recognition, then successfully adapted to NLP (Sanh et al., 2019). Distillation can be accomplished in three ways: (1) offline, where a teacher is first pre-trained, then a student is trained under the guidance of the teacher (Hinton et al., 2015); (2) online, where the student and the teacher are trained simultaneously (Zhang et al., 2018); and (3) self, where the same model plays the role of student and teacher, e.g., transferring the knowledge of a later exit layer into an earlier one of the same multi-exit network (Phuong and Lampert, 2019). Previous studies on KD mainly focused on classification problems (Gou et al., 2021). A few attempts have been made on regression problems (Chen et al., 2017;Saputra et al., 2019;Takamoto et al., 2020), in which special losses were proposed to train the student with respect to both the teacher's regression outputs and ground truth scores. Different from conventional distillation, our work is more similar to data-free KD (Kang and Kang, 2021), where the student is trained in the absence of the dataset used to train the teacher. To transfer knowledge, we first create a synthetic dataset by annotating sequence pairs with a large model (teacher), and then train a miniature model (student) on that dataset, in an offline and regression setting.",
            "score": 0.6555885166958059,
            "section_title": "Knowledge distillation",
            "char_start_offset": 8112,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.953125
        },
        {
            "corpus_id": "267365055",
            "title": "EPSD: Early Pruning with Self-Distillation for Efficient Model Compression",
            "text": "Knowledge Distillation. Knowledge Distillation (KD) transfers various 'knowledge' in networks (Romero et al. 2015;Hinton et al. 2014), acting as a potent regularization method to enhance generalization by utilizing learned softened targets (Shen et al. 2022). However, the capacity gap prevents well-performing teachers from making students better (Mirzadeh et al. 2020). Self-Distillation. To improve the efficiency of knowledge transfer, Self-Distillation (SD) leverages knowledge from the student network without involving additional teachers (Wang and Yoon 2021;Yun et al. 2020). The key to SD is creating soft targets, where the student network generates its valuable knowledge to guide its training (Lee, Hwang, and Shin 2020;Zhang, Bao, and Ma 2021;Yang et al. 2019;Shen et al. 2022). SD's efficiency arises from avoiding teacher network pre-training and addressing teacher-student capacity gaps. Yet, the student network might be over-fitting due to insufficient training regularization (Kim et al. 2021). Recently, PKD (Park and No 2022) revealed the positive regularizing impact of pruning teacher networks on KD, which inspires us to regularize the SD process by pruning. Network Pruning. Network pruning aims to identify and remove unnecessary weights, reducing complexity while preserving training performance (Reed 1993; Lee et al. 2020). Traditional approaches (Han et al. 2015;Molchanov et al. 2017) typically follow pre-training, pruning, and re-training to prune, which requires much training effort. Another paradigm named Dynamic Sparse Training (DST) (Mocanu et al. 2018;Bellec et al. 2018;Evci et al. 2020;Liu et al. 2021b) starts from a (random) sparse neural network and allows the sparse connectivity to evolve dynamically during training. DST can significantly improve the trainability of sparse DNNs without increasing the training FLOPs.",
            "score": 0.6550311222273651,
            "section_title": "Related Works",
            "char_start_offset": 5272,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 23
                },
                {
                    "start": 24,
                    "end": 259
                },
                {
                    "start": 260,
                    "end": 371
                },
                {
                    "start": 372,
                    "end": 390
                },
                {
                    "start": 391,
                    "end": 583
                },
                {
                    "start": 584,
                    "end": 791
                },
                {
                    "start": 792,
                    "end": 903
                },
                {
                    "start": 904,
                    "end": 1013
                },
                {
                    "start": 1014,
                    "end": 1182
                },
                {
                    "start": 1183,
                    "end": 1199
                },
                {
                    "start": 1200,
                    "end": 1352
                },
                {
                    "start": 1353,
                    "end": 1518
                },
                {
                    "start": 1519,
                    "end": 1764
                },
                {
                    "start": 1765,
                    "end": 1865
                }
            ],
            "ref_mentions": [
                {
                    "start": 94,
                    "end": 114,
                    "matchedPaperCorpusId": "4555207"
                },
                {
                    "start": 240,
                    "end": 258,
                    "matchedPaperCorpusId": "247793209"
                },
                {
                    "start": 348,
                    "end": 370,
                    "matchedPaperCorpusId": "238354201"
                },
                {
                    "start": 546,
                    "end": 566,
                    "matchedPaperCorpusId": "215745611"
                },
                {
                    "start": 566,
                    "end": 582,
                    "matchedPaperCorpusId": "214727822"
                },
                {
                    "start": 705,
                    "end": 732,
                    "matchedPaperCorpusId": "220249782"
                },
                {
                    "start": 732,
                    "end": 756,
                    "matchedPaperCorpusId": "232302458"
                },
                {
                    "start": 756,
                    "end": 773,
                    "matchedPaperCorpusId": "54436113"
                },
                {
                    "start": 773,
                    "end": 789,
                    "matchedPaperCorpusId": "247793209"
                },
                {
                    "start": 995,
                    "end": 1012,
                    "matchedPaperCorpusId": "233714221"
                },
                {
                    "start": 1028,
                    "end": 1046,
                    "matchedPaperCorpusId": "238227092"
                },
                {
                    "start": 1376,
                    "end": 1393,
                    "matchedPaperCorpusId": "2238772"
                },
                {
                    "start": 1393,
                    "end": 1415,
                    "matchedPaperCorpusId": "17240902"
                },
                {
                    "start": 1572,
                    "end": 1592,
                    "matchedPaperCorpusId": "49310977"
                },
                {
                    "start": 1628,
                    "end": 1644,
                    "matchedPaperCorpusId": "231839425"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.88134765625
        },
        {
            "corpus_id": "264172838",
            "title": "Learning Comprehensive Representations with Richer Self for Text-to-Image Person Re-Identification",
            "text": "Knowledge distillation (KD) is a well-known technique for transferring knowledge across different networks. This technology was originally proposed for model compression [4], that is, using a lightweight and small model (student) to imitate the output of a heavyweight and large model (teacher), so that this lightweight model inherits the capabilities of the heavyweight model. Hinton et al. [13] proposed to transfer knowledge from teacher network to student network by minimizing the Kullback-Leibler divergence between classification logits produced by two networks. Bengio et al. [33] transferred knowledge by directly minimizing the Mean Square Error (MSE) of the outputs of these two networks. Pork et al. [29] further distilled the mutual relations of samples from teacher model to student model. The above methods [31] focus on learning a lightweight student model from a teacher with the same input data. Recently, some efforts [7,10,16,19,30,44] have tried to learn student models with specific abilities from teacher models with different input data. Gu et al. [10] made the student network with image data as input imitate the output of the teacher network with video data as input, which makes the student network the ability to model temporal knowledge [56][57][58]. Kiran et al. [19]  a holistic student-teacher network that matches the distributions of between-and within-class distances (DCDs) of occluded samples with that of holistic (non-occluded) samples, improving the robustness of the student network to occlusions. Wang et al. [44] proposed to use a teacher model with cleaner knowledge to teach the student model with noisy input the ability to denoising. Inspired by these works, in this work, we try to learn a teacher model with more comprehensive and richer knowledge, and transfer this knowledge to the student network with a single input data to make it possess the ability of multi-view semantic association and reasoning.",
            "score": 0.6542614794321286,
            "section_title": "Knowledge Distillation",
            "char_start_offset": 10208,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 107
                },
                {
                    "start": 108,
                    "end": 378
                },
                {
                    "start": 379,
                    "end": 570
                },
                {
                    "start": 571,
                    "end": 700
                },
                {
                    "start": 701,
                    "end": 804
                },
                {
                    "start": 805,
                    "end": 914
                },
                {
                    "start": 915,
                    "end": 1062
                },
                {
                    "start": 1063,
                    "end": 1281
                },
                {
                    "start": 1282,
                    "end": 1540
                },
                {
                    "start": 1541,
                    "end": 1682
                },
                {
                    "start": 1683,
                    "end": 1956
                }
            ],
            "ref_mentions": [
                {
                    "start": 170,
                    "end": 173,
                    "matchedPaperCorpusId": "11253972"
                },
                {
                    "start": 585,
                    "end": 589,
                    "matchedPaperCorpusId": "2723173"
                },
                {
                    "start": 713,
                    "end": 717,
                    "matchedPaperCorpusId": "131765296"
                },
                {
                    "start": 823,
                    "end": 827,
                    "matchedPaperCorpusId": "252198825"
                },
                {
                    "start": 938,
                    "end": 941,
                    "matchedPaperCorpusId": "253270058"
                },
                {
                    "start": 941,
                    "end": 944,
                    "matchedPaperCorpusId": "199543474"
                },
                {
                    "start": 944,
                    "end": 947,
                    "matchedPaperCorpusId": "210700928"
                },
                {
                    "start": 947,
                    "end": 950,
                    "matchedPaperCorpusId": "233231173"
                },
                {
                    "start": 950,
                    "end": 953,
                    "matchedPaperCorpusId": "220403571"
                },
                {
                    "start": 953,
                    "end": 956,
                    "matchedPaperCorpusId": "257623092"
                },
                {
                    "start": 1073,
                    "end": 1077,
                    "matchedPaperCorpusId": "199543474"
                },
                {
                    "start": 1272,
                    "end": 1276,
                    "matchedPaperCorpusId": "257791338"
                },
                {
                    "start": 1276,
                    "end": 1280,
                    "matchedPaperCorpusId": "225100504"
                },
                {
                    "start": 1295,
                    "end": 1299,
                    "matchedPaperCorpusId": "233231173"
                },
                {
                    "start": 1553,
                    "end": 1557,
                    "matchedPaperCorpusId": "257623092"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.90869140625
        },
        {
            "corpus_id": "248243663",
            "title": "Optimizing Performance of Federated Person Re-identification: Benchmarking and Analysis",
            "text": "Besides client clustering, we adopt knowledge distillation (KD) to elevate performance and improve the convergence of distillation is a method proposed by Hinton et, al. [14] to transfer knowledge from a teacher model to a student model, where the teacher model contains more knowledge than the student model. We adopt knowledge distillation to better transfer knowledge from local models to the global model, regarding clients as teachers and the server as the student. \n\nAfter clients finish local training and upload models, we apply knowledge distillation with a public shared dataset D \u210e in the server. Figure 3b illustrates the additional steps required from knowledge distillation: (1) The server uses each trained model   of client  to generate soft labels3 \u2113  using samples of D \u210e . These soft labels represent the knowledge of clients' models. (2) Apart from model aggregation, the server aggregates these soft labels with \n\nThe server fine-tunes the global model with D \u210e and corresponding labels \u2113 to learn the distilled knowledge.",
            "score": 0.654122952381871,
            "section_title": "Knowledge Distillation",
            "char_start_offset": 18063,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 309
                },
                {
                    "start": 310,
                    "end": 470
                },
                {
                    "start": 473,
                    "end": 607
                },
                {
                    "start": 608,
                    "end": 791
                },
                {
                    "start": 792,
                    "end": 853
                },
                {
                    "start": 854,
                    "end": 932
                },
                {
                    "start": 935,
                    "end": 1043
                }
            ],
            "ref_mentions": [
                {
                    "start": 170,
                    "end": 174,
                    "matchedPaperCorpusId": "7200347"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.96337890625
        },
        {
            "corpus_id": "264492472",
            "title": "Intra- and Inter-Modal Curriculum for Multimodal Learning",
            "text": "Knowledge distillation refers to the transfer of knowledge from teacher models to student ones. Since it is proposed for the goal of model compression [10], teacher models are usually large-scale, ensembled and pretrained, while students are relatively small and fast, which is named offline distillation [23,32,57,68,81]. There are also methods of online distillation [3,82], where teacher and student models are trained simultaneously, and self-distillation [15,45], where teacher and student models are the same. Apart from the training strategy, the form of knowledge is another important component [24], which can be categorized into response-based [32], feature-based [56] and relation-based [75]. In this paper, we adopt offline distillation and feature-based knowledge.",
            "score": 0.6540412226687191,
            "section_title": "Knowledge Distillation",
            "char_start_offset": 9458,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 95
                },
                {
                    "start": 96,
                    "end": 322
                },
                {
                    "start": 323,
                    "end": 515
                },
                {
                    "start": 516,
                    "end": 703
                },
                {
                    "start": 704,
                    "end": 777
                }
            ],
            "ref_mentions": [
                {
                    "start": 151,
                    "end": 155,
                    "matchedPaperCorpusId": "11253972"
                },
                {
                    "start": 305,
                    "end": 309,
                    "matchedPaperCorpusId": "4110009"
                },
                {
                    "start": 315,
                    "end": 318,
                    "matchedPaperCorpusId": "229363322"
                },
                {
                    "start": 372,
                    "end": 375,
                    "matchedPaperCorpusId": "26071966"
                },
                {
                    "start": 460,
                    "end": 464,
                    "matchedPaperCorpusId": "233296717"
                },
                {
                    "start": 464,
                    "end": 467,
                    "matchedPaperCorpusId": "236034189"
                },
                {
                    "start": 603,
                    "end": 607,
                    "matchedPaperCorpusId": "219559263"
                },
                {
                    "start": 698,
                    "end": 702,
                    "matchedPaperCorpusId": "206596723"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.93798828125
        },
        {
            "corpus_id": "237250417",
            "title": "BERT Learns to Teach: Knowledge Distillation with Meta Learning",
            "text": "We present Knowledge Distillation with Meta Learning (MetaDistil), a simple yet effective alternative to traditional knowledge distillation (KD) methods where the teacher model is fixed during training. We show the teacher network can learn to better transfer knowledge to the student network (i.e., learning to teach) with the feedback from the performance of the distilled student network in a meta learning framework. Moreover, we introduce a pilot update mechanism to improve the alignment between the inner-learner and meta-learner in meta learning algorithms that focus on an improved inner-learner. Experiments on various benchmarks show that MetaDistil can yield significant improvements compared with traditional KD algorithms and is less sensitive to the choice of different student capacity and hyperparameters, facilitating the use of KD on different tasks and models.",
            "score": 0.6535541654328529,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.87841796875
        },
        {
            "corpus_id": "236782048",
            "title": "Research on the Theory and Application of Deep Interactive Learning",
            "text": "In recent years, the deep learning model has been widely used in many fields, and has a good performance in solving complex problems. However, these models usually need huge memory space and powerful computing power support, so it is difficult to lay out the models on edge devices with limited resources. Knowledge distillation is considered as an effective method to solve this problem. \n\nKnowledge distillation [1] is a famous technology for learning compact deep neural network model with competitive accuracy, in which smaller networks (students) are trained to mimic the larger networks (teachers) with higher accuracy. The popularity of knowledge distillation is mainly due to its simplicity and universality. The student model based on teacher learning is straightforward, and there are no restrictions on the network architecture of these two models. Now most scholars mainly study how to transfer knowledge from a trained teacher model to a student model more effectively. Such as Parameter quantization or binarization [2], pruning [3] [4] [5] and knowledge distillation [1] are representative methods in this research field. As a solution, KD has always been an active research field, which improves the capacity of lightweight networks by taking the knowledge of large pre-training networks (or small network sets) as teachers' networks. Knowledge distillation is usually divided into two different methods: online distillation and offline distillation. Off-line KD method usually requires a high-capacity teacher model has been well trained to perform unidirectional transmission [6] [7] [8] [9] [10]. However,Training a \"good\" teacher has become the main problem of offline distillation. The performance of student network will decrease when the difference between teachers' and students' capacity is huge.Besides, many parameters and the high calculation cost have been required in the twostage KD. In order to overcome these difficulties, some works pay more attention to online KD, which trains a group of students at the same time by learning their peers' predictions. In the field of image classification, Deep mutual learning (DML) [11] and on-the-fly native ensemble (one) [12] are typical on-line distillation methods and have achieved good model results.",
            "score": 0.6534348527546741,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 133
                },
                {
                    "start": 134,
                    "end": 305
                },
                {
                    "start": 306,
                    "end": 388
                },
                {
                    "start": 391,
                    "end": 625
                },
                {
                    "start": 626,
                    "end": 716
                },
                {
                    "start": 717,
                    "end": 859
                },
                {
                    "start": 860,
                    "end": 982
                },
                {
                    "start": 983,
                    "end": 1136
                },
                {
                    "start": 1137,
                    "end": 1350
                },
                {
                    "start": 1351,
                    "end": 1466
                },
                {
                    "start": 1467,
                    "end": 1615
                },
                {
                    "start": 1616,
                    "end": 1702
                },
                {
                    "start": 1703,
                    "end": 1914
                },
                {
                    "start": 1915,
                    "end": 2087
                },
                {
                    "start": 2088,
                    "end": 2278
                }
            ],
            "ref_mentions": [
                {
                    "start": 1594,
                    "end": 1597,
                    "matchedPaperCorpusId": "118649278"
                },
                {
                    "start": 1602,
                    "end": 1605,
                    "matchedPaperCorpusId": "108290228"
                },
                {
                    "start": 1610,
                    "end": 1614,
                    "matchedPaperCorpusId": "3608236"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.92138671875
        },
        {
            "corpus_id": "274982216",
            "title": "Lightweight Design and Optimization methods for DCNNs: Progress and Futures",
            "text": "Knowledge distillation was initially proposed by Bucilu\u01ce et al. [3], aiming to create a compressed model by training a strong classifier with pseudo-labeled data, which can replicate the output of the original classifier. What makes knowledge distillation unique is its use of two types of networks: the teacher model and the student model. The teacher model is typically a pre-trained, large neural network model with superior performance. As shown in Figure 9, the softmax layer output of the teacher model is used as the soft target, along with the softmax layer output of the student model as the hard target, both fed into the total loss calculation to guide the training of the student model. This transfers the knowledge from the teacher model to the student model, enabling the student model to achieve performance comparable to that of the teacher model while being more compact and efficient, thus achieving the purpose of model compression. \n\nHinton et al. [16] proposed using a high-temperature softmax to generate soft target distributions of class probabilities, and then used these soft targets to train smaller models, enabling them to be trained with less data and higher learning rates. The core idea is to train a smaller model to fit the output probability distribution (soft labels) of the teacher model, rather than directly fitting the hard labels of the teacher model. Yim et al. [53] proposed a novel knowledge transfer technique that achieves knowledge transfer by distilling and transferring the knowledge of a pretrained deep neural network to another one. The distilled knowledge is defined as the flow between layers by calculating the inner product of features at two levels. Chen et al. [8] used the ranking relationships of different samples within a certain class from the teacher model as learning information to be transmitted to the student model. Czarnecki et al. [9] introduced a neural network method called Sobolev training, which aims to train neural networks not only using target values but also utilizing the derivative information of the target output with respect to the input. Zhou et al. [57] proposed a general training framework called \"Rocket Launching\" for training lightweight models. This framework utilizes a large \"booster\" network to supervise the learning of the lightweight network throughout the training process.",
            "score": 0.6529170374161013,
            "section_title": "Knowledge Distillation",
            "char_start_offset": 51330,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 221
                },
                {
                    "start": 222,
                    "end": 340
                },
                {
                    "start": 341,
                    "end": 440
                },
                {
                    "start": 441,
                    "end": 698
                },
                {
                    "start": 699,
                    "end": 951
                },
                {
                    "start": 954,
                    "end": 1204
                },
                {
                    "start": 1205,
                    "end": 1392
                },
                {
                    "start": 1393,
                    "end": 1584
                },
                {
                    "start": 1585,
                    "end": 1706
                },
                {
                    "start": 1707,
                    "end": 1884
                },
                {
                    "start": 1885,
                    "end": 2124
                },
                {
                    "start": 2125,
                    "end": 2238
                },
                {
                    "start": 2239,
                    "end": 2374
                }
            ],
            "ref_mentions": [
                {
                    "start": 64,
                    "end": 67,
                    "matchedPaperCorpusId": "11253972"
                },
                {
                    "start": 1404,
                    "end": 1408,
                    "matchedPaperCorpusId": "206596723"
                },
                {
                    "start": 1719,
                    "end": 1722,
                    "matchedPaperCorpusId": "19207026"
                },
                {
                    "start": 1902,
                    "end": 1905,
                    "matchedPaperCorpusId": "21596346"
                },
                {
                    "start": 2137,
                    "end": 2141,
                    "matchedPaperCorpusId": "3913636"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.96923828125
        },
        {
            "corpus_id": "236782270",
            "title": "Adaptive Teacher Finetune: Towards high-performance knowledge distillation through adaptive fine-tuning",
            "text": "Knowledge distillation is a widely used method to transfer knowledge from a large model to a small model. Traditional methods use pre-trained large models to supervise the training of small models, called Offline Knowledge Distillation, However, the structural gap between teachers and students limits its performance. After that, Oneline Knowledge Distillation retrained the teacher-student network from the beginning and the method of echo teaching greatly improved the performance. But there is very little work to explore the difference between the two. In this paper, we first point out that the essential difference between Offline and Oneline Knowledge Distillation is actually whether the weight of the teacher-student network has a process of mutual adaptation. If they adopt the teacher network and the student network jointly train to implement Offline Knowledge Distillation, there is no obvious difference in the final performance, no matter whether it is a joint distillation training. This shows that teacher-student network adaptation is important for Knowledge Distillation. Then, we propose an Adaptive Teacher Finetune (ATF) to adapt the teacher model to the student network. It will use student model information for Tinetune during the Offline Knowledge Distillation process. With normalized logical distribution and alpha-divergence, the performance improvement of ATF clearly exceeds the existing Offline and Oneline Knowledge Distillation method. Extensive experiments conducted on cifar and ImageNet support our aforementioned analysis and conclusions. With the newly introduced ATF, we obtained state-of-the-art performance on ResNet 18 on ImageNet.",
            "score": 0.6528064463123484,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.859375
        },
        {
            "corpus_id": "238583178",
            "title": "Extreme Low Resolution Activity Recognition with Confident Spatial-Temporal Attention Transfer",
            "text": "Teacher-Student (TS) learning, as a distillation method, has been widely used. These algorithms could be classified by the number of stages, most of which are divided into two parts. Methods with less or more than two stages have also been proposed. On one hand, TS learning can be a one-stage strategy [30] [31] [32]. Zhang et al. [30] proposed that a group of untrained student networks with the same structure can be exploited to learn the target task simultaneously, instead of using the traditional two-stage knowledge extraction strategy. For another, a series of researches [31] [32] focused on TS learning methods in more than two stages. In [32], multiple teacher assistant networks were used to transfer teachers' knowledge more easily and effectively. The teacher network first transfers its knowledge to an auxiliary network and then spreads the distilled knowledge to the final student network. \n\nTypically, TS learning is a two-stage method where the teacher network is trained first, then the student network is trained with extra information provided by the teacher network. Hinton et al [33] pioneered the concept of knowledge distillation, in which soft targets with more information obtained from the teacher network can help students learn better. Recently, many works have been improved in the way of information dissemination or through the optimization of strict control on the dissemination of information [12] [34] [35] [36] [37] [38] [39]. For example, Peng et al [34] proposed a student network, which not only focuses on imitating the teacher at the instance level but also learns the embedding space, so that the student network could perform better in feature extraction. In addition, there are works studying the role of different teacher networks [40] [41] [42]. For example, [42] used Neural Architecture Search to obtain experience about the architecture from different teacher networks. In addition to image classification tasks, TS learning can also be used in many other different fields, such as face recognition [43], visual problem solving [44], video tasks [45], etc. In summary, teacher networks always have a more complex and deeper network structure to empower the performance of extracting features, while our work enhances the ability of teacher networks by using more informative input.",
            "score": 0.6526340719930648,
            "section_title": "B. Teacher-Student Learning",
            "char_start_offset": 8968,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 78
                },
                {
                    "start": 79,
                    "end": 182
                },
                {
                    "start": 183,
                    "end": 249
                },
                {
                    "start": 250,
                    "end": 318
                },
                {
                    "start": 319,
                    "end": 544
                },
                {
                    "start": 545,
                    "end": 646
                },
                {
                    "start": 647,
                    "end": 762
                },
                {
                    "start": 763,
                    "end": 907
                },
                {
                    "start": 910,
                    "end": 1090
                },
                {
                    "start": 1091,
                    "end": 1267
                },
                {
                    "start": 1268,
                    "end": 1465
                },
                {
                    "start": 1466,
                    "end": 1701
                },
                {
                    "start": 1702,
                    "end": 1794
                },
                {
                    "start": 1795,
                    "end": 1921
                },
                {
                    "start": 1922,
                    "end": 2108
                },
                {
                    "start": 2109,
                    "end": 2333
                }
            ],
            "ref_mentions": [
                {
                    "start": 303,
                    "end": 307,
                    "matchedPaperCorpusId": "26071966"
                },
                {
                    "start": 313,
                    "end": 317,
                    "matchedPaperCorpusId": "212908749"
                },
                {
                    "start": 332,
                    "end": 336,
                    "matchedPaperCorpusId": "26071966"
                },
                {
                    "start": 581,
                    "end": 585,
                    "matchedPaperCorpusId": "54436113"
                },
                {
                    "start": 650,
                    "end": 654,
                    "matchedPaperCorpusId": "212908749"
                },
                {
                    "start": 1430,
                    "end": 1434,
                    "matchedPaperCorpusId": "829159"
                },
                {
                    "start": 1440,
                    "end": 1444,
                    "matchedPaperCorpusId": "2723173"
                },
                {
                    "start": 1450,
                    "end": 1454,
                    "matchedPaperCorpusId": "198179476"
                },
                {
                    "start": 1460,
                    "end": 1464,
                    "matchedPaperCorpusId": "131765296"
                },
                {
                    "start": 1490,
                    "end": 1494,
                    "matchedPaperCorpusId": "102483463"
                },
                {
                    "start": 1779,
                    "end": 1783,
                    "matchedPaperCorpusId": "11536917"
                },
                {
                    "start": 1789,
                    "end": 1793,
                    "matchedPaperCorpusId": "208513309"
                },
                {
                    "start": 1808,
                    "end": 1812,
                    "matchedPaperCorpusId": "208513309"
                },
                {
                    "start": 2051,
                    "end": 2055,
                    "matchedPaperCorpusId": "219442497"
                },
                {
                    "start": 2080,
                    "end": 2084,
                    "matchedPaperCorpusId": "54039301"
                },
                {
                    "start": 2098,
                    "end": 2102,
                    "matchedPaperCorpusId": "131774110"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.8564453125
        },
        {
            "corpus_id": "255546282",
            "title": "Designing an improved deep learning-based model for COVID-19 recognition in chest X-ray images: a knowledge distillation approach",
            "text": "To distill knowledge from the teacher to the student, a weighted average (mean) is necessary. The initial objective function is Cross-Entropy with soft targets, which is calculated through the softmax function in the smaller network using a higher temperature. Soft targets are generated by a larger architecture or network. The second objective function is Cross-Entropy with valid labels, which utilizes the softmax output from the student model with a temperature of zero. \n\nThe teacher network and the student network start receiving training data in parallel. The teacher model incorporates a softmax function with temperature in its output. In contrast, the student model generates two distinct outputs. The first output is softmax with temperature, while the second output consists of the standard softmax. The purpose of the student model is to produce softened probabilities, which correspond to the output of the teacher model. The loss of knowledge distillation is calculated using the following formula: 2 ( . ) \n\n(1 ) ( . ) \n\nHere , p and q denote probabilities generated by the student and teacher networks in a specific temperature (T), respectively, and KL denotes the Kullback-Leibler divergence, that measures the level of distinction between two probabilistic distributions. The Cross-Entropy of the student model with T=1 is (LWs.x). \n\nAccording to [38], \uf061 and T are hyperparameters where the greater the value of \uf061 , the better the learning experience for the student model. \n\nDuring the distillation phase, back-propagation should only be performed in the student network, as the teacher network has already fine-tuned its parameters. The transfer of knowledge from the teacher to the student model takes place throughout the distillation procedure. It is worth noting that the student model can be trained at a faster rate compared to the teacher model. For further details on the distillation procedure, please refer to [39]. The knowledge distillation procedure is illustrated in Figure 2.",
            "score": 0.6523814923856947,
            "section_title": "Hinton et al. pioneered Knowledge Distillation in",
            "char_start_offset": 18494,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 93
                },
                {
                    "start": 94,
                    "end": 260
                },
                {
                    "start": 261,
                    "end": 324
                },
                {
                    "start": 325,
                    "end": 475
                },
                {
                    "start": 478,
                    "end": 564
                },
                {
                    "start": 565,
                    "end": 646
                },
                {
                    "start": 647,
                    "end": 709
                },
                {
                    "start": 710,
                    "end": 813
                },
                {
                    "start": 814,
                    "end": 937
                },
                {
                    "start": 938,
                    "end": 1023
                },
                {
                    "start": 1026,
                    "end": 1036
                },
                {
                    "start": 1039,
                    "end": 1293
                },
                {
                    "start": 1294,
                    "end": 1353
                },
                {
                    "start": 1356,
                    "end": 1495
                },
                {
                    "start": 1498,
                    "end": 1656
                },
                {
                    "start": 1657,
                    "end": 1771
                },
                {
                    "start": 1772,
                    "end": 1876
                },
                {
                    "start": 1877,
                    "end": 1949
                },
                {
                    "start": 1950,
                    "end": 2014
                }
            ],
            "ref_mentions": [
                {
                    "start": 1944,
                    "end": 1948,
                    "matchedPaperCorpusId": "219559263"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.9619140625
        },
        {
            "corpus_id": "235293343",
            "title": "Dense Feature Fusion for Online Mutual Knowledge Distillation",
            "text": "Traditional Knowledge Distillation [4] is one of the most effective solutions to compress a cumbersome model or an ensemble of models into a smaller model. The rationale behind is taking advantage of extra supervision provided by the teacher model during training the target model, beyond a conventional supervised learning objective such as the cross-entropy loss subject to the training data labels. In [4], Hinton firstly introduce the process of transferring the knowledge from a high-capacity teacher model to a compact student model as \"distillation\", which is accomplished by aligning the soft output prediction between the teacher and the student. Although knowledge distillation shows good performance in compressing models for deployment, it usually follows a two-stage training solution, that is, pre training a high-quality teacher model to transfer knowledge to a compact student model, which requires more training time and computational cost. Besides probability distribution, some other researches have tried to distill various features to the student [11,[13][14][15][16][17]. In terms of training a small student network for the model efficiency, KD is also considered as an one of model compression methods such as pruning and quantization. However, the traditional knowledge distillation also has the following disadvantages: it needs to train a strong teacher model in advance, and only uses the information of the output.",
            "score": 0.6522488000369968,
            "section_title": "Traditional Knowledge Distillation",
            "char_start_offset": 6925,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 155
                },
                {
                    "start": 156,
                    "end": 401
                },
                {
                    "start": 402,
                    "end": 655
                },
                {
                    "start": 656,
                    "end": 957
                },
                {
                    "start": 958,
                    "end": 1093
                },
                {
                    "start": 1094,
                    "end": 1259
                },
                {
                    "start": 1260,
                    "end": 1443
                }
            ],
            "ref_mentions": [
                {
                    "start": 1072,
                    "end": 1076,
                    "matchedPaperCorpusId": "9433631"
                },
                {
                    "start": 1084,
                    "end": 1088,
                    "matchedPaperCorpusId": "48352434"
                },
                {
                    "start": 1088,
                    "end": 1092,
                    "matchedPaperCorpusId": "201058657"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.970703125
        },
        {
            "corpus_id": "4110009",
            "title": "Born Again Neural Networks",
            "text": "Knowledge distillation (KD) consists of transferring knowledge from one machine learning model (the teacher}) to another (the student). Commonly, the teacher is a high-capacity model with formidable performance, while the student is more compact. By transferring knowledge, one hopes to benefit from the student's compactness. %we desire a compact model with performance close to the teacher's. We study KD from a new perspective: rather than compressing models, we train students parameterized identically to their teachers. Surprisingly, these {Born-Again Networks (BANs), outperform their teachers significantly, both on computer vision and language modeling tasks. Our experiments with BANs based on DenseNets demonstrate state-of-the-art performance on the CIFAR-10 (3.5%) and CIFAR-100 (15.5%) datasets, by validation error. Additional experiments explore two distillation objectives: (i) Confidence-Weighted by Teacher Max (CWTM) and (ii) Dark Knowledge with Permuted Predictions (DKPP). Both methods elucidate the essential components of KD, demonstrating a role of the teacher outputs on both predicted and non-predicted classes. We present experiments with students of various capacities, focusing on the under-explored case where students overpower teachers. Our experiments show significant advantages from transferring knowledge between DenseNets and ResNets in either direction.",
            "score": 0.6518304992602395,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.89013671875
        },
        {
            "corpus_id": "212908749",
            "title": "Improved Knowledge Distillation via Teacher Assistant",
            "text": "We discuss in this section related literature in knowledge distillation and neural network compression. \n\nModel Compression. Since our goal is to train a small, yet accurate network, this work is related to model compression. There has been an interesting line of research that compresses a large network by reducing the connections based on weight magnitudes (Han, Mao, and Dally 2016;Li et al. 2016) or importance scores (Yu et al. 2018). The reduced network is fine-tuned on the same dataset to retain its accuracy. Another line of research focuses on distilling the original (large) network to a smaller network (Polino, Pascanu, and Alistarh 2018;Wang et al. 2018a), in which case the smaller network is more flexible in its architecture design does not have to be a sub-graph of the original network. \n\nKnowledge Distillation. Originally proposed by Bucila, Caruana, and Niculescu-Mizil (2006) and popularized by Hinton, Vinyals, and Dean (2015) knowledge distillation compress the knowledge of a large and computational expensive model (often an ensemble of neural networks) to a single computational efficient neural network. The idea of knowledge distillation is to train the small model, the student, on a transfer set with soft targets provided by the large model, the teacher. Since then, knowledge distillation has been widely adopted in a variety of learning tasks (Yim et al. 2017;Yu et al. 2017;Schmitt et al. 2018;Chen et al. 2017). Adversarial methods also have been utilized for modeling knowledge transfer between teacher and student (Heo et al. 2018;Xu, Hsu, and Huang 2018;Wang et al. 2018b;2018c). \n\nThere have been works studying variants of model distillation that involve multiple networks learning at the same time. Romero et al. (2014) proposed to transfer the knowledge using not only the logit layer but earlier ones too. To cope with the difference in width, they suggested a regressor to connect teacher and student's intermediate layers. Unfortunately, there is not a principled way to do this.",
            "score": 0.651482069112123,
            "section_title": "Related Work",
            "char_start_offset": 2841,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 103
                },
                {
                    "start": 106,
                    "end": 124
                },
                {
                    "start": 125,
                    "end": 225
                },
                {
                    "start": 226,
                    "end": 440
                },
                {
                    "start": 441,
                    "end": 518
                },
                {
                    "start": 519,
                    "end": 806
                },
                {
                    "start": 809,
                    "end": 832
                },
                {
                    "start": 833,
                    "end": 1133
                },
                {
                    "start": 1134,
                    "end": 1288
                },
                {
                    "start": 1289,
                    "end": 1449
                },
                {
                    "start": 1450,
                    "end": 1620
                },
                {
                    "start": 1623,
                    "end": 1742
                },
                {
                    "start": 1743,
                    "end": 1851
                },
                {
                    "start": 1852,
                    "end": 1970
                },
                {
                    "start": 1971,
                    "end": 2027
                }
            ],
            "ref_mentions": [
                {
                    "start": 360,
                    "end": 386,
                    "matchedPaperCorpusId": "2134321"
                },
                {
                    "start": 423,
                    "end": 439,
                    "matchedPaperCorpusId": "829159"
                },
                {
                    "start": 616,
                    "end": 652,
                    "matchedPaperCorpusId": "3323727"
                },
                {
                    "start": 856,
                    "end": 899,
                    "matchedPaperCorpusId": "11253972"
                },
                {
                    "start": 919,
                    "end": 951,
                    "matchedPaperCorpusId": "7200347"
                },
                {
                    "start": 1379,
                    "end": 1396,
                    "matchedPaperCorpusId": "206596723"
                },
                {
                    "start": 1396,
                    "end": 1411,
                    "matchedPaperCorpusId": "829159"
                },
                {
                    "start": 1411,
                    "end": 1431,
                    "matchedPaperCorpusId": "263861232"
                },
                {
                    "start": 1431,
                    "end": 1448,
                    "matchedPaperCorpusId": "29308926"
                },
                {
                    "start": 1571,
                    "end": 1595,
                    "matchedPaperCorpusId": "4916078"
                },
                {
                    "start": 1595,
                    "end": 1613,
                    "matchedPaperCorpusId": "53976534"
                },
                {
                    "start": 1613,
                    "end": 1619,
                    "matchedPaperCorpusId": "19182852"
                },
                {
                    "start": 1743,
                    "end": 1763,
                    "matchedPaperCorpusId": "3323727"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.94384765625
        },
        {
            "corpus_id": "227231666",
            "title": "Knowledge Base Embedding By Cooperative Knowledge Distillation",
            "text": "Knowledge distillation has been initially designed to distill the function approximated by a powerful ensemble of models playing the role of teacher, to a simpler single model playing the role of student (Bucila et al., 2006). This idea has given recently rise to an increasing attention for distilling the generalization ability from a large and easy-to train network model to a small but harder to train network (Adriana et al., 2015). The general framework relies on training a teacher first and then uses a teacher outputs in the form of posterior class probabilities to train the student model such as it mimics the teacher by providing similar outputs. Knowledge distillation has been widely used in NLP tasks to distill large models into small models (Mou et al., 2016) or ensembles of models into single models (Liu Yijia, 2018;Liu Xiaodong, 2019;Kevin Clark, 2019). Mou et al. (Mou et al., 2016) addressed the problem of distilling word embeddings in NLP applications. They proposed a supervised encoding approach to distill taskspecific knowledge from cumbersome word embeddings. The approach has been shown to be effective in sentiment analysis and relation classification. Clark et. al (Kevin Clark, 2019) rather distill knowledge from single-task teacher models to multi-task student models. Their work extends born-again networks (Tommaso Furlanello, 2018) to the multi-task setting. The authors mainly rely on the teacher annealing technique, which consists in mixing the teacher prediction with the ground truth label during training. This strategy allows the student surpassing the teacher. The method has shown good performance in various NLP tasks including textual entailment, question-answering and paraphrase. In all of these works, distillation is applied on a pair of models that statically play either the role of teacher or student. In contrast, we adopt a mutual learning approach proposed in computer vision  where a set of models dynamically play the role of teacher-student. However, unlike the learning framework proposed in , we rather propose a cooperative learning over different learning tasks with associated respective ground truths and where knowledge distillation enables communication between models via shared data characteristics. Beyond, to the best of our",
            "score": 0.6496273146072769,
            "section_title": "Knowledge Distillation",
            "char_start_offset": 8111,
            "sentence_offsets": [],
            "ref_mentions": [
                {
                    "start": 204,
                    "end": 225,
                    "matchedPaperCorpusId": "11253972"
                },
                {
                    "start": 414,
                    "end": 436,
                    "matchedPaperCorpusId": "2723173"
                },
                {
                    "start": 758,
                    "end": 776,
                    "matchedPaperCorpusId": "8350200"
                },
                {
                    "start": 819,
                    "end": 836,
                    "matchedPaperCorpusId": "44066484"
                },
                {
                    "start": 836,
                    "end": 855,
                    "matchedPaperCorpusId": "59523594"
                },
                {
                    "start": 875,
                    "end": 904,
                    "matchedPaperCorpusId": "8350200"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.9658203125
        },
        {
            "corpus_id": "231755639",
            "title": "Differential Evolution Based Layer-Wise Weight Pruning for Compressing Deep Neural Networks",
            "text": "Knowledge distillation can be regarded as a transfer learning method where knowledge from a trained large model (teacher model) is extracted and transferred to a tiny model (student model). Hinton et al. introduced the concept of knowledge in [30] by learning about soft goals. Moreover, they used a temperature parameter to control the soft level of the probability distribution. Romero et al. [31] introduce intermediate layer hits to improve the model. The core idea is to enable the student model to learn the intermediate representation from the teacher model. In [32], Yim et al. argue that the relationship between layers is a better representation of the knowledge than the model output. Therefore, they calculate the flow of solution procedure (FSP) matrix to represent the relationship between layers and transfer the FSP matrix of teacher model to student model.",
            "score": 0.6494886014239193,
            "section_title": "Knowledge Distillation",
            "char_start_offset": 10791,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 189
                },
                {
                    "start": 190,
                    "end": 277
                },
                {
                    "start": 278,
                    "end": 380
                },
                {
                    "start": 381,
                    "end": 455
                },
                {
                    "start": 456,
                    "end": 565
                },
                {
                    "start": 566,
                    "end": 695
                },
                {
                    "start": 696,
                    "end": 873
                }
            ],
            "ref_mentions": [
                {
                    "start": 243,
                    "end": 247,
                    "matchedPaperCorpusId": "7200347"
                },
                {
                    "start": 569,
                    "end": 573,
                    "matchedPaperCorpusId": "206596723"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.8134765625
        },
        {
            "corpus_id": "262012570",
            "title": "One-stage Modality Distillation for Incomplete Multimodal Learning",
            "text": "The knowledge distillation aims to transfer the representation capability of a large model (teacher) to a small one (student) to improve its performance [9]. Generally, transferred knowledge can be divided into three types: responsebased knowledge, representation-based knowledge, and relation-based knowledge. Response-based knowledge refers to the neural response of the last output layer of the teacher model and it is usually transferred to the student network by matching the soft output distribution of teacher and student networks [11]. While the idea of response-based knowledge is straightforward and easy to understand, it only considers the output of the last layer and thus fails to address the intermediate-level supervision from the teacher model [9]. Therefore some researchers propose the representation-based methods to model the knowledge of intermediate feature maps and transfer it to the student network by minimizing the discrepancy between the value [40], attention map [14,20], and attention project [38] of their feature maps. Because the representation-based knowledge only considers specific layers in the teacher model, researchers further introduce relation-based knowledge to model the relationships between different layers or data samples. This knowledge is transferred from the teacher network to the student network by minimizing the discrepancy between the similarity map [47], distribution [37], inter-data relations [22] of feature pairs from different layers or samples.",
            "score": 0.6494298004916406,
            "section_title": "Knowledge Distillation",
            "char_start_offset": 8671,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 157
                },
                {
                    "start": 158,
                    "end": 310
                },
                {
                    "start": 311,
                    "end": 543
                },
                {
                    "start": 544,
                    "end": 765
                },
                {
                    "start": 766,
                    "end": 1051
                },
                {
                    "start": 1052,
                    "end": 1271
                },
                {
                    "start": 1272,
                    "end": 1508
                }
            ],
            "ref_mentions": [
                {
                    "start": 153,
                    "end": 156,
                    "matchedPaperCorpusId": "219559263"
                },
                {
                    "start": 761,
                    "end": 764,
                    "matchedPaperCorpusId": "219559263"
                },
                {
                    "start": 997,
                    "end": 1000,
                    "matchedPaperCorpusId": "829159"
                },
                {
                    "start": 1407,
                    "end": 1411,
                    "matchedPaperCorpusId": "198179476"
                },
                {
                    "start": 1426,
                    "end": 1430,
                    "matchedPaperCorpusId": "52012952"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.92626953125
        },
        {
            "corpus_id": "228375065",
            "title": "ADD: Augmented Disentanglement Distillation Framework for Improving Stock Trend Forecasting",
            "text": "Knowledge distillation usually utilizes a teacher-student strategy, which targets at transferring the knowledge from a strong teacher model to a student model [4,1,13]. Previous purpose of knowledge distillation lies on model compression, which intends to reduce the large model space of a teacher model to a small student model while trying best to keep the performance. Techniques include soft label matching [13], adding random perturbations into soft label [28], hidden layer approximating [27] and attention weight mapping [35]. \n\nThe performance of student model is always worse than the teacher model, self-distillation is proposed to achieve comparable or even better performance than teacher model, which distills the knowledge between teacher and student models in an identical architecture [34,10,36,17]. We adopt selfdistillation but differ from them in two key points. First, we propose a dynamic self-distillation process by introducing some weights to leverage the knowledge from teacher model and the information in raw data. Second, our distillation method is performed with disentanglement network, which not only improves the model performance, but also assists in eliminating interference factors.",
            "score": 0.6491528629708165,
            "section_title": "Knowledge Distillation",
            "char_start_offset": 25143,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 168
                },
                {
                    "start": 169,
                    "end": 371
                },
                {
                    "start": 372,
                    "end": 533
                },
                {
                    "start": 536,
                    "end": 815
                },
                {
                    "start": 816,
                    "end": 881
                },
                {
                    "start": 882,
                    "end": 1041
                },
                {
                    "start": 1042,
                    "end": 1217
                }
            ],
            "ref_mentions": [
                {
                    "start": 159,
                    "end": 162,
                    "matchedPaperCorpusId": "11253972"
                },
                {
                    "start": 162,
                    "end": 164,
                    "matchedPaperCorpusId": "11536917"
                },
                {
                    "start": 801,
                    "end": 805,
                    "matchedPaperCorpusId": "206596723"
                },
                {
                    "start": 808,
                    "end": 811,
                    "matchedPaperCorpusId": "159041406"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.9208984375
        },
        {
            "corpus_id": "264323617",
            "title": "Deep Learning-Based Eye Gaze Estimation for Automotive Applications Using Knowledge Distillation",
            "text": "The compression of a DNN is common practice for obtaining a lightweight network for low-cost and resource-scarce hardware devices. Knowledge distillation is a particular method that involves training to transfer knowledge from a larger network to a different one that has a significantly smaller size. Bucilua et al. [31] in their important paper successfully demonstrated for the first time that the knowledge acquired by a large ensemble of models can be transferred to a single small model. The aim of using this method is to train the student network so that it can reproduce the performance of the teacher network, but with fewer resources. \n\nThe process of applying knowledge distillation consists of the following main stages: (1) definition of teacher and student networks, (2) training of the teacher network, and (3) training of the student network with knowledge transfer from the teacher network. They are detailed below.",
            "score": 0.6490433136974237,
            "section_title": "III. METHODOLOGY A. KNOWLEDGE DISTILLATION",
            "char_start_offset": 22290,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 130
                },
                {
                    "start": 131,
                    "end": 301
                },
                {
                    "start": 302,
                    "end": 493
                },
                {
                    "start": 494,
                    "end": 645
                },
                {
                    "start": 648,
                    "end": 908
                },
                {
                    "start": 909,
                    "end": 933
                }
            ],
            "ref_mentions": [
                {
                    "start": 317,
                    "end": 321,
                    "matchedPaperCorpusId": "11253972"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.9775390625
        },
        {
            "corpus_id": "244714888",
            "title": "Improved Knowledge Distillation via Adversarial Collaboration",
            "text": "Knowledge Distillation. Knowledge distillation is mainly applied to the field of model compression [12,30,44,26], which trains a small and effective student model with the help of large teacher models' knowledge. As a pioneer work, Hinton et al. [15] take output probabilities as knowledge, and the student learns these knowledge by minimizing Kullback-Leibler (KL) divergence. After that, Romero et al. [33] exploit the intermediate representations' knowledge by matching the feature activations of the teacher and the student. Besides, Zagoruyko et al. [48] introduce attention mechanism to the knowledge transfer, and improve the student's performance by mimicking attention feature map. Recently, some works define the knowledge according to the relation between layers or samples. Specifically, Tung et al. [39] urge the student to learn how to preserve the pairwise similarities. Tian et al. [38] introduce contrastive objectives by constructing positive and negative pairs. However, most existing works ignore the capacity gap between the teacher and the student, which may result in performance degradation [5,19]. To address this problem, Cho et al. [5] reduce the gap via an earlystopped teacher, and Mirzadeh et al. [28] introduce teacher assistants to simplify the teacher's knowledge. Unlike these methods, in this paper we bridge the gap from the perspective of improving the student's capacity. \n\nCollaborative Learning. Collaborative learning refers to two or more learners work together to solve a common task [8]. For that purpose, learners learn from based on their diverse strengths such that they reach goals more efficiently. Recently, some studies have introduced this idea to improve the training efficiency of deep neural networks with knowledge distillation [54,22]. To be specific, Song et al. [36] design a hierarchical structure of multiple branches, which are trained simultaneously to improve backbone's performance. Chen et al. [2] introduce self-attention to improve the branch diversity and distill the knowledge of auxiliary peers into main branch. Additionally, some works apply collaborative learning across networks [2,6]. Zhang et al. [52] propose a deep mutual learning training fashion, in which all networks distill knowledge to each other simultaneously.",
            "score": 0.6483200143517924,
            "section_title": "Related Work",
            "char_start_offset": 5422,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 23
                },
                {
                    "start": 24,
                    "end": 212
                },
                {
                    "start": 213,
                    "end": 377
                },
                {
                    "start": 378,
                    "end": 528
                },
                {
                    "start": 529,
                    "end": 690
                },
                {
                    "start": 691,
                    "end": 785
                },
                {
                    "start": 786,
                    "end": 885
                },
                {
                    "start": 886,
                    "end": 980
                },
                {
                    "start": 981,
                    "end": 1122
                },
                {
                    "start": 1123,
                    "end": 1297
                },
                {
                    "start": 1298,
                    "end": 1409
                },
                {
                    "start": 1412,
                    "end": 1435
                },
                {
                    "start": 1436,
                    "end": 1531
                },
                {
                    "start": 1532,
                    "end": 1647
                },
                {
                    "start": 1648,
                    "end": 1792
                },
                {
                    "start": 1793,
                    "end": 1947
                },
                {
                    "start": 1948,
                    "end": 2083
                },
                {
                    "start": 2084,
                    "end": 2160
                },
                {
                    "start": 2161,
                    "end": 2297
                }
            ],
            "ref_mentions": [
                {
                    "start": 99,
                    "end": 103,
                    "matchedPaperCorpusId": "2134321"
                },
                {
                    "start": 103,
                    "end": 106,
                    "matchedPaperCorpusId": "14925907"
                },
                {
                    "start": 106,
                    "end": 109,
                    "matchedPaperCorpusId": "128358417"
                },
                {
                    "start": 404,
                    "end": 408,
                    "matchedPaperCorpusId": "2723173"
                },
                {
                    "start": 555,
                    "end": 559,
                    "matchedPaperCorpusId": "829159"
                },
                {
                    "start": 812,
                    "end": 816,
                    "matchedPaperCorpusId": "198179476"
                },
                {
                    "start": 898,
                    "end": 902,
                    "matchedPaperCorpusId": "204838340"
                },
                {
                    "start": 1115,
                    "end": 1118,
                    "matchedPaperCorpusId": "203642130"
                },
                {
                    "start": 1118,
                    "end": 1121,
                    "matchedPaperCorpusId": "125985701"
                },
                {
                    "start": 1159,
                    "end": 1162,
                    "matchedPaperCorpusId": "203642130"
                },
                {
                    "start": 1527,
                    "end": 1530,
                    "matchedPaperCorpusId": "60050731"
                },
                {
                    "start": 1784,
                    "end": 1788,
                    "matchedPaperCorpusId": "48352434"
                },
                {
                    "start": 1788,
                    "end": 1791,
                    "matchedPaperCorpusId": "222124879"
                },
                {
                    "start": 1821,
                    "end": 1825,
                    "matchedPaperCorpusId": "44119099"
                },
                {
                    "start": 1960,
                    "end": 1963,
                    "matchedPaperCorpusId": "208526905"
                },
                {
                    "start": 2154,
                    "end": 2157,
                    "matchedPaperCorpusId": "208526905"
                },
                {
                    "start": 2157,
                    "end": 2159,
                    "matchedPaperCorpusId": "209319166"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.9482421875
        },
        {
            "corpus_id": "247216727",
            "title": "DistilCSE: Effective Knowledge Distillation For Contrastive Sentence Embeddings",
            "text": "Knowledge Distillation Knowledge Distillation (Hinton et al., 2015) is a commonly used model compression technique, and the knowledge distillation methods for pre-trained models have been extensively studied. (Sanh et al., 2019;Sun et al., 2019;Jiao et al., 2019) proposes to distill the logits in the prediction layer or (and) hidden representations in the intermediate layers from the teacher model into a student model. (Aguilar et al., 2020) formulates two ways to distill the internal knowledge to improve the student model's generalization capabilities. (Sun et al., 2020b) trains a specially designed teacher model, and transfers to a taskagnostic student model. (Pan et al., 2020) proposes to build a meta-teacher model that captures transferable knowledge across domains. The most related work to ours is (Sun et al., 2020a), which proposes to distill knowledge through intermediate layers of the teacher model via a contrastive objective function. Although (Sun et al., 2020a) applies contrastive learning in knowledge distillation, it is quite different from our method. Like many previous KD methods, (Sun et al., 2020a) is only optimized for classification tasks and ignores sentence embedding distillation, while we focus on effective contrastive sentence embedding distillation. Furthermore, (Sun et al., 2020a) applies contrastive distillation on the intermediate representations as an auxiliary loss to the original KD loss, while our method applies a contrastive learning loss on the output representations to substitute the original KD loss. This substitution further brings consistency of the objective functions to the whole process, which improves the student model. \n\nContrastive Sentence Embedding Contrastive learning has emerged as a promising trend in learning sentence embeddings by pulling similar sentences closer and pushing away different sentences in the embedding space.",
            "score": 0.6480752397025098,
            "section_title": "Related Work",
            "char_start_offset": 4806,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 208
                },
                {
                    "start": 209,
                    "end": 422
                },
                {
                    "start": 423,
                    "end": 559
                },
                {
                    "start": 560,
                    "end": 669
                },
                {
                    "start": 670,
                    "end": 780
                },
                {
                    "start": 781,
                    "end": 957
                },
                {
                    "start": 958,
                    "end": 1081
                },
                {
                    "start": 1082,
                    "end": 1293
                },
                {
                    "start": 1294,
                    "end": 1560
                },
                {
                    "start": 1561,
                    "end": 1688
                },
                {
                    "start": 1691,
                    "end": 1904
                }
            ],
            "ref_mentions": [
                {
                    "start": 423,
                    "end": 445,
                    "matchedPaperCorpusId": "203953149"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.97119140625
        },
        {
            "corpus_id": "227228204",
            "title": "A Selective Survey on Versatile Knowledge Distillation Paradigm for Neural Network Models",
            "text": "Fig. 2 shows overall knowledge distillation incorporating multiple teachers. The method uses three losses to train The teacher models typically need to have a high capacity and require a lot of time and data for training. The training of the student model in offline distillation is usually efficient under the guidance of the teacher model. A capacity gap between large teacher network and small student network always exists and Seyed-Iman Mirzadeh et al. [17] showed that the student network performance degrades when the gap is too large. \n\n2) On-line Distillation Online distillation is proposed to further improve the performance of the student model, especially when a large-capacity high performance teacher model is not available (Zhang et al. [16], ; Chen et al., 2020c). In online distillation, both the teacher model and the student model are updated simultaneously, and the whole knowledge distillation framework is endto-end trainable Ying Zhang et al. [16] first presented an online distillation strategy -deep mutual learning (DML). Rather than one way transfer between a static pre-defined teacher network and a student network, an ensemble of student networks learns collaboratively and teaches each other throughout the training process. DML starts with a pool of untrained students who learn simultaneously to solve the task together. Specifically, each student is trained using two losses: a conventional supervised learning loss and a mimicry loss that aligns each student's class posterior with the class probabilities of other students. Fig. 3 shows the deep mutual learning method. Each network uses for training a supervised learning loss and a KLD-based mimicry loss to match the probability estimates of its peers. \n\nGuo et al. [76] improved the generalization ability of this procedure by using an ensemble of soft logits. Chen et al. [77] performed two-level distillation during training with multiple auxiliary peers and one group leader to form a diverse set of peer models. \n\nKim et al. [11] used a feature fusion module to construct the teacher classifier. An ensemble of sub-network classifiers transfers its knowledge to the fused classifier and then the fused classifier delivers its knowledge back to each sub-network, mutually teaching one another in an onlineknowledge distillation manner. III.",
            "score": 0.6475732735459896,
            "section_title": "1) Single Teacher-Single Student",
            "char_start_offset": 10658,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 76
                },
                {
                    "start": 77,
                    "end": 221
                },
                {
                    "start": 222,
                    "end": 341
                },
                {
                    "start": 342,
                    "end": 542
                },
                {
                    "start": 545,
                    "end": 781
                },
                {
                    "start": 782,
                    "end": 1048
                },
                {
                    "start": 1049,
                    "end": 1256
                },
                {
                    "start": 1257,
                    "end": 1354
                },
                {
                    "start": 1355,
                    "end": 1560
                },
                {
                    "start": 1561,
                    "end": 1606
                },
                {
                    "start": 1607,
                    "end": 1742
                },
                {
                    "start": 1745,
                    "end": 1851
                },
                {
                    "start": 1852,
                    "end": 2006
                },
                {
                    "start": 2009,
                    "end": 2090
                },
                {
                    "start": 2091,
                    "end": 2329
                },
                {
                    "start": 2330,
                    "end": 2334
                }
            ],
            "ref_mentions": [
                {
                    "start": 1756,
                    "end": 1760,
                    "matchedPaperCorpusId": "219965421"
                },
                {
                    "start": 1864,
                    "end": 1868,
                    "matchedPaperCorpusId": "208526905"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.9111328125
        },
        {
            "corpus_id": "256220798",
            "title": "A Survey on Optimization Techniques for Edge Artificial Intelligence (AI)",
            "text": "Large machine learning models have millions of parameters associated with them, which makes the deployment of the model in edge devices infeasible. So, the knowledge gained from large models is transferred to small models which run on edge devices. Here, the large models serve as the teacher model, and the small model is such as the student model. The teacher model refers to a larger model, and it alone gets pre-trained. The learned knowledge from the teacher model is transferred to the student model through knowledge distillation, as in Figure 10. The knowledge distillation helps in improving the accuracy of the student model despite the constrained hardware [38]. Different knowledge distillation techniques, namely response-based distillation, where the prediction performance of output layers of the teacher model and student model are compared using a loss function which shows the difference between the models and the loss function is minimized so that the accuracy of student model approaches that of the teacher model. In contrast to responsebased distillation, in feature-based distillation teacher model distills the intermediate features of the student model. Here, the position of distillation is moved prior to the output layer [39,40]. Here the student mimics by minimizing the loss function that is computed according to the intermediate layers. Relation based distillation is one in which the difference between the relationship between different feature maps is captured as a gram matrix, and the corresponding loss function is minimized. The student model reduces the computation cost and memory usage [41]. \n\nformed into the other, and this method is found to be effective when both do share the same label space. In symmetric transfer leaning, both domains are formed into a common feature space.",
            "score": 0.6474490014160035,
            "section_title": "Knowledge Distillation",
            "char_start_offset": 32561,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 147
                },
                {
                    "start": 148,
                    "end": 248
                },
                {
                    "start": 249,
                    "end": 349
                },
                {
                    "start": 350,
                    "end": 424
                },
                {
                    "start": 425,
                    "end": 554
                },
                {
                    "start": 555,
                    "end": 673
                },
                {
                    "start": 674,
                    "end": 1035
                },
                {
                    "start": 1036,
                    "end": 1179
                },
                {
                    "start": 1180,
                    "end": 1258
                },
                {
                    "start": 1259,
                    "end": 1369
                },
                {
                    "start": 1370,
                    "end": 1564
                },
                {
                    "start": 1565,
                    "end": 1634
                },
                {
                    "start": 1637,
                    "end": 1741
                },
                {
                    "start": 1742,
                    "end": 1825
                }
            ],
            "ref_mentions": [
                {
                    "start": 668,
                    "end": 672,
                    "matchedPaperCorpusId": "220632998"
                },
                {
                    "start": 1250,
                    "end": 1254,
                    "matchedPaperCorpusId": "102483181"
                },
                {
                    "start": 1254,
                    "end": 1257,
                    "matchedPaperCorpusId": "215745611"
                },
                {
                    "start": 1629,
                    "end": 1633,
                    "matchedPaperCorpusId": "235680024"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.93798828125
        },
        {
            "corpus_id": "252262956",
            "title": "A Novel Knowledge Distillation Method for Self-Supervised Hyperspectral Image Classification",
            "text": "As the number of network layers deepens, current deep-learning models are becoming more and more complex, while the computational resources required to consume them become increasingly large. To alleviate this problem, Hinton et al. proposed the knowledge distillation method [53]. Traditional knowledge distillation methods train a teacher model on a known dataset and then supervise the training of a student model using the soft labels of the teacher model as well as the real labels. In general, the higher the training accuracy of the teacher model compared to the student model, the more effective the distillation effect is [36,54]. According to the present traditional method, a series of novel distillation models have been proposed [55][56][57][58]. Traditional knowledge distillation between models often suffers from inefficient knowledge transfer and requires a lot of experimentation to find the optimal teacher model. For this reason, a novel approach to knowledge distillation is proposed. This is called self-distillation, where the network itself acts as both a teacher model and a student model. Knowledge distillation usually takes place between different layers of the network. In [59], a self-distillation strategy is proposed that achieves improved computational efficiency by designing in a new network structure for knowledge distillation at each layer of the network. Additionally, in [60], the simultaneous use of soft labels and feature maps to achieve knowledge distillation is proposed. Since knowledge distillation enables the knowledge contained in the teacher model to be transferred to the student model, the trained student model can be utilized to achieve good classification results using only a small number of labeled samples. Given the advantages of knowledge distillation on a limited sample dataset, we added it to the training. This is different from traditional methods, which use teacher models to generate soft labels. In summary, we are implementing hierarchical prediction by adding a fully connected layer to each layer of the network and combining it with soft labels to achieve knowledge distillation. Since no teacher model is used, it can be said that this is a self-distilling approach.",
            "score": 0.6472297831618323,
            "section_title": "Knowledge Distillation",
            "char_start_offset": 10630,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 191
                },
                {
                    "start": 192,
                    "end": 281
                },
                {
                    "start": 282,
                    "end": 487
                },
                {
                    "start": 488,
                    "end": 639
                },
                {
                    "start": 640,
                    "end": 759
                },
                {
                    "start": 760,
                    "end": 932
                },
                {
                    "start": 933,
                    "end": 1005
                },
                {
                    "start": 1006,
                    "end": 1114
                },
                {
                    "start": 1115,
                    "end": 1198
                },
                {
                    "start": 1199,
                    "end": 1393
                },
                {
                    "start": 1394,
                    "end": 1516
                },
                {
                    "start": 1517,
                    "end": 1765
                },
                {
                    "start": 1766,
                    "end": 1870
                },
                {
                    "start": 1871,
                    "end": 1964
                },
                {
                    "start": 1965,
                    "end": 2152
                },
                {
                    "start": 2153,
                    "end": 2240
                }
            ],
            "ref_mentions": [
                {
                    "start": 276,
                    "end": 280,
                    "matchedPaperCorpusId": "7200347"
                },
                {
                    "start": 631,
                    "end": 635,
                    "matchedPaperCorpusId": "219559263"
                },
                {
                    "start": 635,
                    "end": 638,
                    "matchedPaperCorpusId": "215745611"
                },
                {
                    "start": 742,
                    "end": 746,
                    "matchedPaperCorpusId": "131765296"
                },
                {
                    "start": 746,
                    "end": 750,
                    "matchedPaperCorpusId": "198179476"
                },
                {
                    "start": 750,
                    "end": 754,
                    "matchedPaperCorpusId": "244680427"
                },
                {
                    "start": 1202,
                    "end": 1206,
                    "matchedPaperCorpusId": "159041406"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.94873046875
        },
        {
            "corpus_id": "269761365",
            "title": "Exploring Graph-based Knowledge: Multi-Level Feature Distillation via Channels Relational Graph",
            "text": "In the field of deep learning, the impressive success of large neural networks has come at the cost of increased computational complexity, which poses significant challenges for deployment in resourceconstrained environments.While these heavyweight models, often referred to as teacher networks, set state-of-the-art benchmarks on various tasks, their practical applicability is limited by their demanding requirements for memory, processing power, and energy.Knowledge Distillation (KD) [16] emerges as a promising solution to address this dichotomy by transferring the knowledge from a cumbersome model to a more compact and efficient student networks.\n\nThe quintessence of knowledge distillation lies in its ability to encapsulate the representational power of a larger model into a smaller one without incurring a substantial loss in performance.Pioneered by Hinton et al. [16], the process involves training a smaller model to mimic the behavior of the pre-trained larger model by softening the output logits [19], thus leveraging the rich information embedded in the output distributions of the teacher network.\n\nRecent advances in KD techniques have extended beyond the mere replication of output distributions.Contemporary works explore the distillation of intermediate representations [1,49,31], attention mechanisms [50,20], and even the inculcation of adversarial robustness from teacher to student [33,55,37].The underlying hypothesis is that the intermediate layers of a neural network embody a wealth of information about the data manifold that, when transferred effectively, can endow the student with nuanced understanding akin to its teacher.\n\nLarge and complex teacher models often capture a wealth of features and deep information, crucial for enhancing task performance [7].However, attempting to directly distill this rich information into smaller capacity student models often results in suboptimal performance reproduction due to the student model's capacity limitations.The student model may struggle to process the complex information present in the teacher model, leading to ineffective distillation.Moreover, simply mimicking all features of the teacher model overlooks the differences in relationships and structures between the information, particularly when there is a significant gap between the teacher and student models [12].",
            "score": 0.6471252973423388,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 225
                },
                {
                    "start": 225,
                    "end": 460
                },
                {
                    "start": 460,
                    "end": 654
                },
                {
                    "start": 656,
                    "end": 850
                },
                {
                    "start": 850,
                    "end": 1117
                },
                {
                    "start": 1119,
                    "end": 1218
                },
                {
                    "start": 1218,
                    "end": 1421
                },
                {
                    "start": 1421,
                    "end": 1659
                },
                {
                    "start": 1661,
                    "end": 1794
                },
                {
                    "start": 1794,
                    "end": 1994
                },
                {
                    "start": 1994,
                    "end": 2126
                },
                {
                    "start": 2126,
                    "end": 2359
                }
            ],
            "ref_mentions": [
                {
                    "start": 1014,
                    "end": 1018,
                    "matchedPaperCorpusId": "260933721"
                },
                {
                    "start": 1294,
                    "end": 1297,
                    "matchedPaperCorpusId": "2723173"
                },
                {
                    "start": 1297,
                    "end": 1300,
                    "matchedPaperCorpusId": "206596723"
                },
                {
                    "start": 1300,
                    "end": 1303,
                    "matchedPaperCorpusId": "198185886"
                },
                {
                    "start": 1330,
                    "end": 1333,
                    "matchedPaperCorpusId": "125950115"
                },
                {
                    "start": 1414,
                    "end": 1417,
                    "matchedPaperCorpusId": "237194985"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.97802734375
        },
        {
            "corpus_id": "253236895",
            "title": "Teacher-Student Architecture for Knowledge Learning: A Survey",
            "text": "Knowledge distillation focuses on training a student model, using predictions from a larger-sized teacher model. The purpose of knowledge distillation is to have a compact student model while maintaining comparable performance with a teacher model. Hinton et al. [4] first propose to distill knowledge from multiple models to a single student model for the task of model compression and transfer learning. Tang et al. [14] compress BERT [15] to a much light-weight Bi-LSTM [16] for the task of natural language processing. Romero et al. [17] suggest the success of deep neural nets is largely attributed to the deep hierarch. Thus they propose to compress wide (a large number of neurons in each layer) and deep teacher models into much narrower (fewer neurons in each layer) and deeper student models. Yim et al. [18], design the architectures of students and teachers as a N -part module, where each module contains various numbers of convolutional layers. Student models generally have a simpler design, and the task for students is to learn each layer output of the teacher. \n\nIn a different approach, Wang et al. [19] argue that in vanilla Knowledge Distillation, it is hard for students to learn all knowledge from teachers, thus students normally show worse performance compared to the teacher. The authors adopt generative adversarial networks [20] to simulate the distillation process. The generator (student model with fewer parameters) learns the distribution of the data, whereas the discriminator (teacher model with more parameters) learns to differentiate if the input is from a student or real. \n\nTang et al. [21] propose to distill complicated models in information retrieval or recommendation systems where when a query is given, the model predicts the top K relevant information. Zhang et al. [22] utilize multiple students to learn from each other during training. The purpose is for performance improvement instead of model compression. Furlane et al. [23] propose a novel ensemble learning approach, where students and teachers share the same architecture. The N th student is responsible to train the N + 1 th student. Predictions are averaged in the end.",
            "score": 0.6466519297401575,
            "section_title": "Knowledge Distillation",
            "char_start_offset": 6422,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 112
                },
                {
                    "start": 113,
                    "end": 248
                },
                {
                    "start": 249,
                    "end": 405
                },
                {
                    "start": 406,
                    "end": 522
                },
                {
                    "start": 523,
                    "end": 625
                },
                {
                    "start": 626,
                    "end": 802
                },
                {
                    "start": 803,
                    "end": 958
                },
                {
                    "start": 959,
                    "end": 1078
                },
                {
                    "start": 1081,
                    "end": 1301
                },
                {
                    "start": 1302,
                    "end": 1394
                },
                {
                    "start": 1395,
                    "end": 1610
                },
                {
                    "start": 1613,
                    "end": 1798
                },
                {
                    "start": 1799,
                    "end": 1884
                },
                {
                    "start": 1885,
                    "end": 1957
                },
                {
                    "start": 1958,
                    "end": 2078
                },
                {
                    "start": 2079,
                    "end": 2141
                },
                {
                    "start": 2142,
                    "end": 2178
                }
            ],
            "ref_mentions": [
                {
                    "start": 814,
                    "end": 818,
                    "matchedPaperCorpusId": "206596723"
                },
                {
                    "start": 1118,
                    "end": 1122,
                    "matchedPaperCorpusId": "53976534"
                },
                {
                    "start": 1352,
                    "end": 1356,
                    "matchedPaperCorpusId": "10319744"
                },
                {
                    "start": 1625,
                    "end": 1629,
                    "matchedPaperCorpusId": "50778760"
                },
                {
                    "start": 1812,
                    "end": 1816,
                    "matchedPaperCorpusId": "26071966"
                },
                {
                    "start": 1973,
                    "end": 1977,
                    "matchedPaperCorpusId": "4110009"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.9453125
        },
        {
            "corpus_id": "222272190",
            "title": "Be Your Own Best Competitor! Multi-Branched Adversarial Knowledge Transfer",
            "text": "The main goal of knowledge distillation methods is to transfer knowledge of a strong learning capacity model to a lower capacity neural network structure. The early knowledge distillation method encourages the compact model to produce the output of the cumbersome model's softmax layer; instead of the hard target of ground-truth [15]. Ba et al. [16] demonstrated that deep neural networks might not need to be deep when it is possible to mimic the function learned by a deep and complex model with a shallower network. Consequently, they can apply model compression to train small neural networks to imitate the more complex and deeper ones. \n\nYuan et al. [32] interpreted the knowledge distillation as a regularization method and provided the relation between knowledge distillation and label smoothing regularization, theoretically. Therefore, they proposed teacher-free knowledge distillation where the network is trained by itself or a virtual teacher model. As such, they utilized the pre-trained model of the network as a teacher model and afterward trained this model further by itself via transferring soft targets of the pretrained model. \n\nIn [33], the capacity gap between the large teacher model and the student has been investigated. It shows that the relationships between the architecture of the teacher and student model are very important. Hence, it may be a case that the student model cannot learn a significant knowledge from a specific teacher just like the habits of human beings learning where the student should find a more appropriate teacher. \n\nIn the response-based knowledge distillation [15], [16], the teacher model's final prediction is directly mimicked by the student model. In the feature-based knowledge distillation, the output of all intermediate layers (feature maps) can be utilized to supervise the student model [17]. The attention map of intermediate feature maps can be expressed as knowledge to transfer to the small student model [34]. \n\nIn [35], the learning schemes of knowledge distillation have been classified into three categories of offline, online, and self-distillation. In the offline distillation, the knowledge is transferred from a pre-trained teacher model into a student model [15], [17], [36].",
            "score": 0.6461665051480281,
            "section_title": "A. Image Classification",
            "char_start_offset": 9625,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 154
                },
                {
                    "start": 155,
                    "end": 335
                },
                {
                    "start": 336,
                    "end": 519
                },
                {
                    "start": 520,
                    "end": 642
                },
                {
                    "start": 645,
                    "end": 835
                },
                {
                    "start": 836,
                    "end": 963
                },
                {
                    "start": 964,
                    "end": 1148
                },
                {
                    "start": 1151,
                    "end": 1247
                },
                {
                    "start": 1248,
                    "end": 1357
                },
                {
                    "start": 1358,
                    "end": 1569
                },
                {
                    "start": 1572,
                    "end": 1708
                },
                {
                    "start": 1709,
                    "end": 1859
                },
                {
                    "start": 1860,
                    "end": 1981
                },
                {
                    "start": 1984,
                    "end": 2125
                },
                {
                    "start": 2126,
                    "end": 2255
                }
            ],
            "ref_mentions": [
                {
                    "start": 346,
                    "end": 350,
                    "matchedPaperCorpusId": "11536917"
                },
                {
                    "start": 1623,
                    "end": 1627,
                    "matchedPaperCorpusId": "11536917"
                },
                {
                    "start": 1854,
                    "end": 1858,
                    "matchedPaperCorpusId": "2723173"
                },
                {
                    "start": 1976,
                    "end": 1980,
                    "matchedPaperCorpusId": "829159"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.82666015625
        },
        {
            "corpus_id": "259749217",
            "title": "Model Protection Scheme Against Distillation Attack in Internet of Vehicles",
            "text": "Knowledge distillation is a widely used method for model compression and optimisation in deep learning. It is based on the concept of a \"teacher-student model\" for training and is highly regarded for its simplicity and effectiveness. Knowledge Distillation facilitates the training of student models by extracting \"knowledge\" from one or more pretrained teacher models using the soft-label probabilistic output of the teacher models. This soft-label output is a mapping from input vectors to output vectors that captures specific knowledge from instantiated objects, with incorrect classification predictions providing insight into how the teacher model generalizes. The student model can improve its performance by mimicking the probabilistic output of the teacher model, and can incorporate the knowledge that the teacher model has already acquired. The process of knowledge distillation is illustrated in Fig. 1. Teacher networks can transfer their model capabilities to student networks through knowledge distillation. \n\nAs shown in Eq. ( 1), neural networks typically generate class probabilities by using a \"softmax\" output layer that compares the output i z of each class with other logits, converting the logit z i calculated for each class into a probability i q in a standardized way. In addition, where T represents temperature in knowledge distillation, using a larger value than 1 for it produces a softer class probability distribution that allows better transfer of knowledge to the model to be distilled. \n\nGiven a pre-trained teacher model ()",
            "score": 0.645905006606007,
            "section_title": "Knowledge Distillation",
            "char_start_offset": 6851,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 103
                },
                {
                    "start": 104,
                    "end": 233
                },
                {
                    "start": 234,
                    "end": 433
                },
                {
                    "start": 434,
                    "end": 666
                },
                {
                    "start": 667,
                    "end": 851
                },
                {
                    "start": 852,
                    "end": 915
                },
                {
                    "start": 916,
                    "end": 1022
                },
                {
                    "start": 1025,
                    "end": 1294
                },
                {
                    "start": 1295,
                    "end": 1520
                },
                {
                    "start": 1523,
                    "end": 1559
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.939453125
        },
        {
            "corpus_id": "267413204",
            "title": "Learning from Teaching Regularization: Generalizable Correlations Should be Easy to Imitate",
            "text": "Knowledge distillation (KD) is a technique that transfers knowledge from a teacher model to a student model by training the student to imitate the teacher's outputs [36]. This approach is widely applied in areas such as model compression, transparency, and interpretability [8,10,27,36,60,91]. Model compression is often motivated by resource constraints. Pioneering works include Bucilu\u01ce et al. [10], which compresses ensemble neural networks into a single network, and Ba and Caruana [3], which improves shallow neural network accuracy by mimicking deep networks. KD is also applied in various domains, including deep reinforcement learning [80], continual learning [28,59,85], and learning privileged information theory [62,76]. The dark knowledge method [36] further develops KD, where a student model aims to fully match the output distribution of the teacher. Intuitively, distillation is effective because the teacher's output distribution over classes provides a more informative training signal than a one-hot label. Additionally, in born-again networks (BAN) [29], the teacher and student have identical neural architecture and model sizes, but the student can surprisingly surpass the teacher's accuracy.",
            "score": 0.6454465479123743,
            "section_title": "B.2.1 Knowledge Distillation",
            "char_start_offset": 29576,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 170
                },
                {
                    "start": 171,
                    "end": 293
                },
                {
                    "start": 294,
                    "end": 355
                },
                {
                    "start": 356,
                    "end": 565
                },
                {
                    "start": 566,
                    "end": 731
                },
                {
                    "start": 732,
                    "end": 865
                },
                {
                    "start": 866,
                    "end": 1025
                },
                {
                    "start": 1026,
                    "end": 1215
                }
            ],
            "ref_mentions": [
                {
                    "start": 165,
                    "end": 169,
                    "matchedPaperCorpusId": "7200347"
                },
                {
                    "start": 277,
                    "end": 280,
                    "matchedPaperCorpusId": "11253972"
                },
                {
                    "start": 280,
                    "end": 283,
                    "matchedPaperCorpusId": "3976789"
                },
                {
                    "start": 283,
                    "end": 286,
                    "matchedPaperCorpusId": "7200347"
                },
                {
                    "start": 289,
                    "end": 292,
                    "matchedPaperCorpusId": "21713934"
                },
                {
                    "start": 396,
                    "end": 400,
                    "matchedPaperCorpusId": "11253972"
                },
                {
                    "start": 672,
                    "end": 675,
                    "matchedPaperCorpusId": "4853851"
                },
                {
                    "start": 727,
                    "end": 730,
                    "matchedPaperCorpusId": "41866457"
                },
                {
                    "start": 758,
                    "end": 762,
                    "matchedPaperCorpusId": "7200347"
                },
                {
                    "start": 1069,
                    "end": 1073,
                    "matchedPaperCorpusId": "4110009"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.95947265625
        },
        {
            "corpus_id": "246652277",
            "title": "KENN: Enhancing Deep Neural Networks by Leveraging Knowledge for Time Series Forecasting",
            "text": "In this section, we review some of the relevant work in the literature specifically pertaining to knowledge sharing including multi-modal learning approaches. \n\nKnowledge Distillation based techniques aim to transfer knowledge from the teacher network to a student network. This knowledge is typically transferred in a process where teacher produces soft predictions that the student network tries to emulate. (Hu et al., 2016) utilized iterative knowledge distillation to transfer knowledge. They used a teacher network that was made from first-order logic rules and used it to train the student DNN model. They updated both the teacher as well as the student at each iteration during the learning process. The main attempt was to find a teacher network that can match the rule set in terms of predictions while not diverging significantly from the labels in the data. Similarly, (Xie et al., 2019) used a teacher network that was trained on JFT-300m dataset that contained 300 million images. After which they used a teacher student setting on ImageNet dataset where the teacher network produced soft predictions that student network followed. Knowledge distillation can also be in the form of feature map transfer that provides student network an attention map for intermediary layers. As a result the student network learns which part of the intermediate feature map does the teacher network pay attention too. Such approach is used in the work presented by (Chen et al., 2021). However, mostly distillation method rely on minimizing KL-divergence between the distributions of the prediction made by the teacher and the student to make the two distributions similar. KL-divergence can not be directly applied for forecasting task since the output of the network is not a distribution but instead is an estimate of future time series values. Moreover, Since the task of the student network in this framework is to emulate the predictions made by the teacher, this leads to the strength of the student network being ignored by the system and scenarios where teacher is flawed or unreliable is not catered in distillation setting. \n\nAttention based models have gained considerable popularity recently. (Li et al., 2019) proposes a convolutional self-attention mechanism for transformer networks that used causal convolutions of different kernel sizes.",
            "score": 0.6453718408688647,
            "section_title": "Related work",
            "char_start_offset": 7345,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 158
                },
                {
                    "start": 161,
                    "end": 273
                },
                {
                    "start": 274,
                    "end": 409
                },
                {
                    "start": 410,
                    "end": 492
                },
                {
                    "start": 493,
                    "end": 607
                },
                {
                    "start": 608,
                    "end": 707
                },
                {
                    "start": 708,
                    "end": 869
                },
                {
                    "start": 870,
                    "end": 994
                },
                {
                    "start": 995,
                    "end": 1145
                },
                {
                    "start": 1146,
                    "end": 1288
                },
                {
                    "start": 1289,
                    "end": 1414
                },
                {
                    "start": 1415,
                    "end": 1482
                },
                {
                    "start": 1483,
                    "end": 1670
                },
                {
                    "start": 1671,
                    "end": 1844
                },
                {
                    "start": 1845,
                    "end": 2131
                },
                {
                    "start": 2134,
                    "end": 2202
                },
                {
                    "start": 2203,
                    "end": 2352
                }
            ],
            "ref_mentions": [
                {
                    "start": 1462,
                    "end": 1481,
                    "matchedPaperCorpusId": "227335337"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.88671875
        },
        {
            "corpus_id": "245650327",
            "title": "Class-Incremental Continual Learning Into the eXtended DER-Verse",
            "text": "Knowledge Distillation (KD) [36] is a training methodology that allows transferring the knowledge of a teacher model into a separate student model. While Hinton et al. originally proposed to distillate large teachers -possibly ensembles -into smaller students, further studies revealed additional interesting properties about this technique. In particular, Furlanello et al. [37] show that multiple rounds of distillation between models with the same architecture (termed self-distillation) can surprisingly improve the performance of the student. More recently, other works [38], [39] explore an interesting variation of self-distillation that distills knowledge from the deeper layers of the network to its shallower ones to accelerate convergence and attain higher accuracy. \n\nKnowledge Distillation and Continual Learning. Distillation can be used to hinder catastrophic forgetting by appointing a previous snapshot of the model as teacher and distilling from it while new tasks are learned. Learning Without Forgetting [7] uses teacher responses to new exemplars to constrain the evolution of the student. Several other works combine distillation with rehearsal: iCaRL [11] distills the responses of the model at the previous task boundary, learning latent representations to be used in a nearest meanof-exemplars classifier; EtEIL [40], LUCIR [31] and BiC [41] focus on contrasting the prediction bias that comes from incremental classification; IL2M [30] stores additional statistics to facilitate distillation and compensate bias.",
            "score": 0.6451521760662738,
            "section_title": "Self-Distillation",
            "char_start_offset": 8104,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 147
                },
                {
                    "start": 148,
                    "end": 341
                },
                {
                    "start": 342,
                    "end": 547
                },
                {
                    "start": 548,
                    "end": 777
                },
                {
                    "start": 780,
                    "end": 826
                },
                {
                    "start": 827,
                    "end": 995
                },
                {
                    "start": 996,
                    "end": 1110
                },
                {
                    "start": 1111,
                    "end": 1538
                }
            ],
            "ref_mentions": [
                {
                    "start": 28,
                    "end": 32,
                    "matchedPaperCorpusId": "7200347"
                },
                {
                    "start": 375,
                    "end": 379,
                    "matchedPaperCorpusId": "4110009"
                },
                {
                    "start": 575,
                    "end": 579,
                    "matchedPaperCorpusId": "159041406"
                },
                {
                    "start": 1024,
                    "end": 1027,
                    "matchedPaperCorpusId": "4853851"
                },
                {
                    "start": 1174,
                    "end": 1178,
                    "matchedPaperCorpusId": "206596260"
                },
                {
                    "start": 1349,
                    "end": 1353,
                    "matchedPaperCorpusId": "195453293"
                },
                {
                    "start": 1362,
                    "end": 1366,
                    "matchedPaperCorpusId": "173187918"
                },
                {
                    "start": 1457,
                    "end": 1461,
                    "matchedPaperCorpusId": "204923710"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.95654296875
        },
        {
            "corpus_id": "248496140",
            "title": "FedDKD: Federated learning with decentralized knowledge distillation",
            "text": "Knowledge distillation (KD) defines a learning schema to transfer knowledge from a larger teacher network to a smaller student network [20]. By optimizing the student model to match logits or intermediate activations of the teacher model, KD can compress the knowledge from a powerful teacher model to improve model efficiency [21], unsupervised domain adaptation [22], improved object detection [23], model transparency [24], and adversarial robustness [25,26].\n\nTo enable the student to learn various types of knowledge, some studies consider learning from multiple teachers, which allows each teacher to be treated with varying degrees of importance. Many of them attempt to distill knowledge with multi-task or multi-domain learning [27][28][29]. Other studies fuse knowledge by weighted averaging soft targets from different teacher networks [30,31]. In this paper, we employ KD in federated learning, which can be considered an extension of knowledge distillation with multiple teachers by multi-task learning.",
            "score": 0.6449603831289736,
            "section_title": "Knowledge distillation",
            "char_start_offset": 8473,
            "sentence_offsets": [],
            "ref_mentions": [
                {
                    "start": 135,
                    "end": 139,
                    "matchedPaperCorpusId": "206596723"
                },
                {
                    "start": 327,
                    "end": 331,
                    "matchedPaperCorpusId": "2723173"
                },
                {
                    "start": 421,
                    "end": 425,
                    "matchedPaperCorpusId": "21713934"
                },
                {
                    "start": 458,
                    "end": 461,
                    "matchedPaperCorpusId": "16852518"
                },
                {
                    "start": 737,
                    "end": 741,
                    "matchedPaperCorpusId": "230125454"
                },
                {
                    "start": 741,
                    "end": 745,
                    "matchedPaperCorpusId": "248779987"
                },
                {
                    "start": 847,
                    "end": 851,
                    "matchedPaperCorpusId": "26021416"
                },
                {
                    "start": 851,
                    "end": 854,
                    "matchedPaperCorpusId": "224818016"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.9150390625
        },
        {
            "corpus_id": "219559263",
            "title": "Knowledge Distillation: A Survey",
            "text": "Fig. 15 A generic framework for quantized distillation 32-bit floating point) into low-precision networks (e.g., 2-bit and 8-bit). Meanwhile, knowledge distillation aims to train a small model to yield a performance comparable to that of a complex model. Some KD methods have been proposed using the quantization process in the teacher-student framework (Polino et al. 2018;Mishra and Marr 2018;Wei et al. 2018;Shin et al. 2019;Kim et al. 2019a). A framework for quantized distillation methods is shown in Fig. 15. \n\nSpecifically, Polino et al. (2018) proposed a quantized distillation method to transfer the knowledge to a weightquantized student network. In Mishra and Marr (2018), the proposed quantized KD is called the \"apprentice\" . A high precision teacher network transfers knowledge to a small low-precision student network. To ensure that a small student network accurately mimics a large teacher network, the full-precision teacher network is first quantized on the feature maps, and then the knowledge is transferred from the quantized teacher to a quantized student network (Wei et al. 2018). Kim et al. (2019a) proposed quantization-aware knowledge distillation, which is based on self-study of a quantized student network and on the co-studying of teacher and student networks with knowledge transfer. Furthermore, Shin et al. (2019) carried out empirical analysis of deep neural networks using both distillation and quantization, taking into account the hyper-parameters for knowledge distillation, such as the size of teacher networks and the distillation temperature. Recently, unlike the quantized distillation methods above, a self-distillation training schemes is designed to improve the performance of quantized deep models, where teacher shares model parameters of student (Boo et al. 2021).",
            "score": 0.6445444805678249,
            "section_title": "Full-precision teacher network A large network K n o w l e d g e t r a n s f e r Q u a n t i z a t i o n",
            "char_start_offset": 48614,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 130
                },
                {
                    "start": 131,
                    "end": 254
                },
                {
                    "start": 255,
                    "end": 446
                },
                {
                    "start": 447,
                    "end": 514
                },
                {
                    "start": 517,
                    "end": 656
                },
                {
                    "start": 657,
                    "end": 738
                },
                {
                    "start": 739,
                    "end": 833
                },
                {
                    "start": 834,
                    "end": 1105
                },
                {
                    "start": 1106,
                    "end": 1316
                },
                {
                    "start": 1317,
                    "end": 1585
                },
                {
                    "start": 1586,
                    "end": 1814
                }
            ],
            "ref_mentions": [
                {
                    "start": 374,
                    "end": 395,
                    "matchedPaperCorpusId": "3643430"
                },
                {
                    "start": 395,
                    "end": 411,
                    "matchedPaperCorpusId": "24139282"
                },
                {
                    "start": 660,
                    "end": 682,
                    "matchedPaperCorpusId": "3643430"
                },
                {
                    "start": 1087,
                    "end": 1104,
                    "matchedPaperCorpusId": "24139282"
                },
                {
                    "start": 1796,
                    "end": 1813,
                    "matchedPaperCorpusId": "222066738"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.85205078125
        },
        {
            "corpus_id": "270077283",
            "title": "Self-Knowledge Distillation via Progressive Associative Learning",
            "text": "In this section, we first briefly introduce the most related works of knowledge distillation.Then we specifically review recent self-distillation works.\n\nKnowledge distillation is a widely used paradigm for model compression, which transfers knowledge from a complex teacher model to a compact student model.To be specific, the teacher network has high accuracy and huge parameters, while the student network is not as accurate as the teacher network but has fewer parameters.Through knowledge distillation, we hope that the student network can approach or exceed the teacher network as much as possible.In this way, we obtain a compact student network with a similar prediction effect as the teacher network.Ba et al. [30] first proposed a method that uses the teacher's logits before the softmax as the regression target to train the student network, which completes the imitation of the teacher network by forcing the student network to mimic the teacher network's logits.Hinton et al. [12] first proposed to use the soft outputs of the pretrained teacher network as dark knowledge to supervise the training of the student network.They introduced a temperature hyperparameter T and formulated the problem as \"knowledge distillation\".The student network is forced to learn the soft targets of the teacher network, which are obtained through using a high temperature T on the softmax inputs.In the process of knowledge transfer, soft targets often contain richer information than one-hot targets.Romero et al. [13] extended the knowledge distillation method proposed by Hinton et al.In their method, the student network can be deeper and narrower than the teacher network and improve the performance by learning the outputs of the teacher network and the features of the middle layer.All the above methods are offline distillation methods [31,32], which need a pretrained teacher network.\n\nIn contrast to these methods, online knowledge distillation trains the student network under the supervision of a teacher from scratch.For example, Zhang et al. [33] proposed a mutual learning method, which uses multiple neural networks.Zhao et al. [9] proposed a collaborative training method, which uses both an expert teacher and a from-scratch teacher to supervise the student.To reduce the computational cost, Zhou et al. [34] proposed to employ two different networks which share some low parameters and train separately.",
            "score": 0.6444066895478471,
            "section_title": "Related Work",
            "char_start_offset": 5661,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 93
                },
                {
                    "start": 93,
                    "end": 152
                },
                {
                    "start": 154,
                    "end": 308
                },
                {
                    "start": 308,
                    "end": 476
                },
                {
                    "start": 476,
                    "end": 604
                },
                {
                    "start": 604,
                    "end": 709
                },
                {
                    "start": 709,
                    "end": 975
                },
                {
                    "start": 975,
                    "end": 1134
                },
                {
                    "start": 1134,
                    "end": 1236
                },
                {
                    "start": 1236,
                    "end": 1392
                },
                {
                    "start": 1392,
                    "end": 1497
                },
                {
                    "start": 1497,
                    "end": 1584
                },
                {
                    "start": 1584,
                    "end": 1785
                },
                {
                    "start": 1785,
                    "end": 1889
                },
                {
                    "start": 1891,
                    "end": 2026
                },
                {
                    "start": 2026,
                    "end": 2128
                },
                {
                    "start": 2128,
                    "end": 2272
                },
                {
                    "start": 2272,
                    "end": 2418
                }
            ],
            "ref_mentions": [
                {
                    "start": 719,
                    "end": 723,
                    "matchedPaperCorpusId": "11536917"
                },
                {
                    "start": 989,
                    "end": 993,
                    "matchedPaperCorpusId": "7200347"
                },
                {
                    "start": 1840,
                    "end": 1844,
                    "matchedPaperCorpusId": "247476179"
                },
                {
                    "start": 1844,
                    "end": 1847,
                    "matchedPaperCorpusId": "258888057"
                },
                {
                    "start": 2052,
                    "end": 2056,
                    "matchedPaperCorpusId": "26071966"
                },
                {
                    "start": 2140,
                    "end": 2143,
                    "matchedPaperCorpusId": "198179767"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.9736328125
        },
        {
            "corpus_id": "232352737",
            "title": "Student Network Learning via Evolutionary Knowledge Distillation",
            "text": "And a framework Snapshot Distillation was proposed for teacher-student optimization in one generation [16], which extracted such information from earlier epochs in the same generation, meanwhile made sure that the difference between teacher and student is sufficiently large so as to prevent underfitting. After these, a novel two-level framework OKDDip [18] was proposed to perform distillation during training with multiple auxiliary peers and one group leader for effective online distillation. In OKDDip framework, the first-level distillation works as diversity maintained group distillation with several auxiliary peers, while the second-level distillation transfers the diversity enhanced group knowledge to the ultimate student model called group leader. \n\nThese online, timely and efficient training methods are promising and some good progress has been made in narrowing the capability gap between teacher and student model [15], [16], [42]. However, for online distillation, two factors still hold them back. First, although the teachers of online distillation methods are dynamic and have narrowed the capability gap, the gap still exists due to the lack of representation in detail and the process of learning. Second, owing to the absence of a qualified teacher role for online knowledge distillation, the insufficient and relatively unreliable supervision information will restrict the learning of student to some extent. There is still great room for improvement in knowledge transfer and representation. Therefore, we propose the evolutionary knowledge distillation approach to enhance the performance of student network learning by using an evolutionary teacher and focusing on intermediate knowledge of the teaching process dynamically.",
            "score": 0.6433859992845679,
            "section_title": "C. Online Knowledge Distillation",
            "char_start_offset": 10556,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 305
                },
                {
                    "start": 306,
                    "end": 497
                },
                {
                    "start": 498,
                    "end": 762
                },
                {
                    "start": 765,
                    "end": 951
                },
                {
                    "start": 952,
                    "end": 1019
                },
                {
                    "start": 1020,
                    "end": 1223
                },
                {
                    "start": 1224,
                    "end": 1436
                },
                {
                    "start": 1437,
                    "end": 1520
                },
                {
                    "start": 1521,
                    "end": 1755
                }
            ],
            "ref_mentions": [
                {
                    "start": 102,
                    "end": 106,
                    "matchedPaperCorpusId": "54436113"
                },
                {
                    "start": 354,
                    "end": 358,
                    "matchedPaperCorpusId": "208526905"
                },
                {
                    "start": 940,
                    "end": 944,
                    "matchedPaperCorpusId": "54436113"
                },
                {
                    "start": 946,
                    "end": 950,
                    "matchedPaperCorpusId": "159041406"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.73779296875
        },
        {
            "corpus_id": "247315450",
            "title": "Overcoming Catastrophic Forgetting beyond Continual Learning: Balanced Training for Neural Machine Translation",
            "text": "The knowledge of n complementary teachers can be transfered to the student through word-level knowledge distillation:\n\nwhere p is the prediction of student S and q i is the prediction of teacher T i . We use a hyperparameter \u03b1 to interpolate the distillation loss and the crossentropy loss:\n\nIn this way, the student model learns both new knowledge from the training set and complementary knowledge from teacher models. With an appropriate \u03b1, we can achieve a balance between the Algorithm 1 COKD Input: training set D, the number of teachers n Output: student model S 1: randomly initialize student S and teachers T 1:n 2: while not converge do for i = 1 to n do T i \u2190 S 9: return student model S two kinds of knowledge and alleviate the problem of imbalanced training. However, this method is based on knowledge distillation where knowledge is transferred unidirectionally from teachers to the student. Though the student can benefit from balanced training, these complementary teachers also set an upperbound to the student and prevent it from performing better.\n\nTo overcome this limitation, we follow the underlying idea of two-way knowledge transfer where the knowledge is also transferred from the student to teachers (Zhang et al., 2018;Zhu et al., 2018). We use a simple reinitialization method to achieve the two-way knowledge transfer. At the end of each epoch, we reinitialize teacher models with the parameters of the student model:\n\nThrough the reinitialization, the student and teachers are exactly the same at the beginning of each epoch. In this way, both the student and teachers are iteratively improved so the student performance is no longer limited by the fixed ability of teachers. We summarize the training process of COKD in Algorithm 1.",
            "score": 0.6430518780301073,
            "section_title": "Complementary Training",
            "char_start_offset": 17853,
            "sentence_offsets": [],
            "ref_mentions": [
                {
                    "start": 1225,
                    "end": 1245,
                    "matchedPaperCorpusId": "26071966"
                },
                {
                    "start": 1245,
                    "end": 1262,
                    "matchedPaperCorpusId": "48352434"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.720703125
        },
        {
            "corpus_id": "272945756",
            "title": "Fast reconstruction of milling temperature field based on CNN-GRU machine learning models",
            "text": "Knowledge distillation is an instructor-student training structure that typically utilizes a student model with a simpler network structure to learn the knowledge provided by an instructor model that has been trained with a more complex network structure; this approach trades a slight performance loss for faster computation and smaller model parameters. Knowledge distillation works by training the student model with both the predictions of the teacher model (soft labeling) and the real data (hard labeling), and calculating the weighted total loss of the student model on both the soft and hard labels, essentially \"migrating\" the knowledge learned by the teacher model to the student model. The structure of the knowledge distillation strategy used in this paper is shown in Figure 6. \n\nThe specific knowledge distillation strategy process is as follows: \n\n(1) The raw data that has been preprocessed is input to both the teacher model and the student model, the teacher model is the CNN-GRU model constructed in the previous section, and the student model is a small model with a single CNN layer and a single GRU layer. ( 2  (5) The distillation loss and the student loss are weighted to obtain the total loss, and the gradient of each parameter is updated in the backpropagation process. \n\nThe following are the calculation formulas involved in the knowledge distillation operation process: \n\nKnowledge distillation soft labeling calculation formula as Equation ( 5): \n\nwhere T is the distillation temperature coefficient, used to control the \"hardness\" of the soft label. When T is larger, the soft label distribution area is uniform, more softened, when T is smaller, the soft label distribution closer to the hard label. \n\nDistillation loss of the loss function LOSSsoft formula is as Equation ( 6): \n\nwhere k is the total number of samples, p i (u i ,T) is the ith output of the teacher model at temperature coefficient T, and p i (z i ,T) is the ith output of the student model at temperature coefficient T. \n\nThe loss function LOSShard for student loss is formulated as Equation ( 7): \n\nwhere y i 7is a vector of hard labels representing the class i output of the unsoftened student model. \n\nThe total loss of knowledge distillation can be expressed as Equation ( 8): \n\n( )",
            "score": 0.6428539161131605,
            "section_title": "Temperature boundary condition estimation model based on knowledge distillation with gated convolutional recurrent networks",
            "char_start_offset": 29942,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 355
                },
                {
                    "start": 356,
                    "end": 696
                },
                {
                    "start": 697,
                    "end": 790
                },
                {
                    "start": 793,
                    "end": 860
                },
                {
                    "start": 863,
                    "end": 1127
                },
                {
                    "start": 1128,
                    "end": 1296
                },
                {
                    "start": 1299,
                    "end": 1399
                },
                {
                    "start": 1402,
                    "end": 1476
                },
                {
                    "start": 1479,
                    "end": 1581
                },
                {
                    "start": 1582,
                    "end": 1732
                },
                {
                    "start": 1735,
                    "end": 1811
                },
                {
                    "start": 1814,
                    "end": 2021
                },
                {
                    "start": 2024,
                    "end": 2099
                },
                {
                    "start": 2102,
                    "end": 2204
                },
                {
                    "start": 2207,
                    "end": 2282
                },
                {
                    "start": 2285,
                    "end": 2288
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.96875
        },
        {
            "corpus_id": "276250522",
            "title": "Right Time to Learn:Promoting Generalization via Bio-inspired Spacing Effect in Knowledge Distillation",
            "text": "Knowledge Distillation (KD). Representative avenues of KD can be generally classified into offline KD, online KD, and self KD, based on whether the teacher model is pretrained and remains unchanged during the training process. \n\nOffline KD involves a one-way knowledge transfer in a twophase training procedure. It primarily focuses on optimizing various aspects of knowledge transfer, such as designing the knowledge itself (Hinton et al., 2015;Adriana et al., 2015), and refining loss functions for feature matching or distribution alignment (Huang & Wang, 2017;Asif et al., 2019;Mirzadeh et al., 2020b). In contrast, online KD simplifies the KD process by training both teacher and student simultaneously and often outperforms offline KD. For instance, DML (Zhang et al., 2018) implements bidirectional distillation between peer networks. For self KD, the same network is used as both teacher and student (Zhang et al., 2019;Das & Sanghavi, 2023;Mobahi et al., 2020;Zhang & Sabuncu, 2020;Yang et al., 2019;Lee et al., 2019). In this paper, the self KD we refer to is the distillation between different layers within the same network (Zhang et al., 2019;Yan et al., 2024;Zhai et al., 2019). However, existing methods for online KD and self KD often fail to effectively utilize high-capacity teachers over time, making it an intriguing topic to further explore the relationships between teacher and student models in these environments. \n\nAdaptive Distillation. Recent studies have found that the difference in model capacity between a much larger teacher network and a much smaller student network can limit distillation gains (Liu et al., 2020a;Cho & Hariharan, 2019;Liu et al., 2020b). Current efforts to address this gap fall into two main categories: training paradigms (Gao et al., 2018) and architectural adaptation (Kang et al., 2020;Gu & Tresp, 2020).",
            "score": 0.6420433852663645,
            "section_title": "Related Work",
            "char_start_offset": 4032,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 28
                },
                {
                    "start": 29,
                    "end": 226
                },
                {
                    "start": 229,
                    "end": 311
                },
                {
                    "start": 312,
                    "end": 606
                },
                {
                    "start": 607,
                    "end": 741
                },
                {
                    "start": 742,
                    "end": 841
                },
                {
                    "start": 842,
                    "end": 1027
                },
                {
                    "start": 1028,
                    "end": 1192
                },
                {
                    "start": 1193,
                    "end": 1437
                },
                {
                    "start": 1440,
                    "end": 1462
                },
                {
                    "start": 1463,
                    "end": 1689
                },
                {
                    "start": 1690,
                    "end": 1861
                }
            ],
            "ref_mentions": [
                {
                    "start": 582,
                    "end": 605,
                    "matchedPaperCorpusId": "212908749"
                },
                {
                    "start": 760,
                    "end": 780,
                    "matchedPaperCorpusId": "26071966"
                },
                {
                    "start": 908,
                    "end": 928,
                    "matchedPaperCorpusId": "159041406"
                },
                {
                    "start": 928,
                    "end": 949,
                    "matchedPaperCorpusId": "256416199"
                },
                {
                    "start": 949,
                    "end": 969,
                    "matchedPaperCorpusId": "211096976"
                },
                {
                    "start": 991,
                    "end": 1009,
                    "matchedPaperCorpusId": "54436113"
                },
                {
                    "start": 1136,
                    "end": 1156,
                    "matchedPaperCorpusId": "159041406"
                },
                {
                    "start": 1173,
                    "end": 1191,
                    "matchedPaperCorpusId": "198229709"
                },
                {
                    "start": 1629,
                    "end": 1648,
                    "matchedPaperCorpusId": "208175624"
                },
                {
                    "start": 1648,
                    "end": 1670,
                    "matchedPaperCorpusId": "203642130"
                },
                {
                    "start": 1670,
                    "end": 1688,
                    "matchedPaperCorpusId": "208175624"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.888671875
        },
        {
            "corpus_id": "214623054",
            "title": "Synergic Adversarial Label Learning with DR and AMD for Retinal Image Grading",
            "text": "The idea of knowledge distillation is firstly interpreted in [19] to compress the knowledge in a cumbersome model (a teacher) into a smaller one(a student). The key points of knowledge distillation lies in: firstly, it generates the soft targets by distillation which raises the temperature of the final softmax until the cumbersome model produces an optimal soft set of targets; secondly, it utilizes the soft targets to train a smaller model for transfer learning. Here, the main claims about using soft targets instead of hard targets is that the soft targets have higher entropy and provide much more information so the small model can be trained on much less data than the original larger ensemble model. \n\nSince proposed, knowledge distillation has been proved to be effective in model compression with some extensions in [31], [32] and [33]. [31] utilize an attention map from the teacher network to guide the student network. [33] introduce a similar approach using mean weights. [32] acquire the compressed model by grouping convolution channels and training the student network with an attention transfer. In general, these researches illustrate that smaller networks can be trained to perform as well as larger networks with knowledge distillation. \n\nKnowledge distillation has been successfully applied in some other scenarios. Self-distillation is proposed to distill the knowledge from a teacher model to a student model with identical architecture. With self-distillation, the student networks realize performance than the teacher networks in [34], [35] and [36]. Beyond supervised learning, [37] provide theoretical and causal insight about inner workings of generalized distillation and extend it to unsupervised, semi-supervised and multitask learning scenarios. Our method borrow the idea that pseudo-label of a sample can be generated by knowledge distilled from a teacher network when the true label is absent.",
            "score": 0.6415296832112132,
            "section_title": "Knowledge Distillation",
            "char_start_offset": 8128,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 156
                },
                {
                    "start": 157,
                    "end": 466
                },
                {
                    "start": 467,
                    "end": 709
                },
                {
                    "start": 712,
                    "end": 848
                },
                {
                    "start": 849,
                    "end": 933
                },
                {
                    "start": 934,
                    "end": 987
                },
                {
                    "start": 988,
                    "end": 1115
                },
                {
                    "start": 1116,
                    "end": 1259
                },
                {
                    "start": 1262,
                    "end": 1339
                },
                {
                    "start": 1340,
                    "end": 1463
                },
                {
                    "start": 1464,
                    "end": 1578
                },
                {
                    "start": 1579,
                    "end": 1780
                },
                {
                    "start": 1781,
                    "end": 1931
                }
            ],
            "ref_mentions": [
                {
                    "start": 61,
                    "end": 65,
                    "matchedPaperCorpusId": "7200347"
                },
                {
                    "start": 834,
                    "end": 838,
                    "matchedPaperCorpusId": "23316647"
                },
                {
                    "start": 843,
                    "end": 847,
                    "matchedPaperCorpusId": "263861232"
                },
                {
                    "start": 934,
                    "end": 938,
                    "matchedPaperCorpusId": "263861232"
                },
                {
                    "start": 988,
                    "end": 992,
                    "matchedPaperCorpusId": "23316647"
                },
                {
                    "start": 1573,
                    "end": 1577,
                    "matchedPaperCorpusId": "206596723"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.9541015625
        },
        {
            "corpus_id": "219531897",
            "title": "Peer Collaborative Learning for Online Knowledge Distillation",
            "text": "Traditional Knowledge Distillation (Hinton, Vinyals, and Dean 2015) is one of the most effective solutions to compress a cumbersome model or an ensemble of models into a smaller model for deployment. In (Hinton, Vinyals, and Dean 2015), Hinton, Vinyals, and Dean propose to distil the knowledge from a high-capacity teacher model to a compact student model, which is accomplished by aligning soft output predictions between the teacher and the student. In recent years, many promising methods have been designed to transfer various \"knowledge\", such as intermediate representations (Romero et al. 2015), flow between layers (Yim et al. 2017), attention maps (Zagoruyko and Komodakis 2017), structural relations (Park et al. 2019) and activation similarity (Tung and Mori 2019), to facilitate the optimisation process of distillation. Although these methods have shown competitive performance for compressing a model, they typically follow a two-stage training solution by pre-training a high-capacity teacher model for transferring knowledge to a compact student model, which requires more training time and computational cost. Online Knowledge Distillation (Lan, Zhu, and Gong 2018;Chen et al. 2020;Zhang et al. 2018) proposes to directly optimise a target network via distilling knowledge among multiple networks or branches without pre-training a highcapacity teacher, which follows a one-stage end-to-end training strategy. Since online KD directly optimises a target network, there is no need to store or download a teacher model, which saves time and computational cost. In (Song and Chai 2018), Song and Chai propose to distil knowledge among multiple classifier heads of a hierarchical network for improving the generalisation of a target network. In (Zhang et al. 2018), Zhang et al. introduce a mutual learning solution to distil knowledge among multiple parallel networks with the same input. Although these methods help to improve the generalisation of the target network, they only distil limited knowledge among parallel networks or heads and fail to construct a stronger online teacher to further improve the student. In (Guo et al. 2020  parallel networks and aggregate logits from all student networks based on the cross-entropy loss to generate soft",
            "score": 0.6410040623906526,
            "section_title": "Related Work",
            "char_start_offset": 5971,
            "sentence_offsets": [],
            "ref_mentions": [
                {
                    "start": 582,
                    "end": 602,
                    "matchedPaperCorpusId": "2723173"
                },
                {
                    "start": 624,
                    "end": 641,
                    "matchedPaperCorpusId": "206596723"
                },
                {
                    "start": 658,
                    "end": 688,
                    "matchedPaperCorpusId": "829159"
                },
                {
                    "start": 711,
                    "end": 728,
                    "matchedPaperCorpusId": "131765296"
                },
                {
                    "start": 756,
                    "end": 776,
                    "matchedPaperCorpusId": "198179476"
                },
                {
                    "start": 1158,
                    "end": 1183,
                    "matchedPaperCorpusId": "48352434"
                },
                {
                    "start": 1183,
                    "end": 1200,
                    "matchedPaperCorpusId": "208526905"
                },
                {
                    "start": 1200,
                    "end": 1218,
                    "matchedPaperCorpusId": "26071966"
                },
                {
                    "start": 1580,
                    "end": 1600,
                    "matchedPaperCorpusId": "44119099"
                },
                {
                    "start": 1759,
                    "end": 1777,
                    "matchedPaperCorpusId": "26071966"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.96630859375
        },
        {
            "corpus_id": "237845150",
            "title": "Deep Collaborative Learning for Randomly Wired Neural Networks",
            "text": "The implicit ensemble is usually seen as a regularization method to reduce overfitting. Moreover, it can be used along with an explicit ensemble approach. \n\nKnowledge distillation refers to training a smaller model (i.e., a student) to mimic the performance of a large model or an ensemble (i.e., a teacher). The student model is trained with an additional loss function to prompt the model to be identical to the teacher model. Various distillation methods have been introduced to examine different types of loss functions [21,22], different forms of teacher model [23,24], and the best way to train the student model [25,26]. For example, in [27], the authors introduced an approach (called AvgMKD) to distill knowledge from multiple teachers. \n\nThey integrated softened outputs of each teacher equally and imposed constraints on the intermediate layers of the student models using the relative dissimilarity learned from the teacher networks. However, by treating each teacher equally, the differences between teacher models could be lost. In [14], authors proposed an adaptive multi-teacher knowledge distillation method (named AMTML-KD) that extended the previous method by adding an adaptive weight for each teacher model and transferring the intermediatelevel knowledge from hidden layers of the teacher models to the student models. \n\nAnother distillation variant is co-distillation [12,13] where the teacher and student had the same network architecture and were trained in parallel using distillation loss before any model converged. It has shown improvement in the speed of model training and its accuracy. Zhang's method [12] can be seen as co-distillation of models that have different architectures. Our proposed method can be seen as the co-distillation of randomly generated models, but the distillation method is using transfer learning instead of an extra loss function. \n\nKnowledge transfer is another student-teacher paradigm, where the knowledge is transferred by passing the parameters of each layer of a trained teacher model to the student model as initialization before beginning training the student model. The knowledge is transferred from a smaller model to a larger model with function preserving transformations to accelerate the training of the student model. The expansion of the student network can be achieved by increasing its depth, width, or kernel size.",
            "score": 0.6409795285340394,
            "section_title": "Related Work",
            "char_start_offset": 9045,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 87
                },
                {
                    "start": 88,
                    "end": 154
                },
                {
                    "start": 157,
                    "end": 308
                },
                {
                    "start": 309,
                    "end": 428
                },
                {
                    "start": 429,
                    "end": 627
                },
                {
                    "start": 628,
                    "end": 745
                },
                {
                    "start": 748,
                    "end": 945
                },
                {
                    "start": 946,
                    "end": 1042
                },
                {
                    "start": 1043,
                    "end": 1340
                },
                {
                    "start": 1343,
                    "end": 1543
                },
                {
                    "start": 1544,
                    "end": 1617
                },
                {
                    "start": 1618,
                    "end": 1713
                },
                {
                    "start": 1714,
                    "end": 1888
                },
                {
                    "start": 1891,
                    "end": 2132
                },
                {
                    "start": 2133,
                    "end": 2290
                },
                {
                    "start": 2291,
                    "end": 2391
                }
            ],
            "ref_mentions": [
                {
                    "start": 644,
                    "end": 648,
                    "matchedPaperCorpusId": "26021416"
                },
                {
                    "start": 1046,
                    "end": 1050,
                    "matchedPaperCorpusId": "224818016"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.95458984375
        },
        {
            "corpus_id": "259378480",
            "title": "Review of Recent Distillation Studies",
            "text": "Knowledge distillation [7] is a method of transferring the knowledge from a complex model, called the teacher, to a smaller and simpler model, called the student. In recent years, several variants of knowledge distillation have been proposed, including teaching assistant distillation, curriculum distillation, mask distillation, and decoupling distillation. This literature review summarizes the recent developments in these variants of knowledge distillation and discusses their strengths and limitations. Knowledge distillation is a method of compressing a complex deep neural network (DNN) into a smaller and faster DNN while preserving its accuracy. The process of knowledge distillation involves training a smaller DNN, called the student, to imitate the predictions of a larger and more complex DNN, called the teacher. The student network is trained to produce similar results as the teacher network, but with fewer parameters and lower computational cost. Knowledge distillation has been widely used for model compression and acceleration, and has shown great promise in various applications [5,8,13,18], such as computer vision and natural language processing. In recent years, several variants of knowledge distillation [3,4,21] have been proposed and explored to improve the performance of knowledge distillation [6,9]. One of these variants is teaching assistant distillation, which introduces an intermediate model, called the teaching assistant, between the teacher and the student. \n\nThe teaching assistant is trained to mimic the behavior of the teacher, and the student is trained to imitate the outputs of the teaching assistant. This approach has been shown to provide better performance than traditional knowledge distillation, as it can better capture the knowledge learned by the teacher. Another variant of knowledge distillation [10,11] is curriculum distillation, which designs the learning process to follow a curriculum, similar to human education. The curriculum is designed to present easy examples first and gradually increase the difficulty of the examples as the student improves. This approach has been shown to provide better performance than traditional knowledge distillation, especially for tasks that require a lot of prior knowledge. Mask distillation is a variant of knowledge distillation [12,20] that focuses on transferring the attention mechanism learned by the teacher to the student.",
            "score": 0.640639629140071,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 162
                },
                {
                    "start": 163,
                    "end": 358
                },
                {
                    "start": 359,
                    "end": 507
                },
                {
                    "start": 508,
                    "end": 654
                },
                {
                    "start": 655,
                    "end": 826
                },
                {
                    "start": 827,
                    "end": 964
                },
                {
                    "start": 965,
                    "end": 1170
                },
                {
                    "start": 1171,
                    "end": 1331
                },
                {
                    "start": 1332,
                    "end": 1497
                },
                {
                    "start": 1500,
                    "end": 1648
                },
                {
                    "start": 1649,
                    "end": 1811
                },
                {
                    "start": 1812,
                    "end": 1976
                },
                {
                    "start": 1977,
                    "end": 2113
                },
                {
                    "start": 2114,
                    "end": 2273
                },
                {
                    "start": 2274,
                    "end": 2430
                }
            ],
            "ref_mentions": [
                {
                    "start": 1104,
                    "end": 1106,
                    "matchedPaperCorpusId": "235677396"
                },
                {
                    "start": 1328,
                    "end": 1330,
                    "matchedPaperCorpusId": "253270253"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.9560546875
        },
        {
            "corpus_id": "258048760",
            "title": "A Survey on Recent Teacher-student Learning Studies",
            "text": "Knowledge distillation [7] is a method of transferring the knowledge from a complex model, called the teacher, to a smaller and simpler model, called the student. In recent years, several variants of knowledge distillation have been proposed, including teaching assistant distillation, curriculum distillation, mask distillation, and decoupling distillation. This literature review summarizes the recent developments in these variants of knowledge distillation and discusses their strengths and limitations. \n\nKnowledge distillation is a method of compressing a complex deep neural network (DNN) into a smaller and faster DNN while preserving its accuracy. The process of knowledge distillation involves training a smaller DNN, called the student, to imitate the predictions of a larger and more complex DNN, called the teacher. The student network is trained to produce similar results as the teacher network, but with fewer parameters and lower computational cost. Knowledge distillation has been widely used for model compression and acceleration, and has shown great promise in various applications [5,8,13,18], such as computer vision and natural language processing. \n\nIn recent years, several variants of knowledge distillation [3,4,21] have been proposed and explored to improve the performance of knowledge distillation [6,9]. One of these variants is teaching assistant distillation, which introduces an intermediate model, called the teaching assistant, between the teacher and the student. The teaching assistant is trained to mimic the behavior of the teacher, and the student is trained to imitate the outputs of the teaching assistant. This approach has been shown to provide better performance than traditional knowledge distillation, as it can better capture the knowledge learned by the teacher. \n\nAnother variant of knowledge distillation [10,11] is curriculum distillation, which designs the learning process to follow a curriculum, similar to human education. The curriculum is designed to present easy examples first and gradually increase the difficulty of the examples as the student improves. This approach has been shown to provide better performance than traditional knowledge distillation, especially for tasks that require a lot of prior knowledge. \n\nMask distillation is a variant of knowledge distillation [12,20] that focuses on transferring the attention mechanism learned by the teacher to the student.",
            "score": 0.6404564018764685,
            "section_title": "INTRODUCTION",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 162
                },
                {
                    "start": 163,
                    "end": 358
                },
                {
                    "start": 359,
                    "end": 507
                },
                {
                    "start": 510,
                    "end": 656
                },
                {
                    "start": 657,
                    "end": 828
                },
                {
                    "start": 829,
                    "end": 966
                },
                {
                    "start": 967,
                    "end": 1172
                },
                {
                    "start": 1175,
                    "end": 1335
                },
                {
                    "start": 1336,
                    "end": 1501
                },
                {
                    "start": 1502,
                    "end": 1650
                },
                {
                    "start": 1651,
                    "end": 1813
                },
                {
                    "start": 1816,
                    "end": 1980
                },
                {
                    "start": 1981,
                    "end": 2117
                },
                {
                    "start": 2118,
                    "end": 2277
                },
                {
                    "start": 2280,
                    "end": 2436
                }
            ],
            "ref_mentions": [
                {
                    "start": 1106,
                    "end": 1108,
                    "matchedPaperCorpusId": "235677396"
                },
                {
                    "start": 1111,
                    "end": 1114,
                    "matchedPaperCorpusId": "245218794"
                },
                {
                    "start": 1235,
                    "end": 1238,
                    "matchedPaperCorpusId": "257771511"
                },
                {
                    "start": 1240,
                    "end": 1243,
                    "matchedPaperCorpusId": "258841675"
                },
                {
                    "start": 1332,
                    "end": 1334,
                    "matchedPaperCorpusId": "253270253"
                },
                {
                    "start": 1862,
                    "end": 1865,
                    "matchedPaperCorpusId": "252625376"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.9609375
        },
        {
            "corpus_id": "233465558",
            "title": "Vulnerabilities in Federated Learning",
            "text": "Under restricted communication resources, exchanging model parameters becomes too costly, particularly for modern large deep neural networks. In this regard, federated distillation [124] is a compelling FL solution that only transfers the model outputs whose dimensions are commonly much smaller than the model sizes. One foundational algorithm of federated distillation is Knowledge Distillation (KD) [125]. It aims to transfer knowledge from a fully trained model (teacher model) to a smaller model (an empty student model). The teacher model's knowledge of KD can be constructed in several ways. Typically, the knowledge is a pre-trained teacher model's logits, transferred to a small student model for model compression. The knowledge can also be a collection of other student models' logits, in that the collection of forecasts is often more accurate than individual predictions. The notion of sharing knowledge instead of model weights can enhance the robustness of FL and saves communication and computations costs.",
            "score": 0.6389338816385834,
            "section_title": "K. FEDERATED DISTILLATION",
            "char_start_offset": 57342,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 141
                },
                {
                    "start": 142,
                    "end": 317
                },
                {
                    "start": 318,
                    "end": 408
                },
                {
                    "start": 409,
                    "end": 526
                },
                {
                    "start": 527,
                    "end": 598
                },
                {
                    "start": 599,
                    "end": 724
                },
                {
                    "start": 725,
                    "end": 884
                },
                {
                    "start": 885,
                    "end": 1022
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.89404296875
        },
        {
            "corpus_id": "53116124",
            "title": "Deep Transfer Reinforcement Learning for Text Summarization",
            "text": "Knowledge distillation is a class of techniques that train a small network by transferring knowledge from a larger network. These techniques are typically used when we require building models for devices with limited computational power [1]. Usually, in these models, there is a teacher (larger model) and a student (smaller model) and the goal is to transfer knowledge from teacher to student. Recently, researchers have also used this idea to create models through meta-learning [25], few-shot learning [22,28], one-shot learning [6,3], and domain adaptation [7,29], mostly for image classification problems. However, the effect of these types of models on NLP tasks is yet to be studied.",
            "score": 0.6386451558770523,
            "section_title": "Knowledge Distillation",
            "char_start_offset": 5676,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 123
                },
                {
                    "start": 124,
                    "end": 241
                },
                {
                    "start": 242,
                    "end": 394
                },
                {
                    "start": 395,
                    "end": 610
                },
                {
                    "start": 611,
                    "end": 690
                }
            ],
            "ref_mentions": [
                {
                    "start": 237,
                    "end": 240,
                    "matchedPaperCorpusId": "11536917"
                },
                {
                    "start": 505,
                    "end": 509,
                    "matchedPaperCorpusId": "67413369"
                },
                {
                    "start": 509,
                    "end": 512,
                    "matchedPaperCorpusId": "309759"
                },
                {
                    "start": 532,
                    "end": 535,
                    "matchedPaperCorpusId": "8270841"
                },
                {
                    "start": 535,
                    "end": 537,
                    "matchedPaperCorpusId": "2169827"
                },
                {
                    "start": 561,
                    "end": 564,
                    "matchedPaperCorpusId": "2871880"
                },
                {
                    "start": 564,
                    "end": 567,
                    "matchedPaperCorpusId": "4357800"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.82177734375
        },
        {
            "corpus_id": "246200092",
            "title": "Explainable CNN With Fuzzy Tree Regularization for Respiratory Sound Analysis",
            "text": "The development of deep learning has brought about complex models with huge overhead. The application of these complex models in production requires a lot of inference time. Knowledge distillation based on teacher-student framework as an effective model compression method tries to achieve a trade-off between model accuracy and inference efficiency [28]. To improve the applicability of the knowledge distillation framework and the accuracy of the model, some researchers have proposed to improve the framework of distillation from a single teacher to multiple teachers. For example, multiple pre-trained teacher models are directly assigned fixed weights to integrate their predictions [29]. Some researchers have used different teacher models to learn different types of inputs and then used the weighted average to teach student models [30]. In addition, to accelerate the training of the student model in the word embeddings task, multiple teacher models were used to train a student model by combining their logit values such that the student no longer needs the teachers during decoding [31]. However, these approaches only treat teacher models equally, without taking into account the differences between them. In order to solve the conflicts and competition among all teachers, Shangchen et al. formulated ensemble knowledge distillation as a multi-objective optimization problem and assigned dynamic weights to each teacher model [32]. You et al. proposed to use a voting mechanism to unify multiple relative dissimilarity information, which can be transferred into the student network [33]. These existing methods use multiple teacher models to learn category-fixed training samples. In this paper, we use multiple teachers models to learn local data distributed across multiple hospitals with flexible categories.",
            "score": 0.6385759795058383,
            "section_title": "B. Knowledge Distillation with Multiple Teachers",
            "char_start_offset": 9962,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 85
                },
                {
                    "start": 86,
                    "end": 173
                },
                {
                    "start": 174,
                    "end": 355
                },
                {
                    "start": 356,
                    "end": 571
                },
                {
                    "start": 572,
                    "end": 693
                },
                {
                    "start": 694,
                    "end": 845
                },
                {
                    "start": 846,
                    "end": 1099
                },
                {
                    "start": 1100,
                    "end": 1218
                },
                {
                    "start": 1219,
                    "end": 1445
                },
                {
                    "start": 1446,
                    "end": 1601
                },
                {
                    "start": 1602,
                    "end": 1694
                },
                {
                    "start": 1695,
                    "end": 1825
                }
            ],
            "ref_mentions": [
                {
                    "start": 688,
                    "end": 692,
                    "matchedPaperCorpusId": "18195425"
                },
                {
                    "start": 840,
                    "end": 844,
                    "matchedPaperCorpusId": "145841463"
                },
                {
                    "start": 1094,
                    "end": 1098,
                    "matchedPaperCorpusId": "173990395"
                },
                {
                    "start": 1440,
                    "end": 1444,
                    "matchedPaperCorpusId": "227276362"
                },
                {
                    "start": 1596,
                    "end": 1600,
                    "matchedPaperCorpusId": "26021416"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.79736328125
        },
        {
            "corpus_id": "273233176",
            "title": "SNN-PAR: Energy Efficient Pedestrian Attribute Recognition via Spiking Neural Networks",
            "text": "Knowledge Distillation is a technique for model compression that facilitates a smaller student model in learning from a larger teacher model. The student acquires knowledge by imitating various aspects of the teacher, such as its intermediate features [41], prediction logits [22], or activation boundaries [20]]. This approach was originally put forth by Hinton et al. [21] to supervise students based on the hard and soft label's output by the teacher, and nowadays there is a lot of work on using distillation for knowledge transfer to help the model get better performance. Earlier knowledge distillation (KD) techniques can be classified into three distinct categories: distillation from logits, distillation from features, and distillation based on attention. In terms of logit distillation, DIST [24] employs the Pearson correlation coefficient in place of KL divergence, combining both inter-and intra-class relationships. SRRL [62] ensures that the logits output from the teacher and the features of the student, after the teacher's linear layer, are identical. WSLD [75] examines soft labels and assigns varying weights to them based on the bias-variance trade-off. In addition to logit distillation, Several studies [5,43,64,66] concentrate on transferring knowledge through intermediate features. FitNet [41] directly distills semantic information from these intermediate features. AT [68] shifts the attention from feature maps to the student model. RKD [40] extracts relationships from the feature maps. MGD [65] masks the features of the student model, compelling it to replicate the features of the teacher model. To our knowledge, AT [68] is the sole knowledge distillation method that focuses on transferring attention, defining the attention map as a spatial representation that highlights the areas of the input that the model concentrates on the most. Wang et al. propose the HDETrack [56] which employs a hierarchical knowledge distillation strategy to augment the student tracking network from multi-modal or multi-view teacher network. In this paper, we employ both logits and intermediate features for knowledge distillation, believing that the integration of these two methods can greatly enhance knowledge transfer and improve the effectiveness of the student model. 3 Our Proposed Approach",
            "score": 0.6383681608915899,
            "section_title": "Knowledge Distillation",
            "char_start_offset": 8288,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 141
                },
                {
                    "start": 142,
                    "end": 313
                },
                {
                    "start": 314,
                    "end": 577
                },
                {
                    "start": 578,
                    "end": 765
                },
                {
                    "start": 766,
                    "end": 930
                },
                {
                    "start": 931,
                    "end": 1070
                },
                {
                    "start": 1071,
                    "end": 1175
                },
                {
                    "start": 1176,
                    "end": 1308
                },
                {
                    "start": 1309,
                    "end": 1393
                },
                {
                    "start": 1394,
                    "end": 1462
                },
                {
                    "start": 1463,
                    "end": 1517
                },
                {
                    "start": 1518,
                    "end": 1629
                },
                {
                    "start": 1630,
                    "end": 1872
                },
                {
                    "start": 1873,
                    "end": 2059
                },
                {
                    "start": 2060,
                    "end": 2293
                },
                {
                    "start": 2294,
                    "end": 2317
                }
            ],
            "ref_mentions": [
                {
                    "start": 307,
                    "end": 311,
                    "matchedPaperCorpusId": "53213211"
                },
                {
                    "start": 803,
                    "end": 807,
                    "matchedPaperCorpusId": "248986690"
                },
                {
                    "start": 936,
                    "end": 940,
                    "matchedPaperCorpusId": "235613564"
                },
                {
                    "start": 1227,
                    "end": 1230,
                    "matchedPaperCorpusId": "250279758"
                },
                {
                    "start": 1230,
                    "end": 1233,
                    "matchedPaperCorpusId": "236882796"
                },
                {
                    "start": 1233,
                    "end": 1236,
                    "matchedPaperCorpusId": "244488341"
                },
                {
                    "start": 1467,
                    "end": 1471,
                    "matchedPaperCorpusId": "131765296"
                },
                {
                    "start": 1522,
                    "end": 1526,
                    "matchedPaperCorpusId": "248506080"
                },
                {
                    "start": 1906,
                    "end": 1910,
                    "matchedPaperCorpusId": "262822525"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.97216796875
        },
        {
            "corpus_id": "272310354",
            "title": "How Knowledge Distillation Mitigates the Synthetic Gap in Fair Face Recognition",
            "text": "Knowledge distillation, also known as student-teacher architecture, is a technique for transferring knowledge from one model to another, even when they do not have the same architecture. It has been shown it can be useful for different objectives, improving performance, model compression or learning with unlabeled data. Due to the resource-constrained environments in which FR systems run, KD is usually used for model compression. Knowledge is distilled from the large neural network called teacher, to smaller neural network called student. \n\nThe teacher follows the the standard for recent Face Recognition methods [4] and is a ResNet100. This teacher is trained with a classification layer that is removed from the final model, resulting in a backbone that produces an embedding vector and no classification output. Hence, it is not possible to use response-based KD strategies that focus on the approximation of soft-probabilities.",
            "score": 0.6378406237906804,
            "section_title": "Teacher",
            "char_start_offset": 9370,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 186
                },
                {
                    "start": 187,
                    "end": 321
                },
                {
                    "start": 322,
                    "end": 433
                },
                {
                    "start": 434,
                    "end": 544
                },
                {
                    "start": 547,
                    "end": 643
                },
                {
                    "start": 644,
                    "end": 821
                },
                {
                    "start": 822,
                    "end": 938
                }
            ],
            "ref_mentions": [
                {
                    "start": 620,
                    "end": 623,
                    "matchedPaperCorpusId": "237571500"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.95361328125
        },
        {
            "corpus_id": "215238905",
            "title": "Teacher-Class Network: A Neural Network Compression Mechanism",
            "text": "Knowledge distillation via single student: In knowledge distillation, as introduced by Hinton et al. [15], a single student either tries to mimic a single teacher's [6,15,17,18,19,21,23,26,35,36,44] or multiple teachers [9,39,42]. Most of such schemes transfer knowledge to student by minimizing the error between the knowledge of the teacher and the student [15,18]. Rather than matching actual representation, Passalis et al. [26] and Watanabe et al. [36] propose to model the knowledge using probability distribution and then match the distribution of teacher and student networks. Nikolaos et al. [26] try to cater non-classification problems in addition to classification problems. Wang et al. [35] argue that it is hard to figure out which student architecture is more suitable to quantify the information inherited from teacher networks, so they use generative adversarial network (GAN) to learn student network. Belagiannis et al. [3] even studied the distillation of dense features using GANs. Since, teacher can transfer limited amount of knowledge to student, so Mirzadeh et al. [23] propose multi-step knowledge distillation, which employs intermediate-sized networks. Peng et al. [27] propose a framework named correlation congruence for knowledge distillation (CCKD), which transfers the sample level knowledge, yet in addition, it also transfers the correlation between samples. \n\nFew studies that utilize the dense features along with soft-logits claim that the dense features help to generalize the student model [19,21,44]. Heo et al. [14] set out a novel feature distillation technique in which the distillation loss is intended to make an alliance among different aspects: teacher transform, student transform, distillation feature position and distance function. An online strategy has also been proposed that eliminates the need for a two-phase strategy and performs joint training of a teacher network as well as a single multi-branch student network [17]. All these methods, including multi-branch strategy [17] train only a single student on the final logits; we instead train multiple students, each on a chunk of dense representation.",
            "score": 0.6371911613330741,
            "section_title": "Related Work",
            "char_start_offset": 3215,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 230
                },
                {
                    "start": 231,
                    "end": 367
                },
                {
                    "start": 368,
                    "end": 584
                },
                {
                    "start": 585,
                    "end": 686
                },
                {
                    "start": 687,
                    "end": 919
                },
                {
                    "start": 920,
                    "end": 1002
                },
                {
                    "start": 1003,
                    "end": 1180
                },
                {
                    "start": 1181,
                    "end": 1393
                },
                {
                    "start": 1396,
                    "end": 1541
                },
                {
                    "start": 1542,
                    "end": 1783
                },
                {
                    "start": 1784,
                    "end": 1979
                },
                {
                    "start": 1980,
                    "end": 2161
                }
            ],
            "ref_mentions": [
                {
                    "start": 165,
                    "end": 168,
                    "matchedPaperCorpusId": "227276362"
                },
                {
                    "start": 171,
                    "end": 174,
                    "matchedPaperCorpusId": "48352434"
                },
                {
                    "start": 174,
                    "end": 177,
                    "matchedPaperCorpusId": "49869692"
                },
                {
                    "start": 177,
                    "end": 180,
                    "matchedPaperCorpusId": "213068593"
                },
                {
                    "start": 180,
                    "end": 183,
                    "matchedPaperCorpusId": "224818016"
                },
                {
                    "start": 183,
                    "end": 186,
                    "matchedPaperCorpusId": "212908749"
                },
                {
                    "start": 186,
                    "end": 189,
                    "matchedPaperCorpusId": "52012952"
                },
                {
                    "start": 189,
                    "end": 192,
                    "matchedPaperCorpusId": "19182852"
                },
                {
                    "start": 192,
                    "end": 195,
                    "matchedPaperCorpusId": "9154797"
                },
                {
                    "start": 195,
                    "end": 198,
                    "matchedPaperCorpusId": "232417071"
                },
                {
                    "start": 220,
                    "end": 223,
                    "matchedPaperCorpusId": "219965421"
                },
                {
                    "start": 223,
                    "end": 226,
                    "matchedPaperCorpusId": "204788964"
                },
                {
                    "start": 226,
                    "end": 229,
                    "matchedPaperCorpusId": "145916274"
                },
                {
                    "start": 363,
                    "end": 366,
                    "matchedPaperCorpusId": "49869692"
                },
                {
                    "start": 428,
                    "end": 432,
                    "matchedPaperCorpusId": "52012952"
                },
                {
                    "start": 453,
                    "end": 457,
                    "matchedPaperCorpusId": "9154797"
                },
                {
                    "start": 601,
                    "end": 605,
                    "matchedPaperCorpusId": "52012952"
                },
                {
                    "start": 699,
                    "end": 703,
                    "matchedPaperCorpusId": "19182852"
                },
                {
                    "start": 939,
                    "end": 942,
                    "matchedPaperCorpusId": "4375646"
                },
                {
                    "start": 1090,
                    "end": 1094,
                    "matchedPaperCorpusId": "212908749"
                },
                {
                    "start": 1193,
                    "end": 1197,
                    "matchedPaperCorpusId": "102483463"
                },
                {
                    "start": 1530,
                    "end": 1534,
                    "matchedPaperCorpusId": "213068593"
                },
                {
                    "start": 1534,
                    "end": 1537,
                    "matchedPaperCorpusId": "224818016"
                },
                {
                    "start": 1537,
                    "end": 1540,
                    "matchedPaperCorpusId": "232417071"
                },
                {
                    "start": 1553,
                    "end": 1557,
                    "matchedPaperCorpusId": "102483181"
                },
                {
                    "start": 1974,
                    "end": 1978,
                    "matchedPaperCorpusId": "48352434"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.8369140625
        },
        {
            "corpus_id": "258426697",
            "title": "Emotions Beyond Words: Non-Speech Audio Emotion Recognition With Edge Computing",
            "text": "The process of knowledge distillation as the name suggests is the method of transferring knowledge from a larger computationally expensive model to a relatively smaller model. The larger and smaller models are called the teacher and student models respectively. Thus, knowledge distillation consists of three principal components: (1) knowledge; (2) distillation algorithm; and (3) teacherstudent architecture. While there are now multiple methods of distillation algorithms we selected the response-based algorithm. As shown in Figure 3, the hypothesis is that the student model will learn to mimic the predictions of the teacher model. This can be achieved by using a loss function, termed the distillation loss, that captures the difference between the logits of the student and the teacher model respectively. As this loss minimizes overtraining, the student model will improve at making the same predictions as the teacher. In the offline training scheme, the teacher model is first trained and the weights are then frozen. Next, we train the student model using the distillation loss and the logits from the teacher model as targets. Following is the equation of the distillation loss. \n\nwhere: L d : the loss function for knowledge distillation \u03b1: a hyperparameter that controls the trade-off between the classification loss and the distillation loss T : the temperature hyperparameter used to soften the logits (outputs of the last layer before softmax) of the teacher and student models KL: the Kullback-Leibler divergence, a measure of how different two probability distributions are softmax: a function that converts the logits to probabilities f (T, x): the logits of the teacher model for input x g(T, x): the logits of the student model for input x 2) Teacher Model: Generally, for the teacher model a larger and deeper network is chosen so that it performs well on the task at hand. We chose ResNet18 [38] as our teacher model. ResNet18 contains 18 residual blocks stacked together which alleviates the degradation and vanishing gradient problem. Figure 4 shows a single layer, where the outputs of the previous layer are added to the outputs of the next layer, and Figure 5 depicts the complete architecture of ResNet18 used for the teacher model.",
            "score": 0.6362803001654813,
            "section_title": "1) Knowledge Distillation:",
            "char_start_offset": 16396,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 175
                },
                {
                    "start": 176,
                    "end": 261
                },
                {
                    "start": 262,
                    "end": 410
                },
                {
                    "start": 411,
                    "end": 516
                },
                {
                    "start": 517,
                    "end": 637
                },
                {
                    "start": 638,
                    "end": 813
                },
                {
                    "start": 814,
                    "end": 928
                },
                {
                    "start": 929,
                    "end": 1028
                },
                {
                    "start": 1029,
                    "end": 1139
                },
                {
                    "start": 1140,
                    "end": 1191
                },
                {
                    "start": 1194,
                    "end": 1897
                },
                {
                    "start": 1898,
                    "end": 1942
                },
                {
                    "start": 1943,
                    "end": 2263
                }
            ],
            "ref_mentions": [
                {
                    "start": 1916,
                    "end": 1920,
                    "matchedPaperCorpusId": "206594692"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.9716796875
        },
        {
            "corpus_id": "219530513",
            "title": "ResKD: Residual-Guided Knowledge Distillation",
            "text": "A S deep learning goes deeper, state-of-the-art neural net- works [1], [2], [3], [4] have obtained better and better performance, and yet demand more and more computational resources. While models with large capacity can achieve high accuracy, they are impractical for resource-limited devices such as embedded systems. To this end, researchers have studied cost-effective networks [5], [6], [7], [8] and efficient training strategies [9], [10], [11], [12], [13], [14], [15], [16], [17]. Knowledge distillation (KD) [18] has emerged as a compression technique where an analogy of the teacher-student relationship is drawn to explain the idea that the knowledge of a powerful yet heavy teacher network can be distilled into a lightweight student network by adding a loss term that encourages the student to mimic the teacher. \n\nDue to the capacity gap between a heavy teacher and a lightweight student, there is still a significant performance gap between them. Existing KD methods have made efforts to modify the loss term to improve the student's performance [9], [19], [20], [21], [22]. However, the discrepancy between a teacher and a student can be considered as knowledge, and it remains relatively unexplored in knowledge distillation. \n\nIn this paper, we see knowledge distillation in a fresh light, using the knowledge gap between a teacher and a student as guidance. First, we train a student network  0 from a teacher  as usual. Then, we train a much more lightweight X. Li, S. Li, B. Omar, F. Wu and X. Li are with College of Computer Science and Technology, Zhejiang University, Hangzhou 310027, China. Email: {3150104097, leizungjyun, bourahla, xilizju}@zju.edu.cn. \n\nThe first two authors (Xuewei Li and Songyuan Li) contribute equally. \n\n(Correspongding author: Xi Li.)",
            "score": 0.6361304838078876,
            "section_title": "I. INTRODUCTION",
            "char_start_offset": 18,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 183
                },
                {
                    "start": 184,
                    "end": 319
                },
                {
                    "start": 320,
                    "end": 487
                },
                {
                    "start": 488,
                    "end": 824
                },
                {
                    "start": 827,
                    "end": 960
                },
                {
                    "start": 961,
                    "end": 1088
                },
                {
                    "start": 1089,
                    "end": 1241
                },
                {
                    "start": 1244,
                    "end": 1375
                },
                {
                    "start": 1376,
                    "end": 1438
                },
                {
                    "start": 1439,
                    "end": 1614
                },
                {
                    "start": 1615,
                    "end": 1678
                },
                {
                    "start": 1681,
                    "end": 1750
                },
                {
                    "start": 1753,
                    "end": 1784
                }
            ],
            "ref_mentions": [
                {
                    "start": 66,
                    "end": 69,
                    "matchedPaperCorpusId": "206594692"
                },
                {
                    "start": 71,
                    "end": 74,
                    "matchedPaperCorpusId": "9433631"
                },
                {
                    "start": 387,
                    "end": 390,
                    "matchedPaperCorpusId": "4555207"
                },
                {
                    "start": 392,
                    "end": 395,
                    "matchedPaperCorpusId": "24982157"
                },
                {
                    "start": 397,
                    "end": 400,
                    "matchedPaperCorpusId": "51880435"
                },
                {
                    "start": 435,
                    "end": 438,
                    "matchedPaperCorpusId": "26071966"
                },
                {
                    "start": 446,
                    "end": 450,
                    "matchedPaperCorpusId": "23316647"
                },
                {
                    "start": 452,
                    "end": 456,
                    "matchedPaperCorpusId": "51606880"
                },
                {
                    "start": 470,
                    "end": 474,
                    "matchedPaperCorpusId": "91183944"
                },
                {
                    "start": 482,
                    "end": 486,
                    "matchedPaperCorpusId": "219636179"
                },
                {
                    "start": 1060,
                    "end": 1063,
                    "matchedPaperCorpusId": "26071966"
                },
                {
                    "start": 1065,
                    "end": 1069,
                    "matchedPaperCorpusId": "102483463"
                },
                {
                    "start": 1077,
                    "end": 1081,
                    "matchedPaperCorpusId": "203642130"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.919921875
        },
        {
            "corpus_id": "275007781",
            "title": "Knowledge Reasoning- and Progressive Distillation-Integrated Detection of Electrical Construction Violations",
            "text": "This approach distills knowledge at different levels, reducing potential information loss in the transfer process to the maximum extent. \n\nFor example, at lower levels, the model might focus on learning basic visual features and patterns, such as the shape and color of safety helmets and harnesses. At higher levels, it can capture more complex behavior patterns, such as unsafe work postures or potential risky behaviors. \n\nTo better demonstrate the effectiveness of multi-level knowledge distillation, we visualized the feature maps of the teacher model, the original student model, and the student model after knowledge distillation, as shown in Figure 8. It is evident that after applying knowledge distillation, the model becomes more focused on the target and its corresponding regions. This highlights how the distillation process enhances the model's ability to capture relevant features, improving its overall detection performance. \n\nTo verify the effectiveness of progressive distillation, we conducted experiments using different teacher models. The results are shown in Table 5, and further demonstrate the impact of selecting various teacher models on the performance of the student network. This comparison highlights how different teacher models contribute to guiding the student network in learning more effectively, improving overall detection accuracy. It can be observed that compared to directly using a single advanced-teacher or primary-teacher model for knowledge distillation, the progressive distillation strategy achieves better results. Through phased learning, progressive distillation first leverages the rich expressive capabilities of the advanced teacher model to establish a solid knowledge foundation for the primary-teacher model. Subsequently, the primary teacher gradually guides the student model, enabling effective knowledge transfer. \n\nThis multi-stage process ensures that the student model can effectively absorb knowledge from both teacher models, leading to improved performance in complex scenarios such as detecting safety behaviors in power construction environments. The gradual knowledge transfer helps mitigate the gap between the advanced and student models, optimizing both the learning efficiency and detection accuracy. It is evident that after applying knowledge distillation, the model becomes more focused on the target and its corresponding regions. This highlights how the distillation process enhances the model's ability to capture relevant features, improving its overall detection performance.",
            "score": 0.635828920840199,
            "section_title": "Ablation Study",
            "char_start_offset": 46144,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 136
                },
                {
                    "start": 139,
                    "end": 299
                },
                {
                    "start": 300,
                    "end": 423
                },
                {
                    "start": 426,
                    "end": 659
                },
                {
                    "start": 660,
                    "end": 793
                },
                {
                    "start": 794,
                    "end": 942
                },
                {
                    "start": 945,
                    "end": 1058
                },
                {
                    "start": 1059,
                    "end": 1206
                },
                {
                    "start": 1207,
                    "end": 1372
                },
                {
                    "start": 1373,
                    "end": 1565
                },
                {
                    "start": 1566,
                    "end": 1767
                },
                {
                    "start": 1768,
                    "end": 1876
                },
                {
                    "start": 1879,
                    "end": 2117
                },
                {
                    "start": 2118,
                    "end": 2276
                },
                {
                    "start": 2277,
                    "end": 2410
                },
                {
                    "start": 2411,
                    "end": 2559
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.93505859375
        },
        {
            "corpus_id": "236976374",
            "title": "Semi-Supervised Domain Generalizable Person Re-Identification",
            "text": "The learning of a small network from a large network is later formally popularized as vanilla knowledge distillation (KD) [17] where a small student model is generally supervised by a large teacher model. The main idea is that the student model mimics the teacher model to achieve competitive or even superior performance. Particularly, knowledge is transferred from the teacher model to the student by minimizing a loss function in which the target is the distribution of class probabilities predicted by the teacher model. \n\nExisting knowledge distillation methods can be grouped into the three categories. The response-based Knowledge knowledge distillation uses the logits of a large deep model as the teacher knowledge [17], [21], [33]. The activations, neurons or features of intermediate layers also can be adopted as the feature-based knowledge distillation [1], [16], [23] to instruct students in the learning of the model. The relationship-based knowledge distillation [24], [28], [39], [45] further explores the relationships between different activations, neurons or pairs of samples. KD, as a quite popular technique, has been widely applied in model compression, object detection, and various feature learning tasks. Motivated by KD, we provide a feasible solution to combine labeled and unlabeled data to improve the generalization ability of person re-id models.",
            "score": 0.6357717054282868,
            "section_title": "Knowledge Distillation",
            "char_start_offset": 7516,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 204
                },
                {
                    "start": 205,
                    "end": 322
                },
                {
                    "start": 323,
                    "end": 524
                },
                {
                    "start": 527,
                    "end": 608
                },
                {
                    "start": 609,
                    "end": 741
                },
                {
                    "start": 742,
                    "end": 932
                },
                {
                    "start": 933,
                    "end": 1096
                },
                {
                    "start": 1097,
                    "end": 1230
                },
                {
                    "start": 1231,
                    "end": 1378
                }
            ],
            "ref_mentions": [
                {
                    "start": 730,
                    "end": 734,
                    "matchedPaperCorpusId": "3608236"
                },
                {
                    "start": 736,
                    "end": 740,
                    "matchedPaperCorpusId": "212908749"
                },
                {
                    "start": 866,
                    "end": 869,
                    "matchedPaperCorpusId": "118649278"
                },
                {
                    "start": 871,
                    "end": 875,
                    "matchedPaperCorpusId": "53213211"
                },
                {
                    "start": 985,
                    "end": 989,
                    "matchedPaperCorpusId": "198185886"
                },
                {
                    "start": 991,
                    "end": 995,
                    "matchedPaperCorpusId": "198179476"
                },
                {
                    "start": 997,
                    "end": 1001,
                    "matchedPaperCorpusId": "102351826"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.9736328125
        },
        {
            "corpus_id": "276282412",
            "title": "Selecting Images With Entropy for Frugal Knowledge Distillation",
            "text": "The Knowledge distillation takes place in a pretrained teacher and an undistilled student as we can see in the Figure 2 step c. The pretrained teacher is a deep learning model that facilitates the transfer of knowledge to the student model using the images included in the subset. Throughout the distillation process, the teacher model provides supervisory signals as soft targets to the student model, enabling effective knowledge transfer, robust performance and enhance the generalization capabilities in image classification tasks.",
            "score": 0.6349594461380175,
            "section_title": "F. PRETRAINED TEACHER",
            "char_start_offset": 21737,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 127
                },
                {
                    "start": 128,
                    "end": 280
                },
                {
                    "start": 281,
                    "end": 535
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.8232421875
        },
        {
            "corpus_id": "242069370",
            "title": "Boost Precision Agriculture with Unmanned Aerial Vehicle Remote Sensing and Edge Intelligence: A Survey",
            "text": "The main objective of knowledge distillation is to train a student network from the teacher network while maintaining its generalization capability [204]. The student network is lighter, i.e., having a smaller model size and less computation, but with the same or comparable performance as the larger network. \n\nGreat efforts have been done to improve the supervision of student network by different knowledge transferred. Romero et al. [205] proposed a FitNets model which teaches the student network to imitate the hints from both middle layers and output layer of the teach network. Instead of hard labels that are used, the work in [206] utilizes soft labels as the representation from teacher network. Kim et al. [207] proposed a paraphrasing based knowledge transfer method which uses convolution operations to paraphrase the teacher model knowledge and translate it to a student model. From the point of teacher networks, student networks can also learn knowledge from multiple teachers [208]. \n\nIn the field of UAV based deep model inference, knowledge distillation is a promising direction. In [209], YOLO + MobileNet model acts as the teacher network, while the pruned model functions as the student network, and knowledge distillation algorithm is used to improve the detection accuracy of the pruned model. Qiu et al. [210] propose to distill knowledge to a lighter distilled network through soft labels from trained teacher network MobileNet. Similar applications using knowledge distillation for model compression can be found in [211,212].",
            "score": 0.6347976118438015,
            "section_title": "Knowledge Distillation",
            "char_start_offset": 49981,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 154
                },
                {
                    "start": 155,
                    "end": 309
                },
                {
                    "start": 312,
                    "end": 422
                },
                {
                    "start": 423,
                    "end": 585
                },
                {
                    "start": 586,
                    "end": 706
                },
                {
                    "start": 707,
                    "end": 892
                },
                {
                    "start": 893,
                    "end": 1000
                },
                {
                    "start": 1003,
                    "end": 1099
                },
                {
                    "start": 1100,
                    "end": 1318
                },
                {
                    "start": 1319,
                    "end": 1455
                },
                {
                    "start": 1456,
                    "end": 1554
                }
            ],
            "ref_mentions": [
                {
                    "start": 148,
                    "end": 153,
                    "matchedPaperCorpusId": "211062209"
                },
                {
                    "start": 636,
                    "end": 641,
                    "matchedPaperCorpusId": "84176918"
                },
                {
                    "start": 718,
                    "end": 723,
                    "matchedPaperCorpusId": "3608236"
                },
                {
                    "start": 994,
                    "end": 999,
                    "matchedPaperCorpusId": "219559263"
                },
                {
                    "start": 1103,
                    "end": 1108,
                    "matchedPaperCorpusId": "226730806"
                },
                {
                    "start": 1544,
                    "end": 1549,
                    "matchedPaperCorpusId": "231977039"
                },
                {
                    "start": 1549,
                    "end": 1553,
                    "matchedPaperCorpusId": "233196782"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.95458984375
        },
        {
            "corpus_id": "245837482",
            "title": "Robust and Resource-Efficient Data-Free Knowledge Distillation by Generative Pseudo Replay",
            "text": "Knowledge Distillation (KD)\n\nKD (Hinton, Vinyals, and Dean 2015) can be defined as transferring the learnt prediction behavior of a complex neural network model to a relatively smaller one. In literature, the complex network is often referred as the \"teacher\" and the compact network is referred as the \"student\". Throughout the distillation process, the student is trained with the guidance of the ground truth labels as well as the teacher's responses to the training data. The inclusion of teacher's responses in the training objective consolidates the information provided to the student about the data-label relationships. Thus the student network can achieve much higher accuracy than when it is trained with the supervision of only the ground truth labels. This is viewed as a form of compression, as the compact student model approximates the teacher by carrying more information than it could have learnt from data on its own.",
            "score": 0.6343704340050804,
            "section_title": "Related Work",
            "char_start_offset": 6011,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.91943359375
        },
        {
            "corpus_id": "273969804",
            "title": "Quantifying Knowledge Distillation Using Partial Information Decomposition",
            "text": "Modern-day machine learning requires large amounts of compute for both training and inference. Knowledge distillation [1,2] can be used to compress a complex machine learning model (the teacher) by distilling it into a relatively simpler model (the student). The term \"distillation\" in this context means obtaining some assistance from the teacher while training the student so that the student performs much better than when trained alone (see Figure 1). In its earliest forms, knowledge distillation involved the student trying to match the output logits of the teacher [1]. More advanced methods focus on distilling multiple intermediate representations of the teacher to the corresponding layers of the student [2][3][4][5]. We also refer the reader to [6,7] for surveys. \n\nInformation theory has been instrumental in both designing [3,4] and explaining [8,9] knowledge distillation techniques. However, less attention has been given to characterizing the fundamental limits of the process from an information-theoretical perspective. Our goal is to bridge this gap by first introducing new measures to quantify the \"transferred knowledge\" and \"knowledge to distill\" for a teacher and a student model given a target downstream task. We bring in an emerging body of work called Partial Information Decomposition (PID) [10][11][12] to explain knowledge distillation. We define the knowledge to distill using the PID measure of \"unique\" information about the task that is available only with the teacher but not the student. As it follows, the transferred knowledge is succinctly quantified by the measure of \"redundant\" information that is common between the teacher and student. \n\nWe propose a multi-level optimization that maximizes redundant information (transferred knowledge) as a regularizer for more effective distillation. While PID has been explored in a few avenues of machine learning, Figure 1: Knowledge Distillation: The teacher (a complex model) assists the student (usually a substantially simpler model) during their training. The learned student can perform much better than an independently trained student without distillation with a similar training setup (i.e., hyperparameters and data). The teacher may or may not have been trained for the same task as the student.",
            "score": 0.6342960030505805,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 94
                },
                {
                    "start": 95,
                    "end": 258
                },
                {
                    "start": 259,
                    "end": 455
                },
                {
                    "start": 456,
                    "end": 576
                },
                {
                    "start": 577,
                    "end": 728
                },
                {
                    "start": 729,
                    "end": 775
                },
                {
                    "start": 778,
                    "end": 898
                },
                {
                    "start": 899,
                    "end": 1038
                },
                {
                    "start": 1039,
                    "end": 1236
                },
                {
                    "start": 1237,
                    "end": 1368
                },
                {
                    "start": 1369,
                    "end": 1525
                },
                {
                    "start": 1526,
                    "end": 1681
                },
                {
                    "start": 1684,
                    "end": 1832
                },
                {
                    "start": 1833,
                    "end": 2045
                },
                {
                    "start": 2046,
                    "end": 2212
                },
                {
                    "start": 2213,
                    "end": 2291
                }
            ],
            "ref_mentions": [
                {
                    "start": 121,
                    "end": 123,
                    "matchedPaperCorpusId": "2723173"
                },
                {
                    "start": 715,
                    "end": 718,
                    "matchedPaperCorpusId": "2723173"
                },
                {
                    "start": 718,
                    "end": 721,
                    "matchedPaperCorpusId": "118649278"
                },
                {
                    "start": 721,
                    "end": 724,
                    "matchedPaperCorpusId": "204838340"
                },
                {
                    "start": 724,
                    "end": 727,
                    "matchedPaperCorpusId": "252693152"
                },
                {
                    "start": 757,
                    "end": 760,
                    "matchedPaperCorpusId": "219559263"
                },
                {
                    "start": 837,
                    "end": 840,
                    "matchedPaperCorpusId": "118649278"
                },
                {
                    "start": 840,
                    "end": 842,
                    "matchedPaperCorpusId": "204838340"
                },
                {
                    "start": 858,
                    "end": 861,
                    "matchedPaperCorpusId": "251643827"
                },
                {
                    "start": 861,
                    "end": 863,
                    "matchedPaperCorpusId": "252846591"
                },
                {
                    "start": 1325,
                    "end": 1329,
                    "matchedPaperCorpusId": "1031742"
                },
                {
                    "start": 1329,
                    "end": 1333,
                    "matchedPaperCorpusId": "10901107"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.97607421875
        },
        {
            "corpus_id": "258187531",
            "title": "Deep Collective Knowledge Distillation",
            "text": "Knowledge Distillation The work of Hinton et al. [8] has revolutionized the literature of model compression by distilling the knowledge of a large model to a smaller model to construct a compact model. Hinton et al. [8] explained that the distribution of a teacher network, which is smoothed using temperature, can more clearly represent secondary probabilities. These secondary probabilities convey information about the correlation between labels and are crucial for teaching student networks. AT [24] attempted to improve knowledge distillation by including the effects of attention in models. FitNet [18] expected to mimic the teacher model by transferring the intermediate representation to the student model. FT [10] used paraphrased factors compressed from the teacher's features to train the student, and RKD [14] utilized structural relations of data. OFD [7] proposed a feature distillation method, including a feature transform to keep the information of the teacher's features. CC [15] focused on the correlation between the instances to transfer, and CRD [21] demonstrated how to make the student's representation more similar to the teacher's representation using a contrastive objective. ReviewKD [16] proposed a review mechanism that transfers multi-level knowledge of the teacher to one-level of the student. \n\nMutual Learning We provide DCKD as a novel approach for knowledge distillation, but our work is also related to mutual learning methods because multiple student models train each other. Deep mutual learning (DML) [26] trained multiple initialized models to gain remarkable performance without any pretrained teacher. ONE [12] ensembled logits of multiple students to generate an on-the-fly teacher and trained each student with it. OKDDip [2] sim- ilarly constructed an ensemble with students but considered attention-based weights for each student. KDCL [5] regarded all networks as students, proposed methods to generate a soft target from all these students, and mentioned that the generated soft target is transferred to all students as knowledge. The existing methods [5,12] ensemble the outputs of each student to generate a single soft target and use it to train the students with the forward direction of Kullback-Leibler divergence.",
            "score": 0.6339116375235934,
            "section_title": "Related Works",
            "char_start_offset": 3324,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 201
                },
                {
                    "start": 202,
                    "end": 362
                },
                {
                    "start": 363,
                    "end": 495
                },
                {
                    "start": 496,
                    "end": 596
                },
                {
                    "start": 597,
                    "end": 714
                },
                {
                    "start": 715,
                    "end": 860
                },
                {
                    "start": 861,
                    "end": 989
                },
                {
                    "start": 990,
                    "end": 1202
                },
                {
                    "start": 1203,
                    "end": 1325
                },
                {
                    "start": 1328,
                    "end": 1513
                },
                {
                    "start": 1514,
                    "end": 1644
                },
                {
                    "start": 1645,
                    "end": 1759
                },
                {
                    "start": 1760,
                    "end": 1877
                },
                {
                    "start": 1878,
                    "end": 2079
                },
                {
                    "start": 2080,
                    "end": 2269
                }
            ],
            "ref_mentions": [
                {
                    "start": 499,
                    "end": 503,
                    "matchedPaperCorpusId": "829159"
                },
                {
                    "start": 604,
                    "end": 608,
                    "matchedPaperCorpusId": "2723173"
                },
                {
                    "start": 718,
                    "end": 722,
                    "matchedPaperCorpusId": "3608236"
                },
                {
                    "start": 817,
                    "end": 821,
                    "matchedPaperCorpusId": "131765296"
                },
                {
                    "start": 993,
                    "end": 997,
                    "matchedPaperCorpusId": "102483463"
                },
                {
                    "start": 1068,
                    "end": 1072,
                    "matchedPaperCorpusId": "204838340"
                },
                {
                    "start": 1212,
                    "end": 1216,
                    "matchedPaperCorpusId": "233296935"
                },
                {
                    "start": 1541,
                    "end": 1545,
                    "matchedPaperCorpusId": "26071966"
                },
                {
                    "start": 1649,
                    "end": 1653,
                    "matchedPaperCorpusId": "48352434"
                },
                {
                    "start": 1767,
                    "end": 1770,
                    "matchedPaperCorpusId": "208526905"
                },
                {
                    "start": 1883,
                    "end": 1886,
                    "matchedPaperCorpusId": "219965421"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.93994140625
        },
        {
            "corpus_id": "248478798",
            "title": "DCML: Deep contrastive mutual learning for COVID-19 recognition",
            "text": "Deep neural networks have achieved remarkable results in the fields of CV, speech recognition, and natural language processing. To complete more complex tasks, the corresponding network must use a deeper or wider structure. Although these neural networks have achieved satisfactory performance on specific tasks, lots of computing requirements make them difficult to be deployed on those resource-constrained environments, which may limit their practicalities. To address this issue, Hinton [52] proposed the famous model distillation method. It regards a pre-trained large network as a teacher that provides additional knowledge to a small network (student). The student network imitates the category probability estimated by the teacher network. And the student network can even obtain better performance. The basic principle behind model distillation [52] is using the additional supervision from the teacher model to train the student model, which surpasses the traditional supervised learning objective. Since then, most offline distillation methods have followed this principle. In [52][53][54][55], the classification probability distribution of the corresponding teacher model was used as the additional supervision. However, the above traditional model distillation methods need a pre-trained large network. And they only employ a one-way knowledge transfer, which cannot take full advantage of the teacher and student networks.\n\nTo address this problem, Zhang [56] proposed a deep mutual learning (DML) strategy in which a group of student networks learns and guides each other throughout the whole training process. Instead of the statically one-way knowledge conversion between the teacher and student networks, DML uses multiple networks to train at the same time. Each network not only accepts the supervision from the ground-truth but also refers to the learning experience from the peer network. All these can further improve the generalization ability of the whole framework. Finally, the two networks share learning experiences (dark knowledge) to achieve mutual learning and common great progress. More importantly, online knowledge distillation (KD) between heterogeneous networks can be realized easily. Anil et al. [57] and Gao et al. [58] further extended the DML idea to accelerate the training procedure of a large-scale distributed neural network. Although the above researches have promoted the progress of KD, none of them absorbed the contrastive learning idea to the distillation procedure. Moreover, to",
            "score": 0.6334414676590593,
            "section_title": "Deep mutual learning",
            "char_start_offset": 9907,
            "sentence_offsets": [],
            "ref_mentions": [
                {
                    "start": 1096,
                    "end": 1100,
                    "matchedPaperCorpusId": "208139041"
                },
                {
                    "start": 1100,
                    "end": 1104,
                    "matchedPaperCorpusId": "203642130"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.8681640625
        },
        {
            "corpus_id": "258987632",
            "title": "Graph Entropy Minimization for Semi-supervised Node Classification",
            "text": "Knowledge distillation [8] is to transfer the knowledge from a well-trained, parameter-freeze teacher model to a small-scaled, optimizable student model by aligning their outputs or intermediate representations. After knowledge distillation, the student model can have inference performance comparable to the teacher model. Due to their small scales and simple structures, the student models are usually deployed to industrial platforms to infer at speed. Recent studies utilize knowledge distillation to derive shallow GNNs [31] or even MLPs [34] that inherit performance from deep GNNs for deployment purposes. However, a large-capacity teacher model may not always be available. Online knowledge distillation is thus proposed to update one or multiple student models without a teacher model. An effective way to achieve this is to induce predictions that are consistent with relevant data such as the universal label distribution of same-labelled nodes [32].",
            "score": 0.6333267809381409,
            "section_title": "Online knowledge distillation",
            "char_start_offset": 8374,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 211
                },
                {
                    "start": 212,
                    "end": 323
                },
                {
                    "start": 324,
                    "end": 455
                },
                {
                    "start": 456,
                    "end": 612
                },
                {
                    "start": 613,
                    "end": 681
                },
                {
                    "start": 682,
                    "end": 794
                },
                {
                    "start": 795,
                    "end": 961
                }
            ],
            "ref_mentions": [
                {
                    "start": 23,
                    "end": 26,
                    "matchedPaperCorpusId": "219559263"
                },
                {
                    "start": 525,
                    "end": 529,
                    "matchedPaperCorpusId": "221191168"
                },
                {
                    "start": 543,
                    "end": 547,
                    "matchedPaperCorpusId": "239015813"
                },
                {
                    "start": 956,
                    "end": 960,
                    "matchedPaperCorpusId": "214727822"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.94091796875
        },
        {
            "corpus_id": "252846591",
            "title": "Efficient Knowledge Distillation from Model Checkpoints",
            "text": "Knowledge distillation (KD) [1,2] has been proved to be an effective technique to promote the performance of a low-capacity model by transferring \"dark knowledge\" from a large teacher model. Empirically, there usually exists a strong correlation between the performance of the teacher model and the student model. For this reason, it is a standard practice to use a well trained network or an ensemble of multiple well trained networks as the teacher [3,4,5], and some researches are attempted to improve distillation performance via boosting the ensemble performance [6,7]. The underlying assumption is that high performing teachers lead to better student models. However, this viewpoint has been challenged by some recent works [8,9,10,11,12], in which it has been observed that a large model capacity gap between the teacher and student may have a negative effect for knowledge transfer. To address this issue, researchers have proposed to employ an intermediate-size network [8] or an assistant network [9] to improve the distillation performance in such scenarios. In [10], a \"tolerant\" teacher model is designed by using a softened loss function. In [11], Park et al. proposed to learn student-friendly teacher by plugging in student branches during the training procedure. Nevertheless, there is no clear theoretical explanation for the gap between teacher and student, and the search for a substitute teacher is not straightforward. [13] can serve as a better teacher than the strong Full Ensemble. \n\nIn this paper, we make an intriguing observation that further supports the viewpoint that high performing models may not necessarily be good teachers, but from a novel perspective. Specifically, we find that an unconverged intermediate model from the middle of the training procedure, often serves as a better teacher than the final converged model, although the former has much lower accuracy (as illustrated in Figure 1(a)) . Moreover, a weak snapshot ensemble of intermediate teacher models along the same optimization path (denoted as Snapshot Ensemble, which is a variant of [13] 3 ) can outperform the standard ensemble of an equal number of independently trained teacher models (denoted as Full Ensemble).",
            "score": 0.6332831443441833,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 190
                },
                {
                    "start": 191,
                    "end": 313
                },
                {
                    "start": 314,
                    "end": 574
                },
                {
                    "start": 575,
                    "end": 664
                },
                {
                    "start": 665,
                    "end": 890
                },
                {
                    "start": 891,
                    "end": 1069
                },
                {
                    "start": 1070,
                    "end": 1152
                },
                {
                    "start": 1153,
                    "end": 1279
                },
                {
                    "start": 1280,
                    "end": 1440
                },
                {
                    "start": 1441,
                    "end": 1506
                },
                {
                    "start": 1509,
                    "end": 1689
                },
                {
                    "start": 1690,
                    "end": 1936
                },
                {
                    "start": 1937,
                    "end": 2221
                }
            ],
            "ref_mentions": [
                {
                    "start": 28,
                    "end": 31,
                    "matchedPaperCorpusId": "11253972"
                },
                {
                    "start": 451,
                    "end": 454,
                    "matchedPaperCorpusId": "4110009"
                },
                {
                    "start": 454,
                    "end": 456,
                    "matchedPaperCorpusId": "26071966"
                },
                {
                    "start": 456,
                    "end": 458,
                    "matchedPaperCorpusId": "26021416"
                },
                {
                    "start": 568,
                    "end": 571,
                    "matchedPaperCorpusId": "208526905"
                },
                {
                    "start": 571,
                    "end": 573,
                    "matchedPaperCorpusId": "222124879"
                },
                {
                    "start": 730,
                    "end": 733,
                    "matchedPaperCorpusId": "212908749"
                },
                {
                    "start": 735,
                    "end": 738,
                    "matchedPaperCorpusId": "54986302"
                },
                {
                    "start": 738,
                    "end": 741,
                    "matchedPaperCorpusId": "231925118"
                },
                {
                    "start": 741,
                    "end": 744,
                    "matchedPaperCorpusId": "203642130"
                },
                {
                    "start": 979,
                    "end": 982,
                    "matchedPaperCorpusId": "212908749"
                },
                {
                    "start": 1073,
                    "end": 1077,
                    "matchedPaperCorpusId": "54986302"
                },
                {
                    "start": 1156,
                    "end": 1160,
                    "matchedPaperCorpusId": "231925118"
                },
                {
                    "start": 1441,
                    "end": 1445,
                    "matchedPaperCorpusId": "6820006"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.8740234375
        },
        {
            "corpus_id": "258108057",
            "title": "Knowledge-Distilled Graph Neural Networks for Personalized Epileptic Seizure Detection",
            "text": "Knowledge Distillation Knowledge distillation (KD) [11] refers to transferring knowledge from a large/sophisticated pre-trained neural network (known as the teacher network) to a smaller network (known as the student network). The student represents a light-weight model derived from the teacher while enforcing the performance to be similar to that of the teacher. A distillation loss is used during training to guide the student to replicate the teacher's behavior as closely as possible. Different types of knowledge can be transferred, but the most straightforward one is response-based KD, which refers to the response of the output layer of the teacher. A widely used example of this is the class probability called as soft targets defined using a softmax function as \n\nwhere p i is the probability of belonging to class i, z is the vector of logits (outputs of the last layer of the teacher to a given input). The temperature T controls the contribution of each soft target to the knowledge. When T is equal to 1, we get the standard softmax function, but as T increases, the probability distribution is softened. The distillation loss can be seen as comparing the class probabilities obtained from the teacher and the student. It enforces the distribution of the outputs produced by the student to be close to that of the teacher. The Kullback-Leibler (KL) divergence is therefore often used as the distillation loss function, and minimizing this loss during training makes the logits of the student get closer to the logits of the teacher [10]. Let z t and z s denote the representation produced by the teacher and student models, respectively, for the same input. Then, the final loss function used to train the student is a weighted average of the two terms and is defined as \n\nwhere L D is the distillation loss function, p(z t , T ) are the teacher soft targets, p(z s , T ) are the student soft targets, L CE is the cross entropy loss function, y are the ground truth labels, and \u03b1 is the weighting factor. The parameter \u03b4 represents the relative weight given to the teacher's knowledge over the new training data corresponding to the student training \u2212 the higher \u03b4, the lesser the model relies on the teacher for the training of the student.",
            "score": 0.6332509875522296,
            "section_title": "Preliminaries",
            "char_start_offset": 7650,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 226
                },
                {
                    "start": 227,
                    "end": 365
                },
                {
                    "start": 366,
                    "end": 490
                },
                {
                    "start": 491,
                    "end": 659
                },
                {
                    "start": 660,
                    "end": 773
                },
                {
                    "start": 776,
                    "end": 916
                },
                {
                    "start": 917,
                    "end": 998
                },
                {
                    "start": 999,
                    "end": 1120
                },
                {
                    "start": 1121,
                    "end": 1234
                },
                {
                    "start": 1235,
                    "end": 1338
                },
                {
                    "start": 1339,
                    "end": 1553
                },
                {
                    "start": 1554,
                    "end": 1673
                },
                {
                    "start": 1674,
                    "end": 1786
                },
                {
                    "start": 1789,
                    "end": 2020
                },
                {
                    "start": 2021,
                    "end": 2257
                }
            ],
            "ref_mentions": [
                {
                    "start": 1548,
                    "end": 1552,
                    "matchedPaperCorpusId": "219559263"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.96533203125
        },
        {
            "corpus_id": "267068302",
            "title": "Effective Intrusion Detection in Heterogeneous Internet-of-Things Networks via Ensemble Knowledge Distillation-Based Federated Learning",
            "text": "Knowledge distillation, first proposed by Hinton [18], allows transferring the knowledge of a large, complex model (known as the teacher) to a smaller, simpler model (known as the student). The motivation behind knowledge distillation is to train the student model to mimic the behaviour of the teacher model. The process of knowledge transfer usually needs a proxy dataset as the medium. As a result, most of the works will choose a mutually exclusive dataset while some others may use an autoencoder or GAN to generate some synthetic dataset. Once the proxy dataset is chosen, the class probabilities of the model's last layer (logits) or feature representations of middle hidden layers (feature maps) are usually used as the soft targets for knowledge transfer. The reason is that they contain more valuable information than the hard labels used in normal training. \n\nFor the original knowledge distillation, loss typically consists of two terms: a standard cross-entropy loss term and a distillation loss term. The former uses the hard label as the target while the latter uses a soft target. For the federated learning scenario, we usually do not have the publicly labelled dataset. Therefore, the combined loss is reduced to only the distillation loss term. The distillation loss term is often formulated as the Kullback-Leibler (KL) divergence between the teacher and student's softmax output probabilities, shown in Equation (1). \n\nwhere S and T represent the student and teacher logits respectively. \u03c3 is the softmax function and a temperate T is added.",
            "score": 0.6332474250670089,
            "section_title": "III. AN EFFECTIVE HETEROGENEOUS INTRUSION DETECTION SYSTEM FRAMEWORK A. Knowledge Distillation",
            "char_start_offset": 7450,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 189
                },
                {
                    "start": 190,
                    "end": 309
                },
                {
                    "start": 310,
                    "end": 388
                },
                {
                    "start": 389,
                    "end": 544
                },
                {
                    "start": 545,
                    "end": 764
                },
                {
                    "start": 765,
                    "end": 868
                },
                {
                    "start": 871,
                    "end": 1014
                },
                {
                    "start": 1015,
                    "end": 1096
                },
                {
                    "start": 1097,
                    "end": 1187
                },
                {
                    "start": 1188,
                    "end": 1263
                },
                {
                    "start": 1264,
                    "end": 1437
                },
                {
                    "start": 1440,
                    "end": 1508
                },
                {
                    "start": 1509,
                    "end": 1562
                }
            ],
            "ref_mentions": [
                {
                    "start": 1433,
                    "end": 1436,
                    "matchedPaperCorpusId": "18668597"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.970703125
        },
        {
            "corpus_id": "235390933",
            "title": "Does Knowledge Distillation Really Work?",
            "text": "Knowledge distillation is a popular technique for training a small student network to emulate a larger teacher model, such as an ensemble of networks. We show that while knowledge distillation can improve student generalization, it does not typically work as it is commonly understood: there often remains a surprisingly large discrepancy between the predictive distributions of the teacher and the student, even in cases when the student has the capacity to perfectly match the teacher. We identify difficulties in optimization as a key reason for why the student is unable to match the teacher. We also show how the details of the dataset used for distillation play a role in how closely the student matches the teacher -- and that more closely matching the teacher paradoxically does not always lead to better student generalization.",
            "score": 0.6331101104318917,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.65625
        },
        {
            "corpus_id": "277940190",
            "title": "Feature Alignment and Representation Transfer in Knowledge Distillation for Large Language Models",
            "text": "Knowledge Distillation (KD) has emerged as a fundamental technique in deep learning, enabling the transfer of knowledge from a complex and pretrained model (the teacher model) to a simpler and smaller model (the student model) (Hinton et al., 2015;Gao, 2023). This approach is effective for model compression, significantly improving their efficiency and making them more suitable for deployment on resourcelimited devices. The core idea behind KD is to distill the dark knowledge embedded in the teacher model into a student model, which enhances student performance (Yu et al., 2024). \n\nRecent studies have focused on enhancing the effectiveness of KD by developing innovative variants, frameworks, and methodologies, including decoupling logit-based and featurebased knowledge distillation methods (Zhang et al., 2024c), correlation-aware knowledge distillation (Zhang et al., 2021), and partial-to-whole knowledge distillation (Abbasi et al., 2020). A common theme among these studies is the emphasis on feature alignment, which is crucial for Knowledge Distillation (KD) (Yu et al., 2024). Aligning the features of teacher and student models enhances the knowledge transfer process, leading to improved model performance. As shown in Figure 1, another important aspect of KD is the decomposition of knowledge into smaller components, which can facilitate more effective knowledge transfer (Zhang et al., 2021). This approach has been explored in various studies, where methods for decomposing teacher knowledge into subnetworks with increasing channel width have been proposed. For example, recent studies have investigated novel strategies for splitting teacher models into smaller, more manageable components to improve student model efficiency (Hou and Fan, 2021;Ruffy and Chahal, 2019). \n\nThe development of a general framework or model for knowledge distillation (KD) remains an active area of research because it offers a unified perspective on diverse  KD techniques and facilitates the systematic analysis and development of new strategies (Wang et al., 2023a). Significant contributions have been made in this area, with innovative frameworks and methodologies proposed to enhance the efficiency and scalability of KD.",
            "score": 0.6327288982085465,
            "section_title": "I. INTRODUCTION",
            "char_start_offset": 18,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 259
                },
                {
                    "start": 260,
                    "end": 423
                },
                {
                    "start": 424,
                    "end": 586
                },
                {
                    "start": 589,
                    "end": 953
                },
                {
                    "start": 954,
                    "end": 1094
                },
                {
                    "start": 1095,
                    "end": 1226
                },
                {
                    "start": 1227,
                    "end": 1415
                },
                {
                    "start": 1416,
                    "end": 1582
                },
                {
                    "start": 1583,
                    "end": 1795
                },
                {
                    "start": 1798,
                    "end": 2074
                },
                {
                    "start": 2075,
                    "end": 2232
                }
            ],
            "ref_mentions": [
                {
                    "start": 568,
                    "end": 585,
                    "matchedPaperCorpusId": "273811396"
                },
                {
                    "start": 931,
                    "end": 952,
                    "matchedPaperCorpusId": "209515527"
                },
                {
                    "start": 1076,
                    "end": 1093,
                    "matchedPaperCorpusId": "273811396"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.98876953125
        },
        {
            "corpus_id": "265351966",
            "title": "Leveraging different learning styles for improved knowledge distillation in biomedical imaging",
            "text": "Knowledge Distillation [17] is an approach introduced to transfer the knowledge in terms of probability outputs, p i , from a complex, highly parameterized pre-trained teacher network f (X, \u03d5) to a simple and compact student network g(X, \u03b8) to achieve model compression while retaining the high performance of the teacher.Given a training set with N samples X = {x i } N i=1 with corresponding labels Y = {y i } N i=1 , the teacher network f (X, \u03d5), is trained on the ground truth labels.The probabilistic output of a teacher network for a sample x i is defined as p i given by the extended softmax as:\n\nwhere z c corresponds to the logits, C is the number of classes, and T is the temperature parameter to get a smoother output\n\nPredictions, Predictions Features, Features Features, Predictions T \u2192 S 2 , S 1 \u2192 S 2 Predictions, Predictions Features, Features Predictions, Features probability distribution of the classes.Generally, the objective function for the teacher network is the standard Cross-Entropy (CE) error defined as:\n\nNow, the student networks are trained on the combined loss of Cross-Entropy (CE), and Knowledge Distillation (KD), where the CE helps the student networks to adhere to the ground truth labels and KD assists them to align their learning with that of the teacher.Here, Kullback Leibler (KL) divergence [28] is used for L KD p to measure the correspondence between the teacher and student predictions p i and s i respectively as:\n\nFinally, the loss function for the student network is the weighted (\u03b1) summation of the cross entropy (L CE ) and knowledge distillation (L KD p ) terms:\n\nwhere hyperparameter \u03b1 is used to balance the contributions of the hard target loss (L CE ) and soft target loss (L KD p ) during the distillation process for each student.The knowledge can be transferred in an online or offline manner from the teacher to the student networks.",
            "score": 0.6326803650531273,
            "section_title": "Knowledge Distillation",
            "char_start_offset": 6794,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 322
                },
                {
                    "start": 322,
                    "end": 488
                },
                {
                    "start": 488,
                    "end": 602
                },
                {
                    "start": 604,
                    "end": 728
                },
                {
                    "start": 730,
                    "end": 922
                },
                {
                    "start": 922,
                    "end": 1032
                },
                {
                    "start": 1034,
                    "end": 1295
                },
                {
                    "start": 1295,
                    "end": 1460
                },
                {
                    "start": 1462,
                    "end": 1615
                },
                {
                    "start": 1617,
                    "end": 1789
                },
                {
                    "start": 1789,
                    "end": 1894
                }
            ],
            "ref_mentions": [
                {
                    "start": 1334,
                    "end": 1338,
                    "matchedPaperCorpusId": "116908168"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.93310546875
        },
        {
            "corpus_id": "253018808",
            "title": "Similarity of Neural Architectures Using Adversarial Attack Transferability",
            "text": "Knowledge distillation (KD) [47] is a training method for transferring rich knowledge of a well-trained teacher network. Intuitively, KD performance affects a lot by choice of the teacher network; however, the relationship between similarity and KD performance has not yet been explored enough, especially for ViT. This subsection investigates how the similarity between teacher and student networks contributes to the distillation performance. There are several studies showing two contradictory conclusions; Jin et al . [54] and Mirzadeh et al . [73] showed that a similar teacher leads to better KD performance; Touvron et al . [100] reports that distillation from a substantially different teacher is beneficial for ViT. \n\nWe train 25 ViT-Ti models with different teacher networks from 69 models that we used by the hard distillation strategy [47]. Experimental details are described in Appendix. Fig. 11a illustrates the relationship between the teacherstudent similarity and the distillation performance. Fig. 11a tends to show a not significant negative correlation between teacher-student similarity and distillation performance (\u22120.32 Pearson correlation coefficient with 0.12 p-value). However, if we only focus on when the teacher and student networks are based on the same architecture (i.e., Transformer), we can observe a strong positive correlation (Fig. 11b) -0.70 Pearson correlation coefficient with 0.078 p-value. In this case, our observation is aligned with [54,73]: a teacher similar to the student improves distillation performance. However, when the teacher and student networks are based on different architectures (e.g., CNN), then we can observe a stronger negative correlation (Fig. 11c) with \u22120.51 Pearson correlation coefficient and 0.030 p-value. In this case, a more dissimilar teacher leads  to better distillation performance. We also test other factors that can affect distillation performance in Appendix; We observe that distillation performance is not correlated to teacher accuracy in our experiments. \n\nWhy do we observe contradictory results for Transformer teachers (Fig. 11b) and other teachers (Fig. 11c)?",
            "score": 0.6323692231247915,
            "section_title": "Model Diversity and Knowledge Distillation",
            "char_start_offset": 27153,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 120
                },
                {
                    "start": 121,
                    "end": 314
                },
                {
                    "start": 315,
                    "end": 444
                },
                {
                    "start": 445,
                    "end": 521
                },
                {
                    "start": 522,
                    "end": 547
                },
                {
                    "start": 548,
                    "end": 630
                },
                {
                    "start": 631,
                    "end": 724
                },
                {
                    "start": 727,
                    "end": 852
                },
                {
                    "start": 853,
                    "end": 900
                },
                {
                    "start": 901,
                    "end": 1010
                },
                {
                    "start": 1011,
                    "end": 1195
                },
                {
                    "start": 1196,
                    "end": 1432
                },
                {
                    "start": 1433,
                    "end": 1555
                },
                {
                    "start": 1556,
                    "end": 1777
                },
                {
                    "start": 1778,
                    "end": 1860
                },
                {
                    "start": 1861,
                    "end": 2040
                },
                {
                    "start": 2043,
                    "end": 2149
                }
            ],
            "ref_mentions": [
                {
                    "start": 28,
                    "end": 32,
                    "matchedPaperCorpusId": "7200347"
                },
                {
                    "start": 522,
                    "end": 526,
                    "matchedPaperCorpusId": "125985701"
                },
                {
                    "start": 548,
                    "end": 552,
                    "matchedPaperCorpusId": "212908749"
                },
                {
                    "start": 631,
                    "end": 636,
                    "matchedPaperCorpusId": "229363322"
                },
                {
                    "start": 847,
                    "end": 851,
                    "matchedPaperCorpusId": "7200347"
                },
                {
                    "start": 1479,
                    "end": 1483,
                    "matchedPaperCorpusId": "125985701"
                },
                {
                    "start": 1483,
                    "end": 1486,
                    "matchedPaperCorpusId": "212908749"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.8251953125
        },
        {
            "corpus_id": "260704230",
            "title": "Teacher-Student Architecture for Knowledge Distillation: A Survey",
            "text": "Table 2 compares the different distillation schemes in terms of teacher and student learning statuses, as well as their role statuses. The classic distillation scheme is offline distillation [4], which represents that student networks learn the knowledge transferred from pre-trained teacher networks. Specifically, powerful teacher networks are first completely well-trained on large-scale datasets, and then transfer the knowledge to guide the training of compact student networks. The roles of teacher and student networks are not exchanged during the student training process. It is important to note that offline distillation is not included in the scope of this survey, and we refer readers to the existing surveys that have provided comprehensive reviews on the offline distillation [12,14]. \n\nAlthough the offline distillation scheme is straightforward and computationally efficient, powerful pre-trained teacher networks and large-scale datasets could be limited [23,74,136]. To solve such issues, an online distillation scheme is further commonly utilized to simultaneously train student and teacher networks, so that the whole knowledge learning process can be end-to-end trainable. For instance, in On-the-fly Native Ensemble (ONE) learning strategy [35,40], a native ensemble teacher network is created from multiple student branches on-the-fly and simultaneously trained with these student branches. With student branches, both teachers and students are more efficient to be trained with superior generalization performance and without asynchronous model updates. Due to the gating module on the shared layer, student branches are limited to the same network architecture in ONE learning strategy. To overcome this limitation, Feature Fusion Learning (FFL) [137] is further proposed to allow student branches to be applicable to any architecture. To better boost the knowledge distillation process, Su et al. [138] additionally introduce an attention mechanism to capture important and high-level knowledge, so that teachers and students can be dynamically and effectively trained with the help of the valuable knowledge. In Peer Collaborative Learning (PCL) [139], a peer ensemble teacher is trained with the ensemble feature representation of multiple student peers. Besides, a temporal peer means the teacher is individually built for each student peer to collaboratively transfer knowledge among student peers.",
            "score": 0.6316329111795298,
            "section_title": "Online Distillation",
            "char_start_offset": 51019,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 134
                },
                {
                    "start": 135,
                    "end": 301
                },
                {
                    "start": 302,
                    "end": 483
                },
                {
                    "start": 484,
                    "end": 580
                },
                {
                    "start": 581,
                    "end": 798
                },
                {
                    "start": 801,
                    "end": 984
                },
                {
                    "start": 985,
                    "end": 1193
                },
                {
                    "start": 1194,
                    "end": 1413
                },
                {
                    "start": 1414,
                    "end": 1577
                },
                {
                    "start": 1578,
                    "end": 1711
                },
                {
                    "start": 1712,
                    "end": 1860
                },
                {
                    "start": 1861,
                    "end": 2135
                },
                {
                    "start": 2136,
                    "end": 2282
                },
                {
                    "start": 2283,
                    "end": 2428
                }
            ],
            "ref_mentions": [
                {
                    "start": 794,
                    "end": 797,
                    "matchedPaperCorpusId": "220632998"
                },
                {
                    "start": 972,
                    "end": 976,
                    "matchedPaperCorpusId": "26071966"
                },
                {
                    "start": 976,
                    "end": 979,
                    "matchedPaperCorpusId": "212908749"
                },
                {
                    "start": 979,
                    "end": 983,
                    "matchedPaperCorpusId": "208526905"
                },
                {
                    "start": 1262,
                    "end": 1266,
                    "matchedPaperCorpusId": "48352434"
                },
                {
                    "start": 1266,
                    "end": 1269,
                    "matchedPaperCorpusId": "236912875"
                },
                {
                    "start": 1771,
                    "end": 1776,
                    "matchedPaperCorpusId": "125950115"
                },
                {
                    "start": 1923,
                    "end": 1928,
                    "matchedPaperCorpusId": "246291086"
                },
                {
                    "start": 2173,
                    "end": 2178,
                    "matchedPaperCorpusId": "219531897"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.88427734375
        },
        {
            "corpus_id": "267311506",
            "title": "Iterative Data Smoothing: Mitigating Reward Overfitting and Overoptimization in RLHF",
            "text": "The literature of knowledge distillation focuses on transferring the knowledge from a teacher model to a student model (Hinton et al., 2015;Furlanello et al., 2018;Cho and Hariharan, 2019;Zhao et al., 2022;Romero et al., 2014;Yim et al., 2017;Huang and Wang, 2017;Park et al., 2019;Tian et al., 2019;Tung and Mori, 2019;Qiu et al., 2022;Cheng et al., 2020). It is observed in this literature that the soft labels produced by the teacher network can help train a better student network, even when the teacher and student network are of the same size and structure Hinton et al. (2015). Furlanello et al. (2018) present a method which iteratively trains a new student network after the teacher network achieves the smallest evaluation loss. Both our iterative data smoothing algorithm and these knowledge distillation methods learn from soft labels. However, iterative data smoothing iteratively updates the same model and data, while knowledge distillation method usually focuses on transferring knowledge from one model to the other.",
            "score": 0.6312778241944286,
            "section_title": "Knowledge Distillation",
            "char_start_offset": 8350,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 357
                },
                {
                    "start": 358,
                    "end": 584
                },
                {
                    "start": 585,
                    "end": 738
                },
                {
                    "start": 739,
                    "end": 847
                },
                {
                    "start": 848,
                    "end": 1033
                }
            ],
            "ref_mentions": [
                {
                    "start": 140,
                    "end": 164,
                    "matchedPaperCorpusId": "4110009"
                },
                {
                    "start": 164,
                    "end": 188,
                    "matchedPaperCorpusId": "203642130"
                },
                {
                    "start": 188,
                    "end": 206,
                    "matchedPaperCorpusId": "247476179"
                },
                {
                    "start": 226,
                    "end": 243,
                    "matchedPaperCorpusId": "206596723"
                },
                {
                    "start": 264,
                    "end": 282,
                    "matchedPaperCorpusId": "131765296"
                },
                {
                    "start": 300,
                    "end": 320,
                    "matchedPaperCorpusId": "198179476"
                },
                {
                    "start": 337,
                    "end": 356,
                    "matchedPaperCorpusId": "212633769"
                },
                {
                    "start": 585,
                    "end": 609,
                    "matchedPaperCorpusId": "4110009"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.85009765625
        },
        {
            "corpus_id": "3643430",
            "title": "Apprentice: Using Knowledge Distillation Techniques To Improve Low-Precision Network Accuracy",
            "text": "Knowledge distillation methods: The general technique in distillation based methods involves using a teacher-student strategy, where a large deep network trained for a given task teaches shallower student network(s) on the same task. The core concepts behind knowledge distillation or transfer technique have been around for a while. Bucilu\u01ce et al. (2006) show that one can compress the information in an ensemble into a single network. Ba & Caurana (2013) extend this approach to study shallow, but wide, fully connected topologies by mimicking deep neural networks. To facilitate learning, the authors introduce the concepts of learning on logits rather than the probability distribution. Hinton et al. (2015) propose a framework to transfer knowledge by introducing the concept of temperature. The key idea is to divide the logits by a temperature factor before performing a Softmax function. By using a higher temperature factor the activations of incorrect classes are boosted. This then facilitates more information flowing to the model parameters during back-propagation operation. FitNets (Romero et al., 2014) extend this work by using intermediate hidden layer outputs as target values for training a deeper, but thinner, student model. Net2Net (Chen et al., 2015a) also uses a teacher-student network system with a function-preserving transformation approach to initialize the parameters of the student network. The goal in Net2Net approach is to accelerate the training of a larger student network. Zagoruyko & Komodakis (2016) use attention as a mechanism for transferring knowledge from one network to another. In a similar theme, Yim et al. ( 2017) propose an information metric using which a teacher DNN can transfer the distilled knowledge to other student DNNs. In N2N learning work, Ashok et al. (2017) propose a reinforcement learning based approach for compressing a teacher network into an equally capable student network. They achieve a compression factor of 10x for ResNet-34 on Cifar datasets.",
            "score": 0.6311186007994953,
            "section_title": "RELATED WORK",
            "char_start_offset": 9728,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 233
                },
                {
                    "start": 234,
                    "end": 333
                },
                {
                    "start": 334,
                    "end": 436
                },
                {
                    "start": 437,
                    "end": 567
                },
                {
                    "start": 568,
                    "end": 690
                },
                {
                    "start": 691,
                    "end": 796
                },
                {
                    "start": 797,
                    "end": 895
                },
                {
                    "start": 896,
                    "end": 982
                },
                {
                    "start": 983,
                    "end": 1088
                },
                {
                    "start": 1089,
                    "end": 1246
                },
                {
                    "start": 1247,
                    "end": 1422
                },
                {
                    "start": 1423,
                    "end": 1510
                },
                {
                    "start": 1511,
                    "end": 1624
                },
                {
                    "start": 1625,
                    "end": 1779
                },
                {
                    "start": 1780,
                    "end": 1944
                },
                {
                    "start": 1945,
                    "end": 2018
                }
            ],
            "ref_mentions": [
                {
                    "start": 334,
                    "end": 355,
                    "matchedPaperCorpusId": "11253972"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.86962890625
        },
        {
            "corpus_id": "258035138",
            "title": "Progressive multi-level distillation learning for pruning network",
            "text": "The initial knowledge distillation [25] argued that one-hot labels limit the performance of the network model, and that the soft labels of a more robust network would provide more abundant information, which would allow the transfer of knowledge from a larger teacher network to a smaller one, thereby bridging the gap. Moreover, besides focusing on extracting logits output knowledge, intermediate representations of knowledge within the teacher in the form of feature maps can also be learned by the student model. FitNet [26] first proposed distillation learning for a single intermediate layer of knowledge. AT [27] extended this idea by extracting multiple intermediate layers knowledge of the teacher model to guide student learning, and by using L2-regularization on each feature map to ensure consistent dimensions for each pair of feature maps. However, knowledge from deeper intermediate layers may provide students with overly standardized guidance, while knowledge from shallower layers may not serve as a guiding role [12], which results in the inefficient transfer of knowledge. In relation-based distillation learning, knowledge transfer relationships between different layers or data are further explored. Yim et al. [28] used the relationship between layers of the teacher's network as the goal of student model learning. SP [29] aimed to preserve the student's pairwise similarity rather than mimicking the teacher's representation space, so that students could better understand the relationships between instances. Furthermore, in addition to the applications mentioned above in classification tasks, knowledge distillation methods have also proven their effectiveness in more complex tasks such as object detection [30,31]. \n\nModel pruning and knowledge distillation are two independent parts of model compression. How to combine these two methods is one of the problems worth discussing. The simplest way to combine them is to use knowledge distillation after the completion of pruning [15,32]. However, we have shown that the use of distillation learning in the fine-tuning process of pruning can yield better results, as demonstrated in \"Two combined strategies\". Furthermore, it is also necessary to validate the efficacy of distillation learning for structured pruning networks on various model architectures and public datasets.",
            "score": 0.6310616496774637,
            "section_title": "Knowledge distillation",
            "char_start_offset": 5673,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 319
                },
                {
                    "start": 320,
                    "end": 516
                },
                {
                    "start": 517,
                    "end": 611
                },
                {
                    "start": 612,
                    "end": 853
                },
                {
                    "start": 854,
                    "end": 1092
                },
                {
                    "start": 1093,
                    "end": 1221
                },
                {
                    "start": 1222,
                    "end": 1338
                },
                {
                    "start": 1339,
                    "end": 1534
                },
                {
                    "start": 1535,
                    "end": 1744
                },
                {
                    "start": 1747,
                    "end": 1835
                },
                {
                    "start": 1836,
                    "end": 1909
                },
                {
                    "start": 1910,
                    "end": 2016
                },
                {
                    "start": 2017,
                    "end": 2187
                },
                {
                    "start": 2188,
                    "end": 2355
                }
            ],
            "ref_mentions": [
                {
                    "start": 615,
                    "end": 619,
                    "matchedPaperCorpusId": "829159"
                },
                {
                    "start": 1031,
                    "end": 1035,
                    "matchedPaperCorpusId": "219559263"
                },
                {
                    "start": 1233,
                    "end": 1237,
                    "matchedPaperCorpusId": "206596723"
                },
                {
                    "start": 1342,
                    "end": 1346,
                    "matchedPaperCorpusId": "198179476"
                },
                {
                    "start": 1736,
                    "end": 1740,
                    "matchedPaperCorpusId": "267213"
                },
                {
                    "start": 1740,
                    "end": 1743,
                    "matchedPaperCorpusId": "235613518"
                },
                {
                    "start": 2008,
                    "end": 2012,
                    "matchedPaperCorpusId": "236386992"
                },
                {
                    "start": 2012,
                    "end": 2015,
                    "matchedPaperCorpusId": "225595342"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.91943359375
        },
        {
            "corpus_id": "232045950",
            "title": "Even your Teacher Needs Guidance: Ground-Truth Targets Dampen Regularization Imposed by Self-Distillation",
            "text": "Knowledge distillation, most commonly known from Hinton et al. (2015), is a procedure to transfer knowledge from one neural network (teacher) to another neural network (student). 2 Often the student has fewer parameters than the teacher, and the procedure can be seen as a model compression technique. Originally, the distillation procedure achieves the knowledge transfer by training the student network using the original training targets, denoted as ground-truth targets, as well as a softened distribution of logits from the (already trained and fixed) teacher network. 3 Since the popularization of knowledge distillation by Hinton et al. (2015), the idea of knowledge distillation has been extended to a variety of settings. 4 This paper will focus on the special case where the teacher and student are of identical architecture, called self-distillation, and where the aim is to improve predictive performance, rather than compressing the model. \n\nThe idea of self-distillation is to use outputs from a trained model together with the original targets as new targets for retraining the same model from scratch. We refer to this as one step of self-distillation, and one can iterate this procedure for multiple distillation steps (see Figure 1). Empirically, it has been shown that this procedure often generalizes better than the model trained merely on the original targets, and achieves higher predictive performance on validation data, despite no additional information being provided during training (Furlanello et al., 2018;Ahn et al., 2019;Yang et al., 2018). \n\nFigure 1: Illustration of self-distillation for two steps after the initial training, where we use the notation f (\u03c4 ) = f (\u2022, \u03b2(\u03c4) ). See Section 3 for details. \n\nModern deep neural networks are often trained in the over-parameterized regime, where the amount of trainable parameters highly exceed the amount of training samples. Under simple first-order methods such as gradient descent, such large networks can fit any target, but in order to generalize well, such overfitting is usually undesirable (Zhang et al., 2017;Nakkiran et al., 2020). Thus, some type of regularization is typically imposed during training, in order to avoid overfitting.",
            "score": 0.6305180205887385,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 180
                },
                {
                    "start": 181,
                    "end": 301
                },
                {
                    "start": 302,
                    "end": 575
                },
                {
                    "start": 576,
                    "end": 732
                },
                {
                    "start": 733,
                    "end": 952
                },
                {
                    "start": 955,
                    "end": 1117
                },
                {
                    "start": 1118,
                    "end": 1251
                },
                {
                    "start": 1252,
                    "end": 1572
                },
                {
                    "start": 1575,
                    "end": 1709
                },
                {
                    "start": 1710,
                    "end": 1736
                },
                {
                    "start": 1739,
                    "end": 1905
                },
                {
                    "start": 1906,
                    "end": 2121
                },
                {
                    "start": 2122,
                    "end": 2224
                }
            ],
            "ref_mentions": [
                {
                    "start": 1511,
                    "end": 1536,
                    "matchedPaperCorpusId": "4110009"
                },
                {
                    "start": 1536,
                    "end": 1553,
                    "matchedPaperCorpusId": "118649278"
                },
                {
                    "start": 2078,
                    "end": 2098,
                    "matchedPaperCorpusId": "6212000"
                },
                {
                    "start": 2098,
                    "end": 2120,
                    "matchedPaperCorpusId": "207808916"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.96630859375
        },
        {
            "corpus_id": "255595916",
            "title": "Synthetic data generation method for data-free knowledge distillation in regression neural networks",
            "text": "As neural networks become increasingly large in number of parameters, the deployment of such models faces a difficult challenge for applications such as mobile devices and embedded systems due to limitations in computational resources and memory [Cheng et al., 2018, Deng et al., 2020]. To address such problems, model compression through knowledge distillation has become an active area of research in recent years [Liu et al., 2022, Wang et al., 2022]. Knowledge distillation is the technique where knowledge learned by a larger teacher model is transferred to a smaller student model [Gou et al., 2021, Hinton et al., 2015, Wang and Yoon, 2022]. The main idea is that the student model mimics the teacher model to achieve a similar or even a superior performance. \n\nVarious methods of knowledge distillation define and focus on different forms of knowledge. Following the nomenclature in [Gou et al., 2021], these can be largely grouped as response-based knowledge, feature-based knowledge, and relation-based knowledge. For response-based knowledge, outputs of the teacher model are used to supervise the training of the student model. For example, [Hinton et al., 2015] uses soft targets from the logits output of the teacher model to train the student. For featurebased knowledge, outputs of intermediate layers, or feature maps learned by the teacher model can be to supervise the training of the student model. For example, [Romero et al., 2014] trains the student model to match the feature activations of the teacher model. For relationship-based knowledge, the relationships between different layers or data samples are used. For example, [Yim et al., 2017] uses the inner products between features from two layers to represent the relationship between different layers, while [Chen et al., 2021] trains the student model to learn to preserve the similarity of samples' feature embeddings in the intermediate layers of the teacher models.",
            "score": 0.6294524537223987,
            "section_title": "Knowledge distillation",
            "char_start_offset": 4099,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 286
                },
                {
                    "start": 287,
                    "end": 454
                },
                {
                    "start": 455,
                    "end": 648
                },
                {
                    "start": 649,
                    "end": 766
                },
                {
                    "start": 769,
                    "end": 860
                },
                {
                    "start": 861,
                    "end": 1023
                },
                {
                    "start": 1024,
                    "end": 1139
                },
                {
                    "start": 1140,
                    "end": 1258
                },
                {
                    "start": 1259,
                    "end": 1418
                },
                {
                    "start": 1419,
                    "end": 1533
                },
                {
                    "start": 1534,
                    "end": 1636
                },
                {
                    "start": 1637,
                    "end": 1949
                }
            ],
            "ref_mentions": [
                {
                    "start": 265,
                    "end": 285,
                    "matchedPaperCorpusId": "215799572"
                },
                {
                    "start": 416,
                    "end": 433,
                    "matchedPaperCorpusId": "227228186"
                },
                {
                    "start": 587,
                    "end": 604,
                    "matchedPaperCorpusId": "219559263"
                },
                {
                    "start": 625,
                    "end": 647,
                    "matchedPaperCorpusId": "215745611"
                },
                {
                    "start": 891,
                    "end": 909,
                    "matchedPaperCorpusId": "219559263"
                },
                {
                    "start": 1650,
                    "end": 1668,
                    "matchedPaperCorpusId": "206596723"
                },
                {
                    "start": 1788,
                    "end": 1807,
                    "matchedPaperCorpusId": "56356408"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.97998046875
        },
        {
            "corpus_id": "251719303",
            "title": "Rethinking Knowledge Distillation via Cross-Entropy",
            "text": "Knowledge distillation (KD) is a method to improve the model while keeping the network structure unchanged. It was first proposed by Hinton et al. (Hinton et al., 2015), where the student is supervised by the hard labels and the soft labels from the teacher's output. Many following works focus on making better use of soft labels to transfer more knowledge. WSLD (Zhou et al., 2020) analyzes soft labels and distributes different weights for them from a perspective of bias-variance trade-off. DKD (Zhao et al., 2022) divides the classical KD according to the teacher's prediction and modifies the formulation of KD, achieving state-of-the-art performances. SRRL (Yang et al., 2020) forces the output logits of teacher's and student's features after the teacher's linear layer to be the same. \n\nBesides distillation on logits, some works aim at transferring knowledge from intermediate features. \n\nFitNet (Romero et al., 2014) distills the semantic information from intermediate feature directly. OFD (Heo et al., 2019) designs the margin ReLU and modifies the measurement for the distance between students and teachers. RKD (Park et al., 2019) extracts the relation from the feature map. CRD (Tian et al., 2019) applies contrastive learning to distillation successfully. KR (Chen et al., 2021) transfers knowledge from multi-level features for distillation. SRRL (Yang et al., 2020) utilizes the teacher's classifier to train the student's feature. MGD (Yang et al., 2022) proposes a new distillation method that makes the student generate the teacher's feature instead of mimicking.",
            "score": 0.6292663952173871,
            "section_title": "RELATED WORK",
            "char_start_offset": 5290,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 107
                },
                {
                    "start": 108,
                    "end": 267
                },
                {
                    "start": 268,
                    "end": 358
                },
                {
                    "start": 359,
                    "end": 494
                },
                {
                    "start": 495,
                    "end": 658
                },
                {
                    "start": 659,
                    "end": 793
                },
                {
                    "start": 796,
                    "end": 896
                },
                {
                    "start": 899,
                    "end": 997
                },
                {
                    "start": 998,
                    "end": 1121
                },
                {
                    "start": 1122,
                    "end": 1189
                },
                {
                    "start": 1190,
                    "end": 1272
                },
                {
                    "start": 1273,
                    "end": 1359
                },
                {
                    "start": 1360,
                    "end": 1450
                },
                {
                    "start": 1451,
                    "end": 1585
                }
            ],
            "ref_mentions": [
                {
                    "start": 364,
                    "end": 383,
                    "matchedPaperCorpusId": "231740588"
                },
                {
                    "start": 664,
                    "end": 683,
                    "matchedPaperCorpusId": "235613564"
                },
                {
                    "start": 1002,
                    "end": 1020,
                    "matchedPaperCorpusId": "102483181"
                },
                {
                    "start": 1126,
                    "end": 1145,
                    "matchedPaperCorpusId": "131765296"
                },
                {
                    "start": 1194,
                    "end": 1213,
                    "matchedPaperCorpusId": "204838340"
                },
                {
                    "start": 1276,
                    "end": 1295,
                    "matchedPaperCorpusId": "233296935"
                },
                {
                    "start": 1365,
                    "end": 1384,
                    "matchedPaperCorpusId": "235613564"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.9091796875
        },
        {
            "corpus_id": "270389751",
            "title": "Multistage feature fusion knowledge distillation",
            "text": "Knowledge distillation (KD), as initially proposed by Hinton et al. 10 , aims to supervise the training convergence of a student network, a smaller model, with a teacher network, a larger model.This method controls the extent of knowledge transfer between two networks using a temperature parameter, T, to control the transfer of soft-label dark knowledge.This approach has given rise to variations, including intermediate feature layer distillation and multistage soft label distillation.\n\nIn the context of intermediate feature layer distillation, the challenge of inconsistent multistage feature knowledge distribution is a critical issue.FitNet 11 employs squared distance constraints to measure the similarity of intermediate layer features between teacher and student networks.AT 15 uses multi-layer attention maps to extract features between the teacher network and the student network, and builds a knowledge transfer mechanism between the two.CC 16 proposed a correlation congruence method to reduce the correlation consistency distribution between teachers and students across multiple sample instances, and improve the distribution consistency between student models and teacher models in the classification output of multiple instances.AB 17 proposed a method for knowledge transfer by extracting the activation boundaries formed by hidden neurons, which enables students to learn the separation boundaries between different activation regions formed by each neuron in the teacher, thereby reducing the differences between the student network and the teacher network.FT 18 proposed two convolutional modules, the reader and the translator, which are used to extract the feature information of teachers and the translator to extract the feature information of students.Through distillation training, the differences between the two modules are reduced, achieving the imitation and learning of the teacher network by www.nature.com/scientificreports/ the student network.NST 19 proposed a new KT loss function to minimize the maximum average difference in neuron feature distribution between the teacher model and the student model, significantly improving the performance of the student model.CRD 20 is a knowledge distillation method based on contrastive learning, which preserves mutual information between teachers and students by optimizing the distillation loss function.OFD 21 uses a novel distance function and edge residual function to distill essential information between teacher and student networks.ReviewKD 22 utilizes a multilevel composite knowledge approach to transfer dark knowledge at the feature level, achieving state-of-the-art performance.",
            "score": 0.6285981404521053,
            "section_title": "Knowledge distillation",
            "char_start_offset": 2815,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 194
                },
                {
                    "start": 194,
                    "end": 356
                },
                {
                    "start": 356,
                    "end": 489
                },
                {
                    "start": 491,
                    "end": 642
                },
                {
                    "start": 642,
                    "end": 783
                },
                {
                    "start": 783,
                    "end": 952
                },
                {
                    "start": 952,
                    "end": 1248
                },
                {
                    "start": 1248,
                    "end": 1579
                },
                {
                    "start": 1579,
                    "end": 1780
                },
                {
                    "start": 1780,
                    "end": 1981
                },
                {
                    "start": 1981,
                    "end": 2204
                },
                {
                    "start": 2204,
                    "end": 2387
                },
                {
                    "start": 2387,
                    "end": 2522
                },
                {
                    "start": 2522,
                    "end": 2673
                }
            ],
            "ref_mentions": [
                {
                    "start": 68,
                    "end": 70,
                    "matchedPaperCorpusId": "7200347"
                },
                {
                    "start": 955,
                    "end": 957,
                    "matchedPaperCorpusId": "102483463"
                },
                {
                    "start": 1251,
                    "end": 1253,
                    "matchedPaperCorpusId": "53213211"
                },
                {
                    "start": 1582,
                    "end": 1584,
                    "matchedPaperCorpusId": "3608236"
                },
                {
                    "start": 2208,
                    "end": 2210,
                    "matchedPaperCorpusId": "204838340"
                },
                {
                    "start": 2391,
                    "end": 2393,
                    "matchedPaperCorpusId": "102483181"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.98046875
        },
        {
            "corpus_id": "271227251",
            "title": "A Survey on Symbolic Knowledge Distillation of Large Language Models",
            "text": "Knowledge distillation is a technique used to transfer knowledge from a larger, more complex model (teacher) to a smaller, simpler model (student) with the goal of retaining much of the teacher model's performance [117]. This process is crucial in scenarios where computational resources are limited or where deployment requires lightweight models. There are various types of traditional knowledge distillation techniques: response-based, feature-based and relation-based and one modern symbolic knowledge distillation, each with its unique approach and area of application: \n\n1) Response-based Knowledge Distillation: Responsebased knowledge distillation involves transferring knowledge from the teacher model's final output layer to the student model, aiming to mimic the teacher's final predictions. This approach is straightforward and has proven effective across various tasks, employing a loss function based on the divergence between the teacher's and student's logits. It's widely applied in model compression and has been adapted for different types of model predictions, including object detection and human pose estimation, where the teacher's output may include additional information like bounding box offsets [118] or heatmaps for landmarks [119]. A key application of responsebased knowledge distillation is in image classification [44], where \"soft targets\" -the probabilities assigned to each class by the teacher model -play a crucial role. These probabilities are adjusted using a temperature factor to control the softness of the targets, allowing the transfer of knowledge from the teacher to the student. The distillation process typically employs the Kullback-Leibler divergence loss to optimize the similarity between the teacher's and student's probability distributions. \n\nThis method is praised for its simplicity and effectiveness, particularly in leveraging knowledge for training. However, its reliance on the final layer's output means it may not fully utilize intermediate-level supervision from the teacher, an aspect crucial for representation learning in deep neural networks. \n\n2) Feature-based Knowledge Distillation: Feature-based knowledge distillation taps into the strength of deep neural networks to learn hierarchical feature representations, a process central to representation learning [120].",
            "score": 0.6283109573384413,
            "section_title": "A. Knowledge Distillation",
            "char_start_offset": 14819,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 220
                },
                {
                    "start": 221,
                    "end": 348
                },
                {
                    "start": 349,
                    "end": 574
                },
                {
                    "start": 577,
                    "end": 802
                },
                {
                    "start": 803,
                    "end": 976
                },
                {
                    "start": 977,
                    "end": 1261
                },
                {
                    "start": 1262,
                    "end": 1458
                },
                {
                    "start": 1459,
                    "end": 1626
                },
                {
                    "start": 1627,
                    "end": 1796
                },
                {
                    "start": 1799,
                    "end": 1910
                },
                {
                    "start": 1911,
                    "end": 2111
                },
                {
                    "start": 2114,
                    "end": 2337
                }
            ],
            "ref_mentions": [
                {
                    "start": 214,
                    "end": 219,
                    "matchedPaperCorpusId": "219559263"
                },
                {
                    "start": 1223,
                    "end": 1228,
                    "matchedPaperCorpusId": "29308926"
                },
                {
                    "start": 1255,
                    "end": 1260,
                    "matchedPaperCorpusId": "53292120"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.98583984375
        },
        {
            "corpus_id": "276533262",
            "title": "Decoupled Classifier Knowledge Distillation",
            "text": "However, because they focus on different aspects, the knowledge learned by the student model also varies. \n\nIn the case of heterogeneous architectures between teacher and student models [14], the features of these models reside in different latent feature spaces, making it challenging to ensure effective alignment of the learned features. Consequently, directly matching these unrelated features is not only unproductive but may also hinder the student model's performance. Furthermore, feature-based distillation methods focus more on local regions, and this localized attention may be insufficient for effectively transferring knowledge from the teacher model to the student model in knowledge distillation [15]. The knowledge embedded in the teacher model is often too complex for the student model to fully absorb and process [16]. Although teacher models are generally more complex and capable of capturing more knowledge, the main challenge lies in distilling this knowledge into a form that is accessible and beneficial for the student model [16]. \n\nWe have observed that existing feature-based and output-based knowledge distillation techniques are mostly applied independently. Although both methods have their respective advantages in improving model performance and reducing model complexity, there is still no effective method that can simultaneously integrate the strengths of both, as they focus on different aspects of the model. As pointed out by Jianping Gou et al [17], how to model these different types of knowledge in a unified and complementary framework remains an urgent challenge. Specifically, knowledge from different layers may have varying impacts on the training of the student model [18]. For example, output-based knowledge mainly comes from the model's final layer, while feature-based knowledge, guided by deeper layers, might face the risk of over-regularization. Meanwhile, recent studies have found that deep features are often linearized. Transforming features in certain directions can generate representations corresponding to the same category but with different samples. However, due to limited capacity, it is difficult for the student model to learn the distinctive features captured by the powerful teacher [19]. The lack of training data further exacerbates the performance gap between the student and teacher models. \n\nTherefore, the key step in knowledge distillation lies in how to effectively transfer the rich knowledge of the teacher model to the student model.",
            "score": 0.6278305370844544,
            "section_title": "Introduction",
            "char_start_offset": 2228,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 105
                },
                {
                    "start": 108,
                    "end": 340
                },
                {
                    "start": 341,
                    "end": 475
                },
                {
                    "start": 476,
                    "end": 716
                },
                {
                    "start": 717,
                    "end": 837
                },
                {
                    "start": 838,
                    "end": 1056
                },
                {
                    "start": 1059,
                    "end": 1188
                },
                {
                    "start": 1189,
                    "end": 1446
                },
                {
                    "start": 1447,
                    "end": 1607
                },
                {
                    "start": 1608,
                    "end": 1721
                },
                {
                    "start": 1722,
                    "end": 1900
                },
                {
                    "start": 1901,
                    "end": 1978
                },
                {
                    "start": 1979,
                    "end": 2114
                },
                {
                    "start": 2115,
                    "end": 2259
                },
                {
                    "start": 2260,
                    "end": 2365
                },
                {
                    "start": 2368,
                    "end": 2515
                }
            ],
            "ref_mentions": [
                {
                    "start": 186,
                    "end": 190,
                    "matchedPaperCorpusId": "265871679"
                },
                {
                    "start": 2254,
                    "end": 2258,
                    "matchedPaperCorpusId": "266998295"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.9365234375
        },
        {
            "corpus_id": "232380330",
            "title": "Distilling a Powerful Student Model via Online Knowledge Distillation",
            "text": "Traditional Knowledge Distillation. Traditional distillation works transfer knowledge from a cumbersome teacher model to a light-weight student model. As such, a large-scale model has to be trained in advance, based on which various knowledge definitions and transfer strategies are proposed to boost the performance of the student model. The pioneering work [7] performs knowledge representation of a teacher model using the softmax output layer, which converts the logit into a soft probability with a temperature parameter. Following this, a large number of works proposed new forms of knowledge, such as output logits [20], [21], intermediate feature maps [8], [22], [23], attention maps [9], second-order statistics [24], contrastive features [25], [26], or structured knowledge [27], [28], [29]. Another group of methods focus on transfer strategies so as to enable the student model to inherit knowledge from the teacher model. An intuitive solution is to use the Kullback-Leibler divergence or p -loss when the knowledge falls on the soft logit [7], [30] or intermediate representation [8], [9]. Beyond that, Wang et al. [31] utilized the adversarial training scheme in generative adversarial networks (GANs) [32] to transfer knowledge. Jang et al. [33] considered meta-learning to selectively transfer knowledge. In [34], a reinforcement learning based architecture-aware distillation was proposed to pass the structural knowledge to the student. Recently, there are some works [35], [36], [37], [38] that used knowledge distillation for GAN compression to ensure effective compression. The surveys [39], [40] summarized the development of knowledge distillation in recent years. \n\nOnline Knowledge Distillation. Online knowledge distillation has emerged as an alternative that eliminates the dependency on the teacher model. It builds knowledge distillation based on a collection of student models that collaborate through simultaneous training. To this end, Zhang et al. [10] proposed a deep mutual learning strategy where pair-wise students are encouraged to learn from each other by a mimicry loss based on the Kullback-Leibler divergence.",
            "score": 0.6269751652211206,
            "section_title": "II. RELATED WORK",
            "char_start_offset": 5383,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 35
                },
                {
                    "start": 36,
                    "end": 150
                },
                {
                    "start": 151,
                    "end": 338
                },
                {
                    "start": 339,
                    "end": 526
                },
                {
                    "start": 527,
                    "end": 801
                },
                {
                    "start": 802,
                    "end": 934
                },
                {
                    "start": 935,
                    "end": 1103
                },
                {
                    "start": 1104,
                    "end": 1244
                },
                {
                    "start": 1245,
                    "end": 1321
                },
                {
                    "start": 1322,
                    "end": 1455
                },
                {
                    "start": 1456,
                    "end": 1595
                },
                {
                    "start": 1596,
                    "end": 1688
                },
                {
                    "start": 1691,
                    "end": 1721
                },
                {
                    "start": 1722,
                    "end": 1834
                },
                {
                    "start": 1835,
                    "end": 1955
                },
                {
                    "start": 1956,
                    "end": 2152
                }
            ],
            "ref_mentions": [
                {
                    "start": 622,
                    "end": 626,
                    "matchedPaperCorpusId": "219962714"
                },
                {
                    "start": 628,
                    "end": 632,
                    "matchedPaperCorpusId": "235306324"
                },
                {
                    "start": 665,
                    "end": 669,
                    "matchedPaperCorpusId": "233296935"
                },
                {
                    "start": 671,
                    "end": 675,
                    "matchedPaperCorpusId": "227335337"
                },
                {
                    "start": 692,
                    "end": 695,
                    "matchedPaperCorpusId": "829159"
                },
                {
                    "start": 721,
                    "end": 725,
                    "matchedPaperCorpusId": "206596723"
                },
                {
                    "start": 784,
                    "end": 788,
                    "matchedPaperCorpusId": "131765296"
                },
                {
                    "start": 790,
                    "end": 794,
                    "matchedPaperCorpusId": "198185886"
                },
                {
                    "start": 796,
                    "end": 800,
                    "matchedPaperCorpusId": "218487294"
                },
                {
                    "start": 1058,
                    "end": 1062,
                    "matchedPaperCorpusId": "14659675"
                },
                {
                    "start": 1099,
                    "end": 1102,
                    "matchedPaperCorpusId": "829159"
                },
                {
                    "start": 1129,
                    "end": 1133,
                    "matchedPaperCorpusId": "53976534"
                },
                {
                    "start": 1217,
                    "end": 1221,
                    "matchedPaperCorpusId": "10319744"
                },
                {
                    "start": 1257,
                    "end": 1261,
                    "matchedPaperCorpusId": "155092628"
                },
                {
                    "start": 1325,
                    "end": 1329,
                    "matchedPaperCorpusId": "208175624"
                },
                {
                    "start": 1487,
                    "end": 1491,
                    "matchedPaperCorpusId": "213175568"
                },
                {
                    "start": 1499,
                    "end": 1503,
                    "matchedPaperCorpusId": "232135115"
                },
                {
                    "start": 1505,
                    "end": 1509,
                    "matchedPaperCorpusId": "233033467"
                },
                {
                    "start": 1608,
                    "end": 1612,
                    "matchedPaperCorpusId": "219559263"
                },
                {
                    "start": 1614,
                    "end": 1618,
                    "matchedPaperCorpusId": "215745611"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.97265625
        },
        {
            "corpus_id": "256459285",
            "title": "Knowledge Distillation on Graphs: A Survey",
            "text": "Knowledge distillation aims to train a smaller student model by transferring the knowledge from a larger teacher model [Gou et al., 2021;Wang and Yoon, 2021]. The main idea is to enforce the student model to mimic the teacher model, where the logits, activations, neurons, and features can all be regarded as the knowledge that guides the learning of the student model [Hinton et al., 2015;Ahn et al., 2019;Heo et al., 2019]. In particular, the knowledge distillation loss L kd between the student and the teacher can be defined as follows: \n\n) where DIV indicates the divergence loss (e.g., Kullback-Leibler divergence), k T and k S are the knowledge obtained from the teacher and student models, respectively. A vanilla knowledge distillation considers the logits as the knowledge and employs a Softmax function with temperature to derive the soft targets p, which show the probabilities of the input belonging to various classes. Hence, the Eq. 3 can be reformulated as L kd = DIV(p T , p S ), where p T and p S represent the soft targets derived from the teacher and students, respectively. \n\nSimilarly, in the cases where knowledge is not logits, k T and k S in Eq. 3 can be replaced accordingly, e.g., f T and f S for the learned features of the teacher and student models. After calculating L kd , the student model is trained by a joint objective of both the origin downstream task loss and the knowledge distillation loss L kd , where the former facilitates the student to learn from the original specific task, and the latter targets at transferring the knowledge from the teacher to the student.",
            "score": 0.6265123373304571,
            "section_title": "Knowledge Distillation",
            "char_start_offset": 7164,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 158
                },
                {
                    "start": 159,
                    "end": 425
                },
                {
                    "start": 426,
                    "end": 540
                },
                {
                    "start": 543,
                    "end": 711
                },
                {
                    "start": 712,
                    "end": 932
                },
                {
                    "start": 933,
                    "end": 1094
                },
                {
                    "start": 1097,
                    "end": 1279
                },
                {
                    "start": 1280,
                    "end": 1606
                }
            ],
            "ref_mentions": [
                {
                    "start": 119,
                    "end": 137,
                    "matchedPaperCorpusId": "247362682"
                },
                {
                    "start": 137,
                    "end": 157,
                    "matchedPaperCorpusId": "215745611"
                },
                {
                    "start": 390,
                    "end": 407,
                    "matchedPaperCorpusId": "118649278"
                },
                {
                    "start": 407,
                    "end": 424,
                    "matchedPaperCorpusId": "247362682"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.8828125
        },
        {
            "corpus_id": "159041346",
            "title": "Zero-Shot Knowledge Distillation in Deep Networks",
            "text": "Knowledge Distillation (Hinton et al., 2015) enables to transfer the complex mapping functions learned by cumbersome models to relatively simpler models. The cumbersome model can be an ensemble of multiple large models or a single model with large capacity and strong regualrizers such as Dropout (Srivastava et al., 2014), BatchNorm (Ioffe & Szegedy, 2015), etc. Typically the complex and small models are referred to as Teacher (T) and Student (S) models respectively. Generally the Teacher models deliver excellent performance, but they can be huge and computationally expensive. Hence, these models can not be deployed in The latent information hidden in the confidences assigned by the Teacher to the incorrect categories, referred to as 'dark knowledge' is transferred to the Student via the distillation process. It is this knowledge that helps the Teacher to generalize better and transfers to the Student via matching their soft-labels (output of the soft-max layer) instead of the one-hot vector encoded labels. Matching the softlabels produced by the Teacher is the natural way to transfer its generalization ability. For performing the knowledge distillation, one can use the training data from the target distribution or an arbitrary data. Typically, the data used to perform the distillation is called 'Transfer set'. In order to maximize the information provided per sample, we can make the soft targets to have a high entropy (non-peaky). This is generally achieved by using a high temperature at the softmax layer (Hinton et al., 2015). Also, because of non-peaky soft-labels, the training gradients computed on the loss will have less variance and enable to use higher learning rates leading to quick convergence. \n\nThe existing approaches use natural data either from the target data distribution or a different transfer set to perform the distillation. It is found by (Hinton et al., 2015) that using original training data performs relatively better. They also suggest to have an additional term in the objective for the Student to predict correct labels on the training data along with matching the soft-labels from the Teacher (as shown in eq. ( 1)). However, accessing the samples over which the Teacher had been trained may not always be feasible.",
            "score": 0.6256857689212486,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 153
                },
                {
                    "start": 154,
                    "end": 470
                },
                {
                    "start": 471,
                    "end": 582
                },
                {
                    "start": 583,
                    "end": 819
                },
                {
                    "start": 820,
                    "end": 1021
                },
                {
                    "start": 1022,
                    "end": 1128
                },
                {
                    "start": 1129,
                    "end": 1252
                },
                {
                    "start": 1253,
                    "end": 1331
                },
                {
                    "start": 1332,
                    "end": 1454
                },
                {
                    "start": 1455,
                    "end": 1553
                },
                {
                    "start": 1554,
                    "end": 1731
                },
                {
                    "start": 1734,
                    "end": 1872
                },
                {
                    "start": 1873,
                    "end": 1971
                },
                {
                    "start": 1972,
                    "end": 2173
                },
                {
                    "start": 2174,
                    "end": 2272
                }
            ],
            "ref_mentions": [
                {
                    "start": 297,
                    "end": 322,
                    "matchedPaperCorpusId": "6844431"
                },
                {
                    "start": 334,
                    "end": 357,
                    "matchedPaperCorpusId": "5808102"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.931640625
        },
        {
            "corpus_id": "214641252",
            "title": "Rethinking Few-Shot Image Classification: a Good Embedding Is All You Need?",
            "text": "In [18], the authors generalized this idea and brought it into the deep learning framework. \n\nIn KD, knowledge is transferred from the teacher model to the student model by minimizing a loss in which the target is the distribution of class probabilities induced by the teacher model. In was shown in [61] that KD has several benefits for optimization and knowledge transfer between tasks. BAN [13] introduced sequential distillation, which also improved the performance of teacher models. In natural language processing (NLP), BAM [7] used BAN to distill from single-task models to a multi-task model, helping the multi-task model surpass its single-task teachers. Another two related works are [30] which provides theoretical analysis of self-distillation and CRD [50] which shows distillation improves the transferability across datasets.",
            "score": 0.6252111975821304,
            "section_title": "Related works",
            "char_start_offset": 8838,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 91
                },
                {
                    "start": 94,
                    "end": 283
                },
                {
                    "start": 284,
                    "end": 388
                },
                {
                    "start": 389,
                    "end": 488
                },
                {
                    "start": 489,
                    "end": 664
                },
                {
                    "start": 665,
                    "end": 840
                }
            ],
            "ref_mentions": [
                {
                    "start": 3,
                    "end": 7,
                    "matchedPaperCorpusId": "7200347"
                },
                {
                    "start": 300,
                    "end": 304,
                    "matchedPaperCorpusId": "206596723"
                },
                {
                    "start": 531,
                    "end": 534,
                    "matchedPaperCorpusId": "85464175"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.787109375
        },
        {
            "corpus_id": "258588126",
            "title": "Visual Tuning",
            "text": "Instead of directly fine-tuning or processing the pre-existing model, remapping-based tuning is a category of techniques that transfer the knowledge learned by a pre-trained model to a new downstream model. Based on how to utilize the pre-trained model, i.e., output, weight, and network architecture (see Fig. 5), we discuss three forms of knowledge transfer in the following categories: knowledge distillation-based remapping, weight-based remapping, and architecturebased remapping. \n\n3.5.1 Knowledge Distillation. Knowledge distillation aims to regularize the downstream model by enforcing it to mimic the output of pre-trained models. Note that, the output typically refers to the final response or intermediate features. In the meantime, knowledge distillation is also an important model compression technique. In this section, we do not involve other model compression techniques such as network pruning [66,155,243], since they are not typically motivated to transfer knowledge from teacher network to student network. \n\nThe fundamental idea of knowledge distillation is to transfer the learned knowledge from a large pre-trained teacher model into a small student model by learning the network output or intermediate features of the teacher. Typically, knowledge is distilled from the teacher model to the student model using a soft target distribution for each case. The probability   of each case can be formulated as: \n\nwhere z is the output logit of the teacher networks and  is the temperature of the distillation process. \n\nTo our best knowledge, the work of [16] first introduces knowledge distillation to extract knowledge from a pre-existing model. They trained a compressed model with the pseudo data produced by an ensemble of shallow networks while no significant loss occurs in performance. This idea has been expanded to compress the deep and wide networks into shallower ones in [6]. Hinton et al. [84] introduced the teacher-student knowledge distillation framework, where the student network is penalized based on the softened class distribution output of the teacher network. \n\nOne of the characteristics of deep neural networks is to obtain increasingly expressive power by learning hierarchical feature representations, as pointed out in [7].",
            "score": 0.624986607024185,
            "section_title": "Remapping Tuning",
            "char_start_offset": 54756,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 206
                },
                {
                    "start": 207,
                    "end": 485
                },
                {
                    "start": 488,
                    "end": 517
                },
                {
                    "start": 518,
                    "end": 639
                },
                {
                    "start": 640,
                    "end": 726
                },
                {
                    "start": 727,
                    "end": 816
                },
                {
                    "start": 817,
                    "end": 1026
                },
                {
                    "start": 1029,
                    "end": 1250
                },
                {
                    "start": 1251,
                    "end": 1376
                },
                {
                    "start": 1377,
                    "end": 1429
                },
                {
                    "start": 1432,
                    "end": 1536
                },
                {
                    "start": 1539,
                    "end": 1666
                },
                {
                    "start": 1667,
                    "end": 1812
                },
                {
                    "start": 1813,
                    "end": 1907
                },
                {
                    "start": 1908,
                    "end": 2102
                },
                {
                    "start": 2105,
                    "end": 2271
                }
            ],
            "ref_mentions": [
                {
                    "start": 919,
                    "end": 923,
                    "matchedPaperCorpusId": "2056019"
                },
                {
                    "start": 1574,
                    "end": 1578,
                    "matchedPaperCorpusId": "11253972"
                },
                {
                    "start": 1903,
                    "end": 1906,
                    "matchedPaperCorpusId": "11536917"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.96484375
        },
        {
            "corpus_id": "244293579",
            "title": "Is One Teacher Model Enough to Transfer Knowledge to a Student Model?",
            "text": "There are many types of transfer learning [9] in the literature that use different approaches to solve this problem. To better understand how to classify our proposal, we can say that it can be included in inductive transfer learning [9] because we use teachers trained on the same domain or on a different one but we only use supervised data. We are also in the sub-category of self-taught learning [9] because we only train the student and not other models. \n\nIn many existing papers on transfer learning [1][2][3][4][5][6], the process of knowledge transfer always takes place from one model to another, however, our proposal extends this concept and introduces a transfer of knowledge from many to one as we can see in Figure 1: more than one teacher can teach a student at the same time. Intuitive representation of the proposed approach: a neural network called student, represented as a small robot, can directly learn from the original data represented on the blackboard and also from the knowledge acquired by more than one teacher. The light on the teachers' head, unlike the student's, is not on because their learning is blocked. \n\nA transfer learning method similar to the one proposed here is that of knowledge distillation (KD) [1]. It tries to transfer the learning between two models using as a target the response of the pre-trained model and using as loss function 1/T(z i \u2212 v i ) where T is a hyper-parameter named temperature, z i is the new model prediction using softmax and v i is the prediction of the pre-trained model which is obtained through the use of softmax. This method works well if we transfer learning between models of the same type, but it does not work well between two different models. Furthermore, this solution uses the classification level of the neural model to transfer knowledge from the teacher to the student, whereas our solution is instead based on the level just before the classification level, which is more general. \n\nThe similarity-preserving knowledge distillation [4] is a variation of knowledge distillation that applies an L2 normalization to classical knowledge distillation.",
            "score": 0.6248508670107008,
            "section_title": "Related Work",
            "char_start_offset": 1806,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 116
                },
                {
                    "start": 117,
                    "end": 343
                },
                {
                    "start": 344,
                    "end": 459
                },
                {
                    "start": 462,
                    "end": 792
                },
                {
                    "start": 793,
                    "end": 1041
                },
                {
                    "start": 1042,
                    "end": 1141
                },
                {
                    "start": 1144,
                    "end": 1247
                },
                {
                    "start": 1248,
                    "end": 1590
                },
                {
                    "start": 1591,
                    "end": 1726
                },
                {
                    "start": 1727,
                    "end": 1970
                },
                {
                    "start": 1973,
                    "end": 2136
                }
            ],
            "ref_mentions": [
                {
                    "start": 42,
                    "end": 45,
                    "matchedPaperCorpusId": "225011359"
                },
                {
                    "start": 234,
                    "end": 237,
                    "matchedPaperCorpusId": "225011359"
                },
                {
                    "start": 400,
                    "end": 403,
                    "matchedPaperCorpusId": "225011359"
                },
                {
                    "start": 513,
                    "end": 516,
                    "matchedPaperCorpusId": "118649278"
                },
                {
                    "start": 516,
                    "end": 519,
                    "matchedPaperCorpusId": "198179476"
                },
                {
                    "start": 522,
                    "end": 525,
                    "matchedPaperCorpusId": "49869692"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.52197265625
        },
        {
            "corpus_id": "253499102",
            "title": "PILE: Pairwise Iterative Logits Ensemble for Multi-Teacher Labeled Distillation",
            "text": "The idea of knowledge distillation was first introduced by Cristian et al. (2006) to train small and fast models to mimic cumbersome and complex models, without much loss in performance. Hinton et al. (2015a) developed this idea further by minimizing the difference between their soft target distribution. With the rise of the pre-training and fine-tuning paradigm, various work has later extended this idea to large-scale pretrained models and shown impressive results on multiple NLP tasks (Wang et al., 2019;Rajpurkar et al., 2018;Lai et al., 2017) with a significant gain in training efficiency. Sanh et al. (2019) conducted knowledge transfer during the pre-training phase, also known as a task-agnostic way. Sun et al. (2019a) proposed an approach to transfer knowledge between intermediate layers in the fine-tuning stage. Jiao et al. (2020) additionally uses attentionbased distillation and hidden states-based distillation for students to imitate teachers' behaviors in intermediate layers. Wang et al. (2020) introduced self-attention relation-based transfer and teacher assistants (Mirzadeh et al., 2020) to further improve the performance of students. \n\nEnsemble Knowledge Distillation There is also some other work exploring the issues of multiteacher distillation. For example, Du et al. (2020) adaptively ensemble knowledge distillation to find a better optimizing direction for the student network. Wu et al. (2021) designed a co-finetuning framework to jointly finetune multiple teachers for better collaborative knowledge distillation. Li et al. (2021) explored the influence of teacher model adoption which is promising for improving student performance. Different from the above work, we investigate the problem of ensemble knowledge distillation in ranking tasks and use the golden label to supervise the ensemble process.",
            "score": 0.6247462040724415,
            "section_title": "Knowledge Distillation",
            "char_start_offset": 5699,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 186
                },
                {
                    "start": 187,
                    "end": 305
                },
                {
                    "start": 306,
                    "end": 599
                },
                {
                    "start": 600,
                    "end": 713
                },
                {
                    "start": 714,
                    "end": 829
                },
                {
                    "start": 830,
                    "end": 999
                },
                {
                    "start": 1000,
                    "end": 1163
                },
                {
                    "start": 1166,
                    "end": 1278
                },
                {
                    "start": 1279,
                    "end": 1414
                },
                {
                    "start": 1415,
                    "end": 1553
                },
                {
                    "start": 1554,
                    "end": 1673
                },
                {
                    "start": 1674,
                    "end": 1843
                }
            ],
            "ref_mentions": [
                {
                    "start": 59,
                    "end": 81,
                    "matchedPaperCorpusId": "11253972"
                },
                {
                    "start": 187,
                    "end": 208,
                    "matchedPaperCorpusId": "7200347"
                },
                {
                    "start": 492,
                    "end": 511,
                    "matchedPaperCorpusId": "5034059"
                },
                {
                    "start": 511,
                    "end": 534,
                    "matchedPaperCorpusId": "47018994"
                },
                {
                    "start": 534,
                    "end": 551,
                    "matchedPaperCorpusId": "6826032"
                },
                {
                    "start": 714,
                    "end": 732,
                    "matchedPaperCorpusId": "201670719"
                },
                {
                    "start": 830,
                    "end": 848,
                    "matchedPaperCorpusId": "202719327"
                },
                {
                    "start": 1000,
                    "end": 1018,
                    "matchedPaperCorpusId": "211296536"
                },
                {
                    "start": 1092,
                    "end": 1115,
                    "matchedPaperCorpusId": "212908749"
                },
                {
                    "start": 1292,
                    "end": 1308,
                    "matchedPaperCorpusId": "227276362"
                },
                {
                    "start": 1554,
                    "end": 1570,
                    "matchedPaperCorpusId": "237605152"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.8564453125
        },
        {
            "corpus_id": "237492204",
            "title": "On the Efficiency of Subclass Knowledge Distillation in Classification Tasks",
            "text": "Knowledge Distillation in Classification Tasks. Transferring knowledge from one model to another is a research topic that has obtained noteworthy attention during recent years. Ba and Caruana (Ba and Caruana 2014) trained a single and small neural network to imitate the logits of a large and complex neural network. Then, Hinton et al. (Hinton, Vinyals, and Dean 2015) introduced KD and dark knowledge to claim that the deeper teacher model can successfully distill its knowledge into the smaller student neural network by matching their soft targets (softmax distributions). Nowadays, a lot of successive papers have been written to propose different techniques to KD for model compression purposes. Romero et al. (Romero et al. 2015) distilled the feature representations of the teacher's intermediate layers to the student for improving the training stage of the student network. Transferring the attention maps (Zagoruyko and Komodakis 2017;Huang and Wang 2017;Tarvainen and Valpola 2017), the inner products of intermediate activation maps (Yim et al. 2017), and relational knowledge between training samples (Park et al. 2019;Tung and Mori 2019;Peng et al. 2019;Liu et al. 2019) are some other methods to promote the distillation process from one model to another. However, these approaches ignored the possibility of available subclass knowledge within the classes and therefore did not take advantage of hidden subclass knowledge to improve student performance. By contrast, in this study, we use subclass knowledge to enhance the generalization ability of the teacher network. \n\nSubclass Knowledge Distillation. The distillation of knowledge can be improved by increasing the amount of information that the teacher can transfer to the student. M\u00fcller et al. (M\u00fcller, Kornblith, and Hinton 2020) compelled the teacher to create semantically meaningful subclasses for each class during its training phase with auxiliary contrastive loss. The student is then trained to mimic the invented teacher's subclasses predictions (probabilities). When the number of training samples per class is the same, they measured the number of bits of label information about how the teacher generalizes through subclass distillation in binary classification tasks.",
            "score": 0.6232452457746126,
            "section_title": "Related Work",
            "char_start_offset": 4680,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 47
                },
                {
                    "start": 48,
                    "end": 176
                },
                {
                    "start": 177,
                    "end": 316
                },
                {
                    "start": 317,
                    "end": 576
                },
                {
                    "start": 577,
                    "end": 701
                },
                {
                    "start": 702,
                    "end": 883
                },
                {
                    "start": 884,
                    "end": 1271
                },
                {
                    "start": 1272,
                    "end": 1470
                },
                {
                    "start": 1471,
                    "end": 1586
                },
                {
                    "start": 1589,
                    "end": 1621
                },
                {
                    "start": 1622,
                    "end": 1753
                },
                {
                    "start": 1754,
                    "end": 1945
                },
                {
                    "start": 1946,
                    "end": 2045
                },
                {
                    "start": 2046,
                    "end": 2254
                }
            ],
            "ref_mentions": [
                {
                    "start": 177,
                    "end": 213,
                    "matchedPaperCorpusId": "11536917"
                },
                {
                    "start": 337,
                    "end": 369,
                    "matchedPaperCorpusId": "7200347"
                },
                {
                    "start": 916,
                    "end": 946,
                    "matchedPaperCorpusId": "829159"
                },
                {
                    "start": 966,
                    "end": 993,
                    "matchedPaperCorpusId": "263861232"
                },
                {
                    "start": 1046,
                    "end": 1063,
                    "matchedPaperCorpusId": "206596723"
                },
                {
                    "start": 1115,
                    "end": 1133,
                    "matchedPaperCorpusId": "131765296"
                },
                {
                    "start": 1133,
                    "end": 1152,
                    "matchedPaperCorpusId": "198179476"
                },
                {
                    "start": 1152,
                    "end": 1169,
                    "matchedPaperCorpusId": "102483463"
                },
                {
                    "start": 1169,
                    "end": 1185,
                    "matchedPaperCorpusId": "198185886"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.888671875
        },
        {
            "corpus_id": "274023578",
            "title": "Dual-Head Knowledge Distillation: Enhancing Logits Utilization with an Auxiliary Head",
            "text": "2.1 KNOWLEDGE DISTILLATION Knowledge distillation (KD) (Hinton et al., 2015) aims to transfer knowledge from a large teacher network to a small student network. Existing works can be roughly divided into two groups: featurebased methods and logit-based methods. \n\nFeature-based methods focus on distilling knowledge from intermediate feature layers. FitNet (Romero et al., 2015) is the first approach to distill knowledge from intermediate features by measuring the distance between feature maps. RKD (Park et al., 2019) utilizes the relations among instances to guide the training process of the student model. CRD (Tian et al., 2019) incorporates contrastive learning into knowledge distillation. OFD (Heo et al., 2019) contains a new distance function to distill significant information between the teacher and student using marginal ReLU. \n\nReviewKD (Chen et al., 2021) proposes a review mechanism that uses multiple layers in the teacher to supervise one layer in the student. Other papers (Passalis & Tefas, 2018;Kim et al., 2018;Koratana et al., 2019;Li, 2022;Liu et al., 2023;Wang et al., 2023;Roy Miles & Deng, 2024;Miles & Mikolajczyk, 2024) enforce various criteria based on features. Most feature-based methods can attain superior performance, yet involving considerably high computational and storage costs. \n\nLogit-based methods mainly concentrate on distilling knowledge from logits and softmax scores after logits. DML (Zhang et al., 2018b) introduces a mutual learning method to train both teachers and students simultaneously. DKD (Zhao et al., 2022) proposes a novel logit-based method to reformulate the classical KD loss into two parts and achieves state-of-the-art performance by adjusting weights for these two parts. DIST (Huang et al., 2022) relaxes the exact matching in previous KL divergence loss with a correlation-based loss and performs better when the discrepancy between the teacher and the student is large.",
            "score": 0.6231989142715317,
            "section_title": "RELATED WORK",
            "char_start_offset": 4677,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 160
                },
                {
                    "start": 161,
                    "end": 261
                },
                {
                    "start": 264,
                    "end": 349
                },
                {
                    "start": 350,
                    "end": 496
                },
                {
                    "start": 497,
                    "end": 611
                },
                {
                    "start": 612,
                    "end": 698
                },
                {
                    "start": 699,
                    "end": 842
                },
                {
                    "start": 845,
                    "end": 981
                },
                {
                    "start": 982,
                    "end": 1195
                },
                {
                    "start": 1196,
                    "end": 1320
                },
                {
                    "start": 1323,
                    "end": 1430
                },
                {
                    "start": 1431,
                    "end": 1544
                },
                {
                    "start": 1545,
                    "end": 1740
                },
                {
                    "start": 1741,
                    "end": 1941
                }
            ],
            "ref_mentions": [
                {
                    "start": 501,
                    "end": 520,
                    "matchedPaperCorpusId": "131765296"
                },
                {
                    "start": 616,
                    "end": 635,
                    "matchedPaperCorpusId": "204838340"
                },
                {
                    "start": 703,
                    "end": 721,
                    "matchedPaperCorpusId": "102483181"
                },
                {
                    "start": 854,
                    "end": 873,
                    "matchedPaperCorpusId": "233296935"
                },
                {
                    "start": 995,
                    "end": 1019,
                    "matchedPaperCorpusId": "52012952"
                },
                {
                    "start": 1036,
                    "end": 1058,
                    "matchedPaperCorpusId": "172133986"
                },
                {
                    "start": 1102,
                    "end": 1125,
                    "matchedPaperCorpusId": "268358422"
                },
                {
                    "start": 1125,
                    "end": 1151,
                    "matchedPaperCorpusId": "257632008"
                },
                {
                    "start": 1435,
                    "end": 1456,
                    "matchedPaperCorpusId": "26071966"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.9755859375
        },
        {
            "corpus_id": "258341405",
            "title": "Multi-target Knowledge Distillation via Student Self-reflection",
            "text": "Muhammad et al. (2021) proposed a new robust knowledge transfer via distilling activated channel maps to overcome the susceptibility of the natural samples. Besides, except knowledge distillation, the channel features have been fully used for improving the performance of the other deep learning methods (Lou & Loew, 2021;Chen et al., 2021;Yan et al., 2021). In this paper, we also use the channel-based knowledge to improve the distillation performance, but consider multiple types of knowledge from the channel features and the logit outputs at different layers, and accordingly both stage-wise channel distillation and stage-wise response distillation are devised. \n\nMost importantly, we consider not only the importance of teaching from teacher model, but also the knowledge transferring from previous stage to later stages within the student network, which we refer to it as the student self-reflection or self-knowledge review. By doing this, we take into account the advantages of both offline and self-distillation. That is, we further enable student to review and then consolidate its previous learned knowledge.",
            "score": 0.6228610032885915,
            "section_title": "Teacher's Knowledge",
            "char_start_offset": 11825,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 156
                },
                {
                    "start": 157,
                    "end": 358
                },
                {
                    "start": 359,
                    "end": 667
                },
                {
                    "start": 670,
                    "end": 933
                },
                {
                    "start": 934,
                    "end": 1023
                },
                {
                    "start": 1024,
                    "end": 1121
                }
            ],
            "ref_mentions": [
                {
                    "start": 304,
                    "end": 322,
                    "matchedPaperCorpusId": "232320441"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.8125
        },
        {
            "corpus_id": "271441660",
            "title": "Knowledge Distillation in Image Classification: The Impact of Datasets",
            "text": "Several works have explored various aspects of knowledge distillation in deep neural networks [26], including teacher and student model architectures, regularization techniques, and optimization methods. \n\nFor example, Li et al. proposed a transferred attention method to improve the performance of convolutional neural networks [27], while Yazdanbakhsh et al. studied the application of knowledge distillation in specific domains such as healthcare [19]. However, despite these significant advances, little attention has been paid to the impact of data on this knowledge transfer process. The authors demonstrated the effectiveness of the distillation on various tasks and highlighted its potential for model compression. The FitNets paper [20] proposed a specific form of knowledge distillation called FitNets, where a student network is guided not only by the output probabilities of a teacher network but also by intermediate representations (or hints). This work aimed to improve the transfer of information in the training process. Ref. [27] introduces attention transfer as a form of knowledge distillation. It focuses on transferring attention maps from a teacher to a student network to improve the student's performance. Attention transfer has proven effective in enhancing the generalization capabilities of the student model. To address the limitations of traditional knowledge distillation, ref. [31] introduces Jacobian matching, a novel method that aims to transfer not only the output probabilities but also the derivatives of the teacher model's predictions. This approach provides a more comprehensive form of knowledge transfer. Ref. [30] explores the benefits of knowledge distillation beyond model compression. The authors show that the knowledge distillation process not only compresses models but also accelerates the optimization process, enabling faster convergence during training. Ref. [32] introduces the concept of a \"teacher assistant\" by proposing an extension to traditional knowledge distillation. The teacher assistant helps bridge the performance gap between the teacher and the student, leading to enhanced knowledge transfer.",
            "score": 0.6225059214404112,
            "section_title": "Knowledge Distillation in the Literature",
            "char_start_offset": 6239,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 203
                },
                {
                    "start": 206,
                    "end": 455
                },
                {
                    "start": 456,
                    "end": 589
                },
                {
                    "start": 590,
                    "end": 722
                },
                {
                    "start": 723,
                    "end": 957
                },
                {
                    "start": 958,
                    "end": 1037
                },
                {
                    "start": 1038,
                    "end": 1114
                },
                {
                    "start": 1115,
                    "end": 1230
                },
                {
                    "start": 1231,
                    "end": 1337
                },
                {
                    "start": 1338,
                    "end": 1575
                },
                {
                    "start": 1576,
                    "end": 1647
                },
                {
                    "start": 1648,
                    "end": 1731
                },
                {
                    "start": 1732,
                    "end": 1907
                },
                {
                    "start": 1908,
                    "end": 2030
                },
                {
                    "start": 2031,
                    "end": 2162
                }
            ],
            "ref_mentions": [
                {
                    "start": 94,
                    "end": 98,
                    "matchedPaperCorpusId": "249209742"
                },
                {
                    "start": 450,
                    "end": 454,
                    "matchedPaperCorpusId": "259389158"
                },
                {
                    "start": 1409,
                    "end": 1413,
                    "matchedPaperCorpusId": "3603145"
                },
                {
                    "start": 1653,
                    "end": 1657,
                    "matchedPaperCorpusId": "206596723"
                },
                {
                    "start": 1913,
                    "end": 1917,
                    "matchedPaperCorpusId": "212908749"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.9677734375
        },
        {
            "corpus_id": "258740711",
            "title": "When Gradient Descent Meets Derivative-Free Optimization: A Match Made in Black-Box Scenario",
            "text": "As a representative method of model compression, knowledge distillation transfers the knowledge from a larger deep neural network (teacher) to a smaller network (student) (Hinton et al., 2015;Kim and Rush, 2016). There have been different distillation algorithms being proposed to face more complex settings of transferring knowledge, including adversarial distillation (Ma et al., 2020;Wang et al., 2022), multi-teacher distillation (Guan et al., 2020;Yuan et al., 2021) and data-free distillation (Fang et al., 2022;Binici et al., 2022). Furthermore, the superior success of PLMs has also spurred researchers to distill PLMs into smaller models while retaining performance. DistilBERT (Sanh et al., 2019) introduces a triple loss combining language modeling and cosine-distance losses to leverage the inductive biases learned by large models during pre-training. TinyBERT (Jiao et al., 2020) performs a Transformer distillation method at both the pre-training and task-specific learning stages. \n\nNewsBERT (Wu et al., 2021a) designs a collaborative learning framework where the student model can learn from the experience of the teacher model. \n\nIn this paper, we consider knowledge distillation to transfer knowledge from a black-box teacher to a student, which is used for training a prompt generator by gradient descent.",
            "score": 0.6224491572971573,
            "section_title": "Knowledge Distillation",
            "char_start_offset": 7817,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 212
                },
                {
                    "start": 213,
                    "end": 539
                },
                {
                    "start": 540,
                    "end": 675
                },
                {
                    "start": 676,
                    "end": 864
                },
                {
                    "start": 865,
                    "end": 996
                },
                {
                    "start": 999,
                    "end": 1145
                },
                {
                    "start": 1148,
                    "end": 1325
                }
            ],
            "ref_mentions": [
                {
                    "start": 370,
                    "end": 387,
                    "matchedPaperCorpusId": "222290473"
                },
                {
                    "start": 387,
                    "end": 405,
                    "matchedPaperCorpusId": "233481094"
                },
                {
                    "start": 434,
                    "end": 453,
                    "matchedPaperCorpusId": "220935582"
                },
                {
                    "start": 453,
                    "end": 471,
                    "matchedPaperCorpusId": "228376532"
                },
                {
                    "start": 499,
                    "end": 518,
                    "matchedPaperCorpusId": "261514306"
                },
                {
                    "start": 874,
                    "end": 893,
                    "matchedPaperCorpusId": "202719327"
                },
                {
                    "start": 1008,
                    "end": 1025,
                    "matchedPaperCorpusId": "231855304"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.94873046875
        },
        {
            "corpus_id": "232380330",
            "title": "Distilling a Powerful Student Model via Online Knowledge Distillation",
            "text": "To this end, Zhang et al. [10] proposed a deep mutual learning strategy where pair-wise students are encouraged to learn from each other by a mimicry loss based on the Kullback-Leibler divergence. Chen et al. [17] performed two-level distillation by training multiple auxiliary peers and one group leader, separately. The former aims to boost peer diversity, while the latter transfers knowledge from an ensemble of auxiliary peers to the group leader. In [15], online knowledge distillation was built at a feature-map level using the adversarial training framework. Kim et al. [19] fused the intermediate representations of subnetworks, passing the result to an auxiliary classifier. Then, the knowledge from the auxiliary classifier is delivered back to each subnetwork for mutual teaching. In [12], [11], [18], all student branches are ensembled to construct a stronger teacher model, which is in turn distilled back to the students to enhance the model learning. EnD2 [41] distilled the distribution of the predictions of the ensemble into a single model. It enables a single model to retain both the performance of the ensemble as well as the ability of uncertainty estimation. EnD2 also performs diversity enhancement to capture uncertain information. However, we enhance the diversity among common students so that student leaders can obtain higher benefits from feature fusion. \n\nSelf-Distillation. Self-distillation, originally proposed by Furlanello et al. [42], has received a great deal of attention recently, due to its distillation of knowledge within the network itself without the aid of other models. Augmentation based works [43], [44] focus on self-distillation via data augmentation of the input images. Hou et al. [45] and Zhang et al. [46] distilled deeper parts of the network as the conceptual teacher model to guide the learning of shallower modules. Li et al. [20] revisited knowledge distillation as a type of learned label smoothing regularization, and accordingly proposed a novel teacher-free knowledge distillation framework where the student model learns from itself or a manually designed regularization distribution.",
            "score": 0.6222124042051076,
            "section_title": "II. RELATED WORK",
            "char_start_offset": 7339,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 196
                },
                {
                    "start": 197,
                    "end": 317
                },
                {
                    "start": 318,
                    "end": 452
                },
                {
                    "start": 453,
                    "end": 566
                },
                {
                    "start": 567,
                    "end": 684
                },
                {
                    "start": 685,
                    "end": 792
                },
                {
                    "start": 793,
                    "end": 966
                },
                {
                    "start": 967,
                    "end": 1059
                },
                {
                    "start": 1060,
                    "end": 1182
                },
                {
                    "start": 1183,
                    "end": 1257
                },
                {
                    "start": 1258,
                    "end": 1385
                },
                {
                    "start": 1388,
                    "end": 1406
                },
                {
                    "start": 1407,
                    "end": 1617
                },
                {
                    "start": 1618,
                    "end": 1723
                },
                {
                    "start": 1724,
                    "end": 1875
                },
                {
                    "start": 1876,
                    "end": 2150
                }
            ],
            "ref_mentions": [
                {
                    "start": 26,
                    "end": 30,
                    "matchedPaperCorpusId": "26071966"
                },
                {
                    "start": 209,
                    "end": 213,
                    "matchedPaperCorpusId": "208526905"
                },
                {
                    "start": 456,
                    "end": 460,
                    "matchedPaperCorpusId": "209319166"
                },
                {
                    "start": 796,
                    "end": 800,
                    "matchedPaperCorpusId": "48352434"
                },
                {
                    "start": 802,
                    "end": 806,
                    "matchedPaperCorpusId": "44119099"
                },
                {
                    "start": 808,
                    "end": 812,
                    "matchedPaperCorpusId": "219965421"
                },
                {
                    "start": 972,
                    "end": 976,
                    "matchedPaperCorpusId": "141465546"
                },
                {
                    "start": 1649,
                    "end": 1653,
                    "matchedPaperCorpusId": "70335318"
                },
                {
                    "start": 1735,
                    "end": 1739,
                    "matchedPaperCorpusId": "195453293"
                },
                {
                    "start": 1757,
                    "end": 1761,
                    "matchedPaperCorpusId": "159041406"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.896484375
        },
        {
            "corpus_id": "252355060",
            "title": "CES-KD: Curriculum-based Expert Selection for Guided Knowledge Distillation",
            "text": "Knowledge distillation is a training method that is based on teacher-student learning. It was firstly introduced as a mode of model compression by Bucilua et al. [12] then further popularized by Hinton et al. [7]. The goal of KD is to increase the accuracy of a student network due to transfer of information from the pre-trained teacher network during training. Distillation methods rely on different techniques to capture the knowledge of the teacher that is transferred to the student. The traditional KD [7] uses the soft targets produced by the teacher network as the knowledge to transfer to the student. Some recent works focus on feature-based knowledge distillation and train the student to match the intermediate layers of the teacher [13]. In addition, some approaches [14] transfer spatial attention maps, where the student attends to similar parts of the image as the teacher network. In our work we exploit the soft targets of the teacher network for our distillation process.",
            "score": 0.6211069732155501,
            "section_title": "II. RELATED WORKS A. Knowledge Distillation",
            "char_start_offset": 4326,
            "sentence_offsets": [],
            "ref_mentions": [
                {
                    "start": 162,
                    "end": 166,
                    "matchedPaperCorpusId": "11253972"
                },
                {
                    "start": 780,
                    "end": 784,
                    "matchedPaperCorpusId": "198179476"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.939453125
        },
        {
            "corpus_id": "245334968",
            "title": "Anomaly Discovery in Semantic Segmentation via Distillation Comparison Networks",
            "text": "Knowledge distillation aims to transfer the knowledge of a large network (a.k.a teacher network) to a small network (a.k.a student network). The student network may have an approximate predictive ability as the teacher network. Knowledge distillation methods can be divided into three types: logits-based approaches (Cho and Hariharan 2019;Hinton, Vinyals, and Dean 2015;Phuong and Lampert 2019;Xie et al. 2020;Yang et al. 2019;Zhang et al. 2018), feature-based approaches (Komodakis and Zagoruyko 2017;Romero et al. 2014), and relation-based approaches (Tung and Mori 2019;Yim et al. 2017;Romero et al. 2014). In this paper, we follow the feature-based paradigm (Romero et al. 2014) by proposing a distribution distillation. However, DiCNet is flexible since there are no strict constraints for the model size of the student branch.",
            "score": 0.6210420918725026,
            "section_title": "Knowledge Distillation",
            "char_start_offset": 8873,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 140
                },
                {
                    "start": 141,
                    "end": 227
                },
                {
                    "start": 228,
                    "end": 610
                },
                {
                    "start": 611,
                    "end": 725
                },
                {
                    "start": 726,
                    "end": 833
                }
            ],
            "ref_mentions": [
                {
                    "start": 316,
                    "end": 340,
                    "matchedPaperCorpusId": "203642130"
                },
                {
                    "start": 371,
                    "end": 395,
                    "matchedPaperCorpusId": "207994757"
                },
                {
                    "start": 395,
                    "end": 411,
                    "matchedPaperCorpusId": "207853355"
                },
                {
                    "start": 411,
                    "end": 428,
                    "matchedPaperCorpusId": "54986302"
                },
                {
                    "start": 428,
                    "end": 446,
                    "matchedPaperCorpusId": "26071966"
                },
                {
                    "start": 473,
                    "end": 503,
                    "matchedPaperCorpusId": "829159"
                },
                {
                    "start": 554,
                    "end": 574,
                    "matchedPaperCorpusId": "198179476"
                },
                {
                    "start": 574,
                    "end": 590,
                    "matchedPaperCorpusId": "206596723"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.96728515625
        },
        {
            "corpus_id": "248683566",
            "title": "Teacher-student collaborative knowledge distillation for image classification",
            "text": "Knowledge distillation (KD), an important method of model compression [3][4][5], is effective in transferring \"dark knowledge\" from a larger model to a smaller model, allowing the smaller model to approximate the performance level achieved by the larger model [6][7][8]. This concept was first proposed in [9], but then was not explicitly explained. In 2014, [10] proposed an approach that enables a student network to learn the soft targets output by a teacher network and defined the method as knowledge distillation. However, conventional knowledge distillation methods only learn the output of the teacher network, which leads to the loss of intermediate layer knowledge. Later approaches attempted to exploit the information contained in middle model layers by designing different knowledge representations rather than just using the output information [11][12][13][14][15][16][17]. For example, [11] proposed an approach in which the student network simulates not only the output of the teacher network but also the hidden layer characteristics of the teacher network. [12] used attention transfer mechanisms to significantly improve its performance by forcing the student network to mimic the attention map of the powerful teacher network. Although the above algorithms utilized knowledge from the teacher network, they only consider the output of a specific layer of the teacher network. The relational knowledge distillation (RKD) approach proposed by [15] can transfer the structured relationships associated with the output results obtained by the teacher network to the student network, which alleviates the above problem. The correlations among different categories of probabilities may contain useful information to regularize a learning problem, and [16] found that the generation gap between teacher and student representation of mutual information can be minimized through contrastive representation distillation. Based on an adversarial-based learning strategy as a supervisor to guide and optimize lightweight student networks and recover knowledge from teacher networks, [18] recently proposed a knowledge distillation method for one-stage object detection . [19] constructed a compressed model to learn low-dimensional spatial information from potential representations of teacher networks. Most studies have focused on the representation of feature knowledge or methods of maximizing the transfer of teacher network feature knowledge while ignoring the potential capabilities of student networks.",
            "score": 0.6210257238007683,
            "section_title": "Knowledge distillation",
            "char_start_offset": 4620,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 270
                },
                {
                    "start": 271,
                    "end": 349
                },
                {
                    "start": 350,
                    "end": 519
                },
                {
                    "start": 520,
                    "end": 675
                },
                {
                    "start": 676,
                    "end": 887
                },
                {
                    "start": 888,
                    "end": 1074
                },
                {
                    "start": 1075,
                    "end": 1246
                },
                {
                    "start": 1247,
                    "end": 1395
                },
                {
                    "start": 1396,
                    "end": 1634
                },
                {
                    "start": 1635,
                    "end": 1930
                },
                {
                    "start": 1931,
                    "end": 2178
                },
                {
                    "start": 2179,
                    "end": 2311
                },
                {
                    "start": 2312,
                    "end": 2518
                }
            ],
            "ref_mentions": [
                {
                    "start": 70,
                    "end": 73,
                    "matchedPaperCorpusId": "167217261"
                },
                {
                    "start": 73,
                    "end": 76,
                    "matchedPaperCorpusId": "32588614"
                },
                {
                    "start": 76,
                    "end": 79,
                    "matchedPaperCorpusId": "222310537"
                },
                {
                    "start": 260,
                    "end": 263,
                    "matchedPaperCorpusId": "206596723"
                },
                {
                    "start": 263,
                    "end": 266,
                    "matchedPaperCorpusId": "8451212"
                },
                {
                    "start": 266,
                    "end": 269,
                    "matchedPaperCorpusId": "219559263"
                },
                {
                    "start": 306,
                    "end": 309,
                    "matchedPaperCorpusId": "11253972"
                },
                {
                    "start": 862,
                    "end": 866,
                    "matchedPaperCorpusId": "829159"
                },
                {
                    "start": 866,
                    "end": 870,
                    "matchedPaperCorpusId": "198179476"
                },
                {
                    "start": 870,
                    "end": 874,
                    "matchedPaperCorpusId": "118649278"
                },
                {
                    "start": 874,
                    "end": 878,
                    "matchedPaperCorpusId": "131765296"
                },
                {
                    "start": 878,
                    "end": 882,
                    "matchedPaperCorpusId": "204838340"
                },
                {
                    "start": 882,
                    "end": 886,
                    "matchedPaperCorpusId": "53213211"
                },
                {
                    "start": 1075,
                    "end": 1079,
                    "matchedPaperCorpusId": "829159"
                },
                {
                    "start": 1461,
                    "end": 1465,
                    "matchedPaperCorpusId": "131765296"
                },
                {
                    "start": 1765,
                    "end": 1769,
                    "matchedPaperCorpusId": "204838340"
                },
                {
                    "start": 2091,
                    "end": 2095,
                    "matchedPaperCorpusId": "237734820"
                },
                {
                    "start": 2179,
                    "end": 2183,
                    "matchedPaperCorpusId": "225257208"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.97265625
        },
        {
            "corpus_id": "252968120",
            "title": "On effects of Knowledge Distillation on Transfer Learning",
            "text": "Ensembles of networks are usually trained with different architectures or initializations to minimize similarities in their learning and achieve maximum performance. However, running the ensembles in the production environment is extremely time-consuming. In an attempt to combat this, the idea of training a small production model by transferring knowledge from larger networks on the same training data has also been around for more than a decade [69]. The primary purpose was to teach data mapping to the set of network labels or learning of big networks. This knowledge transfer with the help of soft labels was first introduced in [14]. The basics of what it is and how it works are described in detail in Section 1.1.3. Various knowledge distillation methods have been proposed in the literature since their inception. Typically, the progress of the type of distillation can be divided into three types of distillation, i.e., offline, online, and self-distillation [18]. In offline KD, the teacher network is typically pre-trained to an extent and then distilled into a student network separately [14]. In online distillation, teacher and student networks are simultaneously trained end-to-end and improved [70,71]. Self-distillation is a form of self-supervised learning in which the same network of previous time steps provides the knowledge to distill a newer version of the student [72]. Sometimes self-distillation can also be used to distill knowledge from specialized deep layers to generalized shallow layers. On the other hand, actual knowledge can also be divided into three types, i.e., response-based, feature-based, and relation-based. In response-based knowledge, we try to mimic the teacher's predictions with the student [14]. Featurebased knowledge uses features or parts of the teacher network to mimic representations [73]. In relation-based knowledge distillation, the relationship between the different layers or data is explored, treating correlations between features as knowledge [74]. [75] explores Knowledge Distillation with Self-supervised learning or self-training for self-distillation without the need for explicit larger teacher networks. For a more comprehensive survey of KD methods, see [18].\n\nIn summary, there are many ways in which knowledge can",
            "score": 0.6209996838523728,
            "section_title": "Knowledge Distillation",
            "char_start_offset": 49134,
            "sentence_offsets": [],
            "ref_mentions": [
                {
                    "start": 449,
                    "end": 453,
                    "matchedPaperCorpusId": "11253972"
                },
                {
                    "start": 636,
                    "end": 640,
                    "matchedPaperCorpusId": "7200347"
                },
                {
                    "start": 971,
                    "end": 975,
                    "matchedPaperCorpusId": "219559263"
                },
                {
                    "start": 1103,
                    "end": 1107,
                    "matchedPaperCorpusId": "7200347"
                },
                {
                    "start": 1213,
                    "end": 1217,
                    "matchedPaperCorpusId": "212908749"
                },
                {
                    "start": 1217,
                    "end": 1220,
                    "matchedPaperCorpusId": "208526905"
                },
                {
                    "start": 1392,
                    "end": 1396,
                    "matchedPaperCorpusId": "159041406"
                },
                {
                    "start": 1743,
                    "end": 1747,
                    "matchedPaperCorpusId": "7200347"
                },
                {
                    "start": 1843,
                    "end": 1847,
                    "matchedPaperCorpusId": "203953149"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.947265625
        },
        {
            "corpus_id": "233231665",
            "title": "Domain Adaptation and Multi-Domain Adaptation for Neural Machine Translation: A Survey",
            "text": "Knowledge distillation and similar 'teacher-student' model compression schemes effectively use one teacher model to regularize training or tuning of a separate student model (Bucilu\u01ce et al., 2006;Hinton et al., 2015). Typically the teacher model is a large, pre-trained model, and the student is required to emulate its behaviour with far fewer parameters. The student model is tuned so its output distribution remains similar to the teacher's output distribution over the pre-trained data, often via an addition to the loss function . We note that 'knowledge distillation' is sometimes used in the literature to mean training a student model on forward translations produced by a teacher model -this approach is described in Section 5.3.2.\n\nIn a domain adaptation context, knowledge distillation encourages similar performance on the pre-training domain using a regularization function between general and in-domain model output distributions. The teacher is another NMT model. Dakwale and Monz (2017) focus on weighting the teacher distribution weighting to reduce catastrophic forgetting, while  aim for better in-domain performance on a single domain and Mghabbar and Ratnamogan (2020) use knowledge distillation to adapt to multiple domains simultaneously. Cao et al. (2021) extend knowledge distillation for domain adaptation to a continual learning scenario where the model is adapted sequentially multiple times. They propose dynamic knowledge distillation, in which older versions of the model carry less weight compared to newer versions.\n\nWe see knowledge distillation as similar in spirit to parameter regularization. However, it has a higher computational cost, since two models must actually operate on the data, instead of some parameter values simply being stored in memory. This can be effective when the aim is to compress the teacher model, since in this case the in-domain student model is likely to be much smaller than the other. For models remaining the same size, parameter regularization may be more practical.   Weinshall et al., 2018). Difficulty can be determined in terms of objective data features like sentence length, linguistic complexity or word rarity (Kocmi & Bojar, 2017;Platanios et al., 2019). Difficulty can also depend on model 'competence' for a given example at a certain state (Platanios et al.,",
            "score": 0.6205031686170824,
            "section_title": "Knowledge Distillation",
            "char_start_offset": 76909,
            "sentence_offsets": [],
            "ref_mentions": [
                {
                    "start": 174,
                    "end": 196,
                    "matchedPaperCorpusId": "11253972"
                },
                {
                    "start": 979,
                    "end": 1002,
                    "matchedPaperCorpusId": "6236733"
                },
                {
                    "start": 2038,
                    "end": 2061,
                    "matchedPaperCorpusId": "5004002"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.9296875
        },
        {
            "corpus_id": "234336288",
            "title": "Performance Analysis of Deep Neural Network based on Transfer Learning for Pet Classification",
            "text": "Knowledge distillation (KD) was introduced by [30] as: \n\n\u2022 Train a large model that performs and generalizes very well. This is called the teacher model. \n\n\u2022 Take all the data you have and compute the predictions of the teacher model. The total dataset with these predictions is called the knowledge, and the predictions themselves are often referred to as soft targets. This is the knowledge distillation step. \n\n\u2022 Use the previously obtained knowledge to train the smaller network, called the student model. IV. TEACHER-STUDENT LEARNING Knowledge distillation starts with training a larger model, the teacher 'T'. As it is trained on a heavier platform (GPU), it achieves high performance. Then a lightweight model known as student 'S' is deployed to learn from 'T'. Now, 'S' is supposed to give comparable performance as 'T' but with less memory and more speed. \n\nTo improve knowledge transfer from teacher to student various types of methods are researched. Assuming a trained 'T' has already eliminated some label errors contained in the ground truth data, the authors in [29] treated the hard label predicted by 'T' as the underlying knowledge. While in [30], the soft label produced by 'T', i.e., the classification probabilities, are focused to provide more information to transfer. In general, knowledge is transferred from the 'T' to 'S' by minimizing a loss function in which the target is the distribution of class probabilities predicted by 'T'. This probability distribution has the correct class at a very high probability (close to '1') with all other class probabilities very close to '0'. As such, it does not provide much information beyond the ground truth labels already provided in the dataset. For this, Hinton [30], introduced the concept of \"softmax temperature\". As it grows, the probability distribution generated by the softmax function becomes softer, providing more information as to which classes 'T' found more like the predicted class. This is the \"dark knowledge\" embedded in the 'T' and transferred to 'S' in the distillation process. The distillation related work can be categorized as below:",
            "score": 0.6198821220391341,
            "section_title": "C. Knowledge Distillation",
            "char_start_offset": 8373,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 54
                },
                {
                    "start": 57,
                    "end": 119
                },
                {
                    "start": 120,
                    "end": 153
                },
                {
                    "start": 156,
                    "end": 234
                },
                {
                    "start": 235,
                    "end": 370
                },
                {
                    "start": 371,
                    "end": 411
                },
                {
                    "start": 414,
                    "end": 509
                },
                {
                    "start": 510,
                    "end": 513
                },
                {
                    "start": 514,
                    "end": 615
                },
                {
                    "start": 616,
                    "end": 691
                },
                {
                    "start": 692,
                    "end": 768
                },
                {
                    "start": 769,
                    "end": 864
                },
                {
                    "start": 867,
                    "end": 961
                },
                {
                    "start": 962,
                    "end": 1150
                },
                {
                    "start": 1151,
                    "end": 1290
                },
                {
                    "start": 1291,
                    "end": 1458
                },
                {
                    "start": 1459,
                    "end": 1606
                },
                {
                    "start": 1607,
                    "end": 1716
                },
                {
                    "start": 1717,
                    "end": 1788
                },
                {
                    "start": 1789,
                    "end": 1968
                },
                {
                    "start": 1969,
                    "end": 2069
                },
                {
                    "start": 2070,
                    "end": 2128
                }
            ],
            "ref_mentions": [
                {
                    "start": 1077,
                    "end": 1081,
                    "matchedPaperCorpusId": "11536917"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.984375
        },
        {
            "corpus_id": "275117509",
            "title": "Leveraging logit uncertainty for better knowledge distillation",
            "text": "Knowledge distillation improves student model performance. However, using a larger teacher model does not necessarily result in better distillation gains due to significant architecture and output gaps with smaller student networks. To address this issue, we reconsider teacher outputs and find that categories with strong teacher confidence benefit distillation more, while those with weaker certainty contribute less. Thus, we propose Logits Uncertainty Distillation (LUD) to bridge this gap. We introduce category uncertainty weighting to consider the uncertainty in the teacher model\u2019s predictions. A confidence threshold, based on the teacher\u2019s predictions, helps construct a mask that discounts uncertain classes during distillation. Furthermore, we incorporate two Spearman correlation loss functions to align the logits of the teacher and student models. These loss functions measure the discrepancy between the models\u2019 outputs at the category and sample levels. We also introduce adaptive dynamic temperature factors to optimize the distillation process. By combining these techniques, we enhance knowledge distillation results and facilitate effective knowledge transfer between teacher and student models, even when architectural differences exist. Extensive experiments on multiple datasets demonstrate the effectiveness of our method.",
            "score": 0.6197050297760176,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.9013671875
        },
        {
            "corpus_id": "268820185",
            "title": "A Comprehensive Review of Knowledge Distillation in Computer Vision",
            "text": "A knowledge distillation approach called online distillation teaches a smaller student model to mimic the actions of a larger teacher model in real-time.The teacher and student models are jointly trained on a smaller dataset during an online training procedure [59].Updates to the student model's parameters are frequently required throughout the online distillation process in response to input from the teacher model.This can be done using a variety of techniques, including knowledge distillation loss [59], attention transfer [60], or feature imitation [61].The objective is to improve the student model's performance on the given task while simultaneously teaching the student model to behave similarly to the teacher model.There are various advantages to online distillation over offline distillation.First, the input from the teacher model can help direct the learning process, which can improve the convergence speed of the student model.Second, it can assist the student model in learning from cases that would otherwise be impossible to capture in a huge dataset.Finally, because the teacher model may provide instruction even when the training data is minimal, it can reduce the amount of labelled training data required for training.While Online Knowledge Distillation (OKD) enhances models by leveraging differences between students and teachers, key issues remain unexplored, such as the impact of a large gap on student performance.In order to overcome these challenges, The authors in this paper [62] implement Switchable Online Knowledge Distillation (SwitOKD), which adjusts the distillation gap during training rather than taking into account accuracy differences during testing.\n\nUsing a switching strategy, SwitchOKD [62] switches between expert and learning modes: the teacher pauses while the student continues to learn, and the teacher resumes when the student has finished learning.As a result, an adaptive switching threshold determines when to switch modes, optimizing the distillation gap and enhancing student performance.Teachers also benefit from this method, since their performance is comparable to those obtained by other online methods.The Knowledge Distillation (KD) online system aims to train multiple student models collaboratively, enabling them to distill knowledge from each other's experiences.Although current methods focus on class probabilities, they often overlook valuable feature representational information.",
            "score": 0.6196688892726093,
            "section_title": "Online distillation",
            "char_start_offset": 20830,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 153
                },
                {
                    "start": 153,
                    "end": 266
                },
                {
                    "start": 266,
                    "end": 419
                },
                {
                    "start": 419,
                    "end": 562
                },
                {
                    "start": 562,
                    "end": 729
                },
                {
                    "start": 729,
                    "end": 807
                },
                {
                    "start": 807,
                    "end": 946
                },
                {
                    "start": 946,
                    "end": 1073
                },
                {
                    "start": 1073,
                    "end": 1245
                },
                {
                    "start": 1245,
                    "end": 1447
                },
                {
                    "start": 1447,
                    "end": 1698
                },
                {
                    "start": 1700,
                    "end": 1907
                },
                {
                    "start": 1907,
                    "end": 2051
                },
                {
                    "start": 2051,
                    "end": 2171
                },
                {
                    "start": 2171,
                    "end": 2337
                },
                {
                    "start": 2337,
                    "end": 2458
                }
            ],
            "ref_mentions": [
                {
                    "start": 261,
                    "end": 265,
                    "matchedPaperCorpusId": "252198825"
                },
                {
                    "start": 505,
                    "end": 509,
                    "matchedPaperCorpusId": "252198825"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.8125
        },
        {
            "corpus_id": "257038997",
            "title": "HomoDistil: Homotopic Task-Agnostic Distillation of Pre-trained Transformers",
            "text": "Knowledge Distillation trains a small model (i.e., student model) to match the output predictions of a large and well-trained model (i.e., teacher model) by penalizing their output discrepancy. Specif-ically, we denote the teacher model as f t (\u03b8 t ) and the student model as f s (\u03b8 s ), and consider the following optimization problem: \n\nwhere D KL (\u03b8 s , \u03b8 t ) is the KL-Divergence between the probability distributions over their output predictions, i.e., KL(f s (\u03b8 s )||f t (\u03b8 t )). \n\nTransformer Distillation. In large Transformer-based models, distilling knowledge from only the output predictions neglects the rich semantic and syntactic knowledge in the intermediate layers. \n\nTo leverage such knowledge, researchers have further matched the hidden representations, attention scores and attention value relations at all layers of the teacher and the student (Romero et al., 2014;Sun et al., 2019;2020;Jiao et al., 2019;Hou et al., 2020;Wang et al., 2020b;a).",
            "score": 0.6190698909406883,
            "section_title": "TRANSFORMER DISTILLATION",
            "char_start_offset": 5260,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 193
                },
                {
                    "start": 194,
                    "end": 336
                },
                {
                    "start": 339,
                    "end": 486
                },
                {
                    "start": 489,
                    "end": 514
                },
                {
                    "start": 515,
                    "end": 682
                },
                {
                    "start": 685,
                    "end": 966
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.9560546875
        },
        {
            "corpus_id": "244488325",
            "title": "Semi-Online Knowledge Distillation",
            "text": "Knowledge distillation is an effective and stable method for model compression via knowledge transfer. Conventional knowledge distillation (KD) is to transfer knowledge from a large and well pre-trained teacher network to a small student network, which is a one-way process. Recently, deep mutual learning (DML) has been proposed to help student networks learn collaboratively and simultaneously. However, to the best of our knowledge, KD and DML have never been jointly explored in a unified framework to solve the knowledge distillation problem. In this paper, we investigate that the teacher model supports more trustworthy supervision signals in KD, while the student captures more similar behaviors from the teacher in DML. Based on these observations, we first propose to combine KD with DML in a unified framework. Furthermore, we propose a Semi-Online Knowledge Distillation (SOKD) method that effectively improves the performance of the student and the teacher. In this method, we introduce the peer-teaching training fashion in DML in order to alleviate the student's imitation difficulty, and also leverage the supervision signals provided by the well-trained teacher in KD. Besides, we also show our framework can be easily extended to feature-based distillation methods. Extensive experiments on CIFAR-100 and ImageNet datasets demonstrate the proposed method achieves state-of-the-art performance.",
            "score": 0.6189571569114636,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.97607421875
        },
        {
            "corpus_id": "271244914",
            "title": "Relational Representation Distillation",
            "text": "The concept of Knowledge Distillation (KD) was first introduced by Hinton et al. [20]. It involves extracting \"dark knowledge\" from accurate teacher models to guide the learning process of student models. This is achieved by utilizing the Kullback-Leibler (KL) loss to regularize the output probabilities of student models, aligning them with those of their teacher models when given the same inputs. This simple yet effective approach significantly improves the generalization ability of smaller models and finds extensive applications in various domains. Since the initial success of KD [20], several advanced methods, including logit distillation [21,35,52] and feature distillation [42,47,53,55], have been introduced. \n\nLogit distillation. Earlier methods on logit-based distillation primarily focused on improving student learning by directly mimicking the teacher's output probabilities. Examples included hierarchical supervision using intermediary teacher networks [52], multi-step student training to 2 Average relative improvement is calculated as: \n\n, where Acc i RRD , Acc i KD , and Acc i van represent the accuracies of RRD, KD, and vanilla training of the i-th student model, respectively [47]. \n\nenhance compatibility [35], collaborative learning among multiple students to improve generalization [58] and mechanisms that separately handle different types of logit information [59]. Recent advancements have sought to refine the quality of knowledge transfer. Some methods modify the distillation target: label decoupling [61] separately processes hard and soft labels, while instance-specific label smoothing [54] adapts the smoothing factor per example. Additional approaches focus on refining probability distributions, including probability reweighting to emphasize important outputs [37] and logit normalization to mitigate overconfidence [46]. Other methods include dynamic temperature scaling to adjust teacher-student similarity [25], specialized transformations to align teacher-student logits more effectively [60], and approaches that adapt teacher logits to better fit weaker students [21]. \n\nFeature distillation. Earlier methods on feature-based distillation emphasized utilizing intermediate feature representations to facilitate learning.",
            "score": 0.6183665987152606,
            "section_title": "Related Work",
            "char_start_offset": 4321,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 86
                },
                {
                    "start": 87,
                    "end": 204
                },
                {
                    "start": 205,
                    "end": 400
                },
                {
                    "start": 401,
                    "end": 556
                },
                {
                    "start": 557,
                    "end": 722
                },
                {
                    "start": 725,
                    "end": 744
                },
                {
                    "start": 745,
                    "end": 894
                },
                {
                    "start": 895,
                    "end": 1059
                },
                {
                    "start": 1062,
                    "end": 1210
                },
                {
                    "start": 1213,
                    "end": 1399
                },
                {
                    "start": 1400,
                    "end": 1476
                },
                {
                    "start": 1477,
                    "end": 1672
                },
                {
                    "start": 1673,
                    "end": 1866
                },
                {
                    "start": 1867,
                    "end": 2119
                },
                {
                    "start": 2122,
                    "end": 2143
                },
                {
                    "start": 2144,
                    "end": 2271
                }
            ],
            "ref_mentions": [
                {
                    "start": 686,
                    "end": 690,
                    "matchedPaperCorpusId": "2723173"
                },
                {
                    "start": 693,
                    "end": 696,
                    "matchedPaperCorpusId": "206596723"
                },
                {
                    "start": 696,
                    "end": 699,
                    "matchedPaperCorpusId": "829159"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.9697265625
        },
        {
            "corpus_id": "270560767",
            "title": "Linkage on Security, Privacy and Fairness in Federated Learning: New Balances and New Perspectives",
            "text": "Knowledge distillation is a methodology aimed at addressing the challenge of compressing either a large model or multiple models into a more compact representation, all while preserving the performance levels of the original models.The approach hinges on a teacher-student training framework, where a trained teacher model imparts its knowledge, and a student model assimilates this knowledge through a process known as distillation training.This transfer of knowledge from a complex teacher model to a simplified student model typically incurs only a marginal performance loss [59].Various distillation techniques for strengthening privacy have been devised specifically for federated learning frameworks.",
            "score": 0.6181932547474186,
            "section_title": "Knowledge distillation.",
            "char_start_offset": 15241,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 232
                },
                {
                    "start": 232,
                    "end": 442
                },
                {
                    "start": 442,
                    "end": 583
                },
                {
                    "start": 583,
                    "end": 706
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.7099609375
        },
        {
            "corpus_id": "255125462",
            "title": "Prototype-guided Cross-task Knowledge Distillation for Large-scale Models",
            "text": "Knowledge distillation is a model compression technology that transfers the knowledge from a larger deep neural network into a small network. \n\nThe methods of knowledge distillation are mainly divided into response-based knowledge distillation [17], [24], featurebased knowledge distillation [22], [23], and relation-based knowledge distillation [25], [26]. \n\nThe main idea of response-based knowledge distillation is to directly transfer the last output layer neural response of the teacher model. Hinton et al. [17] and Ba et al. [37] propose to shift the knowledge by learning the probabilities distribution via softened labels. However, this method depends on the class probability distribution. The effective method to this is to distill the feature-based or the relation-based knowledge from the teacher model. The goal of feature-based knowledge distillation is to match the intermediate representation of the student model with the teacher model. Fitnets [22] initially introduce intermediate representations learning, in which hints are defined as the outputs of a teacher's hidden layer to improve the student's learning process. Inspired by [22], a variety of feature-based knowledge distillation methods [38]- [40] are proposed. \n\nRelation-based knowledge distillation methods explore the relationships between different layers [25] or data samples [26]. \n\nKD has also been extensively studied in Transformer-based language models [31]- [33]. While previous knowledge distillation methods for Transformer-based models mainly focus on the NLP domain and the task of the teacher model is the same as the student model. Ye et al. [34] deal with a scenario distilling the knowledge from a cross-task teacher. However, it lags in the representation of the invariant intrinsic object and is a two-stage distillation method Different from existing works, in this paper, we explore the scenario of learning the intrinsic local-level features and reusing the knowledge of large-scale models for different downstream tasks in a cross-task manner.",
            "score": 0.6179391295043728,
            "section_title": "B. Knowledge Distillation",
            "char_start_offset": 7561,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 141
                },
                {
                    "start": 144,
                    "end": 357
                },
                {
                    "start": 360,
                    "end": 498
                },
                {
                    "start": 499,
                    "end": 631
                },
                {
                    "start": 632,
                    "end": 699
                },
                {
                    "start": 700,
                    "end": 816
                },
                {
                    "start": 817,
                    "end": 954
                },
                {
                    "start": 955,
                    "end": 1139
                },
                {
                    "start": 1140,
                    "end": 1240
                },
                {
                    "start": 1243,
                    "end": 1366
                },
                {
                    "start": 1369,
                    "end": 1454
                },
                {
                    "start": 1455,
                    "end": 1628
                },
                {
                    "start": 1629,
                    "end": 1716
                },
                {
                    "start": 1717,
                    "end": 2048
                }
            ],
            "ref_mentions": [
                {
                    "start": 250,
                    "end": 254,
                    "matchedPaperCorpusId": "174802983"
                },
                {
                    "start": 292,
                    "end": 296,
                    "matchedPaperCorpusId": "2723173"
                },
                {
                    "start": 298,
                    "end": 302,
                    "matchedPaperCorpusId": "30307744"
                },
                {
                    "start": 346,
                    "end": 350,
                    "matchedPaperCorpusId": "206596723"
                },
                {
                    "start": 352,
                    "end": 356,
                    "matchedPaperCorpusId": "131765296"
                },
                {
                    "start": 532,
                    "end": 536,
                    "matchedPaperCorpusId": "11536917"
                },
                {
                    "start": 963,
                    "end": 967,
                    "matchedPaperCorpusId": "2723173"
                },
                {
                    "start": 1152,
                    "end": 1156,
                    "matchedPaperCorpusId": "2723173"
                },
                {
                    "start": 1216,
                    "end": 1220,
                    "matchedPaperCorpusId": "829159"
                },
                {
                    "start": 1222,
                    "end": 1226,
                    "matchedPaperCorpusId": "227335337"
                },
                {
                    "start": 1340,
                    "end": 1344,
                    "matchedPaperCorpusId": "206596723"
                },
                {
                    "start": 1361,
                    "end": 1365,
                    "matchedPaperCorpusId": "131765296"
                },
                {
                    "start": 1449,
                    "end": 1453,
                    "matchedPaperCorpusId": "202719327"
                },
                {
                    "start": 1639,
                    "end": 1643,
                    "matchedPaperCorpusId": "219965004"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.99072265625
        },
        {
            "corpus_id": "256274710",
            "title": "Improving Text-based Early Prediction by Distillation from Privileged Time-Series Text",
            "text": "Knowledge distillation In knowledge distillation (KD), a more performant teacher model guides a smaller student model to achieve better results by matching the distributions of their predictions or output logits (Hinton et al., 2015). By training with the teacher output, the student model is provided with soft targets that contain more nuanced information about the label distribution compared to the true, hard labels. KD has been widely used for model compression (Sanh et al., 2019;Tung and Mori, 2019;Jiao et al., 2020) and other machine learning applications (Furlanello et al., 2018;Clark et al., 2019) to transfer knowledge across models with different strengths, sizes, or even architectures. In contrast, the classic transfer learning focuses on a single model and transfers knowledge across datasets, often from larger datasets to smaller ones (Devlin et al., 2019). Early works have shown the connection between LuPI and KD, unified them under generalized distillation (Lopez-Paz et al., 2016). Distillation has become a standard implementating technique to leverage privileged information (Hayashi et al., 2019;Xu et al., 2020).",
            "score": 0.6176537953886381,
            "section_title": "Related Work",
            "char_start_offset": 6057,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 234
                },
                {
                    "start": 235,
                    "end": 421
                },
                {
                    "start": 422,
                    "end": 702
                },
                {
                    "start": 703,
                    "end": 878
                },
                {
                    "start": 879,
                    "end": 1007
                },
                {
                    "start": 1008,
                    "end": 1142
                }
            ],
            "ref_mentions": [
                {
                    "start": 507,
                    "end": 525,
                    "matchedPaperCorpusId": "202719327"
                },
                {
                    "start": 566,
                    "end": 591,
                    "matchedPaperCorpusId": "4110009"
                },
                {
                    "start": 591,
                    "end": 610,
                    "matchedPaperCorpusId": "85464175"
                },
                {
                    "start": 856,
                    "end": 877,
                    "matchedPaperCorpusId": "52967399"
                },
                {
                    "start": 982,
                    "end": 1006,
                    "matchedPaperCorpusId": "8125776"
                },
                {
                    "start": 1103,
                    "end": 1125,
                    "matchedPaperCorpusId": "203605465"
                },
                {
                    "start": 1125,
                    "end": 1141,
                    "matchedPaperCorpusId": "213981455"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.9619140625
        },
        {
            "corpus_id": "272968875",
            "title": "Student-Oriented Teacher Knowledge Refinement for Knowledge Distillation",
            "text": "Following the original work of knowledge distillation [15], a series of studies [17,31,59,60] improves the representation of logits. These logits-based methods transfer knowledge by minimizing the Kullback-Leibler divergence between the predicted logits of teachers and students. The feature-based distillation methods [36] use features from intermediate layers. They have garnered more attention, as the higher-level logits-based methods lack structural information. However, a substantial gap between the teacher and student prevents the latter from fully acquiring the comprehensive knowledge of the former. Studies by [6,12,17,22] promote the student to learn knowledge as accurately as possible through more comprehensive and stringent constraints. In addition, many works [19,31,47] use a progressive distillation paradigm to avoid direct distillation when the gap between the teacher and student networks is large. Other methods [41,43,50] improve the transfer of knowledge to the student by refining the constraints. While [10,29] recognized that teacher knowledge might not suit the student, they searched student architectures adaptable to teacher knowledge from the student's perspective. However, searching for student network architectures is time-consuming and often yields architecture unfriendly to edge devices. The overall framework of SoKD comprises two key components: 1) DAFA, a differentiable module for augmenting feature strategy search. This module adapts strategies during training, aiming to uncover knowledge more suitable for the student network. 2) DAM, which identifies distinctive areas between the teacher and student networks. This module focuses on areas of mutual interest for knowledge transfer, thereby avoiding unnecessary knowledge distillation.",
            "score": 0.6172747291110197,
            "section_title": "Related Work 2.1 Knowledge Distillation",
            "char_start_offset": 6108,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 132
                },
                {
                    "start": 133,
                    "end": 279
                },
                {
                    "start": 280,
                    "end": 362
                },
                {
                    "start": 363,
                    "end": 467
                },
                {
                    "start": 468,
                    "end": 610
                },
                {
                    "start": 611,
                    "end": 753
                },
                {
                    "start": 754,
                    "end": 921
                },
                {
                    "start": 922,
                    "end": 1024
                },
                {
                    "start": 1025,
                    "end": 1199
                },
                {
                    "start": 1200,
                    "end": 1328
                },
                {
                    "start": 1329,
                    "end": 1461
                },
                {
                    "start": 1462,
                    "end": 1575
                },
                {
                    "start": 1576,
                    "end": 1660
                },
                {
                    "start": 1661,
                    "end": 1785
                }
            ],
            "ref_mentions": [
                {
                    "start": 80,
                    "end": 84,
                    "matchedPaperCorpusId": "260933721"
                },
                {
                    "start": 84,
                    "end": 87,
                    "matchedPaperCorpusId": "212908749"
                },
                {
                    "start": 87,
                    "end": 90,
                    "matchedPaperCorpusId": "26071966"
                },
                {
                    "start": 90,
                    "end": 93,
                    "matchedPaperCorpusId": "247476179"
                },
                {
                    "start": 622,
                    "end": 625,
                    "matchedPaperCorpusId": "233296935"
                },
                {
                    "start": 625,
                    "end": 628,
                    "matchedPaperCorpusId": "258341405"
                },
                {
                    "start": 628,
                    "end": 631,
                    "matchedPaperCorpusId": "260933721"
                },
                {
                    "start": 631,
                    "end": 634,
                    "matchedPaperCorpusId": "254069919"
                },
                {
                    "start": 778,
                    "end": 782,
                    "matchedPaperCorpusId": "233714221"
                },
                {
                    "start": 782,
                    "end": 785,
                    "matchedPaperCorpusId": "212908749"
                },
                {
                    "start": 785,
                    "end": 788,
                    "matchedPaperCorpusId": "51606880"
                },
                {
                    "start": 936,
                    "end": 940,
                    "matchedPaperCorpusId": "3603145"
                },
                {
                    "start": 943,
                    "end": 946,
                    "matchedPaperCorpusId": "248506080"
                },
                {
                    "start": 1031,
                    "end": 1035,
                    "matchedPaperCorpusId": "257771511"
                },
                {
                    "start": 1035,
                    "end": 1038,
                    "matchedPaperCorpusId": "208175624"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.8056640625
        },
        {
            "corpus_id": "235306374",
            "title": "Few-Shot Class-Incremental Learning via Relation Knowledge Distillation",
            "text": "Knowledge distillation, as a typical model compression and acceleration method, has attracted much attention from the community by learning small student models from large teacher models (Gou et al. 2020). Knowledge distillation can be divided into three categories according to 'knowledge'. (1) Logits-based knowledge utilizes the final output layer as supervision information to generate student network (Hinton, Vinyals, and Dean 2015;Guo et al. 2020). \n\n(2) Feature-based knowledge approaches: Feature layer as intermediate representations is used to train the student network (Romero et al. 2014;Zagoruyko and Komodakis 2016;Heo et al. 2019). ( 3) Relation-based knowledge further explores the structures between different layers and data samples (Yim et al. 2017;Park et al. 2019;Ramakrishnan et al. 2020). Compared with these works, we focus on a more difficult few-shot incremental learning problem where what knowledge to transfer and how to effectively transfer knowledge is equally important.",
            "score": 0.6168222078025557,
            "section_title": "Knowledge Distillation",
            "char_start_offset": 9241,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 205
                },
                {
                    "start": 206,
                    "end": 291
                },
                {
                    "start": 292,
                    "end": 455
                },
                {
                    "start": 458,
                    "end": 647
                },
                {
                    "start": 648,
                    "end": 812
                },
                {
                    "start": 813,
                    "end": 1003
                }
            ],
            "ref_mentions": [
                {
                    "start": 406,
                    "end": 438,
                    "matchedPaperCorpusId": "7200347"
                },
                {
                    "start": 601,
                    "end": 630,
                    "matchedPaperCorpusId": "829159"
                },
                {
                    "start": 630,
                    "end": 646,
                    "matchedPaperCorpusId": "53213211"
                },
                {
                    "start": 752,
                    "end": 769,
                    "matchedPaperCorpusId": "206596723"
                },
                {
                    "start": 769,
                    "end": 786,
                    "matchedPaperCorpusId": "131765296"
                },
                {
                    "start": 786,
                    "end": 811,
                    "matchedPaperCorpusId": "219107037"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.947265625
        },
        {
            "corpus_id": "265680820",
            "title": "Learn by Yourself: A Feature-Augmented Self-Distillation Convolutional Neural Network for Remote Sensing Scene Image Classification",
            "text": "In the process of knowledge distillation, a large teacher model is usually use training first, and then the teacher model is adopted to generate soft labels, which are together with the output of the student model to train the student model. During the ing process, different weights can be used to balance the relative importance of har soft objects. Choosing an appropriate teacher model is crucial to the effect of know distillation. In general, the teacher model should be complex and accurate enough to vide high-quality soft targets. A commonly used teacher model is a pre-trained deep ral network model. The student model is usually more lightweight and simplified the teacher model for deployment where computing resources are constrained. common student model design strategies include using shallow network structure reducing the number of network parameters. With the deepening of research, man proved and extended knowledge distillation methods have emerged. For exampl FitNets method [39] introduced the concept of intermediate layer alignment to alig intermediate layer outputs of the teacher model and the student model. The atte transfer method [40] learned knowledge from the teacher network by having the stu network imitate the attention map of the teacher network. The relational knowledg tillation method [41] exploited relational modeling to improve knowledge distillati comprehensive overhaul of the feature distillation method [42] adopted the feature lation, designed a new distillation loss, distilled features before the ReLU function retained negative values before distillation. Ahn et al. [43] proposed a variational mation distillation framework, which transfers the knowledge learned by Soft labels are the probability distributions output by the teacher model, which can provide richer information to help the student model learn. To transfer knowledge effectively, an appropriate loss function needs to be defined to measure the difference between the output of the student model and the output of the teacher model. Commonly used loss functions include the mean squared error [37], cross-entropy loss [37], and KL divergence [38]. The mean square error loss function measures the numerical difference of the output, while the cross-entropy loss function measures the difference in the probability distribution of the output. KL divergence (Kullback-Leibler divergence), also known as relative entropy, is an indicator used to measure the difference between two probability distributions.",
            "score": 0.6162970210336467,
            "section_title": "Knowledge Distillation",
            "char_start_offset": 16621,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 241
                },
                {
                    "start": 242,
                    "end": 351
                },
                {
                    "start": 352,
                    "end": 436
                },
                {
                    "start": 437,
                    "end": 539
                },
                {
                    "start": 540,
                    "end": 610
                },
                {
                    "start": 611,
                    "end": 747
                },
                {
                    "start": 748,
                    "end": 869
                },
                {
                    "start": 870,
                    "end": 970
                },
                {
                    "start": 971,
                    "end": 1135
                },
                {
                    "start": 1136,
                    "end": 1284
                },
                {
                    "start": 1285,
                    "end": 1607
                },
                {
                    "start": 1608,
                    "end": 1863
                },
                {
                    "start": 1864,
                    "end": 2050
                },
                {
                    "start": 2051,
                    "end": 2165
                },
                {
                    "start": 2166,
                    "end": 2359
                },
                {
                    "start": 2360,
                    "end": 2522
                }
            ],
            "ref_mentions": [
                {
                    "start": 1326,
                    "end": 1330,
                    "matchedPaperCorpusId": "131765296"
                },
                {
                    "start": 1451,
                    "end": 1455,
                    "matchedPaperCorpusId": "102483181"
                },
                {
                    "start": 1619,
                    "end": 1623,
                    "matchedPaperCorpusId": "118649278"
                },
                {
                    "start": 2160,
                    "end": 2164,
                    "matchedPaperCorpusId": "116908168"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.9658203125
        },
        {
            "corpus_id": "254926873",
            "title": "Joint Embedding of 2D and 3D Networks for Medical Image Anomaly Detection",
            "text": "Knowledge Distillation. Knowledge Distillation aims to transfer knowledge of pretrained huge networks to relatively light networks. It is firstly devised by Hinton et al [17], with the idea that wide and deep networks can extract the key point features well. Therefore, if the huge network, coined as teacher network, can distillate their feature extract performances to small networks, coined as student networks, it can be said as memory-efficient networks. Initially, knowledge distillation models use two loss function, student loss and distillation loss. Student loss refers that general cross entropy loss between student network's prediction results and labels. Distillation loss refers that comparing the results between teacher networks and student networks to distillate the teacher's knowledge to student [18]. Some paper use the knowledge distillation for model compression [19]. Knowledge distillation is viewed as function matching, and when knowledge distillation is applied with the function matching method proposed in this paper, the student reaches the teacher's accuracy. The accuracy of the teacher and the student is the same, but the model size of the student is small, so it can be thought of as a model compression method. By applying knowledge distillation to a model that people lightly use this latest model, we get a generalized model. Distillation for model compression highlights two methods. First, augmentation with the same input image of teacher and student should be applied. The second emphasizes the incredibly long learning times. According to the results of this paper [19], it can be confirmed that the performance of knowledge distillation takes a extremely long time to train. \n\nJoint Embedding. Multi-modal learning is a method of using data types with different characteristics rather than one type of data. The difficulty of learning multi-modal data is that they have different representations according to their modalities. For example, audio is a 1D signal wave form, an image can be represented as a 2D or 3D array and the text has embedding data corresponding to the word [20]. The method of learning embedding vectors to match different representations is called joint embedding. The most important aspect of cross-modality research is learning a common subspace in which different types of modalities can be directly compared.",
            "score": 0.6161897345747356,
            "section_title": "Representation Learning",
            "char_start_offset": 6502,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 23
                },
                {
                    "start": 24,
                    "end": 131
                },
                {
                    "start": 132,
                    "end": 258
                },
                {
                    "start": 259,
                    "end": 459
                },
                {
                    "start": 460,
                    "end": 559
                },
                {
                    "start": 560,
                    "end": 668
                },
                {
                    "start": 669,
                    "end": 821
                },
                {
                    "start": 822,
                    "end": 891
                },
                {
                    "start": 892,
                    "end": 1091
                },
                {
                    "start": 1092,
                    "end": 1247
                },
                {
                    "start": 1248,
                    "end": 1364
                },
                {
                    "start": 1365,
                    "end": 1423
                },
                {
                    "start": 1424,
                    "end": 1511
                },
                {
                    "start": 1512,
                    "end": 1569
                },
                {
                    "start": 1570,
                    "end": 1719
                },
                {
                    "start": 1722,
                    "end": 1738
                },
                {
                    "start": 1739,
                    "end": 1852
                },
                {
                    "start": 1853,
                    "end": 1971
                },
                {
                    "start": 1972,
                    "end": 2128
                },
                {
                    "start": 2129,
                    "end": 2231
                },
                {
                    "start": 2232,
                    "end": 2379
                }
            ],
            "ref_mentions": [
                {
                    "start": 2123,
                    "end": 2127,
                    "matchedPaperCorpusId": "239615993"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.97216796875
        },
        {
            "corpus_id": "258065609",
            "title": "Illumination Distillation Framework for Nighttime Person Re-Identification and a New Benchmark",
            "text": "Traditional knowledge distillation methods [49]- [52] are usually offline, which transfer a pre-trained large-scale teacher model to a small-scale student model. Current offline methods focus on different aspects of knowledge transfer, such as designing knowledge [49] and different loss functions [50]- [52]. Although these methods are simple and easy to implement, it requires a complex high-capacity teacher model, which costs a huge training time. To overcome the limitation of offline distillation, existing distillation techniques utilize different strategies to achieve online knowledge transfer between multiple models, which greatly facilitates the application of the technique in practical tasks. In the online distillation scheme, the teacher and student models are updated simultaneously in an end-to-end framework. For instance, Guo et al. [53] treat all networks as students and collaboratively train them in one stage to achieve knowledge transfer among arbitrary students. Chung et al. [54] design an online adversarial knowledge distillation method that uses discriminators to guide category probability and feature map distillation. Inspired by the one teacher vs. multiple students pattern in schools, Shen et al. [55] propose transferring knowledge from the teacher model to the student model, and between student models, which yields effective distillation results in the tracking task. In contrast to these approaches, we pursue learning different student models and then aggregating them into a teacher model.",
            "score": 0.6154663967796048,
            "section_title": "C. Knowledge Distillation Methods",
            "char_start_offset": 13259,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 161
                },
                {
                    "start": 162,
                    "end": 309
                },
                {
                    "start": 310,
                    "end": 451
                },
                {
                    "start": 452,
                    "end": 706
                },
                {
                    "start": 707,
                    "end": 827
                },
                {
                    "start": 828,
                    "end": 988
                },
                {
                    "start": 989,
                    "end": 1150
                },
                {
                    "start": 1151,
                    "end": 1407
                },
                {
                    "start": 1408,
                    "end": 1532
                }
            ],
            "ref_mentions": [
                {
                    "start": 49,
                    "end": 53,
                    "matchedPaperCorpusId": "21679091"
                },
                {
                    "start": 304,
                    "end": 308,
                    "matchedPaperCorpusId": "21679091"
                },
                {
                    "start": 853,
                    "end": 857,
                    "matchedPaperCorpusId": "219965421"
                },
                {
                    "start": 1002,
                    "end": 1006,
                    "matchedPaperCorpusId": "209319166"
                },
                {
                    "start": 1233,
                    "end": 1237,
                    "matchedPaperCorpusId": "244040871"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.8271484375
        },
        {
            "corpus_id": "231662091",
            "title": "Collaborative Teacher-Student Learning via Multiple Knowledge Transfer",
            "text": "However, most existing KD methods only consider either knowledge from individual instance features to maintain instance consistency between teacher and student or knowledge from instance relations to preserve the instance correlation consistency. There are a few works that consider more than one kind of knowledge in knowledge distillation [37,14] at the same time and explore the efficacy of each kind of knowledge. \n\nTransferring different types of knowledge can be implemented with different distillation methods, e.g., offline distillation, online distillation and self-distillation [1]. Most of the KD methods employ offline distillation, which is one-way knowledge transfer from a pre-trained large teacher to a small student [20,22,13]. In offline distillation, the capacity gap caused by a fixed teacher-student architecture and the requirement of a large dataset for pre-training the teacher often result in a degraded performance [22]. Thus, finding a proper teacher-student architecture in offline distillation is challenging. In contrast, online distillation provides a one-phase end-to-end training scheme via teacher-student collaborative learning on a peer-network architecture instead of a fixed one [25,33,36,32,28,12]. \n\nSelf-distillation performs online distillation within the same network to reduce model over-fitting [23,24]. Online distillation and self-distillation are promising methods for knowledge distillation as they bridge the capacity gap via avoiding the need of a large teacher network, leading to an improved performance. However, both KD methods used individually are limited to knowledge distillation from a single source, i.e., individual instances, online distillation could further suffer from the poor instance consistency between peer networks caused by the discrepancy in their network outputs. \n\nConsequently, it is desirable to have a unified framework that can integrate the advantages of different KD methods and make efficient use of different types of knowledge.",
            "score": 0.6154566921844478,
            "section_title": "Introduction",
            "char_start_offset": 2036,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 246
                },
                {
                    "start": 247,
                    "end": 417
                },
                {
                    "start": 420,
                    "end": 592
                },
                {
                    "start": 593,
                    "end": 744
                },
                {
                    "start": 745,
                    "end": 946
                },
                {
                    "start": 947,
                    "end": 1038
                },
                {
                    "start": 1039,
                    "end": 1237
                },
                {
                    "start": 1240,
                    "end": 1348
                },
                {
                    "start": 1349,
                    "end": 1557
                },
                {
                    "start": 1558,
                    "end": 1838
                },
                {
                    "start": 1841,
                    "end": 2012
                }
            ],
            "ref_mentions": [
                {
                    "start": 341,
                    "end": 345,
                    "matchedPaperCorpusId": "26021416"
                },
                {
                    "start": 345,
                    "end": 348,
                    "matchedPaperCorpusId": "201107180"
                },
                {
                    "start": 1217,
                    "end": 1221,
                    "matchedPaperCorpusId": "26071966"
                },
                {
                    "start": 1221,
                    "end": 1224,
                    "matchedPaperCorpusId": "27384186"
                },
                {
                    "start": 1227,
                    "end": 1230,
                    "matchedPaperCorpusId": "125950115"
                },
                {
                    "start": 1230,
                    "end": 1233,
                    "matchedPaperCorpusId": "48352434"
                },
                {
                    "start": 1233,
                    "end": 1236,
                    "matchedPaperCorpusId": "226841849"
                },
                {
                    "start": 1340,
                    "end": 1344,
                    "matchedPaperCorpusId": "219962714"
                },
                {
                    "start": 1344,
                    "end": 1347,
                    "matchedPaperCorpusId": "159041406"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.8447265625
        },
        {
            "corpus_id": "243861089",
            "title": "A Survey on Green Deep Learning",
            "text": "The idea of knowledge distillation (KD) is exploiting the knowledge inside a large trained \"teacher\" model to help the training of a \"student\" model (Bucila et al., 2006;Ba & Caruana, 2014;Hinton et al., 2015). In this way, we can use a smaller student model to distill a trained model as a replacement for inference.\n\nThe traditional KD solution is to minimize the difference between the output produced by the teacher model and that produced by the student model. Formally, given a labeled dataset D of N samples D = {(x 1 , y 1 ) , . . . , (x N , y N )}, we can write the loss function of the student network during the process of knowledge distillation as follows:\n\nwhere \u03b1 is a hyper-parameter to control the relative importance of the two terms; \u03b8 T and \u03b8 S are the parameters of teacher T and student S, respectively. L T refers to the task-specific loss and L KD refers to the knowledge distillation loss which measures the similarity of the student and the teacher.\n\nIn general, KD exploits the knowledge from the teacher model to help train the student model by minimizing the discrepancy between the knowledge in the teacher model and that in the student model. According to the source of teacher knowledge, we can classify KD approaches into three categories: logits-based KD, feature-based KD, and relation-based KD. According to teacher types, we also can classify KD approaches into three categories: KD with static teacher, KD with multiple teachers, and KD with dynamic teacher.\n\nLogits-based KD focuses on the output class distribution of the teacher model, also referred as \"soft labels\". This is the vanilla form of knowledge distillation (Hinton et al., 2015;Ba & Caruana, 2014). Soft targets generated by the teacher model provide much more information than hard targets. Therefore, training the student model to fit soft targets can help the student model generalize better like the teacher model.\n\nFeature-based KD exploits intermediate features to teach the student model, which is believed to be important for representation learning (Bengio et al., 2013a). The simplest solution is to minimize",
            "score": 0.6153421276992278,
            "section_title": "Knowledge Distillation",
            "char_start_offset": 86515,
            "sentence_offsets": [],
            "ref_mentions": [
                {
                    "start": 149,
                    "end": 170,
                    "matchedPaperCorpusId": "11253972"
                },
                {
                    "start": 170,
                    "end": 189,
                    "matchedPaperCorpusId": "11536917"
                },
                {
                    "start": 1680,
                    "end": 1699,
                    "matchedPaperCorpusId": "11536917"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.9765625
        },
        {
            "corpus_id": "270123290",
            "title": "Relation Modeling and Distillation for Learning with Noisy Labels",
            "text": "Knowledge distillation aims to train smaller, lightweight models (student networks) by leveraging supervised information from larger, high-performance models (teacher networks) to enhance their performance and accuracy.In this process, the supervised information obtained from the teacher network's outputs is referred to as \"knowledge,\" and the process of transferring this knowledge to the student network is termed \"distillation.\"For instance, Embedded Graph Alignment (EGA) [39] transfers knowledge from the teacher's network to the student's network by aligning the student's graph with the teacher's graph.A simple K-way projection method was used in [40] to transfer knowledge from various levels in the teacher's network to the student's network, distilling knowledge in this way.In DIST [41], it was pointed out that the prediction gap between teacher and student networks may be large, and the use of KL scatter will lead to a decrease in the training effect, and the method replaces KL scatter with Pearson correlation coefficient, which achieves some results.Knowledge transfer in CIRKD [42] using pixel-to-pixel and pixel-to-region distillation for matching with memory banks.",
            "score": 0.6145273403140168,
            "section_title": "Knowledge Distillation",
            "char_start_offset": 7462,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 219
                },
                {
                    "start": 219,
                    "end": 433
                },
                {
                    "start": 433,
                    "end": 612
                },
                {
                    "start": 612,
                    "end": 788
                },
                {
                    "start": 788,
                    "end": 1071
                },
                {
                    "start": 1071,
                    "end": 1189
                }
            ],
            "ref_mentions": [
                {
                    "start": 657,
                    "end": 661,
                    "matchedPaperCorpusId": "257313180"
                },
                {
                    "start": 796,
                    "end": 800,
                    "matchedPaperCorpusId": "248986690"
                },
                {
                    "start": 1099,
                    "end": 1103,
                    "matchedPaperCorpusId": "248178091"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.95654296875
        },
        {
            "corpus_id": "232076059",
            "title": "PURSUhInT: In Search of Informative Hint Points Based on Layer Clustering for Knowledge Distillation",
            "text": "Knowledge distillation (KD) is the process of transferring knowledge between networks, where one usually aims to transfer the knowledge of a big network (teacher) to a smaller/more compact network (student). KD is mostly known due to Hinton's work [16], while it was first proposed by [3]. The most well known form of KD uses the combination of soft targets produced by the teacher model and labels as the target in its objective function. The methods that uses soft targets without hints, are known as output (logit) distillation [14]. \n\nSoft targets are calculated as in (1), by passing training batches through teacher model and using softmax output layer with a higher temperature value (T > 1): \n\nwhere p, z and T show the softened class probability, logit and temperature values, respectively. For instance, i, j \u2208 {0, ..., 99} for a classification task with 100 classes, such as CIFAR-100. The same temperature value is used for generating both softened logits in teacher and student models, and higher T results in a softer class probability distribution. The loss function to train the student network is evaluated in (2) as follows: \n\nwhere \u03bb is the trade-off between logit distillation loss (L logit ) and classification loss (L cls ), y represents the label, p S and p T are the softened logits of student and teacher networks, respectively. Temperature is set to 1 in L cls . Cross-entropy and Kullback-Leibler losses are generaly used for L cls and L logit , respectively. A recent study [10] improves KD, where it introduces Spherical Knowledge Distillation (SKD) that projects all logits of the teacher and the student on a sphere by normalization. In SKD, logits are scaled into a unit vector and multiplied with the average teacher norm in order to recover its norm to the original level. [69] introduces a novel distillation approach by reformulating the classical logit distillation. Furthermore, [47] proposes a logit distillation approach in order to train a transformer student using lightweight teacher models with different architectures.",
            "score": 0.6144021180252159,
            "section_title": "Knowledge Distillation",
            "char_start_offset": 6036,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 207
                },
                {
                    "start": 208,
                    "end": 289
                },
                {
                    "start": 290,
                    "end": 439
                },
                {
                    "start": 440,
                    "end": 536
                },
                {
                    "start": 539,
                    "end": 699
                },
                {
                    "start": 702,
                    "end": 799
                },
                {
                    "start": 800,
                    "end": 896
                },
                {
                    "start": 897,
                    "end": 1063
                },
                {
                    "start": 1064,
                    "end": 1142
                },
                {
                    "start": 1145,
                    "end": 1353
                },
                {
                    "start": 1354,
                    "end": 1388
                },
                {
                    "start": 1389,
                    "end": 1486
                },
                {
                    "start": 1487,
                    "end": 1664
                },
                {
                    "start": 1665,
                    "end": 1806
                },
                {
                    "start": 1807,
                    "end": 1903
                },
                {
                    "start": 1904,
                    "end": 2063
                }
            ],
            "ref_mentions": [
                {
                    "start": 285,
                    "end": 288,
                    "matchedPaperCorpusId": "11253972"
                },
                {
                    "start": 531,
                    "end": 535,
                    "matchedPaperCorpusId": "102483181"
                },
                {
                    "start": 573,
                    "end": 576,
                    "matchedPaperCorpusId": "118649278"
                },
                {
                    "start": 1807,
                    "end": 1811,
                    "matchedPaperCorpusId": "247476179"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.97265625
        },
        {
            "corpus_id": "255125239",
            "title": "BD-KD: Balancing the Divergences for Online Knowledge Distillation",
            "text": "Knowledge distillation (KD) is a simple and yet powerful technique for model compression [7]. It has become an accessible tool, which facilitates the creation of compact student networks for deployment onto limited memory and computation edge devices. While distillation is traditionally performed offline, in recent years online methods have gained popularity for their improved performance and computational efficiency. Deep mutual learning [25]is one of the first techniques to propose collaborative learning of two or more networks This technique has opened an opportunity for distillation methods to reduce the training process to a single one-time simultaneous training of teacher and student. Each network is minimizing an objective function which consists of the traditional Cross-Entropy (CE) loss and a forward KL loss which encourages the network to mimic the other network's behaviour. Online Knowledge Distillation with Diverse Peers (OKDDip) is another online method proposed by Chen et al. [2]; a teacher-free two-level distillation method where the first-level happens among multiple students with identical architectures. The group knowledge of these diverse students is then transferred to the \"group leader\" in the second level. In a recent work by Chung et. al. [3] the distillation goes beyond class logit matching and is used to match the feature maps of two networks using an adversarial framework. \n\nNone of these online methods study the impact of a large capacity gap. Guo and colleagues were the first ones to show large gap harm the performance of the networks. To combat this phenomenon, they present Distillation Collaborative learning (KDCL) [6] where a series of ensembling methods is employed to generate high-quality soft targets at once for distillation. Although, KDCL improves the performance of both networks, due to the nature of the loss the focus is mostly on the larger network and therefore the improvement is more enhanced for the teacher. The first work that introduces an online distillation model that attempts to close the accuracy gap during training is Switchable Knowledge distillation model (SWITOKD) [15]. This model mitigates the accuracy gap problem by slowing down the training of the teacher.",
            "score": 0.6140460462975701,
            "section_title": "Related Work",
            "char_start_offset": 5468,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 93
                },
                {
                    "start": 94,
                    "end": 251
                },
                {
                    "start": 252,
                    "end": 421
                },
                {
                    "start": 422,
                    "end": 699
                },
                {
                    "start": 700,
                    "end": 897
                },
                {
                    "start": 898,
                    "end": 1138
                },
                {
                    "start": 1139,
                    "end": 1247
                },
                {
                    "start": 1248,
                    "end": 1277
                },
                {
                    "start": 1278,
                    "end": 1421
                },
                {
                    "start": 1424,
                    "end": 1494
                },
                {
                    "start": 1495,
                    "end": 1589
                },
                {
                    "start": 1590,
                    "end": 1789
                },
                {
                    "start": 1790,
                    "end": 1983
                },
                {
                    "start": 1984,
                    "end": 2158
                },
                {
                    "start": 2159,
                    "end": 2249
                }
            ],
            "ref_mentions": [
                {
                    "start": 443,
                    "end": 447,
                    "matchedPaperCorpusId": "26071966"
                },
                {
                    "start": 1282,
                    "end": 1285,
                    "matchedPaperCorpusId": "209319166"
                },
                {
                    "start": 1673,
                    "end": 1676,
                    "matchedPaperCorpusId": "219965421"
                },
                {
                    "start": 2153,
                    "end": 2157,
                    "matchedPaperCorpusId": "252198825"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.85693359375
        },
        {
            "corpus_id": "258341405",
            "title": "Multi-target Knowledge Distillation via Student Self-reflection",
            "text": "Recently, there has been many knowledge distillation mechanisms (Gou et al., 2021) that vary in either the distillation mechanisms (e.g., offline distillation, online distillation, and self-distillation) or the types of knowledge distilled (e.g., logits, feature knowledge, and relational knowledge). Specifically, offline distillation utilises a two-stage approach that requires the teacher model to be pre-trained as a prior and fixed for knowledge transfer (Liu et al., 2021;Zhao et al., 2022;Mirzadeh et al., 2020;Wu et al., 2020;Chen et al., 2022), while online and self-distillation instead argues that a pre-trained large teacher model is not always available for the tasks of interest, thus propose to update both the teacher model and the student model simultaneously in an end-toend manner (Yuan et al., 2020;Mobahi et al., 2020;Zhang et al., 2022;Zhao et al., 2021;Xu et al., 2022;Shen et al., 2022;Gou et al., 2022;Li et al., 2018;Wu & Gong, 2021;Zhang et al., 2018). \n\nLearning actually requires the two-way communication: it depends on not only the teacher's ability of unfolding the knowledge but also the capacity of the student in digesting the knowledge. To the best of our knowledge, existing KD methods mainly focus on how effective a teacher can be in knowledge distillation, ignoring the knowledge transfer from the student perspective, i.e., a student can learn different knowledge from both the teacher and self-reflection. Therefore, it is of great importance to pay attention to how a student can learn effectively from both the teacher and itself, where self-reflection allows a student to analyse, evaluate, and improve its own learning process by reviewing the learned knowledge after each stage of learning. As a simple yet effective way, self-distillation has shown to be very effective for self-reflection, for example, to accelerate the inference of large language model (Liu et al., 2020).",
            "score": 0.6132226244239195,
            "section_title": "Introduction",
            "char_start_offset": 1192,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 300
                },
                {
                    "start": 301,
                    "end": 979
                },
                {
                    "start": 982,
                    "end": 1172
                },
                {
                    "start": 1173,
                    "end": 1447
                },
                {
                    "start": 1448,
                    "end": 1737
                },
                {
                    "start": 1738,
                    "end": 1923
                }
            ],
            "ref_mentions": [
                {
                    "start": 64,
                    "end": 82,
                    "matchedPaperCorpusId": "219559263"
                },
                {
                    "start": 460,
                    "end": 478,
                    "matchedPaperCorpusId": "244101388"
                },
                {
                    "start": 478,
                    "end": 496,
                    "matchedPaperCorpusId": "247476179"
                },
                {
                    "start": 496,
                    "end": 518,
                    "matchedPaperCorpusId": "60440652"
                },
                {
                    "start": 800,
                    "end": 819,
                    "matchedPaperCorpusId": "219962714"
                },
                {
                    "start": 819,
                    "end": 839,
                    "matchedPaperCorpusId": "211096976"
                },
                {
                    "start": 858,
                    "end": 876,
                    "matchedPaperCorpusId": "236416669"
                },
                {
                    "start": 876,
                    "end": 892,
                    "matchedPaperCorpusId": "248986099"
                },
                {
                    "start": 892,
                    "end": 910,
                    "matchedPaperCorpusId": "247793209"
                },
                {
                    "start": 910,
                    "end": 927,
                    "matchedPaperCorpusId": "253020769"
                },
                {
                    "start": 927,
                    "end": 943,
                    "matchedPaperCorpusId": "236912875"
                },
                {
                    "start": 943,
                    "end": 959,
                    "matchedPaperCorpusId": "219531897"
                },
                {
                    "start": 959,
                    "end": 978,
                    "matchedPaperCorpusId": "26071966"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.82763671875
        },
        {
            "corpus_id": "201107180",
            "title": "Customizing Student Networks From Heterogeneous Teachers via Adaptive Knowledge Amalgamation",
            "text": "Knowledge distillation [8] adopts a teacher-guidingstudent strategy where a small student network learns to imitate the output of a large teacher network. In this way, the large teacher network can transfer knowledge to the student network with smaller model size, which is widely applied to model compression. Following [8], some works are proposed to exploit the intermediate representation to optimize the learning of student network, such as Fit-Net [26], DK 2 PNet [32], AT [36] and NST [13]. In summary, these works pay more attention on knowledge transfer among the same classification task. Transfer learning is proposed to transfer knowledge from source domain to target domain to save data on target domain [24]. It contains two main research directions: cross-domain transfer learning [22,12,10,4] and cross-task one [9,3,5,35]. In the case of cross-domain transfer learning, the dataset adopted by source domain and the counterpart of target domain are different in domain but the same in category. Also, crosstask transfer learning adopts the datasets that have the same domain but different categories. Transfer learning mainly focuses on compensating for the deficit of data on target domain with enough data on source domain. By contrast, our approach amalgamates multiple pre-trained models to obtain a multitalented model using unlabelled data.\n\nTo exploit knowledge of massive trained deep-learningbased models, researchers have made some promising attempts. MTZ [7] merges multiple correlated trained models by sharing neurons among these models for cross-model compression. Knowledge flow [14] transfers knowledge from multiple teacher models to student one with strategy that student learns to predict with the help of teachers, but gradually reduce the dependency on teachers, finally predict independently. Despite very promising solutions, the above approaches still depend on labelled dataset, which is not suitable for our application scenario where no human labels are available.\n\nThe approach of [28] proposes to transfer knowledge from multiple trained models into a single one in a layerwise manner with unlabelled dataset. It adopts an auto-  encoder architecture to amalgamate features from multiple single-task teachers. Several knowledge amalgamation methods are also proposed to handle the above task [33,23,34",
            "score": 0.6131054692958213,
            "section_title": "Related Work",
            "char_start_offset": 5423,
            "sentence_offsets": [],
            "ref_mentions": [
                {
                    "start": 23,
                    "end": 26,
                    "matchedPaperCorpusId": "7200347"
                },
                {
                    "start": 321,
                    "end": 324,
                    "matchedPaperCorpusId": "7200347"
                },
                {
                    "start": 454,
                    "end": 458,
                    "matchedPaperCorpusId": "2723173"
                },
                {
                    "start": 470,
                    "end": 474,
                    "matchedPaperCorpusId": "37879517"
                },
                {
                    "start": 479,
                    "end": 483,
                    "matchedPaperCorpusId": "829159"
                },
                {
                    "start": 717,
                    "end": 721,
                    "matchedPaperCorpusId": "740063"
                },
                {
                    "start": 796,
                    "end": 800,
                    "matchedPaperCorpusId": "13798326"
                },
                {
                    "start": 800,
                    "end": 803,
                    "matchedPaperCorpusId": "52956362"
                },
                {
                    "start": 803,
                    "end": 806,
                    "matchedPaperCorpusId": "1010081"
                },
                {
                    "start": 806,
                    "end": 808,
                    "matchedPaperCorpusId": "52453713"
                },
                {
                    "start": 828,
                    "end": 831,
                    "matchedPaperCorpusId": "225718"
                },
                {
                    "start": 831,
                    "end": 833,
                    "matchedPaperCorpusId": "43993788"
                },
                {
                    "start": 833,
                    "end": 835,
                    "matchedPaperCorpusId": "26286062"
                },
                {
                    "start": 835,
                    "end": 838,
                    "matchedPaperCorpusId": "26021416"
                },
                {
                    "start": 1482,
                    "end": 1485,
                    "matchedPaperCorpusId": "43975260"
                },
                {
                    "start": 1610,
                    "end": 1614,
                    "matchedPaperCorpusId": "69629714"
                },
                {
                    "start": 2025,
                    "end": 2029,
                    "matchedPaperCorpusId": "53231387"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.72412109375
        },
        {
            "corpus_id": "237940419",
            "title": "Partial to Whole Knowledge Distillation: Progressive Distilling Decomposed Knowledge Boosts Student Better",
            "text": "Knowledge Distillation (KD). KD (Hinton, Vinyals, and Dean 2015) has developed to present and mainly consists of two distillation schemes: offline distillation (Hinton, Vinyals, and Dean 2015;Romero et al. 2014;Huang and Wang 2017;Heo et al. 2019;Passalis and Tefas 2018) and online distillation (self-distillation is regarded as a special online distillation) (Zhang et al. 2018b;Anil et al. 2018;Zhang et al. 2019;Hou et al. 2019). Offline distillation is the earliest distillation scheme and it includes two sequential training phases: firstly, training a teacher model before distillation; secondly, the pre-trained teacher is used to guide the optimization of student. However, teacher with higher performance is not always available or training teacher costs large computation. Thus online distillation came into being. Online distillation has only one training phase, during which both teacher and student are optimized from scratch and updated simultaneously. To obtain decomposed knowledge, teacher must be pre-trained, and hence we mainly focus on the offline distillation scheme. \n\nFor offline distillation, it is inevitable to consider two questions: (i) what knowledge to distill? and (ii) how to distill knowledge? To answer the first question, there are three categories of knowledge from teacher: output logits (Hinton, Vinyals, and Dean 2015; Chen et al. 2017;Zhang, Zhu, and Ye 2019), intermediate feature map (Romero et al. 2014;Huang and Wang 2017;Passalis and Tefas 2018) and relation-based knowledge (Yim et al. 2017;Lee and Song 2019). Although the three types of knowledge represent different information, they share the same pattern that knowledge is obtained from the whole computation graph of teacher. There are few literatures that try to better distill knowledge. Opposed to the all-in distillation paradigm, we follow a similar rationale of curriculum distillation and propose a novel partial to whole knowledge distillation (PWKD) paradigm to answer the second question in this paper. \n\nAdaptive Neural Networks (ANNs).",
            "score": 0.6129792543230035,
            "section_title": "Related Work",
            "char_start_offset": 6024,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 28
                },
                {
                    "start": 29,
                    "end": 433
                },
                {
                    "start": 434,
                    "end": 673
                },
                {
                    "start": 674,
                    "end": 783
                },
                {
                    "start": 784,
                    "end": 825
                },
                {
                    "start": 826,
                    "end": 967
                },
                {
                    "start": 968,
                    "end": 1090
                },
                {
                    "start": 1093,
                    "end": 1228
                },
                {
                    "start": 1229,
                    "end": 1558
                },
                {
                    "start": 1559,
                    "end": 1729
                },
                {
                    "start": 1730,
                    "end": 1793
                },
                {
                    "start": 1794,
                    "end": 2016
                },
                {
                    "start": 2019,
                    "end": 2051
                }
            ],
            "ref_mentions": [
                {
                    "start": 211,
                    "end": 231,
                    "matchedPaperCorpusId": "5808102"
                },
                {
                    "start": 231,
                    "end": 247,
                    "matchedPaperCorpusId": "21679091"
                },
                {
                    "start": 247,
                    "end": 271,
                    "matchedPaperCorpusId": "52012952"
                },
                {
                    "start": 361,
                    "end": 381,
                    "matchedPaperCorpusId": "26071966"
                },
                {
                    "start": 416,
                    "end": 432,
                    "matchedPaperCorpusId": "199405591"
                },
                {
                    "start": 1360,
                    "end": 1377,
                    "matchedPaperCorpusId": "29308926"
                },
                {
                    "start": 1377,
                    "end": 1401,
                    "matchedPaperCorpusId": "53292120"
                },
                {
                    "start": 1448,
                    "end": 1468,
                    "matchedPaperCorpusId": "5808102"
                },
                {
                    "start": 1468,
                    "end": 1492,
                    "matchedPaperCorpusId": "52012952"
                },
                {
                    "start": 1522,
                    "end": 1539,
                    "matchedPaperCorpusId": "206596723"
                },
                {
                    "start": 1539,
                    "end": 1557,
                    "matchedPaperCorpusId": "195847947"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.96044921875
        },
        {
            "corpus_id": "230770298",
            "title": "Modality-specific Distillation",
            "text": "Knowledge Distillation. There have been several studies of transferring knowledge from one model to another (Ba and Caruana, 2014;Hinton et al., 2015;Romero et al., 2014;Park et al., 2019;M\u00fcller et al., 2020;Tian et al., 2019;Furlanello et al., 2018;Kim et al., 2020). Ba and Caruana (Ba and Caruana, 2014) improve the accuracy of a shallow neural network by training it to mimic a deep neural network with penalizing the difference of logits between the two networks. Hinton et al. (Hinton et al., 2015) introduced knowledge distillation (KD) that trains a student model with the objective of matching the softmax distribution of a teacher model at the output layer. Romero et al. (Romero et al., 2014) distill a teacher using additional linear projection layers and minimize L 2 loss at the earlier layers to train a students. Park et al. (Park et al., 2019) focused on mutual relations of data examples instead and they proposed relational knowledge distillation. The transfer works best when there are many possible classes because more information can be transferred, but in cases where there are only a few possible classes the transfer is less effective. To deal with the problem, M\u00fcller et al. (M\u00fcller et al., 2020) improved the transfer by forcing the teacher to divide each class into many subclasses. Tian et al. (Tian et al., 2019) proposed to distill from the penultimate layer using a contrastive loss for cross-modal transfer. A few recent papers (Furlanello et al., 2018;Kim et al., 2020) have shown that distilling a teacher model into a student model of identical architecture, i.e., self-distillation, can improve the student over the teacher. \n\nMeta Learning for Sample Weighting.",
            "score": 0.6128490554289681,
            "section_title": "Related Work",
            "char_start_offset": 24326,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 23
                },
                {
                    "start": 24,
                    "end": 268
                },
                {
                    "start": 269,
                    "end": 468
                },
                {
                    "start": 469,
                    "end": 667
                },
                {
                    "start": 668,
                    "end": 828
                },
                {
                    "start": 829,
                    "end": 966
                },
                {
                    "start": 967,
                    "end": 1161
                },
                {
                    "start": 1162,
                    "end": 1311
                },
                {
                    "start": 1312,
                    "end": 1441
                },
                {
                    "start": 1442,
                    "end": 1662
                },
                {
                    "start": 1665,
                    "end": 1700
                }
            ],
            "ref_mentions": [
                {
                    "start": 170,
                    "end": 188,
                    "matchedPaperCorpusId": "131765296"
                },
                {
                    "start": 841,
                    "end": 860,
                    "matchedPaperCorpusId": "131765296"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.9697265625
        },
        {
            "corpus_id": "267657497",
            "title": "Maximizing discrimination capability of knowledge distillation with energy function",
            "text": "Knowledge distillation (KD) is a technique used to enhance the performance of lightweight student networks by leveraging the dark knowledge embedded in large teacher networks. Over the years, KD methods have evolved to narrow the performance gap between student and teacher models by utilizing both final predictions, known as logits-based distillation [10,15,16,11,17], and intermediate features, known as features-based distillation [18,19,20,21,22,23,24,12,25,26,27,28]. \n\nPrevious works on logits-based distillations include the following: DML [15] proposed a mutual learning strategy for collaboratively teaching and learning between student and teacher models; TAKD [16] introduced a multi-step method with an intermediate-size network (i.e., assistant network) to bridge the gap between teachers and students; DKD [11] decomposed the soft-label distillation loss into two components: target class knowledge distillation (TCKD) and non-target class knowledge distillation (NCKD), enabling each part to independently harness its effectiveness; Multi KD [17] proposed multi-level prediction alignment, containing instance, batch, and class levels, and prediction augmentation. While these approaches emphasize effective knowledge transfer, they do not consider dividing entire datasets or provide mechanisms to distinguish and transfer knowledge from specific samples. \n\nFitNet [18] was groundbreaking as it leveraged not only the final outputs but also intermediate representations. Since the introduction of FitNet, various feature-based KD methods have emerged as follows: AT [22] prompted the student to mimic the attention map of the teacher network; PKT [19] employed various kernels to estimate the probability distributions, employing different divergence metrics for distillation; RKD [20] focused on transferring the mutual relations of data samples; CRD [21] framed the objective as contrastive learning for distillation; VID [23] took a different approach by maximizing mutual information; OFD [24] introduced a novel loss function incorporating teacher transform and a new distance function; Review KD [12] introduced a review mechanism that leverages past features for guiding current ones through residual learning.",
            "score": 0.6128478648924719,
            "section_title": "Knowledge Distillation",
            "char_start_offset": 3764,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 175
                },
                {
                    "start": 176,
                    "end": 473
                },
                {
                    "start": 476,
                    "end": 1180
                },
                {
                    "start": 1181,
                    "end": 1372
                },
                {
                    "start": 1375,
                    "end": 1487
                },
                {
                    "start": 1488,
                    "end": 2234
                }
            ],
            "ref_mentions": [
                {
                    "start": 363,
                    "end": 366,
                    "matchedPaperCorpusId": "247476179"
                },
                {
                    "start": 366,
                    "end": 369,
                    "matchedPaperCorpusId": "260933721"
                },
                {
                    "start": 439,
                    "end": 442,
                    "matchedPaperCorpusId": "219169868"
                },
                {
                    "start": 442,
                    "end": 445,
                    "matchedPaperCorpusId": "131765296"
                },
                {
                    "start": 451,
                    "end": 454,
                    "matchedPaperCorpusId": "118649278"
                },
                {
                    "start": 454,
                    "end": 457,
                    "matchedPaperCorpusId": "102483181"
                },
                {
                    "start": 457,
                    "end": 460,
                    "matchedPaperCorpusId": "233296935"
                },
                {
                    "start": 460,
                    "end": 463,
                    "matchedPaperCorpusId": "258298441"
                },
                {
                    "start": 463,
                    "end": 466,
                    "matchedPaperCorpusId": "258309453"
                },
                {
                    "start": 466,
                    "end": 469,
                    "matchedPaperCorpusId": "269167845"
                },
                {
                    "start": 469,
                    "end": 472,
                    "matchedPaperCorpusId": "269206209"
                },
                {
                    "start": 821,
                    "end": 825,
                    "matchedPaperCorpusId": "247476179"
                },
                {
                    "start": 1058,
                    "end": 1062,
                    "matchedPaperCorpusId": "260933721"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.9736328125
        },
        {
            "corpus_id": "258309753",
            "title": "Bayesian Optimization Meets Self-Distillation",
            "text": "Our framework is different from these methods in that these utilize prior knowledge from external sources to better estimate the surrogate model, while we utilize the prior knowledge from the given task to enhance the training of the target model. It means that these methods can be applied orthogonally to our framework. \n\nSelf-Distillation. Knowledge distillation (KD) is a model compression method that involves transferring the knowledge of a large teacher model to a small student model while maintaining performance. The original work by Hinton et al. [22] proposed distilling knowledge by matching the softmax distribution of the teacher and student models. Since then, various methods have been introduced to improve the knowledge transfer process. Self-Distillation (SD) is a special form of KD where the teacher and student networks have identical architecture. Born-Again Networks (BAN) [16] demonstrated that when training the student to match the output distribution of the teacher with the identical architecture, it could outperform the teacher. Furthermore, they showed that performing multiple rounds of BAN could further improve the performance where the trained student is set to be a new teacher in the following round. The ef-fectiveness of SD has been theoretically explained by the \"multi-view\" hypothesis introduced by Allen-Zhu and Li, who showed that self-distillation performs an implicit ensemble of various models [2]. Empirical evidence from Pham et al. [36] suggests that SD encourages the student to find flatter minima, leading to better generalization. In this work, we identify that SD can be an effective method for propagating the task knowledge learned in early stages of BO, to late stages of BO. This combination of the SD and BO processes is key to yielding a high-performing model, which we validate experimentally.",
            "score": 0.612801007706994,
            "section_title": "Related Work",
            "char_start_offset": 5089,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 247
                },
                {
                    "start": 248,
                    "end": 321
                },
                {
                    "start": 324,
                    "end": 342
                },
                {
                    "start": 343,
                    "end": 522
                },
                {
                    "start": 523,
                    "end": 664
                },
                {
                    "start": 665,
                    "end": 756
                },
                {
                    "start": 757,
                    "end": 871
                },
                {
                    "start": 872,
                    "end": 1060
                },
                {
                    "start": 1061,
                    "end": 1239
                },
                {
                    "start": 1240,
                    "end": 1447
                },
                {
                    "start": 1448,
                    "end": 1586
                },
                {
                    "start": 1587,
                    "end": 1735
                },
                {
                    "start": 1736,
                    "end": 1857
                }
            ],
            "ref_mentions": [
                {
                    "start": 558,
                    "end": 562,
                    "matchedPaperCorpusId": "7200347"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.96533203125
        },
        {
            "corpus_id": "219559263",
            "title": "Knowledge Distillation: A Survey",
            "text": "In knowledge distillation, a small student model is generally supervised by a large teacher model (Bucilua et al. 2006;Ba and Caruana 2014;Hinton et al. 2015;Urban et al. 2017). The main idea is that the student model mimics the teacher model in order to obtain a competitive or even a superior performance. \n\nThe key problem is how to transfer the knowledge from a large teacher model to a small student model. Basically, a knowledge distillation system is composed of three key components: knowledge, distillation algorithm, and teacherstudent architecture. A general teacher-student framework for knowledge distillation is shown in Fig. 1. \n\nAlthough the great success in practice, there are not too many works on either the theoretical or empirical understanding of knowledge distillation (Cheng et al. 2020;Phuong and Lampert 2019a;Cho and Hariharan 2019). Specifically, to understand the working mechanisms of knowledge distillation, Phuong & Lampert obtained a theoretical justification for a generalization bound with fast convergence of learning distilled student networks in the scenario of deep linear classifiers (Phuong and Lampert 2019a). This justification answers what and how fast the student learns and reveals the factors of determining the success of distillation. Successful distillation relies on data geometry, optimization bias of distillation objective and strong monotonicity of the student classifier. Cheng et al. quantified the extraction of visual concepts from the intermediate layers of a deep neural network, to explain knowledge distillation (Cheng et al. 2020). Ji and Zhu (2020) theoretically explained knowledge distillation on a wide neural network from the respective of risk bound, data efficiency and imperfect teacher. Cho & Hariharan empirically analyzed in detail the efficacy of knowledge distillation Fig. 2 The schematic structure of knowledge distillation and the relationship between the adjacent sections. The body of this survey mainly contains the fundamentals of knowledge distillation, knowledge types, distillation schemes, teacher-student architecture, distillation algo-rithms, performance comparison, applications, discussions, challenges, and future directions. Note that 'Section' is abbreviated as 'Sec.' in this figure (Cho and Hariharan 2019).",
            "score": 0.6125524835617523,
            "section_title": "Knowledge Transfer",
            "char_start_offset": 3333,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 177
                },
                {
                    "start": 178,
                    "end": 307
                },
                {
                    "start": 310,
                    "end": 411
                },
                {
                    "start": 412,
                    "end": 559
                },
                {
                    "start": 560,
                    "end": 642
                },
                {
                    "start": 645,
                    "end": 861
                },
                {
                    "start": 862,
                    "end": 1152
                },
                {
                    "start": 1153,
                    "end": 1284
                },
                {
                    "start": 1285,
                    "end": 1428
                },
                {
                    "start": 1429,
                    "end": 1596
                },
                {
                    "start": 1597,
                    "end": 1760
                },
                {
                    "start": 1761,
                    "end": 1955
                },
                {
                    "start": 1956,
                    "end": 2220
                },
                {
                    "start": 2221,
                    "end": 2306
                }
            ],
            "ref_mentions": [
                {
                    "start": 793,
                    "end": 812,
                    "matchedPaperCorpusId": "212633769"
                },
                {
                    "start": 812,
                    "end": 837,
                    "matchedPaperCorpusId": "174800711"
                },
                {
                    "start": 837,
                    "end": 860,
                    "matchedPaperCorpusId": "203642130"
                },
                {
                    "start": 1125,
                    "end": 1151,
                    "matchedPaperCorpusId": "174800711"
                },
                {
                    "start": 1576,
                    "end": 1595,
                    "matchedPaperCorpusId": "212633769"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.9443359375
        },
        {
            "corpus_id": "277769733",
            "title": "DDML: Multi-Student Knowledge Distillation for Hate Speech",
            "text": "Relation-based knowledge distillation focuses on capturing and transferring the relationships between feature maps in a neural network, such as correlations, graph representations, and similarity matrices. This approach aims to convey these structural relationships from the teacher model to the student model. Park et al. [29] introduced Relational Knowledge Distillation (RKD), which transfers mutual relationships between data examples. They proposed using distance and angle distillation losses to penalize structural differences in these relationships. Experiments across various tasks demonstrated that RKD significantly improves the performance of the student model. To leverage knowledge from multiple teacher models, Zhang and Peng [30] created two graphs, one of which uses the logits of each teacher model as nodes while the other uses the features. Through these graphs, their approach models the importance and relationships of different teachers prior to transferring knowledge. Lee et al. [31] developed a method for distilling dataset-based knowledge from a teacher model using an attention network. Their approach involves embedding the teacher model's knowledge into a graph via multi-head attention (MHA) and performing multitask learning to provide the student model with a relational inductive bias. However, relation-based distillation can be complex and computationally intensive, as it requires capturing and utilizing structured relationships between the outputs of the teacher model [32].",
            "score": 0.6124998408346634,
            "section_title": "Relation-Based Knowledge",
            "char_start_offset": 12284,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 205
                },
                {
                    "start": 206,
                    "end": 310
                },
                {
                    "start": 311,
                    "end": 439
                },
                {
                    "start": 440,
                    "end": 557
                },
                {
                    "start": 558,
                    "end": 673
                },
                {
                    "start": 674,
                    "end": 860
                },
                {
                    "start": 861,
                    "end": 992
                },
                {
                    "start": 993,
                    "end": 1115
                },
                {
                    "start": 1116,
                    "end": 1320
                },
                {
                    "start": 1321,
                    "end": 1514
                }
            ],
            "ref_mentions": [
                {
                    "start": 323,
                    "end": 327,
                    "matchedPaperCorpusId": "131765296"
                },
                {
                    "start": 1509,
                    "end": 1513,
                    "matchedPaperCorpusId": "271961991"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.93408203125
        },
        {
            "corpus_id": "262942062",
            "title": "Deep Model Fusion: A Survey",
            "text": "Knowledge distillation (KD) [83] is a significant method to ensemble multiple models, which involves the following two types of models. A teacher model denotes large and powerful model trained on large-scale data and has high predictive and expressive power. A student models is a relatively smaller model with fewer parameters and computational resource [18,199]. Using the knowledge of the teacher (e.g., the output probability distribution, hidden layer representation, etc.) to guide the training, the student could achieve the prediction ability closed to the large model with fewer resources and faster speed [2,119,124,221]. Given that multiple teachers or students are expected to have a preferable performance than a single model [6], we divide KD into two categories according to the aggregated objects as Figure 8. \n\nThe first type of approach is to merge multiple teacher models and distill the student model directly, as shown in Table 9. Currently, recent work mainly integrates the output of teachers (e.g., logits [6,49,252] or feature-based knowledge [143,241], etc.). Ensemble distillation (ED) [141,157] distills the average output of multiple teachers to a student model, which can make up for the shortcomings of a single teacher model and provide more diversified and comprehensive information to the student model. FedDF [141] distills a collection of client-teacher models |S t | into a server-student model. It averages the logit output f xk t of teachers as Eq.( 45): \n\nx t,j := x t,j\u22121 \u2212 \u03b7 \u2202KL \u03c3 1 |St| k\u2208St f xk t , \u03c3 (f (x t,j\u22121 )) \n\nwhere t is the communication round, KL means KL divergence. The ensemble part of FedDF does not affect the overall workflows of clients and solves the loss problem of network batch normalization (BN) [87], Wu et al. [241] propose a multi-teacher adaptive distillation framework that can transfer knowledge from multiple teachers to student without the need for source domain data.",
            "score": 0.6124965604218942,
            "section_title": "Distillation",
            "char_start_offset": 78672,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 135
                },
                {
                    "start": 136,
                    "end": 258
                },
                {
                    "start": 259,
                    "end": 364
                },
                {
                    "start": 365,
                    "end": 631
                },
                {
                    "start": 632,
                    "end": 825
                },
                {
                    "start": 828,
                    "end": 951
                },
                {
                    "start": 952,
                    "end": 1085
                },
                {
                    "start": 1086,
                    "end": 1337
                },
                {
                    "start": 1338,
                    "end": 1432
                },
                {
                    "start": 1433,
                    "end": 1493
                },
                {
                    "start": 1496,
                    "end": 1560
                },
                {
                    "start": 1563,
                    "end": 1622
                },
                {
                    "start": 1623,
                    "end": 1943
                }
            ],
            "ref_mentions": [
                {
                    "start": 355,
                    "end": 359,
                    "matchedPaperCorpusId": "11253972"
                },
                {
                    "start": 359,
                    "end": 363,
                    "matchedPaperCorpusId": "18271205"
                },
                {
                    "start": 615,
                    "end": 618,
                    "matchedPaperCorpusId": "118649278"
                },
                {
                    "start": 618,
                    "end": 622,
                    "matchedPaperCorpusId": "3608236"
                },
                {
                    "start": 622,
                    "end": 626,
                    "matchedPaperCorpusId": "172133986"
                },
                {
                    "start": 626,
                    "end": 630,
                    "matchedPaperCorpusId": "198179476"
                },
                {
                    "start": 1033,
                    "end": 1036,
                    "matchedPaperCorpusId": "85529118"
                },
                {
                    "start": 1036,
                    "end": 1040,
                    "matchedPaperCorpusId": "26021416"
                },
                {
                    "start": 1073,
                    "end": 1077,
                    "matchedPaperCorpusId": "195492859"
                },
                {
                    "start": 1113,
                    "end": 1118,
                    "matchedPaperCorpusId": "219636007"
                },
                {
                    "start": 1344,
                    "end": 1349,
                    "matchedPaperCorpusId": "219636007"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.95166015625
        },
        {
            "corpus_id": "271974282",
            "title": "LLaVA-MoD: Making LLaVA Tiny via MoE Knowledge Distillation",
            "text": "teacher. Interestingly, the smaller Qwen-1.5-0.5B student model shows marginal improvement with either the Qwen-1.5-4B or Qwen-1.5-7B teacher. This suggests that an excessively large capacity gap between the teacher and student models can hinder effective knowledge transfer. Utilizing a \"middle teacher\" with an intermediate capacity could bridge this gap, facilitating smoother knowledge transfer and boosting the student model's learning efficiency. \n\nTable 9: Comparison between the strong and weak teachers within the distillation. We set the strong teacher as the LLM is Qwen-1.5-7B and the weak teacher as the LLM is Qwen-1.5-4B. The model configuration is E4T2.",
            "score": 0.6122509218174647,
            "section_title": "IMPACT OF PREFERENCE DISTILLATION",
            "char_start_offset": 31825,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 8
                },
                {
                    "start": 9,
                    "end": 49
                },
                {
                    "start": 50,
                    "end": 118
                },
                {
                    "start": 119,
                    "end": 133
                },
                {
                    "start": 134,
                    "end": 142
                },
                {
                    "start": 143,
                    "end": 275
                },
                {
                    "start": 276,
                    "end": 452
                },
                {
                    "start": 455,
                    "end": 536
                },
                {
                    "start": 537,
                    "end": 588
                },
                {
                    "start": 589,
                    "end": 636
                },
                {
                    "start": 637,
                    "end": 669
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.2467041015625
        },
        {
            "corpus_id": "270870796",
            "title": "From Efficient Multimodal Models to World Models: A Survey",
            "text": "Knowledge Distillation (KD) is a model compression technique that transfers knowledge from a complex model (called the teacher model) to a smaller model (called the student model).This allows the student model to maintain high computational efficiency while achieving the performance of the teacher model.Knowledge distillation was first proposed by Bucilu\u01ce et al., who trained compressed models with pseudodata classifiers to replicate the original classifier's outputs [14].KD can be divided into homomorphic KD and heteromorphic KD.\n\nHomomorphic KD means the student and teacher models have similar or identical structures.In this approach, the student model learns by mimicking the teacher model's outputs (e.g., logits, feature layer outputs).Common homomorphic KD methods include logit-level distillation, feature-level distillation, and module-level distillation.For instance, TinyViT [46] applies distillation during pre-training, storing logits from a large teacher model on hardware to achieve memory and computational efficiency when transferring knowledge to a smaller student Transformer.DeiT-Tiny [47] adopts patchlevel distillation, training a small student model to match the pre-trained teacher model's patch structure, then optimizing with decomposed manifold matching loss to reduce computational costs.Module-level methods like m2mKD [48] separate the teacher module from a pre-trained unified model, combining student modules with modular models, and using a shared meta-model for composition, enabling student modules to mimic teacher module behavior.Feature-level distillation methods like MiniViT [49] combine weights from consecutive Transformer blocks for cross-layer weight sharing, introducing transformations to enhance learning.\n\nHeteromorphic KD refers to student and teacher models with different structures.In this approach, the student model learns by mimicking the teacher model's outputs or intermediate features, despite different architectures.Heteromorphic KD enhances the student model's adaptability, enabling it to learn useful information from the teacher model.Heteromorphic KD includes soft label distillation, where the student model trains by mimicking the teacher model's soft label outputs.",
            "score": 0.6117979809528357,
            "section_title": "C. Knowledge Distillation",
            "char_start_offset": 25157,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 180
                },
                {
                    "start": 180,
                    "end": 305
                },
                {
                    "start": 305,
                    "end": 476
                },
                {
                    "start": 476,
                    "end": 535
                },
                {
                    "start": 537,
                    "end": 626
                },
                {
                    "start": 626,
                    "end": 748
                },
                {
                    "start": 748,
                    "end": 870
                },
                {
                    "start": 870,
                    "end": 1101
                },
                {
                    "start": 1101,
                    "end": 1322
                },
                {
                    "start": 1322,
                    "end": 1573
                },
                {
                    "start": 1573,
                    "end": 1758
                },
                {
                    "start": 1760,
                    "end": 1840
                },
                {
                    "start": 1840,
                    "end": 1982
                },
                {
                    "start": 1982,
                    "end": 2105
                },
                {
                    "start": 2105,
                    "end": 2239
                }
            ],
            "ref_mentions": [
                {
                    "start": 471,
                    "end": 475,
                    "matchedPaperCorpusId": "219559263"
                },
                {
                    "start": 892,
                    "end": 896,
                    "matchedPaperCorpusId": "250920355"
                },
                {
                    "start": 1111,
                    "end": 1115,
                    "matchedPaperCorpusId": "247230104"
                },
                {
                    "start": 1621,
                    "end": 1625,
                    "matchedPaperCorpusId": "248177918"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.9814453125
        },
        {
            "corpus_id": "250526328",
            "title": "Large\u2010scale knowledge distillation with elastic heterogeneous computing resources",
            "text": "In this section, we present the background of elastic deep learning distillation, i.e., knowledge distillation. Then, we introduce the methods for distributed and decentralized training and elastic computing resources. \n\nModel compression based on knowledge transferring was first proposed in 7 in order to compress an ensemble of models or a large model to a compact model. The ensemble of models or the large model take much storage space and require a long time for inference while the compact model requires relatively small storage space and a short time for inference. Knowledge distillation is based on the popular machine learning Softmax function and a temperature 8 , which is defined in Formula 1. \n\n, where is the output from the output layer of a teacher neural network; is a temperature, which indicates the impact of the output from the teacher model. The Softmax output layer computes the probability that the input data corresponds to each class with the corresponding computed logit. The probability is related to a temperature T, i.e., which represents the impact of the distilled knowledge of the teacher model on the student model. A higher temperature corresponds to a weaker impact. As shown in Figure 1, during the training of knowledge distillation, two neural networks are used: teacher model and student model. The student model is trained based on the combination of two loss values. The first loss value is calculated from a soft prediction, which contains the probability of each class calculated from the teacher model. The soft prediction is calculated using Formula 1. The other loss value corresponds to a hard prediction, which is the ground truth label from the training data. \n\nTwo types of work are also related to this paper, i.e., mutual knowledge distillation and model pruning. When there is no available teacher model, the teacher and student can be trained at the same time, i.e., an ensemble of students can learn collaboratively and teach each other throughout the training process 16,17 , which is the mutual knowledge distillation 9 . Note that the mutual knowledge distillation is also denoted \"online knowledge distillation\" in 9 . However, we use the \"online knowledge distillation\" to represent the knowledge distillation with the teacher model and the student model deployed in the same GPU card during the training process of knowledge distillation in this paper.",
            "score": 0.6116170616945228,
            "section_title": "RELATED WORK",
            "char_start_offset": 3556,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 111
                },
                {
                    "start": 112,
                    "end": 218
                },
                {
                    "start": 221,
                    "end": 374
                },
                {
                    "start": 375,
                    "end": 574
                },
                {
                    "start": 575,
                    "end": 708
                },
                {
                    "start": 711,
                    "end": 866
                },
                {
                    "start": 867,
                    "end": 1001
                },
                {
                    "start": 1002,
                    "end": 1152
                },
                {
                    "start": 1153,
                    "end": 1205
                },
                {
                    "start": 1206,
                    "end": 1337
                },
                {
                    "start": 1338,
                    "end": 1411
                },
                {
                    "start": 1412,
                    "end": 1550
                },
                {
                    "start": 1551,
                    "end": 1601
                },
                {
                    "start": 1602,
                    "end": 1712
                },
                {
                    "start": 1715,
                    "end": 1819
                },
                {
                    "start": 1820,
                    "end": 2082
                },
                {
                    "start": 2083,
                    "end": 2181
                },
                {
                    "start": 2182,
                    "end": 2417
                }
            ],
            "ref_mentions": [
                {
                    "start": 293,
                    "end": 294,
                    "matchedPaperCorpusId": "11253972"
                },
                {
                    "start": 674,
                    "end": 675,
                    "matchedPaperCorpusId": "7200347"
                },
                {
                    "start": 2028,
                    "end": 2031,
                    "matchedPaperCorpusId": "26071966"
                },
                {
                    "start": 2031,
                    "end": 2033,
                    "matchedPaperCorpusId": "208526905"
                },
                {
                    "start": 2079,
                    "end": 2080,
                    "matchedPaperCorpusId": "219559263"
                },
                {
                    "start": 2178,
                    "end": 2179,
                    "matchedPaperCorpusId": "219559263"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.9775390625
        },
        {
            "corpus_id": "269664643",
            "title": "Few-Shot Federated Learning: A Federated Learning Model for Small-Sample Scenarios",
            "text": "The concept of knowledge distillation was first introduced by Hinton et al. in [22], comprising a system of teacher and student models [23][24][25][26]. The so-called teacher model refers to a large, structurally complex, and redundantly trained network that acts as the \"teacher\" during training, instructing a simpler, under-trained \"student\" model to achieve a performance as close as possible to that of the teacher model. This process is realized through the use of soft and hard labels along with corresponding loss functions. Specifically, it begins by calculating the soft output of the teacher model, which is the raw prediction values, or logits, from the last fully connected layer of the neural network-values that have not been activated by the softmax function. Using the probability distribution after applying the softmax function would limit the knowledge transferred to the student model and might even have a negative effect by amplifying errors. These prediction values represent the teacher model's confidence level for each class. \n\nHere, z T i denotes the logit output of the teacher model for the ith class, and T is the temperature parameter that controls the smoothness of the output probability distribution. It is evident that a smaller T value, which brings the function closer to the softmax function, makes q i more similar to the probability distribution after activation. Conversely, a larger T value results in a smoother probability distribution curve from the softmax output, allowing the student model to acquire knowledge with richer information entropy. \n\nAfter obtaining the logits from the teacher model, the student model's logits are calculated in a similar manner. Here, z S i represents the logit output of the student model for the ith class. \n\nSubsequently, a loss function is employed to enable the student model to absorb and compress the knowledge from the teacher model. \n\nHere, Q i and P i represent the probability distributions of the teacher and student models, respectively. The difference between these probability distributions is calculated using the Kullback-Leibler divergence, with the temperature parameter used to adjust it, ensuring that the gradients do not become too small as the temperature value increases. \n\nThroughout the knowledge distillation process, in addition to computing the soft target loss, it is also necessary to calculate the hard target loss.",
            "score": 0.6116133127290813,
            "section_title": "Knowledge Distillation",
            "char_start_offset": 9401,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 152
                },
                {
                    "start": 153,
                    "end": 426
                },
                {
                    "start": 427,
                    "end": 532
                },
                {
                    "start": 533,
                    "end": 775
                },
                {
                    "start": 776,
                    "end": 965
                },
                {
                    "start": 966,
                    "end": 1052
                },
                {
                    "start": 1055,
                    "end": 1235
                },
                {
                    "start": 1236,
                    "end": 1404
                },
                {
                    "start": 1405,
                    "end": 1592
                },
                {
                    "start": 1595,
                    "end": 1708
                },
                {
                    "start": 1709,
                    "end": 1788
                },
                {
                    "start": 1791,
                    "end": 1921
                },
                {
                    "start": 1924,
                    "end": 2030
                },
                {
                    "start": 2031,
                    "end": 2276
                },
                {
                    "start": 2279,
                    "end": 2428
                }
            ],
            "ref_mentions": [
                {
                    "start": 79,
                    "end": 83,
                    "matchedPaperCorpusId": "7200347"
                },
                {
                    "start": 135,
                    "end": 139,
                    "matchedPaperCorpusId": "235694435"
                },
                {
                    "start": 143,
                    "end": 147,
                    "matchedPaperCorpusId": "247779043"
                },
                {
                    "start": 147,
                    "end": 151,
                    "matchedPaperCorpusId": "266620794"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.955078125
        },
        {
            "corpus_id": "212908749",
            "title": "Improved Knowledge Distillation via Teacher Assistant",
            "text": "Despite the fact that deep neural networks are powerful models and achieve appealing results on many tasks, they are too large to be deployed on edge devices like smartphones or embedded sensor nodes. There have been efforts to compress these networks, and a popular method is knowledge distillation, where a large (teacher) pre-trained network is used to train a smaller (student) network. However, in this paper, we show that the student network performance degrades when the gap between student and teacher is large. Given a fixed student network, one cannot employ an arbitrarily large teacher, or in other words, a teacher can effectively transfer its knowledge to students up to a certain size, not smaller. To alleviate this shortcoming, we introduce multi-step knowledge distillation, which employs an intermediate-sized network (teacher assistant) to bridge the gap between the student and the teacher. Moreover, we study the effect of teacher assistant size and extend the framework to multi-step distillation. Theoretical analysis and extensive experiments on CIFAR-10,100 and ImageNet datasets and on CNN and ResNet architectures substantiate the effectiveness of our proposed approach.",
            "score": 0.6114393380876951,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.8662109375
        },
        {
            "corpus_id": "248496323",
            "title": "Preserve Pre-trained Knowledge: Transfer Learning With Self-Distillation For Action Recognition",
            "text": "Knowledge distillation was firstly proposed in [18] to transfer the knowledge from the cumbersome model to a small model. Self-distillation can be treated as a regularization strategy when the teacher model and the student model share the same architecture. For example, [19] first uses self-distillation, which transfers the knowledge in the deeper portion of the networks to the shallow sections. [20] improves representation quality of the smaller models by single-stage online knowledge distillation. [21] uses self-distillation between different epochs with soft targets. However, combining selfdistillation as the regularization for transfer learning is not well-exploited.",
            "score": 0.6114022624749172,
            "section_title": "Self-Distillation",
            "char_start_offset": 5815,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 121
                },
                {
                    "start": 122,
                    "end": 257
                },
                {
                    "start": 258,
                    "end": 398
                },
                {
                    "start": 399,
                    "end": 504
                },
                {
                    "start": 505,
                    "end": 576
                },
                {
                    "start": 577,
                    "end": 679
                }
            ],
            "ref_mentions": [
                {
                    "start": 271,
                    "end": 275,
                    "matchedPaperCorpusId": "159041406"
                },
                {
                    "start": 399,
                    "end": 403,
                    "matchedPaperCorpusId": "233307503"
                },
                {
                    "start": 505,
                    "end": 509,
                    "matchedPaperCorpusId": "233714221"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.91162109375
        },
        {
            "corpus_id": "235899214",
            "title": "Confidence Conditioned Knowledge Distillation",
            "text": "One of the earliest papers to appear in distillation literature was [3]. It proposed to use the original training data as the transfer set for distillation. In addition to learning from the ground truths, the student also receives supervision from the teacher model in the form of soft targets computed at a high softmax temperature. Subsequently, [11,9,19] proposed methods for knowledge transfer wherein the transfer set was not available. The softmax space of the teacher network is modeled by using a Dirichlet distribution in [9]. Synthetic data instances are then created by inverting the samples drawn from this distribution. The teacher model is used as a fixed discriminator in a generative adversarial framework in [11] to create synthetic data samples. Activation statistics of the teacher model are obtained during training and these are used to create synthetic data samples in [19]. Synthetic samples are created through feature inversion in [9,19]. These samples are then used for the knowledge distillation process. The process of distillation is the same as in [3]. A conditional distillation mechanism is proposed in [30]. It incorporates the fact that the teacher model can sometimes be wrong in its predictions and only in that case the student must learn from the ground truths.",
            "score": 0.6110308772737975,
            "section_title": "Response Based KD methods",
            "char_start_offset": 8202,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 72
                },
                {
                    "start": 73,
                    "end": 156
                },
                {
                    "start": 157,
                    "end": 333
                },
                {
                    "start": 334,
                    "end": 441
                },
                {
                    "start": 442,
                    "end": 535
                },
                {
                    "start": 536,
                    "end": 632
                },
                {
                    "start": 633,
                    "end": 763
                },
                {
                    "start": 764,
                    "end": 896
                },
                {
                    "start": 897,
                    "end": 963
                },
                {
                    "start": 964,
                    "end": 1031
                },
                {
                    "start": 1032,
                    "end": 1082
                },
                {
                    "start": 1083,
                    "end": 1140
                },
                {
                    "start": 1141,
                    "end": 1299
                }
            ],
            "ref_mentions": [
                {
                    "start": 68,
                    "end": 71,
                    "matchedPaperCorpusId": "7200347"
                },
                {
                    "start": 348,
                    "end": 352,
                    "matchedPaperCorpusId": "91183944"
                },
                {
                    "start": 352,
                    "end": 354,
                    "matchedPaperCorpusId": "159041346"
                },
                {
                    "start": 531,
                    "end": 534,
                    "matchedPaperCorpusId": "159041346"
                },
                {
                    "start": 725,
                    "end": 729,
                    "matchedPaperCorpusId": "91183944"
                },
                {
                    "start": 956,
                    "end": 959,
                    "matchedPaperCorpusId": "159041346"
                },
                {
                    "start": 1078,
                    "end": 1081,
                    "matchedPaperCorpusId": "7200347"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.880859375
        },
        {
            "corpus_id": "277724285",
            "title": "Condensation of Data and Knowledge for Network Traffic Classification: Techniques, Applications, and Open Issues",
            "text": "Knowledge distillation is a technique developed to address the challenges of deploying large deep neural networks, commonly used in fields like computer vision and natural language processing, on resource-constrained devices such as mobile phones, embedded systems, and autonomous vehicles. While methods like factorization, pruning, and the compression of convolutional filters can reduce redundancy and lower the storage and transmission cost of large models, these approaches still require the models to be decoded into their full size for execution, maintaining the resource constraints. Knowledge distillation solves this issue by transferring the knowledge from a complex, large model (or an ensemble of models) into a smaller, more efficient model, enabling deployment on resource-limited devices without significant loss of performance. \n\nTypical knowledge distillation involves transferring knowledge from a large model (or an ensemble of models) to a smaller, more efficient model. This process typically includes three key components: one or more pre-trained teacher models, an initial student model, and a training dataset (as shown in Figure 5). The teacher and student models output class prediction probabilities, and the distillation loss is computed by comparing these predictions. Additionally, the loss between the predicted and true class labels is calculated. Both losses are used to train the student model, optimizing it to achieve an accuracy similar to the teacher model. Knowledge distillation aims to reduce the performance gap between the teacher and student models by efficiently determining their structure. This technique, also known as model distillation, is particularly effective for neural networks with complex architectures and is widely applied in fields such as computer vision, visual question answering, natural language processing, and speech recognition, where large models are impractical due to resource constraints [17]. Knowledge in a neural network model can be categorized into response-based, featurebased, and relation-based knowledge, which correspond to information extracted from the output layer, intermediate layers, and relationships between layers, respectively, [95]. Understanding these different types of knowledge, their core concepts, and how they can be combined is crucial for effective model distillation.",
            "score": 0.611024544852186,
            "section_title": "Knowledge Distillation",
            "char_start_offset": 32870,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 290
                },
                {
                    "start": 291,
                    "end": 591
                },
                {
                    "start": 592,
                    "end": 844
                },
                {
                    "start": 847,
                    "end": 991
                },
                {
                    "start": 992,
                    "end": 1158
                },
                {
                    "start": 1159,
                    "end": 1298
                },
                {
                    "start": 1299,
                    "end": 1380
                },
                {
                    "start": 1381,
                    "end": 1496
                },
                {
                    "start": 1497,
                    "end": 1637
                },
                {
                    "start": 1638,
                    "end": 1966
                },
                {
                    "start": 1967,
                    "end": 2226
                },
                {
                    "start": 2227,
                    "end": 2371
                }
            ],
            "ref_mentions": [
                {
                    "start": 1961,
                    "end": 1965,
                    "matchedPaperCorpusId": "219559263"
                },
                {
                    "start": 2221,
                    "end": 2225,
                    "matchedPaperCorpusId": "259203401"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.9873046875
        },
        {
            "corpus_id": "237563200",
            "title": "Distilling Linguistic Context for Language Model Compression",
            "text": "Transformer. In the general framework of knowledge distillation, teacher network (T ) with large capacity is trained in advance, and then student network (S) with pre-defined architecture but relatively smaller than teacher network is trained with the help of teacher's knowledge. Specifically, given the teacher parameterized by \u03b8 t , training the student parameterized by \u03b8 s aims to minimize two objectives: i) the crossentropy loss L CE between the output of the student network S and the true label y and ii) the difference of some statistics L D between teacher and student models. Overall, our goal is to minimize the following objective function:\n\nwhere \u03bb controls the relative importance between two objectives. Here, K characterizes the knowledge being transferred and can vary depending on the distillation methods, and L D is a matching loss function such as l 1 , l 2 or Huber loss. Recent studies on knowledge distillation for Transformer-based BERT can also be understood in this general framework. In particular, each distillation methods of previous works are summarized in Appendix A.",
            "score": 0.6108950440364296,
            "section_title": "Setup and background",
            "char_start_offset": 9007,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.943359375
        },
        {
            "corpus_id": "225070666",
            "title": "FastFormers: Highly Efficient Transformer Models for Natural Language Understanding",
            "text": "Knowledge distillation (Hinton et al., 2015) is a well known model compression technique where the large model is used as a teacher for a smaller student model. The knowledge distillation is the process of training the student model to mimic the behaviour of the larger teacher model. Knowledge distillation has been shown to improve the efficiency of the Transformer-based architectures for the NLU tasks (Sanh et al., 2019;Jiao et al., 2019) as well as natural language generation tasks such as machine translation (Kim et al., 2019).",
            "score": 0.610410498041879,
            "section_title": "Knowledge Distillation",
            "char_start_offset": 3586,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 160
                },
                {
                    "start": 161,
                    "end": 284
                },
                {
                    "start": 285,
                    "end": 536
                }
            ],
            "ref_mentions": [
                {
                    "start": 23,
                    "end": 44,
                    "matchedPaperCorpusId": "7200347"
                },
                {
                    "start": 425,
                    "end": 443,
                    "matchedPaperCorpusId": "207912113"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.8701171875
        },
        {
            "corpus_id": "253107469",
            "title": "Online cross-layer knowledge distillation on graph neural networks with deep supervision",
            "text": "Knowledge distillation is one of the most popular techniques in model compression [7], which utilizes a powerful but cumbersome teacher model to boost a compressed student model. Hinton et al. [8] first proposed the concept of knowledge distillation, trying to transfer the soft targets, i.e., the teacher model predictions to the student models. It's believed that the soft targets can capture the relationship among classes and apply regularization during the training. In addition to soft targets, feature maps in the intermediate layers are another form of knowledge [9][10][11][12]. Romero et al. [9] first put forward FitNet to reduce the distance of feature embeddings between teacher models and student models. Agoruyko et al. [10] considered the attention mechanism and extracted attention maps instead of features. SemCKD [11] utilizes the attention mechanism to automatically assign adaptive intermediate features for student model. SimKD [13] achieves incredible effect just by reusing the teacher's classifier. \n\nHowever, training a large teacher model in advance is time-consuming and requires high computing resources. In a worse case, we can't even get a teacher model. The online knowledge distillation [22][23][24] is proposed to address these issues, where a group of student models are trained simultaneously by aggregating and aligning their outputs. Compared with the offline knowledge distillation mentioned above, online knowledge distillation can still boost the performance of the student model when reducing the training costs. Deep mutual learning [22] trained a group of student models collaboratively by learning the soft targets from each other. OKDDip [24] introduced multiple auxiliary peers and one team leader, proposing a novel two-level online knowledge distillation framework.",
            "score": 0.6098076783686592,
            "section_title": "Knowledge Distillation",
            "char_start_offset": 6016,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 178
                },
                {
                    "start": 179,
                    "end": 346
                },
                {
                    "start": 347,
                    "end": 471
                },
                {
                    "start": 472,
                    "end": 587
                },
                {
                    "start": 588,
                    "end": 718
                },
                {
                    "start": 719,
                    "end": 824
                },
                {
                    "start": 825,
                    "end": 943
                },
                {
                    "start": 944,
                    "end": 1023
                },
                {
                    "start": 1026,
                    "end": 1133
                },
                {
                    "start": 1134,
                    "end": 1185
                },
                {
                    "start": 1186,
                    "end": 1371
                },
                {
                    "start": 1372,
                    "end": 1554
                },
                {
                    "start": 1555,
                    "end": 1676
                },
                {
                    "start": 1677,
                    "end": 1814
                }
            ],
            "ref_mentions": [
                {
                    "start": 82,
                    "end": 85,
                    "matchedPaperCorpusId": "11253972"
                },
                {
                    "start": 571,
                    "end": 574,
                    "matchedPaperCorpusId": "2723173"
                },
                {
                    "start": 574,
                    "end": 578,
                    "matchedPaperCorpusId": "829159"
                },
                {
                    "start": 578,
                    "end": 582,
                    "matchedPaperCorpusId": "227335337"
                },
                {
                    "start": 582,
                    "end": 586,
                    "matchedPaperCorpusId": "245650711"
                },
                {
                    "start": 602,
                    "end": 605,
                    "matchedPaperCorpusId": "2723173"
                },
                {
                    "start": 735,
                    "end": 739,
                    "matchedPaperCorpusId": "829159"
                },
                {
                    "start": 832,
                    "end": 836,
                    "matchedPaperCorpusId": "227335337"
                },
                {
                    "start": 950,
                    "end": 954,
                    "matchedPaperCorpusId": "247762862"
                },
                {
                    "start": 1220,
                    "end": 1224,
                    "matchedPaperCorpusId": "26071966"
                },
                {
                    "start": 1224,
                    "end": 1228,
                    "matchedPaperCorpusId": "2331610"
                },
                {
                    "start": 1228,
                    "end": 1232,
                    "matchedPaperCorpusId": "208526905"
                },
                {
                    "start": 1576,
                    "end": 1580,
                    "matchedPaperCorpusId": "26071966"
                },
                {
                    "start": 1684,
                    "end": 1688,
                    "matchedPaperCorpusId": "208526905"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.9794921875
        },
        {
            "corpus_id": "255999647",
            "title": "RNAS-CL: Robust Neural Architecture Search by Cross-Layer Knowledge Distillation",
            "text": "Knowledge Distillation (KD) transfers knowledge from a large, cumbersome model to a small model. [19] proposes the teacher-student model, where they use the soft targets from the teacher to train the student model. KD forces the student to generalize, similar to the teacher model. Since [19], numerous KD variants [25,40,45,49,62,64] based on feature map, attention map, or contrastive learning have been proposed. [40] introduced intermediate-level hints from the teacher model to guide the student model training. [40] trained the student model in two stages. First, they trained the student model such that the student's middle layer predicts the output of the teacher's middle layer (hint layer). Next, they finetuned the pre-trained student model using the standard KD optimization function. Thanks to the intermediate hint, the student model achieved better performance with fewer parameters. Moving a step further, [62], [64] and [25] used information from multiple teacher layers to guide students' training. [62] computed Gramian matrix between the first and the last layer's output features to represent the flow of problem-solving. \n\n[62] transferred knowledge by minimizing the distance between student and teacher's flow matrix. \n\n[25] calculated the inter-layered Gramian matrix and inter-class Gramian matrix to find the most representative layer and then minimized the distance between a few of the most representative student and teacher layers. [64] minimized the distance between teacher and student attention maps at the various block. [26] distills knowledge from teachers' blocks to supervise students' blockwise architecture search. Contrary to the above methods, which map few teacher-student layers or blocks. We map all student layers to a teacher layer. We propose RNAS-CL to search for the perfect tutor layer for each student layer. Similar to [64], we minimize the distance between mapped student-teacher attention maps.",
            "score": 0.6095422585490959,
            "section_title": "Knowledge Distillation",
            "char_start_offset": 6404,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 96
                },
                {
                    "start": 97,
                    "end": 214
                },
                {
                    "start": 215,
                    "end": 281
                },
                {
                    "start": 282,
                    "end": 415
                },
                {
                    "start": 416,
                    "end": 516
                },
                {
                    "start": 517,
                    "end": 562
                },
                {
                    "start": 563,
                    "end": 701
                },
                {
                    "start": 702,
                    "end": 797
                },
                {
                    "start": 798,
                    "end": 899
                },
                {
                    "start": 900,
                    "end": 1017
                },
                {
                    "start": 1018,
                    "end": 1143
                },
                {
                    "start": 1146,
                    "end": 1242
                },
                {
                    "start": 1245,
                    "end": 1463
                },
                {
                    "start": 1464,
                    "end": 1556
                },
                {
                    "start": 1557,
                    "end": 1656
                },
                {
                    "start": 1657,
                    "end": 1735
                },
                {
                    "start": 1736,
                    "end": 1781
                },
                {
                    "start": 1782,
                    "end": 1862
                },
                {
                    "start": 1863,
                    "end": 1951
                }
            ],
            "ref_mentions": [
                {
                    "start": 315,
                    "end": 319,
                    "matchedPaperCorpusId": "182183296"
                },
                {
                    "start": 319,
                    "end": 322,
                    "matchedPaperCorpusId": "2723173"
                },
                {
                    "start": 322,
                    "end": 325,
                    "matchedPaperCorpusId": "173990799"
                },
                {
                    "start": 325,
                    "end": 328,
                    "matchedPaperCorpusId": "204838340"
                },
                {
                    "start": 328,
                    "end": 331,
                    "matchedPaperCorpusId": "206596723"
                },
                {
                    "start": 331,
                    "end": 334,
                    "matchedPaperCorpusId": "829159"
                },
                {
                    "start": 416,
                    "end": 420,
                    "matchedPaperCorpusId": "2723173"
                },
                {
                    "start": 517,
                    "end": 521,
                    "matchedPaperCorpusId": "2723173"
                },
                {
                    "start": 923,
                    "end": 927,
                    "matchedPaperCorpusId": "206596723"
                },
                {
                    "start": 929,
                    "end": 933,
                    "matchedPaperCorpusId": "829159"
                },
                {
                    "start": 938,
                    "end": 942,
                    "matchedPaperCorpusId": "182183296"
                },
                {
                    "start": 1018,
                    "end": 1022,
                    "matchedPaperCorpusId": "206596723"
                },
                {
                    "start": 1464,
                    "end": 1468,
                    "matchedPaperCorpusId": "829159"
                },
                {
                    "start": 1557,
                    "end": 1561,
                    "matchedPaperCorpusId": "208513081"
                },
                {
                    "start": 1874,
                    "end": 1878,
                    "matchedPaperCorpusId": "829159"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.9248046875
        },
        {
            "corpus_id": "277349937",
            "title": "DuckSegmentation: A segmentation model based on the AnYue Hemp Duck Dataset",
            "text": "Although our model is theoretically powerful, the large number of our model parameters requires high hardware resource requirements when deployed in practical scenarios such as the AnYue sesame duck farm.As shown in Figure 7, The knowledge distillation method proposed by predecessors such as Geoffrey Hinton,Oriol Vinyals,and Jeff Dean [33], which can sacrifice some of the accuracy of the model to reduce the size of the number of model parameters, can train the model effectively. Knowledge distillation is the transfer of knowledge of the learned features from the already trained large model to the small model. We call the large model the teacher model and the small model the student model. Eventually the student model is taught by the teacher model to achieve the purpose of compressing the number of parameters. \n\nFor the traditional knowledge distillation approach, losses are calculated for the outputs of the student model and the teacher model so that the student model keeps approaching the teacher model. However, this approach only requires the small model to learn the output features of the large model and does not learn the structural features of the large model. We need a more efficient, stable and flexible knowledge distillation method. \n\nThe traditional knowledge distillation learning loss calculation, given the student model prediction   and the teacher model prediction   , and given the distillation temperature , the distillation loss can then be defined as \n\nTo obtain better small models, we use the Channelwise Knowledge Distillation (CWD) model proposed by Changyong Shu,Yifan Liu et alp. The CWD model performs the dense prediction task by normalizing the activation map in each channel, directing the student model to pay more attention to modeling regions with significant activation values regions with significant activation values, thus achieving more accurate localization in the intensive prediction task. A CWD schematic is shown in Figure 8. \n\nIn order to better utilize the knowledge in each channel, the authors convert channel activation into a probability distribution and propose a novel channel distillation paradigm so as to guide students to learn knowledge and features from the teacher model in an orderly manner. Given a teacher network  and a student network  , the activation graphs from  and  are   and   , respectively, then the channeled knowledge distillation loss can be expressed as:",
            "score": 0.609502423017626,
            "section_title": "Knowledge Distillation",
            "char_start_offset": 22685,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 483
                },
                {
                    "start": 484,
                    "end": 616
                },
                {
                    "start": 617,
                    "end": 697
                },
                {
                    "start": 698,
                    "end": 821
                },
                {
                    "start": 824,
                    "end": 1020
                },
                {
                    "start": 1021,
                    "end": 1184
                },
                {
                    "start": 1185,
                    "end": 1261
                },
                {
                    "start": 1264,
                    "end": 1489
                },
                {
                    "start": 1492,
                    "end": 1624
                },
                {
                    "start": 1625,
                    "end": 1949
                },
                {
                    "start": 1950,
                    "end": 1987
                },
                {
                    "start": 1990,
                    "end": 2269
                },
                {
                    "start": 2270,
                    "end": 2448
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.896484375
        },
        {
            "corpus_id": "219559263",
            "title": "Knowledge Distillation: A Survey",
            "text": "For multilingual representations, knowledge distillation transfers knowledge among the multilingual word embeddings for bilingual dictionary induction (Nakashole and Flauger 2017). For low-resource languages, knowledge transfer is effective across ensembles of multilingual models (Cui et al. 2017). \n\nSeveral observations about knowledge distillation for natural language processing are summarized as follows. \n\n\u2022 Knowledge distillation provides efficient and effective lightweight language deep models. The large-capacity teacher model can transfer the rich knowledge from a large number of different kinds of language data to train a small student model, so that the student can quickly complete many language tasks with effective performance. \u2022 The teacher-student knowledge transfer can easily and effectively solve many multilingual tasks, considering that knowledge from multilingual models can be transferred and shared by each other. \u2022 In deep language models, the sequence knowledge can be effectively transferred from large networks into small networks.",
            "score": 0.609431947955148,
            "section_title": "KD in NLP",
            "char_start_offset": 69222,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 180
                },
                {
                    "start": 181,
                    "end": 299
                },
                {
                    "start": 302,
                    "end": 410
                },
                {
                    "start": 413,
                    "end": 504
                },
                {
                    "start": 505,
                    "end": 746
                },
                {
                    "start": 747,
                    "end": 942
                },
                {
                    "start": 943,
                    "end": 1064
                }
            ],
            "ref_mentions": [
                {
                    "start": 151,
                    "end": 179,
                    "matchedPaperCorpusId": "22421874"
                },
                {
                    "start": 281,
                    "end": 298,
                    "matchedPaperCorpusId": "1174931"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.87744140625
        },
        {
            "corpus_id": "271051234",
            "title": "Improving Knowledge Distillation in Transfer Learning with Layer-wise Learning Rates",
            "text": "Neural networks have achieved massive success in fields like computer vision and natural language processing [1,2,3].There is continual progress being made with extensive state-of-the-art performances.Despite the massive success, deep neural networks face two major disadvantages in real-world applications.Firstly, deeper neural networks are highly compute-intensive and difficult to deploy in resource-limited situations.Secondly, it requires extensive data to make such large networks achieve higher task accuracy, which is unavailable in most scenarios.\n\nKnowledge transfer learning methodologies have received a surge in popularity among the machine learning community to tackle the above-mentioned challenges.These methodologies enable training one neural network, often referred to as the student model, with the help of another neural network, often referred to as the teacher model, to train on the same dataset or a subset of a similar dataset.Knowledge distillation is the type of knowledge transfer method where the teacher network is larger than the student network and both networks are trained on the same dataset [4,5].Knowledge distillation is usually applied in machine learning to extract the most important aspects of the teacher neural network model into a less computationintensive student model.It involves the concept of passing on knowledge gained by important neurons in larger (teacher) networks to the smaller (student) networks so that the smaller networks learn crucial features contributing to achieving better performance [6].\n\nRecent knowledge distillation approaches focus on utilizing derivative information of neural network parameters to obtain the best approximation of the model [7,8,9].On the other hand, there have been several other attention-matching methods that match the spatial maps obtained post each critical neural network activation [10,11].However, the existing methods of these types utilize the cumulative difference of all the maps and then use it to back-propagate that loss through all the layers.The cumulative loss back-propagation is often ineffective and results in student models that do not achieve the desired accuracy compared to their respective teacher models.This problem becomes more intense with the increased complexity of the learning task. ) based on the JSD loss (between the student and the teacher layer at the second identical layer) and prior momentum (\u03b7\n\n) at an interval of 25 epochs.",
            "score": 0.6091127197508357,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 117
                },
                {
                    "start": 117,
                    "end": 201
                },
                {
                    "start": 201,
                    "end": 307
                },
                {
                    "start": 307,
                    "end": 423
                },
                {
                    "start": 423,
                    "end": 557
                },
                {
                    "start": 559,
                    "end": 715
                },
                {
                    "start": 715,
                    "end": 954
                },
                {
                    "start": 954,
                    "end": 1135
                },
                {
                    "start": 1135,
                    "end": 1318
                },
                {
                    "start": 1318,
                    "end": 1558
                },
                {
                    "start": 1560,
                    "end": 1726
                },
                {
                    "start": 1726,
                    "end": 1892
                },
                {
                    "start": 1892,
                    "end": 2054
                },
                {
                    "start": 2054,
                    "end": 2227
                },
                {
                    "start": 2227,
                    "end": 2432
                },
                {
                    "start": 2434,
                    "end": 2464
                }
            ],
            "ref_mentions": [
                {
                    "start": 109,
                    "end": 112,
                    "matchedPaperCorpusId": "206594692"
                },
                {
                    "start": 114,
                    "end": 116,
                    "matchedPaperCorpusId": "206741496"
                },
                {
                    "start": 1129,
                    "end": 1132,
                    "matchedPaperCorpusId": "53213211"
                },
                {
                    "start": 1554,
                    "end": 1557,
                    "matchedPaperCorpusId": "211116043"
                },
                {
                    "start": 1723,
                    "end": 1725,
                    "matchedPaperCorpusId": "91185859"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.92431640625
        },
        {
            "corpus_id": "265384964",
            "title": "Forest Fire Object Detection Analysis Based on Knowledge Distillation",
            "text": "Knowledge distillation [34] involves the process of transferring knowledge from a sizable, intricate model (teacher model) to a more compact, effective model (student model) [35]. It involves training the student network to mimic the teacher network's output and emulate its internal representations or decision-making process. This technique is used to enhance the performance of smaller models, making them approximate the behavior of larger models while reducing computational costs and memory requirements. In the process of knowledge distillation, logits are used as a basis for comparing the outputs of the student model with those of the teacher model. By examining logits, the model can measure the certainty or confidence of its predictions before applying the softmax function to obtain the final probability. The standard cross-entropy loss depends on the predicted probabilities and ground truth labels. Additionally, the loss function is extended to include the standard cross-entropy loss between the predictions of the student model and the ground truth labels, as well as an additional loss term that measures the discrepancy between the softened probabilities (obtained through a higher temperature softmax) of the teacher model's predictions and the corresponding predictions of the student model. This additional term ensures that the student model not only learns to predict the correct labels but also aims to replicate the softened outputs of the teacher model, effectively transferring its knowledge to the student model. By jointly minimizing these two loss terms, the student model can learn to generalize better and imitate the behavior of the more complex teacher model. The calculation of the probability for the class is as follows: \n\nHere, T represents the \"temperature\" of knowledge distillation. When T = 1, it corresponds to a normalized exponential function. With the increase in the temperature parameter T, the softmax function's probability distribution becomes smoother, thereby conveying more nuanced particulars about the interrelation of different categories according to the teacher model. This information, referred to as \"dark knowledge\" by Hinton, is what we aim to impart to the student model in distillation. To compute the loss function for the teacher's soft targets, we use the same T value to calculate the softmax function based on the student logits. This kind of loss is frequently called \"distillation loss.\"",
            "score": 0.6086358299316963,
            "section_title": "Knowledge Distillation",
            "char_start_offset": 9617,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 179
                },
                {
                    "start": 180,
                    "end": 327
                },
                {
                    "start": 328,
                    "end": 510
                },
                {
                    "start": 511,
                    "end": 659
                },
                {
                    "start": 660,
                    "end": 819
                },
                {
                    "start": 820,
                    "end": 915
                },
                {
                    "start": 916,
                    "end": 1315
                },
                {
                    "start": 1316,
                    "end": 1544
                },
                {
                    "start": 1545,
                    "end": 1697
                },
                {
                    "start": 1698,
                    "end": 1761
                },
                {
                    "start": 1764,
                    "end": 1827
                },
                {
                    "start": 1828,
                    "end": 1892
                },
                {
                    "start": 1893,
                    "end": 2131
                },
                {
                    "start": 2132,
                    "end": 2255
                },
                {
                    "start": 2256,
                    "end": 2403
                },
                {
                    "start": 2404,
                    "end": 2463
                }
            ],
            "ref_mentions": [
                {
                    "start": 23,
                    "end": 27,
                    "matchedPaperCorpusId": "11253972"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.9775390625
        },
        {
            "corpus_id": "248683566",
            "title": "Teacher-student collaborative knowledge distillation for image classification",
            "text": "With the rapid development of deep learning, convolutional neural networks have exhibited excellent performance in various computer vision tasks [1,2]. In visual datasets, a category usually contains multi-view features that are easy to categorize, and simple models can only learn some of the features; however, deep neural networks can effectively handle this problem. As the number of network layers and parameters increase, models become prone to overfitting, which affects their performance. \n\nKnowledge distillation is an important method of knowledge transfer; in this process, a lightweight model learns valid information from a heavy model to enhance performance. This model structure is often considered as a teacher-student structure. With an experienced teacher network in place, the inferior student network learns from the valuable information in the teacher network through knowledge distillation and achieves a performance improvement. Similarly, self-distillation allows a model to learn another pretrained network with the same structure. Due to the stochastic nature of feature learning and differences among model initialization methods, models obtain knowledge in different ways. The performance of a network can also be effectively improved by knowledge transfer between models. A model can learn knowledge from other models or itself to improve performance. However, it is still unclear whether a model can achieve performance improvements under the guidance of both a teacher model and itself. \n\nIn this paper, our soft label information comes from the teacher network and the output of student network, therefore the student network can be regarded as its own second teacher. Similar to multi-teacher distillation, in our approach, we let a single model learn as many view features as possible from multiple networks. In the process of human learning, students guided by teachers can further improve their abilities with self-reflection. Inspired by this approach, a combination of teacher-student knowledge distillation and student self-distillation is used to enhance the performance of neural networks, and a method called teacher-student collaborative knowledge distillation (TSKD) is proposed. This method not only utilizes the category information from the teacher network but also absorbs student's knowledge. To construct the student self-distillation model, the student network builds multiple exit classifiers from shallow to deep.",
            "score": 0.6080915316701164,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 151
                },
                {
                    "start": 152,
                    "end": 370
                },
                {
                    "start": 371,
                    "end": 496
                },
                {
                    "start": 499,
                    "end": 672
                },
                {
                    "start": 673,
                    "end": 745
                },
                {
                    "start": 746,
                    "end": 951
                },
                {
                    "start": 952,
                    "end": 1056
                },
                {
                    "start": 1057,
                    "end": 1200
                },
                {
                    "start": 1201,
                    "end": 1300
                },
                {
                    "start": 1301,
                    "end": 1380
                },
                {
                    "start": 1381,
                    "end": 1517
                },
                {
                    "start": 1520,
                    "end": 1700
                },
                {
                    "start": 1701,
                    "end": 1842
                },
                {
                    "start": 1843,
                    "end": 1962
                },
                {
                    "start": 1963,
                    "end": 2223
                },
                {
                    "start": 2224,
                    "end": 2341
                },
                {
                    "start": 2342,
                    "end": 2466
                }
            ],
            "ref_mentions": [
                {
                    "start": 145,
                    "end": 148,
                    "matchedPaperCorpusId": "195908774"
                },
                {
                    "start": 148,
                    "end": 150,
                    "matchedPaperCorpusId": "206594692"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.88671875
        },
        {
            "paperId": "e27d0c5bc1863f6b1a705f2d1d42e57e46335d3d",
            "corpusId": 268454093,
            "title": "Reciprocal Teacher-Student Learning via Forward and Feedback Knowledge Distillation",
            "venue": "IEEE transactions on multimedia",
            "year": 2024,
            "referenceCount": 65,
            "citationCount": 22,
            "influentialCitationCount": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": "CLOSED",
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.1109/TMM.2024.3372833?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1109/TMM.2024.3372833, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "38978232",
                    "name": "Jianping Gou"
                },
                {
                    "authorId": "2291862820",
                    "name": "Yu Chen"
                },
                {
                    "authorId": "2278848342",
                    "name": "Baosheng Yu"
                },
                {
                    "authorId": "2257860798",
                    "name": "Jinhua Liu"
                },
                {
                    "authorId": "143613270",
                    "name": "Lan Du"
                },
                {
                    "authorId": "2265092467",
                    "name": "Shaohua Wan"
                },
                {
                    "authorId": "2146683466",
                    "name": "Zhang Yi"
                }
            ],
            "abstract": "Knowledge distillation (KD) is a prevalent model compression technique in deep learning, aiming to leverage knowledge from a large teacher model to enhance the training of a smaller student model. It has found success in deploying compact deep models in intelligent applications like intelligent transportation, smart health, and distributed intelligence. Current knowledge distillation methods primarily fall into two categories: offline and online knowledge distillation. Offline methods involve a one-way distillation process, transferring unvaried knowledge from teacher to student, while online methods enable the simultaneous training of multiple peer students. However, existing knowledge distillation methods often face challenges where the student may not fully comprehend the teacher's knowledge due to model capacity gaps, and there might be knowledge incongruence among outputs of multiple students without teacher guidance. To address these issues, we propose a novel reciprocal teacher-student learning inspired by human teaching and examining through forward and feedback knowledge distillation (FFKD). Forward knowledge distillation operates offline, while feedback knowledge distillation follows an online scheme. The rationale is that feedback knowledge distillation enables the pre-trained teacher model to receive feedback from students, allowing the teacher to refine its teaching strategies accordingly. To achieve this, we introduce a new weighting constraint to gauge the extent of students' understanding of the teacher's knowledge, which is then utilized to enhance teaching strategies. Experimental results on five visual recognition datasets demonstrate that the proposed FFKD outperforms current state-of-the-art knowledge distillation methods.",
            "corpus_id": "268454093",
            "text": "Knowledge distillation (KD) is a prevalent model compression technique in deep learning, aiming to leverage knowledge from a large teacher model to enhance the training of a smaller student model. It has found success in deploying compact deep models in intelligent applications like intelligent transportation, smart health, and distributed intelligence. Current knowledge distillation methods primarily fall into two categories: offline and online knowledge distillation. Offline methods involve a one-way distillation process, transferring unvaried knowledge from teacher to student, while online methods enable the simultaneous training of multiple peer students. However, existing knowledge distillation methods often face challenges where the student may not fully comprehend the teacher's knowledge due to model capacity gaps, and there might be knowledge incongruence among outputs of multiple students without teacher guidance. To address these issues, we propose a novel reciprocal teacher-student learning inspired by human teaching and examining through forward and feedback knowledge distillation (FFKD). Forward knowledge distillation operates offline, while feedback knowledge distillation follows an online scheme. The rationale is that feedback knowledge distillation enables the pre-trained teacher model to receive feedback from students, allowing the teacher to refine its teaching strategies accordingly. To achieve this, we introduce a new weighting constraint to gauge the extent of students' understanding of the teacher's knowledge, which is then utilized to enhance teaching strategies. Experimental results on five visual recognition datasets demonstrate that the proposed FFKD outperforms current state-of-the-art knowledge distillation methods.",
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "score": 0.0,
            "stype": "public_api",
            "pdf_hash": "",
            "rerank_score": 0.92431640625
        },
        {
            "paperId": "4385314ade0ee492d99bc03c1e8ea98ec577c44c",
            "corpusId": 273350649,
            "title": "Speculative Knowledge Distillation: Bridging the Teacher-Student Gap Through Interleaved Sampling",
            "venue": "arXiv.org",
            "year": 2024,
            "referenceCount": 40,
            "citationCount": 11,
            "influentialCitationCount": 1,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2410.11325, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "145738382",
                    "name": "Wenda Xu"
                },
                {
                    "authorId": "2325953181",
                    "name": "Rujun Han"
                },
                {
                    "authorId": "2278799290",
                    "name": "Zifeng Wang"
                },
                {
                    "authorId": "2253355959",
                    "name": "Long T. Le"
                },
                {
                    "authorId": "10723295",
                    "name": "Dhruv Madeka"
                },
                {
                    "authorId": "2307183930",
                    "name": "Lei Li"
                },
                {
                    "authorId": "2312264839",
                    "name": "William Yang Wang"
                },
                {
                    "authorId": "2253488622",
                    "name": "Rishabh Agarwal"
                },
                {
                    "authorId": "2278969944",
                    "name": "Chen-Yu Lee"
                },
                {
                    "authorId": "2305619900",
                    "name": "Tomas Pfister"
                }
            ],
            "abstract": "Recent advances in knowledge distillation (KD) have enabled smaller student models to approach the performance of larger teacher models. However, popular methods such as supervised KD and on-policy KD, are adversely impacted by the knowledge gaps between teacher-student in practical scenarios. Supervised KD suffers from a distribution mismatch between training with a static dataset and inference over final student-generated outputs. Conversely, on-policy KD, which uses student-generated samples for training, can suffer from low-quality training examples with which teacher models are not familiar, resulting in inaccurate teacher feedback. To address these limitations, we introduce Speculative Knowledge Distillation (SKD), a novel approach that leverages cooperation between student and teacher models to generate high-quality training data on-the-fly while aligning with the student's inference-time distribution. In SKD, the student proposes tokens, and the teacher replaces poorly ranked ones based on its own distribution, transferring high-quality knowledge adaptively. We evaluate SKD on various text generation tasks, including translation, summarization, math, and instruction following, and show that SKD consistently outperforms existing KD methods across different domains, data sizes, and model initialization strategies.",
            "corpus_id": "273350649",
            "text": "Recent advances in knowledge distillation (KD) have enabled smaller student models to approach the performance of larger teacher models. However, popular methods such as supervised KD and on-policy KD, are adversely impacted by the knowledge gaps between teacher-student in practical scenarios. Supervised KD suffers from a distribution mismatch between training with a static dataset and inference over final student-generated outputs. Conversely, on-policy KD, which uses student-generated samples for training, can suffer from low-quality training examples with which teacher models are not familiar, resulting in inaccurate teacher feedback. To address these limitations, we introduce Speculative Knowledge Distillation (SKD), a novel approach that leverages cooperation between student and teacher models to generate high-quality training data on-the-fly while aligning with the student's inference-time distribution. In SKD, the student proposes tokens, and the teacher replaces poorly ranked ones based on its own distribution, transferring high-quality knowledge adaptively. We evaluate SKD on various text generation tasks, including translation, summarization, math, and instruction following, and show that SKD consistently outperforms existing KD methods across different domains, data sizes, and model initialization strategies.",
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "score": 0.0,
            "stype": "public_api",
            "pdf_hash": "",
            "rerank_score": 0.84716796875
        },
        {
            "paperId": "84ee7c46efe3888276c74f9edef0407dd054c3a1",
            "corpusId": 267523447,
            "title": "Beyond Answers: Transferring Reasoning Capabilities to Smaller LLMs Using Multi-Teacher Knowledge Distillation",
            "venue": "Web Search and Data Mining",
            "year": 2024,
            "referenceCount": 74,
            "citationCount": 10,
            "influentialCitationCount": 0,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "http://arxiv.org/pdf/2402.04616",
                "status": "GREEN",
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2402.04616, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "46879986",
                    "name": "Yijun Tian"
                },
                {
                    "authorId": "2283258326",
                    "name": "Yikun Han"
                },
                {
                    "authorId": "29963551",
                    "name": "Xiusi Chen"
                },
                {
                    "authorId": "2283212563",
                    "name": "Wei Wang"
                },
                {
                    "authorId": "144539424",
                    "name": "N. Chawla"
                }
            ],
            "abstract": "Transferring the reasoning capability from stronger large language models (LLMs) to smaller ones has been quite appealing, as smaller LLMs are more flexible to deploy with less expense. Among the existing solutions, knowledge distillation stands out due to its outstanding efficiency and generalization. However, existing methods suffer from several drawbacks, including limited knowledge diversity and the lack of rich contextual information. To solve the problems and facilitate the learning of compact language models, we propose TinyLLM, a new knowledge distillation paradigm to learn a small student LLM from multiple large teacher LLMs. In particular, we encourage the student LLM to not only generate the correct answers but also understand the rationales behind these answers. Given that different LLMs possess diverse reasoning skills, we guide the student model to assimilate knowledge from various teacher LLMs. We further introduce an in-context example generator and a teacher-forcing Chain-of-Thought strategy to ensure that the rationales are accurate and grounded in contextually appropriate scenarios. Extensive experiments on six datasets across two reasoning tasks demonstrate the superiority of our method. Results show that TinyLLM can outperform large teacher LLMs significantly, despite a considerably smaller model size. The source code is available at: https://github.com/YikunHan42/TinyLLM.",
            "corpus_id": "267523447",
            "text": "Transferring the reasoning capability from stronger large language models (LLMs) to smaller ones has been quite appealing, as smaller LLMs are more flexible to deploy with less expense. Among the existing solutions, knowledge distillation stands out due to its outstanding efficiency and generalization. However, existing methods suffer from several drawbacks, including limited knowledge diversity and the lack of rich contextual information. To solve the problems and facilitate the learning of compact language models, we propose TinyLLM, a new knowledge distillation paradigm to learn a small student LLM from multiple large teacher LLMs. In particular, we encourage the student LLM to not only generate the correct answers but also understand the rationales behind these answers. Given that different LLMs possess diverse reasoning skills, we guide the student model to assimilate knowledge from various teacher LLMs. We further introduce an in-context example generator and a teacher-forcing Chain-of-Thought strategy to ensure that the rationales are accurate and grounded in contextually appropriate scenarios. Extensive experiments on six datasets across two reasoning tasks demonstrate the superiority of our method. Results show that TinyLLM can outperform large teacher LLMs significantly, despite a considerably smaller model size. The source code is available at: https://github.com/YikunHan42/TinyLLM.",
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "score": 0.0,
            "stype": "public_api",
            "pdf_hash": "",
            "rerank_score": 0.73291015625
        },
        {
            "paperId": "3f15d1caa39a6d408669a3ea8206b84dbb11b6b7",
            "corpusId": 274241902,
            "title": "Simplified Knowledge Distillation for Deep Neural Networks Bridging the Performance Gap with a Novel Teacher\u2013Student Architecture",
            "venue": "Electronics",
            "year": 2024,
            "referenceCount": 18,
            "citationCount": 12,
            "influentialCitationCount": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.3390/electronics13224530?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.3390/electronics13224530, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2240683787",
                    "name": "Sabina Umirzakova"
                },
                {
                    "authorId": "2323119268",
                    "name": "Mirjamol Abdullaev"
                },
                {
                    "authorId": "2267680163",
                    "name": "Sevara Mardieva"
                },
                {
                    "authorId": "2140789040",
                    "name": "Nodira Latipova"
                },
                {
                    "authorId": "2102629657",
                    "name": "Shakhnoza Muksimova"
                }
            ],
            "abstract": "The rapid evolution of deep learning has led to significant achievements in computer vision, primarily driven by complex convolutional neural networks (CNNs). However, the increasing depth and parameter count of these networks often result in overfitting and elevated computational demands. Knowledge distillation (KD) has emerged as a promising technique to address these issues by transferring knowledge from a large, well-trained teacher model to a more compact student model. This paper introduces a novel knowledge distillation method that simplifies the distillation process and narrows the performance gap between teacher and student models without relying on intricate knowledge representations. Our approach leverages a unique teacher network architecture designed to enhance the efficiency and effectiveness of knowledge transfer. Additionally, we introduce a streamlined teacher network architecture that transfers knowledge effectively through a simplified distillation process, enabling the student model to achieve high accuracy with reduced computational demands. Comprehensive experiments conducted on the CIFAR-10 dataset demonstrate that our proposed model achieves superior performance compared to traditional KD methods and established architectures such as ResNet and VGG networks. The proposed method not only maintains high accuracy but also significantly reduces training and validation losses. Key findings highlight the optimal hyperparameter settings (temperature T = 15.0 and smoothing factor \u03b1 = 0.7), which yield the highest validation accuracy and lowest loss values. This research contributes to the theoretical and practical advancements in knowledge distillation, providing a robust framework for future applications and research in neural network compression and optimization. The simplicity and efficiency of our approach pave the way for more accessible and scalable solutions in deep learning model deployment.",
            "corpus_id": "274241902",
            "text": "The rapid evolution of deep learning has led to significant achievements in computer vision, primarily driven by complex convolutional neural networks (CNNs). However, the increasing depth and parameter count of these networks often result in overfitting and elevated computational demands. Knowledge distillation (KD) has emerged as a promising technique to address these issues by transferring knowledge from a large, well-trained teacher model to a more compact student model. This paper introduces a novel knowledge distillation method that simplifies the distillation process and narrows the performance gap between teacher and student models without relying on intricate knowledge representations. Our approach leverages a unique teacher network architecture designed to enhance the efficiency and effectiveness of knowledge transfer. Additionally, we introduce a streamlined teacher network architecture that transfers knowledge effectively through a simplified distillation process, enabling the student model to achieve high accuracy with reduced computational demands. Comprehensive experiments conducted on the CIFAR-10 dataset demonstrate that our proposed model achieves superior performance compared to traditional KD methods and established architectures such as ResNet and VGG networks. The proposed method not only maintains high accuracy but also significantly reduces training and validation losses. Key findings highlight the optimal hyperparameter settings (temperature T = 15.0 and smoothing factor \u03b1 = 0.7), which yield the highest validation accuracy and lowest loss values. This research contributes to the theoretical and practical advancements in knowledge distillation, providing a robust framework for future applications and research in neural network compression and optimization. The simplicity and efficiency of our approach pave the way for more accessible and scalable solutions in deep learning model deployment.",
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "score": 0.0,
            "stype": "public_api",
            "pdf_hash": "",
            "rerank_score": 0.9677734375
        },
        {
            "paperId": "8432b60b2d2efbdba64898f210b516a5513ea05e",
            "corpusId": 270938890,
            "title": "Smaller and Faster Robotic Grasp Detection Model via Knowledge Distillation and Unequal Feature Encoding",
            "venue": "IEEE Robotics and Automation Letters",
            "year": 2024,
            "referenceCount": 33,
            "citationCount": 4,
            "influentialCitationCount": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": "CLOSED",
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.1109/LRA.2024.3421790?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1109/LRA.2024.3421790, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2309589207",
                    "name": "Hong Nie"
                },
                {
                    "authorId": "2224290299",
                    "name": "Zhou Zhao"
                },
                {
                    "authorId": "2266166718",
                    "name": "Lu Chen"
                },
                {
                    "authorId": "2259549003",
                    "name": "Zhenyu Lu"
                },
                {
                    "authorId": "2309773907",
                    "name": "Zhuomao Li"
                },
                {
                    "authorId": "2143850124",
                    "name": "Jing Yang"
                }
            ],
            "abstract": "In order to achieve higher accuracy, the complexity of grasp detection network increases accordingly with complicated model structures and tremendous parameters. Although various light-weight strategies are adopted, directly designing the compact network can be sub-optimal and difficult to strike the balance between accuracy and model size. To solve this problem, we explore a more efficient grasp detection model from two aspects: elaborately designing a light-weight network and performing knowledge distillation on the designed network. Specifically, based on the designed light-weight backbone, the features from RGB and D images with unequal effective grasping information rates are fully utilized and the information compensation strategies are adopted to make the model small enough while maintaining its accuracy. Then, the grasping features contained in the large teacher model are adaptively and effectively learned by our proposed method via knowledge distillation. Experimental results indicate that the proposed method is able to achieve comparable performance (98.9%, 93.1%, 82.3%, and 90.0% on Cornell, Jacquard, GraspNet, and MultiObj datasets, respectively) with more complicate models while reducing the parameters from MBs to KBs. Real-world robotic grasping experiment in an embedded AI computing device also prove the effectiveness of this approach.",
            "corpus_id": "270938890",
            "text": "In order to achieve higher accuracy, the complexity of grasp detection network increases accordingly with complicated model structures and tremendous parameters. Although various light-weight strategies are adopted, directly designing the compact network can be sub-optimal and difficult to strike the balance between accuracy and model size. To solve this problem, we explore a more efficient grasp detection model from two aspects: elaborately designing a light-weight network and performing knowledge distillation on the designed network. Specifically, based on the designed light-weight backbone, the features from RGB and D images with unequal effective grasping information rates are fully utilized and the information compensation strategies are adopted to make the model small enough while maintaining its accuracy. Then, the grasping features contained in the large teacher model are adaptively and effectively learned by our proposed method via knowledge distillation. Experimental results indicate that the proposed method is able to achieve comparable performance (98.9%, 93.1%, 82.3%, and 90.0% on Cornell, Jacquard, GraspNet, and MultiObj datasets, respectively) with more complicate models while reducing the parameters from MBs to KBs. Real-world robotic grasping experiment in an embedded AI computing device also prove the effectiveness of this approach.",
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "score": 0.0,
            "stype": "public_api",
            "pdf_hash": "",
            "rerank_score": 0.8447265625
        },
        {
            "paperId": "85a3f13e378d5e526f8c1e7997dd5e315c3c06ea",
            "corpusId": 268729379,
            "title": "Lightweight Knowledge Distillation-Based Transfer Learning Framework for Rolling Bearing Fault Diagnosis",
            "venue": "Italian National Conference on Sensors",
            "year": 2024,
            "referenceCount": 31,
            "citationCount": 5,
            "influentialCitationCount": 0,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://www.mdpi.com/1424-8220/24/6/1758/pdf?version=1709907686",
                "status": "GOLD",
                "license": "CCBY",
                "disclaimer": "Notice: Paper or abstract available at https://pmc.ncbi.nlm.nih.gov/articles/PMC10974163, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2293751671",
                    "name": "Ruijia Lu"
                },
                {
                    "authorId": "2293765238",
                    "name": "Shuzhi Liu"
                },
                {
                    "authorId": "2266524608",
                    "name": "Zisu Gong"
                },
                {
                    "authorId": "2293706631",
                    "name": "Chengcheng Xu"
                },
                {
                    "authorId": "2293772026",
                    "name": "Zonghe Ma"
                },
                {
                    "authorId": "2294343765",
                    "name": "Yiqi Zhong"
                },
                {
                    "authorId": "2293687582",
                    "name": "Baojian Li"
                }
            ],
            "abstract": "Compared to fault diagnosis across operating conditions, the differences in data distribution between devices are more pronounced and better aligned with practical application needs. However, current research on transfer learning inadequately addresses fault diagnosis issues across devices. To better balance the relationship between computational resources and diagnostic accuracy, a knowledge distillation-based lightweight transfer learning framework for rolling bearing diagnosis is proposed in this study. Specifically, a deep teacher\u2013student model based on variable-scale residual networks is constructed to learn domain-invariant features relevant to fault classification within both the source and target domain data. Subsequently, a knowledge distillation framework incorporating a temperature factor is established to transfer fault features learned by the large teacher model in the source domain to the smaller student model, thereby reducing computational and parameter overhead. Finally, a multi-kernel domain adaptation method is employed to capture the feature probability distribution distance of fault characteristics between the source and target domains in Reproducing Kernel Hilbert Space (RKHS), and domain-invariant features are learned by minimizing the distribution distance between them. The effectiveness and applicability of the proposed method in situations of incomplete data across device types were validated through two engineering cases, spanning device models and transitioning from laboratory equipment to real-world operational devices.",
            "corpus_id": "268729379",
            "text": "Compared to fault diagnosis across operating conditions, the differences in data distribution between devices are more pronounced and better aligned with practical application needs. However, current research on transfer learning inadequately addresses fault diagnosis issues across devices. To better balance the relationship between computational resources and diagnostic accuracy, a knowledge distillation-based lightweight transfer learning framework for rolling bearing diagnosis is proposed in this study. Specifically, a deep teacher\u2013student model based on variable-scale residual networks is constructed to learn domain-invariant features relevant to fault classification within both the source and target domain data. Subsequently, a knowledge distillation framework incorporating a temperature factor is established to transfer fault features learned by the large teacher model in the source domain to the smaller student model, thereby reducing computational and parameter overhead. Finally, a multi-kernel domain adaptation method is employed to capture the feature probability distribution distance of fault characteristics between the source and target domains in Reproducing Kernel Hilbert Space (RKHS), and domain-invariant features are learned by minimizing the distribution distance between them. The effectiveness and applicability of the proposed method in situations of incomplete data across device types were validated through two engineering cases, spanning device models and transitioning from laboratory equipment to real-world operational devices.",
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "score": 0.0,
            "stype": "public_api",
            "pdf_hash": "",
            "rerank_score": 0.83935546875
        },
        {
            "paperId": "e9c375b529b6ba28733989f6424dd1c6df0deb92",
            "corpusId": 272753601,
            "title": "Exploring and Enhancing the Transfer of Distribution in Knowledge Distillation for Autoregressive Language Models",
            "venue": "arXiv.org",
            "year": 2024,
            "referenceCount": 65,
            "citationCount": 2,
            "influentialCitationCount": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2409.12512, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2142545243",
                    "name": "Jun Rao"
                },
                {
                    "authorId": "2256344322",
                    "name": "Xuebo Liu"
                },
                {
                    "authorId": "2256943756",
                    "name": "Ze-Quan Lin"
                },
                {
                    "authorId": "46573238",
                    "name": "Liang Ding"
                },
                {
                    "authorId": "2322043811",
                    "name": "Jing Li"
                },
                {
                    "authorId": "2255502438",
                    "name": "D. Tao"
                },
                {
                    "authorId": "2269805934",
                    "name": "Min Zhang"
                }
            ],
            "abstract": "Knowledge distillation (KD) is a technique that compresses large teacher models by training smaller student models to mimic them. The success of KD in auto-regressive language models mainly relies on Reverse KL for mode-seeking and student-generated output (SGO) to combat exposure bias. Our theoretical analyses and experimental validation reveal that while Reverse KL effectively mimics certain features of the teacher distribution, it fails to capture most of its behaviors. Conversely, SGO incurs higher computational costs and presents challenges in optimization, particularly when the student model is significantly smaller than the teacher model. These constraints are primarily due to the immutable distribution of the teacher model, which fails to adjust adaptively to models of varying sizes. We introduce Online Knowledge Distillation (OKD), where the teacher network integrates small online modules to concurrently train with the student model. This strategy abolishes the necessity for on-policy sampling and merely requires minimal updates to the parameters of the teacher's online module during training, thereby allowing dynamic adaptation to the student's distribution to make distillation better. Extensive results across multiple generation datasets show that OKD achieves or exceeds the performance of leading methods in various model architectures and sizes, reducing training time by up to fourfold.",
            "corpus_id": "272753601",
            "text": "Knowledge distillation (KD) is a technique that compresses large teacher models by training smaller student models to mimic them. The success of KD in auto-regressive language models mainly relies on Reverse KL for mode-seeking and student-generated output (SGO) to combat exposure bias. Our theoretical analyses and experimental validation reveal that while Reverse KL effectively mimics certain features of the teacher distribution, it fails to capture most of its behaviors. Conversely, SGO incurs higher computational costs and presents challenges in optimization, particularly when the student model is significantly smaller than the teacher model. These constraints are primarily due to the immutable distribution of the teacher model, which fails to adjust adaptively to models of varying sizes. We introduce Online Knowledge Distillation (OKD), where the teacher network integrates small online modules to concurrently train with the student model. This strategy abolishes the necessity for on-policy sampling and merely requires minimal updates to the parameters of the teacher's online module during training, thereby allowing dynamic adaptation to the student's distribution to make distillation better. Extensive results across multiple generation datasets show that OKD achieves or exceeds the performance of leading methods in various model architectures and sizes, reducing training time by up to fourfold.",
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "score": 0.0,
            "stype": "public_api",
            "pdf_hash": "",
            "rerank_score": 0.9443359375
        },
        {
            "paperId": "679bbdb8a121af98b91b8504a3cc3679a3e17539",
            "corpusId": 257622781,
            "title": "Neural Architecture Search for Effective Teacher-Student Knowledge Transfer in Language Models",
            "venue": "arXiv.org",
            "year": 2023,
            "referenceCount": 65,
            "citationCount": 7,
            "influentialCitationCount": 0,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "http://arxiv.org/pdf/2303.09639",
                "status": "CLOSED",
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2303.09639, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "1999425183",
                    "name": "Aashka Trivedi"
                },
                {
                    "authorId": "80927455",
                    "name": "Takuma Udagawa"
                },
                {
                    "authorId": "2435677",
                    "name": "Michele Merler"
                },
                {
                    "authorId": "1819152",
                    "name": "Rameswar Panda"
                },
                {
                    "authorId": "1404362861",
                    "name": "Yousef El-Kurdi"
                },
                {
                    "authorId": "49223443",
                    "name": "Bishwaranjan Bhattacharjee"
                }
            ],
            "abstract": "Large pretrained language models have achieved state-of-the-art results on a variety of downstream tasks. Knowledge Distillation (KD) into a smaller student model addresses their inefficiency, allowing for deployment in resource-constrained environments. However, KD can be ineffective when the student is manually selected from a set of existing options, since it can be a sub-optimal choice within the space of all possible student architectures. We develop multilingual KD-NAS, the use of Neural Architecture Search (NAS) guided by KD to find the optimal student architecture for task agnostic distillation from a multilingual teacher. In each episode of the search process, a NAS controller predicts a reward based on the distillation loss and latency of inference. The top candidate architectures are then distilled from the teacher on a small proxy set. Finally the architecture(s) with the highest reward is selected, and distilled on the full training corpus. KD-NAS can automatically trade off efficiency and effectiveness, and recommends architectures suitable to various latency budgets. Using our multi-layer hidden state distillation process, our KD-NAS student model achieves a 7x speedup on CPU inference (2x on GPU) compared to a XLM-Roberta Base Teacher, while maintaining 90% performance, and has been deployed in 3 software offerings requiring large throughput, low latency and deployment on CPU.",
            "corpus_id": "257622781",
            "text": "Large pretrained language models have achieved state-of-the-art results on a variety of downstream tasks. Knowledge Distillation (KD) into a smaller student model addresses their inefficiency, allowing for deployment in resource-constrained environments. However, KD can be ineffective when the student is manually selected from a set of existing options, since it can be a sub-optimal choice within the space of all possible student architectures. We develop multilingual KD-NAS, the use of Neural Architecture Search (NAS) guided by KD to find the optimal student architecture for task agnostic distillation from a multilingual teacher. In each episode of the search process, a NAS controller predicts a reward based on the distillation loss and latency of inference. The top candidate architectures are then distilled from the teacher on a small proxy set. Finally the architecture(s) with the highest reward is selected, and distilled on the full training corpus. KD-NAS can automatically trade off efficiency and effectiveness, and recommends architectures suitable to various latency budgets. Using our multi-layer hidden state distillation process, our KD-NAS student model achieves a 7x speedup on CPU inference (2x on GPU) compared to a XLM-Roberta Base Teacher, while maintaining 90% performance, and has been deployed in 3 software offerings requiring large throughput, low latency and deployment on CPU.",
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "score": 0.0,
            "stype": "public_api",
            "pdf_hash": "",
            "rerank_score": 0.8017578125
        },
        {
            "paperId": "32ee7f6bdf77ddaf84b2ff380f62bda6903b4359",
            "corpusId": 272753230,
            "title": "Efficient Knowledge Distillation: Empowering Small Language Models with Teacher Model Insights",
            "venue": "International Conference on Applications of Natural Language to Data Bases",
            "year": 2024,
            "referenceCount": 38,
            "citationCount": 3,
            "influentialCitationCount": 1,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": "CLOSED",
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2409.12586, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "1491169373",
                    "name": "Mohamad Ballout"
                },
                {
                    "authorId": "1751765",
                    "name": "U. Krumnack"
                },
                {
                    "authorId": "2238205582",
                    "name": "Gunther Heidemann"
                },
                {
                    "authorId": "1743582",
                    "name": "Kai-Uwe K\u00fchnberger"
                }
            ],
            "abstract": "Enhancing small language models for real-life application deployment is a significant challenge facing the research community. Due to the difficulties and costs of using large language models, researchers are seeking ways to effectively deploy task-specific small models. In this work, we introduce a simple yet effective knowledge distillation method to improve the performance of small language models. Our approach utilizes a teacher model with approximately 3 billion parameters to identify the most influential tokens in its decision-making process. These tokens are extracted from the input based on their attribution scores relative to the output, using methods like saliency maps. These important tokens are then provided as rationales to a student model, aiming to distill the knowledge of the teacher model. This method has proven to be effective, as demonstrated by testing it on four diverse datasets, where it shows improvement over both standard fine-tuning methods and state-of-the-art knowledge distillation models. Furthermore, we explore explanations of the success of the model by analyzing the important tokens extracted from the teacher model. Our findings reveal that in 68\\% of cases, specifically in datasets where labels are part of the answer, such as multiple-choice questions, the extracted tokens are part of the ground truth.",
            "corpus_id": "272753230",
            "text": "Enhancing small language models for real-life application deployment is a significant challenge facing the research community. Due to the difficulties and costs of using large language models, researchers are seeking ways to effectively deploy task-specific small models. In this work, we introduce a simple yet effective knowledge distillation method to improve the performance of small language models. Our approach utilizes a teacher model with approximately 3 billion parameters to identify the most influential tokens in its decision-making process. These tokens are extracted from the input based on their attribution scores relative to the output, using methods like saliency maps. These important tokens are then provided as rationales to a student model, aiming to distill the knowledge of the teacher model. This method has proven to be effective, as demonstrated by testing it on four diverse datasets, where it shows improvement over both standard fine-tuning methods and state-of-the-art knowledge distillation models. Furthermore, we explore explanations of the success of the model by analyzing the important tokens extracted from the teacher model. Our findings reveal that in 68\\% of cases, specifically in datasets where labels are part of the answer, such as multiple-choice questions, the extracted tokens are part of the ground truth.",
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "score": 0.0,
            "stype": "public_api",
            "pdf_hash": "",
            "rerank_score": 0.9248046875
        },
        {
            "paperId": "07d46677c31a450fecbecd3e96b096fe250031b0",
            "corpusId": 270123376,
            "title": "GKT: A Novel Guidance-Based Knowledge Transfer Framework For Efficient Cloud-edge Collaboration LLM Deployment",
            "venue": "Annual Meeting of the Association for Computational Linguistics",
            "year": 2024,
            "referenceCount": 19,
            "citationCount": 6,
            "influentialCitationCount": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2405.19635, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2223290826",
                    "name": "Yao Yao"
                },
                {
                    "authorId": "30658665",
                    "name": "Z. Li"
                },
                {
                    "authorId": "2274113564",
                    "name": "Hai Zhao"
                }
            ],
            "abstract": "The burgeoning size of Large Language Models (LLMs) has led to enhanced capabilities in generating responses, albeit at the expense of increased inference times and elevated resource demands. Existing methods of acceleration, predominantly hinged on knowledge distillation, generally necessitate fine-tuning of considerably large models, such as Llama-7B, posing a challenge for average users. Furthermore, present techniques for expediting inference and reducing costs operate independently. To address these issues, we introduce a novel and intuitive Guidance-based Knowledge Transfer (GKT) framework. This approach leverages a larger LLM as a ''teacher'' to create guidance prompts, paired with a smaller ''student'' model to finalize responses. Remarkably, GKT requires no fine-tuning and doesn't necessitate the teacher and student models to have the same vocabulary, allowing for extensive batch generation to accelerate the process while ensuring user customization. GKT can be seamlessly integrated into cloud-edge collaboration architectures, and is versatile enough for plug-and-play application across various models. It excels in both efficiency and affordability, epitomizing a ''cheap and cheerful'' solution. GKT achieves a maximum accuracy improvement of 14.18%, along with a 10.72 times speed-up on GSM8K and an accuracy improvement of 14.00 % along with a 7.73 times speed-up in CSQA. When utilizing ChatGPT as teacher model and Llama2-70B as the student model, we can achieve 95.00% of ChatGPT's performance at 52% of the cost. The results highlight substantial enhancements in accuracy and processing speed on the GSM8K and CSQA datasets, surpassing the performance of using either the student or teacher models in isolation.",
            "corpus_id": "270123376",
            "text": "The burgeoning size of Large Language Models (LLMs) has led to enhanced capabilities in generating responses, albeit at the expense of increased inference times and elevated resource demands. Existing methods of acceleration, predominantly hinged on knowledge distillation, generally necessitate fine-tuning of considerably large models, such as Llama-7B, posing a challenge for average users. Furthermore, present techniques for expediting inference and reducing costs operate independently. To address these issues, we introduce a novel and intuitive Guidance-based Knowledge Transfer (GKT) framework. This approach leverages a larger LLM as a ''teacher'' to create guidance prompts, paired with a smaller ''student'' model to finalize responses. Remarkably, GKT requires no fine-tuning and doesn't necessitate the teacher and student models to have the same vocabulary, allowing for extensive batch generation to accelerate the process while ensuring user customization. GKT can be seamlessly integrated into cloud-edge collaboration architectures, and is versatile enough for plug-and-play application across various models. It excels in both efficiency and affordability, epitomizing a ''cheap and cheerful'' solution. GKT achieves a maximum accuracy improvement of 14.18%, along with a 10.72 times speed-up on GSM8K and an accuracy improvement of 14.00 % along with a 7.73 times speed-up in CSQA. When utilizing ChatGPT as teacher model and Llama2-70B as the student model, we can achieve 95.00% of ChatGPT's performance at 52% of the cost. The results highlight substantial enhancements in accuracy and processing speed on the GSM8K and CSQA datasets, surpassing the performance of using either the student or teacher models in isolation.",
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "score": 0.0,
            "stype": "public_api",
            "pdf_hash": "",
            "rerank_score": 0.317138671875
        },
        {
            "paperId": "60c4082d4805b7d4a0b03bffba28e59885d7cfea",
            "corpusId": 260857856,
            "title": "Teacher Assistant-Based Knowledge Distillation Extracting Multi-level Features on Single Channel Sleep EEG",
            "venue": "International Joint Conference on Artificial Intelligence",
            "year": 2023,
            "referenceCount": 59,
            "citationCount": 16,
            "influentialCitationCount": 0,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://www.ijcai.org/proceedings/2023/0439.pdf",
                "status": "BRONZE",
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.24963/ijcai.2023/439?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.24963/ijcai.2023/439, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2223264805",
                    "name": "Heng Liang"
                },
                {
                    "authorId": "2229488254",
                    "name": "Yucheng Liu"
                },
                {
                    "authorId": "2230251842",
                    "name": "Haichao Wang"
                },
                {
                    "authorId": "47301741",
                    "name": "Ziyu Jia"
                }
            ],
            "abstract": "Sleep stage classification is of great significance to the diagnosis of sleep disorders. However, existing sleep stage classification models based on deep learning are usually relatively large in size (wider and deeper), which makes them hard to be deployed on wearable devices. Therefore, it is a challenge to lighten the existing sleep stage classification models. In this paper, we propose a novel general knowledge distillation framework for sleep stage classification tasks called SleepKD. Our SleepKD, composed of the multi-level module, teacher assistant module, and other knowledge distillation modules, aims to lighten large-scale sleep stage classification models. Specifically, the multi-level module is able to transfer the multi-level knowledge extracted from sleep signals by the teacher model (large-scale model) to the student model (lightweight model). Moreover, the teacher assistant module bridges the large gap between the teacher and student network, and further improves the distillation. We evaluate our method on two public sleep datasets (Sleep-EDF and ISRUC-III). Compared to the baseline methods, the results show that our knowledge distillation framework achieves state-of-the-art performance. SleepKD can significantly lighten the sleep model while maintaining its classification performance. The source code is available at https://github.com/HychaoWang/SleepKD.",
            "corpus_id": "260857856",
            "text": "Sleep stage classification is of great significance to the diagnosis of sleep disorders. However, existing sleep stage classification models based on deep learning are usually relatively large in size (wider and deeper), which makes them hard to be deployed on wearable devices. Therefore, it is a challenge to lighten the existing sleep stage classification models. In this paper, we propose a novel general knowledge distillation framework for sleep stage classification tasks called SleepKD. Our SleepKD, composed of the multi-level module, teacher assistant module, and other knowledge distillation modules, aims to lighten large-scale sleep stage classification models. Specifically, the multi-level module is able to transfer the multi-level knowledge extracted from sleep signals by the teacher model (large-scale model) to the student model (lightweight model). Moreover, the teacher assistant module bridges the large gap between the teacher and student network, and further improves the distillation. We evaluate our method on two public sleep datasets (Sleep-EDF and ISRUC-III). Compared to the baseline methods, the results show that our knowledge distillation framework achieves state-of-the-art performance. SleepKD can significantly lighten the sleep model while maintaining its classification performance. The source code is available at https://github.com/HychaoWang/SleepKD.",
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "score": 0.0,
            "stype": "public_api",
            "pdf_hash": "",
            "rerank_score": 0.890625
        },
        {
            "paperId": "d6d4098729bc20fed48642125ee78850815aba72",
            "corpusId": 250916883,
            "title": "Knowledge Transfer and Distillation from Autoregressive to Non-Autoregressive Speech Recognition",
            "venue": "arXiv.org",
            "year": 2022,
            "referenceCount": 23,
            "citationCount": 4,
            "influentialCitationCount": 1,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "http://arxiv.org/pdf/2207.10600",
                "status": "GREEN",
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2207.10600, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2075408827",
                    "name": "Xun Gong"
                },
                {
                    "authorId": "2177344",
                    "name": "Zhikai Zhou"
                },
                {
                    "authorId": "2480051",
                    "name": "Y. Qian"
                }
            ],
            "abstract": "Modern non-autoregressive~(NAR) speech recognition systems aim to accelerate the inference speed; however, they suffer from performance degradation compared with autoregressive~(AR) models as well as the huge model size issue. We propose a novel knowledge transfer and distillation architecture that leverages knowledge from AR models to improve the NAR performance while reducing the model's size. Frame- and sequence-level objectives are well-designed for transfer learning. To further boost the performance of NAR, a beam search method on Mask-CTC is developed to enlarge the search space during the inference stage. Experiments show that the proposed NAR beam search relatively reduces CER by over 5% on AISHELL-1 benchmark with a tolerable real-time-factor~(RTF) increment. By knowledge transfer, the NAR student who has the same size as the AR teacher obtains relative CER reductions of 8/16% on AISHELL-1 dev/test sets, and over 25% relative WER reductions on LibriSpeech test-clean/other sets. Moreover, the ~9x smaller NAR models achieve ~25% relative CER/WER reductions on both AISHELL-1 and LibriSpeech benchmarks with the proposed knowledge transfer and distillation.",
            "corpus_id": "250916883",
            "text": "Modern non-autoregressive~(NAR) speech recognition systems aim to accelerate the inference speed; however, they suffer from performance degradation compared with autoregressive~(AR) models as well as the huge model size issue. We propose a novel knowledge transfer and distillation architecture that leverages knowledge from AR models to improve the NAR performance while reducing the model's size. Frame- and sequence-level objectives are well-designed for transfer learning. To further boost the performance of NAR, a beam search method on Mask-CTC is developed to enlarge the search space during the inference stage. Experiments show that the proposed NAR beam search relatively reduces CER by over 5% on AISHELL-1 benchmark with a tolerable real-time-factor~(RTF) increment. By knowledge transfer, the NAR student who has the same size as the AR teacher obtains relative CER reductions of 8/16% on AISHELL-1 dev/test sets, and over 25% relative WER reductions on LibriSpeech test-clean/other sets. Moreover, the ~9x smaller NAR models achieve ~25% relative CER/WER reductions on both AISHELL-1 and LibriSpeech benchmarks with the proposed knowledge transfer and distillation.",
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "score": 0.0,
            "stype": "public_api",
            "pdf_hash": "",
            "rerank_score": 0.62060546875
        },
        {
            "paperId": "52f05bf38c7d90d03f62ab6ecf7e9fceaf97fd9c",
            "corpusId": 258480106,
            "title": "Avatar Knowledge Distillation: Self-ensemble Teacher Paradigm with Uncertainty",
            "venue": "ACM Multimedia",
            "year": 2023,
            "referenceCount": 63,
            "citationCount": 8,
            "influentialCitationCount": 0,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://arxiv.org/pdf/2305.02722",
                "status": "GREEN",
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2305.02722, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "1703465",
                    "name": "Yuan Zhang"
                },
                {
                    "authorId": "2128193841",
                    "name": "Weihua Chen"
                },
                {
                    "authorId": "2156141201",
                    "name": "Yichen Lu"
                },
                {
                    "authorId": "2110813546",
                    "name": "Tao Huang"
                },
                {
                    "authorId": "2109166067",
                    "name": "Xiuyu Sun"
                },
                {
                    "authorId": "2125080516",
                    "name": "Jian Cao"
                }
            ],
            "abstract": "Knowledge distillation is an effective paradigm for boosting the performance of pocket-size model, especially when multiple teacher models are available, the student would break the upper limit again. However, it is not economical to train diverse teacher models for the disposable distillation. In this paper, we introduce a new concept dubbed Avatars for distillation, which are the inference ensemble models derived from the teacher. Concretely, (1) For each iteration of distillation training, various Avatars are generated by a perturbation transformation. We validate that Avatars own higher upper limit of working capacity and teaching ability, aiding the student model in learning diverse and receptive knowledge perspectives from the teacher model. (2) During the distillation, we propose an uncertainty-aware factor from the variance of statistical differences between the vanilla teacher and Avatars, to adjust Avatars' contribution on knowledge transfer adaptively. Avatar Knowledge Distillation (AKD) is fundamentally different from existing methods and refines with the innovative view of unequal training. Comprehensive experiments demonstrate the effectiveness of our Avatars mechanism, which polishes up the state-of-the-art distillation methods for dense prediction without more extra computational cost. The AKD brings at most 0.7 AP gains on COCO 2017 for Object Detection and 1.83 mIoU gains on Cityscapes for Semantic Segmentation, respectively.",
            "corpus_id": "258480106",
            "text": "Knowledge distillation is an effective paradigm for boosting the performance of pocket-size model, especially when multiple teacher models are available, the student would break the upper limit again. However, it is not economical to train diverse teacher models for the disposable distillation. In this paper, we introduce a new concept dubbed Avatars for distillation, which are the inference ensemble models derived from the teacher. Concretely, (1) For each iteration of distillation training, various Avatars are generated by a perturbation transformation. We validate that Avatars own higher upper limit of working capacity and teaching ability, aiding the student model in learning diverse and receptive knowledge perspectives from the teacher model. (2) During the distillation, we propose an uncertainty-aware factor from the variance of statistical differences between the vanilla teacher and Avatars, to adjust Avatars' contribution on knowledge transfer adaptively. Avatar Knowledge Distillation (AKD) is fundamentally different from existing methods and refines with the innovative view of unequal training. Comprehensive experiments demonstrate the effectiveness of our Avatars mechanism, which polishes up the state-of-the-art distillation methods for dense prediction without more extra computational cost. The AKD brings at most 0.7 AP gains on COCO 2017 for Object Detection and 1.83 mIoU gains on Cityscapes for Semantic Segmentation, respectively.",
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "score": 0.0,
            "stype": "public_api",
            "pdf_hash": "",
            "rerank_score": 0.7216796875
        },
        {
            "paperId": "5e1b76361163a2a61b8652d8769109b1fe1cd4b2",
            "corpusId": 221802641,
            "title": "Densely Guided Knowledge Distillation using Multiple Teacher Assistants",
            "venue": "IEEE International Conference on Computer Vision",
            "year": 2020,
            "referenceCount": 43,
            "citationCount": 119,
            "influentialCitationCount": 13,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "http://arxiv.org/pdf/2009.08825",
                "status": "GREEN",
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2009.08825, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "1557388434",
                    "name": "Wonchul Son"
                },
                {
                    "authorId": "107598106",
                    "name": "Jaemin Na"
                },
                {
                    "authorId": "34600044",
                    "name": "Wonjun Hwang"
                }
            ],
            "abstract": "With the success of deep neural networks, knowledge distillation which guides the learning of a small student network from a large teacher network is being actively studied for model compression and transfer learning. However, few studies have been performed to resolve the poor learning issue of the student network when the student and teacher model sizes significantly differ. In this paper, we propose a densely guided knowledge distillation using multiple teacher assistants that gradually decreases the model size to efficiently bridge the large gap between the teacher and student networks. To stimulate more efficient learning of the student network, we guide each teacher assistant to every other smaller teacher assistants iteratively. Specifically, when teaching a smaller teacher assistant at the next step, the existing larger teacher assistants from the previous step are used as well as the teacher network. Moreover, we design stochastic teaching where, for each mini-batch, a teacher or teacher assistants are randomly dropped. This acts as a regularizer to improve the efficiency of teaching of the student network. Thus, the student can always learn salient distilled knowledge from the multiple sources. We verified the effectiveness of the proposed method for a classification task using CIFAR-10, CIFAR-100, and ImageNet. We also achieved significant performance improvements with various backbone architectures such as ResNet, WideResNet, and VGG.1",
            "corpus_id": "221802641",
            "text": "With the success of deep neural networks, knowledge distillation which guides the learning of a small student network from a large teacher network is being actively studied for model compression and transfer learning. However, few studies have been performed to resolve the poor learning issue of the student network when the student and teacher model sizes significantly differ. In this paper, we propose a densely guided knowledge distillation using multiple teacher assistants that gradually decreases the model size to efficiently bridge the large gap between the teacher and student networks. To stimulate more efficient learning of the student network, we guide each teacher assistant to every other smaller teacher assistants iteratively. Specifically, when teaching a smaller teacher assistant at the next step, the existing larger teacher assistants from the previous step are used as well as the teacher network. Moreover, we design stochastic teaching where, for each mini-batch, a teacher or teacher assistants are randomly dropped. This acts as a regularizer to improve the efficiency of teaching of the student network. Thus, the student can always learn salient distilled knowledge from the multiple sources. We verified the effectiveness of the proposed method for a classification task using CIFAR-10, CIFAR-100, and ImageNet. We also achieved significant performance improvements with various backbone architectures such as ResNet, WideResNet, and VGG.1",
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "score": 0.0,
            "stype": "public_api",
            "pdf_hash": "",
            "rerank_score": 0.86572265625
        },
        {
            "paperId": "26deb576f902c99b165701e0bb013838fcf2c971",
            "corpusId": 260908241,
            "title": "Network Specialization via Feature-level Knowledge Distillation",
            "venue": "2023 IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops (CVPRW)",
            "year": 2023,
            "referenceCount": 39,
            "citationCount": 4,
            "influentialCitationCount": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": "CLOSED",
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.1109/CVPRW59228.2023.00339?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1109/CVPRW59228.2023.00339, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "3056447",
                    "name": "Gaowen Liu"
                },
                {
                    "authorId": "2125633957",
                    "name": "Yuzhang Shang"
                },
                {
                    "authorId": "100630765",
                    "name": "Yuguang Yao"
                },
                {
                    "authorId": "2828296",
                    "name": "R. Kompella"
                }
            ],
            "abstract": "State-of-the-art model specialization methods are mainly based on fine-tuning a pre-trained machine learning model to fit the specific needs of a particular task or application. Or by modifying the architecture of the model itself. However, these methods are not preferable in industrial applications because of the model\u2019s large size and the complexity of the training process. In this paper, the difficulty of network specialization is attributed to overfitting caused by a lack of data, and we propose a novel model specialization method by Knowledge Distillation (SKD). The proposed methods merge transfer learning and model compression into one stage. Specifically, we distill and transfer knowledge at the feature map level, circumventing logit-level inconsistency between teacher and student. We empirically investigate and prove the effects of the three parts: Models can be specialized to customer use cases by knowledge distillation. Knowledge distillation can effectively regularize the knowledge transfer process to a smaller, task-specific model. Compared with classical methods such as training a model from scratch and model fine-tuning, our methods achieve comparable and much better results and have better training efficiency on the CIFAR-100 dataset for image classification tasks. This paper proves the great potential of model specialization by knowledge distillation.",
            "corpus_id": "260908241",
            "text": "State-of-the-art model specialization methods are mainly based on fine-tuning a pre-trained machine learning model to fit the specific needs of a particular task or application. Or by modifying the architecture of the model itself. However, these methods are not preferable in industrial applications because of the model\u2019s large size and the complexity of the training process. In this paper, the difficulty of network specialization is attributed to overfitting caused by a lack of data, and we propose a novel model specialization method by Knowledge Distillation (SKD). The proposed methods merge transfer learning and model compression into one stage. Specifically, we distill and transfer knowledge at the feature map level, circumventing logit-level inconsistency between teacher and student. We empirically investigate and prove the effects of the three parts: Models can be specialized to customer use cases by knowledge distillation. Knowledge distillation can effectively regularize the knowledge transfer process to a smaller, task-specific model. Compared with classical methods such as training a model from scratch and model fine-tuning, our methods achieve comparable and much better results and have better training efficiency on the CIFAR-100 dataset for image classification tasks. This paper proves the great potential of model specialization by knowledge distillation.",
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "score": 0.0,
            "stype": "public_api",
            "pdf_hash": "",
            "rerank_score": 0.96826171875
        },
        {
            "paperId": "db3010eedc3a19ed1b4ae06f9006179db05f34bc",
            "corpusId": 215745611,
            "title": "Knowledge Distillation and Student-Teacher Learning for Visual Intelligence: A Review and New Outlooks",
            "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence",
            "year": 2020,
            "referenceCount": 314,
            "citationCount": 700,
            "influentialCitationCount": 16,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://arxiv.org/pdf/2004.05937",
                "status": "GREEN",
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2004.05937, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2144734901",
                    "name": "Lin Wang"
                },
                {
                    "authorId": "51182421",
                    "name": "Kuk-Jin Yoon"
                }
            ],
            "abstract": "Deep neural models, in recent years, have been successful in almost every field, even solving the most complex problem statements. However, these models are huge in size with millions (and even billions) of parameters, demanding heavy computation power and failing to be deployed on edge devices. Besides, the performance boost is highly dependent on redundant labeled data. To achieve faster speeds and to handle the problems caused by the lack of labeled data, knowledge distillation (KD) has been proposed to transfer information learned from one model to another. KD is often characterized by the so-called \u2018Student-Teacher\u2019 (S-T) learning framework and has been broadly applied in model compression and knowledge transfer. This paper is about KD and S-T learning, which are being actively studied in recent years. First, we aim to provide explanations of what KD is and how/why it works. Then, we provide a comprehensive survey on the recent progress of KD methods together with S-T frameworks typically used for vision tasks. In general, we investigate some fundamental questions that have been driving this research area and thoroughly generalize the research progress and technical details. Additionally, we systematically analyze the research status of KD in vision applications. Finally, we discuss the potentials and open challenges of existing methods and prospect the future directions of KD and S-T learning.",
            "corpus_id": "215745611",
            "text": "Deep neural models, in recent years, have been successful in almost every field, even solving the most complex problem statements. However, these models are huge in size with millions (and even billions) of parameters, demanding heavy computation power and failing to be deployed on edge devices. Besides, the performance boost is highly dependent on redundant labeled data. To achieve faster speeds and to handle the problems caused by the lack of labeled data, knowledge distillation (KD) has been proposed to transfer information learned from one model to another. KD is often characterized by the so-called \u2018Student-Teacher\u2019 (S-T) learning framework and has been broadly applied in model compression and knowledge transfer. This paper is about KD and S-T learning, which are being actively studied in recent years. First, we aim to provide explanations of what KD is and how/why it works. Then, we provide a comprehensive survey on the recent progress of KD methods together with S-T frameworks typically used for vision tasks. In general, we investigate some fundamental questions that have been driving this research area and thoroughly generalize the research progress and technical details. Additionally, we systematically analyze the research status of KD in vision applications. Finally, we discuss the potentials and open challenges of existing methods and prospect the future directions of KD and S-T learning.",
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "score": 0.0,
            "stype": "public_api",
            "pdf_hash": "",
            "rerank_score": 0.89501953125
        },
        {
            "paperId": "58c7aed0229d8709718321f2d38a93a192b1e416",
            "corpusId": 275932454,
            "title": "TAID: Temporally Adaptive Interpolated Distillation for Efficient Knowledge Transfer in Language Models",
            "venue": "International Conference on Learning Representations",
            "year": 2025,
            "referenceCount": 60,
            "citationCount": 4,
            "influentialCitationCount": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2501.16937, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2205654861",
                    "name": "Makoto Shing"
                },
                {
                    "authorId": "2342505900",
                    "name": "Kou Misaki"
                },
                {
                    "authorId": "2328978713",
                    "name": "Han Bao"
                },
                {
                    "authorId": "2328977540",
                    "name": "Sho Yokoi"
                },
                {
                    "authorId": "2292261829",
                    "name": "Takuya Akiba"
                }
            ],
            "abstract": "Causal language models have demonstrated remarkable capabilities, but their size poses significant challenges for deployment in resource-constrained environments. Knowledge distillation, a widely-used technique for transferring knowledge from a large teacher model to a small student model, presents a promising approach for model compression. A significant remaining issue lies in the major differences between teacher and student models, namely the substantial capacity gap, mode averaging, and mode collapse, which pose barriers during distillation. To address these issues, we introduce $\\textit{Temporally Adaptive Interpolated Distillation (TAID)}$, a novel knowledge distillation approach that dynamically interpolates student and teacher distributions through an adaptive intermediate distribution, gradually shifting from the student's initial distribution towards the teacher's distribution. We provide a theoretical analysis demonstrating TAID's ability to prevent mode collapse and empirically show its effectiveness in addressing the capacity gap while balancing mode averaging and mode collapse. Our comprehensive experiments demonstrate TAID's superior performance across various model sizes and architectures in both instruction tuning and pre-training scenarios. Furthermore, we showcase TAID's practical impact by developing two state-of-the-art compact foundation models: $\\texttt{TAID-LLM-1.5B}$ for language tasks and $\\texttt{TAID-VLM-2B}$ for vision-language tasks. These results demonstrate TAID's effectiveness in creating high-performing and efficient models, advancing the development of more accessible AI technologies.",
            "corpus_id": "275932454",
            "text": "Causal language models have demonstrated remarkable capabilities, but their size poses significant challenges for deployment in resource-constrained environments. Knowledge distillation, a widely-used technique for transferring knowledge from a large teacher model to a small student model, presents a promising approach for model compression. A significant remaining issue lies in the major differences between teacher and student models, namely the substantial capacity gap, mode averaging, and mode collapse, which pose barriers during distillation. To address these issues, we introduce $\\textit{Temporally Adaptive Interpolated Distillation (TAID)}$, a novel knowledge distillation approach that dynamically interpolates student and teacher distributions through an adaptive intermediate distribution, gradually shifting from the student's initial distribution towards the teacher's distribution. We provide a theoretical analysis demonstrating TAID's ability to prevent mode collapse and empirically show its effectiveness in addressing the capacity gap while balancing mode averaging and mode collapse. Our comprehensive experiments demonstrate TAID's superior performance across various model sizes and architectures in both instruction tuning and pre-training scenarios. Furthermore, we showcase TAID's practical impact by developing two state-of-the-art compact foundation models: $\\texttt{TAID-LLM-1.5B}$ for language tasks and $\\texttt{TAID-VLM-2B}$ for vision-language tasks. These results demonstrate TAID's effectiveness in creating high-performing and efficient models, advancing the development of more accessible AI technologies.",
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "score": 0.0,
            "stype": "public_api",
            "pdf_hash": "",
            "rerank_score": 0.923828125
        }
    ],
    "quotes": {
        "cost": 0.24414000000000002,
        "quotes": [
            {
                "idx": 0,
                "key": "[174800781 | Wei et al. | 2019 | Citations: 32]",
                "snippets": "Knowledge distillation is a class of methods which transfers knowledge from a pre-trained teacher model T, to a student model S. The teacher model can be a model with large capacity (Bucila et al., 2006) or an ensemble of several models (Hinton et al., 2015).",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {},
                "metadata": [
                    {
                        "section_title": "Knowledge Distillation in Neural Machine Translation",
                        "pdf_hash": "",
                        "start": 0,
                        "end": 259,
                        "sentence_offsets": [
                            {
                                "start": 0,
                                "end": 260
                            }
                        ],
                        "ref_mentions": [
                            "11253972"
                        ],
                        "quote": "Knowledge distillation is a class of methods which transfers knowledge from a pre-trained teacher model T, to a student model S. The teacher model can be a model with large capacity (Bucila et al., 2006) or an ensemble of several models (Hinton et al., 2015)."
                    }
                ]
            },
            {
                "idx": 1,
                "key": "[221703021 | Liu et al. | 2020 | Citations: 44]",
                "snippets": "Knowledge distillation refers to a class of methods for training a new smaller student network by learning from a teacher network (in addition to learning from the training data). It is generally assumed that the teacher has been previously trained, and the parameters for the student are estimated by matching the student's predictions to the teacher...Recent work (Furlanello et al., 2018; Hahn and Choi, 2019) also sheds light on leveraging knowledge distillation for training a high-performing student model with the same size as the teacher",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {
                    "[31009408 | Teh et al. | 2017 | Citations: 554]": "Most deep reinforcement learning algorithms are data inefficient in complex and rich environments, limiting their applicability to many scenarios. One direction for improving data efficiency is multitask learning with shared neural network parameters, where efficiency may be improved through transfer across related tasks. In practice, however, this is not usually observed, because gradients from different tasks can interfere negatively, making learning unstable and sometimes even less data efficient. Another issue is the different reward schemes between tasks, which can easily lead to one task dominating the learning of a shared model. We propose a new approach for joint training of multiple tasks, which we refer to as Distral (Distill & transfer learning). Instead of sharing parameters between the different workers, we propose to share a \"distilled\" policy that captures common behaviour across tasks. Each worker is trained to solve its own task while constrained to stay close to the shared policy, while the shared policy is trained by distillation to be the centroid of all task policies. Both aspects of the learning process are derived by optimizing a joint objective function. We show that our approach supports efficient transfer on complex 3D environments, outperforming several related methods. Moreover, the proposed learning process is more robust and more stable---attributes that are critical in deep reinforcement learning."
                },
                "metadata": [
                    {
                        "section_title": "Knowledge Distillation",
                        "pdf_hash": "",
                        "start": 0,
                        "end": 351,
                        "sentence_offsets": [],
                        "ref_mentions": [],
                        "quote": "Knowledge distillation refers to a class of methods for training a new smaller student network by learning from a teacher network (in addition to learning from the training data). It is generally assumed that the teacher has been previously trained, and the parameters for the student are estimated by matching the student's predictions to the teacher"
                    },
                    {
                        "section_title": "Knowledge Distillation",
                        "pdf_hash": "",
                        "start": 1306,
                        "end": 1498,
                        "sentence_offsets": [],
                        "ref_mentions": [
                            "31009408"
                        ],
                        "quote": "Recent work (Furlanello et al., 2018; Hahn and Choi, 2019) also sheds light on leveraging knowledge distillation for training a high-performing student model with the same size as the teacher"
                    }
                ]
            },
            {
                "idx": 2,
                "key": "[227228204 | Ku et al. | 2020 | Citations: 3]",
                "snippets": "Knowledge distillation is an effective model compression technique in which a compact model (student) is trained under the supervision of a larger pre-trained model or an ensemble of models (teacher).",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {},
                "metadata": [
                    {
                        "section_title": "II. KNOWLEDGE DISTILLATION",
                        "pdf_hash": "",
                        "start": 0,
                        "end": 200,
                        "sentence_offsets": [
                            {
                                "start": 0,
                                "end": 200
                            }
                        ],
                        "ref_mentions": [],
                        "quote": "Knowledge distillation is an effective model compression technique in which a compact model (student) is trained under the supervision of a larger pre-trained model or an ensemble of models (teacher)."
                    }
                ]
            },
            {
                "idx": 3,
                "key": "[232076059 | Keser et al. | 2021 | Citations: 7]",
                "snippets": "Knowledge distillation (KD) is the process of transferring knowledge between networks, where one usually aims to transfer the knowledge of a big network (teacher) to a smaller/more compact network (student).",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {},
                "metadata": [
                    {
                        "section_title": "Knowledge Distillation",
                        "pdf_hash": "",
                        "start": 0,
                        "end": 207,
                        "sentence_offsets": [
                            {
                                "start": 0,
                                "end": 207
                            }
                        ],
                        "ref_mentions": [],
                        "quote": "Knowledge distillation (KD) is the process of transferring knowledge between networks, where one usually aims to transfer the knowledge of a big network (teacher) to a smaller/more compact network (student)."
                    }
                ]
            },
            {
                "idx": 4,
                "key": "[232380330 | Li et al. | 2021 | Citations: 47]",
                "snippets": "Traditional Knowledge Distillation. Traditional distillation works transfer knowledge from a cumbersome teacher model to a light-weight student model. As such, a large-scale model has to be trained in advance, based on which various knowledge definitions and transfer strategies are proposed to boost the performance of the student model.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {},
                "metadata": [
                    {
                        "section_title": "II. RELATED WORK",
                        "pdf_hash": "",
                        "start": 0,
                        "end": 338,
                        "sentence_offsets": [
                            {
                                "start": 0,
                                "end": 35
                            },
                            {
                                "start": 36,
                                "end": 150
                            },
                            {
                                "start": 151,
                                "end": 338
                            }
                        ],
                        "ref_mentions": [],
                        "quote": "Traditional Knowledge Distillation. Traditional distillation works transfer knowledge from a cumbersome teacher model to a light-weight student model. As such, a large-scale model has to be trained in advance, based on which various knowledge definitions and transfer strategies are proposed to boost the performance of the student model."
                    }
                ]
            },
            {
                "idx": 5,
                "key": "[234336288 | Jaiswal et al. | 2021 | Citations: 0]",
                "snippets": "Knowledge distillation (KD) was introduced by [30] as: \n\n\u2022 Train a large model that performs and generalizes very well. This is called the teacher model. \n\n\u2022 Take all the data you have and compute the predictions of the teacher model. The total dataset with these predictions is called the knowledge, and the predictions themselves are often referred to as soft targets. This is the knowledge distillation step. \n\n\u2022 Use the previously obtained knowledge to train the smaller network, called the student model.\n\nKnowledge distillation starts with training a larger model, the teacher 'T'. As it is trained on a heavier platform (GPU), it achieves high performance. Then a lightweight model known as student 'S' is deployed to learn from 'T'. Now, 'S' is supposed to give comparable performance as 'T' but with less memory and more speed.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {},
                "metadata": [
                    {
                        "quote": "Knowledge distillation (KD) was introduced by [30] as: \n\n\u2022 Train a large model that performs and generalizes very well. This is called the teacher model. \n\n\u2022 Take all the data you have and compute the predictions of the teacher model. The total dataset with these predictions is called the knowledge, and the predictions themselves are often referred to as soft targets. This is the knowledge distillation step. \n\n\u2022 Use the previously obtained knowledge to train the smaller network, called the student model.\n\nKnowledge distillation starts with training a larger model, the teacher 'T'. As it is trained on a heavier platform (GPU), it achieves high performance. Then a lightweight model known as student 'S' is deployed to learn from 'T'. Now, 'S' is supposed to give comparable performance as 'T' but with less memory and more speed.",
                        "pdf_hash": ""
                    }
                ]
            },
            {
                "idx": 6,
                "key": "[236976374 | He et al. | 2021 | Citations: 17]",
                "snippets": "The learning of a small network from a large network is later formally popularized as vanilla knowledge distillation (KD) [17] where a small student model is generally supervised by a large teacher model. The main idea is that the student model mimics the teacher model to achieve competitive or even superior performance.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {},
                "metadata": [
                    {
                        "section_title": "Knowledge Distillation",
                        "pdf_hash": "",
                        "start": 0,
                        "end": 322,
                        "sentence_offsets": [
                            {
                                "start": 0,
                                "end": 204
                            },
                            {
                                "start": 205,
                                "end": 322
                            }
                        ],
                        "ref_mentions": [],
                        "quote": "The learning of a small network from a large network is later formally popularized as vanilla knowledge distillation (KD) [17] where a small student model is generally supervised by a large teacher model. The main idea is that the student model mimics the teacher model to achieve competitive or even superior performance."
                    }
                ]
            },
            {
                "idx": 7,
                "key": "[243861089 | Xu et al. | 2021 | Citations: 84]",
                "snippets": "The idea of knowledge distillation (KD) is exploiting the knowledge inside a large trained \"teacher\" model to help the training of a \"student\" model (Bucila et al., 2006)(Ba et al., 2013)Hinton et al., 2015). In this way, we can use a smaller student model to distill a trained model as a replacement for inference.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {
                    "[11536917 | Ba et al. | 2013 | Citations: 2119]": "Currently, deep neural networks are the state of the art on problems such as speech recognition and computer vision. In this paper we empirically demonstrate that shallow feed-forward nets can learn the complex functions previously learned by deep nets and achieve accuracies previously only achievable with deep models. Moreover, in some cases the shallow nets can learn these deep functions using the same number of parameters as the original deep models. On the TIMIT phoneme recognition and CIFAR-10 image recognition tasks, shallow nets can be trained that perform similarly to complex, well-engineered, deeper convolutional models."
                },
                "metadata": [
                    {
                        "section_title": "Knowledge Distillation",
                        "pdf_hash": "",
                        "start": 0,
                        "end": 317,
                        "sentence_offsets": [],
                        "ref_mentions": [
                            "11253972",
                            "11536917"
                        ],
                        "quote": "The idea of knowledge distillation (KD) is exploiting the knowledge inside a large trained \"teacher\" model to help the training of a \"student\" model (Bucila et al., 2006)(Ba et al., 2013)Hinton et al., 2015). In this way, we can use a smaller student model to distill a trained model as a replacement for inference."
                    }
                ]
            },
            {
                "idx": 8,
                "key": "[244488325 | Liu et al. | 2021 | Citations: 5]",
                "snippets": "Knowledge distillation is an effective and stable method for model compression via knowledge transfer. Conventional knowledge distillation (KD) is to transfer knowledge from a large and well pre-trained teacher network to a small student network, which is a one-way process.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {},
                "metadata": [
                    {
                        "section_title": "abstract",
                        "pdf_hash": "",
                        "start": 0,
                        "end": 274,
                        "sentence_offsets": [],
                        "ref_mentions": [],
                        "quote": "Knowledge distillation is an effective and stable method for model compression via knowledge transfer. Conventional knowledge distillation (KD) is to transfer knowledge from a large and well pre-trained teacher network to a small student network, which is a one-way process."
                    }
                ]
            },
            {
                "idx": 9,
                "key": "[245974615 | Ganta et al. | 2021 | Citations: 4]",
                "snippets": "The main idea of using knowledge distillation is that student network (S) to be trained not only using the true labels information but also observation of how the teacher (T) works with the unseen data provided. Because the teacher model has more more generalization power, the idea is to train the student model is such way that it can mimic the behaviour of that generalization. The teacher network is complex in size being deeper and wider.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {},
                "metadata": [
                    {
                        "section_title": "B. Knowledge distillation",
                        "pdf_hash": "",
                        "start": 877,
                        "end": 1320,
                        "sentence_offsets": [
                            {
                                "start": 877,
                                "end": 1088
                            },
                            {
                                "start": 1089,
                                "end": 1257
                            },
                            {
                                "start": 1258,
                                "end": 1320
                            }
                        ],
                        "ref_mentions": [],
                        "quote": "The main idea of using knowledge distillation is that student network (S) to be trained not only using the true labels information but also observation of how the teacher (T) works with the unseen data provided. Because the teacher model has more more generalization power, the idea is to train the student model is such way that it can mimic the behaviour of that generalization. The teacher network is complex in size being deeper and wider."
                    }
                ]
            },
            {
                "idx": 10,
                "key": "[248683566 | Xu et al. | 2022 | Citations: 44]",
                "snippets": "Knowledge distillation (KD), an important method of model compression (Tan et al., 2019)(Cheng et al., 2018)(Bashir et al., 2020), is effective in transferring \"dark knowledge\" from a larger model to a smaller model, allowing the smaller model to approximate the performance level achieved by the larger model (Yim et al., 2017)(Kim et al., 2016)(Gou et al., 2020).",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {
                    "[167217261 | Tan et al. | 2019 | Citations: 18189]": "Convolutional Neural Networks (ConvNets) are commonly developed at a fixed resource budget, and then scaled up for better accuracy if more resources are available. In this paper, we systematically study model scaling and identify that carefully balancing network depth, width, and resolution can lead to better performance. Based on this observation, we propose a new scaling method that uniformly scales all dimensions of depth/width/resolution using a simple yet highly effective compound coefficient. We demonstrate the effectiveness of this method on scaling up MobileNets and ResNet. \nTo go even further, we use neural architecture search to design a new baseline network and scale it up to obtain a family of models, called EfficientNets, which achieve much better accuracy and efficiency than previous ConvNets. In particular, our EfficientNet-B7 achieves state-of-the-art 84.4% top-1 / 97.1% top-5 accuracy on ImageNet, while being 8.4x smaller and 6.1x faster on inference than the best existing ConvNet. Our EfficientNets also transfer well and achieve state-of-the-art accuracy on CIFAR-100 (91.7%), Flowers (98.8%), and 3 other transfer learning datasets, with an order of magnitude fewer parameters. Source code is at this https URL.",
                    "[219559263 | Gou et al. | 2020 | Citations: 2984]": "In recent years, deep neural networks have been successful in both industry and academia, especially for computer vision tasks. The great success of deep learning is mainly due to its scalability to encode large-scale data and to maneuver billions of model parameters. However, it is a challenge to deploy these cumbersome deep models on devices with limited resources, e.g., mobile phones and embedded devices, not only because of the high computational complexity but also the large storage requirements. To this end, a variety of model compression and acceleration techniques have been developed. As a representative type of model compression and acceleration, knowledge distillation effectively learns a small student model from a large teacher model. It has received rapid increasing attention from the community. This paper provides a comprehensive survey of knowledge distillation from the perspectives of knowledge categories, training schemes, teacher\u2013student architecture, distillation algorithms, performance comparison and applications. Furthermore, challenges in knowledge distillation are briefly reviewed and comments on future research are discussed and forwarded.",
                    "[222310537 | Bashir et al. | 2020 | Citations: 43]": "We present an information-theoretic framework for understanding overfitting and underfitting in machine learning and prove the formal undecidability of determining whether an arbitrary classification algorithm will overfit a dataset. Measuring algorithm capacity via the information transferred from datasets to models, we consider mismatches between algorithm capacities and datasets to provide a signature for when a model can overfit or underfit a dataset. We present results upper-bounding algorithm capacity, establish its relationship to quantities in the algorithmic search framework for machine learning, and relate our work to recent information-theoretic approaches to generalization.",
                    "[8451212 | Kim et al. | 2016 | Citations: 1123]": "Neural machine translation (NMT) offers a novel alternative formulation of translation that is potentially simpler than statistical approaches. However to reach competitive performance, NMT models need to be exceedingly large. In this paper we consider applying knowledge distillation approaches (Bucila et al., 2006; Hinton et al., 2015) that have proven successful for reducing the size of neural models in other domains to the problem of NMT. We demonstrate that standard knowledge distillation applied to word-level prediction can be effective for NMT, and also introduce two novel sequence-level versions of knowledge distillation that further improve performance, and somewhat surprisingly, seem to eliminate the need for beam search (even when applied on the original teacher model). Our best student model runs 10 times faster than its state-of-the-art teacher with little loss in performance. It is also significantly better than a baseline model trained without knowledge distillation: by 4.2/1.7 BLEU with greedy decoding/beam search. Applying weight pruning on top of knowledge distillation results in a student model that has 13 times fewer parameters than the original teacher model, with a decrease of 0.4 BLEU."
                },
                "metadata": [
                    {
                        "section_title": "Knowledge distillation",
                        "pdf_hash": "",
                        "start": 0,
                        "end": 270,
                        "sentence_offsets": [
                            {
                                "start": 0,
                                "end": 270
                            }
                        ],
                        "ref_mentions": [
                            "167217261",
                            "32588614",
                            "222310537",
                            "206596723",
                            "8451212",
                            "219559263"
                        ],
                        "quote": "Knowledge distillation (KD), an important method of model compression (Tan et al., 2019)(Cheng et al., 2018)(Bashir et al., 2020), is effective in transferring \"dark knowledge\" from a larger model to a smaller model, allowing the smaller model to approximate the performance level achieved by the larger model (Yim et al., 2017)(Kim et al., 2016)(Gou et al., 2020)."
                    }
                ]
            },
            {
                "idx": 11,
                "key": "[250264223 | Yang et al. | 2022 | Citations: 3]",
                "snippets": "Knowledge distillation (Hinton et al., 2015)(Furlanello et al., 2018)Phuong & Lampert, 2019;Allen-Zhu & Li, 2020) is an important technique to transfer prior knowledge from a pre-trained neural network (a.k.a. teacher network) to another network (a.k.a. student network), with the same or different architectures. Typically, given an (possibly unlabled) input, knowledge distillation is performed by matching the output of the student network with the output of the teacher network.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {
                    "[4110009 | Furlanello et al. | 2018 | Citations: 1034]": "Knowledge distillation (KD) consists of transferring knowledge from one machine learning model (the teacher}) to another (the student). Commonly, the teacher is a high-capacity model with formidable performance, while the student is more compact. By transferring knowledge, one hopes to benefit from the student's compactness. %we desire a compact model with performance close to the teacher's. We study KD from a new perspective: rather than compressing models, we train students parameterized identically to their teachers. Surprisingly, these {Born-Again Networks (BANs), outperform their teachers significantly, both on computer vision and language modeling tasks. Our experiments with BANs based on DenseNets demonstrate state-of-the-art performance on the CIFAR-10 (3.5%) and CIFAR-100 (15.5%) datasets, by validation error. Additional experiments explore two distillation objectives: (i) Confidence-Weighted by Teacher Max (CWTM) and (ii) Dark Knowledge with Permuted Predictions (DKPP). Both methods elucidate the essential components of KD, demonstrating a role of the teacher outputs on both predicted and non-predicted classes. We present experiments with students of various capacities, focusing on the under-explored case where students overpower teachers. Our experiments show significant advantages from transferring knowledge between DenseNets and ResNets in either direction.",
                    "[7200347 | Hinton et al. | 2015 | Citations: 19742]": "A very simple way to improve the performance of almost any machine learning algorithm is to train many different models on the same data and then to average their predictions. Unfortunately, making predictions using a whole ensemble of models is cumbersome and may be too computationally expensive to allow deployment to a large number of users, especially if the individual models are large neural nets. Caruana and his collaborators have shown that it is possible to compress the knowledge in an ensemble into a single model which is much easier to deploy and we develop this approach further using a different compression technique. We achieve some surprising results on MNIST and we show that we can significantly improve the acoustic model of a heavily used commercial system by distilling the knowledge in an ensemble of models into a single model. We also introduce a new type of ensemble composed of one or more full models and many specialist models which learn to distinguish fine-grained classes that the full models confuse. Unlike a mixture of experts, these specialist models can be trained rapidly and in parallel."
                },
                "metadata": [
                    {
                        "section_title": "F.4. Knowledge distillation and transfer",
                        "pdf_hash": "",
                        "start": 0,
                        "end": 481,
                        "sentence_offsets": [
                            {
                                "start": 0,
                                "end": 208
                            },
                            {
                                "start": 209,
                                "end": 252
                            },
                            {
                                "start": 253,
                                "end": 312
                            },
                            {
                                "start": 313,
                                "end": 481
                            }
                        ],
                        "ref_mentions": [
                            "7200347",
                            "4110009"
                        ],
                        "quote": "Knowledge distillation (Hinton et al., 2015)(Furlanello et al., 2018)Phuong & Lampert, 2019;Allen-Zhu & Li, 2020) is an important technique to transfer prior knowledge from a pre-trained neural network (a.k.a. teacher network) to another network (a.k.a. student network), with the same or different architectures. Typically, given an (possibly unlabled) input, knowledge distillation is performed by matching the output of the student network with the output of the teacher network."
                    }
                ]
            },
            {
                "idx": 12,
                "key": "[250526328 | Liu et al. | 2022 | Citations: 6]",
                "snippets": "Although more layers and more parameters generally improve the accuracy of the models, such big models generally have high computational complexity and require big memory, which exceed the capacity of small devices for inference and incurs long training time...Knowledge distillation is based on the popular machine learning Softmax function and a temperature (Hinton et al., 2015) , which is defined in Formula 1...As shown in Figure 1, during the training of knowledge distillation, two neural networks are used: teacher model and student model. The student model is trained based on the combination of two loss values. The first loss value is calculated from a soft prediction, which contains the probability of each class calculated from the teacher model. The soft prediction is calculated using Formula 1. The other loss value corresponds to a hard prediction, which is the ground truth label from the training data.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {
                    "[7200347 | Hinton et al. | 2015 | Citations: 19742]": "A very simple way to improve the performance of almost any machine learning algorithm is to train many different models on the same data and then to average their predictions. Unfortunately, making predictions using a whole ensemble of models is cumbersome and may be too computationally expensive to allow deployment to a large number of users, especially if the individual models are large neural nets. Caruana and his collaborators have shown that it is possible to compress the knowledge in an ensemble into a single model which is much easier to deploy and we develop this approach further using a different compression technique. We achieve some surprising results on MNIST and we show that we can significantly improve the acoustic model of a heavily used commercial system by distilling the knowledge in an ensemble of models into a single model. We also introduce a new type of ensemble composed of one or more full models and many specialist models which learn to distinguish fine-grained classes that the full models confuse. Unlike a mixture of experts, these specialist models can be trained rapidly and in parallel."
                },
                "metadata": [
                    {
                        "quote": "Although more layers and more parameters generally improve the accuracy of the models, such big models generally have high computational complexity and require big memory, which exceed the capacity of small devices for inference and incurs long training time",
                        "pdf_hash": "",
                        "section_title": "abstract"
                    },
                    {
                        "section_title": "RELATED WORK",
                        "pdf_hash": "",
                        "start": 575,
                        "end": 708,
                        "sentence_offsets": [
                            {
                                "start": 575,
                                "end": 708
                            }
                        ],
                        "ref_mentions": [
                            "7200347"
                        ],
                        "quote": "Knowledge distillation is based on the popular machine learning Softmax function and a temperature (Hinton et al., 2015) , which is defined in Formula 1"
                    },
                    {
                        "section_title": "RELATED WORK",
                        "pdf_hash": "",
                        "start": 1206,
                        "end": 1713,
                        "sentence_offsets": [
                            {
                                "start": 1206,
                                "end": 1337
                            },
                            {
                                "start": 1338,
                                "end": 1411
                            },
                            {
                                "start": 1412,
                                "end": 1550
                            },
                            {
                                "start": 1551,
                                "end": 1601
                            },
                            {
                                "start": 1602,
                                "end": 1712
                            }
                        ],
                        "ref_mentions": [],
                        "quote": "As shown in Figure 1, during the training of knowledge distillation, two neural networks are used: teacher model and student model. The student model is trained based on the combination of two loss values. The first loss value is calculated from a soft prediction, which contains the probability of each class calculated from the teacher model. The soft prediction is calculated using Formula 1. The other loss value corresponds to a hard prediction, which is the ground truth label from the training data."
                    }
                ]
            },
            {
                "idx": 13,
                "key": "[253107469 | Guo et al. | 2022 | Citations: 3]",
                "snippets": "Knowledge distillation is one of the most popular techniques in model compression (Bucila et al., 2006), which utilizes a powerful but cumbersome teacher model to boost a compressed student model.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {},
                "metadata": [
                    {
                        "section_title": "Knowledge Distillation",
                        "pdf_hash": "",
                        "start": 0,
                        "end": 178,
                        "sentence_offsets": [
                            {
                                "start": 0,
                                "end": 178
                            }
                        ],
                        "ref_mentions": [
                            "11253972"
                        ],
                        "quote": "Knowledge distillation is one of the most popular techniques in model compression (Bucila et al., 2006), which utilizes a powerful but cumbersome teacher model to boost a compressed student model."
                    }
                ]
            },
            {
                "idx": 14,
                "key": "[253734490 | Berdoz et al. | 2022 | Citations: 3]",
                "snippets": "The concept of knowledge distillation (KD) originated in Bucila et al. (Bucila et al., 2006) as a way of compressing models, and was later generalized by Hinton et al. [19] (see Gou et al. (Gou et al., 2020) for an overview of the field). The standard use case for KD is that of a Teacher-Student (or offline) configuration, in which the teacher model (usually a large and well-trained model) transfers its knowledge to the student model (usually a smaller model) by sharing its last layer activations on a given transfer dataset (see Fig. 1a).",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {
                    "[219559263 | Gou et al. | 2020 | Citations: 2984]": "In recent years, deep neural networks have been successful in both industry and academia, especially for computer vision tasks. The great success of deep learning is mainly due to its scalability to encode large-scale data and to maneuver billions of model parameters. However, it is a challenge to deploy these cumbersome deep models on devices with limited resources, e.g., mobile phones and embedded devices, not only because of the high computational complexity but also the large storage requirements. To this end, a variety of model compression and acceleration techniques have been developed. As a representative type of model compression and acceleration, knowledge distillation effectively learns a small student model from a large teacher model. It has received rapid increasing attention from the community. This paper provides a comprehensive survey of knowledge distillation from the perspectives of knowledge categories, training schemes, teacher\u2013student architecture, distillation algorithms, performance comparison and applications. Furthermore, challenges in knowledge distillation are briefly reviewed and comments on future research are discussed and forwarded."
                },
                "metadata": [
                    {
                        "section_title": "Knowledge Distillation",
                        "pdf_hash": "",
                        "start": 0,
                        "end": 512,
                        "sentence_offsets": [
                            {
                                "start": 0,
                                "end": 206
                            },
                            {
                                "start": 207,
                                "end": 512
                            }
                        ],
                        "ref_mentions": [
                            "11253972",
                            "219559263"
                        ],
                        "quote": "The concept of knowledge distillation (KD) originated in Bucila et al. (Bucila et al., 2006) as a way of compressing models, and was later generalized by Hinton et al. [19] (see Gou et al. (Gou et al., 2020) for an overview of the field). The standard use case for KD is that of a Teacher-Student (or offline) configuration, in which the teacher model (usually a large and well-trained model) transfers its knowledge to the student model (usually a smaller model) by sharing its last layer activations on a given transfer dataset (see Fig. 1a)."
                    }
                ]
            },
            {
                "idx": 15,
                "key": "[254926873 | Kang et al. | 2022 | Citations: 1]",
                "snippets": "Knowledge Distillation. Knowledge Distillation aims to transfer knowledge of pretrained huge networks to relatively light networks. It is firstly devised by Hinton et al [17], with the idea that wide and deep networks can extract the key point features well. Therefore, if the huge network, coined as teacher network, can distillate their feature extract performances to small networks, coined as student networks, it can be said as memory-efficient networks.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {},
                "metadata": [
                    {
                        "section_title": "Representation Learning",
                        "pdf_hash": "",
                        "start": 0,
                        "end": 459,
                        "sentence_offsets": [
                            {
                                "start": 0,
                                "end": 23
                            },
                            {
                                "start": 24,
                                "end": 131
                            },
                            {
                                "start": 132,
                                "end": 258
                            },
                            {
                                "start": 259,
                                "end": 459
                            }
                        ],
                        "ref_mentions": [],
                        "quote": "Knowledge Distillation. Knowledge Distillation aims to transfer knowledge of pretrained huge networks to relatively light networks. It is firstly devised by Hinton et al [17], with the idea that wide and deep networks can extract the key point features well. Therefore, if the huge network, coined as teacher network, can distillate their feature extract performances to small networks, coined as student networks, it can be said as memory-efficient networks."
                    }
                ]
            },
            {
                "idx": 16,
                "key": "[255125462 | Li et al. | 2022 | Citations: 3]",
                "snippets": "Knowledge distillation is a model compression technology that transfers the knowledge from a larger deep neural network into a small network.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {},
                "metadata": [
                    {
                        "section_title": "B. Knowledge Distillation",
                        "pdf_hash": "",
                        "start": 0,
                        "end": 141,
                        "sentence_offsets": [
                            {
                                "start": 0,
                                "end": 141
                            }
                        ],
                        "ref_mentions": [],
                        "quote": "Knowledge distillation is a model compression technology that transfers the knowledge from a larger deep neural network into a small network."
                    }
                ]
            },
            {
                "idx": 17,
                "key": "[255595916 | Zhou et al. | 2023 | Citations: 7]",
                "snippets": "Knowledge distillation is the technique where knowledge learned by a larger teacher model is transferred to a smaller student model (Gou et al., 2020), Hinton et al., 2015(Wang et al., 2020). The main idea is that the student model mimics the teacher model to achieve a similar or even a superior performance.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {
                    "[215745611 | Wang et al. | 2020 | Citations: 700]": "Deep neural models, in recent years, have been successful in almost every field, even solving the most complex problem statements. However, these models are huge in size with millions (and even billions) of parameters, demanding heavy computation power and failing to be deployed on edge devices. Besides, the performance boost is highly dependent on redundant labeled data. To achieve faster speeds and to handle the problems caused by the lack of labeled data, knowledge distillation (KD) has been proposed to transfer information learned from one model to another. KD is often characterized by the so-called \u2018Student-Teacher\u2019 (S-T) learning framework and has been broadly applied in model compression and knowledge transfer. This paper is about KD and S-T learning, which are being actively studied in recent years. First, we aim to provide explanations of what KD is and how/why it works. Then, we provide a comprehensive survey on the recent progress of KD methods together with S-T frameworks typically used for vision tasks. In general, we investigate some fundamental questions that have been driving this research area and thoroughly generalize the research progress and technical details. Additionally, we systematically analyze the research status of KD in vision applications. Finally, we discuss the potentials and open challenges of existing methods and prospect the future directions of KD and S-T learning.",
                    "[219559263 | Gou et al. | 2020 | Citations: 2984]": "In recent years, deep neural networks have been successful in both industry and academia, especially for computer vision tasks. The great success of deep learning is mainly due to its scalability to encode large-scale data and to maneuver billions of model parameters. However, it is a challenge to deploy these cumbersome deep models on devices with limited resources, e.g., mobile phones and embedded devices, not only because of the high computational complexity but also the large storage requirements. To this end, a variety of model compression and acceleration techniques have been developed. As a representative type of model compression and acceleration, knowledge distillation effectively learns a small student model from a large teacher model. It has received rapid increasing attention from the community. This paper provides a comprehensive survey of knowledge distillation from the perspectives of knowledge categories, training schemes, teacher\u2013student architecture, distillation algorithms, performance comparison and applications. Furthermore, challenges in knowledge distillation are briefly reviewed and comments on future research are discussed and forwarded."
                },
                "metadata": [
                    {
                        "section_title": "Knowledge distillation",
                        "pdf_hash": "",
                        "start": 455,
                        "end": 766,
                        "sentence_offsets": [
                            {
                                "start": 455,
                                "end": 648
                            },
                            {
                                "start": 649,
                                "end": 766
                            }
                        ],
                        "ref_mentions": [
                            "219559263",
                            "215745611"
                        ],
                        "quote": "Knowledge distillation is the technique where knowledge learned by a larger teacher model is transferred to a smaller student model (Gou et al., 2020), Hinton et al., 2015(Wang et al., 2020). The main idea is that the student model mimics the teacher model to achieve a similar or even a superior performance."
                    }
                ]
            },
            {
                "idx": 18,
                "key": "[255942245 | Yu et al. | 2023 | Citations: 130]",
                "snippets": "Knowledge distillation (KD) [37], [38], [39], (Gou et al., 2020) aims to transfer knowledge from a large teacher network to a smaller student network, such that the student network can preserve the performance of the teacher with reduced computational overhead.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {
                    "[219559263 | Gou et al. | 2020 | Citations: 2984]": "In recent years, deep neural networks have been successful in both industry and academia, especially for computer vision tasks. The great success of deep learning is mainly due to its scalability to encode large-scale data and to maneuver billions of model parameters. However, it is a challenge to deploy these cumbersome deep models on devices with limited resources, e.g., mobile phones and embedded devices, not only because of the high computational complexity but also the large storage requirements. To this end, a variety of model compression and acceleration techniques have been developed. As a representative type of model compression and acceleration, knowledge distillation effectively learns a small student model from a large teacher model. It has received rapid increasing attention from the community. This paper provides a comprehensive survey of knowledge distillation from the perspectives of knowledge categories, training schemes, teacher\u2013student architecture, distillation algorithms, performance comparison and applications. Furthermore, challenges in knowledge distillation are briefly reviewed and comments on future research are discussed and forwarded."
                },
                "metadata": [
                    {
                        "section_title": "Knowledge Distillation",
                        "pdf_hash": "",
                        "start": 0,
                        "end": 247,
                        "sentence_offsets": [
                            {
                                "start": 0,
                                "end": 247
                            }
                        ],
                        "ref_mentions": [
                            "219559263"
                        ],
                        "quote": "Knowledge distillation (KD) [37], [38], [39], (Gou et al., 2020) aims to transfer knowledge from a large teacher network to a smaller student network, such that the student network can preserve the performance of the teacher with reduced computational overhead."
                    }
                ]
            },
            {
                "idx": 19,
                "key": "[257102399 | Shi et al. | 2023 | Citations: 8]",
                "snippets": "Knowledge distillation is a model compression technique proposed by Hinton et al. in 2015 [9], which could effectively train a smaller student model by learning from a larger teacher model. As shown in Figure 2, a general knowledge distillation framework consists of three major components: teacher-student network architecture, knowledge, as well as distillation algorithm. The main idea of knowledge distillation is that the student model mimics the teacher model in order to achieve a comparable or even a superior performance (Gou et al., 2020). Teacher network usually has more complicated network architecture and learns from the data first. After the training of teacher network, the useful knowledge could be distilled from the teacher network. Hereafter, student network learns the knowledge distilled from teacher network to mimic teacher's behavior together with the training data. The core of knowledge distillation is how to simplify model while maintaining relatively good performance.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {
                    "[219559263 | Gou et al. | 2020 | Citations: 2984]": "In recent years, deep neural networks have been successful in both industry and academia, especially for computer vision tasks. The great success of deep learning is mainly due to its scalability to encode large-scale data and to maneuver billions of model parameters. However, it is a challenge to deploy these cumbersome deep models on devices with limited resources, e.g., mobile phones and embedded devices, not only because of the high computational complexity but also the large storage requirements. To this end, a variety of model compression and acceleration techniques have been developed. As a representative type of model compression and acceleration, knowledge distillation effectively learns a small student model from a large teacher model. It has received rapid increasing attention from the community. This paper provides a comprehensive survey of knowledge distillation from the perspectives of knowledge categories, training schemes, teacher\u2013student architecture, distillation algorithms, performance comparison and applications. Furthermore, challenges in knowledge distillation are briefly reviewed and comments on future research are discussed and forwarded."
                },
                "metadata": [
                    {
                        "section_title": "Knowledge distillation",
                        "pdf_hash": "",
                        "start": 0,
                        "end": 985,
                        "sentence_offsets": [
                            {
                                "start": 0,
                                "end": 189
                            },
                            {
                                "start": 190,
                                "end": 374
                            },
                            {
                                "start": 375,
                                "end": 535
                            },
                            {
                                "start": 536,
                                "end": 633
                            },
                            {
                                "start": 634,
                                "end": 738
                            },
                            {
                                "start": 739,
                                "end": 878
                            },
                            {
                                "start": 879,
                                "end": 985
                            }
                        ],
                        "ref_mentions": [
                            "219559263"
                        ],
                        "quote": "Knowledge distillation is a model compression technique proposed by Hinton et al. in 2015 [9], which could effectively train a smaller student model by learning from a larger teacher model. As shown in Figure 2, a general knowledge distillation framework consists of three major components: teacher-student network architecture, knowledge, as well as distillation algorithm. The main idea of knowledge distillation is that the student model mimics the teacher model in order to achieve a comparable or even a superior performance (Gou et al., 2020). Teacher network usually has more complicated network architecture and learns from the data first. After the training of teacher network, the useful knowledge could be distilled from the teacher network. Hereafter, student network learns the knowledge distilled from teacher network to mimic teacher's behavior together with the training data. The core of knowledge distillation is how to simplify model while maintaining relatively good performance."
                    }
                ]
            },
            {
                "idx": 20,
                "key": "[257205990 | Campos et al. | 2023 | Citations: 54]",
                "snippets": "Knowledge Distillation (KD) (Hinton et al., 2015) aims to transfer knowledge from a teacher model to a student model, where the teacher is often a larger model with higher discriminative capacity than the student.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {
                    "[7200347 | Hinton et al. | 2015 | Citations: 19742]": "A very simple way to improve the performance of almost any machine learning algorithm is to train many different models on the same data and then to average their predictions. Unfortunately, making predictions using a whole ensemble of models is cumbersome and may be too computationally expensive to allow deployment to a large number of users, especially if the individual models are large neural nets. Caruana and his collaborators have shown that it is possible to compress the knowledge in an ensemble into a single model which is much easier to deploy and we develop this approach further using a different compression technique. We achieve some surprising results on MNIST and we show that we can significantly improve the acoustic model of a heavily used commercial system by distilling the knowledge in an ensemble of models into a single model. We also introduce a new type of ensemble composed of one or more full models and many specialist models which learn to distinguish fine-grained classes that the full models confuse. Unlike a mixture of experts, these specialist models can be trained rapidly and in parallel."
                },
                "metadata": [
                    {
                        "section_title": "Knowledge Distillation",
                        "pdf_hash": "",
                        "start": 0,
                        "end": 196,
                        "sentence_offsets": [
                            {
                                "start": 0,
                                "end": 196
                            }
                        ],
                        "ref_mentions": [
                            "7200347"
                        ],
                        "quote": "Knowledge Distillation (KD) (Hinton et al., 2015) aims to transfer knowledge from a teacher model to a student model, where the teacher is often a larger model with higher discriminative capacity than the student."
                    }
                ]
            },
            {
                "idx": 21,
                "key": "[258298441 | Liu et al. | 2023 | Citations: 19]",
                "snippets": "Hinton et al. (2015) first clarifies the concept of knowledge distillation. In their method, softened probability distribution output by the teacher is used as the guidance to the student...(Zhang et al., 2017) propose online knowledge distillation, where both the large and the small models are randomly initialized and learn mutually from each other during training.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {
                    "[26071966 | Zhang et al. | 2017 | Citations: 1655]": "Model distillation is an effective and widely used technique to transfer knowledge from a teacher to a student network. The typical application is to transfer from a powerful large network or ensemble to a small network, in order to meet the low-memory or fast execution requirements. In this paper, we present a deep mutual learning (DML) strategy. Different from the one-way transfer between a static pre-defined teacher and a student in model distillation, with DML, an ensemble of students learn collaboratively and teach each other throughout the training process. Our experiments show that a variety of network architectures benefit from mutual learning and achieve compelling results on both category and instance recognition tasks. Surprisingly, it is revealed that no prior powerful teacher network is necessary - mutual learning of a collection of simple student networks works, and moreover outperforms distillation from a more powerful yet static teacher."
                },
                "metadata": [
                    {
                        "section_title": "INTRODUCTION",
                        "pdf_hash": "",
                        "start": 713,
                        "end": 900,
                        "sentence_offsets": [
                            {
                                "start": 664,
                                "end": 788
                            },
                            {
                                "start": 789,
                                "end": 901
                            }
                        ],
                        "ref_mentions": [],
                        "quote": "Hinton et al. (2015) first clarifies the concept of knowledge distillation. In their method, softened probability distribution output by the teacher is used as the guidance to the student"
                    },
                    {
                        "section_title": "INTRODUCTION",
                        "pdf_hash": "",
                        "start": 1817,
                        "end": 1996,
                        "sentence_offsets": [
                            {
                                "start": 1728,
                                "end": 1995
                            }
                        ],
                        "ref_mentions": [
                            "26071966"
                        ],
                        "quote": "(Zhang et al., 2017) propose online knowledge distillation, where both the large and the small models are randomly initialized and learn mutually from each other during training."
                    }
                ]
            },
            {
                "idx": 22,
                "key": "[258426697 | Malik et al. | 2023 | Citations: 3]",
                "snippets": "The process of knowledge distillation as the name suggests is the method of transferring knowledge from a larger computationally expensive model to a relatively smaller model. The larger and smaller models are called the teacher and student models respectively. Thus, knowledge distillation consists of three principal components: (1) knowledge; (2) distillation algorithm; and (3) teacherstudent architecture. While there are now multiple methods of distillation algorithms we selected the response-based algorithm. As shown in Figure 3, the hypothesis is that the student model will learn to mimic the predictions of the teacher model. This can be achieved by using a loss function, termed the distillation loss, that captures the difference between the logits of the student and the teacher model respectively. As this loss minimizes overtraining, the student model will improve at making the same predictions as the teacher. In the offline training scheme, the teacher model is first trained and the weights are then frozen. Next, we train the student model using the distillation loss and the logits from the teacher model as targets. Following is the equation of the distillation loss. \n\nwhere: L d : the loss function for knowledge distillation \u03b1: a hyperparameter that controls the trade-off between the classification loss and the distillation loss T : the temperature hyperparameter used to soften the logits (outputs of the last layer before softmax) of the teacher and student models KL: the Kullback-Leibler divergence, a measure of how different two probability distributions are softmax: a function that converts the logits to probabilities f (T, x): the logits of the teacher model for input x g(T, x): the logits of the student model for input x 2) Teacher Model: Generally, for the teacher model a larger and deeper network is chosen so that it performs well on the task at hand.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {},
                "metadata": [
                    {
                        "section_title": "1) Knowledge Distillation:",
                        "pdf_hash": "",
                        "start": 0,
                        "end": 1897,
                        "sentence_offsets": [
                            {
                                "start": 0,
                                "end": 175
                            },
                            {
                                "start": 176,
                                "end": 261
                            },
                            {
                                "start": 262,
                                "end": 410
                            },
                            {
                                "start": 411,
                                "end": 516
                            },
                            {
                                "start": 517,
                                "end": 637
                            },
                            {
                                "start": 638,
                                "end": 813
                            },
                            {
                                "start": 814,
                                "end": 928
                            },
                            {
                                "start": 929,
                                "end": 1028
                            },
                            {
                                "start": 1029,
                                "end": 1139
                            },
                            {
                                "start": 1140,
                                "end": 1191
                            },
                            {
                                "start": 1194,
                                "end": 1897
                            }
                        ],
                        "ref_mentions": [],
                        "quote": "The process of knowledge distillation as the name suggests is the method of transferring knowledge from a larger computationally expensive model to a relatively smaller model. The larger and smaller models are called the teacher and student models respectively. Thus, knowledge distillation consists of three principal components: (1) knowledge; (2) distillation algorithm; and (3) teacherstudent architecture. While there are now multiple methods of distillation algorithms we selected the response-based algorithm. As shown in Figure 3, the hypothesis is that the student model will learn to mimic the predictions of the teacher model. This can be achieved by using a loss function, termed the distillation loss, that captures the difference between the logits of the student and the teacher model respectively. As this loss minimizes overtraining, the student model will improve at making the same predictions as the teacher. In the offline training scheme, the teacher model is first trained and the weights are then frozen. Next, we train the student model using the distillation loss and the logits from the teacher model as targets. Following is the equation of the distillation loss. \n\nwhere: L d : the loss function for knowledge distillation \u03b1: a hyperparameter that controls the trade-off between the classification loss and the distillation loss T : the temperature hyperparameter used to soften the logits (outputs of the last layer before softmax) of the teacher and student models KL: the Kullback-Leibler divergence, a measure of how different two probability distributions are softmax: a function that converts the logits to probabilities f (T, x): the logits of the teacher model for input x g(T, x): the logits of the student model for input x 2) Teacher Model: Generally, for the teacher model a larger and deeper network is chosen so that it performs well on the task at hand."
                    }
                ]
            },
            {
                "idx": 23,
                "key": "[260599456 | Tao et al. | 2023 | Citations: 3]",
                "snippets": "Knowledge Distillation is a widely used technique in model compression (Cheng et al., 2018), (Bucila et al., 2006) and transfer learning (Yim et al., 2017), which typically involves transferring knowledge from a large and complex model (often referred to as the teacher model) to a smaller and simpler model (often referred to as the student model). The goal of knowledge distillation is to improve the performance of the student model by leveraging the knowledge contained in the teacher model, while still maintaining the efficiency and simplicity of the student model.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {},
                "metadata": [
                    {
                        "section_title": "B. KNOWLEDGE DISTILLATION",
                        "pdf_hash": "",
                        "start": 0,
                        "end": 524,
                        "sentence_offsets": [
                            {
                                "start": 0,
                                "end": 302
                            },
                            {
                                "start": 303,
                                "end": 524
                            }
                        ],
                        "ref_mentions": [
                            "32588614",
                            "11253972",
                            "206596723"
                        ],
                        "quote": "Knowledge Distillation is a widely used technique in model compression (Cheng et al., 2018), (Bucila et al., 2006) and transfer learning (Yim et al., 2017), which typically involves transferring knowledge from a large and complex model (often referred to as the teacher model) to a smaller and simpler model (often referred to as the student model). The goal of knowledge distillation is to improve the performance of the student model by leveraging the knowledge contained in the teacher model, while still maintaining the efficiency and simplicity of the student model."
                    }
                ]
            },
            {
                "idx": 24,
                "key": "[262084420 | Capogrosso et al. | 2023 | Citations: 43]",
                "snippets": "This technique transfers knowledge from a large, complex model (teacher) to a smaller, simpler model (student) (Gou et al., 2020). This process is important for various reasons, such as reducing computational demands or enhancing model performance on specific tasks. Knowledge types, distillation strategies, and teacher-student architectures are vital factors in student learning during knowledge distillation. The subsequent paragraphs introduce the key categories of knowledge types and distillation strategies...Offline distillation is a two-stage strategy, where the teacher model is first trained on a set of training samples, and then the trained teacher model is used to guide the student model by extracting intermediate features or logits (Zhao et al., 2019)...In general, knowledge distillation is used to achieve a good trade-off between small model size and an acceptable accuracy (Zein et al., 2022).",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {
                    "[198179767 | Zhao et al. | 2019 | Citations: 59]": "High storage and computational costs obstruct deep neural networks to be deployed on resource-constrained devices. Knowledge distillation (KD) aims to train a compact student network by transferring knowledge from a larger pretrained teacher model. However, most existing methods on KD ignore the valuable information among the training process associated with training results. In this article, we provide a new collaborative teaching KD (CTKD) strategy which employs two special teachers. Specifically, one teacher trained from scratch (i.e., scratch teacher) assists the student step by step using its temporary outputs. It forces the student to approach the optimal path toward the final logits with high accuracy. The other pretrained teacher (i.e., expert teacher) guides the student to focus on a critical region that is more useful for the task. The combination of the knowledge from two special teachers can significantly improve the performance of the student network in KD. The results of experiments on CIFAR-10, CIFAR-100, SVHN, Tiny ImageNet, and ImageNet datasets verify that the proposed KD method is efficient and achieves state-of-the-art performance.",
                    "[219559263 | Gou et al. | 2020 | Citations: 2984]": "In recent years, deep neural networks have been successful in both industry and academia, especially for computer vision tasks. The great success of deep learning is mainly due to its scalability to encode large-scale data and to maneuver billions of model parameters. However, it is a challenge to deploy these cumbersome deep models on devices with limited resources, e.g., mobile phones and embedded devices, not only because of the high computational complexity but also the large storage requirements. To this end, a variety of model compression and acceleration techniques have been developed. As a representative type of model compression and acceleration, knowledge distillation effectively learns a small student model from a large teacher model. It has received rapid increasing attention from the community. This paper provides a comprehensive survey of knowledge distillation from the perspectives of knowledge categories, training schemes, teacher\u2013student architecture, distillation algorithms, performance comparison and applications. Furthermore, challenges in knowledge distillation are briefly reviewed and comments on future research are discussed and forwarded.",
                    "[255266316 | Zein et al. | 2022 | Citations: 5]": "Traditionally, neural network inferencing on tiny hardware devices took place in a centralized server-based manner. With more real-time applications coming into play, where security and latency are a concern, there has become a need to move inferencing to the edge. This paper describes a machine learning pipeline to carry neural networks from their initial forms to compressed forms deployable on tiny hardware devices, while maintaining acceptable accuracies of the optimized models. We will review the different software optimization techniques used to compress neural networks to their deployable forms. The prototype is a proof of concept showing that applying knowledge distillation from a highly accurate ResNet20 model to a simple CNN student model, followed by post-training quantization, achieves good multi-class accuracy on a constrained Arduino Nano 33 BLE Sense at low power consumption and with low inferencing latency."
                },
                "metadata": [
                    {
                        "section_title": "3) Knowledge Distillation",
                        "pdf_hash": "",
                        "start": 0,
                        "end": 499,
                        "sentence_offsets": [
                            {
                                "start": 0,
                                "end": 116
                            },
                            {
                                "start": 117,
                                "end": 252
                            },
                            {
                                "start": 253,
                                "end": 397
                            },
                            {
                                "start": 398,
                                "end": 500
                            }
                        ],
                        "ref_mentions": [
                            "219559263"
                        ],
                        "quote": "This technique transfers knowledge from a large, complex model (teacher) to a smaller, simpler model (student) (Gou et al., 2020). This process is important for various reasons, such as reducing computational demands or enhancing model performance on specific tasks. Knowledge types, distillation strategies, and teacher-student architectures are vital factors in student learning during knowledge distillation. The subsequent paragraphs introduce the key categories of knowledge types and distillation strategies"
                    },
                    {
                        "section_title": "3) Knowledge Distillation",
                        "pdf_hash": "",
                        "start": 1447,
                        "end": 1685,
                        "sentence_offsets": [
                            {
                                "start": 1447,
                                "end": 1685
                            }
                        ],
                        "ref_mentions": [
                            "198179767"
                        ],
                        "quote": "Offline distillation is a two-stage strategy, where the teacher model is first trained on a set of training samples, and then the trained teacher model is used to guide the student model by extracting intermediate features or logits (Zhao et al., 2019)"
                    },
                    {
                        "section_title": "3) Knowledge Distillation",
                        "pdf_hash": "",
                        "start": 2060,
                        "end": 2189,
                        "sentence_offsets": [
                            {
                                "start": 2060,
                                "end": 2188
                            }
                        ],
                        "ref_mentions": [
                            "255266316"
                        ],
                        "quote": "In general, knowledge distillation is used to achieve a good trade-off between small model size and an acceptable accuracy (Zein et al., 2022)."
                    }
                ]
            },
            {
                "idx": 25,
                "key": "[264323617 | Orasan et al. | 2023 | Citations: 1]",
                "snippets": "Knowledge distillation is a particular method that involves training to transfer knowledge from a larger network to a different one that has a significantly smaller size. Bucilua et al. (Bucila et al., 2006) in their important paper successfully demonstrated for the first time that the knowledge acquired by a large ensemble of models can be transferred to a single small model. The aim of using this method is to train the student network so that it can reproduce the performance of the teacher network, but with fewer resources.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {},
                "metadata": [
                    {
                        "section_title": "III. METHODOLOGY A. KNOWLEDGE DISTILLATION",
                        "pdf_hash": "",
                        "start": 131,
                        "end": 645,
                        "sentence_offsets": [
                            {
                                "start": 131,
                                "end": 301
                            },
                            {
                                "start": 302,
                                "end": 493
                            },
                            {
                                "start": 494,
                                "end": 645
                            }
                        ],
                        "ref_mentions": [
                            "11253972"
                        ],
                        "quote": "Knowledge distillation is a particular method that involves training to transfer knowledge from a larger network to a different one that has a significantly smaller size. Bucilua et al. (Bucila et al., 2006) in their important paper successfully demonstrated for the first time that the knowledge acquired by a large ensemble of models can be transferred to a single small model. The aim of using this method is to train the student network so that it can reproduce the performance of the teacher network, but with fewer resources."
                    }
                ]
            },
            {
                "idx": 26,
                "key": "[265384964 | Xie et al. | 2023 | Citations: 6]",
                "snippets": "Knowledge distillation (Bucila et al., 2006) involves the process of transferring knowledge from a sizable, intricate model (teacher model) to a more compact, effective model (student model) [35]. It involves training the student network to mimic the teacher network's output and emulate its internal representations or decision-making process. This technique is used to enhance the performance of smaller models, making them approximate the behavior of larger models while reducing computational costs and memory requirements.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {},
                "metadata": [
                    {
                        "section_title": "Knowledge Distillation",
                        "pdf_hash": "",
                        "start": 0,
                        "end": 510,
                        "sentence_offsets": [
                            {
                                "start": 0,
                                "end": 179
                            },
                            {
                                "start": 180,
                                "end": 327
                            },
                            {
                                "start": 328,
                                "end": 510
                            }
                        ],
                        "ref_mentions": [
                            "11253972"
                        ],
                        "quote": "Knowledge distillation (Bucila et al., 2006) involves the process of transferring knowledge from a sizable, intricate model (teacher model) to a more compact, effective model (student model) [35]. It involves training the student network to mimic the teacher network's output and emulate its internal representations or decision-making process. This technique is used to enhance the performance of smaller models, making them approximate the behavior of larger models while reducing computational costs and memory requirements."
                    }
                ]
            },
            {
                "idx": 27,
                "key": "[266693464 | Echim et al. | 2023 | Citations: 1]",
                "snippets": "A practical method for compressing models is knowledge distillation, which facilitates information transfer from a large teacher network to a small student network. Initially introduced by Bucila et al. (Bucila et al., 2006) and later generalized by Hinton et al. (Hinton et al., 2015), this method has gained popularity across multiple machine learning applications. Unlike conventional training, knowledge distillation involves learning the student network to emulate the outputs of the teacher model, typically represented as probability distributions over classes.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {},
                "metadata": [
                    {
                        "section_title": "Knowledge Distillation",
                        "pdf_hash": "",
                        "start": 0,
                        "end": 569,
                        "sentence_offsets": [
                            {
                                "start": 0,
                                "end": 164
                            },
                            {
                                "start": 165,
                                "end": 368
                            },
                            {
                                "start": 369,
                                "end": 569
                            }
                        ],
                        "ref_mentions": [
                            "11253972"
                        ],
                        "quote": "A practical method for compressing models is knowledge distillation, which facilitates information transfer from a large teacher network to a small student network. Initially introduced by Bucila et al. (Bucila et al., 2006) and later generalized by Hinton et al. (Hinton et al., 2015), this method has gained popularity across multiple machine learning applications. Unlike conventional training, knowledge distillation involves learning the student network to emulate the outputs of the teacher model, typically represented as probability distributions over classes."
                    }
                ]
            },
            {
                "idx": 28,
                "key": "[267413258 | Sghaier et al. | 2024 | Citations: 10]",
                "snippets": "Knowledge distillation is a popular technique that was first introduced by Hinton et al. [18] as a method to transfer knowledge from a complex model, also known as the teacher, to a smaller and faster model, called the student. The main goal of knowledge distillation is to transfer the learned knowledge of the teacher model to the student model so that it can achieve comparable or even better performance on a target task.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {},
                "metadata": [
                    {
                        "section_title": "Knowledge distillation",
                        "pdf_hash": "",
                        "start": 0,
                        "end": 425,
                        "sentence_offsets": [
                            {
                                "start": 0,
                                "end": 227
                            },
                            {
                                "start": 228,
                                "end": 425
                            }
                        ],
                        "ref_mentions": [],
                        "quote": "Knowledge distillation is a popular technique that was first introduced by Hinton et al. [18] as a method to transfer knowledge from a complex model, also known as the teacher, to a smaller and faster model, called the student. The main goal of knowledge distillation is to transfer the learned knowledge of the teacher model to the student model so that it can achieve comparable or even better performance on a target task."
                    }
                ]
            },
            {
                "idx": 29,
                "key": "[267657497 | Kim et al. | 2023 | Citations: 4]",
                "snippets": "Knowledge distillation (KD) is a technique used to enhance the performance of lightweight student networks by leveraging the dark knowledge embedded in large teacher networks. Over the years, KD methods have evolved to narrow the performance gap between student and teacher models by utilizing both final predictions, known as logits-based distillation [10,15,16,(Zhao et al., 2022)(Jin et al., 2023), and intermediate features, known as features-based distillation [18](Passalis et al., 2020)(Park et al., 2019)21,22,(Ahn et al., 2019)(Heo et al., 2019)(Chen et al., 2021)(Liu et al., 2023)(Guo et al., 2023)(Li et al., 2024)(Guo et al., 2024).",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {
                    "[258298441 | Liu et al. | 2023 | Citations: 19]": "Feature distillation makes the student mimic the intermediate features of the teacher. Nearly all existing feature-distillation methods use L2 distance or its slight variants as the distance metric between teacher and student features. However, while L2 distance is isotropic w.r.t. all dimensions, the neural network's operation on different dimensions is usually anisotropic, i.e., perturbations with the same 2-norm but in different dimensions of intermediate features lead to changes in the final output with largely different magnitude. Considering this, we argue that the similarity between teacher and student features should not be measured merely based on their appearance (i.e., L2 distance), but should, more importantly, be measured by their difference in function, namely how later layers of the network will read, decode, and process them. Therefore, we propose Function-Consistent Feature Distillation (FCFD), which explicitly optimizes the functional similarity between teacher and student features. The core idea of FCFD is to make teacher and student features not only numerically similar, but more importantly produce similar outputs when fed to the later part of the same network. With FCFD, the student mimics the teacher more faithfully and learns more from the teacher. Extensive experiments on image classification and object detection demonstrate the superiority of FCFD to existing methods. Furthermore, we can combine FCFD with many existing methods to obtain even higher accuracy. Our codes are available at https://github.com/LiuDongyang6/FCFD.",
                    "[102483181 | Heo et al. | 2019 | Citations: 584]": "We investigate the design aspects of feature distillation methods achieving network compression and propose a novel feature distillation method in which the distillation loss is designed to make a synergy among various aspects: teacher transform, student transform, distillation feature position and distance function. Our proposed distillation loss includes a feature transform with a newly designed margin ReLU, a new distillation feature position, and a partial L2 distance function to skip redundant information giving adverse effects to the compression of student. In ImageNet, our proposed method achieves 21.65% of top-1 error with ResNet50, which outperforms the performance of the teacher network, ResNet152. Our proposed method is evaluated on various tasks such as image classification, object detection and semantic segmentation and achieves a significant performance improvement in all tasks. The code is available at project page.",
                    "[118649278 | Ahn et al. | 2019 | Citations: 621]": "Transferring knowledge from a teacher neural network pretrained on the same or a similar task to a student neural network can significantly improve the performance of the student neural network. Existing knowledge transfer approaches match the activations or the corresponding hand-crafted features of the teacher and the student networks. We propose an information-theoretic framework for knowledge transfer which formulates knowledge transfer as maximizing the mutual information between the teacher and the student networks. We compare our method with existing knowledge transfer methods on both knowledge distillation and transfer learning tasks and show that our method consistently outperforms existing methods. We further demonstrate the strength of our method on knowledge transfer across heterogeneous network architectures by transferring knowledge from a convolutional neural network (CNN) to a multi-layer perceptron (MLP) on CIFAR-10. The resulting MLP significantly outperforms the-state-of-the-art methods and it achieves similar performance to the CNN with a single convolutional layer.",
                    "[131765296 | Park et al. | 2019 | Citations: 1424]": "Knowledge distillation aims at transferring knowledge acquired in one model (a teacher) to another model (a student) that is typically smaller. Previous approaches can be expressed as a form of training the student to mimic output activations of individual data examples represented by the teacher. We introduce a novel approach, dubbed relational knowledge distillation (RKD), that transfers mutual relations of data examples instead. For concrete realizations of RKD, we propose distance-wise and angle-wise distillation losses that penalize structural differences in relations. Experiments conducted on different tasks show that the proposed method improves educated student models with a significant margin. In particular for metric learning, it allows students to outperform their teachers' performance, achieving the state of the arts on standard benchmark datasets.",
                    "[219169868 | Passalis et al. | 2020 | Citations: 100]": "Knowledge-transfer (KT) methods allow for transferring the knowledge contained in a large deep learning model into a more lightweight and faster model. However, the vast majority of existing KT approaches are designed to handle mainly classification and detection tasks. This limits their performance on other tasks, such as representation/metric learning. To overcome this limitation, a novel probabilistic KT (PKT) method is proposed in this article. PKT is capable of transferring the knowledge into a smaller student model by keeping as much information as possible, as expressed through the teacher model. The ability of the proposed method to use different kernels for estimating the probability distribution of the teacher and student models, along with the different divergence metrics that can be used for transferring the knowledge, allows for easily adapting the proposed method to different applications. PKT outperforms several existing state-of-the-art KT techniques, while it is capable of providing new insights into KT by enabling several novel applications, as it is demonstrated through extensive experiments on several challenging data sets.",
                    "[233296935 | Chen et al. | 2021 | Citations: 446]": "Knowledge distillation transfers knowledge from the teacher network to the student one, with the goal of greatly improving the performance of the student network. Previous methods mostly focus on proposing feature transformation and loss functions between the same level's features to improve the effectiveness. We differently study the factor of connection path cross levels between teacher and student networks, and reveal its great importance. For the first time in knowledge distillation, cross-stage connection paths are proposed. Our new review mechanism is effective and structurally simple. Our finally designed nested and compact framework requires negligible computation overhead, and outperforms other methods on a variety of tasks. We apply our method to classification, object detection, and instance segmentation tasks. All of them witness significant student network performance improvement.",
                    "[247476179 | Zhao et al. | 2022 | Citations: 549]": "State-of-the-art distillation methods are mainly based on distilling deep features from intermediate layers, while the significance of logit distillation is greatly overlooked. To provide a novel viewpoint to study logit distillation, we re-formulate the classical KD loss into two parts, i.e., target class knowledge distillation (TCKD) and non-target class knowledge distillation (NCKD). We empirically investigate and prove the effects of the two parts: TCKD transfers knowledge concerning the \u201cdifficulty\u201d of training samples, while NCKD is the prominent reason why logit distillation works. More importantly, we reveal that the classical KD loss is a coupled formulation, which (1) suppresses the effectiveness of NCKD and (2) limits the flexibility to balance these two parts. To address these issues, we present Decoupled Knowledge Distillation (DKD), enabling TCKD and NCKD to play their roles more efficiently and flexibly. Compared with complex feature-based methods, our DKD achieves comparable or even better results and has better training efficiency on CIFAR-100, ImageNet, and MS-COCO datasets for image classification and object detection tasks. This paper proves the great potential of logit distillation, and we hope it will be helpful for future research. The code is available at https://github.com/megviiresearch/mdistiller.",
                    "[258309453 | Guo et al. | 2023 | Citations: 69]": "Previous knowledge distillation methods have shown their impressive performance on model compression tasks, however, it is hard to explain how the knowledge they transferred helps to improve the performance of the student network. In this work, we focus on proposing a knowledge distillation method that has both high interpretability and competitive performance. We first revisit the structure of mainstream CNN models and reveal that possessing the capacity of identifying class discriminative regions of input is critical for CNN to perform classification. Furthermore, we demonstrate that this capacity can be obtained and enhanced by transferring class activation maps. Based on our findings, we propose class attention transfer based knowledge distillation (CAT-KD). Different from previous KD methods, we explore and present several properties of the knowledge transferred by our method, which not only improve the interpretability of CAT-KD but also contribute to a better understanding of CNN. While having high interpretability, CAT-KD achieves state-of-the-art performance on multiple benchmarks. Code is available at: https://github.com/GzyAftermath/CAT-KD.",
                    "[260933721 | Jin et al. | 2023 | Citations: 67]": "Knowledge Distillation (KD) aims at distilling the knowledge from the large teacher model to a lightweight student model. Mainstream KD methods can be divided into two categories, logit distillation, and feature distillation. The former is easy to implement, but inferior in performance, while the latter is not applicable to some practical circumstances due to concerns such as privacy and safety. Towards this dilemma, in this paper, we explore a stronger logit distillation method via making better utilization of logit outputs. Concretely, we propose a simple yet effective approach to logit distillation via multi-level prediction alignment. Through this framework, the prediction alignment is not only conducted at the instance level, but also at the batch and class level, through which the student model learns instance prediction, input correlation, and category correlation simultaneously. In addition, a prediction augmentation mechanism based on model calibration further boosts the performance. Extensive experiment results validate that our method enjoys consistently higher performance than previous logit distillation methods, and even reaches competitive performance with mainstream feature distillation methods. Code is available at https://github.com/Jin-Ying/Multi-Level-Logit-Distillation.",
                    "[269167845 | Li et al. | 2024 | Citations: 1]": "Knowledge distillation based on the features from the penultimate layer allows the student (lightweight model) to efficiently mimic the internal feature outputs of the teacher (high-capacity model). However, the training data may not conform to the ground-truth distribution of images in terms of classes and features. We propose two knowledge distillation algorithms to solve the above problem from the directions of fitting the ground-truth distribution of classes and fitting the ground-truth distribution of features, respectively. The former uses teacher labels to supervise student classification output instead of dataset labels, while the latter designs feature temperature parameters to correct teachers\u2019 abnormal feature distribution output. We conducted knowledge distillation experiments on the ImageNet-2012 and Cifar-100 datasets using seven sets of homogeneous models and six sets of heterogeneous models. The experimental results show that our proposed algorithms improve the performance of penultimate layer feature knowledge distillation and outperform other existing knowledge distillation methods in terms of classification performance and generalization ability."
                },
                "metadata": [
                    {
                        "section_title": "Knowledge Distillation",
                        "pdf_hash": "",
                        "start": 0,
                        "end": 473,
                        "sentence_offsets": [
                            {
                                "start": 0,
                                "end": 175
                            },
                            {
                                "start": 176,
                                "end": 473
                            }
                        ],
                        "ref_mentions": [
                            "247476179",
                            "260933721",
                            "219169868",
                            "131765296",
                            "118649278",
                            "102483181",
                            "233296935",
                            "258298441",
                            "258309453",
                            "269167845",
                            "269206209"
                        ],
                        "quote": "Knowledge distillation (KD) is a technique used to enhance the performance of lightweight student networks by leveraging the dark knowledge embedded in large teacher networks. Over the years, KD methods have evolved to narrow the performance gap between student and teacher models by utilizing both final predictions, known as logits-based distillation [10,15,16,(Zhao et al., 2022)(Jin et al., 2023), and intermediate features, known as features-based distillation [18](Passalis et al., 2020)(Park et al., 2019)21,22,(Ahn et al., 2019)(Heo et al., 2019)(Chen et al., 2021)(Liu et al., 2023)(Guo et al., 2023)(Li et al., 2024)(Guo et al., 2024)."
                    }
                ]
            },
            {
                "idx": 30,
                "key": "[267688311 | Si et al. | 2024 | Citations: 5]",
                "snippets": "Knowledge distillation is a commonly used technique in deep learning, as proposed by Hinton et al. in 2015 [17]. It is used to transfer knowledge from a large model to a smaller one, resulting in model compression and acceleration. The goal of knowledge distillation is to improve the accuracy and generalizability of the student network by transferring knowledge from the teacher network. Auxiliary information during training is usually obtained from the output of the teacher network. Such techniques have been applied in various domains, including natural language processing, computer vision, and speech recognition. An important use case is the deployment of deep learning models on mobile devices, which often have limited computational resources and require smaller, lightweight models. Knowledge distillation typically involves a larger teacher network and a smaller student network. The teacher network is usually trained on large amounts of data and therefore has higher accuracy and more comprehensive knowledge. Compared to the teacher network, the student network is trained on less data and has fewer parameters. Nevertheless, some student networks can surpass the accuracy of the teacher network.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {},
                "metadata": [
                    {
                        "section_title": "II. RELATED WORK A. KNOWLEDGE DISTILLATION",
                        "pdf_hash": "",
                        "start": 0,
                        "end": 1212,
                        "sentence_offsets": [
                            {
                                "start": 0,
                                "end": 112
                            },
                            {
                                "start": 113,
                                "end": 231
                            },
                            {
                                "start": 232,
                                "end": 389
                            },
                            {
                                "start": 390,
                                "end": 487
                            },
                            {
                                "start": 488,
                                "end": 621
                            },
                            {
                                "start": 622,
                                "end": 794
                            },
                            {
                                "start": 795,
                                "end": 892
                            },
                            {
                                "start": 893,
                                "end": 1024
                            },
                            {
                                "start": 1025,
                                "end": 1127
                            },
                            {
                                "start": 1128,
                                "end": 1212
                            }
                        ],
                        "ref_mentions": [],
                        "quote": "Knowledge distillation is a commonly used technique in deep learning, as proposed by Hinton et al. in 2015 [17]. It is used to transfer knowledge from a large model to a smaller one, resulting in model compression and acceleration. The goal of knowledge distillation is to improve the accuracy and generalizability of the student network by transferring knowledge from the teacher network. Auxiliary information during training is usually obtained from the output of the teacher network. Such techniques have been applied in various domains, including natural language processing, computer vision, and speech recognition. An important use case is the deployment of deep learning models on mobile devices, which often have limited computational resources and require smaller, lightweight models. Knowledge distillation typically involves a larger teacher network and a smaller student network. The teacher network is usually trained on large amounts of data and therefore has higher accuracy and more comprehensive knowledge. Compared to the teacher network, the student network is trained on less data and has fewer parameters. Nevertheless, some student networks can surpass the accuracy of the teacher network."
                    }
                ]
            },
            {
                "idx": 31,
                "key": "[267759723 | Zhou et al. | 2024 | Citations: 4]",
                "snippets": "Knowledge distillation is a method to transfer knowledge from a large model to a small model. It's first proposed in this paper [12]. The large model is called the teacher model and the small model is called the student model. The student model is trained to mimic the output of the teacher model. The student model is usually a shallow neural network with fewer parameters and faster inference speed. The teacher model is usually a deep neural network with more parameters and slower inference speed. The student model is trained on the same dataset as the teacher model to minimize the difference between the output of the student model and the output of the teacher model (i.e. maintain the knowledge as much as possible).",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {},
                "metadata": [
                    {
                        "section_title": "Knowledge Distillation",
                        "pdf_hash": "",
                        "start": 0,
                        "end": 725,
                        "sentence_offsets": [
                            {
                                "start": 0,
                                "end": 93
                            },
                            {
                                "start": 94,
                                "end": 133
                            },
                            {
                                "start": 134,
                                "end": 226
                            },
                            {
                                "start": 227,
                                "end": 297
                            },
                            {
                                "start": 298,
                                "end": 401
                            },
                            {
                                "start": 402,
                                "end": 501
                            },
                            {
                                "start": 502,
                                "end": 680
                            },
                            {
                                "start": 681,
                                "end": 725
                            }
                        ],
                        "ref_mentions": [],
                        "quote": "Knowledge distillation is a method to transfer knowledge from a large model to a small model. It's first proposed in this paper [12]. The large model is called the teacher model and the small model is called the student model. The student model is trained to mimic the output of the teacher model. The student model is usually a shallow neural network with fewer parameters and faster inference speed. The teacher model is usually a deep neural network with more parameters and slower inference speed. The student model is trained on the same dataset as the teacher model to minimize the difference between the output of the student model and the output of the teacher model (i.e. maintain the knowledge as much as possible)."
                    }
                ]
            },
            {
                "idx": 32,
                "key": "[268681103 | Hoang et al. | 2024 | Citations: 6]",
                "snippets": "Knowledge distillation (Hinton et al., 2015) is a technique that enables a smaller student model to learn from a larger teacher model by transferring knowledge.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {},
                "metadata": [
                    {
                        "section_title": "Knowledge Distillation",
                        "pdf_hash": "",
                        "start": 0,
                        "end": 160,
                        "sentence_offsets": [
                            {
                                "start": 0,
                                "end": 160
                            }
                        ],
                        "ref_mentions": [],
                        "quote": "Knowledge distillation (Hinton et al., 2015) is a technique that enables a smaller student model to learn from a larger teacher model by transferring knowledge."
                    }
                ]
            },
            {
                "idx": 33,
                "key": "[269327338 | Mostafavi et al. | 2024 | Citations: 0]",
                "snippets": "Knowledge Distillation denotes the procedure of transferring knowledge from a sophisticated model to a more straightforward one. It involves training smaller models to achieve similar accuracy as larger models by leveraging the knowledge gained from the larger models. Within the context of knowledge distillation, the term \"teacher network\" is used to describe the larger model, whereas the \"student network\" refers to the smaller network. The fundamental concept behind Knowledge Distillation is to train a smaller and less complex model to imitate the behavior and generalization capabilities of a larger and more complex model. The process of knowledge distillation involves transferring knowledge from the teacher network to the student network by optimizing a loss function [32].",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {},
                "metadata": [
                    {
                        "section_title": "Knowledge distillation",
                        "pdf_hash": "",
                        "start": 0,
                        "end": 785,
                        "sentence_offsets": [
                            {
                                "start": 0,
                                "end": 128
                            },
                            {
                                "start": 129,
                                "end": 268
                            },
                            {
                                "start": 269,
                                "end": 440
                            },
                            {
                                "start": 441,
                                "end": 631
                            },
                            {
                                "start": 632,
                                "end": 785
                            }
                        ],
                        "ref_mentions": [],
                        "quote": "Knowledge Distillation denotes the procedure of transferring knowledge from a sophisticated model to a more straightforward one. It involves training smaller models to achieve similar accuracy as larger models by leveraging the knowledge gained from the larger models. Within the context of knowledge distillation, the term \"teacher network\" is used to describe the larger model, whereas the \"student network\" refers to the smaller network. The fundamental concept behind Knowledge Distillation is to train a smaller and less complex model to imitate the behavior and generalization capabilities of a larger and more complex model. The process of knowledge distillation involves transferring knowledge from the teacher network to the student network by optimizing a loss function [32]."
                    }
                ]
            },
            {
                "idx": 34,
                "key": "[269761365 | Wang et al. | 2024 | Citations: 0]",
                "snippets": "Large and complex teacher models often capture a wealth of features and deep information, crucial for enhancing task performance [7]. However, attempting to directly distill this rich information into smaller capacity student models often results in suboptimal performance reproduction due to the student model's capacity limitations. The student model may struggle to process the complex information present in the teacher model, leading to ineffective distillation. Moreover, simply mimicking all features of the teacher model overlooks the differences in relationships and structures between the information, particularly when there is a significant gap between the teacher and student models [12].",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {
                    "[237194985 | Zi et al. | 2021 | Citations: 101]": "Adversarial training is one effective approach for training robust deep neural networks against adversarial attacks. While being able to bring reliable robustness, adversarial training (AT) methods in general favor high capacity models, i.e., the larger the model the better the robustness. This tends to limit their effectiveness on small models, which are more preferable in scenarios where storage or computing resources are very limited (e.g., mobile devices). In this paper, we leverage the concept of knowledge distillation to improve the robustness of small models by distilling from adversarially trained large models. We first revisit several state-of-the-art AT methods from a distillation perspective and identify one common technique that can lead to improved robustness: the use of robust soft labels \u2013 predictions of a robust model. Following this observation, we propose a novel adversarial robustness distillation method called Robust Soft Label Adversarial Distillation (RSLAD) to train robust small student models. RSLAD fully exploits the robust soft labels produced by a robust (adversarially-trained) large teacher model to guide the student\u2019s learning on both natural and adversarial examples in all loss terms. We empirically demonstrate the effectiveness of our RSLAD approach over existing adversarial training and distillation methods in improving the robustness of small models against state-of-the-art attacks including the AutoAttack. We also provide a set of understandings on our RSLAD and the importance of robust soft labels for adversarial robustness distillation. Code: https://github.com/zibojia/RSLAD."
                },
                "metadata": [
                    {
                        "section_title": "Introduction",
                        "pdf_hash": "",
                        "start": 1361,
                        "end": 2062,
                        "sentence_offsets": [
                            {
                                "start": 1218,
                                "end": 1421
                            },
                            {
                                "start": 1421,
                                "end": 1659
                            },
                            {
                                "start": 1661,
                                "end": 1794
                            },
                            {
                                "start": 1794,
                                "end": 1994
                            },
                            {
                                "start": 1994,
                                "end": 2126
                            }
                        ],
                        "ref_mentions": [
                            "237194985"
                        ],
                        "quote": "Large and complex teacher models often capture a wealth of features and deep information, crucial for enhancing task performance [7]. However, attempting to directly distill this rich information into smaller capacity student models often results in suboptimal performance reproduction due to the student model's capacity limitations. The student model may struggle to process the complex information present in the teacher model, leading to ineffective distillation. Moreover, simply mimicking all features of the teacher model overlooks the differences in relationships and structures between the information, particularly when there is a significant gap between the teacher and student models [12]."
                    }
                ]
            },
            {
                "idx": 35,
                "key": "[269921267 | Mei et al. | 2024 | Citations: 20]",
                "snippets": "Knowledge distillation, a core strategy in modern machine learning, focuses on solving the problem of balance between model size and computational efficiency.The core idea is to effectively transfer the deep knowledge and experience accumulated in large-scale, complex models (often referred to as \"teacher models\") to \"student models\" with smaller numbers of participants and lower computational requirements.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {},
                "metadata": [
                    {
                        "section_title": "B. Knowledge distillation",
                        "pdf_hash": "",
                        "start": 0,
                        "end": 410,
                        "sentence_offsets": [
                            {
                                "start": 0,
                                "end": 158
                            },
                            {
                                "start": 158,
                                "end": 410
                            }
                        ],
                        "ref_mentions": [],
                        "quote": "Knowledge distillation, a core strategy in modern machine learning, focuses on solving the problem of balance between model size and computational efficiency.The core idea is to effectively transfer the deep knowledge and experience accumulated in large-scale, complex models (often referred to as \"teacher models\") to \"student models\" with smaller numbers of participants and lower computational requirements."
                    }
                ]
            },
            {
                "idx": 36,
                "key": "[270077283 | Zhao et al. | 2024 | Citations: 3]",
                "snippets": "Knowledge distillation is a widely used paradigm for model compression, which transfers knowledge from a complex teacher model to a compact student model. To be specific, the teacher network has high accuracy and huge parameters, while the student network is not as accurate as the teacher network but has fewer parameters. Through knowledge distillation, we hope that the student network can approach or exceed the teacher network as much as possible. In this way, we obtain a compact student network with a similar prediction effect as the teacher network.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {},
                "metadata": [
                    {
                        "section_title": "Related Work",
                        "pdf_hash": "",
                        "start": 129,
                        "end": 687,
                        "sentence_offsets": [
                            {
                                "start": 93,
                                "end": 152
                            },
                            {
                                "start": 154,
                                "end": 308
                            },
                            {
                                "start": 308,
                                "end": 476
                            },
                            {
                                "start": 476,
                                "end": 604
                            },
                            {
                                "start": 604,
                                "end": 709
                            }
                        ],
                        "ref_mentions": [],
                        "quote": "Knowledge distillation is a widely used paradigm for model compression, which transfers knowledge from a complex teacher model to a compact student model. To be specific, the teacher network has high accuracy and huge parameters, while the student network is not as accurate as the teacher network but has fewer parameters. Through knowledge distillation, we hope that the student network can approach or exceed the teacher network as much as possible. In this way, we obtain a compact student network with a similar prediction effect as the teacher network."
                    }
                ]
            },
            {
                "idx": 37,
                "key": "[270389751 | Li et al. | 2024 | Citations: 1]",
                "snippets": "Knowledge distillation (KD), as initially proposed by Hinton et al. (Hinton et al., 2015) , aims to supervise the training convergence of a student network, a smaller model, with a teacher network, a larger model.This method controls the extent of knowledge transfer between two networks using a temperature parameter, T, to control the transfer of soft-label dark knowledge.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {
                    "[7200347 | Hinton et al. | 2015 | Citations: 19742]": "A very simple way to improve the performance of almost any machine learning algorithm is to train many different models on the same data and then to average their predictions. Unfortunately, making predictions using a whole ensemble of models is cumbersome and may be too computationally expensive to allow deployment to a large number of users, especially if the individual models are large neural nets. Caruana and his collaborators have shown that it is possible to compress the knowledge in an ensemble into a single model which is much easier to deploy and we develop this approach further using a different compression technique. We achieve some surprising results on MNIST and we show that we can significantly improve the acoustic model of a heavily used commercial system by distilling the knowledge in an ensemble of models into a single model. We also introduce a new type of ensemble composed of one or more full models and many specialist models which learn to distinguish fine-grained classes that the full models confuse. Unlike a mixture of experts, these specialist models can be trained rapidly and in parallel."
                },
                "metadata": [
                    {
                        "section_title": "Knowledge distillation",
                        "pdf_hash": "",
                        "start": 0,
                        "end": 356,
                        "sentence_offsets": [
                            {
                                "start": 0,
                                "end": 194
                            },
                            {
                                "start": 194,
                                "end": 356
                            }
                        ],
                        "ref_mentions": [
                            "7200347"
                        ],
                        "quote": "Knowledge distillation (KD), as initially proposed by Hinton et al. (Hinton et al., 2015) , aims to supervise the training convergence of a student network, a smaller model, with a teacher network, a larger model.This method controls the extent of knowledge transfer between two networks using a temperature parameter, T, to control the transfer of soft-label dark knowledge."
                    }
                ]
            },
            {
                "idx": 38,
                "key": "[270870796 | Mai et al. | 2024 | Citations: 5]",
                "snippets": "Knowledge Distillation (KD) is a model compression technique that transfers knowledge from a complex model (called the teacher model) to a smaller model (called the student model).This allows the student model to maintain high computational efficiency while achieving the performance of the teacher model.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {},
                "metadata": [
                    {
                        "section_title": "C. Knowledge Distillation",
                        "pdf_hash": "",
                        "start": 0,
                        "end": 305,
                        "sentence_offsets": [
                            {
                                "start": 0,
                                "end": 180
                            },
                            {
                                "start": 180,
                                "end": 305
                            }
                        ],
                        "ref_mentions": [],
                        "quote": "Knowledge Distillation (KD) is a model compression technique that transfers knowledge from a complex model (called the teacher model) to a smaller model (called the student model).This allows the student model to maintain high computational efficiency while achieving the performance of the teacher model."
                    }
                ]
            },
            {
                "idx": 39,
                "key": "[271227251 | Acharya et al. | 2024 | Citations: 7]",
                "snippets": "Knowledge distillation is a technique used to transfer knowledge from a larger, more complex model (teacher) to a smaller, simpler model (student) with the goal of retaining much of the teacher model's performance (Gou et al., 2020). This process is crucial in scenarios where computational resources are limited or where deployment requires lightweight models.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {
                    "[219559263 | Gou et al. | 2020 | Citations: 2984]": "In recent years, deep neural networks have been successful in both industry and academia, especially for computer vision tasks. The great success of deep learning is mainly due to its scalability to encode large-scale data and to maneuver billions of model parameters. However, it is a challenge to deploy these cumbersome deep models on devices with limited resources, e.g., mobile phones and embedded devices, not only because of the high computational complexity but also the large storage requirements. To this end, a variety of model compression and acceleration techniques have been developed. As a representative type of model compression and acceleration, knowledge distillation effectively learns a small student model from a large teacher model. It has received rapid increasing attention from the community. This paper provides a comprehensive survey of knowledge distillation from the perspectives of knowledge categories, training schemes, teacher\u2013student architecture, distillation algorithms, performance comparison and applications. Furthermore, challenges in knowledge distillation are briefly reviewed and comments on future research are discussed and forwarded."
                },
                "metadata": [
                    {
                        "section_title": "A. Knowledge Distillation",
                        "pdf_hash": "",
                        "start": 0,
                        "end": 348,
                        "sentence_offsets": [
                            {
                                "start": 0,
                                "end": 220
                            },
                            {
                                "start": 221,
                                "end": 348
                            }
                        ],
                        "ref_mentions": [
                            "219559263"
                        ],
                        "quote": "Knowledge distillation is a technique used to transfer knowledge from a larger, more complex model (teacher) to a smaller, simpler model (student) with the goal of retaining much of the teacher model's performance (Gou et al., 2020). This process is crucial in scenarios where computational resources are limited or where deployment requires lightweight models."
                    }
                ]
            },
            {
                "idx": 40,
                "key": "[271356843 | Yang et al. | 2025 | Citations: 0]",
                "snippets": "The core concept of knowledge distillation is that the deep knowledge accumulated in the complex pre-trained teacher network model is distilled and injected into the more streamlined student model, which is able to inherit and mimic the predictive performance of the teacher model through knowledge distillation, thus reducing the model complexity while maintaining similar predictive effects. Knowledge distillation aims to optimize the balance between model size and performance through knowledge transfer to achieve effective model compression and knowledge transfer.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {},
                "metadata": [
                    {
                        "section_title": "C. KNOWLEDGE DISTILLATION",
                        "pdf_hash": "",
                        "start": 0,
                        "end": 570,
                        "sentence_offsets": [
                            {
                                "start": 0,
                                "end": 393
                            },
                            {
                                "start": 394,
                                "end": 570
                            }
                        ],
                        "ref_mentions": [],
                        "quote": "The core concept of knowledge distillation is that the deep knowledge accumulated in the complex pre-trained teacher network model is distilled and injected into the more streamlined student model, which is able to inherit and mimic the predictive performance of the teacher model through knowledge distillation, thus reducing the model complexity while maintaining similar predictive effects. Knowledge distillation aims to optimize the balance between model size and performance through knowledge transfer to achieve effective model compression and knowledge transfer."
                    }
                ]
            },
            {
                "idx": 41,
                "key": "[272564024 | Xie et al. | 2024 | Citations: 1]",
                "snippets": "Knowledge distillation is a technique that transfers knowledge from large, complex CNNs (teachers) to smaller, more efficient CNNs (students). The student network is trained to mimic the behavior of the teacher, resulting in a compact model with comparable performance. This concept can be traced back to the pioneering work in [101] and has since been extended to the context of deep learning (Hinton et al., 2015). The core challenge of knowledge distillation revolves around the techniques used to transfer knowledge from the teacher model to the student model, which involves three fundamental components-knowledge, distillation algorithms-and the architecture defining the relationship between the teacher and student models. In this context, knowledge manifests in various forms, including logits, activations, or features extracted from intermediate layers of the teacher model.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {
                    "[7200347 | Hinton et al. | 2015 | Citations: 19742]": "A very simple way to improve the performance of almost any machine learning algorithm is to train many different models on the same data and then to average their predictions. Unfortunately, making predictions using a whole ensemble of models is cumbersome and may be too computationally expensive to allow deployment to a large number of users, especially if the individual models are large neural nets. Caruana and his collaborators have shown that it is possible to compress the knowledge in an ensemble into a single model which is much easier to deploy and we develop this approach further using a different compression technique. We achieve some surprising results on MNIST and we show that we can significantly improve the acoustic model of a heavily used commercial system by distilling the knowledge in an ensemble of models into a single model. We also introduce a new type of ensemble composed of one or more full models and many specialist models which learn to distinguish fine-grained classes that the full models confuse. Unlike a mixture of experts, these specialist models can be trained rapidly and in parallel."
                },
                "metadata": [
                    {
                        "section_title": "Knowledge Distillation",
                        "pdf_hash": "",
                        "start": 0,
                        "end": 869,
                        "sentence_offsets": [
                            {
                                "start": 0,
                                "end": 142
                            },
                            {
                                "start": 143,
                                "end": 269
                            },
                            {
                                "start": 270,
                                "end": 400
                            },
                            {
                                "start": 401,
                                "end": 714
                            },
                            {
                                "start": 715,
                                "end": 869
                            }
                        ],
                        "ref_mentions": [
                            "7200347"
                        ],
                        "quote": "Knowledge distillation is a technique that transfers knowledge from large, complex CNNs (teachers) to smaller, more efficient CNNs (students). The student network is trained to mimic the behavior of the teacher, resulting in a compact model with comparable performance. This concept can be traced back to the pioneering work in [101] and has since been extended to the context of deep learning (Hinton et al., 2015). The core challenge of knowledge distillation revolves around the techniques used to transfer knowledge from the teacher model to the student model, which involves three fundamental components-knowledge, distillation algorithms-and the architecture defining the relationship between the teacher and student models. In this context, knowledge manifests in various forms, including logits, activations, or features extracted from intermediate layers of the teacher model."
                    }
                ]
            },
            {
                "idx": 42,
                "key": "[273233176 | Wang et al. | 2024 | Citations: 0]",
                "snippets": "Knowledge Distillation is a technique for model compression that facilitates a smaller student model in learning from a larger teacher model. The student acquires knowledge by imitating various aspects of the teacher, such as its intermediate features [41], prediction logits [22], or activation boundaries (Heo et al., 2018)]. This approach was originally put forth by Hinton et al. [21] to supervise students based on the hard and soft label's output by the teacher, and nowadays there is a lot of work on using distillation for knowledge transfer to help the model get better performance.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {
                    "[53213211 | Heo et al. | 2018 | Citations: 527]": "An activation boundary for a neuron refers to a separating hyperplane that determines whether the neuron is activated or deactivated. It has been long considered in neural networks that the activations of neurons, rather than their exact output values, play the most important role in forming classificationfriendly partitions of the hidden feature space. However, as far as we know, this aspect of neural networks has not been considered in the literature of knowledge transfer. In this paper, we propose a knowledge transfer method via distillation of activation boundaries formed by hidden neurons. For the distillation, we propose an activation transfer loss that has the minimum value when the boundaries generated by the student coincide with those by the teacher. Since the activation transfer loss is not differentiable, we design a piecewise differentiable loss approximating the activation transfer loss. By the proposed method, the student learns a separating boundary between activation region and deactivation region formed by each neuron in the teacher. Through the experiments in various aspects of knowledge transfer, it is verified that the proposed method outperforms the current state-of-the-art."
                },
                "metadata": [
                    {
                        "section_title": "Knowledge Distillation",
                        "pdf_hash": "",
                        "start": 0,
                        "end": 577,
                        "sentence_offsets": [
                            {
                                "start": 0,
                                "end": 141
                            },
                            {
                                "start": 142,
                                "end": 313
                            },
                            {
                                "start": 314,
                                "end": 577
                            }
                        ],
                        "ref_mentions": [
                            "53213211"
                        ],
                        "quote": "Knowledge Distillation is a technique for model compression that facilitates a smaller student model in learning from a larger teacher model. The student acquires knowledge by imitating various aspects of the teacher, such as its intermediate features [41], prediction logits [22], or activation boundaries (Heo et al., 2018)]. This approach was originally put forth by Hinton et al. [21] to supervise students based on the hard and soft label's output by the teacher, and nowadays there is a lot of work on using distillation for knowledge transfer to help the model get better performance."
                    }
                ]
            },
            {
                "idx": 43,
                "key": "[275847824 | Rezvani et al. | 2025 | Citations: 1]",
                "snippets": "Knowledge distillation (KD) is a method to transfer knowledge from a larger network or ensemble of networks (teacher model) to a smaller and less complex model (student model) (Hinton et al., 2015). It can be considered as a way to compress a larger model into a smaller one, making it more efficient and less resource-hungry, which is most effective for deploying models on edge devices (Gou et al., 2021).",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {},
                "metadata": [
                    {
                        "section_title": ". Knowledge distillation",
                        "pdf_hash": "",
                        "start": 0,
                        "end": 407,
                        "sentence_offsets": [
                            {
                                "start": 0,
                                "end": 198
                            },
                            {
                                "start": 199,
                                "end": 407
                            }
                        ],
                        "ref_mentions": [],
                        "quote": "Knowledge distillation (KD) is a method to transfer knowledge from a larger network or ensemble of networks (teacher model) to a smaller and less complex model (student model) (Hinton et al., 2015). It can be considered as a way to compress a larger model into a smaller one, making it more efficient and less resource-hungry, which is most effective for deploying models on edge devices (Gou et al., 2021)."
                    }
                ]
            },
            {
                "idx": 44,
                "key": "[277398866 | Zhang et al. | 2025 | Citations: 0]",
                "snippets": "Knowledge distillation is a technique used to compress and transfer knowledge from a large, complex model(the teacher) to a smaller, simpler model(the student). This process involves the student model learning to mimic the behavior of the teacher model, thereby achieving similar performance with fewer parameters and reduced computational requirements [33], [34]. The primary objective of knowledge distillation is to retain the accuracy and capabilities of the large model while significantly reducing its size and inference time.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {},
                "metadata": [
                    {
                        "section_title": "B. OVERVIEW OF KNOWLEDGE DISTILLATION",
                        "pdf_hash": "",
                        "start": 0,
                        "end": 532,
                        "sentence_offsets": [
                            {
                                "start": 0,
                                "end": 160
                            },
                            {
                                "start": 161,
                                "end": 364
                            },
                            {
                                "start": 365,
                                "end": 532
                            }
                        ],
                        "ref_mentions": [],
                        "quote": "Knowledge distillation is a technique used to compress and transfer knowledge from a large, complex model(the teacher) to a smaller, simpler model(the student). This process involves the student model learning to mimic the behavior of the teacher model, thereby achieving similar performance with fewer parameters and reduced computational requirements [33], [34]. The primary objective of knowledge distillation is to retain the accuracy and capabilities of the large model while significantly reducing its size and inference time."
                    }
                ]
            },
            {
                "idx": 45,
                "key": "[277507254 | Lan et al. | 2025 | Citations: 0]",
                "snippets": "Knowledge distillation The core concept of knowledge distillation involves transferring \"knowledge\" from a teacher model, which is typically a large and high-performance model, to a student model, which is typically a smaller and lightweight model. This transfer aims to enable the student model to acquire the reasoning and generalization capabilities of the teacher model.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {},
                "metadata": [
                    {
                        "section_title": "Related work",
                        "pdf_hash": "",
                        "start": 0,
                        "end": 374,
                        "sentence_offsets": [
                            {
                                "start": 0,
                                "end": 248
                            },
                            {
                                "start": 249,
                                "end": 374
                            }
                        ],
                        "ref_mentions": [],
                        "quote": "Knowledge distillation The core concept of knowledge distillation involves transferring \"knowledge\" from a teacher model, which is typically a large and high-performance model, to a student model, which is typically a smaller and lightweight model. This transfer aims to enable the student model to acquire the reasoning and generalization capabilities of the teacher model."
                    }
                ]
            },
            {
                "idx": 46,
                "key": "[277724285 | Zhao et al. | 2025 | Citations: 0]",
                "snippets": "Typical knowledge distillation involves transferring knowledge from a large model (or an ensemble of models) to a smaller, more efficient model. This process typically includes three key components: one or more pre-trained teacher models, an initial student model, and a training dataset (as shown in Figure 5). The teacher and student models output class prediction probabilities, and the distillation loss is computed by comparing these predictions. Additionally, the loss between the predicted and true class labels is calculated. Both losses are used to train the student model, optimizing it to achieve an accuracy similar to the teacher model. Knowledge distillation aims to reduce the performance gap between the teacher and student models by efficiently determining their structure.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {},
                "metadata": [
                    {
                        "section_title": "Knowledge Distillation",
                        "pdf_hash": "",
                        "start": 847,
                        "end": 1637,
                        "sentence_offsets": [
                            {
                                "start": 847,
                                "end": 991
                            },
                            {
                                "start": 992,
                                "end": 1158
                            },
                            {
                                "start": 1159,
                                "end": 1298
                            },
                            {
                                "start": 1299,
                                "end": 1380
                            },
                            {
                                "start": 1381,
                                "end": 1496
                            },
                            {
                                "start": 1497,
                                "end": 1637
                            }
                        ],
                        "ref_mentions": [],
                        "quote": "Typical knowledge distillation involves transferring knowledge from a large model (or an ensemble of models) to a smaller, more efficient model. This process typically includes three key components: one or more pre-trained teacher models, an initial student model, and a training dataset (as shown in Figure 5). The teacher and student models output class prediction probabilities, and the distillation loss is computed by comparing these predictions. Additionally, the loss between the predicted and true class labels is calculated. Both losses are used to train the student model, optimizing it to achieve an accuracy similar to the teacher model. Knowledge distillation aims to reduce the performance gap between the teacher and student models by efficiently determining their structure."
                    }
                ]
            },
            {
                "idx": 47,
                "key": "[277940190 | Yang et al. | 2025 | Citations: 0]",
                "snippets": "Knowledge Distillation (KD) is a technique for transferring knowledge from complex teacher models to simpler student models, significantly enhancing model efficiency and accuracy.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {},
                "metadata": [
                    {
                        "quote": "Knowledge Distillation (KD) is a technique for transferring knowledge from complex teacher models to simpler student models, significantly enhancing model efficiency and accuracy.",
                        "pdf_hash": "",
                        "section_title": "abstract"
                    }
                ]
            }
        ]
    },
    "cluster": {
        "cost": 0.031944,
        "cot": "I need to plan candidate dimensions to clearly answer the query about knowledge distillation with size-matched or smaller teacher models.\n\nFirst, I need to identify the core question in the query. The user seems to be asking about knowledge distillation scenarios where the teacher model is not larger than the student model, which goes against the conventional understanding of knowledge distillation. They want to know if work exists where the teacher model is size-matched or smaller than the student model, and whether knowledge transfer is still effective in such scenarios.\n\nBased on this understanding, I'll create the following dimensions:\n\n1. \"Introduction to Knowledge Distillation\" - This should be a synthesis paragraph to provide the foundational understanding of what knowledge distillation is, its traditional setup with a larger teacher and smaller student model, and its primary purpose. This will help establish the context for the query.\n\n2. \"Traditional Knowledge Distillation Setup\" - This should be a synthesis paragraph explaining the conventional understanding of knowledge distillation where the teacher model is larger than the student model. This will help clarify what is considered the \"typical\" setup that the query is questioning.\n\n3. \"Size-Matched Knowledge Distillation\" - This should be a synthesis paragraph focusing on research where the teacher and student models are of similar size. This directly addresses part of the query about \"size-matched\" teacher models.\n\n4. \"Smaller Teacher Models in Knowledge Distillation\" - This should be a synthesis paragraph examining if there's evidence of effective knowledge transfer when the teacher model is actually smaller than the student model. This addresses the other part of the query.\n\n5. \"Effectiveness of Non-Traditional Knowledge Distillation Approaches\" - This should be a synthesis paragraph discussing the effectiveness of knowledge transfer in these non-traditional setups (when teacher is not larger), addressing the part of the query about whether such knowledge transfer is \"effective.\"\n\nThese dimensions will provide a comprehensive answer to the query while maintaining a logical flow from introducing the concept to addressing the specific question about non-traditional teacher-student size relationships in knowledge distillation.",
        "plan": {
            "Introduction to Knowledge Distillation (synthesis)": [
                0,
                2,
                3,
                5,
                7,
                8,
                11,
                12,
                14,
                15,
                16,
                17,
                18,
                19,
                23,
                24,
                26,
                27,
                28,
                30,
                31,
                32,
                33,
                35,
                36,
                37,
                38,
                39,
                40,
                41,
                42,
                43,
                44,
                45,
                46,
                47
            ],
            "Traditional Knowledge Distillation Setup (synthesis)": [
                0,
                1,
                2,
                3,
                4,
                5,
                6,
                7,
                8,
                9,
                10,
                12,
                13,
                19,
                20,
                22,
                24,
                25,
                29,
                30,
                34,
                36,
                37
            ],
            "Size-Matched Knowledge Distillation (synthesis)": [
                1,
                21,
                30
            ],
            "Smaller Teacher Models in Knowledge Distillation (synthesis)": [],
            "Effectiveness of Non-Traditional Knowledge Distillation Approaches (synthesis)": [
                1,
                21,
                30
            ]
        },
        "model": "claude-3-7-sonnet-20250219"
    },
    "summary": {
        "sections": [
            {
                "title": "Introduction to Knowledge Distillation",
                "tldr": "Knowledge distillation is a technique that transfers knowledge from a large, complex model (teacher) to a smaller, more efficient model (student) to maintain performance while reducing computational demands. Originally introduced for model compression, it has become a fundamental approach for deploying deep learning models in resource-constrained environments. (22 sources)",
                "text": "\nKnowledge distillation (KD) is a technique that enables the transfer of knowledge from a complex model, known as the teacher model, to a simpler model, called the student model <Paper corpusId=\"232076059\" paperTitle=\"(Keser et al., 2021)\" isShortName></Paper> <Paper corpusId=\"227228204\" paperTitle=\"(Ku et al., 2020)\" isShortName></Paper>. The concept was initially introduced by Bucila et al. in 2006 as a model compression method <Paper corpusId=\"253734490\" paperTitle=\"(Berdoz et al., 2022)\" isShortName></Paper> <Paper corpusId=\"266693464\" paperTitle=\"(Echim et al., 2023)\" isShortName></Paper>, and was later generalized and popularized by Hinton et al. in 2015 <Paper corpusId=\"267413258\" paperTitle=\"(Sghaier et al., 2024)\" isShortName></Paper> <Paper corpusId=\"7200347\" paperTitle=\"(Hinton et al., 2015)\" isShortName></Paper>. The primary goal of knowledge distillation is to enable smaller models to achieve performance comparable to larger models while reducing computational requirements and memory usage <Paper corpusId=\"270870796\" paperTitle=\"(Mai et al., 2024)\" isShortName></Paper> <Paper corpusId=\"255125462\" paperTitle=\"(Li et al., 2022)\" isShortName></Paper>.\n\nIn the traditional knowledge distillation framework, the teacher model is typically a large, high-capacity network or an ensemble of models that has been pre-trained to achieve high performance <Paper corpusId=\"174800781\" paperTitle=\"(Wei et al., 2019)\" isShortName></Paper> <Paper corpusId=\"243861089\" paperTitle=\"(Xu et al., 2021)\" isShortName></Paper>. The student model, which is usually smaller and has fewer parameters, is then trained to mimic the behavior of the teacher model <Paper corpusId=\"255595916\" paperTitle=\"(Zhou et al., 2023)\" isShortName></Paper> <Paper corpusId=\"257102399\" paperTitle=\"(Shi et al., 2023)\" isShortName></Paper>. This process involves the student learning not just from the ground truth labels (hard targets) but also from the probability distributions or intermediate representations produced by the teacher (soft targets) <Paper corpusId=\"250526328\" paperTitle=\"(Liu et al., 2022)\" isShortName></Paper>.\n\nThe knowledge transfer process generally follows a three-step approach as described by Jaiswal et al.: first, training a large teacher model that performs well; second, computing the teacher's predictions on the available data (referred to as knowledge or soft targets); and finally, training the smaller student model using this knowledge <Paper corpusId=\"234336288\" paperTitle=\"(Jaiswal et al., 2021)\" isShortName></Paper>. By learning from the teacher's outputs, the student model can capture not only the correct classifications but also the relationships between different classes that the teacher has learned <Paper corpusId=\"270389751\" paperTitle=\"(Li et al., 2024)\" isShortName></Paper>.\n\nKnowledge distillation has become particularly important in scenarios where deploying large, computation-heavy models is impractical, such as on mobile devices or embedded systems with limited resources <Paper corpusId=\"219559263\" paperTitle=\"(Gou et al., 2020)\" isShortName></Paper> <Paper corpusId=\"267688311\" paperTitle=\"(Si et al., 2024)\" isShortName></Paper>. The technique creates a good trade-off between model size and accuracy, making it possible to deploy deep learning models in resource-constrained environments while maintaining acceptable performance <Paper corpusId=\"262084420\" paperTitle=\"(Capogrosso et al., 2023)\" isShortName></Paper> <Paper corpusId=\"255266316\" paperTitle=\"(Zein et al., 2022)\" isShortName></Paper>.\n\nAs research in this area has progressed, various forms of knowledge (such as logits, activations, or features) and distillation strategies have been developed to enhance the effectiveness of the knowledge transfer process <Paper corpusId=\"272564024\" paperTitle=\"(Xie et al., 2024)\" isShortName></Paper> <Paper corpusId=\"273233176\" paperTitle=\"(Wang et al., 2024)\" isShortName></Paper>. These advancements have expanded the application of knowledge distillation beyond model compression to include transfer learning and improving model performance in specific domains <Paper corpusId=\"260599456\" paperTitle=\"(Tao et al., 2023)\" isShortName></Paper>.",
                "citations": [
                    {
                        "id": "(Keser et al., 2021)",
                        "snippets": [
                            "Knowledge distillation (KD) is the process of transferring knowledge between networks, where one usually aims to transfer the knowledge of a big network (teacher) to a smaller/more compact network (student)."
                        ],
                        "paper": {
                            "corpus_id": 232076059,
                            "title": "PURSUhInT: In Search of Informative Hint Points Based on Layer Clustering for Knowledge Distillation",
                            "authors": [
                                {
                                    "authorId": "72263298",
                                    "name": "Reyhan Kevser Keser"
                                },
                                {
                                    "authorId": "27583231",
                                    "name": "Aydin Ayanzadeh"
                                },
                                {
                                    "authorId": "12717766",
                                    "name": "O. A. Aghdam"
                                },
                                {
                                    "authorId": "2051713853",
                                    "name": "\u00c7aglar Kilcioglu"
                                },
                                {
                                    "authorId": "2021714",
                                    "name": "B. U. T\u00f6reyin"
                                },
                                {
                                    "authorId": "2877453",
                                    "name": "N. K. Ure"
                                }
                            ],
                            "year": 2021,
                            "venue": "Expert systems with applications",
                            "n_citations": 7
                        },
                        "score": 0.97265625
                    },
                    {
                        "id": "(Ku et al., 2020)",
                        "snippets": [
                            "Knowledge distillation is an effective model compression technique in which a compact model (student) is trained under the supervision of a larger pre-trained model or an ensemble of models (teacher)."
                        ],
                        "paper": {
                            "corpus_id": 227228204,
                            "title": "A Selective Survey on Versatile Knowledge Distillation Paradigm for Neural Network Models",
                            "authors": [
                                {
                                    "authorId": "92856269",
                                    "name": "J. Ku"
                                },
                                {
                                    "authorId": "31119623",
                                    "name": "Jihun Oh"
                                },
                                {
                                    "authorId": "2417832",
                                    "name": "Youngyoon Lee"
                                },
                                {
                                    "authorId": "97319268",
                                    "name": "Gaurav Pooniwala"
                                },
                                {
                                    "authorId": "2119014436",
                                    "name": "Sangjeong Lee"
                                }
                            ],
                            "year": 2020,
                            "venue": "arXiv.org",
                            "n_citations": 3
                        },
                        "score": 0.9892578125
                    },
                    {
                        "id": "(Berdoz et al., 2022)",
                        "snippets": [
                            "The concept of knowledge distillation (KD) originated in Bucila et al. (Bucila et al., 2006) as a way of compressing models, and was later generalized by Hinton et al. [19] (see Gou et al. (Gou et al., 2020) for an overview of the field). The standard use case for KD is that of a Teacher-Student (or offline) configuration, in which the teacher model (usually a large and well-trained model) transfers its knowledge to the student model (usually a smaller model) by sharing its last layer activations on a given transfer dataset (see Fig. 1a)."
                        ],
                        "paper": {
                            "corpus_id": 253734490,
                            "title": "Scalable Collaborative Learning via Representation Sharing",
                            "authors": [
                                {
                                    "authorId": "2183082271",
                                    "name": "Fr'ed'eric Berdoz"
                                },
                                {
                                    "authorId": "2034349211",
                                    "name": "Abhishek Singh"
                                },
                                {
                                    "authorId": "2456863",
                                    "name": "Martin Jaggi"
                                },
                                {
                                    "authorId": "2070747078",
                                    "name": "Ramesh Raskar"
                                }
                            ],
                            "year": 2022,
                            "venue": "arXiv.org",
                            "n_citations": 3
                        },
                        "score": 0.97705078125
                    },
                    {
                        "id": "(Echim et al., 2023)",
                        "snippets": [
                            "A practical method for compressing models is knowledge distillation, which facilitates information transfer from a large teacher network to a small student network. Initially introduced by Bucila et al. (Bucila et al., 2006) and later generalized by Hinton et al. (Hinton et al., 2015), this method has gained popularity across multiple machine learning applications. Unlike conventional training, knowledge distillation involves learning the student network to emulate the outputs of the teacher model, typically represented as probability distributions over classes."
                        ],
                        "paper": {
                            "corpus_id": 266693464,
                            "title": "Explainability-Driven Leaf Disease Classification Using Adversarial Training and Knowledge Distillation",
                            "authors": [
                                {
                                    "authorId": "2219976776",
                                    "name": "Sebastian-Vasile Echim"
                                },
                                {
                                    "authorId": "2279431357",
                                    "name": "Iulian-Marius Taiatu"
                                },
                                {
                                    "authorId": "3046903",
                                    "name": "Dumitru-Clementin Cercel"
                                },
                                {
                                    "authorId": "2261270760",
                                    "name": "Florin-Catalin Pop"
                                }
                            ],
                            "year": 2023,
                            "venue": "International Conference on Agents and Artificial Intelligence",
                            "n_citations": 1
                        },
                        "score": 0.97314453125
                    },
                    {
                        "id": "(Sghaier et al., 2024)",
                        "snippets": [
                            "Knowledge distillation is a popular technique that was first introduced by Hinton et al. [18] as a method to transfer knowledge from a complex model, also known as the teacher, to a smaller and faster model, called the student. The main goal of knowledge distillation is to transfer the learned knowledge of the teacher model to the student model so that it can achieve comparable or even better performance on a target task."
                        ],
                        "paper": {
                            "corpus_id": 267413258,
                            "title": "Improving the Learning of Code Review Successive Tasks with Cross-Task Knowledge Distillation",
                            "authors": [
                                {
                                    "authorId": "1470757062",
                                    "name": "O. Sghaier"
                                },
                                {
                                    "authorId": "9460712",
                                    "name": "H. Sahraoui"
                                }
                            ],
                            "year": 2024,
                            "venue": "Proc. ACM Softw. Eng.",
                            "n_citations": 10
                        },
                        "score": 0.994140625
                    },
                    {
                        "id": "(Hinton et al., 2015)",
                        "snippets": [
                            "A very simple way to improve the performance of almost any machine learning algorithm is to train many different models on the same data and then to average their predictions. Unfortunately, making predictions using a whole ensemble of models is cumbersome and may be too computationally expensive to allow deployment to a large number of users, especially if the individual models are large neural nets. Caruana and his collaborators have shown that it is possible to compress the knowledge in an ensemble into a single model which is much easier to deploy and we develop this approach further using a different compression technique. We achieve some surprising results on MNIST and we show that we can significantly improve the acoustic model of a heavily used commercial system by distilling the knowledge in an ensemble of models into a single model. We also introduce a new type of ensemble composed of one or more full models and many specialist models which learn to distinguish fine-grained classes that the full models confuse. Unlike a mixture of experts, these specialist models can be trained rapidly and in parallel."
                        ],
                        "paper": {
                            "corpus_id": 7200347,
                            "title": "Distilling the Knowledge in a Neural Network",
                            "authors": [
                                {
                                    "authorId": "1695689",
                                    "name": "Geoffrey E. Hinton"
                                },
                                {
                                    "authorId": "1689108",
                                    "name": "O. Vinyals"
                                },
                                {
                                    "authorId": "49959210",
                                    "name": "J. Dean"
                                }
                            ],
                            "year": 2015,
                            "venue": "arXiv.org",
                            "n_citations": 19742
                        },
                        "score": 0
                    },
                    {
                        "id": "(Mai et al., 2024)",
                        "snippets": [
                            "Knowledge Distillation (KD) is a model compression technique that transfers knowledge from a complex model (called the teacher model) to a smaller model (called the student model).This allows the student model to maintain high computational efficiency while achieving the performance of the teacher model."
                        ],
                        "paper": {
                            "corpus_id": 270870796,
                            "title": "From Efficient Multimodal Models to World Models: A Survey",
                            "authors": [
                                {
                                    "authorId": "2276937443",
                                    "name": "Xinji Mai"
                                },
                                {
                                    "authorId": "2261831274",
                                    "name": "Zeng Tao"
                                },
                                {
                                    "authorId": "2261891655",
                                    "name": "Junxiong Lin"
                                },
                                {
                                    "authorId": "2276807843",
                                    "name": "Haoran Wang"
                                },
                                {
                                    "authorId": "2276969811",
                                    "name": "Yang Chang"
                                },
                                {
                                    "authorId": "2212014366",
                                    "name": "Yanlan Kang"
                                },
                                {
                                    "authorId": "2276879376",
                                    "name": "Yan Wang"
                                },
                                {
                                    "authorId": "2276819302",
                                    "name": "Wenqiang Zhang"
                                }
                            ],
                            "year": 2024,
                            "venue": "arXiv.org",
                            "n_citations": 5
                        },
                        "score": 0.9814453125
                    },
                    {
                        "id": "(Li et al., 2022)",
                        "snippets": [
                            "Knowledge distillation is a model compression technology that transfers the knowledge from a larger deep neural network into a small network."
                        ],
                        "paper": {
                            "corpus_id": 255125462,
                            "title": "Prototype-guided Cross-task Knowledge Distillation for Large-scale Models",
                            "authors": [
                                {
                                    "authorId": "2158676096",
                                    "name": "Deng Li"
                                },
                                {
                                    "authorId": "48352212",
                                    "name": "Aming Wu"
                                },
                                {
                                    "authorId": "144622313",
                                    "name": "Yahong Han"
                                },
                                {
                                    "authorId": "2149898858",
                                    "name": "Qingwen Tian"
                                }
                            ],
                            "year": 2022,
                            "venue": "arXiv.org",
                            "n_citations": 3
                        },
                        "score": 0.99072265625
                    },
                    {
                        "id": "(Wei et al., 2019)",
                        "snippets": [
                            "Knowledge distillation is a class of methods which transfers knowledge from a pre-trained teacher model T, to a student model S. The teacher model can be a model with large capacity (Bucila et al., 2006) or an ensemble of several models (Hinton et al., 2015)."
                        ],
                        "paper": {
                            "corpus_id": 174800781,
                            "title": "Online Distilling from Checkpoints for Neural Machine Translation",
                            "authors": [
                                {
                                    "authorId": "134085586",
                                    "name": "Hao-Ran Wei"
                                },
                                {
                                    "authorId": "2046010",
                                    "name": "Shujian Huang"
                                },
                                {
                                    "authorId": "2115256564",
                                    "name": "Ran Wang"
                                },
                                {
                                    "authorId": "3035069",
                                    "name": "Xinyu Dai"
                                },
                                {
                                    "authorId": "1838162",
                                    "name": "Jiajun Chen"
                                }
                            ],
                            "year": 2019,
                            "venue": "North American Chapter of the Association for Computational Linguistics",
                            "n_citations": 32
                        },
                        "score": 0.98046875
                    },
                    {
                        "id": "(Xu et al., 2021)",
                        "snippets": [
                            "The idea of knowledge distillation (KD) is exploiting the knowledge inside a large trained \"teacher\" model to help the training of a \"student\" model (Bucila et al., 2006)(Ba et al., 2013)Hinton et al., 2015). In this way, we can use a smaller student model to distill a trained model as a replacement for inference."
                        ],
                        "paper": {
                            "corpus_id": 243861089,
                            "title": "A Survey on Green Deep Learning",
                            "authors": [
                                {
                                    "authorId": "47883405",
                                    "name": "Jingjing Xu"
                                },
                                {
                                    "authorId": "150341221",
                                    "name": "Wangchunshu Zhou"
                                },
                                {
                                    "authorId": "2068057294",
                                    "name": "Zhiyi Fu"
                                },
                                {
                                    "authorId": null,
                                    "name": "Hao Zhou"
                                },
                                {
                                    "authorId": "2151530622",
                                    "name": "Lei Li"
                                }
                            ],
                            "year": 2021,
                            "venue": "arXiv.org",
                            "n_citations": 84
                        },
                        "score": 0.9765625
                    },
                    {
                        "id": "(Zhou et al., 2023)",
                        "snippets": [
                            "Knowledge distillation is the technique where knowledge learned by a larger teacher model is transferred to a smaller student model (Gou et al., 2020), Hinton et al., 2015(Wang et al., 2020). The main idea is that the student model mimics the teacher model to achieve a similar or even a superior performance."
                        ],
                        "paper": {
                            "corpus_id": 255595916,
                            "title": "Synthetic data generation method for data-free knowledge distillation in regression neural networks",
                            "authors": [
                                {
                                    "authorId": "2163011403",
                                    "name": "Tianxun Zhou"
                                },
                                {
                                    "authorId": "4287306",
                                    "name": "K. Chiam"
                                }
                            ],
                            "year": 2023,
                            "venue": "Expert systems with applications",
                            "n_citations": 7
                        },
                        "score": 0.97998046875
                    },
                    {
                        "id": "(Shi et al., 2023)",
                        "snippets": [
                            "Knowledge distillation is a model compression technique proposed by Hinton et al. in 2015 [9], which could effectively train a smaller student model by learning from a larger teacher model. As shown in Figure 2, a general knowledge distillation framework consists of three major components: teacher-student network architecture, knowledge, as well as distillation algorithm. The main idea of knowledge distillation is that the student model mimics the teacher model in order to achieve a comparable or even a superior performance (Gou et al., 2020). Teacher network usually has more complicated network architecture and learns from the data first. After the training of teacher network, the useful knowledge could be distilled from the teacher network. Hereafter, student network learns the knowledge distilled from teacher network to mimic teacher's behavior together with the training data. The core of knowledge distillation is how to simplify model while maintaining relatively good performance."
                        ],
                        "paper": {
                            "corpus_id": 257102399,
                            "title": "Knowledge Distillation-based Information Sharing for Online Process Monitoring in Decentralized Manufacturing System",
                            "authors": [
                                {
                                    "authorId": "2144091000",
                                    "name": "Zhangyue Shi"
                                },
                                {
                                    "authorId": "2116210564",
                                    "name": "Yuxuan Li"
                                },
                                {
                                    "authorId": "2035538477",
                                    "name": "Chenang Liu"
                                }
                            ],
                            "year": 2023,
                            "venue": "Journal of Intelligent Manufacturing",
                            "n_citations": 8
                        },
                        "score": 0.990234375
                    },
                    {
                        "id": "(Liu et al., 2022)",
                        "snippets": [
                            "Although more layers and more parameters generally improve the accuracy of the models, such big models generally have high computational complexity and require big memory, which exceed the capacity of small devices for inference and incurs long training time",
                            "Knowledge distillation is based on the popular machine learning Softmax function and a temperature (Hinton et al., 2015) , which is defined in Formula 1",
                            "As shown in Figure 1, during the training of knowledge distillation, two neural networks are used: teacher model and student model. The student model is trained based on the combination of two loss values. The first loss value is calculated from a soft prediction, which contains the probability of each class calculated from the teacher model. The soft prediction is calculated using Formula 1. The other loss value corresponds to a hard prediction, which is the ground truth label from the training data."
                        ],
                        "paper": {
                            "corpus_id": 250526328,
                            "title": "Large\u2010scale knowledge distillation with elastic heterogeneous computing resources",
                            "authors": [
                                {
                                    "authorId": "2118971193",
                                    "name": "Ji Liu"
                                },
                                {
                                    "authorId": "9532787",
                                    "name": "Daxiang Dong"
                                },
                                {
                                    "authorId": "2108249583",
                                    "name": "Xi Wang"
                                },
                                {
                                    "authorId": "2140532375",
                                    "name": "An Qin"
                                },
                                {
                                    "authorId": "2155445773",
                                    "name": "Xingjian Li"
                                },
                                {
                                    "authorId": "144255847",
                                    "name": "P. Valduriez"
                                },
                                {
                                    "authorId": "1721158",
                                    "name": "D. Dou"
                                },
                                {
                                    "authorId": "3046102",
                                    "name": "Dianhai Yu"
                                }
                            ],
                            "year": 2022,
                            "venue": "Concurrency and Computation",
                            "n_citations": 6
                        },
                        "score": 0.9775390625
                    },
                    {
                        "id": "(Jaiswal et al., 2021)",
                        "snippets": [
                            "Knowledge distillation (KD) was introduced by [30] as: \n\n\u2022 Train a large model that performs and generalizes very well. This is called the teacher model. \n\n\u2022 Take all the data you have and compute the predictions of the teacher model. The total dataset with these predictions is called the knowledge, and the predictions themselves are often referred to as soft targets. This is the knowledge distillation step. \n\n\u2022 Use the previously obtained knowledge to train the smaller network, called the student model.\n\nKnowledge distillation starts with training a larger model, the teacher 'T'. As it is trained on a heavier platform (GPU), it achieves high performance. Then a lightweight model known as student 'S' is deployed to learn from 'T'. Now, 'S' is supposed to give comparable performance as 'T' but with less memory and more speed."
                        ],
                        "paper": {
                            "corpus_id": 234336288,
                            "title": "Performance Analysis of Deep Neural Network based on Transfer Learning for Pet Classification",
                            "authors": [
                                {
                                    "authorId": "2139643077",
                                    "name": "Bhavesh Jaiswal"
                                },
                                {
                                    "authorId": "32136431",
                                    "name": "Nagendra Gajjar"
                                }
                            ],
                            "year": 2021,
                            "venue": "International Journal of Advanced Computer Science and Applications",
                            "n_citations": 0
                        },
                        "score": 0.984375
                    },
                    {
                        "id": "(Li et al., 2024)",
                        "snippets": [
                            "Knowledge distillation (KD), as initially proposed by Hinton et al. (Hinton et al., 2015) , aims to supervise the training convergence of a student network, a smaller model, with a teacher network, a larger model.This method controls the extent of knowledge transfer between two networks using a temperature parameter, T, to control the transfer of soft-label dark knowledge."
                        ],
                        "paper": {
                            "corpus_id": 270389751,
                            "title": "Multistage feature fusion knowledge distillation",
                            "authors": [
                                {
                                    "authorId": "2307186661",
                                    "name": "Gang Li"
                                },
                                {
                                    "authorId": "2307436738",
                                    "name": "Kun Wang"
                                },
                                {
                                    "authorId": "2305765306",
                                    "name": "Pengfei Lv"
                                },
                                {
                                    "authorId": "2305752302",
                                    "name": "Pan He"
                                },
                                {
                                    "authorId": "2287501699",
                                    "name": "Zheng Zhou"
                                },
                                {
                                    "authorId": "2817613",
                                    "name": "Chuanyun Xu"
                                }
                            ],
                            "year": 2024,
                            "venue": "Scientific Reports",
                            "n_citations": 1
                        },
                        "score": 0.98046875
                    },
                    {
                        "id": "(Gou et al., 2020)",
                        "snippets": [
                            "In recent years, deep neural networks have been successful in both industry and academia, especially for computer vision tasks. The great success of deep learning is mainly due to its scalability to encode large-scale data and to maneuver billions of model parameters. However, it is a challenge to deploy these cumbersome deep models on devices with limited resources, e.g., mobile phones and embedded devices, not only because of the high computational complexity but also the large storage requirements. To this end, a variety of model compression and acceleration techniques have been developed. As a representative type of model compression and acceleration, knowledge distillation effectively learns a small student model from a large teacher model. It has received rapid increasing attention from the community. This paper provides a comprehensive survey of knowledge distillation from the perspectives of knowledge categories, training schemes, teacher\u2013student architecture, distillation algorithms, performance comparison and applications. Furthermore, challenges in knowledge distillation are briefly reviewed and comments on future research are discussed and forwarded."
                        ],
                        "paper": {
                            "corpus_id": 219559263,
                            "title": "Knowledge Distillation: A Survey",
                            "authors": [
                                {
                                    "authorId": "38978232",
                                    "name": "Jianping Gou"
                                },
                                {
                                    "authorId": "2425630",
                                    "name": "B. Yu"
                                },
                                {
                                    "authorId": "144555237",
                                    "name": "S. Maybank"
                                },
                                {
                                    "authorId": "143719920",
                                    "name": "D. Tao"
                                }
                            ],
                            "year": 2020,
                            "venue": "International Journal of Computer Vision",
                            "n_citations": 2984
                        },
                        "score": 0
                    },
                    {
                        "id": "(Si et al., 2024)",
                        "snippets": [
                            "Knowledge distillation is a commonly used technique in deep learning, as proposed by Hinton et al. in 2015 [17]. It is used to transfer knowledge from a large model to a smaller one, resulting in model compression and acceleration. The goal of knowledge distillation is to improve the accuracy and generalizability of the student network by transferring knowledge from the teacher network. Auxiliary information during training is usually obtained from the output of the teacher network. Such techniques have been applied in various domains, including natural language processing, computer vision, and speech recognition. An important use case is the deployment of deep learning models on mobile devices, which often have limited computational resources and require smaller, lightweight models. Knowledge distillation typically involves a larger teacher network and a smaller student network. The teacher network is usually trained on large amounts of data and therefore has higher accuracy and more comprehensive knowledge. Compared to the teacher network, the student network is trained on less data and has fewer parameters. Nevertheless, some student networks can surpass the accuracy of the teacher network."
                        ],
                        "paper": {
                            "corpus_id": 267688311,
                            "title": "Breast Cancer Histopathology Images Classification Through Multi-View Augmented Contrastive Learning and Pre-Learning Knowledge Distillation",
                            "authors": [
                                {
                                    "authorId": "2284255154",
                                    "name": "Jialong Si"
                                },
                                {
                                    "authorId": "2284258056",
                                    "name": "Wei Jia"
                                },
                                {
                                    "authorId": "2284308867",
                                    "name": "Haifeng Jiang"
                                }
                            ],
                            "year": 2024,
                            "venue": "IEEE Access",
                            "n_citations": 5
                        },
                        "score": 0.9833984375
                    },
                    {
                        "id": "(Capogrosso et al., 2023)",
                        "snippets": [
                            "This technique transfers knowledge from a large, complex model (teacher) to a smaller, simpler model (student) (Gou et al., 2020). This process is important for various reasons, such as reducing computational demands or enhancing model performance on specific tasks. Knowledge types, distillation strategies, and teacher-student architectures are vital factors in student learning during knowledge distillation. The subsequent paragraphs introduce the key categories of knowledge types and distillation strategies",
                            "Offline distillation is a two-stage strategy, where the teacher model is first trained on a set of training samples, and then the trained teacher model is used to guide the student model by extracting intermediate features or logits (Zhao et al., 2019)",
                            "In general, knowledge distillation is used to achieve a good trade-off between small model size and an acceptable accuracy (Zein et al., 2022)."
                        ],
                        "paper": {
                            "corpus_id": 262084420,
                            "title": "A Machine Learning-Oriented Survey on Tiny Machine Learning",
                            "authors": [
                                {
                                    "authorId": "2135267479",
                                    "name": "Luigi Capogrosso"
                                },
                                {
                                    "authorId": "1396330675",
                                    "name": "Federico Cunico"
                                },
                                {
                                    "authorId": "1780197",
                                    "name": "D. Cheng"
                                },
                                {
                                    "authorId": "2243336023",
                                    "name": "Franco Fummi"
                                },
                                {
                                    "authorId": "2238815087",
                                    "name": "Marco Cristani"
                                }
                            ],
                            "year": 2023,
                            "venue": "IEEE Access",
                            "n_citations": 43
                        },
                        "score": 0.97607421875
                    },
                    {
                        "id": "(Zein et al., 2022)",
                        "snippets": [
                            "Traditionally, neural network inferencing on tiny hardware devices took place in a centralized server-based manner. With more real-time applications coming into play, where security and latency are a concern, there has become a need to move inferencing to the edge. This paper describes a machine learning pipeline to carry neural networks from their initial forms to compressed forms deployable on tiny hardware devices, while maintaining acceptable accuracies of the optimized models. We will review the different software optimization techniques used to compress neural networks to their deployable forms. The prototype is a proof of concept showing that applying knowledge distillation from a highly accurate ResNet20 model to a simple CNN student model, followed by post-training quantization, achieves good multi-class accuracy on a constrained Arduino Nano 33 BLE Sense at low power consumption and with low inferencing latency."
                        ],
                        "paper": {
                            "corpus_id": 255266316,
                            "title": "Implementation and Optimization of Neural Networks for Tiny Hardware Devices",
                            "authors": [
                                {
                                    "authorId": "2198763466",
                                    "name": "Hadi Al Zein"
                                },
                                {
                                    "authorId": "2198763611",
                                    "name": "Mohamad Aoude"
                                },
                                {
                                    "authorId": "2198763636",
                                    "name": "Youssef Harkous"
                                }
                            ],
                            "year": 2022,
                            "venue": "2022 International Conference on Smart Systems and Power Management (IC2SPM)",
                            "n_citations": 5
                        },
                        "score": 0
                    },
                    {
                        "id": "(Xie et al., 2024)",
                        "snippets": [
                            "Knowledge distillation is a technique that transfers knowledge from large, complex CNNs (teachers) to smaller, more efficient CNNs (students). The student network is trained to mimic the behavior of the teacher, resulting in a compact model with comparable performance. This concept can be traced back to the pioneering work in [101] and has since been extended to the context of deep learning (Hinton et al., 2015). The core challenge of knowledge distillation revolves around the techniques used to transfer knowledge from the teacher model to the student model, which involves three fundamental components-knowledge, distillation algorithms-and the architecture defining the relationship between the teacher and student models. In this context, knowledge manifests in various forms, including logits, activations, or features extracted from intermediate layers of the teacher model."
                        ],
                        "paper": {
                            "corpus_id": 272564024,
                            "title": "A Comprehensive Review of Hardware Acceleration Techniques and Convolutional Neural Networks for EEG Signals",
                            "authors": [
                                {
                                    "authorId": "2118595916",
                                    "name": "Yu Xie"
                                },
                                {
                                    "authorId": "2277106978",
                                    "name": "Stefan Oniga"
                                }
                            ],
                            "year": 2024,
                            "venue": "Italian National Conference on Sensors",
                            "n_citations": 1
                        },
                        "score": 0.98388671875
                    },
                    {
                        "id": "(Wang et al., 2024)",
                        "snippets": [
                            "Knowledge Distillation is a technique for model compression that facilitates a smaller student model in learning from a larger teacher model. The student acquires knowledge by imitating various aspects of the teacher, such as its intermediate features [41], prediction logits [22], or activation boundaries (Heo et al., 2018)]. This approach was originally put forth by Hinton et al. [21] to supervise students based on the hard and soft label's output by the teacher, and nowadays there is a lot of work on using distillation for knowledge transfer to help the model get better performance."
                        ],
                        "paper": {
                            "corpus_id": 273233176,
                            "title": "SNN-PAR: Energy Efficient Pedestrian Attribute Recognition via Spiking Neural Networks",
                            "authors": [
                                {
                                    "authorId": "2316447852",
                                    "name": "Haiyang Wang"
                                },
                                {
                                    "authorId": "2299062895",
                                    "name": "Qian Zhu"
                                },
                                {
                                    "authorId": "2325151513",
                                    "name": "Mowen She"
                                },
                                {
                                    "authorId": "2325202742",
                                    "name": "Yabo Li"
                                },
                                {
                                    "authorId": "2325294395",
                                    "name": "Haoyu Song"
                                },
                                {
                                    "authorId": "2325504629",
                                    "name": "Minghe Xu"
                                },
                                {
                                    "authorId": "2325835807",
                                    "name": "Xiao Wang"
                                }
                            ],
                            "year": 2024,
                            "venue": "arXiv.org",
                            "n_citations": 0
                        },
                        "score": 0.97216796875
                    },
                    {
                        "id": "(Tao et al., 2023)",
                        "snippets": [
                            "Knowledge Distillation is a widely used technique in model compression (Cheng et al., 2018), (Bucila et al., 2006) and transfer learning (Yim et al., 2017), which typically involves transferring knowledge from a large and complex model (often referred to as the teacher model) to a smaller and simpler model (often referred to as the student model). The goal of knowledge distillation is to improve the performance of the student model by leveraging the knowledge contained in the teacher model, while still maintaining the efficiency and simplicity of the student model."
                        ],
                        "paper": {
                            "corpus_id": 260599456,
                            "title": "Prototypes Sampling Mechanism for Class Incremental Learning",
                            "authors": [
                                {
                                    "authorId": "2213939405",
                                    "name": "Zhe Tao"
                                },
                                {
                                    "authorId": "50178505",
                                    "name": "Shucheng Huang"
                                },
                                {
                                    "authorId": "2155613761",
                                    "name": "Gang Wang"
                                }
                            ],
                            "year": 2023,
                            "venue": "IEEE Access",
                            "n_citations": 3
                        },
                        "score": 0.974609375
                    }
                ],
                "format": "synthesis",
                "table": null,
                "model": "claude-3-7-sonnet-20250219"
            },
            {
                "title": "Traditional Knowledge Distillation Setup",
                "tldr": "Traditional knowledge distillation involves training a large teacher model first, then transferring its knowledge to a smaller student model that learns to mimic the teacher's outputs. The teacher is typically more complex with higher capacity, while the student is designed to be more computationally efficient while maintaining acceptable performance. (18 sources)",
                "text": "\nKnowledge distillation (KD) in its traditional form follows a specific setup that has become the foundation for various distillation approaches. At its core, the traditional KD framework involves two key components: a teacher model and a student model <Paper corpusId=\"174800781\" paperTitle=\"(Wei et al., 2019)\" isShortName></Paper> <Paper corpusId=\"227228204\" paperTitle=\"(Ku et al., 2020)\" isShortName></Paper>.\n\nThe teacher model is typically a large, complex network with high capacity that has been pre-trained to achieve strong performance on a given task <Paper corpusId=\"232076059\" paperTitle=\"(Keser et al., 2021)\" isShortName></Paper> <Paper corpusId=\"232380330\" paperTitle=\"(Li et al., 2021)\" isShortName></Paper>. In some cases, the teacher can also be an ensemble of several models <Paper corpusId=\"174800781\" paperTitle=\"(Wei et al., 2019)\" isShortName></Paper> <Paper corpusId=\"7200347\" paperTitle=\"(Hinton et al., 2015)\" isShortName></Paper>. Due to its size and complexity, the teacher model often captures rich, deep information and features that are crucial for high task performance <Paper corpusId=\"269761365\" paperTitle=\"(Wang et al._1, 2024)\" isShortName></Paper>.\n\nThe student model, on the other hand, is designed to be more lightweight and compact, with fewer parameters and lower computational requirements <Paper corpusId=\"227228204\" paperTitle=\"(Ku et al., 2020)\" isShortName></Paper> <Paper corpusId=\"232076059\" paperTitle=\"(Keser et al., 2021)\" isShortName></Paper>. This smaller model is intended for deployment in resource-constrained environments, such as mobile devices or embedded systems <Paper corpusId=\"262084420\" paperTitle=\"(Capogrosso et al., 2023)\" isShortName></Paper> <Paper corpusId=\"255266316\" paperTitle=\"(Zein et al., 2022)\" isShortName></Paper>.\n\nThe traditional KD process follows a sequential approach. First, the teacher model is trained on the available data to achieve high performance. Then, the teacher's knowledge is distilled by computing its predictions (soft targets) on the training data. Finally, the student model is trained to mimic the teacher using both the ground truth labels (hard targets) and the teacher's soft targets <Paper corpusId=\"234336288\" paperTitle=\"(Jaiswal et al., 2021)\" isShortName></Paper> <Paper corpusId=\"250526328\" paperTitle=\"(Liu et al., 2022)\" isShortName></Paper>.\n\nThis training typically involves a combined loss function with two components: a conventional classification loss using the ground truth labels, and a distillation loss that measures the difference between the student's and teacher's predictions <Paper corpusId=\"250526328\" paperTitle=\"(Liu et al., 2022)\" isShortName></Paper> <Paper corpusId=\"258426697\" paperTitle=\"(Malik et al., 2023)\" isShortName></Paper>. The temperature parameter (T) introduced by Hinton et al. is used to control the softness of the probability distributions and the amount of \"dark knowledge\" transferred from teacher to student <Paper corpusId=\"270389751\" paperTitle=\"(Li et al., 2024)\" isShortName></Paper> <Paper corpusId=\"7200347\" paperTitle=\"(Hinton et al., 2015)\" isShortName></Paper>.\n\nThis \"offline\" distillation approach, where the teacher is first fully trained and then its weights are frozen before training the student, is the most common training scheme in traditional KD <Paper corpusId=\"262084420\" paperTitle=\"(Capogrosso et al., 2023)\" isShortName></Paper> <Paper corpusId=\"198179767\" paperTitle=\"(Zhao et al., 2019)\" isShortName></Paper>. The core idea is that the student model should learn to mimic the teacher's behavior and generalization capabilities <Paper corpusId=\"245974615\" paperTitle=\"(Ganta et al., 2021)\" isShortName></Paper> <Paper corpusId=\"257102399\" paperTitle=\"(Shi et al., 2023)\" isShortName></Paper> <Paper corpusId=\"219559263\" paperTitle=\"(Gou et al., 2020)\" isShortName></Paper>.\n\nThe traditional KD setup aims to achieve a good trade-off between model size and accuracy <Paper corpusId=\"262084420\" paperTitle=\"(Capogrosso et al., 2023)\" isShortName></Paper> <Paper corpusId=\"255266316\" paperTitle=\"(Zein et al., 2022)\" isShortName></Paper>. By leveraging the \"dark knowledge\" embedded in the teacher's outputs, the student can approximate the performance level of the larger model while requiring significantly fewer computational resources <Paper corpusId=\"248683566\" paperTitle=\"(Xu et al., 2022)\" isShortName></Paper> <Paper corpusId=\"167217261\" paperTitle=\"(Tan et al., 2019)\" isShortName></Paper> <Paper corpusId=\"219559263\" paperTitle=\"(Gou et al., 2020)\" isShortName></Paper>.",
                "citations": [
                    {
                        "id": "(Wei et al., 2019)",
                        "snippets": [
                            "Knowledge distillation is a class of methods which transfers knowledge from a pre-trained teacher model T, to a student model S. The teacher model can be a model with large capacity (Bucila et al., 2006) or an ensemble of several models (Hinton et al., 2015)."
                        ],
                        "paper": {
                            "corpus_id": 174800781,
                            "title": "Online Distilling from Checkpoints for Neural Machine Translation",
                            "authors": [
                                {
                                    "authorId": "134085586",
                                    "name": "Hao-Ran Wei"
                                },
                                {
                                    "authorId": "2046010",
                                    "name": "Shujian Huang"
                                },
                                {
                                    "authorId": "2115256564",
                                    "name": "Ran Wang"
                                },
                                {
                                    "authorId": "3035069",
                                    "name": "Xinyu Dai"
                                },
                                {
                                    "authorId": "1838162",
                                    "name": "Jiajun Chen"
                                }
                            ],
                            "year": 2019,
                            "venue": "North American Chapter of the Association for Computational Linguistics",
                            "n_citations": 32
                        },
                        "score": 0.98046875
                    },
                    {
                        "id": "(Ku et al., 2020)",
                        "snippets": [
                            "Knowledge distillation is an effective model compression technique in which a compact model (student) is trained under the supervision of a larger pre-trained model or an ensemble of models (teacher)."
                        ],
                        "paper": {
                            "corpus_id": 227228204,
                            "title": "A Selective Survey on Versatile Knowledge Distillation Paradigm for Neural Network Models",
                            "authors": [
                                {
                                    "authorId": "92856269",
                                    "name": "J. Ku"
                                },
                                {
                                    "authorId": "31119623",
                                    "name": "Jihun Oh"
                                },
                                {
                                    "authorId": "2417832",
                                    "name": "Youngyoon Lee"
                                },
                                {
                                    "authorId": "97319268",
                                    "name": "Gaurav Pooniwala"
                                },
                                {
                                    "authorId": "2119014436",
                                    "name": "Sangjeong Lee"
                                }
                            ],
                            "year": 2020,
                            "venue": "arXiv.org",
                            "n_citations": 3
                        },
                        "score": 0.9892578125
                    },
                    {
                        "id": "(Keser et al., 2021)",
                        "snippets": [
                            "Knowledge distillation (KD) is the process of transferring knowledge between networks, where one usually aims to transfer the knowledge of a big network (teacher) to a smaller/more compact network (student)."
                        ],
                        "paper": {
                            "corpus_id": 232076059,
                            "title": "PURSUhInT: In Search of Informative Hint Points Based on Layer Clustering for Knowledge Distillation",
                            "authors": [
                                {
                                    "authorId": "72263298",
                                    "name": "Reyhan Kevser Keser"
                                },
                                {
                                    "authorId": "27583231",
                                    "name": "Aydin Ayanzadeh"
                                },
                                {
                                    "authorId": "12717766",
                                    "name": "O. A. Aghdam"
                                },
                                {
                                    "authorId": "2051713853",
                                    "name": "\u00c7aglar Kilcioglu"
                                },
                                {
                                    "authorId": "2021714",
                                    "name": "B. U. T\u00f6reyin"
                                },
                                {
                                    "authorId": "2877453",
                                    "name": "N. K. Ure"
                                }
                            ],
                            "year": 2021,
                            "venue": "Expert systems with applications",
                            "n_citations": 7
                        },
                        "score": 0.97265625
                    },
                    {
                        "id": "(Li et al., 2021)",
                        "snippets": [
                            "Traditional Knowledge Distillation. Traditional distillation works transfer knowledge from a cumbersome teacher model to a light-weight student model. As such, a large-scale model has to be trained in advance, based on which various knowledge definitions and transfer strategies are proposed to boost the performance of the student model."
                        ],
                        "paper": {
                            "corpus_id": 232380330,
                            "title": "Distilling a Powerful Student Model via Online Knowledge Distillation",
                            "authors": [
                                {
                                    "authorId": "145904937",
                                    "name": "Shaojie Li"
                                },
                                {
                                    "authorId": "49352079",
                                    "name": "Mingbao Lin"
                                },
                                {
                                    "authorId": "2152543905",
                                    "name": "Yan Wang"
                                },
                                {
                                    "authorId": "1835006",
                                    "name": "Feiyue Huang"
                                },
                                {
                                    "authorId": "47096329",
                                    "name": "Yongjian Wu"
                                },
                                {
                                    "authorId": "40161651",
                                    "name": "Yonghong Tian"
                                },
                                {
                                    "authorId": "144082425",
                                    "name": "Ling Shao"
                                },
                                {
                                    "authorId": "1572139630",
                                    "name": "Rongrong Ji"
                                }
                            ],
                            "year": 2021,
                            "venue": "IEEE Transactions on Neural Networks and Learning Systems",
                            "n_citations": 47
                        },
                        "score": 0.97265625
                    },
                    {
                        "id": "(Hinton et al., 2015)",
                        "snippets": [
                            "A very simple way to improve the performance of almost any machine learning algorithm is to train many different models on the same data and then to average their predictions. Unfortunately, making predictions using a whole ensemble of models is cumbersome and may be too computationally expensive to allow deployment to a large number of users, especially if the individual models are large neural nets. Caruana and his collaborators have shown that it is possible to compress the knowledge in an ensemble into a single model which is much easier to deploy and we develop this approach further using a different compression technique. We achieve some surprising results on MNIST and we show that we can significantly improve the acoustic model of a heavily used commercial system by distilling the knowledge in an ensemble of models into a single model. We also introduce a new type of ensemble composed of one or more full models and many specialist models which learn to distinguish fine-grained classes that the full models confuse. Unlike a mixture of experts, these specialist models can be trained rapidly and in parallel."
                        ],
                        "paper": {
                            "corpus_id": 7200347,
                            "title": "Distilling the Knowledge in a Neural Network",
                            "authors": [
                                {
                                    "authorId": "1695689",
                                    "name": "Geoffrey E. Hinton"
                                },
                                {
                                    "authorId": "1689108",
                                    "name": "O. Vinyals"
                                },
                                {
                                    "authorId": "49959210",
                                    "name": "J. Dean"
                                }
                            ],
                            "year": 2015,
                            "venue": "arXiv.org",
                            "n_citations": 19742
                        },
                        "score": 0
                    },
                    {
                        "id": "(Wang et al._1, 2024)",
                        "snippets": [
                            "Large and complex teacher models often capture a wealth of features and deep information, crucial for enhancing task performance [7]. However, attempting to directly distill this rich information into smaller capacity student models often results in suboptimal performance reproduction due to the student model's capacity limitations. The student model may struggle to process the complex information present in the teacher model, leading to ineffective distillation. Moreover, simply mimicking all features of the teacher model overlooks the differences in relationships and structures between the information, particularly when there is a significant gap between the teacher and student models [12]."
                        ],
                        "paper": {
                            "corpus_id": 269761365,
                            "title": "Exploring Graph-based Knowledge: Multi-Level Feature Distillation via Channels Relational Graph",
                            "authors": [
                                {
                                    "authorId": "2301262030",
                                    "name": "Zhiwei Wang"
                                },
                                {
                                    "authorId": "2301263964",
                                    "name": "Jun Huang"
                                },
                                {
                                    "authorId": "2301265454",
                                    "name": "Longhua Ma"
                                },
                                {
                                    "authorId": "2301254673",
                                    "name": "Chengyu Wu"
                                },
                                {
                                    "authorId": "2301265433",
                                    "name": "Hongyu Ma"
                                }
                            ],
                            "year": 2024,
                            "venue": "arXiv.org",
                            "n_citations": 0
                        },
                        "score": 0.97802734375
                    },
                    {
                        "id": "(Capogrosso et al., 2023)",
                        "snippets": [
                            "This technique transfers knowledge from a large, complex model (teacher) to a smaller, simpler model (student) (Gou et al., 2020). This process is important for various reasons, such as reducing computational demands or enhancing model performance on specific tasks. Knowledge types, distillation strategies, and teacher-student architectures are vital factors in student learning during knowledge distillation. The subsequent paragraphs introduce the key categories of knowledge types and distillation strategies",
                            "Offline distillation is a two-stage strategy, where the teacher model is first trained on a set of training samples, and then the trained teacher model is used to guide the student model by extracting intermediate features or logits (Zhao et al., 2019)",
                            "In general, knowledge distillation is used to achieve a good trade-off between small model size and an acceptable accuracy (Zein et al., 2022)."
                        ],
                        "paper": {
                            "corpus_id": 262084420,
                            "title": "A Machine Learning-Oriented Survey on Tiny Machine Learning",
                            "authors": [
                                {
                                    "authorId": "2135267479",
                                    "name": "Luigi Capogrosso"
                                },
                                {
                                    "authorId": "1396330675",
                                    "name": "Federico Cunico"
                                },
                                {
                                    "authorId": "1780197",
                                    "name": "D. Cheng"
                                },
                                {
                                    "authorId": "2243336023",
                                    "name": "Franco Fummi"
                                },
                                {
                                    "authorId": "2238815087",
                                    "name": "Marco Cristani"
                                }
                            ],
                            "year": 2023,
                            "venue": "IEEE Access",
                            "n_citations": 43
                        },
                        "score": 0.97607421875
                    },
                    {
                        "id": "(Zein et al., 2022)",
                        "snippets": [
                            "Traditionally, neural network inferencing on tiny hardware devices took place in a centralized server-based manner. With more real-time applications coming into play, where security and latency are a concern, there has become a need to move inferencing to the edge. This paper describes a machine learning pipeline to carry neural networks from their initial forms to compressed forms deployable on tiny hardware devices, while maintaining acceptable accuracies of the optimized models. We will review the different software optimization techniques used to compress neural networks to their deployable forms. The prototype is a proof of concept showing that applying knowledge distillation from a highly accurate ResNet20 model to a simple CNN student model, followed by post-training quantization, achieves good multi-class accuracy on a constrained Arduino Nano 33 BLE Sense at low power consumption and with low inferencing latency."
                        ],
                        "paper": {
                            "corpus_id": 255266316,
                            "title": "Implementation and Optimization of Neural Networks for Tiny Hardware Devices",
                            "authors": [
                                {
                                    "authorId": "2198763466",
                                    "name": "Hadi Al Zein"
                                },
                                {
                                    "authorId": "2198763611",
                                    "name": "Mohamad Aoude"
                                },
                                {
                                    "authorId": "2198763636",
                                    "name": "Youssef Harkous"
                                }
                            ],
                            "year": 2022,
                            "venue": "2022 International Conference on Smart Systems and Power Management (IC2SPM)",
                            "n_citations": 5
                        },
                        "score": 0
                    },
                    {
                        "id": "(Jaiswal et al., 2021)",
                        "snippets": [
                            "Knowledge distillation (KD) was introduced by [30] as: \n\n\u2022 Train a large model that performs and generalizes very well. This is called the teacher model. \n\n\u2022 Take all the data you have and compute the predictions of the teacher model. The total dataset with these predictions is called the knowledge, and the predictions themselves are often referred to as soft targets. This is the knowledge distillation step. \n\n\u2022 Use the previously obtained knowledge to train the smaller network, called the student model.\n\nKnowledge distillation starts with training a larger model, the teacher 'T'. As it is trained on a heavier platform (GPU), it achieves high performance. Then a lightweight model known as student 'S' is deployed to learn from 'T'. Now, 'S' is supposed to give comparable performance as 'T' but with less memory and more speed."
                        ],
                        "paper": {
                            "corpus_id": 234336288,
                            "title": "Performance Analysis of Deep Neural Network based on Transfer Learning for Pet Classification",
                            "authors": [
                                {
                                    "authorId": "2139643077",
                                    "name": "Bhavesh Jaiswal"
                                },
                                {
                                    "authorId": "32136431",
                                    "name": "Nagendra Gajjar"
                                }
                            ],
                            "year": 2021,
                            "venue": "International Journal of Advanced Computer Science and Applications",
                            "n_citations": 0
                        },
                        "score": 0.984375
                    },
                    {
                        "id": "(Liu et al., 2022)",
                        "snippets": [
                            "Although more layers and more parameters generally improve the accuracy of the models, such big models generally have high computational complexity and require big memory, which exceed the capacity of small devices for inference and incurs long training time",
                            "Knowledge distillation is based on the popular machine learning Softmax function and a temperature (Hinton et al., 2015) , which is defined in Formula 1",
                            "As shown in Figure 1, during the training of knowledge distillation, two neural networks are used: teacher model and student model. The student model is trained based on the combination of two loss values. The first loss value is calculated from a soft prediction, which contains the probability of each class calculated from the teacher model. The soft prediction is calculated using Formula 1. The other loss value corresponds to a hard prediction, which is the ground truth label from the training data."
                        ],
                        "paper": {
                            "corpus_id": 250526328,
                            "title": "Large\u2010scale knowledge distillation with elastic heterogeneous computing resources",
                            "authors": [
                                {
                                    "authorId": "2118971193",
                                    "name": "Ji Liu"
                                },
                                {
                                    "authorId": "9532787",
                                    "name": "Daxiang Dong"
                                },
                                {
                                    "authorId": "2108249583",
                                    "name": "Xi Wang"
                                },
                                {
                                    "authorId": "2140532375",
                                    "name": "An Qin"
                                },
                                {
                                    "authorId": "2155445773",
                                    "name": "Xingjian Li"
                                },
                                {
                                    "authorId": "144255847",
                                    "name": "P. Valduriez"
                                },
                                {
                                    "authorId": "1721158",
                                    "name": "D. Dou"
                                },
                                {
                                    "authorId": "3046102",
                                    "name": "Dianhai Yu"
                                }
                            ],
                            "year": 2022,
                            "venue": "Concurrency and Computation",
                            "n_citations": 6
                        },
                        "score": 0.9775390625
                    },
                    {
                        "id": "(Malik et al., 2023)",
                        "snippets": [
                            "The process of knowledge distillation as the name suggests is the method of transferring knowledge from a larger computationally expensive model to a relatively smaller model. The larger and smaller models are called the teacher and student models respectively. Thus, knowledge distillation consists of three principal components: (1) knowledge; (2) distillation algorithm; and (3) teacherstudent architecture. While there are now multiple methods of distillation algorithms we selected the response-based algorithm. As shown in Figure 3, the hypothesis is that the student model will learn to mimic the predictions of the teacher model. This can be achieved by using a loss function, termed the distillation loss, that captures the difference between the logits of the student and the teacher model respectively. As this loss minimizes overtraining, the student model will improve at making the same predictions as the teacher. In the offline training scheme, the teacher model is first trained and the weights are then frozen. Next, we train the student model using the distillation loss and the logits from the teacher model as targets. Following is the equation of the distillation loss. \n\nwhere: L d : the loss function for knowledge distillation \u03b1: a hyperparameter that controls the trade-off between the classification loss and the distillation loss T : the temperature hyperparameter used to soften the logits (outputs of the last layer before softmax) of the teacher and student models KL: the Kullback-Leibler divergence, a measure of how different two probability distributions are softmax: a function that converts the logits to probabilities f (T, x): the logits of the teacher model for input x g(T, x): the logits of the student model for input x 2) Teacher Model: Generally, for the teacher model a larger and deeper network is chosen so that it performs well on the task at hand."
                        ],
                        "paper": {
                            "corpus_id": 258426697,
                            "title": "Emotions Beyond Words: Non-Speech Audio Emotion Recognition With Edge Computing",
                            "authors": [
                                {
                                    "authorId": "2105891149",
                                    "name": "Ibrahim Malik"
                                },
                                {
                                    "authorId": "24040678",
                                    "name": "S. Latif"
                                },
                                {
                                    "authorId": "1483726395",
                                    "name": "Sanaullah Manzoor"
                                },
                                {
                                    "authorId": "145474282",
                                    "name": "Muhammad Usama"
                                },
                                {
                                    "authorId": "1734917",
                                    "name": "Junaid Qadir"
                                },
                                {
                                    "authorId": "1770270",
                                    "name": "R. Jurdak"
                                }
                            ],
                            "year": 2023,
                            "venue": "arXiv.org",
                            "n_citations": 3
                        },
                        "score": 0.9716796875
                    },
                    {
                        "id": "(Li et al., 2024)",
                        "snippets": [
                            "Knowledge distillation (KD), as initially proposed by Hinton et al. (Hinton et al., 2015) , aims to supervise the training convergence of a student network, a smaller model, with a teacher network, a larger model.This method controls the extent of knowledge transfer between two networks using a temperature parameter, T, to control the transfer of soft-label dark knowledge."
                        ],
                        "paper": {
                            "corpus_id": 270389751,
                            "title": "Multistage feature fusion knowledge distillation",
                            "authors": [
                                {
                                    "authorId": "2307186661",
                                    "name": "Gang Li"
                                },
                                {
                                    "authorId": "2307436738",
                                    "name": "Kun Wang"
                                },
                                {
                                    "authorId": "2305765306",
                                    "name": "Pengfei Lv"
                                },
                                {
                                    "authorId": "2305752302",
                                    "name": "Pan He"
                                },
                                {
                                    "authorId": "2287501699",
                                    "name": "Zheng Zhou"
                                },
                                {
                                    "authorId": "2817613",
                                    "name": "Chuanyun Xu"
                                }
                            ],
                            "year": 2024,
                            "venue": "Scientific Reports",
                            "n_citations": 1
                        },
                        "score": 0.98046875
                    },
                    {
                        "id": "(Zhao et al., 2019)",
                        "snippets": [
                            "High storage and computational costs obstruct deep neural networks to be deployed on resource-constrained devices. Knowledge distillation (KD) aims to train a compact student network by transferring knowledge from a larger pretrained teacher model. However, most existing methods on KD ignore the valuable information among the training process associated with training results. In this article, we provide a new collaborative teaching KD (CTKD) strategy which employs two special teachers. Specifically, one teacher trained from scratch (i.e., scratch teacher) assists the student step by step using its temporary outputs. It forces the student to approach the optimal path toward the final logits with high accuracy. The other pretrained teacher (i.e., expert teacher) guides the student to focus on a critical region that is more useful for the task. The combination of the knowledge from two special teachers can significantly improve the performance of the student network in KD. The results of experiments on CIFAR-10, CIFAR-100, SVHN, Tiny ImageNet, and ImageNet datasets verify that the proposed KD method is efficient and achieves state-of-the-art performance."
                        ],
                        "paper": {
                            "corpus_id": 198179767,
                            "title": "Highlight Every Step: Knowledge Distillation via Collaborative Teaching",
                            "authors": [
                                {
                                    "authorId": "50981688",
                                    "name": "Haoran Zhao"
                                },
                                {
                                    "authorId": "144326521",
                                    "name": "Xin Sun"
                                },
                                {
                                    "authorId": "1964397",
                                    "name": "Junyu Dong"
                                },
                                {
                                    "authorId": "10944885",
                                    "name": "Changrui Chen"
                                },
                                {
                                    "authorId": "2087106420",
                                    "name": "Zihe Dong"
                                }
                            ],
                            "year": 2019,
                            "venue": "IEEE Transactions on Cybernetics",
                            "n_citations": 59
                        },
                        "score": 0
                    },
                    {
                        "id": "(Ganta et al., 2021)",
                        "snippets": [
                            "The main idea of using knowledge distillation is that student network (S) to be trained not only using the true labels information but also observation of how the teacher (T) works with the unseen data provided. Because the teacher model has more more generalization power, the idea is to train the student model is such way that it can mimic the behaviour of that generalization. The teacher network is complex in size being deeper and wider."
                        ],
                        "paper": {
                            "corpus_id": 245974615,
                            "title": "Knowledge Distillation via Weighted Ensemble of Teaching Assistants",
                            "authors": [
                                {
                                    "authorId": "2150123681",
                                    "name": "Durga Prasad Ganta"
                                },
                                {
                                    "authorId": "1944682834",
                                    "name": "Himel Das Gupta"
                                },
                                {
                                    "authorId": "2278313995",
                                    "name": "Victor S. Sheng"
                                }
                            ],
                            "year": 2021,
                            "venue": "International Conference on Big Knowledge",
                            "n_citations": 4
                        },
                        "score": 0.98974609375
                    },
                    {
                        "id": "(Shi et al., 2023)",
                        "snippets": [
                            "Knowledge distillation is a model compression technique proposed by Hinton et al. in 2015 [9], which could effectively train a smaller student model by learning from a larger teacher model. As shown in Figure 2, a general knowledge distillation framework consists of three major components: teacher-student network architecture, knowledge, as well as distillation algorithm. The main idea of knowledge distillation is that the student model mimics the teacher model in order to achieve a comparable or even a superior performance (Gou et al., 2020). Teacher network usually has more complicated network architecture and learns from the data first. After the training of teacher network, the useful knowledge could be distilled from the teacher network. Hereafter, student network learns the knowledge distilled from teacher network to mimic teacher's behavior together with the training data. The core of knowledge distillation is how to simplify model while maintaining relatively good performance."
                        ],
                        "paper": {
                            "corpus_id": 257102399,
                            "title": "Knowledge Distillation-based Information Sharing for Online Process Monitoring in Decentralized Manufacturing System",
                            "authors": [
                                {
                                    "authorId": "2144091000",
                                    "name": "Zhangyue Shi"
                                },
                                {
                                    "authorId": "2116210564",
                                    "name": "Yuxuan Li"
                                },
                                {
                                    "authorId": "2035538477",
                                    "name": "Chenang Liu"
                                }
                            ],
                            "year": 2023,
                            "venue": "Journal of Intelligent Manufacturing",
                            "n_citations": 8
                        },
                        "score": 0.990234375
                    },
                    {
                        "id": "(Gou et al., 2020)",
                        "snippets": [
                            "In recent years, deep neural networks have been successful in both industry and academia, especially for computer vision tasks. The great success of deep learning is mainly due to its scalability to encode large-scale data and to maneuver billions of model parameters. However, it is a challenge to deploy these cumbersome deep models on devices with limited resources, e.g., mobile phones and embedded devices, not only because of the high computational complexity but also the large storage requirements. To this end, a variety of model compression and acceleration techniques have been developed. As a representative type of model compression and acceleration, knowledge distillation effectively learns a small student model from a large teacher model. It has received rapid increasing attention from the community. This paper provides a comprehensive survey of knowledge distillation from the perspectives of knowledge categories, training schemes, teacher\u2013student architecture, distillation algorithms, performance comparison and applications. Furthermore, challenges in knowledge distillation are briefly reviewed and comments on future research are discussed and forwarded."
                        ],
                        "paper": {
                            "corpus_id": 219559263,
                            "title": "Knowledge Distillation: A Survey",
                            "authors": [
                                {
                                    "authorId": "38978232",
                                    "name": "Jianping Gou"
                                },
                                {
                                    "authorId": "2425630",
                                    "name": "B. Yu"
                                },
                                {
                                    "authorId": "144555237",
                                    "name": "S. Maybank"
                                },
                                {
                                    "authorId": "143719920",
                                    "name": "D. Tao"
                                }
                            ],
                            "year": 2020,
                            "venue": "International Journal of Computer Vision",
                            "n_citations": 2984
                        },
                        "score": 0
                    },
                    {
                        "id": "(Xu et al., 2022)",
                        "snippets": [
                            "Knowledge distillation (KD), an important method of model compression (Tan et al., 2019)(Cheng et al., 2018)(Bashir et al., 2020), is effective in transferring \"dark knowledge\" from a larger model to a smaller model, allowing the smaller model to approximate the performance level achieved by the larger model (Yim et al., 2017)(Kim et al., 2016)(Gou et al., 2020)."
                        ],
                        "paper": {
                            "corpus_id": 248683566,
                            "title": "Teacher-student collaborative knowledge distillation for image classification",
                            "authors": [
                                {
                                    "authorId": "2817613",
                                    "name": "Chuanyun Xu"
                                },
                                {
                                    "authorId": "2164051968",
                                    "name": "Wenjian Gao"
                                },
                                {
                                    "authorId": "2164318078",
                                    "name": "Tian Li"
                                },
                                {
                                    "authorId": "2164821600",
                                    "name": "Nanlan Bai"
                                },
                                {
                                    "authorId": "2155121570",
                                    "name": "Gang Li"
                                },
                                {
                                    "authorId": "2145954082",
                                    "name": "Yang Zhang"
                                }
                            ],
                            "year": 2022,
                            "venue": "Applied intelligence (Boston)",
                            "n_citations": 44
                        },
                        "score": 0.97265625
                    },
                    {
                        "id": "(Tan et al., 2019)",
                        "snippets": [
                            "Convolutional Neural Networks (ConvNets) are commonly developed at a fixed resource budget, and then scaled up for better accuracy if more resources are available. In this paper, we systematically study model scaling and identify that carefully balancing network depth, width, and resolution can lead to better performance. Based on this observation, we propose a new scaling method that uniformly scales all dimensions of depth/width/resolution using a simple yet highly effective compound coefficient. We demonstrate the effectiveness of this method on scaling up MobileNets and ResNet. \nTo go even further, we use neural architecture search to design a new baseline network and scale it up to obtain a family of models, called EfficientNets, which achieve much better accuracy and efficiency than previous ConvNets. In particular, our EfficientNet-B7 achieves state-of-the-art 84.4% top-1 / 97.1% top-5 accuracy on ImageNet, while being 8.4x smaller and 6.1x faster on inference than the best existing ConvNet. Our EfficientNets also transfer well and achieve state-of-the-art accuracy on CIFAR-100 (91.7%), Flowers (98.8%), and 3 other transfer learning datasets, with an order of magnitude fewer parameters. Source code is at this https URL."
                        ],
                        "paper": {
                            "corpus_id": 167217261,
                            "title": "EfficientNet: Rethinking Model Scaling for Convolutional Neural Networks",
                            "authors": [
                                {
                                    "authorId": "120805419",
                                    "name": "Mingxing Tan"
                                },
                                {
                                    "authorId": "2827616",
                                    "name": "Quoc V. Le"
                                }
                            ],
                            "year": 2019,
                            "venue": "International Conference on Machine Learning",
                            "n_citations": 18189
                        },
                        "score": 0
                    }
                ],
                "format": "synthesis",
                "table": null,
                "model": "claude-3-7-sonnet-20250219"
            },
            {
                "title": "Size-Matched Knowledge Distillation",
                "tldr": "Traditional knowledge distillation typically involves transferring knowledge from a larger teacher to a smaller student, but recent research has challenged this assumption by exploring size-matched distillation where the student has similar or identical architecture to the teacher. These approaches have demonstrated that even when the student model is the same size as the teacher, knowledge distillation can still lead to performance improvements. (5 sources)",
                "text": "\nWhile knowledge distillation was originally proposed as a technique to transfer knowledge from a large, powerful teacher model to a smaller, more efficient student model, recent research has challenged this conventional setup by exploring scenarios where the student and teacher are of comparable size. Size-matched knowledge distillation refers to the approach where the student model has similar or even identical architecture and capacity as the teacher model <Paper corpusId=\"221703021\" paperTitle=\"(Liu et al., 2020)\" isShortName></Paper>.\n\nThis departure from the traditional model compression objective has been motivated by findings that knowledge distillation can provide benefits beyond just making models smaller. Research by Furlanello et al. and Hahn and Choi has shown that knowledge distillation can be leveraged to train high-performing student models with the same size as the teacher <Paper corpusId=\"221703021\" paperTitle=\"(Liu et al., 2020)\" isShortName></Paper>. This suggests that the knowledge transfer process captures more than just a compression of information; it may also provide regularization effects or transfer beneficial inductive biases.\n\nOne innovative approach in this direction is the deep mutual learning (DML) strategy proposed by Zhang et al., where an ensemble of students with similar architectures learn collaboratively and teach each other throughout the training process. Unlike traditional one-way knowledge transfer from a static pre-defined teacher to a student, mutual learning enables networks to benefit from peer teaching <Paper corpusId=\"258298441\" paperTitle=\"(Liu et al., 2023)\" isShortName></Paper> <Paper corpusId=\"26071966\" paperTitle=\"(Zhang et al., 2017)\" isShortName></Paper>. Surprisingly, this collaborative learning approach has been shown to outperform distillation from more powerful yet static teachers in some cases <Paper corpusId=\"26071966\" paperTitle=\"(Zhang et al., 2017)\" isShortName></Paper>.\n\nAnother related concept is online knowledge distillation, where both the large and small models are randomly initialized and learn from each other during training, instead of following the traditional sequential approach where the teacher is fully trained first <Paper corpusId=\"258298441\" paperTitle=\"(Liu et al., 2023)\" isShortName></Paper>. This dynamic interaction between models of similar size allows for a more collaborative learning process.\n\nSome research has even demonstrated that student networks in knowledge distillation scenarios can sometimes surpass the accuracy of their teacher networks <Paper corpusId=\"267688311\" paperTitle=\"(Si et al., 2024)\" isShortName></Paper>. This counter-intuitive finding challenges the assumption that a student model's performance is inherently bounded by its teacher's capabilities.\n\nThe Distral (Distill & transfer learning) approach represents another variation that leverages the idea of knowledge sharing between equally capable models. Instead of directly sharing parameters between different workers (models), Distral proposes to share a \"distilled\" policy that captures common behavior across tasks, with each worker trained to solve its specific task while staying close to the shared policy <Paper corpusId=\"31009408\" paperTitle=\"(Teh et al., 2017)\" isShortName></Paper>. This approach has shown efficiency improvements in complex environments and demonstrates greater stability in the learning process.",
                "citations": [
                    {
                        "id": "(Liu et al., 2020)",
                        "snippets": [
                            "Knowledge distillation refers to a class of methods for training a new smaller student network by learning from a teacher network (in addition to learning from the training data). It is generally assumed that the teacher has been previously trained, and the parameters for the student are estimated by matching the student's predictions to the teacher",
                            "Recent work (Furlanello et al., 2018; Hahn and Choi, 2019) also sheds light on leveraging knowledge distillation for training a high-performing student model with the same size as the teacher"
                        ],
                        "paper": {
                            "corpus_id": 221703021,
                            "title": "Noisy Self-Knowledge Distillation for Text Summarization",
                            "authors": [
                                {
                                    "authorId": "39798499",
                                    "name": "Yang Liu"
                                },
                                {
                                    "authorId": "2072819533",
                                    "name": "S. Shen"
                                },
                                {
                                    "authorId": "1747893",
                                    "name": "Mirella Lapata"
                                }
                            ],
                            "year": 2020,
                            "venue": "North American Chapter of the Association for Computational Linguistics",
                            "n_citations": 44
                        },
                        "score": 0.98046875
                    },
                    {
                        "id": "(Liu et al., 2023)",
                        "snippets": [
                            "Hinton et al. (2015) first clarifies the concept of knowledge distillation. In their method, softened probability distribution output by the teacher is used as the guidance to the student",
                            "(Zhang et al., 2017) propose online knowledge distillation, where both the large and the small models are randomly initialized and learn mutually from each other during training."
                        ],
                        "paper": {
                            "corpus_id": 258298441,
                            "title": "Function-Consistent Feature Distillation",
                            "authors": [
                                {
                                    "authorId": "2152509045",
                                    "name": "Dongyang Liu"
                                },
                                {
                                    "authorId": "1693589",
                                    "name": "Meina Kan"
                                },
                                {
                                    "authorId": "145455919",
                                    "name": "S. Shan"
                                },
                                {
                                    "authorId": "46772547",
                                    "name": "Xilin Chen"
                                }
                            ],
                            "year": 2023,
                            "venue": "International Conference on Learning Representations",
                            "n_citations": 19
                        },
                        "score": 0.98095703125
                    },
                    {
                        "id": "(Zhang et al., 2017)",
                        "snippets": [
                            "Model distillation is an effective and widely used technique to transfer knowledge from a teacher to a student network. The typical application is to transfer from a powerful large network or ensemble to a small network, in order to meet the low-memory or fast execution requirements. In this paper, we present a deep mutual learning (DML) strategy. Different from the one-way transfer between a static pre-defined teacher and a student in model distillation, with DML, an ensemble of students learn collaboratively and teach each other throughout the training process. Our experiments show that a variety of network architectures benefit from mutual learning and achieve compelling results on both category and instance recognition tasks. Surprisingly, it is revealed that no prior powerful teacher network is necessary - mutual learning of a collection of simple student networks works, and moreover outperforms distillation from a more powerful yet static teacher."
                        ],
                        "paper": {
                            "corpus_id": 26071966,
                            "title": "Deep Mutual Learning",
                            "authors": [
                                {
                                    "authorId": "46868596",
                                    "name": "Ying Zhang"
                                },
                                {
                                    "authorId": "145406421",
                                    "name": "T. Xiang"
                                },
                                {
                                    "authorId": "1697755",
                                    "name": "Timothy M. Hospedales"
                                },
                                {
                                    "authorId": "153176123",
                                    "name": "Huchuan Lu"
                                }
                            ],
                            "year": 2017,
                            "venue": "2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition",
                            "n_citations": 1655
                        },
                        "score": 0
                    },
                    {
                        "id": "(Si et al., 2024)",
                        "snippets": [
                            "Knowledge distillation is a commonly used technique in deep learning, as proposed by Hinton et al. in 2015 [17]. It is used to transfer knowledge from a large model to a smaller one, resulting in model compression and acceleration. The goal of knowledge distillation is to improve the accuracy and generalizability of the student network by transferring knowledge from the teacher network. Auxiliary information during training is usually obtained from the output of the teacher network. Such techniques have been applied in various domains, including natural language processing, computer vision, and speech recognition. An important use case is the deployment of deep learning models on mobile devices, which often have limited computational resources and require smaller, lightweight models. Knowledge distillation typically involves a larger teacher network and a smaller student network. The teacher network is usually trained on large amounts of data and therefore has higher accuracy and more comprehensive knowledge. Compared to the teacher network, the student network is trained on less data and has fewer parameters. Nevertheless, some student networks can surpass the accuracy of the teacher network."
                        ],
                        "paper": {
                            "corpus_id": 267688311,
                            "title": "Breast Cancer Histopathology Images Classification Through Multi-View Augmented Contrastive Learning and Pre-Learning Knowledge Distillation",
                            "authors": [
                                {
                                    "authorId": "2284255154",
                                    "name": "Jialong Si"
                                },
                                {
                                    "authorId": "2284258056",
                                    "name": "Wei Jia"
                                },
                                {
                                    "authorId": "2284308867",
                                    "name": "Haifeng Jiang"
                                }
                            ],
                            "year": 2024,
                            "venue": "IEEE Access",
                            "n_citations": 5
                        },
                        "score": 0.9833984375
                    },
                    {
                        "id": "(Teh et al., 2017)",
                        "snippets": [
                            "Most deep reinforcement learning algorithms are data inefficient in complex and rich environments, limiting their applicability to many scenarios. One direction for improving data efficiency is multitask learning with shared neural network parameters, where efficiency may be improved through transfer across related tasks. In practice, however, this is not usually observed, because gradients from different tasks can interfere negatively, making learning unstable and sometimes even less data efficient. Another issue is the different reward schemes between tasks, which can easily lead to one task dominating the learning of a shared model. We propose a new approach for joint training of multiple tasks, which we refer to as Distral (Distill & transfer learning). Instead of sharing parameters between the different workers, we propose to share a \"distilled\" policy that captures common behaviour across tasks. Each worker is trained to solve its own task while constrained to stay close to the shared policy, while the shared policy is trained by distillation to be the centroid of all task policies. Both aspects of the learning process are derived by optimizing a joint objective function. We show that our approach supports efficient transfer on complex 3D environments, outperforming several related methods. Moreover, the proposed learning process is more robust and more stable---attributes that are critical in deep reinforcement learning."
                        ],
                        "paper": {
                            "corpus_id": 31009408,
                            "title": "Distral: Robust multitask reinforcement learning",
                            "authors": [
                                {
                                    "authorId": "1725303",
                                    "name": "Y. Teh"
                                },
                                {
                                    "authorId": "2603033",
                                    "name": "V. Bapst"
                                },
                                {
                                    "authorId": "144792148",
                                    "name": "Wojciech M. Czarnecki"
                                },
                                {
                                    "authorId": "34660073",
                                    "name": "John Quan"
                                },
                                {
                                    "authorId": "2066516991",
                                    "name": "J. Kirkpatrick"
                                },
                                {
                                    "authorId": "2315504",
                                    "name": "R. Hadsell"
                                },
                                {
                                    "authorId": "2801204",
                                    "name": "N. Heess"
                                },
                                {
                                    "authorId": "1996134",
                                    "name": "Razvan Pascanu"
                                }
                            ],
                            "year": 2017,
                            "venue": "Neural Information Processing Systems",
                            "n_citations": 554
                        },
                        "score": 0
                    }
                ],
                "format": "synthesis",
                "table": null,
                "model": "claude-3-7-sonnet-20250219"
            },
            {
                "title": "Smaller Teacher Models in Knowledge Distillation",
                "tldr": "Contrary to traditional approaches, research has shown that smaller or weaker teachers can sometimes be more effective for knowledge distillation than larger models. This counterintuitive finding suggests that the alignment between teacher and student capabilities, rather than just teacher performance, is crucial for successful knowledge transfer. (LLM Memory)",
                "text": "\nWhile conventional wisdom suggests that more powerful teacher models lead to better student performance in knowledge distillation, recent research has challenged this assumption by exploring scenarios where smaller or less powerful teachers can actually be more effective. This phenomenon, sometimes referred to as \"the student surpassing the teacher\" paradox, has been observed in several studies and offers new insights into the knowledge transfer process <Model name=\"Anthropic\" version=\"claude-3-7-sonnet-20250219\">.\n\nMirzadeh et al. introduced the concept of the \"teacher assistant (TA)\" knowledge distillation, where they found that directly transferring knowledge from a very large teacher to a small student can be suboptimal. They demonstrated that using an intermediate-sized model (the TA) as a bridge between the large teacher and small student often leads to better performance. This suggests that too large a gap in model capacity between teacher and student can actually hinder effective knowledge transfer <Model name=\"Anthropic\" version=\"claude-3-7-sonnet-20250219\">.\n\nThis observation aligns with the concept of \"capacity gap\" in knowledge distillation, which refers to the difference in representation capabilities between teacher and student models. When this gap is too large, the student may struggle to mimic the complex functions learned by the teacher, resulting in suboptimal transfer. A smaller, less powerful teacher whose capacity is closer to that of the student may provide more accessible and relevant knowledge <Model name=\"Anthropic\" version=\"claude-3-7-sonnet-20250219\">.\n\nSome researchers have even shown that deliberately using weaker teachers can be beneficial in certain contexts. For instance, Zhou et al. found that training a student with a less accurate but more suitable teacher can lead to better generalization performance than using a highly accurate but complex teacher. The intuition is that a simpler teacher may provide more digestible knowledge that aligns better with the student's learning capacity <Model name=\"Anthropic\" version=\"claude-3-7-sonnet-20250219\">.\n\nInterestingly, studies have also demonstrated that early-stopped teachers\u2014models that haven't been trained to convergence\u2014can sometimes be more effective for knowledge distillation. These partially trained teachers may provide softer probability distributions that contain more informative signals about class relationships than fully converged models with very peaky distributions <Model name=\"Anthropic\" version=\"claude-3-7-sonnet-20250219\">.\n\nThe effectiveness of smaller teachers has practical implications as well. Using smaller teacher models reduces the computational overhead of the knowledge distillation process, making it more accessible and environmentally friendly. This approach also challenges the resource-intensive paradigm of \"bigger is better\" that has dominated much of deep learning research <Model name=\"Anthropic\" version=\"claude-3-7-sonnet-20250219\">.\n\nOverall, these findings suggest that the effectiveness of knowledge distillation depends not just on how powerful the teacher is, but on how well the teacher's knowledge can be assimilated by the student. The optimal teacher may not necessarily be the largest or most accurate model, but rather one that provides knowledge at an appropriate level of abstraction and complexity for the student to effectively learn from <Model name=\"Anthropic\" version=\"claude-3-7-sonnet-20250219\">.",
                "citations": [],
                "format": "synthesis",
                "table": null,
                "model": "claude-3-7-sonnet-20250219"
            },
            {
                "title": "Effectiveness of Non-Traditional Knowledge Distillation Approaches",
                "tldr": "Non-traditional knowledge distillation approaches such as mutual learning, online distillation, and size-matched distillation have shown surprisingly effective results that challenge conventional wisdom. These innovative methods demonstrate that collaborative learning between peer networks, rather than just one-way transfer from a static teacher, can lead to performance improvements and even enable student models to surpass their teachers. (5 sources)",
                "text": "\nBuilding upon the insights from traditional and size-matched knowledge distillation approaches, researchers have developed several non-traditional methods that have proven remarkably effective. One such innovative approach is deep mutual learning (DML), introduced by Zhang et al., which fundamentally changes the knowledge transfer dynamics. Unlike the conventional one-way transfer from a pre-defined teacher to a student, DML enables an ensemble of student networks to learn collaboratively and teach each other throughout the training process. Surprisingly, this collaborative approach has not only shown to be effective but in some cases has outperformed distillation from more powerful yet static teachers <Paper corpusId=\"26071966\" paperTitle=\"(Zhang et al., 2017)\" isShortName></Paper>.\n\nThe concept of online knowledge distillation represents another departure from traditional methods. In this approach, both large and small models are randomly initialized and learn mutually from each other during training, rather than following the sequential teacher-then-student training process <Paper corpusId=\"258298441\" paperTitle=\"(Liu et al., 2023)\" isShortName></Paper> <Paper corpusId=\"26071966\" paperTitle=\"(Zhang et al., 2017)\" isShortName></Paper>. This dynamic interaction creates a more collaborative learning environment that can lead to improved performance for all participating models.\n\nPerhaps most surprisingly, several studies have shown that student networks in knowledge distillation scenarios can sometimes surpass the accuracy of their teacher networks <Paper corpusId=\"267688311\" paperTitle=\"(Si et al., 2024)\" isShortName></Paper>. This counterintuitive finding challenges the fundamental assumption that a student's performance is inherently bounded by its teacher's capabilities and suggests that the knowledge transfer process may provide additional benefits beyond simply mimicking the teacher's behavior.\n\nThe Distral (Distill & transfer learning) approach represents yet another effective non-traditional method. Rather than directly sharing parameters between different models, Distral proposes to share a \"distilled\" policy that captures common behavior across tasks. Each model is then trained to solve its specific task while staying close to this shared policy <Paper corpusId=\"221703021\" paperTitle=\"(Liu et al., 2020)\" isShortName></Paper> <Paper corpusId=\"31009408\" paperTitle=\"(Teh et al., 2017)\" isShortName></Paper>. This approach has demonstrated improved data efficiency and greater stability in the learning process.\n\nThese non-traditional approaches collectively suggest that knowledge distillation is more versatile and powerful than initially conceived. Rather than being limited to a one-way compression technique from large to small models, these methods demonstrate that various forms of knowledge sharing between models\u2014regardless of their relative sizes\u2014can lead to performance improvements. This broader understanding opens up new possibilities for model training and knowledge transfer that extend well beyond the original model compression objective.",
                "citations": [
                    {
                        "id": "(Zhang et al., 2017)",
                        "snippets": [
                            "Model distillation is an effective and widely used technique to transfer knowledge from a teacher to a student network. The typical application is to transfer from a powerful large network or ensemble to a small network, in order to meet the low-memory or fast execution requirements. In this paper, we present a deep mutual learning (DML) strategy. Different from the one-way transfer between a static pre-defined teacher and a student in model distillation, with DML, an ensemble of students learn collaboratively and teach each other throughout the training process. Our experiments show that a variety of network architectures benefit from mutual learning and achieve compelling results on both category and instance recognition tasks. Surprisingly, it is revealed that no prior powerful teacher network is necessary - mutual learning of a collection of simple student networks works, and moreover outperforms distillation from a more powerful yet static teacher."
                        ],
                        "paper": {
                            "corpus_id": 26071966,
                            "title": "Deep Mutual Learning",
                            "authors": [
                                {
                                    "authorId": "46868596",
                                    "name": "Ying Zhang"
                                },
                                {
                                    "authorId": "145406421",
                                    "name": "T. Xiang"
                                },
                                {
                                    "authorId": "1697755",
                                    "name": "Timothy M. Hospedales"
                                },
                                {
                                    "authorId": "153176123",
                                    "name": "Huchuan Lu"
                                }
                            ],
                            "year": 2017,
                            "venue": "2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition",
                            "n_citations": 1655
                        },
                        "score": 0
                    },
                    {
                        "id": "(Liu et al., 2023)",
                        "snippets": [
                            "Hinton et al. (2015) first clarifies the concept of knowledge distillation. In their method, softened probability distribution output by the teacher is used as the guidance to the student",
                            "(Zhang et al., 2017) propose online knowledge distillation, where both the large and the small models are randomly initialized and learn mutually from each other during training."
                        ],
                        "paper": {
                            "corpus_id": 258298441,
                            "title": "Function-Consistent Feature Distillation",
                            "authors": [
                                {
                                    "authorId": "2152509045",
                                    "name": "Dongyang Liu"
                                },
                                {
                                    "authorId": "1693589",
                                    "name": "Meina Kan"
                                },
                                {
                                    "authorId": "145455919",
                                    "name": "S. Shan"
                                },
                                {
                                    "authorId": "46772547",
                                    "name": "Xilin Chen"
                                }
                            ],
                            "year": 2023,
                            "venue": "International Conference on Learning Representations",
                            "n_citations": 19
                        },
                        "score": 0.98095703125
                    },
                    {
                        "id": "(Si et al., 2024)",
                        "snippets": [
                            "Knowledge distillation is a commonly used technique in deep learning, as proposed by Hinton et al. in 2015 [17]. It is used to transfer knowledge from a large model to a smaller one, resulting in model compression and acceleration. The goal of knowledge distillation is to improve the accuracy and generalizability of the student network by transferring knowledge from the teacher network. Auxiliary information during training is usually obtained from the output of the teacher network. Such techniques have been applied in various domains, including natural language processing, computer vision, and speech recognition. An important use case is the deployment of deep learning models on mobile devices, which often have limited computational resources and require smaller, lightweight models. Knowledge distillation typically involves a larger teacher network and a smaller student network. The teacher network is usually trained on large amounts of data and therefore has higher accuracy and more comprehensive knowledge. Compared to the teacher network, the student network is trained on less data and has fewer parameters. Nevertheless, some student networks can surpass the accuracy of the teacher network."
                        ],
                        "paper": {
                            "corpus_id": 267688311,
                            "title": "Breast Cancer Histopathology Images Classification Through Multi-View Augmented Contrastive Learning and Pre-Learning Knowledge Distillation",
                            "authors": [
                                {
                                    "authorId": "2284255154",
                                    "name": "Jialong Si"
                                },
                                {
                                    "authorId": "2284258056",
                                    "name": "Wei Jia"
                                },
                                {
                                    "authorId": "2284308867",
                                    "name": "Haifeng Jiang"
                                }
                            ],
                            "year": 2024,
                            "venue": "IEEE Access",
                            "n_citations": 5
                        },
                        "score": 0.9833984375
                    },
                    {
                        "id": "(Liu et al., 2020)",
                        "snippets": [
                            "Knowledge distillation refers to a class of methods for training a new smaller student network by learning from a teacher network (in addition to learning from the training data). It is generally assumed that the teacher has been previously trained, and the parameters for the student are estimated by matching the student's predictions to the teacher",
                            "Recent work (Furlanello et al., 2018; Hahn and Choi, 2019) also sheds light on leveraging knowledge distillation for training a high-performing student model with the same size as the teacher"
                        ],
                        "paper": {
                            "corpus_id": 221703021,
                            "title": "Noisy Self-Knowledge Distillation for Text Summarization",
                            "authors": [
                                {
                                    "authorId": "39798499",
                                    "name": "Yang Liu"
                                },
                                {
                                    "authorId": "2072819533",
                                    "name": "S. Shen"
                                },
                                {
                                    "authorId": "1747893",
                                    "name": "Mirella Lapata"
                                }
                            ],
                            "year": 2020,
                            "venue": "North American Chapter of the Association for Computational Linguistics",
                            "n_citations": 44
                        },
                        "score": 0.98046875
                    },
                    {
                        "id": "(Teh et al., 2017)",
                        "snippets": [
                            "Most deep reinforcement learning algorithms are data inefficient in complex and rich environments, limiting their applicability to many scenarios. One direction for improving data efficiency is multitask learning with shared neural network parameters, where efficiency may be improved through transfer across related tasks. In practice, however, this is not usually observed, because gradients from different tasks can interfere negatively, making learning unstable and sometimes even less data efficient. Another issue is the different reward schemes between tasks, which can easily lead to one task dominating the learning of a shared model. We propose a new approach for joint training of multiple tasks, which we refer to as Distral (Distill & transfer learning). Instead of sharing parameters between the different workers, we propose to share a \"distilled\" policy that captures common behaviour across tasks. Each worker is trained to solve its own task while constrained to stay close to the shared policy, while the shared policy is trained by distillation to be the centroid of all task policies. Both aspects of the learning process are derived by optimizing a joint objective function. We show that our approach supports efficient transfer on complex 3D environments, outperforming several related methods. Moreover, the proposed learning process is more robust and more stable---attributes that are critical in deep reinforcement learning."
                        ],
                        "paper": {
                            "corpus_id": 31009408,
                            "title": "Distral: Robust multitask reinforcement learning",
                            "authors": [
                                {
                                    "authorId": "1725303",
                                    "name": "Y. Teh"
                                },
                                {
                                    "authorId": "2603033",
                                    "name": "V. Bapst"
                                },
                                {
                                    "authorId": "144792148",
                                    "name": "Wojciech M. Czarnecki"
                                },
                                {
                                    "authorId": "34660073",
                                    "name": "John Quan"
                                },
                                {
                                    "authorId": "2066516991",
                                    "name": "J. Kirkpatrick"
                                },
                                {
                                    "authorId": "2315504",
                                    "name": "R. Hadsell"
                                },
                                {
                                    "authorId": "2801204",
                                    "name": "N. Heess"
                                },
                                {
                                    "authorId": "1996134",
                                    "name": "Razvan Pascanu"
                                }
                            ],
                            "year": 2017,
                            "venue": "Neural Information Processing Systems",
                            "n_citations": 554
                        },
                        "score": 0
                    }
                ],
                "format": "synthesis",
                "table": null,
                "model": "claude-3-7-sonnet-20250219"
            }
        ],
        "cost": 0.159144
    }
}