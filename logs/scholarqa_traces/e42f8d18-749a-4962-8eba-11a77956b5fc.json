{
    "query": "are the differences encoder decoder causal non causal decoder only LMs",
    "user_id": "lib_user",
    "task_id": "e42f8d18-749a-4962-8eba-11a77956b5fc",
    "timestamp": "2025-06-24T00:59:11.275501",
    "n_retrieval": 256,
    "n_retrieved": 270,
    "n_candidates": 37,
    "n_rerank": 50,
    "opt_in": true,
    "total_cost": 0.6176639999999999,
    "decomposed_query": {
        "rewritten_query": "Differences between encoder-decoder, causal, non-causal, and decoder-only language models.",
        "keyword_query": "differences encoder decoder causal non causal decoder language models",
        "search_filters": {
            "fieldsOfStudy": "Computer Science"
        },
        "cost": 0.009678,
        "model": "claude-3-7-sonnet-20250219"
    },
    "candidates": [
        {
            "title": "Token-Scaled Logit Distillation for Ternary Weight Generative Language Models",
            "venue": "Neural Information Processing Systems",
            "year": 2023,
            "reference_count": 50,
            "citation_count": 15,
            "influential_citation_count": 1,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://arxiv.org/pdf/2308.06744",
                "status": "CLOSED",
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2308.06744, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2141320070",
                    "name": "Minsoo Kim"
                },
                {
                    "authorId": "2144376191",
                    "name": "Sihwa Lee"
                },
                {
                    "authorId": "2265920992",
                    "name": "Janghwan Lee"
                },
                {
                    "authorId": "2158125346",
                    "name": "S. Hong"
                },
                {
                    "authorId": "2180828053",
                    "name": "Duhyeuk Chang"
                },
                {
                    "authorId": "66936521",
                    "name": "Wonyong Sung"
                },
                {
                    "authorId": "2506452",
                    "name": "Jungwook Choi"
                }
            ],
            "abstract": "Generative Language Models (GLMs) have shown impressive performance in tasks such as text generation, understanding, and reasoning. However, the large model size poses challenges for practical deployment. To solve this problem, Quantization-Aware Training (QAT) has become increasingly popular. However, current QAT methods for generative models have resulted in a noticeable loss of accuracy. To counteract this issue, we propose a novel knowledge distillation method specifically designed for GLMs. Our method, called token-scaled logit distillation, prevents overfitting and provides superior learning from the teacher model and ground truth. This research marks the first evaluation of ternary weight quantization-aware training of large-scale GLMs with less than 1.0 degradation in perplexity and no loss of accuracy in a reasoning task.",
            "corpus_id": 260886785,
            "sentences": [
                {
                    "corpus_id": "260886785",
                    "title": "Token-Scaled Logit Distillation for Ternary Weight Generative Language Models",
                    "text": "In fine-tuning processes, encoder-only models, often used in Natural Language Understanding (NLU), and decoder-only models, common in Natural Language Generation (NLG) with causal language modeling approach, exhibit different mechanisms for receiving ground truth loss. Fig. 1(b) illustrates the differences. In NLU tasks, encoder models leverage the representation of a unique special token, transformed into a logit vector, for calculating cross-entropy loss, with the number of classes usually being a few [12]. Conversely, decoder models in NLG tasks predict the subsequent token for each input token, transforming each token representation into a logit vector with a class size equivalent to the vocabulary size, often exceeding 50k [25]. Hence, while encoder models gain ground truth loss from a single special token's logit, decoder models derive it from the output logits of all input tokens, which could provide more detailed token-level prediction information. Motivation. The inherent nature of masked self-attention, where each token representation builds upon the representation of the previous tokens, presents previously unseen challenges when applying quantization to decoder models. For a clearer understanding of the decoder model, we conducted a comparative analysis with the encoder model to examine the impact of quantization error on the model. In Fig. 2 (a), the quantization error of the encoder self-attention map exhibits a widespread presence of errors due to the absence of masking in self-attention, and the per-token quantization errors along the layers also show irregular patterns depending on the token index. However, in Fig. 2 (b), the heat map of the decoder model reveals an increasing brightness of quantization errors as we move toward the later tokens. When examining the token index, the phenomenon of quantization errors accumulating toward the later tokens becomes even more pronounced. This previously unconsidered phenomenon of token quantization error accumulation in the decoder model is a crucial feature to consider in GLM QAT. Reflecting on this feature, we analyze the effectiveness of prior KD methods for language modeling and explore suitable KD approaches for the decoder model. \n\nComparison of KD Methods for Decoder QAT.",
                    "score": 0.5074862452350472,
                    "section_title": "Quantization Challenges on GLMs",
                    "char_start_offset": 11276,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 269
                        },
                        {
                            "start": 270,
                            "end": 308
                        },
                        {
                            "start": 309,
                            "end": 514
                        },
                        {
                            "start": 515,
                            "end": 743
                        },
                        {
                            "start": 744,
                            "end": 970
                        },
                        {
                            "start": 971,
                            "end": 982
                        },
                        {
                            "start": 983,
                            "end": 1199
                        },
                        {
                            "start": 1200,
                            "end": 1366
                        },
                        {
                            "start": 1367,
                            "end": 1642
                        },
                        {
                            "start": 1643,
                            "end": 1792
                        },
                        {
                            "start": 1793,
                            "end": 1929
                        },
                        {
                            "start": 1930,
                            "end": 2076
                        },
                        {
                            "start": 2077,
                            "end": 2233
                        },
                        {
                            "start": 2236,
                            "end": 2277
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 509,
                            "end": 513,
                            "matchedPaperCorpusId": "52967399"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.93896484375
                },
                {
                    "corpus_id": "260886785",
                    "title": "Token-Scaled Logit Distillation for Ternary Weight Generative Language Models",
                    "text": "In this section, we draw a comparison between the computations of Transformer encoders and decoders to deepen our understanding of the fresh challenges that surface within the realm of GLMs. \n\nCumulative Quantization Errors in Causal Attention. Causal attention, which integrates masking into self-attention to avoid referencing future tokens, is vital for text generation tasks. To comprehend the quantization characteristics of GLMs, we contrast the computation procedure of the self-attention map. For a Transformer encoder, the quantization error reflected on the self-attention map due to Q, K, and V computation is uniformly spread because of the simultaneous computing nature inherent in Transformer encoders. However, the mask in the causal attention accumulates quantization errors from each token, creating uneven errors in the output computation. \n\nTo elucidate, consider the encoder self-attention depicted at the top of Fig. 1(a). During the computation of the weighted sum of value representations for tokens 1 and 3, all attention probabilities in the self-attention map impacted by quantization errors are incorporated. On the other hand, in the decoder's causal attention, as illustrated at the bottom of Fig. 1(a), the output value representation is computed using only the self-attention probabilities of the current token and those before it. For example, while generating the output value representation for token 1 (highlighted in a blue box), only two attention probabilities affected by quantization error are used. In contrast, for token 3 (indicated by the red box), probabilities from all preceding tokens are incorporated. This observation underscores that the inherent design of causal attention results in an accumulation of quantization errors, especially towards the latter tokens. Given this characteristic, it becomes imperative to devise a decoder QAT approach that mitigates this uneven distribution of quantization errors within the causal attention mechanism. \n\nNecessity of Ground Truth Loss. In fine-tuning processes, encoder-only models, often used in Natural Language Understanding (NLU), and decoder-only models, common in Natural Language Generation (NLG) with causal language modeling approach, exhibit different mechanisms for receiving ground truth loss.",
                    "score": 0.4684601268620513,
                    "section_title": "Quantization Challenges on GLMs",
                    "char_start_offset": 9244,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 190
                        },
                        {
                            "start": 193,
                            "end": 244
                        },
                        {
                            "start": 245,
                            "end": 379
                        },
                        {
                            "start": 380,
                            "end": 500
                        },
                        {
                            "start": 501,
                            "end": 716
                        },
                        {
                            "start": 717,
                            "end": 857
                        },
                        {
                            "start": 860,
                            "end": 943
                        },
                        {
                            "start": 944,
                            "end": 1135
                        },
                        {
                            "start": 1136,
                            "end": 1362
                        },
                        {
                            "start": 1363,
                            "end": 1539
                        },
                        {
                            "start": 1540,
                            "end": 1650
                        },
                        {
                            "start": 1651,
                            "end": 1813
                        },
                        {
                            "start": 1814,
                            "end": 1997
                        },
                        {
                            "start": 2000,
                            "end": 2031
                        },
                        {
                            "start": 2032,
                            "end": 2301
                        }
                    ],
                    "ref_mentions": [],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.65283203125
                }
            ],
            "relevance_judgement": 0.93896484375,
            "relevance_judgment_input_expanded": "# Title: Token-Scaled Logit Distillation for Ternary Weight Generative Language Models\n# Venue: Neural Information Processing Systems\n# Authors: Minsoo Kim, Sihwa Lee, Janghwan Lee, S. Hong, Duhyeuk Chang, Wonyong Sung, Jungwook Choi\n## Abstract\nGenerative Language Models (GLMs) have shown impressive performance in tasks such as text generation, understanding, and reasoning. However, the large model size poses challenges for practical deployment. To solve this problem, Quantization-Aware Training (QAT) has become increasingly popular. However, current QAT methods for generative models have resulted in a noticeable loss of accuracy. To counteract this issue, we propose a novel knowledge distillation method specifically designed for GLMs. Our method, called token-scaled logit distillation, prevents overfitting and provides superior learning from the teacher model and ground truth. This research marks the first evaluation of ternary weight quantization-aware training of large-scale GLMs with less than 1.0 degradation in perplexity and no loss of accuracy in a reasoning task.\n## Quantization Challenges on GLMs\nIn this section, we draw a comparison between the computations of Transformer encoders and decoders to deepen our understanding of the fresh challenges that surface within the realm of GLMs. \n\nCumulative Quantization Errors in Causal Attention. Causal attention, which integrates masking into self-attention to avoid referencing future tokens, is vital for text generation tasks. To comprehend the quantization characteristics of GLMs, we contrast the computation procedure of the self-attention map. For a Transformer encoder, the quantization error reflected on the self-attention map due to Q, K, and V computation is uniformly spread because of the simultaneous computing nature inherent in Transformer encoders. However, the mask in the causal attention accumulates quantization errors from each token, creating uneven errors in the output computation. \n\nTo elucidate, consider the encoder self-attention depicted at the top of Fig. 1(a). During the computation of the weighted sum of value representations for tokens 1 and 3, all attention probabilities in the self-attention map impacted by quantization errors are incorporated. On the other hand, in the decoder's causal attention, as illustrated at the bottom of Fig. 1(a), the output value representation is computed using only the self-attention probabilities of the current token and those before it. For example, while generating the output value representation for token 1 (highlighted in a blue box), only two attention probabilities affected by quantization error are used. In contrast, for token 3 (indicated by the red box), probabilities from all preceding tokens are incorporated. This observation underscores that the inherent design of causal attention results in an accumulation of quantization errors, especially towards the latter tokens. Given this characteristic, it becomes imperative to devise a decoder QAT approach that mitigates this uneven distribution of quantization errors within the causal attention mechanism. \n\nNecessity of Ground Truth Loss. In fine-tuning processes, encoder-only models, often used in Natural Language Understanding (NLU), and decoder-only models, common in Natural Language Generation (NLG) with causal language modeling approach, exhibit different mechanisms for receiving ground truth loss.\n...\nIn fine-tuning processes, encoder-only models, often used in Natural Language Understanding (NLU), and decoder-only models, common in Natural Language Generation (NLG) with causal language modeling approach, exhibit different mechanisms for receiving ground truth loss. Fig. 1(b) illustrates the differences. In NLU tasks, encoder models leverage the representation of a unique special token, transformed into a logit vector, for calculating cross-entropy loss, with the number of classes usually being a few [12]. Conversely, decoder models in NLG tasks predict the subsequent token for each input token, transforming each token representation into a logit vector with a class size equivalent to the vocabulary size, often exceeding 50k [25]. Hence, while encoder models gain ground truth loss from a single special token's logit, decoder models derive it from the output logits of all input tokens, which could provide more detailed token-level prediction information. Motivation. The inherent nature of masked self-attention, where each token representation builds upon the representation of the previous tokens, presents previously unseen challenges when applying quantization to decoder models. For a clearer understanding of the decoder model, we conducted a comparative analysis with the encoder model to examine the impact of quantization error on the model. In Fig. 2 (a), the quantization error of the encoder self-attention map exhibits a widespread presence of errors due to the absence of masking in self-attention, and the per-token quantization errors along the layers also show irregular patterns depending on the token index. However, in Fig. 2 (b), the heat map of the decoder model reveals an increasing brightness of quantization errors as we move toward the later tokens. When examining the token index, the phenomenon of quantization errors accumulating toward the later tokens becomes even more pronounced. This previously unconsidered phenomenon of token quantization error accumulation in the decoder model is a crucial feature to consider in GLM QAT. Reflecting on this feature, we analyze the effectiveness of prior KD methods for language modeling and explore suitable KD approaches for the decoder model. \n\nComparison of KD Methods for Decoder QAT.",
            "reference_string": "[260886785 | Kim et al. | 2023 | Citations: 15]"
        },
        {
            "title": "ENTP: Encoder-only Next Token Prediction",
            "venue": "arXiv.org",
            "year": 2024,
            "reference_count": 26,
            "citation_count": 4,
            "influential_citation_count": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2410.01600, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2323781863",
                    "name": "Ethan Ewer"
                },
                {
                    "authorId": "2253659910",
                    "name": "Daewon Chae"
                },
                {
                    "authorId": "2323820473",
                    "name": "Thomas Zeng"
                },
                {
                    "authorId": "2323851531",
                    "name": "Jinkyu Kim"
                },
                {
                    "authorId": "2323790154",
                    "name": "Kangwook Lee"
                }
            ],
            "abstract": "Next-token prediction is conventionally done using decoder-only Transformers with causal attention, as this approach allows for efficient reuse of keys and values. What if we were not compute-limited, should we still use decoder-only Transformers? In this work, we introduce Encoder-only Next Token Prediction (ENTP). We explore the differences between ENTP and decoder-only Transformers in expressive power and complexity, highlighting potential advantages of ENTP in settings with unbounded compute. We introduce the $\\operatorname{Count3}$ task and show, both theoretically and experimentally, that while ENTP can perform this task easily, a decoder-only Transformer cannot. Finally, we empirically demonstrate the superior performance of ENTP across representative tasks where next-token prediction based Transformers can be evaluated, including addition, in-context learning, and language modeling.",
            "corpus_id": 273025546,
            "sentences": [
                {
                    "corpus_id": "273025546",
                    "title": "ENTP: Encoder-only Next Token Prediction",
                    "text": "In contrast, the causal decoder-only model (Brown et al., 2020;Chowdhery et al., 2023) uses only the Transformer decoder and applies causal attention to all tokens to perform nexttoken prediction, ensuring that each token attends only to previous tokens. The prefix decoder-only model (Raffel et al., 2020;Wu et al., 2021) is similar to the causal decoder-only model but differs in that it applies non-causal attention (i.e., full self-attention) to the input sequence (see Figure 8 for visualizations of the attention patterns in these variants). \n\nWith the development of these models, recent studies have investigated the performance of each variant across various tasks. Notably, Wang et al. (2022) examined the zero-shot generalization performance of each model along with various objectives, and Ding et al. (2024) analyzed the performance of causal decoder-only and prefix decoder-only models in in-context learning. However, despite these diverse studies, there is a lack of research on encoder-only models that do not impose the constraint of causal attention for every next token prediction. Therefore, in this work, we analyze the characteristics of encoder-only next-token prediction (ENTP), comparing them with decoder-only models.",
                    "score": 0.6511514326530388,
                    "section_title": "RELATED WORK",
                    "char_start_offset": 5409,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 254
                        },
                        {
                            "start": 255,
                            "end": 547
                        },
                        {
                            "start": 550,
                            "end": 674
                        },
                        {
                            "start": 675,
                            "end": 923
                        },
                        {
                            "start": 924,
                            "end": 1101
                        },
                        {
                            "start": 1102,
                            "end": 1244
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 43,
                            "end": 63,
                            "matchedPaperCorpusId": "218971783"
                        },
                        {
                            "start": 63,
                            "end": 86,
                            "matchedPaperCorpusId": "247951931"
                        },
                        {
                            "start": 285,
                            "end": 306,
                            "matchedPaperCorpusId": "204838007"
                        },
                        {
                            "start": 684,
                            "end": 702,
                            "matchedPaperCorpusId": "248118752"
                        },
                        {
                            "start": 802,
                            "end": 820,
                            "matchedPaperCorpusId": "260887420"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.904296875
                },
                {
                    "corpus_id": "273025546",
                    "title": "ENTP: Encoder-only Next Token Prediction",
                    "text": "Traditionally, auto-regressive language modeling has relied on decoder-only Transformers (Vaswani et al., 2017) with causal attention, trained using the next-token prediction objective. Causal attention ensures that each token can only attend to previous tokens, preventing future tokens from influencing past outputs. This mechanism makes training and inference more efficient, as past keys and values do not need to be recomputed for each token. This efficiency enables the scaling of decoder-only Transformers, such as GPT-4 (Achiam et al., 2023) and Llama-3 (Dubey et al., 2024), up to billions of parameters using current hardware. \n\nHowever, causal attention also introduces artificial constraints. Given tokens x 1 , x 2 , ..., x n , the contextual embedding of x j (where j < n) can only attend to embeddings of earlier tokens, even when predicting x n+1 . While this constraint ensures a strict causal structure, it may not always be necessary or beneficial. In principle, there is no inherent reason that language models should be limited by this restriction. Encoder-only Transformers, which are typically used for tasks like classification, do not impose this causality constraint. Though traditionally not used for auto-regressive tasks, encoder-only architectures can be adapted for next-token prediction. When computing the output at the current time step, an encoder-only Transformer, or any sequence model, can be made causal by only providing inputs Decoders use a causal attention, ensuring that each token attends only to the preceding tokens. In contrast, encoders allow all tokens to attend to each other by performing attention computation from scratch for each token prediction. \n\nup to and including the current time step. Therefore, in this work, we investigate the idea of using encoder-only Transformers for next-token prediction. We summarize our findings below. \n\nFunctions expressible with Decoder-only and Encoder-only Transformers. We demonstrate that the sets of functions expressible by decoder-only and encoder-only Transformers are not comparable, which goes against intuition that the expressivity of encoders would subsume that of decoders.",
                    "score": 0.5294747968085247,
                    "section_title": "INTRODUCTION",
                    "char_start_offset": 15,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 185
                        },
                        {
                            "start": 186,
                            "end": 318
                        },
                        {
                            "start": 319,
                            "end": 447
                        },
                        {
                            "start": 448,
                            "end": 636
                        },
                        {
                            "start": 639,
                            "end": 704
                        },
                        {
                            "start": 705,
                            "end": 864
                        },
                        {
                            "start": 865,
                            "end": 967
                        },
                        {
                            "start": 968,
                            "end": 1069
                        },
                        {
                            "start": 1070,
                            "end": 1193
                        },
                        {
                            "start": 1194,
                            "end": 1319
                        },
                        {
                            "start": 1320,
                            "end": 1563
                        },
                        {
                            "start": 1564,
                            "end": 1702
                        },
                        {
                            "start": 1705,
                            "end": 1747
                        },
                        {
                            "start": 1748,
                            "end": 1858
                        },
                        {
                            "start": 1859,
                            "end": 1891
                        },
                        {
                            "start": 1894,
                            "end": 1964
                        },
                        {
                            "start": 1965,
                            "end": 2179
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 89,
                            "end": 111,
                            "matchedPaperCorpusId": "13756489"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.9013671875
                },
                {
                    "corpus_id": "273025546",
                    "title": "ENTP: Encoder-only Next Token Prediction",
                    "text": "For any L \u2265 2 and D \u2265 1, there exists a position-free decoder D that has L-layers and embedding dimension D, such that for any encoder E, there exists some input sequence \n\nTheorem 2. For any L \u2265 2 and D \u2265 1, there exists a position-free encoder E that has L-layers and embedding dimension D, such that for any decoder D with positional embeddings satisfying p 1 \u0338 = p 2 , there exists some input sequence (x 1 , x 2 , . . . \n\nThese theorems are existential in nature. Informally, Theorem 1 says that if we consider causal model defined over the entirety of R D as its vocabulary, we can find some decoder, for which any encoder will differ from it on some input sequence. Theorem 2 makes a similar (albeit weaker statement) in the other direction; namely the existence of a causal function computable by an encoder, but not by any decoder that uses \"non-trivial\" positional embeddings (e.g. embeddings for different positions are unique). Detailed proof of both theorems are deferred to Appendix A. \n\nOf course, the setting and assumptions of the above two statements are not necessarily very realistic. For one, they focus on general class of causal models rather than only auto-regressive ones. Furthermore, the results only pertain to exact realization and say nothing about approximation. The assumption of unbounded domain is also not realistic as in practice decoders are trained and used over a finite domain of tokens, each with some fixed embeddings. And specific to Theorem 2, no claim is made about decoders that do not use positional embeddings. But despite the limitations, these theorems give an indication that the expressive power of encoder and decoder model are different -despite the almost identical description modulo the attention mask. Changing the mask on the attention scores causes significant changes to the properties of the model. Thus, in the following sections we propose an auto-regressive tasks and run experiments comparing encoders and decoders that corroborates this view.",
                    "score": 0.5620288621685603,
                    "section_title": "EXPRESSIVE POWER OF ENCODER-ONLY VS. DECODER-ONLY TRANSFORMERS",
                    "char_start_offset": 11805,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 170
                        },
                        {
                            "start": 173,
                            "end": 424
                        },
                        {
                            "start": 427,
                            "end": 468
                        },
                        {
                            "start": 469,
                            "end": 672
                        },
                        {
                            "start": 673,
                            "end": 891
                        },
                        {
                            "start": 892,
                            "end": 939
                        },
                        {
                            "start": 940,
                            "end": 999
                        },
                        {
                            "start": 1002,
                            "end": 1104
                        },
                        {
                            "start": 1105,
                            "end": 1197
                        },
                        {
                            "start": 1198,
                            "end": 1293
                        },
                        {
                            "start": 1294,
                            "end": 1460
                        },
                        {
                            "start": 1461,
                            "end": 1558
                        },
                        {
                            "start": 1559,
                            "end": 1759
                        },
                        {
                            "start": 1760,
                            "end": 1860
                        },
                        {
                            "start": 1861,
                            "end": 2009
                        }
                    ],
                    "ref_mentions": [],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.72265625
                }
            ],
            "relevance_judgement": 0.904296875,
            "relevance_judgment_input_expanded": "# Title: ENTP: Encoder-only Next Token Prediction\n# Venue: arXiv.org\n# Authors: Ethan Ewer, Daewon Chae, Thomas Zeng, Jinkyu Kim, Kangwook Lee\n## Abstract\nNext-token prediction is conventionally done using decoder-only Transformers with causal attention, as this approach allows for efficient reuse of keys and values. What if we were not compute-limited, should we still use decoder-only Transformers? In this work, we introduce Encoder-only Next Token Prediction (ENTP). We explore the differences between ENTP and decoder-only Transformers in expressive power and complexity, highlighting potential advantages of ENTP in settings with unbounded compute. We introduce the $\\operatorname{Count3}$ task and show, both theoretically and experimentally, that while ENTP can perform this task easily, a decoder-only Transformer cannot. Finally, we empirically demonstrate the superior performance of ENTP across representative tasks where next-token prediction based Transformers can be evaluated, including addition, in-context learning, and language modeling.\n## INTRODUCTION\nTraditionally, auto-regressive language modeling has relied on decoder-only Transformers (Vaswani et al., 2017) with causal attention, trained using the next-token prediction objective. Causal attention ensures that each token can only attend to previous tokens, preventing future tokens from influencing past outputs. This mechanism makes training and inference more efficient, as past keys and values do not need to be recomputed for each token. This efficiency enables the scaling of decoder-only Transformers, such as GPT-4 (Achiam et al., 2023) and Llama-3 (Dubey et al., 2024), up to billions of parameters using current hardware. \n\nHowever, causal attention also introduces artificial constraints. Given tokens x 1 , x 2 , ..., x n , the contextual embedding of x j (where j < n) can only attend to embeddings of earlier tokens, even when predicting x n+1 . While this constraint ensures a strict causal structure, it may not always be necessary or beneficial. In principle, there is no inherent reason that language models should be limited by this restriction. Encoder-only Transformers, which are typically used for tasks like classification, do not impose this causality constraint. Though traditionally not used for auto-regressive tasks, encoder-only architectures can be adapted for next-token prediction. When computing the output at the current time step, an encoder-only Transformer, or any sequence model, can be made causal by only providing inputs Decoders use a causal attention, ensuring that each token attends only to the preceding tokens. In contrast, encoders allow all tokens to attend to each other by performing attention computation from scratch for each token prediction. \n\nup to and including the current time step. Therefore, in this work, we investigate the idea of using encoder-only Transformers for next-token prediction. We summarize our findings below. \n\nFunctions expressible with Decoder-only and Encoder-only Transformers. We demonstrate that the sets of functions expressible by decoder-only and encoder-only Transformers are not comparable, which goes against intuition that the expressivity of encoders would subsume that of decoders.\n\n## RELATED WORK\nIn contrast, the causal decoder-only model (Brown et al., 2020;Chowdhery et al., 2023) uses only the Transformer decoder and applies causal attention to all tokens to perform nexttoken prediction, ensuring that each token attends only to previous tokens. The prefix decoder-only model (Raffel et al., 2020;Wu et al., 2021) is similar to the causal decoder-only model but differs in that it applies non-causal attention (i.e., full self-attention) to the input sequence (see Figure 8 for visualizations of the attention patterns in these variants). \n\nWith the development of these models, recent studies have investigated the performance of each variant across various tasks. Notably, Wang et al. (2022) examined the zero-shot generalization performance of each model along with various objectives, and Ding et al. (2024) analyzed the performance of causal decoder-only and prefix decoder-only models in in-context learning. However, despite these diverse studies, there is a lack of research on encoder-only models that do not impose the constraint of causal attention for every next token prediction. Therefore, in this work, we analyze the characteristics of encoder-only next-token prediction (ENTP), comparing them with decoder-only models.\n\n## EXPRESSIVE POWER OF ENCODER-ONLY VS. DECODER-ONLY TRANSFORMERS\nFor any L \u2265 2 and D \u2265 1, there exists a position-free decoder D that has L-layers and embedding dimension D, such that for any encoder E, there exists some input sequence \n\nTheorem 2. For any L \u2265 2 and D \u2265 1, there exists a position-free encoder E that has L-layers and embedding dimension D, such that for any decoder D with positional embeddings satisfying p 1 \u0338 = p 2 , there exists some input sequence (x 1 , x 2 , . . . \n\nThese theorems are existential in nature. Informally, Theorem 1 says that if we consider causal model defined over the entirety of R D as its vocabulary, we can find some decoder, for which any encoder will differ from it on some input sequence. Theorem 2 makes a similar (albeit weaker statement) in the other direction; namely the existence of a causal function computable by an encoder, but not by any decoder that uses \"non-trivial\" positional embeddings (e.g. embeddings for different positions are unique). Detailed proof of both theorems are deferred to Appendix A. \n\nOf course, the setting and assumptions of the above two statements are not necessarily very realistic. For one, they focus on general class of causal models rather than only auto-regressive ones. Furthermore, the results only pertain to exact realization and say nothing about approximation. The assumption of unbounded domain is also not realistic as in practice decoders are trained and used over a finite domain of tokens, each with some fixed embeddings. And specific to Theorem 2, no claim is made about decoders that do not use positional embeddings. But despite the limitations, these theorems give an indication that the expressive power of encoder and decoder model are different -despite the almost identical description modulo the attention mask. Changing the mask on the attention scores causes significant changes to the properties of the model. Thus, in the following sections we propose an auto-regressive tasks and run experiments comparing encoders and decoders that corroborates this view.",
            "reference_string": "[273025546 | Ewer et al. | 2024 | Citations: 4]"
        },
        {
            "title": "UL2: Unifying Language Learning Paradigms",
            "venue": "International Conference on Learning Representations",
            "year": 2022,
            "reference_count": 144,
            "citation_count": 313,
            "influential_citation_count": 39,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2205.05131, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "144447820",
                    "name": "Yi Tay"
                },
                {
                    "authorId": "3226635",
                    "name": "Mostafa Dehghani"
                },
                {
                    "authorId": "2057663102",
                    "name": "Vinh Q. Tran"
                },
                {
                    "authorId": "143936294",
                    "name": "Xavier Garc\u00eda"
                },
                {
                    "authorId": "119640649",
                    "name": "Jason Wei"
                },
                {
                    "authorId": "1524732527",
                    "name": "Xuezhi Wang"
                },
                {
                    "authorId": "3351938",
                    "name": "Hyung Won Chung"
                },
                {
                    "authorId": "2119725651",
                    "name": "Dara Bahri"
                },
                {
                    "authorId": "32303439",
                    "name": "Tal Schuster"
                },
                {
                    "authorId": "2115689465",
                    "name": "H. Zheng"
                },
                {
                    "authorId": "65855107",
                    "name": "Denny Zhou"
                },
                {
                    "authorId": "2815290",
                    "name": "N. Houlsby"
                },
                {
                    "authorId": "1680617",
                    "name": "Donald Metzler"
                }
            ],
            "abstract": "Existing pre-trained models are generally geared towards a particular class of problems. To date, there seems to be still no consensus on what the right architecture and pre-training setup should be. This paper presents a unified framework for pre-training models that are universally effective across datasets and setups. We begin by disentangling architectural archetypes with pre-training objectives -- two concepts that are commonly conflated. Next, we present a generalized&unified perspective for self-supervision in NLP and show how different pre-training objectives can be cast as one another and how interpolating between different objectives can be effective. We then propose Mixture-of-Denoisers (MoD), a pre-training objective that combines diverse pre-training paradigms together. We furthermore introduce a notion of mode switching, wherein downstream fine-tuning is associated with specific pre-training schemes. We conduct extensive ablative experiments to compare multiple pre-training objectives and find that our method pushes the Pareto-frontier by outperforming T5&GPT-like models across multiple diverse setups. By scaling our model up to 20B parameters, we achieve SOTA performance on 50 well-established supervised finetuning based NLP tasks. Our model also achieve strong results at in-context learning, outperforming 175B GPT-3 on zero-shot SuperGLUE and tripling the performance of T5-XXL on one-shot summarization. On 0-shot MMLU, UL2 20B outperforms T0 and T5 models. UL2 20B also works well with chain-of-thought prompting and reasoning, making it an appealing choice for research into reasoning at a small to medium scale of 20B parameters. Finally, we apply FLAN instruction tuning to the UL2 20B model, achieving MMLU and Big-Bench scores competitive to FLAN-PaLM 62B. We release Flax-based T5X checkpoints for the UL2 20B&Flan-UL2 20B.",
            "corpus_id": 252780443,
            "sentences": [
                {
                    "corpus_id": "252780443",
                    "title": "UL2: Unifying Language Learning Paradigms",
                    "text": "The line between decoder-only and encoder-decoder models is less clear. PrefixLM models are almost encoder-decoder models with shared parameters (but not quite). From an inductive bias point of view, there are multiple differences. Encoder-Decoder models process input and targets independently with a different set of parameters. This is a form of sparsity where different set of parameters are used for different tokens. Encoder-Decoder models also have a cross attention component that connects input tokens to target tokens. Meanwhile, decoder-only models process inputs and targets by concatenating them. Hence, the representations of inputs and targets are concurrently build layer by layer as the input/targets propagate up the network. Conversely, the decoder in Encoder-decoder models generally only looks at the fully processed encoder input. Overall, the inductive bias of PrefixLM decoder-only models and Encoder-Decoder models could be pretty similar modulo the subtle differences stated above. The distinct property is that Encoder-Decoder models are generally approximately 2x parameters of a decoder-only model when compute-matched. \n\nSparse Models On a side note, there have also been also an emerging trend of sparse pretrained models that achieve state-of-the-art performance. Sparse mixture-of-expert models such as the Switch Transformer (Fedus et al., 2021), GLaM (Du et al., 2021) and/or GShard (Lepikhin et al., 2020) have also demonstrated a lot of promise. While orthogonal to the topic of pretraining objectives, sparse models achieve a very different flop-per-parameter ratio compared to dense models -a core recurring motif in the debate surrounding encoder-decoder models vs decoder-only models.",
                    "score": 0.46186137106523967,
                    "section_title": "Decoder-only vs Encoder-Decoder",
                    "char_start_offset": 10300,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 71
                        },
                        {
                            "start": 72,
                            "end": 161
                        },
                        {
                            "start": 162,
                            "end": 231
                        },
                        {
                            "start": 232,
                            "end": 330
                        },
                        {
                            "start": 331,
                            "end": 422
                        },
                        {
                            "start": 423,
                            "end": 528
                        },
                        {
                            "start": 529,
                            "end": 609
                        },
                        {
                            "start": 610,
                            "end": 743
                        },
                        {
                            "start": 744,
                            "end": 852
                        },
                        {
                            "start": 853,
                            "end": 1007
                        },
                        {
                            "start": 1008,
                            "end": 1148
                        },
                        {
                            "start": 1151,
                            "end": 1295
                        },
                        {
                            "start": 1296,
                            "end": 1482
                        },
                        {
                            "start": 1483,
                            "end": 1725
                        }
                    ],
                    "ref_mentions": [],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.84814453125
                }
            ],
            "relevance_judgement": 0.84814453125,
            "relevance_judgment_input_expanded": "# Title: UL2: Unifying Language Learning Paradigms\n# Venue: International Conference on Learning Representations\n# Authors: Yi Tay, Mostafa Dehghani, Vinh Q. Tran, Xavier Garc\u00eda, Jason Wei, Xuezhi Wang, Hyung Won Chung, Dara Bahri, Tal Schuster, H. Zheng, Denny Zhou, N. Houlsby, Donald Metzler\n## Abstract\nExisting pre-trained models are generally geared towards a particular class of problems. To date, there seems to be still no consensus on what the right architecture and pre-training setup should be. This paper presents a unified framework for pre-training models that are universally effective across datasets and setups. We begin by disentangling architectural archetypes with pre-training objectives -- two concepts that are commonly conflated. Next, we present a generalized&unified perspective for self-supervision in NLP and show how different pre-training objectives can be cast as one another and how interpolating between different objectives can be effective. We then propose Mixture-of-Denoisers (MoD), a pre-training objective that combines diverse pre-training paradigms together. We furthermore introduce a notion of mode switching, wherein downstream fine-tuning is associated with specific pre-training schemes. We conduct extensive ablative experiments to compare multiple pre-training objectives and find that our method pushes the Pareto-frontier by outperforming T5&GPT-like models across multiple diverse setups. By scaling our model up to 20B parameters, we achieve SOTA performance on 50 well-established supervised finetuning based NLP tasks. Our model also achieve strong results at in-context learning, outperforming 175B GPT-3 on zero-shot SuperGLUE and tripling the performance of T5-XXL on one-shot summarization. On 0-shot MMLU, UL2 20B outperforms T0 and T5 models. UL2 20B also works well with chain-of-thought prompting and reasoning, making it an appealing choice for research into reasoning at a small to medium scale of 20B parameters. Finally, we apply FLAN instruction tuning to the UL2 20B model, achieving MMLU and Big-Bench scores competitive to FLAN-PaLM 62B. We release Flax-based T5X checkpoints for the UL2 20B&Flan-UL2 20B.\n## Decoder-only vs Encoder-Decoder\nThe line between decoder-only and encoder-decoder models is less clear. PrefixLM models are almost encoder-decoder models with shared parameters (but not quite). From an inductive bias point of view, there are multiple differences. Encoder-Decoder models process input and targets independently with a different set of parameters. This is a form of sparsity where different set of parameters are used for different tokens. Encoder-Decoder models also have a cross attention component that connects input tokens to target tokens. Meanwhile, decoder-only models process inputs and targets by concatenating them. Hence, the representations of inputs and targets are concurrently build layer by layer as the input/targets propagate up the network. Conversely, the decoder in Encoder-decoder models generally only looks at the fully processed encoder input. Overall, the inductive bias of PrefixLM decoder-only models and Encoder-Decoder models could be pretty similar modulo the subtle differences stated above. The distinct property is that Encoder-Decoder models are generally approximately 2x parameters of a decoder-only model when compute-matched. \n\nSparse Models On a side note, there have also been also an emerging trend of sparse pretrained models that achieve state-of-the-art performance. Sparse mixture-of-expert models such as the Switch Transformer (Fedus et al., 2021), GLaM (Du et al., 2021) and/or GShard (Lepikhin et al., 2020) have also demonstrated a lot of promise. While orthogonal to the topic of pretraining objectives, sparse models achieve a very different flop-per-parameter ratio compared to dense models -a core recurring motif in the debate surrounding encoder-decoder models vs decoder-only models.",
            "reference_string": "[252780443 | Tay et al. | 2022 | Citations: 313]"
        },
        {
            "title": "What Language Model Architecture and Pretraining Objective Work Best for Zero-Shot Generalization?",
            "venue": "International Conference on Machine Learning",
            "year": 2022,
            "reference_count": 86,
            "citation_count": 175,
            "influential_citation_count": 9,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "http://arxiv.org/pdf/2204.05832",
                "status": "GREEN",
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2204.05832, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2135734748",
                    "name": "Thomas Wang"
                },
                {
                    "authorId": "145625142",
                    "name": "Adam Roberts"
                },
                {
                    "authorId": "80424302",
                    "name": "Daniel Hesslow"
                },
                {
                    "authorId": "1379806208",
                    "name": "Teven Le Scao"
                },
                {
                    "authorId": "3351938",
                    "name": "Hyung Won Chung"
                },
                {
                    "authorId": "46181066",
                    "name": "Iz Beltagy"
                },
                {
                    "authorId": "143945447",
                    "name": "Julien Launay"
                },
                {
                    "authorId": "2402716",
                    "name": "Colin Raffel"
                }
            ],
            "abstract": "Large pretrained Transformer language models have been shown to exhibit zero-shot generalization, i.e. they can perform a wide variety of tasks that they were not explicitly trained on. However, the architectures and pretraining objectives used across state-of-the-art models differ significantly, and there has been limited systematic comparison of these factors. In this work, we present a large-scale evaluation of modeling choices and their impact on zero-shot generalization. In particular, we focus on text-to-text models and experiment with three model architectures (causal/non-causal decoder-only and encoder-decoder), trained with two different pretraining objectives (autoregressive and masked language modeling), and evaluated with and without multitask prompted finetuning. We train models with over 5 billion parameters for more than 170 billion tokens, thereby increasing the likelihood that our conclusions will transfer to even larger scales. Our experiments show that causal decoder-only models trained on an autoregressive language modeling objective exhibit the strongest zero-shot generalization after purely unsupervised pretraining. However, models with non-causal visibility on their input trained with a masked language modeling objective followed by multitask finetuning perform the best among our experiments. We therefore consider the adaptation of pretrained models across architectures and objectives. We find that pretrained non-causal decoder models can be adapted into performant generative causal decoder models, using autoregressive language modeling as a downstream task. Furthermore, we find that pretrained causal decoder models can be efficiently adapted into non-causal decoder models, ultimately achieving competitive performance after multitask finetuning. Code and checkpoints are available at https://github.com/bigscience-workshop/architecture-objective.",
            "corpus_id": 248118752,
            "sentences": [
                {
                    "corpus_id": "248118752",
                    "title": "What Language Model Architecture and Pretraining Objective Work Best for Zero-Shot Generalization?",
                    "text": "When studying adaptation and the conversion from one architecture to another, we also considered converting to and from encoder-decoder models. Conversion across causal and non-causal decoder-only models is straightforward, simply by switching the attention mask; for encoder-decoder, parameters have to be either pruned or added for both the entire encoder, and for the cross-attention in the decoder. Results from one of our attempt to convert an encoder-decoder into a causal decoder are reported in Figure 10. While converting across causal/non-causal decoder provides an improvement over training from scratch, this is not the case here. Validation loss when adapting an encoder-decoder pretrained with MLM to a causal decoder-only using FLM. We adapted a pretrained (for 168B tokens) encoder-decoder model to decoder-only by feeding an empty input into the encoder and causally training with a FLM objective on the decoder. We stopped this adaptation once it was clear the performance would not match that of a causal FLM trained from scratch, in contrast with the other adaptations we studied.",
                    "score": 0.6298750196169863,
                    "section_title": "E.4 Adaptation from an encoder-decoder",
                    "char_start_offset": 46680,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 143
                        },
                        {
                            "start": 144,
                            "end": 402
                        },
                        {
                            "start": 403,
                            "end": 513
                        },
                        {
                            "start": 514,
                            "end": 642
                        },
                        {
                            "start": 643,
                            "end": 747
                        },
                        {
                            "start": 748,
                            "end": 929
                        },
                        {
                            "start": 930,
                            "end": 1100
                        }
                    ],
                    "ref_mentions": [],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.84423828125
                },
                {
                    "corpus_id": "248118752",
                    "title": "What Language Model Architecture and Pretraining Objective Work Best for Zero-Shot Generalization?",
                    "text": "We consider causal decoder (CD) , encoder-decoder (ED) , and non-causal decoder (ND) architectures. All models share the basic configuration outlined in Table 1. For fair comparison across architectures, we aim to approximately match pretraining compute budget; accordingly, our encoder-decoder models have twice as many layers as the decoder-only models. This results in encoder-decoder models with 11B parameters and decoder-only models with 4.8B parameters. We note that due to the cross-attention layers, encoder-decoder models are approximately \u223c 10% more computationally expensive to run than the decoder-only models we consider.",
                    "score": 0.5834455430461523,
                    "section_title": "Architecture",
                    "char_start_offset": 21774,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 99
                        },
                        {
                            "start": 100,
                            "end": 161
                        },
                        {
                            "start": 162,
                            "end": 355
                        },
                        {
                            "start": 356,
                            "end": 460
                        },
                        {
                            "start": 461,
                            "end": 635
                        }
                    ],
                    "ref_mentions": [],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.83642578125
                },
                {
                    "corpus_id": "248118752",
                    "title": "What Language Model Architecture and Pretraining Objective Work Best for Zero-Shot Generalization?",
                    "text": "In this paper, we systematically studied the effects of pretraining objective and architecture choices on the zero-shot generalization abilities of large language models. Specifically, we compared language modeling and masked language modeling objectives applied to causal/non-causal decoder-only and encoder-decoder architectures. We also evaluated zero-shot performance with and without multitask finetuning. Notably, we found that the best objective and architecture is the opposite in these two settings: a causal decoder-only pretrained with full language modeling performs best if evaluated immediately after pretraining, whereas when adding a multitask finetuning step, an encoder-decoder pretrained with masked language modeling performs best. We therefore evaluate the practice of adaptation, to convert models across architectures and objectives. We found a simple efficient compromise, where a causal decoder-only model pretrained with full language modeling underwent additional masked language model training as a non-causal decoder-only model, yielding significant speedup in convergence over starting from scratch. This enables practitioners to get both an excellent generative model and a model that delivers good performance after multitask finetuning. Our results provide significant new insights into the design of LLMs. In the future, we are interested in work investigating architectures and objectives that perform well regardless of whether multitask finetuning is performed. To facilitate future work, we release all models, code, and data used in our study.",
                    "score": 0.537863017077425,
                    "section_title": "Conclusion",
                    "char_start_offset": 40729,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 170
                        },
                        {
                            "start": 171,
                            "end": 331
                        },
                        {
                            "start": 332,
                            "end": 410
                        },
                        {
                            "start": 411,
                            "end": 751
                        },
                        {
                            "start": 752,
                            "end": 856
                        },
                        {
                            "start": 857,
                            "end": 1129
                        },
                        {
                            "start": 1130,
                            "end": 1269
                        },
                        {
                            "start": 1270,
                            "end": 1339
                        },
                        {
                            "start": 1340,
                            "end": 1498
                        },
                        {
                            "start": 1499,
                            "end": 1582
                        }
                    ],
                    "ref_mentions": [],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.83203125
                },
                {
                    "corpus_id": "248118752",
                    "title": "What Language Model Architecture and Pretraining Objective Work Best for Zero-Shot Generalization?",
                    "text": "Notable pretrained language models using an encoder-decoder architecture include BART [Lewis et al., 2019] and T5 [Raffel et al., 2020]. T5 in particular was recently used as the foundation for the T0 model [Sanh et al., 2021], which leveraged large-scale multitask finetuning to achieve strong zero-shot generalization, outperforming decoder-only models an order of magnitude larger. \n\nCausal decoder-only. Although the encoder-decoder is the original Transformer variant, most recent LLMs use a decoder-only architecture. These models can be trained as a traditional language model (i.e. to predict the next token in a sequence). Decoder-only models have no independent means of processing or representing the input sequence and target sequence differently-all tokens are processed in an equivalent fashion, and, because of the causal masking pattern, conditioning is simply based on past tokens (see Figure 2, on the left). On the one hand, this means that the representation for any conditioning text is inherently weaker; on the other hand, it yields a simpler architecture that is naturally suited to a standard autoregressive next-step-prediction pretraining objective. We refer to this architecture as causal decoder-only (CD) . Most notably, the CD architecture makes up the backbone of the GPT series of models [Radford et al., 2018, 2019, Brown et al., 2020] as well as many other recent record-breaking LLMs [Zeng et al., 2021, Kim et al., 2021, Smith et al., 2022, Thoppilan et al., 2022, Rae et al., 2021, Hoffmann et al., 2022, Chowdhery et al., 2022]. \n\nNon-causal decoder-only. To allow decoder-only models to build richer non-causal representations of the input/conditioning text, it has been proposed to simply modify the attention mask used. Specifically, the self-attention masking pattern can be changed so that the region of the input sequence corresponding to conditioning information has a non-causal mask (i.e.",
                    "score": 0.7331205509213734,
                    "section_title": "Architectures",
                    "char_start_offset": 10211,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 136
                        },
                        {
                            "start": 137,
                            "end": 384
                        },
                        {
                            "start": 387,
                            "end": 407
                        },
                        {
                            "start": 408,
                            "end": 523
                        },
                        {
                            "start": 524,
                            "end": 631
                        },
                        {
                            "start": 632,
                            "end": 926
                        },
                        {
                            "start": 927,
                            "end": 1176
                        },
                        {
                            "start": 1177,
                            "end": 1236
                        },
                        {
                            "start": 1237,
                            "end": 1567
                        },
                        {
                            "start": 1570,
                            "end": 1594
                        },
                        {
                            "start": 1595,
                            "end": 1761
                        },
                        {
                            "start": 1762,
                            "end": 1936
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 114,
                            "end": 135,
                            "matchedPaperCorpusId": "204838007"
                        },
                        {
                            "start": 1321,
                            "end": 1342,
                            "matchedPaperCorpusId": "49313245"
                        },
                        {
                            "start": 1438,
                            "end": 1456,
                            "matchedPaperCorpusId": "237485423"
                        },
                        {
                            "start": 1476,
                            "end": 1500,
                            "matchedPaperCorpusId": "238582964"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.794921875
                },
                {
                    "corpus_id": "248118752",
                    "title": "What Language Model Architecture and Pretraining Objective Work Best for Zero-Shot Generalization?",
                    "text": "We now focus on the relatively new practice of multitask finetuning, where there has not yet been any systematic study of the influence of the architecture and training objective. Notably, the two main papers advocating this practice use completely different approaches: Sanh et al. [2021] finetunes an encoder-decoder model pretrained with span corruption, whereas Wei et al. [2021] finetunes a decoder-only pretrained with full language modeling. It is not immediately clear which approach is more natural: while decoder-only models trained with full language modeling are better at zero-shot generalization (as evidenced in Section 4.1), encoder-decoder models and masked language modeling pretraining have been shown to perform significantly better after finetuning [Raffel et al., 2020]. We therefore evaluate every architecture and objective combination after multitask finetuning. \n\nOur results are outlined in Figure 4. The encoder-decoder pretrained with span corruption offers the best performance after multitask finetuning. Specifically, on EAI-Eval, the best performance is achieved by the encoder-decoder with MLM, and the non-causal decoder with MLM comes in a close second. However, the difference is more significant on T0-Eval, where the encoder-decoder with MLM pretraining outperforms other models by a large margin. Finally, encoder-decoder pretrained with PLM and causal decoder with MLM achieve significantly worse performance than other models. These results are consistent across all levels of pretraining (see early checkpoints in Appendix D). \n\nFinding 2. Encoder-decoder models trained with masked language modeling achieve the best zero-shot performance after multitask finetuning . More broadly, approaches that perform well in the single-task finetuning setting perform well on multitask finetuning. \n\nTable 3: After full or prefix language modeling pretraining, the causal decoder (FLM) exhibits the best zero-shot generalization abilities, followed closely by the non-causal decoder (PLM).",
                    "score": 0.5465808938586033,
                    "section_title": "After multitask finetuning",
                    "char_start_offset": 29597,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 179
                        },
                        {
                            "start": 180,
                            "end": 448
                        },
                        {
                            "start": 449,
                            "end": 792
                        },
                        {
                            "start": 793,
                            "end": 887
                        },
                        {
                            "start": 890,
                            "end": 927
                        },
                        {
                            "start": 928,
                            "end": 1035
                        },
                        {
                            "start": 1036,
                            "end": 1189
                        },
                        {
                            "start": 1190,
                            "end": 1336
                        },
                        {
                            "start": 1337,
                            "end": 1468
                        },
                        {
                            "start": 1469,
                            "end": 1569
                        },
                        {
                            "start": 1572,
                            "end": 1711
                        },
                        {
                            "start": 1712,
                            "end": 1830
                        },
                        {
                            "start": 1833,
                            "end": 2022
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 770,
                            "end": 791,
                            "matchedPaperCorpusId": "204838007"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.7509765625
                },
                {
                    "corpus_id": "248118752",
                    "title": "What Language Model Architecture and Pretraining Objective Work Best for Zero-Shot Generalization?",
                    "text": "Large pretrained Transformer language models have been shown to exhibit zero-shot generalization, i.e. they can perform a wide variety of tasks that they were not explicitly trained on. However, the architectures and pretraining objectives used across state-of-the-art models differ significantly, and there has been limited systematic comparison of these factors. In this work, we present a large-scale evaluation of modeling choices and their impact on zero-shot generalization. In particular, we focus on text-to-text models and experiment with three model architectures (causal/non-causal decoder-only and encoder-decoder), trained with two different pretraining objectives (autoregressive and masked language modeling), and evaluated with and without multitask prompted finetuning. We train models with over 5 billion parameters for more than 170 billion tokens, thereby increasing the likelihood that our conclusions will transfer to even larger scales. Our experiments show that causal decoder-only models trained on an autoregressive language modeling objective exhibit the strongest zero-shot generalization after purely unsupervised pretraining. However, models with non-causal visibility on their input trained with a masked language modeling objective followed by multitask finetuning perform the best among our experiments. We therefore consider the adaptation of pretrained models across architectures and objectives. We find that pretrained non-causal decoder models can be adapted into performant generative causal decoder models, using autoregressive language modeling as a downstream task. Furthermore, we find that pretrained causal decoder models can be efficiently adapted into non-causal decoder models, ultimately achieving competitive performance after multitask finetuning. Code and checkpoints are available at https://github.com/bigscience-workshop/architecture-objective.",
                    "score": 0.5104326896624234,
                    "section_title": "abstract",
                    "char_start_offset": 0,
                    "sentence_offsets": [],
                    "ref_mentions": [],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.73388671875
                },
                {
                    "corpus_id": "248118752",
                    "title": "What Language Model Architecture and Pretraining Objective Work Best for Zero-Shot Generalization?",
                    "text": ". First, we propose to pretrain a non-causal decoder model with an MLM objective and then further train the model as a causal decoder with a FLM objective (language modeling adaptation). This conversion is simple, as the parameters and overall architecture can be kept the same, and only the attention mask needs to be switched. We note that we also attempted this adaptation from the decoder portion of an encoder-decoder model, but it performed significantly worse than training from scratch, as discussed in Appendix E.4. \n\nValidations losses are plotted in Figure 6, on the left. Starting from an MLM-pretrained non-causal decoder model speeds up convergence significantly compared to training a causal-decoder model with an FLM objective from scratch. To achieve a loss comparable to the one achieved after 168B tokens of FLM pretraining, language modeling adaptation requires only 105B additional tokens (a 1.6\u00d7 speed-up). This makes it possible to obtain both a high-quality zero-shot model and a good generative model, for only 1.6\u00d7 the cost of training a single model. A causal decoder-only pretrained with FLM from scratch (Causal FLM) compared to a model being adapted with FLM following 168B pretraining tokens as a non-causal masked language model (LMadapted Non-causal MLM). The adaptation requires 63% of the tokens (1.6\u00d7 speedup) versus training from scratch. Right: Causal and non-causal decoder-only masked language models (Causal MLM, Non-causal MLM) trained from scratch compared to a model being adapted to a non-causal MLM following 168B pretraining tokens as a causal FLM (MLM-adapted Causal FLM). The adaptation requires 30% of the tokens (3.3\u00d7 speedup) versus training the non-causal MLM from scratch. \n\nNon-causal masked language modeling adaptation (NC-A). To investigate alternative avenues for adaptation, we now introduce non-causal masked language modeling adaptation: starting from a causal decoder model pretrained with FLM as the objective, we then continue training the model as a non-causal decoder using an MLM objective.",
                    "score": 0.6302885311351464,
                    "section_title": "Language modeling adaptation (LM-A)",
                    "char_start_offset": 35459,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 186
                        },
                        {
                            "start": 187,
                            "end": 328
                        },
                        {
                            "start": 329,
                            "end": 524
                        },
                        {
                            "start": 527,
                            "end": 583
                        },
                        {
                            "start": 584,
                            "end": 756
                        },
                        {
                            "start": 757,
                            "end": 928
                        },
                        {
                            "start": 929,
                            "end": 1077
                        },
                        {
                            "start": 1078,
                            "end": 1288
                        },
                        {
                            "start": 1289,
                            "end": 1375
                        },
                        {
                            "start": 1376,
                            "end": 1620
                        },
                        {
                            "start": 1621,
                            "end": 1726
                        },
                        {
                            "start": 1729,
                            "end": 1783
                        },
                        {
                            "start": 1784,
                            "end": 2058
                        }
                    ],
                    "ref_mentions": [],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.71435546875
                },
                {
                    "corpus_id": "248118752",
                    "title": "What Language Model Architecture and Pretraining Objective Work Best for Zero-Shot Generalization?",
                    "text": "We find that the MLM-adapted model performs best by a significant margin and outperforms every other model we considered on EAI-Eval. Furthermore, the measured zero-shot generalization is in line with the MLM-pretrained non-causal decoder reported in Figure 4, though it still lags behind the MLM-pretrained encoder-decoder, despite the adapted models having seen 51 billion additional tokens. Finally, we note that performing non-causal multitask finetuning of the causal model produces no meaningful change in performance. \n\nFinding 3. Decoder-only models can be efficiently adapted from one architecture/objective prior to the other. Specifically, to obtain both a generative and a multitask model with the smallest total compute budget possible, we recommend starting with a causal decoder-only model, pretraining it with a full language modeling objective, and then using non-causal masked language modeling adaptation before taking it through multitask finetuning . Figure 7: Applying non-causal MLM adaptation to a causal decoder-only FLM before multitask finetuning improves zero-shot performance, even when controlling for additional LM pretraining for the same number of tokens. Zero-shot generalization on T0-Eval (left) and EAI-Eval (right), for the T5-LM and T0 baselines (grey), and for models from our study. Converting the model into a non-causal decoder for multitask finetuning only does not improve performance on T0-Eval. Results after adaptation are in line with non-causal decoder-only pretrained with MLM in Figure 4.",
                    "score": 0.5567292838615006,
                    "section_title": "Language modeling adaptation (LM-A)",
                    "char_start_offset": 39174,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 133
                        },
                        {
                            "start": 134,
                            "end": 393
                        },
                        {
                            "start": 394,
                            "end": 524
                        },
                        {
                            "start": 527,
                            "end": 636
                        },
                        {
                            "start": 637,
                            "end": 971
                        },
                        {
                            "start": 972,
                            "end": 1188
                        },
                        {
                            "start": 1189,
                            "end": 1323
                        },
                        {
                            "start": 1324,
                            "end": 1441
                        },
                        {
                            "start": 1442,
                            "end": 1540
                        }
                    ],
                    "ref_mentions": [],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.689453125
                },
                {
                    "corpus_id": "248118752",
                    "title": "What Language Model Architecture and Pretraining Objective Work Best for Zero-Shot Generalization?",
                    "text": "Large language models (LLMs) pretrained on unstructured text data have been shown to be capable of performing a wide variety of text processing tasks without additional training. This ability has been referred to as zero-shot generalization since these models are typically pretrained with a self-supervised objective that is not specific to a downstream task. Zero-shot generalization is particularly useful because it does not require any additional data or training in order to enable the model to perform a given task. As such, there has been an explosion of work on developing LLMs and training techniques that produce strong zero-shot generalization [Brown et al., 2020, Wang and Komatsuzaki, 2021, Du et al., 2021, Lin et al., 2021, Rae et al., 2021, Hoffmann et al., 2022, Chowdhery et al., 2022]. One recent line of work [Sanh et al., 2021, Wei et al., 2021, Xu et al., 2022] has demonstrated that adding an explicit multitask finetuning stage on an ensemble of prompted tasks after pretraining can significantly boost the zero-shot capabilities of LLMs. \n\nModern LLMs are based on the Transformer architecture [Vaswani et al., 2017]. While the original Transformer included a separate encoder that processes input text and a decoder that generates target text, most recent LLMs are causal decoder-only (CD) models trained to autoregressively predict a text sequence [Liu et al., 2018, Radford et al., 2018, Al-Rfou et al., 2019]. In contrast with this trend, Raffel et al. [2020] has shown that encoder-decoder (ED) models outperform decoder-only LLMs for transfer learning (i.e. where a pretrained model is finetuned on a single downstream task). Non-causal decoders (ND) [Liu et al., 2018, Dong et al., 2019] use a modified attention mask to bridge the gap between decoder-only and encoder-decoder models. However, they have seen limited adoption.",
                    "score": 0.44552282191581016,
                    "section_title": "Introduction",
                    "char_start_offset": 561,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 178
                        },
                        {
                            "start": 179,
                            "end": 360
                        },
                        {
                            "start": 361,
                            "end": 522
                        },
                        {
                            "start": 523,
                            "end": 805
                        },
                        {
                            "start": 806,
                            "end": 1063
                        },
                        {
                            "start": 1066,
                            "end": 1143
                        },
                        {
                            "start": 1144,
                            "end": 1439
                        },
                        {
                            "start": 1440,
                            "end": 1589
                        },
                        {
                            "start": 1590,
                            "end": 1657
                        },
                        {
                            "start": 1658,
                            "end": 1817
                        },
                        {
                            "start": 1818,
                            "end": 1859
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 1120,
                            "end": 1142,
                            "matchedPaperCorpusId": "13756489"
                        },
                        {
                            "start": 1376,
                            "end": 1393,
                            "matchedPaperCorpusId": "3608234"
                        },
                        {
                            "start": 1393,
                            "end": 1415,
                            "matchedPaperCorpusId": "49313245"
                        },
                        {
                            "start": 1415,
                            "end": 1438,
                            "matchedPaperCorpusId": "52004855"
                        },
                        {
                            "start": 1469,
                            "end": 1489,
                            "matchedPaperCorpusId": "204838007"
                        },
                        {
                            "start": 1683,
                            "end": 1700,
                            "matchedPaperCorpusId": "3608234"
                        },
                        {
                            "start": 1700,
                            "end": 1720,
                            "matchedPaperCorpusId": "147704286"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.658203125
                },
                {
                    "corpus_id": "248118752",
                    "title": "What Language Model Architecture and Pretraining Objective Work Best for Zero-Shot Generalization?",
                    "text": "Specifically, the self-attention masking pattern can be changed so that the region of the input sequence corresponding to conditioning information has a non-causal mask (i.e. attention in this region is not restricted to past tokens, see middle of Figure 2), as in the encoder of an encoder-decoder architecture. We refer to this architecture as non-causal decoder-only (ND) . Sometimes called a prefix language model, this approach was introduced by [Liu et al., 2018] and was later explored as an architectural variant by [Raffel et al., 2020, Wu et al., 2021]. Despite single-task finetuning performance nearly on par with encoder-decoder models [Raffel et al., 2020], it has seen limited adoption in the literature. \n\nEncoder-only. As an aside, we note that another popular architectural variant is to only use a Transformer encoder layer stack. This model architecture underlies the ubiquitous BERT [Devlin et al., 2018] and its derivatives. However, this architecture is limited to producing the same number of tokens as it was fed as input, considerably limiting its applicability and making it only rarely used in the zero-shot setting [Tamborrino et al., 2020]. We therefore omit it from consideration. For full language modeling, all tokens in a sequence are used during training. For prefix language modeling, we randomly select a prefix size, and hence only half of the tokens are used on average to derive the loss. At inference time, the prefix would be over the input/conditioning information. Finally, for masked language modeling, we mask 15% of the tokens, in spans of 3 tokens on average. We use sentinel tokens to replace spans (not represented here), and the model outputs subsequently each sentinel followed by its prediction of the content masked by the sentinel. \n\nComparisons across architectures. Decoder-only models process a single sequence consisting of the concatenation of the input and target text. On the other hand, in an encoder-decoder, the encoder processes only the input and the decoder processes only the target.",
                    "score": 0.5715552541593032,
                    "section_title": "Architectures",
                    "char_start_offset": 11973,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 174
                        },
                        {
                            "start": 175,
                            "end": 312
                        },
                        {
                            "start": 313,
                            "end": 376
                        },
                        {
                            "start": 377,
                            "end": 563
                        },
                        {
                            "start": 564,
                            "end": 719
                        },
                        {
                            "start": 722,
                            "end": 735
                        },
                        {
                            "start": 736,
                            "end": 849
                        },
                        {
                            "start": 850,
                            "end": 946
                        },
                        {
                            "start": 947,
                            "end": 1170
                        },
                        {
                            "start": 1171,
                            "end": 1211
                        },
                        {
                            "start": 1212,
                            "end": 1290
                        },
                        {
                            "start": 1291,
                            "end": 1428
                        },
                        {
                            "start": 1429,
                            "end": 1508
                        },
                        {
                            "start": 1509,
                            "end": 1607
                        },
                        {
                            "start": 1608,
                            "end": 1786
                        },
                        {
                            "start": 1789,
                            "end": 1822
                        },
                        {
                            "start": 1823,
                            "end": 1930
                        },
                        {
                            "start": 1931,
                            "end": 2052
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 451,
                            "end": 469,
                            "matchedPaperCorpusId": "3608234"
                        },
                        {
                            "start": 524,
                            "end": 544,
                            "matchedPaperCorpusId": "204838007"
                        },
                        {
                            "start": 649,
                            "end": 670,
                            "matchedPaperCorpusId": "204838007"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.62841796875
                }
            ],
            "relevance_judgement": 0.84423828125,
            "relevance_judgment_input_expanded": "# Title: What Language Model Architecture and Pretraining Objective Work Best for Zero-Shot Generalization?\n# Venue: International Conference on Machine Learning\n# Authors: Thomas Wang, Adam Roberts, Daniel Hesslow, Teven Le Scao, Hyung Won Chung, Iz Beltagy, Julien Launay, Colin Raffel\n## Abstract\nLarge pretrained Transformer language models have been shown to exhibit zero-shot generalization, i.e. they can perform a wide variety of tasks that they were not explicitly trained on. However, the architectures and pretraining objectives used across state-of-the-art models differ significantly, and there has been limited systematic comparison of these factors. In this work, we present a large-scale evaluation of modeling choices and their impact on zero-shot generalization. In particular, we focus on text-to-text models and experiment with three model architectures (causal/non-causal decoder-only and encoder-decoder), trained with two different pretraining objectives (autoregressive and masked language modeling), and evaluated with and without multitask prompted finetuning. We train models with over 5 billion parameters for more than 170 billion tokens, thereby increasing the likelihood that our conclusions will transfer to even larger scales. Our experiments show that causal decoder-only models trained on an autoregressive language modeling objective exhibit the strongest zero-shot generalization after purely unsupervised pretraining. However, models with non-causal visibility on their input trained with a masked language modeling objective followed by multitask finetuning perform the best among our experiments. We therefore consider the adaptation of pretrained models across architectures and objectives. We find that pretrained non-causal decoder models can be adapted into performant generative causal decoder models, using autoregressive language modeling as a downstream task. Furthermore, we find that pretrained causal decoder models can be efficiently adapted into non-causal decoder models, ultimately achieving competitive performance after multitask finetuning. Code and checkpoints are available at https://github.com/bigscience-workshop/architecture-objective.\n## Introduction\nLarge language models (LLMs) pretrained on unstructured text data have been shown to be capable of performing a wide variety of text processing tasks without additional training. This ability has been referred to as zero-shot generalization since these models are typically pretrained with a self-supervised objective that is not specific to a downstream task. Zero-shot generalization is particularly useful because it does not require any additional data or training in order to enable the model to perform a given task. As such, there has been an explosion of work on developing LLMs and training techniques that produce strong zero-shot generalization [Brown et al., 2020, Wang and Komatsuzaki, 2021, Du et al., 2021, Lin et al., 2021, Rae et al., 2021, Hoffmann et al., 2022, Chowdhery et al., 2022]. One recent line of work [Sanh et al., 2021, Wei et al., 2021, Xu et al., 2022] has demonstrated that adding an explicit multitask finetuning stage on an ensemble of prompted tasks after pretraining can significantly boost the zero-shot capabilities of LLMs. \n\nModern LLMs are based on the Transformer architecture [Vaswani et al., 2017]. While the original Transformer included a separate encoder that processes input text and a decoder that generates target text, most recent LLMs are causal decoder-only (CD) models trained to autoregressively predict a text sequence [Liu et al., 2018, Radford et al., 2018, Al-Rfou et al., 2019]. In contrast with this trend, Raffel et al. [2020] has shown that encoder-decoder (ED) models outperform decoder-only LLMs for transfer learning (i.e. where a pretrained model is finetuned on a single downstream task). Non-causal decoders (ND) [Liu et al., 2018, Dong et al., 2019] use a modified attention mask to bridge the gap between decoder-only and encoder-decoder models. However, they have seen limited adoption.\n\n## Architectures\nNotable pretrained language models using an encoder-decoder architecture include BART [Lewis et al., 2019] and T5 [Raffel et al., 2020]. T5 in particular was recently used as the foundation for the T0 model [Sanh et al., 2021], which leveraged large-scale multitask finetuning to achieve strong zero-shot generalization, outperforming decoder-only models an order of magnitude larger. \n\nCausal decoder-only. Although the encoder-decoder is the original Transformer variant, most recent LLMs use a decoder-only architecture. These models can be trained as a traditional language model (i.e. to predict the next token in a sequence). Decoder-only models have no independent means of processing or representing the input sequence and target sequence differently-all tokens are processed in an equivalent fashion, and, because of the causal masking pattern, conditioning is simply based on past tokens (see Figure 2, on the left). On the one hand, this means that the representation for any conditioning text is inherently weaker; on the other hand, it yields a simpler architecture that is naturally suited to a standard autoregressive next-step-prediction pretraining objective. We refer to this architecture as causal decoder-only (CD) . Most notably, the CD architecture makes up the backbone of the GPT series of models [Radford et al., 2018, 2019, Brown et al., 2020] as well as many other recent record-breaking LLMs [Zeng et al., 2021, Kim et al., 2021, Smith et al., 2022, Thoppilan et al., 2022, Rae et al., 2021, Hoffmann et al., 2022, Chowdhery et al., 2022]. \n\nNon-causal decoder-only. To allow decoder-only models to build richer non-causal representations of the input/conditioning text, it has been proposed to simply modify the attention mask used. Specifically, the self-attention masking pattern can be changed so that the region of the input sequence corresponding to conditioning information has a non-causal mask (i.e.\n...\nSpecifically, the self-attention masking pattern can be changed so that the region of the input sequence corresponding to conditioning information has a non-causal mask (i.e. attention in this region is not restricted to past tokens, see middle of Figure 2), as in the encoder of an encoder-decoder architecture. We refer to this architecture as non-causal decoder-only (ND) . Sometimes called a prefix language model, this approach was introduced by [Liu et al., 2018] and was later explored as an architectural variant by [Raffel et al., 2020, Wu et al., 2021]. Despite single-task finetuning performance nearly on par with encoder-decoder models [Raffel et al., 2020], it has seen limited adoption in the literature. \n\nEncoder-only. As an aside, we note that another popular architectural variant is to only use a Transformer encoder layer stack. This model architecture underlies the ubiquitous BERT [Devlin et al., 2018] and its derivatives. However, this architecture is limited to producing the same number of tokens as it was fed as input, considerably limiting its applicability and making it only rarely used in the zero-shot setting [Tamborrino et al., 2020]. We therefore omit it from consideration. For full language modeling, all tokens in a sequence are used during training. For prefix language modeling, we randomly select a prefix size, and hence only half of the tokens are used on average to derive the loss. At inference time, the prefix would be over the input/conditioning information. Finally, for masked language modeling, we mask 15% of the tokens, in spans of 3 tokens on average. We use sentinel tokens to replace spans (not represented here), and the model outputs subsequently each sentinel followed by its prediction of the content masked by the sentinel. \n\nComparisons across architectures. Decoder-only models process a single sequence consisting of the concatenation of the input and target text. On the other hand, in an encoder-decoder, the encoder processes only the input and the decoder processes only the target.\n\n## Architecture\nWe consider causal decoder (CD) , encoder-decoder (ED) , and non-causal decoder (ND) architectures. All models share the basic configuration outlined in Table 1. For fair comparison across architectures, we aim to approximately match pretraining compute budget; accordingly, our encoder-decoder models have twice as many layers as the decoder-only models. This results in encoder-decoder models with 11B parameters and decoder-only models with 4.8B parameters. We note that due to the cross-attention layers, encoder-decoder models are approximately \u223c 10% more computationally expensive to run than the decoder-only models we consider.\n\n## After multitask finetuning\nWe now focus on the relatively new practice of multitask finetuning, where there has not yet been any systematic study of the influence of the architecture and training objective. Notably, the two main papers advocating this practice use completely different approaches: Sanh et al. [2021] finetunes an encoder-decoder model pretrained with span corruption, whereas Wei et al. [2021] finetunes a decoder-only pretrained with full language modeling. It is not immediately clear which approach is more natural: while decoder-only models trained with full language modeling are better at zero-shot generalization (as evidenced in Section 4.1), encoder-decoder models and masked language modeling pretraining have been shown to perform significantly better after finetuning [Raffel et al., 2020]. We therefore evaluate every architecture and objective combination after multitask finetuning. \n\nOur results are outlined in Figure 4. The encoder-decoder pretrained with span corruption offers the best performance after multitask finetuning. Specifically, on EAI-Eval, the best performance is achieved by the encoder-decoder with MLM, and the non-causal decoder with MLM comes in a close second. However, the difference is more significant on T0-Eval, where the encoder-decoder with MLM pretraining outperforms other models by a large margin. Finally, encoder-decoder pretrained with PLM and causal decoder with MLM achieve significantly worse performance than other models. These results are consistent across all levels of pretraining (see early checkpoints in Appendix D). \n\nFinding 2. Encoder-decoder models trained with masked language modeling achieve the best zero-shot performance after multitask finetuning . More broadly, approaches that perform well in the single-task finetuning setting perform well on multitask finetuning. \n\nTable 3: After full or prefix language modeling pretraining, the causal decoder (FLM) exhibits the best zero-shot generalization abilities, followed closely by the non-causal decoder (PLM).\n\n## Language modeling adaptation (LM-A)\n. First, we propose to pretrain a non-causal decoder model with an MLM objective and then further train the model as a causal decoder with a FLM objective (language modeling adaptation). This conversion is simple, as the parameters and overall architecture can be kept the same, and only the attention mask needs to be switched. We note that we also attempted this adaptation from the decoder portion of an encoder-decoder model, but it performed significantly worse than training from scratch, as discussed in Appendix E.4. \n\nValidations losses are plotted in Figure 6, on the left. Starting from an MLM-pretrained non-causal decoder model speeds up convergence significantly compared to training a causal-decoder model with an FLM objective from scratch. To achieve a loss comparable to the one achieved after 168B tokens of FLM pretraining, language modeling adaptation requires only 105B additional tokens (a 1.6\u00d7 speed-up). This makes it possible to obtain both a high-quality zero-shot model and a good generative model, for only 1.6\u00d7 the cost of training a single model. A causal decoder-only pretrained with FLM from scratch (Causal FLM) compared to a model being adapted with FLM following 168B pretraining tokens as a non-causal masked language model (LMadapted Non-causal MLM). The adaptation requires 63% of the tokens (1.6\u00d7 speedup) versus training from scratch. Right: Causal and non-causal decoder-only masked language models (Causal MLM, Non-causal MLM) trained from scratch compared to a model being adapted to a non-causal MLM following 168B pretraining tokens as a causal FLM (MLM-adapted Causal FLM). The adaptation requires 30% of the tokens (3.3\u00d7 speedup) versus training the non-causal MLM from scratch. \n\nNon-causal masked language modeling adaptation (NC-A). To investigate alternative avenues for adaptation, we now introduce non-causal masked language modeling adaptation: starting from a causal decoder model pretrained with FLM as the objective, we then continue training the model as a non-causal decoder using an MLM objective.\n...\nWe find that the MLM-adapted model performs best by a significant margin and outperforms every other model we considered on EAI-Eval. Furthermore, the measured zero-shot generalization is in line with the MLM-pretrained non-causal decoder reported in Figure 4, though it still lags behind the MLM-pretrained encoder-decoder, despite the adapted models having seen 51 billion additional tokens. Finally, we note that performing non-causal multitask finetuning of the causal model produces no meaningful change in performance. \n\nFinding 3. Decoder-only models can be efficiently adapted from one architecture/objective prior to the other. Specifically, to obtain both a generative and a multitask model with the smallest total compute budget possible, we recommend starting with a causal decoder-only model, pretraining it with a full language modeling objective, and then using non-causal masked language modeling adaptation before taking it through multitask finetuning . Figure 7: Applying non-causal MLM adaptation to a causal decoder-only FLM before multitask finetuning improves zero-shot performance, even when controlling for additional LM pretraining for the same number of tokens. Zero-shot generalization on T0-Eval (left) and EAI-Eval (right), for the T5-LM and T0 baselines (grey), and for models from our study. Converting the model into a non-causal decoder for multitask finetuning only does not improve performance on T0-Eval. Results after adaptation are in line with non-causal decoder-only pretrained with MLM in Figure 4.\n\n## Conclusion\nIn this paper, we systematically studied the effects of pretraining objective and architecture choices on the zero-shot generalization abilities of large language models. Specifically, we compared language modeling and masked language modeling objectives applied to causal/non-causal decoder-only and encoder-decoder architectures. We also evaluated zero-shot performance with and without multitask finetuning. Notably, we found that the best objective and architecture is the opposite in these two settings: a causal decoder-only pretrained with full language modeling performs best if evaluated immediately after pretraining, whereas when adding a multitask finetuning step, an encoder-decoder pretrained with masked language modeling performs best. We therefore evaluate the practice of adaptation, to convert models across architectures and objectives. We found a simple efficient compromise, where a causal decoder-only model pretrained with full language modeling underwent additional masked language model training as a non-causal decoder-only model, yielding significant speedup in convergence over starting from scratch. This enables practitioners to get both an excellent generative model and a model that delivers good performance after multitask finetuning. Our results provide significant new insights into the design of LLMs. In the future, we are interested in work investigating architectures and objectives that perform well regardless of whether multitask finetuning is performed. To facilitate future work, we release all models, code, and data used in our study.\n\n## E.4 Adaptation from an encoder-decoder\nWhen studying adaptation and the conversion from one architecture to another, we also considered converting to and from encoder-decoder models. Conversion across causal and non-causal decoder-only models is straightforward, simply by switching the attention mask; for encoder-decoder, parameters have to be either pruned or added for both the entire encoder, and for the cross-attention in the decoder. Results from one of our attempt to convert an encoder-decoder into a causal decoder are reported in Figure 10. While converting across causal/non-causal decoder provides an improvement over training from scratch, this is not the case here. Validation loss when adapting an encoder-decoder pretrained with MLM to a causal decoder-only using FLM. We adapted a pretrained (for 168B tokens) encoder-decoder model to decoder-only by feeding an empty input into the encoder and causally training with a FLM objective on the decoder. We stopped this adaptation once it was clear the performance would not match that of a causal FLM trained from scratch, in contrast with the other adaptations we studied.",
            "reference_string": "[248118752 | Wang et al. | 2022 | Citations: 175]"
        },
        {
            "title": "A review on the applications of Transformer-based language models for nucleotide sequence analysis",
            "venue": "Computational and Structural Biotechnology Journal",
            "year": 2024,
            "reference_count": 100,
            "citation_count": 0,
            "influential_citation_count": 0,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://doi.org/10.1016/j.csbj.2025.03.024",
                "status": "GOLD",
                "license": "CCBYNCND",
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2412.07201, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2343636534",
                    "name": "Nimisha Ghosh"
                },
                {
                    "authorId": "2299143904",
                    "name": "Daniele Santoni"
                },
                {
                    "authorId": "2105376090",
                    "name": "I. Saha"
                },
                {
                    "authorId": "2261390733",
                    "name": "Giovanni Felici"
                }
            ],
            "abstract": null,
            "corpus_id": 277128409,
            "sentences": [
                {
                    "corpus_id": "277128409",
                    "title": "A review on the applications of Transformer-based language models for nucleotide sequence analysis",
                    "text": "The functioning of the Decoder has many parts in common with the Encoder, yet there are some major differences. In a nutshell, the Decoder processes the output component of the input-output pairs of the training data; encodes such information and produces its own embedding using a multi-head attention scheme similar to the Encoder, but: \n\n1. once the output has been encoded, it is combined with the  and  matrices coming from the Encoder. This is the step where Transformers learn the relation between the input of the training (e.g., the question) and the output (e.g., the answer to the question); considering the question-answer example, the Decoder performs cross-attention on the Encoder output (which represents the question) while processing the answer; 2. at the end of the process a linear layer has a number of output neurons equal to the size of the vocabulary; such network uses a softmax function to produce likelihood for each term in the vocabulary. Then, the term with the highest likelihood is the output of the Decoder i.e. the predicted next word; 3. The Decoder produces the output one word at a time; in the training phase, knowing the correct word, the error is computed and used to drive the backpropagation step and the correction of the weights to reduce the error. \n\nThe steps as mentioned above can be visualised in Fig. 1(c). To fully appreciate the subtleties of the Encoder-Decoder mechanism, one should be aware that Encoder and Decoder are trained simultaneously, both in pretraining and in finetuning. Both strategies depend on the model type. For example, the weights obtained in pretraining for Encoder-only model like BERT, are based on randomly masked words; on the other hand, for Decoder-only model like GPT, causal masking is used, where only the future tokens are masked, and the model predicts the next token given past tokens. In finetuning, the pretrained weights are refined when input-output training pairs are presented to the system.",
                    "score": 0.47549074833648897,
                    "section_title": "Encoding and decoding",
                    "char_start_offset": 16803,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 111
                        },
                        {
                            "start": 112,
                            "end": 338
                        },
                        {
                            "start": 341,
                            "end": 441
                        },
                        {
                            "start": 442,
                            "end": 967
                        },
                        {
                            "start": 968,
                            "end": 1293
                        },
                        {
                            "start": 1296,
                            "end": 1356
                        },
                        {
                            "start": 1357,
                            "end": 1537
                        },
                        {
                            "start": 1538,
                            "end": 1579
                        },
                        {
                            "start": 1580,
                            "end": 1872
                        },
                        {
                            "start": 1873,
                            "end": 1984
                        }
                    ],
                    "ref_mentions": [],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.83349609375
                }
            ],
            "relevance_judgement": 0.83349609375,
            "relevance_judgment_input_expanded": "# Title: A review on the applications of Transformer-based language models for nucleotide sequence analysis\n# Venue: Computational and Structural Biotechnology Journal\n# Authors: Nimisha Ghosh, Daniele Santoni, I. Saha, Giovanni Felici\n## Abstract\nNone\n## Encoding and decoding\nThe functioning of the Decoder has many parts in common with the Encoder, yet there are some major differences. In a nutshell, the Decoder processes the output component of the input-output pairs of the training data; encodes such information and produces its own embedding using a multi-head attention scheme similar to the Encoder, but: \n\n1. once the output has been encoded, it is combined with the  and  matrices coming from the Encoder. This is the step where Transformers learn the relation between the input of the training (e.g., the question) and the output (e.g., the answer to the question); considering the question-answer example, the Decoder performs cross-attention on the Encoder output (which represents the question) while processing the answer; 2. at the end of the process a linear layer has a number of output neurons equal to the size of the vocabulary; such network uses a softmax function to produce likelihood for each term in the vocabulary. Then, the term with the highest likelihood is the output of the Decoder i.e. the predicted next word; 3. The Decoder produces the output one word at a time; in the training phase, knowing the correct word, the error is computed and used to drive the backpropagation step and the correction of the weights to reduce the error. \n\nThe steps as mentioned above can be visualised in Fig. 1(c). To fully appreciate the subtleties of the Encoder-Decoder mechanism, one should be aware that Encoder and Decoder are trained simultaneously, both in pretraining and in finetuning. Both strategies depend on the model type. For example, the weights obtained in pretraining for Encoder-only model like BERT, are based on randomly masked words; on the other hand, for Decoder-only model like GPT, causal masking is used, where only the future tokens are masked, and the model predicts the next token given past tokens. In finetuning, the pretrained weights are refined when input-output training pairs are presented to the system.",
            "reference_string": "[277128409 | Ghosh et al. | 2024 | Citations: 0]"
        },
        {
            "title": "Understanding LLMs: A Comprehensive Overview from Training to Inference",
            "venue": "Neurocomputing",
            "year": 2024,
            "reference_count": 203,
            "citation_count": 74,
            "influential_citation_count": 1,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2401.02038, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2116426849",
                    "name": "Yi-Hsueh Liu"
                },
                {
                    "authorId": "2155082967",
                    "name": "Haoyang He"
                },
                {
                    "authorId": "2184719751",
                    "name": "Tianle Han"
                },
                {
                    "authorId": "2273584640",
                    "name": "Xu Zhang"
                },
                {
                    "authorId": "2210636248",
                    "name": "Mengyuan Liu"
                },
                {
                    "authorId": "2257433902",
                    "name": "Jiaming Tian"
                },
                {
                    "authorId": "2257095790",
                    "name": "Yutong Zhang"
                },
                {
                    "authorId": "2110238778",
                    "name": "Jiaqi Wang"
                },
                {
                    "authorId": "2277869261",
                    "name": "Xiaohui Gao"
                },
                {
                    "authorId": "2215167446",
                    "name": "Tianyang Zhong"
                },
                {
                    "authorId": "2221032216",
                    "name": "Yi Pan"
                },
                {
                    "authorId": "2211904452",
                    "name": "Shaochen Xu"
                },
                {
                    "authorId": "2263593041",
                    "name": "Zihao Wu"
                },
                {
                    "authorId": "2145977326",
                    "name": "Zheng Liu"
                },
                {
                    "authorId": "2257586495",
                    "name": "Xin Zhang"
                },
                {
                    "authorId": "2277750447",
                    "name": "Shu Zhang"
                },
                {
                    "authorId": "1742535",
                    "name": "Xintao Hu"
                },
                {
                    "authorId": "49104946",
                    "name": "Tuo Zhang"
                },
                {
                    "authorId": "2251076040",
                    "name": "Ning Qiang"
                },
                {
                    "authorId": "2254792886",
                    "name": "Tianming Liu"
                },
                {
                    "authorId": "2257302793",
                    "name": "Bao Ge"
                }
            ],
            "abstract": "The introduction of ChatGPT has led to a significant increase in the utilization of Large Language Models (LLMs) for addressing downstream tasks. There's an increasing focus on cost-efficient training and deployment within this context. Low-cost training and deployment of LLMs represent the future development trend. This paper reviews the evolution of large language model training techniques and inference deployment technologies aligned with this emerging trend. The discussion on training includes various aspects, including data preprocessing, training architecture, pre-training tasks, parallel training, and relevant content related to model fine-tuning. On the inference side, the paper covers topics such as model compression, parallel computation, memory scheduling, and structural optimization. It also explores LLMs' utilization and provides insights into their future development.",
            "corpus_id": 266755678,
            "sentences": [
                {
                    "corpus_id": "266755678",
                    "title": "Understanding LLMs: A Comprehensive Overview from Training to Inference",
                    "text": "LLMs with a Decoder-only architecture utilize the decoder component of the traditional Transformer architecture. Unlike the Encoder-Decoder architecture, which incorporates both an encoder and a decoder, the Decoder-only architecture is solely focused on the decoding process. In this configuration, the model sequentially generates tokens, attending to preceding tokens in the sequence. This architecture has been applied to various language generation tasks, showcasing its effectiveness in various tasks such as text generation without the need for an explicit encoding phase. The Decoder-only architecture can be further classified into two categories: the Causal Decoder architecture and the Prefix Decoder architecture. \n\nThe Causal Decoder Architecture: In the Causal Decoder architecture, each token in the model input sequence can only attend to past input tokens and itself during the decoding process. It achieves unidirectional attention to the input sequence by using a specific mask as shown in Figure 1. In fact, different architectures are mainly implemented by configuring different mask matrices. The figure illustrates a comparison of mask configurations between the Encoderdecoder and Decoder-only architectures (including Casual Decoder and Prefix Decoder). The representative LLMs for the Causal Decoder architecture are the GPT series [18; 7; 8; 93; 19]. The GPT series of LLMs are currently known for their superior performance, with their foundational Causal Decoder architecture widely applied in other LLMs such as BLOOM [38], OPT [83], Gopher [84], and LLaMA [9]. \n\nThe Prefix Decoder Architecture: The Prefix Decoder architecture combines the advantages of both the Encoderdecoder and Causal Decoder architectures. It leverages its unique mask configurations, as illustrated in Figure 1, enabling bidirectional attention for tokens in the prefix while maintaining unidirectional attention for generating subsequent tokens [54]. This design allows for the autoregressive generation of the output sequence with the flexibility to attend bi-directionally to the prefix tokens. Representative LLMs utilizing the Prefix Decoder architecture include PaLM [36] and GLM [37].",
                    "score": 0.5546775486231706,
                    "section_title": "Decoder-only Architecture",
                    "char_start_offset": 35165,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 112
                        },
                        {
                            "start": 113,
                            "end": 276
                        },
                        {
                            "start": 277,
                            "end": 387
                        },
                        {
                            "start": 388,
                            "end": 579
                        },
                        {
                            "start": 580,
                            "end": 725
                        },
                        {
                            "start": 728,
                            "end": 912
                        },
                        {
                            "start": 913,
                            "end": 1018
                        },
                        {
                            "start": 1019,
                            "end": 1114
                        },
                        {
                            "start": 1115,
                            "end": 1278
                        },
                        {
                            "start": 1279,
                            "end": 1377
                        },
                        {
                            "start": 1378,
                            "end": 1591
                        },
                        {
                            "start": 1594,
                            "end": 1743
                        },
                        {
                            "start": 1744,
                            "end": 1956
                        },
                        {
                            "start": 1957,
                            "end": 2102
                        },
                        {
                            "start": 2103,
                            "end": 2196
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 1951,
                            "end": 1955,
                            "matchedPaperCorpusId": "147704286"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.8115234375
                }
            ],
            "relevance_judgement": 0.8115234375,
            "relevance_judgment_input_expanded": "# Title: Understanding LLMs: A Comprehensive Overview from Training to Inference\n# Venue: Neurocomputing\n# Authors: Yi-Hsueh Liu, Haoyang He, Tianle Han, Xu Zhang, Mengyuan Liu, Jiaming Tian, Yutong Zhang, Jiaqi Wang, Xiaohui Gao, Tianyang Zhong, Yi Pan, Shaochen Xu, Zihao Wu, Zheng Liu, Xin Zhang, Shu Zhang, Xintao Hu, Tuo Zhang, Ning Qiang, Tianming Liu, Bao Ge\n## Abstract\nThe introduction of ChatGPT has led to a significant increase in the utilization of Large Language Models (LLMs) for addressing downstream tasks. There's an increasing focus on cost-efficient training and deployment within this context. Low-cost training and deployment of LLMs represent the future development trend. This paper reviews the evolution of large language model training techniques and inference deployment technologies aligned with this emerging trend. The discussion on training includes various aspects, including data preprocessing, training architecture, pre-training tasks, parallel training, and relevant content related to model fine-tuning. On the inference side, the paper covers topics such as model compression, parallel computation, memory scheduling, and structural optimization. It also explores LLMs' utilization and provides insights into their future development.\n## Decoder-only Architecture\nLLMs with a Decoder-only architecture utilize the decoder component of the traditional Transformer architecture. Unlike the Encoder-Decoder architecture, which incorporates both an encoder and a decoder, the Decoder-only architecture is solely focused on the decoding process. In this configuration, the model sequentially generates tokens, attending to preceding tokens in the sequence. This architecture has been applied to various language generation tasks, showcasing its effectiveness in various tasks such as text generation without the need for an explicit encoding phase. The Decoder-only architecture can be further classified into two categories: the Causal Decoder architecture and the Prefix Decoder architecture. \n\nThe Causal Decoder Architecture: In the Causal Decoder architecture, each token in the model input sequence can only attend to past input tokens and itself during the decoding process. It achieves unidirectional attention to the input sequence by using a specific mask as shown in Figure 1. In fact, different architectures are mainly implemented by configuring different mask matrices. The figure illustrates a comparison of mask configurations between the Encoderdecoder and Decoder-only architectures (including Casual Decoder and Prefix Decoder). The representative LLMs for the Causal Decoder architecture are the GPT series [18; 7; 8; 93; 19]. The GPT series of LLMs are currently known for their superior performance, with their foundational Causal Decoder architecture widely applied in other LLMs such as BLOOM [38], OPT [83], Gopher [84], and LLaMA [9]. \n\nThe Prefix Decoder Architecture: The Prefix Decoder architecture combines the advantages of both the Encoderdecoder and Causal Decoder architectures. It leverages its unique mask configurations, as illustrated in Figure 1, enabling bidirectional attention for tokens in the prefix while maintaining unidirectional attention for generating subsequent tokens [54]. This design allows for the autoregressive generation of the output sequence with the flexibility to attend bi-directionally to the prefix tokens. Representative LLMs utilizing the Prefix Decoder architecture include PaLM [36] and GLM [37].",
            "reference_string": "[266755678 | Liu et al. | 2024 | Citations: 74]"
        },
        {
            "title": "Integrating Structural and Semantic Signals in Text-Attributed Graphs with BiGTex",
            "venue": "arXiv.org",
            "year": 2025,
            "reference_count": 46,
            "citation_count": 0,
            "influential_citation_count": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2504.12474, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "66841167",
                    "name": "Azadeh Beiranvand"
                },
                {
                    "authorId": "35409259",
                    "name": "S. M. Vahidipour"
                }
            ],
            "abstract": "Text-attributed graphs (TAGs) present unique challenges in representation learning by requiring models to capture both the semantic richness of node-associated texts and the structural dependencies of the graph. While graph neural networks (GNNs) excel at modeling topological information, they lack the capacity to process unstructured text. Conversely, large language models (LLMs) are proficient in text understanding but are typically unaware of graph structure. In this work, we propose BiGTex (Bidirectional Graph Text), a novel architecture that tightly integrates GNNs and LLMs through stacked Graph-Text Fusion Units. Each unit allows for mutual attention between textual and structural representations, enabling information to flow in both directions, text influencing structure and structure guiding textual interpretation. The proposed architecture is trained using parameter-efficient fine-tuning (LoRA), keeping the LLM frozen while adapting to task-specific signals. Extensive experiments on five benchmark datasets demonstrate that BiGTex achieves state-of-the-art performance in node classification and generalizes effectively to link prediction. An ablation study further highlights the importance of soft prompting and bi-directional attention in the model's success.",
            "corpus_id": 277857043,
            "sentences": [
                {
                    "corpus_id": "277857043",
                    "title": "Integrating Structural and Semantic Signals in Text-Attributed Graphs with BiGTex",
                    "text": "Continuing from the architectural foundations of large language models, it is important to distinguish between two major classes of transformer-based designs: encoder-only and decoder-only models. Each follows a unique training paradigm and serves different purposes in natural language understanding or generation tasks. \n\nEncoder-only models, such as BERT [18], are designed to generate holistic representations of input sequences. These models take an entire sequence as input and process it bidirectionally-each token has access to the full left and right context during training. This characteristic allows the model to deeply capture semantic dependencies across the input. \n\nThe standard training objective for such models is Masked Language Modelling (MLM), where a random subset of tokens is replaced with a special [MASK] token. The model is then trained to reconstruct the masked tokens using the surrounding unmasked tokens. This approach encourages the network to develop contextualized embeddings grounded in full-sequence comprehension. The loss function for MLM is typically expressed as: \n\nwhere \\M denotes the set of masked positions, x \\M is the unmasked sequence, and \u03b8 represents model parameters. \n\nIn contrast, decoder-only models, such as GPT [19], operate unidirectionally. They are trained in an autoregressive fashion, where each token is generated based only on preceding tokens. The model has no access to future inputs during training or inference, enforcing a strict left-to-right dependency. This makes them particularly well-suited for generative tasks such as open-ended text generation or dialogue modelling. \n\nThe pretraining objective for decoder-only models is Causal Language Modelling (CLM), where the model learns to predict the next token in a sequence, given all previous ones. The associated loss is defined as: \n\nHere, each token x t is conditioned solely on the sequence x <t , making the model capable of generating coherent text step by step. \n\nThe distinction between encoder-only and decoder-only models is foundational: the former are optimized for understanding input context and are widely used in classification and embedding tasks, while the latter are tailored for sequential prediction and text generation.",
                    "score": 0.5254484805332319,
                    "section_title": "Large language Models (LLMs) and Attention Mechanism",
                    "char_start_offset": 11813,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 196
                        },
                        {
                            "start": 197,
                            "end": 321
                        },
                        {
                            "start": 324,
                            "end": 433
                        },
                        {
                            "start": 434,
                            "end": 584
                        },
                        {
                            "start": 585,
                            "end": 679
                        },
                        {
                            "start": 682,
                            "end": 838
                        },
                        {
                            "start": 839,
                            "end": 936
                        },
                        {
                            "start": 937,
                            "end": 1051
                        },
                        {
                            "start": 1052,
                            "end": 1104
                        },
                        {
                            "start": 1107,
                            "end": 1218
                        },
                        {
                            "start": 1221,
                            "end": 1298
                        },
                        {
                            "start": 1299,
                            "end": 1407
                        },
                        {
                            "start": 1408,
                            "end": 1523
                        },
                        {
                            "start": 1524,
                            "end": 1643
                        },
                        {
                            "start": 1646,
                            "end": 1820
                        },
                        {
                            "start": 1821,
                            "end": 1855
                        },
                        {
                            "start": 1858,
                            "end": 1990
                        },
                        {
                            "start": 1993,
                            "end": 2263
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 1267,
                            "end": 1271,
                            "matchedPaperCorpusId": "218971783"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.810546875
                }
            ],
            "relevance_judgement": 0.810546875,
            "relevance_judgment_input_expanded": "# Title: Integrating Structural and Semantic Signals in Text-Attributed Graphs with BiGTex\n# Venue: arXiv.org\n# Authors: Azadeh Beiranvand, S. M. Vahidipour\n## Abstract\nText-attributed graphs (TAGs) present unique challenges in representation learning by requiring models to capture both the semantic richness of node-associated texts and the structural dependencies of the graph. While graph neural networks (GNNs) excel at modeling topological information, they lack the capacity to process unstructured text. Conversely, large language models (LLMs) are proficient in text understanding but are typically unaware of graph structure. In this work, we propose BiGTex (Bidirectional Graph Text), a novel architecture that tightly integrates GNNs and LLMs through stacked Graph-Text Fusion Units. Each unit allows for mutual attention between textual and structural representations, enabling information to flow in both directions, text influencing structure and structure guiding textual interpretation. The proposed architecture is trained using parameter-efficient fine-tuning (LoRA), keeping the LLM frozen while adapting to task-specific signals. Extensive experiments on five benchmark datasets demonstrate that BiGTex achieves state-of-the-art performance in node classification and generalizes effectively to link prediction. An ablation study further highlights the importance of soft prompting and bi-directional attention in the model's success.\n## Large language Models (LLMs) and Attention Mechanism\nContinuing from the architectural foundations of large language models, it is important to distinguish between two major classes of transformer-based designs: encoder-only and decoder-only models. Each follows a unique training paradigm and serves different purposes in natural language understanding or generation tasks. \n\nEncoder-only models, such as BERT [18], are designed to generate holistic representations of input sequences. These models take an entire sequence as input and process it bidirectionally-each token has access to the full left and right context during training. This characteristic allows the model to deeply capture semantic dependencies across the input. \n\nThe standard training objective for such models is Masked Language Modelling (MLM), where a random subset of tokens is replaced with a special [MASK] token. The model is then trained to reconstruct the masked tokens using the surrounding unmasked tokens. This approach encourages the network to develop contextualized embeddings grounded in full-sequence comprehension. The loss function for MLM is typically expressed as: \n\nwhere \\M denotes the set of masked positions, x \\M is the unmasked sequence, and \u03b8 represents model parameters. \n\nIn contrast, decoder-only models, such as GPT [19], operate unidirectionally. They are trained in an autoregressive fashion, where each token is generated based only on preceding tokens. The model has no access to future inputs during training or inference, enforcing a strict left-to-right dependency. This makes them particularly well-suited for generative tasks such as open-ended text generation or dialogue modelling. \n\nThe pretraining objective for decoder-only models is Causal Language Modelling (CLM), where the model learns to predict the next token in a sequence, given all previous ones. The associated loss is defined as: \n\nHere, each token x t is conditioned solely on the sequence x <t , making the model capable of generating coherent text step by step. \n\nThe distinction between encoder-only and decoder-only models is foundational: the former are optimized for understanding input context and are widely used in classification and embedding tasks, while the latter are tailored for sequential prediction and text generation.",
            "reference_string": "[277857043 | Beiranvand et al. | 2025 | Citations: 0]"
        },
        {
            "title": "Efficient Transformers: A Survey",
            "venue": "ACM Computing Surveys",
            "year": 2020,
            "reference_count": 105,
            "citation_count": 1128,
            "influential_citation_count": 80,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://dl.acm.org/doi/pdf/10.1145/3530811",
                "status": "BRONZE",
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2009.06732, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "144447820",
                    "name": "Yi Tay"
                },
                {
                    "authorId": "3226635",
                    "name": "Mostafa Dehghani"
                },
                {
                    "authorId": "11774695",
                    "name": "Dara Bahri"
                },
                {
                    "authorId": "1680617",
                    "name": "Donald Metzler"
                }
            ],
            "abstract": "Transformer model architectures have garnered immense interest lately due to their effectiveness across a range of domains like language, vision, and reinforcement learning. In the field of natural language processing for example, Transformers have become an indispensable staple in the modern deep learning stack. Recently, a dizzying number of \u201cX-former\u201d models have been proposed\u2014Reformer, Linformer, Performer, Longformer, to name a few\u2014which improve upon the original Transformer architecture, many of which make improvements around computational and memory efficiency. With the aim of helping the avid researcher navigate this flurry, this article characterizes a large and thoughtful selection of recent efficiency-flavored \u201cX-former\u201d models, providing an organized and comprehensive overview of existing work and models across multiple domains.",
            "corpus_id": 221702858,
            "sentences": [
                {
                    "corpus_id": "221702858",
                    "title": "Efficient Transformers: A Survey",
                    "text": "It is important to note the differences in how the Transformer blocks are used. Transformers can primarily be used in three ways, namely: (1) encoder-only (e.g., for classification), (2) decoder-only (e.g., for language modeling), and (3) encoder-decoder (e.g., for machine translation). In encoder-decoder mode, there are usually multiple multi-headed self-attention modules, including a standard self-attention in both the encoder and the decoder, along with an encoder-decoder cross-attention that allows the decoder to utilize information from the encoder. This influences the design of the self-attention mechanism. In the encoder mode, there is no restriction or constraint that the self-attention mechanism has to be causal, i.e., dependent solely on the present and past tokens. In the encoder-decoder setting, self-attention used in the decoder (i.e. across decoding positions) must be causal since each auto-regressive decoding step can only depend on previous tokens, whereas the selfattention used in the encoder need not. Fulfilling this requirement can prove challenging for many efficient self-attention designs. \n\nThe mode of usage of a Transformer model generally depends on the target application. Given an input sequence, the sequence is typically passed through an encoder stack. At this stage, there might be too options. For multi-class classification, a linear layer with Softmax outputs typically projects the sequence representation down to the number of classes. In the case of BERT (Devlin et al., 2018), this is a [CLS] token that is appended to the start of the sequence as a prefix. Recent work has also explored the usage of Encoder-Decoder architectures for classification, such as T5 (Raffel et al., 2019). Decoder-only models are typically used for generation and are trained using a language modeling objective (of predicting the next token). Due to the nature of the loss, these models are often superior for open ended generation (Brown et al., 2020).",
                    "score": 0.5169425056444182,
                    "section_title": "Transformer Mode",
                    "char_start_offset": 9721,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 79
                        },
                        {
                            "start": 80,
                            "end": 287
                        },
                        {
                            "start": 288,
                            "end": 560
                        },
                        {
                            "start": 561,
                            "end": 620
                        },
                        {
                            "start": 621,
                            "end": 786
                        },
                        {
                            "start": 787,
                            "end": 859
                        },
                        {
                            "start": 860,
                            "end": 1034
                        },
                        {
                            "start": 1035,
                            "end": 1127
                        },
                        {
                            "start": 1130,
                            "end": 1215
                        },
                        {
                            "start": 1216,
                            "end": 1299
                        },
                        {
                            "start": 1300,
                            "end": 1342
                        },
                        {
                            "start": 1343,
                            "end": 1488
                        },
                        {
                            "start": 1489,
                            "end": 1612
                        },
                        {
                            "start": 1613,
                            "end": 1739
                        },
                        {
                            "start": 1740,
                            "end": 1877
                        },
                        {
                            "start": 1878,
                            "end": 1988
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 1509,
                            "end": 1530,
                            "matchedPaperCorpusId": "52967399"
                        },
                        {
                            "start": 1717,
                            "end": 1738,
                            "matchedPaperCorpusId": "204838007"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.806640625
                }
            ],
            "relevance_judgement": 0.806640625,
            "relevance_judgment_input_expanded": "# Title: Efficient Transformers: A Survey\n# Venue: ACM Computing Surveys\n# Authors: Yi Tay, Mostafa Dehghani, Dara Bahri, Donald Metzler\n## Abstract\nTransformer model architectures have garnered immense interest lately due to their effectiveness across a range of domains like language, vision, and reinforcement learning. In the field of natural language processing for example, Transformers have become an indispensable staple in the modern deep learning stack. Recently, a dizzying number of \u201cX-former\u201d models have been proposed\u2014Reformer, Linformer, Performer, Longformer, to name a few\u2014which improve upon the original Transformer architecture, many of which make improvements around computational and memory efficiency. With the aim of helping the avid researcher navigate this flurry, this article characterizes a large and thoughtful selection of recent efficiency-flavored \u201cX-former\u201d models, providing an organized and comprehensive overview of existing work and models across multiple domains.\n## Transformer Mode\nIt is important to note the differences in how the Transformer blocks are used. Transformers can primarily be used in three ways, namely: (1) encoder-only (e.g., for classification), (2) decoder-only (e.g., for language modeling), and (3) encoder-decoder (e.g., for machine translation). In encoder-decoder mode, there are usually multiple multi-headed self-attention modules, including a standard self-attention in both the encoder and the decoder, along with an encoder-decoder cross-attention that allows the decoder to utilize information from the encoder. This influences the design of the self-attention mechanism. In the encoder mode, there is no restriction or constraint that the self-attention mechanism has to be causal, i.e., dependent solely on the present and past tokens. In the encoder-decoder setting, self-attention used in the decoder (i.e. across decoding positions) must be causal since each auto-regressive decoding step can only depend on previous tokens, whereas the selfattention used in the encoder need not. Fulfilling this requirement can prove challenging for many efficient self-attention designs. \n\nThe mode of usage of a Transformer model generally depends on the target application. Given an input sequence, the sequence is typically passed through an encoder stack. At this stage, there might be too options. For multi-class classification, a linear layer with Softmax outputs typically projects the sequence representation down to the number of classes. In the case of BERT (Devlin et al., 2018), this is a [CLS] token that is appended to the start of the sequence as a prefix. Recent work has also explored the usage of Encoder-Decoder architectures for classification, such as T5 (Raffel et al., 2019). Decoder-only models are typically used for generation and are trained using a language modeling objective (of predicting the next token). Due to the nature of the loss, these models are often superior for open ended generation (Brown et al., 2020).",
            "reference_string": "[221702858 | Tay et al. | 2020 | Citations: 1128]"
        },
        {
            "title": "How Powerful are Decoder-Only Transformer Neural Models?",
            "venue": "IEEE International Joint Conference on Neural Network",
            "year": 2023,
            "reference_count": 21,
            "citation_count": 19,
            "influential_citation_count": 0,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://arxiv.org/pdf/2305.17026",
                "status": "GREEN",
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2305.17026, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2115904887",
                    "name": "Jesse Roberts"
                }
            ],
            "abstract": "In this article we prove that the general transformer neural model undergirding modern large language models (LLMs) is Turing complete under reasonable assumptions. This is the first work to directly address the Turing completeness of the underlying technology employed in GPT-x as past work has focused on the more expressive, full auto-encoder transformer architecture. From this theoretical analysis, we show that the sparsity/compressibility of the word embedding is an important consideration for Turing completeness to hold. We also show that Transformers are a variant of B machines studied by Hao Wang.",
            "corpus_id": 258947629,
            "sentences": [
                {
                    "corpus_id": "258947629",
                    "title": "How Powerful are Decoder-Only Transformer Neural Models?",
                    "text": "To create a decoder-only model, the vanilla architecture is modified in two ways. First, the connection to the encoder is removed. Second, the cross-attention which allows the decoder to conditionally attend to the encoder output at each layer of the decoder is eliminated. These, along with the entire encoder, are surrounded by a dashed yellow line in 1 to visualize what is eliminated. As mentioned previously, this superficially suggests that encoder-only and decoder-only architectures are identical as seen in 2. \n\n2) Differentiating Encoder-only and Decoder-only Models: Decoder-only models have three necessary characteristics which are derived from their function in the vanilla transformer. The decoder must (1) provide a means of autoregressively predicting the next token based on the tokens generated so far given the encoder input as contextualization. In 2 this is shown as the recursive red connection mapping the output vector back into the last element of the input sequence of vectors. To be suited to this task, decoder-only models must (2) not see future values when evaluating a query on the input sequence of vectors. This is why decoder-only models are often referred to as causal language models (CLM). In 2, we refer to the decoder attention heads as causal attention heads rather than masked attention heads as they are called in [1]. The model must be (3) trained to predict the next token given the current input sequence of vectors. This training method coupled with recursion allows decoder-only models to autoregressively generate arbitrarily long (up to the max size of the input vector sequence) sequences. \n\nIf any of the above are violated, the model can't be reasonably considered a decoder-only model as it is no longer capable of auto-regressive next token prediction.",
                    "score": 0.6859072080984483,
                    "section_title": "1) Modifying the Vanilla Transformer to form a Decoderonly Model:",
                    "char_start_offset": 3733,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 81
                        },
                        {
                            "start": 82,
                            "end": 130
                        },
                        {
                            "start": 131,
                            "end": 273
                        },
                        {
                            "start": 274,
                            "end": 388
                        },
                        {
                            "start": 389,
                            "end": 518
                        },
                        {
                            "start": 521,
                            "end": 700
                        },
                        {
                            "start": 701,
                            "end": 866
                        },
                        {
                            "start": 867,
                            "end": 1004
                        },
                        {
                            "start": 1005,
                            "end": 1140
                        },
                        {
                            "start": 1141,
                            "end": 1227
                        },
                        {
                            "start": 1228,
                            "end": 1361
                        },
                        {
                            "start": 1362,
                            "end": 1462
                        },
                        {
                            "start": 1463,
                            "end": 1640
                        },
                        {
                            "start": 1643,
                            "end": 1807
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 1357,
                            "end": 1360,
                            "matchedPaperCorpusId": "13756489"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.8037109375
                }
            ],
            "relevance_judgement": 0.8037109375,
            "relevance_judgment_input_expanded": "# Title: How Powerful are Decoder-Only Transformer Neural Models?\n# Venue: IEEE International Joint Conference on Neural Network\n# Authors: Jesse Roberts\n## Abstract\nIn this article we prove that the general transformer neural model undergirding modern large language models (LLMs) is Turing complete under reasonable assumptions. This is the first work to directly address the Turing completeness of the underlying technology employed in GPT-x as past work has focused on the more expressive, full auto-encoder transformer architecture. From this theoretical analysis, we show that the sparsity/compressibility of the word embedding is an important consideration for Turing completeness to hold. We also show that Transformers are a variant of B machines studied by Hao Wang.\n## 1) Modifying the Vanilla Transformer to form a Decoderonly Model:\nTo create a decoder-only model, the vanilla architecture is modified in two ways. First, the connection to the encoder is removed. Second, the cross-attention which allows the decoder to conditionally attend to the encoder output at each layer of the decoder is eliminated. These, along with the entire encoder, are surrounded by a dashed yellow line in 1 to visualize what is eliminated. As mentioned previously, this superficially suggests that encoder-only and decoder-only architectures are identical as seen in 2. \n\n2) Differentiating Encoder-only and Decoder-only Models: Decoder-only models have three necessary characteristics which are derived from their function in the vanilla transformer. The decoder must (1) provide a means of autoregressively predicting the next token based on the tokens generated so far given the encoder input as contextualization. In 2 this is shown as the recursive red connection mapping the output vector back into the last element of the input sequence of vectors. To be suited to this task, decoder-only models must (2) not see future values when evaluating a query on the input sequence of vectors. This is why decoder-only models are often referred to as causal language models (CLM). In 2, we refer to the decoder attention heads as causal attention heads rather than masked attention heads as they are called in [1]. The model must be (3) trained to predict the next token given the current input sequence of vectors. This training method coupled with recursion allows decoder-only models to autoregressively generate arbitrarily long (up to the max size of the input vector sequence) sequences. \n\nIf any of the above are violated, the model can't be reasonably considered a decoder-only model as it is no longer capable of auto-regressive next token prediction.",
            "reference_string": "[258947629 | Roberts | 2023 | Citations: 19]"
        },
        {
            "title": "Encoder-Decoder Gemma: Improving the Quality-Efficiency Trade-Off via Adaptation",
            "venue": "arXiv.org",
            "year": 2025,
            "reference_count": 54,
            "citation_count": 0,
            "influential_citation_count": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2504.06225, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2354284757",
                    "name": "Biao Zhang"
                },
                {
                    "authorId": "2165469946",
                    "name": "Fedor Moiseev"
                },
                {
                    "authorId": "2343748926",
                    "name": "Joshua Ainslie"
                },
                {
                    "authorId": "1658871094",
                    "name": "P. Suganthan"
                },
                {
                    "authorId": "2352024723",
                    "name": "Min Ma"
                },
                {
                    "authorId": "9692128",
                    "name": "Surya Bhupatiraju"
                },
                {
                    "authorId": "2275184616",
                    "name": "Federico Lebron"
                },
                {
                    "authorId": "2273534960",
                    "name": "Orhan Firat"
                },
                {
                    "authorId": "2319608",
                    "name": "Armand Joulin"
                },
                {
                    "authorId": "2349772191",
                    "name": "Zhe Dong"
                }
            ],
            "abstract": "While decoder-only large language models (LLMs) have shown impressive results, encoder-decoder models are still widely adopted in real-world applications for their inference efficiency and richer encoder representation. In this paper, we study a novel problem: adapting pretrained decoder-only LLMs to encoder-decoder, with the goal of leveraging the strengths of both approaches to achieve a more favorable quality-efficiency trade-off. We argue that adaptation not only enables inheriting the capability of decoder-only LLMs but also reduces the demand for computation compared to pretraining from scratch. We rigorously explore different pretraining objectives and parameter initialization/optimization techniques. Through extensive experiments based on Gemma 2 (2B and 9B) and a suite of newly pretrained mT5-sized models (up to 1.6B), we demonstrate the effectiveness of adaptation and the advantage of encoder-decoder LLMs. Under similar inference budget, encoder-decoder LLMs achieve comparable (often better) pretraining performance but substantially better finetuning performance than their decoder-only counterpart. For example, Gemma 2B-2B outperforms Gemma 2B by $\\sim$7\\% after instruction tuning. Encoder-decoder adaptation also allows for flexible combination of different-sized models, where Gemma 9B-2B significantly surpasses Gemma 2B-2B by $>$3\\%. The adapted encoder representation also yields better results on SuperGLUE. We will release our checkpoints to facilitate future research.",
            "corpus_id": 277626724,
            "sentences": [
                {
                    "corpus_id": "277626724",
                    "title": "Encoder-Decoder Gemma: Improving the Quality-Efficiency Trade-Off via Adaptation",
                    "text": "Decoder-only pretraining often adopts causal language modeling on a single sequence. In contrast, encoder-decoder adaptation requires separate input and target sequences to be fed to the encoder and decoder separately. We explore two classical pretraining objectives for encoder-decoder modeling: prefix language modeling (PrefixLM) and UL2 (Tay et al., 2022;Wang et al., 2022). \n\nPrefixLM behaves similar to causal language modeling except for its prefix condition. To simplify the preprocessing, we split a sequence equally into two halves, the first half used as input and the second one as target. This also eases the adoption of knowledge distillation from decoder-only models. UL2 is more complicated. It is composed of several denoising tasks at different levels of complexity. We prepare UL2 data following Tay et al. (2022). We compare their performance in experiments.",
                    "score": 0.702513057489843,
                    "section_title": "Pretraining Objective",
                    "char_start_offset": 9025,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 84
                        },
                        {
                            "start": 85,
                            "end": 218
                        },
                        {
                            "start": 219,
                            "end": 378
                        },
                        {
                            "start": 381,
                            "end": 466
                        },
                        {
                            "start": 467,
                            "end": 601
                        },
                        {
                            "start": 602,
                            "end": 682
                        },
                        {
                            "start": 683,
                            "end": 707
                        },
                        {
                            "start": 708,
                            "end": 784
                        },
                        {
                            "start": 785,
                            "end": 833
                        },
                        {
                            "start": 834,
                            "end": 878
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 341,
                            "end": 359,
                            "matchedPaperCorpusId": "252780443"
                        },
                        {
                            "start": 359,
                            "end": 377,
                            "matchedPaperCorpusId": "248118752"
                        },
                        {
                            "start": 815,
                            "end": 832,
                            "matchedPaperCorpusId": "252780443"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.8017578125
                }
            ],
            "relevance_judgement": 0.8017578125,
            "relevance_judgment_input_expanded": "# Title: Encoder-Decoder Gemma: Improving the Quality-Efficiency Trade-Off via Adaptation\n# Venue: arXiv.org\n# Authors: Biao Zhang, Fedor Moiseev, Joshua Ainslie, P. Suganthan, Min Ma, Surya Bhupatiraju, Federico Lebron, Orhan Firat, Armand Joulin, Zhe Dong\n## Abstract\nWhile decoder-only large language models (LLMs) have shown impressive results, encoder-decoder models are still widely adopted in real-world applications for their inference efficiency and richer encoder representation. In this paper, we study a novel problem: adapting pretrained decoder-only LLMs to encoder-decoder, with the goal of leveraging the strengths of both approaches to achieve a more favorable quality-efficiency trade-off. We argue that adaptation not only enables inheriting the capability of decoder-only LLMs but also reduces the demand for computation compared to pretraining from scratch. We rigorously explore different pretraining objectives and parameter initialization/optimization techniques. Through extensive experiments based on Gemma 2 (2B and 9B) and a suite of newly pretrained mT5-sized models (up to 1.6B), we demonstrate the effectiveness of adaptation and the advantage of encoder-decoder LLMs. Under similar inference budget, encoder-decoder LLMs achieve comparable (often better) pretraining performance but substantially better finetuning performance than their decoder-only counterpart. For example, Gemma 2B-2B outperforms Gemma 2B by $\\sim$7\\% after instruction tuning. Encoder-decoder adaptation also allows for flexible combination of different-sized models, where Gemma 9B-2B significantly surpasses Gemma 2B-2B by $>$3\\%. The adapted encoder representation also yields better results on SuperGLUE. We will release our checkpoints to facilitate future research.\n## Pretraining Objective\nDecoder-only pretraining often adopts causal language modeling on a single sequence. In contrast, encoder-decoder adaptation requires separate input and target sequences to be fed to the encoder and decoder separately. We explore two classical pretraining objectives for encoder-decoder modeling: prefix language modeling (PrefixLM) and UL2 (Tay et al., 2022;Wang et al., 2022). \n\nPrefixLM behaves similar to causal language modeling except for its prefix condition. To simplify the preprocessing, we split a sequence equally into two halves, the first half used as input and the second one as target. This also eases the adoption of knowledge distillation from decoder-only models. UL2 is more complicated. It is composed of several denoising tasks at different levels of complexity. We prepare UL2 data following Tay et al. (2022). We compare their performance in experiments.",
            "reference_string": "[277626724 | Zhang et al. | 2025 | Citations: 0]"
        },
        {
            "title": "LLM for SoC Security: A Paradigm Shift",
            "venue": "IEEE Access",
            "year": 2023,
            "reference_count": 242,
            "citation_count": 54,
            "influential_citation_count": 3,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2310.06046, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2256992493",
                    "name": "Dipayan Saha"
                },
                {
                    "authorId": "2114625129",
                    "name": "Shams Tarek"
                },
                {
                    "authorId": "2256991081",
                    "name": "Katayoon Yahyaei"
                },
                {
                    "authorId": "2231854143",
                    "name": "Sujan Kumar Saha"
                },
                {
                    "authorId": "2257235852",
                    "name": "Jingbo Zhou"
                },
                {
                    "authorId": "145954982",
                    "name": "M. Tehranipoor"
                },
                {
                    "authorId": "1997019",
                    "name": "Farimah Farahmandi"
                }
            ],
            "abstract": "As the ubiquity and complexity of system-on-chip (SoC) designs increase across electronic devices, incorporating security into an SoC design flow poses significant challenges. Existing security solutions are inadequate to effectively verify modern SoC designs due to their limitations in scalability, comprehensiveness, and adaptability. On the other hand, large language models (LLMs) are celebrated for their remarkable success in language understanding, advanced reasoning, and program synthesis tasks. Recognizing an opportunity, our research explores leveraging the emergent capabilities of generative pre-trained transformers (GPTs) to address the existing gaps in SoC security, aiming for a more efficient, scalable, and adaptable methodology. By integrating LLMs into the SoC security verification paradigm, we open a new frontier of possibilities and challenges to ensure the security of increasingly complex SoCs. This paper offers an in-depth analysis of existing works, presents practical case studies, and demonstrates comprehensive experiments. We also present the achievements, prospects, and challenges of employing LLM in different SoC security verification tasks.",
            "corpus_id": 263829839,
            "sentences": [
                {
                    "corpus_id": "263829839",
                    "title": "LLM for SoC Security: A Paradigm Shift",
                    "text": "The encoder component transforms input tokens into vectors using embeddings and positional encodings, then applies multihead self-attention and feedforward networks. The decoder, starting similarly, incorporates masked self-attention and crossattention with the output of the encoder, ensuring alignment and preventing future word prediction. Sequence-to-sequence models often use this architecture, where the input and output sequences can be of different lengths. These models shine in tasks where there is a direct and complex transformation between inputs and outputs. Examples like machine translation and text summarization are prototypical, as they require the model to understand the input deeply and generate a coherent and contextually accurate output. BART [5], T5 [137], and UL2 [138] are a few well-known encoder-decoder models to be named. \n\nIn Context of SoC Security: The encoder-decoder architecture, renowned for its ability in natural language understanding tasks, exhibits versatility in SoC security. Its twostage process of encoding the input data and then decoding it to produce an output makes it suitable for tasks that require both comprehension and generation. This model is particularly adept at vulnerability mitigation, where understanding the context (encoder) and generating a solution (decoder) are both crucial. However, while it is also a good fit for tasks like vulnerability insertion, security verification, and assessment, it might not always be the optimal choice when the task leans heavily toward either comprehension or generation b) Decoder-Only: Decoder-only LLMs have established impressive benchmarks in numerous NLP tasks, especially in the generation of free-form text. In a decoder-only model, a sequence is fed into the model, which then directly predicts the next token or word in the sequence. It operates autoregressively, using its generated tokens as context for subsequent predictions. It has two variants: causal decoder and prefix decoder. In a standard decoder (causal decoder), the unidirectional attention masking ensures that a token attends only to previous tokens and itself. Prefix decoders permit bidirectional attention over prefix tokens while maintaining unidirectional attention to generated tokens.",
                    "score": 0.4421994543458883,
                    "section_title": "A. Preliminaries",
                    "char_start_offset": 37836,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 165
                        },
                        {
                            "start": 166,
                            "end": 342
                        },
                        {
                            "start": 343,
                            "end": 465
                        },
                        {
                            "start": 466,
                            "end": 572
                        },
                        {
                            "start": 573,
                            "end": 762
                        },
                        {
                            "start": 763,
                            "end": 853
                        },
                        {
                            "start": 856,
                            "end": 1021
                        },
                        {
                            "start": 1022,
                            "end": 1187
                        },
                        {
                            "start": 1188,
                            "end": 1345
                        },
                        {
                            "start": 1346,
                            "end": 1718
                        },
                        {
                            "start": 1719,
                            "end": 1846
                        },
                        {
                            "start": 1847,
                            "end": 1942
                        },
                        {
                            "start": 1943,
                            "end": 1998
                        },
                        {
                            "start": 1999,
                            "end": 2140
                        },
                        {
                            "start": 2141,
                            "end": 2270
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 776,
                            "end": 781,
                            "matchedPaperCorpusId": "204838007"
                        },
                        {
                            "start": 791,
                            "end": 796,
                            "matchedPaperCorpusId": "252780443"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.79833984375
                }
            ],
            "relevance_judgement": 0.79833984375,
            "relevance_judgment_input_expanded": "# Title: LLM for SoC Security: A Paradigm Shift\n# Venue: IEEE Access\n# Authors: Dipayan Saha, Shams Tarek, Katayoon Yahyaei, Sujan Kumar Saha, Jingbo Zhou, M. Tehranipoor, Farimah Farahmandi\n## Abstract\nAs the ubiquity and complexity of system-on-chip (SoC) designs increase across electronic devices, incorporating security into an SoC design flow poses significant challenges. Existing security solutions are inadequate to effectively verify modern SoC designs due to their limitations in scalability, comprehensiveness, and adaptability. On the other hand, large language models (LLMs) are celebrated for their remarkable success in language understanding, advanced reasoning, and program synthesis tasks. Recognizing an opportunity, our research explores leveraging the emergent capabilities of generative pre-trained transformers (GPTs) to address the existing gaps in SoC security, aiming for a more efficient, scalable, and adaptable methodology. By integrating LLMs into the SoC security verification paradigm, we open a new frontier of possibilities and challenges to ensure the security of increasingly complex SoCs. This paper offers an in-depth analysis of existing works, presents practical case studies, and demonstrates comprehensive experiments. We also present the achievements, prospects, and challenges of employing LLM in different SoC security verification tasks.\n## A. Preliminaries\nThe encoder component transforms input tokens into vectors using embeddings and positional encodings, then applies multihead self-attention and feedforward networks. The decoder, starting similarly, incorporates masked self-attention and crossattention with the output of the encoder, ensuring alignment and preventing future word prediction. Sequence-to-sequence models often use this architecture, where the input and output sequences can be of different lengths. These models shine in tasks where there is a direct and complex transformation between inputs and outputs. Examples like machine translation and text summarization are prototypical, as they require the model to understand the input deeply and generate a coherent and contextually accurate output. BART [5], T5 [137], and UL2 [138] are a few well-known encoder-decoder models to be named. \n\nIn Context of SoC Security: The encoder-decoder architecture, renowned for its ability in natural language understanding tasks, exhibits versatility in SoC security. Its twostage process of encoding the input data and then decoding it to produce an output makes it suitable for tasks that require both comprehension and generation. This model is particularly adept at vulnerability mitigation, where understanding the context (encoder) and generating a solution (decoder) are both crucial. However, while it is also a good fit for tasks like vulnerability insertion, security verification, and assessment, it might not always be the optimal choice when the task leans heavily toward either comprehension or generation b) Decoder-Only: Decoder-only LLMs have established impressive benchmarks in numerous NLP tasks, especially in the generation of free-form text. In a decoder-only model, a sequence is fed into the model, which then directly predicts the next token or word in the sequence. It operates autoregressively, using its generated tokens as context for subsequent predictions. It has two variants: causal decoder and prefix decoder. In a standard decoder (causal decoder), the unidirectional attention masking ensures that a token attends only to previous tokens and itself. Prefix decoders permit bidirectional attention over prefix tokens while maintaining unidirectional attention to generated tokens.",
            "reference_string": "[263829839 | Saha et al. | 2023 | Citations: 54]"
        },
        {
            "title": "The evolution, applications, and future prospects of large language models: An in-depth overview",
            "venue": "Applied and Computational Engineering",
            "year": 2024,
            "reference_count": 36,
            "citation_count": 2,
            "influential_citation_count": 0,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://www.ewadirect.com/proceedings/ace/article/view/10056/pdf",
                "status": "HYBRID",
                "license": "CCBY",
                "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.54254/2755-2721/35/20230399?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.54254/2755-2721/35/20230399, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2282449950",
                    "name": "Jiayin Li"
                }
            ],
            "abstract": "The evolution of natural language processing has transpired through three primary phases, with large-scale language models significantly transforming the field. These models have heightened the machine's capability to understand, produce, and interact with human language in unprecedented ways. Progressing from RNNs to transformer models, transitioning from encoder-decoder frameworks to decoder-centric designs, and the journey from BERT to the Chat-GPT series have marked significant shifts in the academic discourse. Impressively, these sophisticated models have infiltrated a range of sectors, including finance, healthcare, biology, and education, revolutionizing both traditional and emerging domains. However, as these advancements are celebrated, the ethical and economic challenges they introduce must also be addressed. Confronting these pivotal issues and harnessing technology for societal betterment has become a priority for academia and industry alike, sparking intense research endeavors in recent times. This review dives into the history of natural language processing, highlighting the pivotal developments and core principles of large language models. It provides a comprehensive perspective on their adoption and influence within the financial sector, crafting a detailed narrative of their deployment. In conclusion, the analysis reflects on the current challenges posed by these models and presents potential solutions. This study stands as a definitive guide, offering readers an in-depth understanding of the development, application, and future trajectories of large-scale language models.",
            "corpus_id": 267402678,
            "sentences": [
                {
                    "corpus_id": "267402678",
                    "title": "The evolution, applications, and future prospects of large language models: An in-depth overview",
                    "text": "As shown in Figure 3, similar to most seq2seq models, the Transformer architecture consists of an encoder and a decoder. Overall, LLM (Language Model) models can be categorized into three major types: Encoder-decoder Architecture, Causal Decoder Architecture, and Prefix Decoder Architecture [11]. The Encoder-decoder Architecture uses the most basic structure and was initially introduced by the Seq2Seq model to address sequence-to-sequence tasks, such as machine translation. It consists of an encoder and a decoder. The encoder is responsible for transforming the input sequence into a fixeddimensional semantic representation, while the decoder uses this semantic representation to generate the output sequence. Within the encoder-decoder structure, self-attention mechanisms are commonly employed for sequence modeling, enabling the model to handle variable-length input and output sequences. This architecture has proven to be highly effective in various sequence-to-sequence tasks, such as text translation, text summarization, and dialogue generation. Prominent examples of large language models following this architecture include ELMo, BERT, RoBERTa, among others [12]. \n\nCurrently, the most widely used architecture is the Causal Decoder, which is primarily employed for handling autoregressive generation tasks, where each element of the output sequence depends on previously generated elements. The Causal Decoder Architecture is an improvement over the Encoderdecoder structure, as it introduces an autoregressive mechanism in the decoder. This means that when generating the current element, the model only uses the elements generated before it. This ensures that the model does not have access to future information during the generation process, thereby preserving causality. The GPT series (e.g., GPT-3) is a typical example of models that use the Causal Decoder Architecture. These models generate text by sequentially producing words one by one, avoiding information leakage and enabling the generation of coherent and plausible text. \n\nCompared to the Encoder-decoder models, Decoder-only models offer several advantages due to their simpler structure, faster training and inference speed, suitability for pure generation tasks, and advantages in decoder self-supervision.",
                    "score": 0.5231945909442883,
                    "section_title": "Different structures for combining encoders and decoders.",
                    "char_start_offset": 10124,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 120
                        },
                        {
                            "start": 121,
                            "end": 297
                        },
                        {
                            "start": 298,
                            "end": 478
                        },
                        {
                            "start": 479,
                            "end": 519
                        },
                        {
                            "start": 520,
                            "end": 716
                        },
                        {
                            "start": 717,
                            "end": 898
                        },
                        {
                            "start": 899,
                            "end": 1060
                        },
                        {
                            "start": 1061,
                            "end": 1180
                        },
                        {
                            "start": 1183,
                            "end": 1408
                        },
                        {
                            "start": 1409,
                            "end": 1554
                        },
                        {
                            "start": 1555,
                            "end": 1661
                        },
                        {
                            "start": 1662,
                            "end": 1793
                        },
                        {
                            "start": 1794,
                            "end": 1895
                        },
                        {
                            "start": 1896,
                            "end": 2055
                        },
                        {
                            "start": 2058,
                            "end": 2294
                        }
                    ],
                    "ref_mentions": [],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.78076171875
                }
            ],
            "relevance_judgement": 0.78076171875,
            "relevance_judgment_input_expanded": "# Title: The evolution, applications, and future prospects of large language models: An in-depth overview\n# Venue: Applied and Computational Engineering\n# Authors: Jiayin Li\n## Abstract\nThe evolution of natural language processing has transpired through three primary phases, with large-scale language models significantly transforming the field. These models have heightened the machine's capability to understand, produce, and interact with human language in unprecedented ways. Progressing from RNNs to transformer models, transitioning from encoder-decoder frameworks to decoder-centric designs, and the journey from BERT to the Chat-GPT series have marked significant shifts in the academic discourse. Impressively, these sophisticated models have infiltrated a range of sectors, including finance, healthcare, biology, and education, revolutionizing both traditional and emerging domains. However, as these advancements are celebrated, the ethical and economic challenges they introduce must also be addressed. Confronting these pivotal issues and harnessing technology for societal betterment has become a priority for academia and industry alike, sparking intense research endeavors in recent times. This review dives into the history of natural language processing, highlighting the pivotal developments and core principles of large language models. It provides a comprehensive perspective on their adoption and influence within the financial sector, crafting a detailed narrative of their deployment. In conclusion, the analysis reflects on the current challenges posed by these models and presents potential solutions. This study stands as a definitive guide, offering readers an in-depth understanding of the development, application, and future trajectories of large-scale language models.\n## Different structures for combining encoders and decoders.\nAs shown in Figure 3, similar to most seq2seq models, the Transformer architecture consists of an encoder and a decoder. Overall, LLM (Language Model) models can be categorized into three major types: Encoder-decoder Architecture, Causal Decoder Architecture, and Prefix Decoder Architecture [11]. The Encoder-decoder Architecture uses the most basic structure and was initially introduced by the Seq2Seq model to address sequence-to-sequence tasks, such as machine translation. It consists of an encoder and a decoder. The encoder is responsible for transforming the input sequence into a fixeddimensional semantic representation, while the decoder uses this semantic representation to generate the output sequence. Within the encoder-decoder structure, self-attention mechanisms are commonly employed for sequence modeling, enabling the model to handle variable-length input and output sequences. This architecture has proven to be highly effective in various sequence-to-sequence tasks, such as text translation, text summarization, and dialogue generation. Prominent examples of large language models following this architecture include ELMo, BERT, RoBERTa, among others [12]. \n\nCurrently, the most widely used architecture is the Causal Decoder, which is primarily employed for handling autoregressive generation tasks, where each element of the output sequence depends on previously generated elements. The Causal Decoder Architecture is an improvement over the Encoderdecoder structure, as it introduces an autoregressive mechanism in the decoder. This means that when generating the current element, the model only uses the elements generated before it. This ensures that the model does not have access to future information during the generation process, thereby preserving causality. The GPT series (e.g., GPT-3) is a typical example of models that use the Causal Decoder Architecture. These models generate text by sequentially producing words one by one, avoiding information leakage and enabling the generation of coherent and plausible text. \n\nCompared to the Encoder-decoder models, Decoder-only models offer several advantages due to their simpler structure, faster training and inference speed, suitability for pure generation tasks, and advantages in decoder self-supervision.",
            "reference_string": "[267402678 | Li | 2024 | Citations: 2]"
        },
        {
            "title": "Exploring the Role of Large Language Models in Prompt Encoding for Diffusion Models",
            "venue": "Neural Information Processing Systems",
            "year": 2024,
            "reference_count": 66,
            "citation_count": 23,
            "influential_citation_count": 4,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2406.11831, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2261489892",
                    "name": "Bingqi Ma"
                },
                {
                    "authorId": "1571400317",
                    "name": "Zhuofan Zong"
                },
                {
                    "authorId": "12920342",
                    "name": "Guanglu Song"
                },
                {
                    "authorId": "2261394248",
                    "name": "Hongsheng Li"
                },
                {
                    "authorId": "2261417717",
                    "name": "Yu Liu"
                }
            ],
            "abstract": "Large language models (LLMs) based on decoder-only transformers have demonstrated superior text understanding capabilities compared to CLIP and T5-series models. However, the paradigm for utilizing current advanced LLMs in text-to-image diffusion models remains to be explored. We observed an unusual phenomenon: directly using a large language model as the prompt encoder significantly degrades the prompt-following ability in image generation. We identified two main obstacles behind this issue. One is the misalignment between the next token prediction training in LLM and the requirement for discriminative prompt features in diffusion models. The other is the intrinsic positional bias introduced by the decoder-only architecture. To deal with this issue, we propose a novel framework to fully harness the capabilities of LLMs. Through the carefully designed usage guidance, we effectively enhance the text representation capability for prompt encoding and eliminate its inherent positional bias. This allows us to integrate state-of-the-art LLMs into the text-to-image generation model flexibly. Furthermore, we also provide an effective manner to fuse multiple LLMs into our framework. Considering the excellent performance and scaling capabilities demonstrated by the transformer architecture, we further design an LLM-Infused Diffusion Transformer (LI-DiT) based on the framework. We conduct extensive experiments to validate LI-DiT across model size and data size. Benefiting from the inherent ability of the LLMs and our innovative designs, the prompt understanding performance of LI-DiT easily surpasses state-of-the-art open-source models as well as mainstream closed-source commercial models including Stable Diffusion 3, DALL-E 3, and Midjourney V6. The LLM-Infused Diffuser framework is also one of the core technologies powering SenseMirage, a highly advanced text-to-image model.",
            "corpus_id": 270560675,
            "sentences": [
                {
                    "corpus_id": "270560675",
                    "title": "Exploring the Role of Large Language Models in Prompt Encoding for Diffusion Models",
                    "text": "As outlined in Sec. 1, we observe two discrepancies between decoder-only LLMs and encoder-decoder models: optimization objective and model architecture. Specifically, the decoder-only LLMs are typically optimized using the next token prediction task while the encoder-decoder models are trained with the masked language modeling task. Besides, the former tokens in a sequence cannot attend the latter tokens in decoder-only LLMs while every token in the sequence can attend each other in the encoder models. Based on the observations, we conduct elaborate experiments to investigate how such discrepancies affect the prompt encoding capacity of LLMs.",
                    "score": 0.5588926847533273,
                    "section_title": "Prompt Encoding with Language Models",
                    "char_start_offset": 4842,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 152
                        },
                        {
                            "start": 153,
                            "end": 334
                        },
                        {
                            "start": 335,
                            "end": 507
                        },
                        {
                            "start": 508,
                            "end": 650
                        }
                    ],
                    "ref_mentions": [],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.7783203125
                }
            ],
            "relevance_judgement": 0.7783203125,
            "relevance_judgment_input_expanded": "# Title: Exploring the Role of Large Language Models in Prompt Encoding for Diffusion Models\n# Venue: Neural Information Processing Systems\n# Authors: Bingqi Ma, Zhuofan Zong, Guanglu Song, Hongsheng Li, Yu Liu\n## Abstract\nLarge language models (LLMs) based on decoder-only transformers have demonstrated superior text understanding capabilities compared to CLIP and T5-series models. However, the paradigm for utilizing current advanced LLMs in text-to-image diffusion models remains to be explored. We observed an unusual phenomenon: directly using a large language model as the prompt encoder significantly degrades the prompt-following ability in image generation. We identified two main obstacles behind this issue. One is the misalignment between the next token prediction training in LLM and the requirement for discriminative prompt features in diffusion models. The other is the intrinsic positional bias introduced by the decoder-only architecture. To deal with this issue, we propose a novel framework to fully harness the capabilities of LLMs. Through the carefully designed usage guidance, we effectively enhance the text representation capability for prompt encoding and eliminate its inherent positional bias. This allows us to integrate state-of-the-art LLMs into the text-to-image generation model flexibly. Furthermore, we also provide an effective manner to fuse multiple LLMs into our framework. Considering the excellent performance and scaling capabilities demonstrated by the transformer architecture, we further design an LLM-Infused Diffusion Transformer (LI-DiT) based on the framework. We conduct extensive experiments to validate LI-DiT across model size and data size. Benefiting from the inherent ability of the LLMs and our innovative designs, the prompt understanding performance of LI-DiT easily surpasses state-of-the-art open-source models as well as mainstream closed-source commercial models including Stable Diffusion 3, DALL-E 3, and Midjourney V6. The LLM-Infused Diffuser framework is also one of the core technologies powering SenseMirage, a highly advanced text-to-image model.\n## Prompt Encoding with Language Models\nAs outlined in Sec. 1, we observe two discrepancies between decoder-only LLMs and encoder-decoder models: optimization objective and model architecture. Specifically, the decoder-only LLMs are typically optimized using the next token prediction task while the encoder-decoder models are trained with the masked language modeling task. Besides, the former tokens in a sequence cannot attend the latter tokens in decoder-only LLMs while every token in the sequence can attend each other in the encoder models. Based on the observations, we conduct elaborate experiments to investigate how such discrepancies affect the prompt encoding capacity of LLMs.",
            "reference_string": "[270560675 | Ma et al. | 2024 | Citations: 23]"
        },
        {
            "title": "A Review of Current Trends, Techniques, and Challenges in Large Language Models (LLMs)",
            "venue": "Applied Sciences",
            "year": 2024,
            "reference_count": 108,
            "citation_count": 80,
            "influential_citation_count": 1,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://www.mdpi.com/2076-3417/14/5/2074/pdf?version=1709291698",
                "status": "GOLD",
                "license": "CCBY",
                "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.3390/app14052074?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.3390/app14052074, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2289385425",
                    "name": "Rajvardhan Patil"
                },
                {
                    "authorId": "117730513",
                    "name": "Venkat Gudivada"
                }
            ],
            "abstract": "Natural language processing (NLP) has significantly transformed in the last decade, especially in the field of language modeling. Large language models (LLMs) have achieved SOTA performances on natural language understanding (NLU) and natural language generation (NLG) tasks by learning language representation in self-supervised ways. This paper provides a comprehensive survey to capture the progression of advances in language models. In this paper, we examine the different aspects of language models, which started with a few million parameters but have reached the size of a trillion in a very short time. We also look at how these LLMs transitioned from task-specific to task-independent to task-and-language-independent architectures. This paper extensively discusses different pretraining objectives, benchmarks, and transfer learning methods used in LLMs. It also examines different finetuning and in-context learning techniques used in downstream tasks. Moreover, it explores how LLMs can perform well across many domains and datasets if sufficiently trained on a large and diverse dataset. Next, it discusses how, over time, the availability of cheap computational power and large datasets have improved LLM\u2019s capabilities and raised new challenges. As part of our study, we also inspect LLMs from the perspective of scalability to see how their performance is affected by the model\u2019s depth, width, and data size. Lastly, we provide an empirical comparison of existing trends and techniques and a comprehensive analysis of where the field of LLM currently stands.",
            "corpus_id": 268157336,
            "sentences": [
                {
                    "corpus_id": "268157336",
                    "title": "A Review of Current Trends, Techniques, and Challenges in Large Language Models (LLMs)",
                    "text": "Prefix language models are also decoder-only-based models but differ in the masking mechanism. Instead of a causal mask, a fully visible mask is used for the prefix part of the input sequence, and a causal mask is used for the target sequence. \n\nFor example, to translate an English sentence \"I am doing well\" to French, the model would apply a fully visible mask to the prefix \"translate English to French: I am doing well. Target:\", followed by causal masking while predicting the target \"je vais bien\". Also, unlike causal language models where the targets-only paradigm is used, the prefix language model uses the input-to-target paradigm. Both causal and prefix model architectures are autoregressive as the objective is to predict the next token. However, the causal model uses a unidirectional attention mask, while the prefix model modifies the masking mechanism to employ bidirectional attention over prefix tokens. Figure 4 demonstrates the mechanism of the above architectures. The lines represent the attention visibility. Dark lines represent the fully visible masking (bidirectional attention), and light gray lines represent causal masking (unidirectional attention). As shown in Figure 4, in the encoder-decoder architecture, fully visible masking is used in the encoder and causal masking is used in the decoder. In a decoder-only model, the input and target are concatenated, and then a causal mask is used throughout. A decoder-only model with a prefix allows fully visible masking over part of the input token (prefix), followed by causal masking on the rest of the sequence. In general, autoencoding models learn bidirectional contextualized representation suited for NLU tasks, whereas autoregressive models learn to generate the next token and hence are suited for NLG tasks.",
                    "score": 0.5904376792323018,
                    "section_title": "Prefix (Non-Causal) Language Model",
                    "char_start_offset": 19013,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 94
                        },
                        {
                            "start": 95,
                            "end": 243
                        },
                        {
                            "start": 246,
                            "end": 424
                        },
                        {
                            "start": 425,
                            "end": 505
                        },
                        {
                            "start": 506,
                            "end": 643
                        },
                        {
                            "start": 644,
                            "end": 752
                        },
                        {
                            "start": 753,
                            "end": 924
                        },
                        {
                            "start": 925,
                            "end": 988
                        },
                        {
                            "start": 989,
                            "end": 1034
                        },
                        {
                            "start": 1035,
                            "end": 1182
                        },
                        {
                            "start": 1183,
                            "end": 1329
                        },
                        {
                            "start": 1330,
                            "end": 1436
                        },
                        {
                            "start": 1437,
                            "end": 1595
                        },
                        {
                            "start": 1596,
                            "end": 1798
                        }
                    ],
                    "ref_mentions": [],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.77587890625
                },
                {
                    "corpus_id": "268157336",
                    "title": "A Review of Current Trends, Techniques, and Challenges in Large Language Models (LLMs)",
                    "text": "An example of encoder-decoder architecture is the transformer model proposed in [24]. Its encoder and decoder blocks are stacked with multiple layers. As shown in Figure 3, the transformer encoder layer consists of a self-attention layer and a position-wise feed-forward layer. In addition to these two layers, decoder consists of a third cross-attention layer, which is responsible for attending to encoder output. \n\nEncoder-decoder models adopt bidirectional attention for the encoder, unidirectional attention for the decoder, and a cross-attention mechanism between them. Cross-attention in the decoder has access only to the fully processed encoder output and is responsible for connecting input tokens to target tokens. The encoder-decoder-based models are pretrained for seq2seq tasks. They can also be pretrained on conditional generation tasks, where the output is generated in regard to the given input, for example in summarizing, question answering, and translation tasks. T5 [25] uses encoder-decoder architecture. As stated in T5, using encoder-decoder structure helped to achieve good performance regarding classification as well as for generative tasks. \n\nAlthough encoder-decoder models end up having twice as many parameters as their decoder-only or encoder-only counterparts, they still have similar computational cost. Compared to PrefixLM models where the parameters are shared, here, the input and target are independently processed and use separate sets of parameters. Unlike decoder-only language models that are trained to generate the input, encoder-decoder models output target tokens. \n\nThe original transformer consisted of encoder-decoder blocks and was initially used for sequence-to-sequence tasks, such as NMT. However, it was discovered that, with the change in how the input is fed to the model, the single-stack (decoder or encoder) could also complete sequence-sequence model tasks. As a result, the subsequent models started containing either an encoder or decoder architecture. Below, we discuss these architectural variants of the original transformer model.",
                    "score": 0.48287359742819547,
                    "section_title": "Encoder-Decoder-Based Model",
                    "char_start_offset": 15589,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 85
                        },
                        {
                            "start": 86,
                            "end": 150
                        },
                        {
                            "start": 151,
                            "end": 277
                        },
                        {
                            "start": 278,
                            "end": 415
                        },
                        {
                            "start": 418,
                            "end": 575
                        },
                        {
                            "start": 576,
                            "end": 725
                        },
                        {
                            "start": 726,
                            "end": 792
                        },
                        {
                            "start": 793,
                            "end": 984
                        },
                        {
                            "start": 985,
                            "end": 1027
                        },
                        {
                            "start": 1028,
                            "end": 1169
                        },
                        {
                            "start": 1172,
                            "end": 1338
                        },
                        {
                            "start": 1339,
                            "end": 1491
                        },
                        {
                            "start": 1492,
                            "end": 1612
                        },
                        {
                            "start": 1615,
                            "end": 1743
                        },
                        {
                            "start": 1744,
                            "end": 1919
                        },
                        {
                            "start": 1920,
                            "end": 2016
                        },
                        {
                            "start": 2017,
                            "end": 2098
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 988,
                            "end": 992,
                            "matchedPaperCorpusId": "204838007"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.68017578125
                }
            ],
            "relevance_judgement": 0.77587890625,
            "relevance_judgment_input_expanded": "# Title: A Review of Current Trends, Techniques, and Challenges in Large Language Models (LLMs)\n# Venue: Applied Sciences\n# Authors: Rajvardhan Patil, Venkat Gudivada\n## Abstract\nNatural language processing (NLP) has significantly transformed in the last decade, especially in the field of language modeling. Large language models (LLMs) have achieved SOTA performances on natural language understanding (NLU) and natural language generation (NLG) tasks by learning language representation in self-supervised ways. This paper provides a comprehensive survey to capture the progression of advances in language models. In this paper, we examine the different aspects of language models, which started with a few million parameters but have reached the size of a trillion in a very short time. We also look at how these LLMs transitioned from task-specific to task-independent to task-and-language-independent architectures. This paper extensively discusses different pretraining objectives, benchmarks, and transfer learning methods used in LLMs. It also examines different finetuning and in-context learning techniques used in downstream tasks. Moreover, it explores how LLMs can perform well across many domains and datasets if sufficiently trained on a large and diverse dataset. Next, it discusses how, over time, the availability of cheap computational power and large datasets have improved LLM\u2019s capabilities and raised new challenges. As part of our study, we also inspect LLMs from the perspective of scalability to see how their performance is affected by the model\u2019s depth, width, and data size. Lastly, we provide an empirical comparison of existing trends and techniques and a comprehensive analysis of where the field of LLM currently stands.\n## Encoder-Decoder-Based Model\nAn example of encoder-decoder architecture is the transformer model proposed in [24]. Its encoder and decoder blocks are stacked with multiple layers. As shown in Figure 3, the transformer encoder layer consists of a self-attention layer and a position-wise feed-forward layer. In addition to these two layers, decoder consists of a third cross-attention layer, which is responsible for attending to encoder output. \n\nEncoder-decoder models adopt bidirectional attention for the encoder, unidirectional attention for the decoder, and a cross-attention mechanism between them. Cross-attention in the decoder has access only to the fully processed encoder output and is responsible for connecting input tokens to target tokens. The encoder-decoder-based models are pretrained for seq2seq tasks. They can also be pretrained on conditional generation tasks, where the output is generated in regard to the given input, for example in summarizing, question answering, and translation tasks. T5 [25] uses encoder-decoder architecture. As stated in T5, using encoder-decoder structure helped to achieve good performance regarding classification as well as for generative tasks. \n\nAlthough encoder-decoder models end up having twice as many parameters as their decoder-only or encoder-only counterparts, they still have similar computational cost. Compared to PrefixLM models where the parameters are shared, here, the input and target are independently processed and use separate sets of parameters. Unlike decoder-only language models that are trained to generate the input, encoder-decoder models output target tokens. \n\nThe original transformer consisted of encoder-decoder blocks and was initially used for sequence-to-sequence tasks, such as NMT. However, it was discovered that, with the change in how the input is fed to the model, the single-stack (decoder or encoder) could also complete sequence-sequence model tasks. As a result, the subsequent models started containing either an encoder or decoder architecture. Below, we discuss these architectural variants of the original transformer model.\n\n## Prefix (Non-Causal) Language Model\nPrefix language models are also decoder-only-based models but differ in the masking mechanism. Instead of a causal mask, a fully visible mask is used for the prefix part of the input sequence, and a causal mask is used for the target sequence. \n\nFor example, to translate an English sentence \"I am doing well\" to French, the model would apply a fully visible mask to the prefix \"translate English to French: I am doing well. Target:\", followed by causal masking while predicting the target \"je vais bien\". Also, unlike causal language models where the targets-only paradigm is used, the prefix language model uses the input-to-target paradigm. Both causal and prefix model architectures are autoregressive as the objective is to predict the next token. However, the causal model uses a unidirectional attention mask, while the prefix model modifies the masking mechanism to employ bidirectional attention over prefix tokens. Figure 4 demonstrates the mechanism of the above architectures. The lines represent the attention visibility. Dark lines represent the fully visible masking (bidirectional attention), and light gray lines represent causal masking (unidirectional attention). As shown in Figure 4, in the encoder-decoder architecture, fully visible masking is used in the encoder and causal masking is used in the decoder. In a decoder-only model, the input and target are concatenated, and then a causal mask is used throughout. A decoder-only model with a prefix allows fully visible masking over part of the input token (prefix), followed by causal masking on the rest of the sequence. In general, autoencoding models learn bidirectional contextualized representation suited for NLU tasks, whereas autoregressive models learn to generate the next token and hence are suited for NLG tasks.",
            "reference_string": "[268157336 | Patil et al. | 2024 | Citations: 80]"
        },
        {
            "title": "Copiloting the Copilots: Fusing Large Language Models with Completion Engines for Automated Program Repair",
            "venue": "ESEC/SIGSOFT FSE",
            "year": 2023,
            "reference_count": 77,
            "citation_count": 100,
            "influential_citation_count": 6,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://arxiv.org/pdf/2309.00608",
                "status": "GREEN",
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2309.00608, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2237736409",
                    "name": "Yuxiang Wei"
                },
                {
                    "authorId": "145349987",
                    "name": "Chun Xia"
                },
                {
                    "authorId": "2237429253",
                    "name": "Lingming Zhang"
                }
            ],
            "abstract": "During Automated Program Repair (APR), it can be challenging to synthesize correct patches for real-world systems in general-purpose programming languages. Recent Large Language Models (LLMs) have been shown to be helpful \u201ccopilots\u201d in assisting developers with various coding tasks, and have also been directly applied for patch synthesis. However, most LLMs treat programs as sequences of tokens, meaning that they are ignorant of the underlying semantics constraints of the target programming language. This results in plenty of statically invalid generated patches, impeding the practicality of the technique. Therefore, we propose Repilot, a framework to further copilot the AI \u201ccopilots\u201d (i.e., LLMs) by synthesizing more valid patches during the repair process. Our key insight is that many LLMs produce outputs autoregressively (i.e., token by token), resembling human writing programs, which can be significantly boosted and guided through a Completion Engine. Repilot synergistically synthesizes a candidate patch through the interaction between an LLM and a Completion Engine, which 1) prunes away infeasible tokens suggested by the LLM and 2) proactively completes the token based on the suggestions provided by the Completion Engine. Our evaluation on a subset of the widely-used Defects4j 1.2 and 2.0 datasets shows that Repilot fixes 66 and 50 bugs, respectively, surpassing the best-performing baseline by 14 and 16 bugs fixed. More importantly, Repilot is capable of producing more valid and correct patches than the base LLM when given the same generation budget.",
            "corpus_id": 261494010,
            "sentences": [
                {
                    "corpus_id": "261494010",
                    "title": "Copiloting the Copilots: Fusing Large Language Models with Completion Engines for Automated Program Repair",
                    "text": "Recent advances in Natural Language Processing (NLP) have empowered the idea of using Large Language Models (LLMs) that are pre-trained on enormous corpora of natural language and code for various code-related tasks [4,5,10,38,71]. LLMs are based on the transformer architecture [61] that can be categorized into encoderonly, decoder-only and encoder-decoder. Encoder-only models use only the encoder component by training using Masked Language Modeling (MLM) [14] objective where a small percentage (e.g., 15%) of the tokens are masked on. The goal of MLM is to recover these masked tokens given the surrounding context. Encoderonly models such as CodeBERT [15] and GraphCodeBERT [22] are designed to provide a representation of the input code to be used for downstream tasks such as code classification [72]. Decoder-only models, on the other hand, aim to autoregressively generate tokens based on all previously generated tokens. CodeGEN [51,52], Codex [10] and PolyCoder [71] are examples of decoder-only LLMs where they can be used for code autocompletion tasks. Different from encoder-and decoder-only LLMs, encoder-decoder models (e.g., CodeT5 [62,63] and PLBART [3]) combine both encoder and decoder together and jointly train both components together. A commonly used pre-training objective for encoder-decoder models is Masked Span Prediction (MSP) where random spans (multiple consecutive tokens) are replaced with single masked tokens and the models learn to fill in the masked span with the correct sequence of tokens. Furthermore, decoder-only models like InCoder [17] can also perform infilling by training through causal language modeling [2] objective. Instead of using the decoder to predict the next token in the original training data, similar to MSP, InCoder also replaces random spans with masked span tokens. During training, InCoder learns to autoregressively recover the original spans.",
                    "score": 0.49589470358197923,
                    "section_title": "BACKGROUND AND RELATED WORK 2.1 Large Language Models for Code",
                    "char_start_offset": 9083,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 231
                        },
                        {
                            "start": 232,
                            "end": 359
                        },
                        {
                            "start": 360,
                            "end": 540
                        },
                        {
                            "start": 541,
                            "end": 621
                        },
                        {
                            "start": 622,
                            "end": 810
                        },
                        {
                            "start": 811,
                            "end": 932
                        },
                        {
                            "start": 933,
                            "end": 1067
                        },
                        {
                            "start": 1068,
                            "end": 1260
                        },
                        {
                            "start": 1261,
                            "end": 1531
                        },
                        {
                            "start": 1532,
                            "end": 1669
                        },
                        {
                            "start": 1670,
                            "end": 1831
                        },
                        {
                            "start": 1832,
                            "end": 1911
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 219,
                            "end": 221,
                            "matchedPaperCorpusId": "250144196"
                        },
                        {
                            "start": 224,
                            "end": 227,
                            "matchedPaperCorpusId": "246527904"
                        },
                        {
                            "start": 227,
                            "end": 230,
                            "matchedPaperCorpusId": "247158549"
                        },
                        {
                            "start": 279,
                            "end": 283,
                            "matchedPaperCorpusId": "13756489"
                        },
                        {
                            "start": 460,
                            "end": 464,
                            "matchedPaperCorpusId": "52967399"
                        },
                        {
                            "start": 681,
                            "end": 685,
                            "matchedPaperCorpusId": "221761146"
                        },
                        {
                            "start": 975,
                            "end": 979,
                            "matchedPaperCorpusId": "247158549"
                        },
                        {
                            "start": 1155,
                            "end": 1158,
                            "matchedPaperCorpusId": "237386541"
                        },
                        {
                            "start": 1578,
                            "end": 1582,
                            "matchedPaperCorpusId": "248157108"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.75537109375
                }
            ],
            "relevance_judgement": 0.75537109375,
            "relevance_judgment_input_expanded": "# Title: Copiloting the Copilots: Fusing Large Language Models with Completion Engines for Automated Program Repair\n# Venue: ESEC/SIGSOFT FSE\n# Authors: Yuxiang Wei, Chun Xia, Lingming Zhang\n## Abstract\nDuring Automated Program Repair (APR), it can be challenging to synthesize correct patches for real-world systems in general-purpose programming languages. Recent Large Language Models (LLMs) have been shown to be helpful \u201ccopilots\u201d in assisting developers with various coding tasks, and have also been directly applied for patch synthesis. However, most LLMs treat programs as sequences of tokens, meaning that they are ignorant of the underlying semantics constraints of the target programming language. This results in plenty of statically invalid generated patches, impeding the practicality of the technique. Therefore, we propose Repilot, a framework to further copilot the AI \u201ccopilots\u201d (i.e., LLMs) by synthesizing more valid patches during the repair process. Our key insight is that many LLMs produce outputs autoregressively (i.e., token by token), resembling human writing programs, which can be significantly boosted and guided through a Completion Engine. Repilot synergistically synthesizes a candidate patch through the interaction between an LLM and a Completion Engine, which 1) prunes away infeasible tokens suggested by the LLM and 2) proactively completes the token based on the suggestions provided by the Completion Engine. Our evaluation on a subset of the widely-used Defects4j 1.2 and 2.0 datasets shows that Repilot fixes 66 and 50 bugs, respectively, surpassing the best-performing baseline by 14 and 16 bugs fixed. More importantly, Repilot is capable of producing more valid and correct patches than the base LLM when given the same generation budget.\n## BACKGROUND AND RELATED WORK 2.1 Large Language Models for Code\nRecent advances in Natural Language Processing (NLP) have empowered the idea of using Large Language Models (LLMs) that are pre-trained on enormous corpora of natural language and code for various code-related tasks [4,5,10,38,71]. LLMs are based on the transformer architecture [61] that can be categorized into encoderonly, decoder-only and encoder-decoder. Encoder-only models use only the encoder component by training using Masked Language Modeling (MLM) [14] objective where a small percentage (e.g., 15%) of the tokens are masked on. The goal of MLM is to recover these masked tokens given the surrounding context. Encoderonly models such as CodeBERT [15] and GraphCodeBERT [22] are designed to provide a representation of the input code to be used for downstream tasks such as code classification [72]. Decoder-only models, on the other hand, aim to autoregressively generate tokens based on all previously generated tokens. CodeGEN [51,52], Codex [10] and PolyCoder [71] are examples of decoder-only LLMs where they can be used for code autocompletion tasks. Different from encoder-and decoder-only LLMs, encoder-decoder models (e.g., CodeT5 [62,63] and PLBART [3]) combine both encoder and decoder together and jointly train both components together. A commonly used pre-training objective for encoder-decoder models is Masked Span Prediction (MSP) where random spans (multiple consecutive tokens) are replaced with single masked tokens and the models learn to fill in the masked span with the correct sequence of tokens. Furthermore, decoder-only models like InCoder [17] can also perform infilling by training through causal language modeling [2] objective. Instead of using the decoder to predict the next token in the original training data, similar to MSP, InCoder also replaces random spans with masked span tokens. During training, InCoder learns to autoregressively recover the original spans.",
            "reference_string": "[261494010 | Wei et al. | 2023 | Citations: 100]"
        },
        {
            "title": "A Survey on Non-Autoregressive Generation for Neural Machine Translation and Beyond",
            "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence",
            "year": 2022,
            "reference_count": 258,
            "citation_count": 87,
            "influential_citation_count": 4,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://arxiv.org/pdf/2204.09269",
                "status": "GREEN",
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2204.09269, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "152678922",
                    "name": "Yisheng Xiao"
                },
                {
                    "authorId": "47767550",
                    "name": "Lijun Wu"
                },
                {
                    "authorId": "13838086",
                    "name": "Junliang Guo"
                },
                {
                    "authorId": "2109013629",
                    "name": "Juntao Li"
                },
                {
                    "authorId": "39767557",
                    "name": "M. Zhang"
                },
                {
                    "authorId": "143826491",
                    "name": "Tao Qin"
                },
                {
                    "authorId": "2110264337",
                    "name": "Tie-Yan Liu"
                }
            ],
            "abstract": "Non-autoregressive (NAR) generation, which is first proposed in neural machine translation (NMT) to speed up inference, has attracted much attention in both machine learning and natural language processing communities. While NAR generation can significantly accelerate inference speed for machine translation, the speedup comes at the cost of sacrificed translation accuracy compared to its counterpart, autoregressive (AR) generation. In recent years, many new models and algorithms have been designed/proposed to bridge the accuracy gap between NAR generation and AR generation. In this paper, we conduct a systematic survey with comparisons and discussions of various non-autoregressive translation (NAT) models from different aspects. Specifically, we categorize the efforts of NAT into several groups, including data manipulation, modeling methods, training criterion, decoding algorithms, and the benefit from pre-trained models. Furthermore, we briefly review other applications of NAR models beyond machine translation, such as grammatical error correction, text summarization, text style transfer, dialogue, semantic parsing, automatic speech recognition, and so on. In addition, we also discuss potential directions for future exploration, including releasing the dependency of KD, reasonable training objectives, pre-training for NAR, and wider applications, etc. We hope this survey can help researchers capture the latest progress in NAR generation, inspire the design of advanced NAR models and algorithms, and enable industry practitioners to choose appropriate solutions for their applications.",
            "corpus_id": 248266379,
            "sentences": [
                {
                    "corpus_id": "248266379",
                    "title": "A Survey on Non-Autoregressive Generation for Neural Machine Translation and Beyond",
                    "text": "Model Architecture. As for model architecture, both AT and NAT models take the encoder and decoder framework for translation. The encoder and decoder can be different neural networks, such as RNN [9], CNN [11], and Transformer [13]. Due to the superior performance of the Transformer network, we focus on the Transformer model for discussion in this survey. The encoder is used to encode the source sentences, while the decoder is utilized for decoding the target sentence. Compared to AT and NAT models, they adopt the same encoder architecture, and the differences are reflected in the decoders to match the specific training objective. (1) Specifically, AT models need to prevent earlier decoding steps from peeking at information from later steps. Therefore, the constraint of an autoregressive factorization of the output distribution is required, and they adopt the strict causal mask by applying a lower triangular matrix in the self-attention module of the conventional Transformer decoder [13]. (2) However, for NAT models, including the iteration-based NAT models, this constraint is no longer necessary, so they adopt the unmasked self-attention over all target tokens [16]. \n\n(3) As for SAT models, they adopt a coarse-grained lower triangular matrix as the causal mask, which means that they allow k tokens to peep later information in the same group while keeping the constraint between different groups. Inference Schedule. When going to the inference stage, the differences are as follows. (1) The AT models predict the target tokens in a one-by-one manner, and the tokens predicted previously are fed back into the decoder to generate the next token. (2) While SAT models predict a group of target tokens at one time, the previously generated groups of tokens are fed into the decoder to generate the next group of tokens, which is the same as the AT models. (3) For iterationbased NAT models, it needs k iterations for inference. The translated results of the previous iteration will be fed into the decoder again for refinements. (4) As for fully NAT models, they generate all predicted target tokens at only one step, which greatly speeds up inference.",
                    "score": 0.5044484716249439,
                    "section_title": "Comparison",
                    "char_start_offset": 12320,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 19
                        },
                        {
                            "start": 20,
                            "end": 125
                        },
                        {
                            "start": 126,
                            "end": 232
                        },
                        {
                            "start": 233,
                            "end": 357
                        },
                        {
                            "start": 358,
                            "end": 473
                        },
                        {
                            "start": 474,
                            "end": 638
                        },
                        {
                            "start": 639,
                            "end": 751
                        },
                        {
                            "start": 752,
                            "end": 1003
                        },
                        {
                            "start": 1004,
                            "end": 1185
                        },
                        {
                            "start": 1188,
                            "end": 1418
                        },
                        {
                            "start": 1419,
                            "end": 1438
                        },
                        {
                            "start": 1439,
                            "end": 1505
                        },
                        {
                            "start": 1506,
                            "end": 1667
                        },
                        {
                            "start": 1668,
                            "end": 1875
                        },
                        {
                            "start": 1876,
                            "end": 1947
                        },
                        {
                            "start": 1948,
                            "end": 2048
                        },
                        {
                            "start": 2049,
                            "end": 2172
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 196,
                            "end": 199,
                            "matchedPaperCorpusId": "17048224"
                        },
                        {
                            "start": 205,
                            "end": 209,
                            "matchedPaperCorpusId": "3648736"
                        },
                        {
                            "start": 227,
                            "end": 231,
                            "matchedPaperCorpusId": "13756489"
                        },
                        {
                            "start": 998,
                            "end": 1002,
                            "matchedPaperCorpusId": "13756489"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.75
                }
            ],
            "relevance_judgement": 0.75,
            "relevance_judgment_input_expanded": "# Title: A Survey on Non-Autoregressive Generation for Neural Machine Translation and Beyond\n# Venue: IEEE Transactions on Pattern Analysis and Machine Intelligence\n# Authors: Yisheng Xiao, Lijun Wu, Junliang Guo, Juntao Li, M. Zhang, Tao Qin, Tie-Yan Liu\n## Abstract\nNon-autoregressive (NAR) generation, which is first proposed in neural machine translation (NMT) to speed up inference, has attracted much attention in both machine learning and natural language processing communities. While NAR generation can significantly accelerate inference speed for machine translation, the speedup comes at the cost of sacrificed translation accuracy compared to its counterpart, autoregressive (AR) generation. In recent years, many new models and algorithms have been designed/proposed to bridge the accuracy gap between NAR generation and AR generation. In this paper, we conduct a systematic survey with comparisons and discussions of various non-autoregressive translation (NAT) models from different aspects. Specifically, we categorize the efforts of NAT into several groups, including data manipulation, modeling methods, training criterion, decoding algorithms, and the benefit from pre-trained models. Furthermore, we briefly review other applications of NAR models beyond machine translation, such as grammatical error correction, text summarization, text style transfer, dialogue, semantic parsing, automatic speech recognition, and so on. In addition, we also discuss potential directions for future exploration, including releasing the dependency of KD, reasonable training objectives, pre-training for NAR, and wider applications, etc. We hope this survey can help researchers capture the latest progress in NAR generation, inspire the design of advanced NAR models and algorithms, and enable industry practitioners to choose appropriate solutions for their applications.\n## Comparison\nModel Architecture. As for model architecture, both AT and NAT models take the encoder and decoder framework for translation. The encoder and decoder can be different neural networks, such as RNN [9], CNN [11], and Transformer [13]. Due to the superior performance of the Transformer network, we focus on the Transformer model for discussion in this survey. The encoder is used to encode the source sentences, while the decoder is utilized for decoding the target sentence. Compared to AT and NAT models, they adopt the same encoder architecture, and the differences are reflected in the decoders to match the specific training objective. (1) Specifically, AT models need to prevent earlier decoding steps from peeking at information from later steps. Therefore, the constraint of an autoregressive factorization of the output distribution is required, and they adopt the strict causal mask by applying a lower triangular matrix in the self-attention module of the conventional Transformer decoder [13]. (2) However, for NAT models, including the iteration-based NAT models, this constraint is no longer necessary, so they adopt the unmasked self-attention over all target tokens [16]. \n\n(3) As for SAT models, they adopt a coarse-grained lower triangular matrix as the causal mask, which means that they allow k tokens to peep later information in the same group while keeping the constraint between different groups. Inference Schedule. When going to the inference stage, the differences are as follows. (1) The AT models predict the target tokens in a one-by-one manner, and the tokens predicted previously are fed back into the decoder to generate the next token. (2) While SAT models predict a group of target tokens at one time, the previously generated groups of tokens are fed into the decoder to generate the next group of tokens, which is the same as the AT models. (3) For iterationbased NAT models, it needs k iterations for inference. The translated results of the previous iteration will be fed into the decoder again for refinements. (4) As for fully NAT models, they generate all predicted target tokens at only one step, which greatly speeds up inference.",
            "reference_string": "[248266379 | Xiao et al. | 2022 | Citations: 87]"
        },
        {
            "title": "CrisisSense-LLM: Instruction Fine-Tuned Large Language Model for Multi-label Social Media Text Classification in Disaster Informatics",
            "venue": "arXiv.org",
            "year": 2024,
            "reference_count": 37,
            "citation_count": 12,
            "influential_citation_count": 1,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2406.15477, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2265383225",
                    "name": "Kai Yin"
                },
                {
                    "authorId": "2308073678",
                    "name": "Chengkai Liu"
                },
                {
                    "authorId": "2258714985",
                    "name": "Ali Mostafavi"
                },
                {
                    "authorId": "2308068627",
                    "name": "Xia Hu"
                }
            ],
            "abstract": "In the field of crisis/disaster informatics, social media is increasingly being used for improving situational awareness to inform response and relief efforts. Efficient and accurate text classification tools have been a focal area of investigation in crisis informatics. However, current methods mostly rely on single-label text classification models, which fails to capture different insights embedded in dynamic and multifaceted disaster-related social media data. This study introduces a novel approach to disaster text classification by enhancing a pre-trained Large Language Model (LLM) through instruction fine-tuning targeted for multi-label classification of disaster-related tweets. Our methodology involves creating a comprehensive instruction dataset from disaster-related tweets, which is then used to fine-tune an open-source LLM, thereby embedding it with disaster-specific knowledge. This fine-tuned model can classify multiple aspects of disaster-related information simultaneously, such as the type of event, informativeness, and involvement of human aid, significantly improving the utility of social media data for situational awareness in disasters. The results demonstrate that this approach enhances the categorization of critical information from social media posts, thereby facilitating a more effective deployment for situational awareness during emergencies. This research paves the way for more advanced, adaptable, and robust disaster management tools, leveraging the capabilities of LLMs to improve real-time situational awareness and response strategies in disaster scenarios.",
            "corpus_id": 270702559,
            "sentences": [
                {
                    "corpus_id": "270702559",
                    "title": "CrisisSense-LLM: Instruction Fine-Tuned Large Language Model for Multi-label Social Media Text Classification in Disaster Informatics",
                    "text": "Typical architectures for LLMs can be categorized as encoder-decoder, causal decoder, and prefix decoder (Wang and Hesslow, 2022;Zhao et al., 2023).Among them, the causal decoder architecture is the most frequently used by various LLMs, such as OPT (Susan Zhang et al., 2023), LLAMA (Touvron et al., 2023a), BLOOM (Scao et al., 2023) due to its superior zero-shot and fewshot generalization capacity (Wang and Hesslow, 2022) and the effectiveness of scaling law (Brown et al., 2020;Kaplan et al., 2020).The causal decoder architecture employs a unidirectional attention mask, ensuring that each input token can only attend to preceding tokens and itself.Both input and output tokens are processed identically through the decoder (Zhao et al., 2023).\n\nThe prefix decoder architecture, also known as the non-causal decoder, modifies the masking mechanism used in causal decoders (Zhang et al., 2020).This adjustment allows for bidirectional attention over the prefix tokens while restricting attention to unidirectional generated tokens (Dong et al., 2019).Similar to the encoder-decoder architecture, prefix decoders can bidirectionally encode the prefix sequence and autoregressively predict the output tokens sequentially (Zhao et al., 2023).The same parameters are shared during both encoding and decoding processes.Representative LLMs based on prefix decoders include GLM-130B (Zeng et al., 2023) and U-PaLM (Tay et al., 2022).\n\nThe vanilla Transformer model is constructed on the encoder-decoder architecture (Vaswani et al., 2017), comprising two stacks of Transformer blocks designated as the encoder and decoder, respectively.The encoder utilizes stacked multi-head self-attention layers to encode the input sequence into its latent representations.Meanwhile, the decoder employs cross-attention mechanisms on these representations, and autoregressive generates the target sequence.",
                    "score": 0.5614064964146818,
                    "section_title": "Architecture of LLMs",
                    "char_start_offset": 7065,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 148
                        },
                        {
                            "start": 148,
                            "end": 503
                        },
                        {
                            "start": 503,
                            "end": 654
                        },
                        {
                            "start": 654,
                            "end": 749
                        },
                        {
                            "start": 751,
                            "end": 898
                        },
                        {
                            "start": 898,
                            "end": 1055
                        },
                        {
                            "start": 1055,
                            "end": 1243
                        },
                        {
                            "start": 1243,
                            "end": 1318
                        },
                        {
                            "start": 1318,
                            "end": 1430
                        },
                        {
                            "start": 1432,
                            "end": 1633
                        },
                        {
                            "start": 1633,
                            "end": 1756
                        },
                        {
                            "start": 1756,
                            "end": 1889
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 1513,
                            "end": 1535,
                            "matchedPaperCorpusId": "13756489"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.74267578125
                }
            ],
            "relevance_judgement": 0.74267578125,
            "relevance_judgment_input_expanded": "# Title: CrisisSense-LLM: Instruction Fine-Tuned Large Language Model for Multi-label Social Media Text Classification in Disaster Informatics\n# Venue: arXiv.org\n# Authors: Kai Yin, Chengkai Liu, Ali Mostafavi, Xia Hu\n## Abstract\nIn the field of crisis/disaster informatics, social media is increasingly being used for improving situational awareness to inform response and relief efforts. Efficient and accurate text classification tools have been a focal area of investigation in crisis informatics. However, current methods mostly rely on single-label text classification models, which fails to capture different insights embedded in dynamic and multifaceted disaster-related social media data. This study introduces a novel approach to disaster text classification by enhancing a pre-trained Large Language Model (LLM) through instruction fine-tuning targeted for multi-label classification of disaster-related tweets. Our methodology involves creating a comprehensive instruction dataset from disaster-related tweets, which is then used to fine-tune an open-source LLM, thereby embedding it with disaster-specific knowledge. This fine-tuned model can classify multiple aspects of disaster-related information simultaneously, such as the type of event, informativeness, and involvement of human aid, significantly improving the utility of social media data for situational awareness in disasters. The results demonstrate that this approach enhances the categorization of critical information from social media posts, thereby facilitating a more effective deployment for situational awareness during emergencies. This research paves the way for more advanced, adaptable, and robust disaster management tools, leveraging the capabilities of LLMs to improve real-time situational awareness and response strategies in disaster scenarios.\n## Architecture of LLMs\nTypical architectures for LLMs can be categorized as encoder-decoder, causal decoder, and prefix decoder (Wang and Hesslow, 2022;Zhao et al., 2023).Among them, the causal decoder architecture is the most frequently used by various LLMs, such as OPT (Susan Zhang et al., 2023), LLAMA (Touvron et al., 2023a), BLOOM (Scao et al., 2023) due to its superior zero-shot and fewshot generalization capacity (Wang and Hesslow, 2022) and the effectiveness of scaling law (Brown et al., 2020;Kaplan et al., 2020).The causal decoder architecture employs a unidirectional attention mask, ensuring that each input token can only attend to preceding tokens and itself.Both input and output tokens are processed identically through the decoder (Zhao et al., 2023).\n\nThe prefix decoder architecture, also known as the non-causal decoder, modifies the masking mechanism used in causal decoders (Zhang et al., 2020).This adjustment allows for bidirectional attention over the prefix tokens while restricting attention to unidirectional generated tokens (Dong et al., 2019).Similar to the encoder-decoder architecture, prefix decoders can bidirectionally encode the prefix sequence and autoregressively predict the output tokens sequentially (Zhao et al., 2023).The same parameters are shared during both encoding and decoding processes.Representative LLMs based on prefix decoders include GLM-130B (Zeng et al., 2023) and U-PaLM (Tay et al., 2022).\n\nThe vanilla Transformer model is constructed on the encoder-decoder architecture (Vaswani et al., 2017), comprising two stacks of Transformer blocks designated as the encoder and decoder, respectively.The encoder utilizes stacked multi-head self-attention layers to encode the input sequence into its latent representations.Meanwhile, the decoder employs cross-attention mechanisms on these representations, and autoregressive generates the target sequence.",
            "reference_string": "[270702559 | Yin et al. | 2024 | Citations: 12]"
        },
        {
            "title": "BLOOM: A 176B-Parameter Open-Access Multilingual Language Model",
            "venue": "arXiv.org",
            "year": 2022,
            "reference_count": 172,
            "citation_count": 2393,
            "influential_citation_count": 230,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "http://arxiv.org/pdf/2211.05100",
                "status": "GREEN",
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2211.05100, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "1379806208",
                    "name": "Teven Le Scao"
                },
                {
                    "authorId": "144270981",
                    "name": "Angela Fan"
                },
                {
                    "authorId": "2003696840",
                    "name": "Christopher Akiki"
                },
                {
                    "authorId": "2949185",
                    "name": "Ellie Pavlick"
                },
                {
                    "authorId": "2066663381",
                    "name": "Suzana Ili'c"
                },
                {
                    "authorId": "80424302",
                    "name": "Daniel Hesslow"
                },
                {
                    "authorId": "2190282134",
                    "name": "Roman Castagn'e"
                },
                {
                    "authorId": "2993731",
                    "name": "A. Luccioni"
                },
                {
                    "authorId": "1846431",
                    "name": "Fran\u00e7ois Yvon"
                },
                {
                    "authorId": "2907260",
                    "name": "Matthias Gall\u00e9"
                },
                {
                    "authorId": "50195579",
                    "name": "J. Tow"
                },
                {
                    "authorId": "2531268",
                    "name": "Alexander M. Rush"
                },
                {
                    "authorId": "103476203",
                    "name": "Stella Biderman"
                },
                {
                    "authorId": "1991019030",
                    "name": "Albert Webson"
                },
                {
                    "authorId": "1451644426",
                    "name": "Pawan Sasanka Ammanamanchi"
                },
                {
                    "authorId": "2135734748",
                    "name": "Thomas Wang"
                },
                {
                    "authorId": "68990982",
                    "name": "Beno\u00eet Sagot"
                },
                {
                    "authorId": "2037383772",
                    "name": "Niklas Muennighoff"
                },
                {
                    "authorId": "46219923",
                    "name": "Albert Villanova del Moral"
                },
                {
                    "authorId": "2537545",
                    "name": "Olatunji Ruwase"
                },
                {
                    "authorId": "48983885",
                    "name": "Rachel Bawden"
                },
                {
                    "authorId": "32136590",
                    "name": "Stas Bekman"
                },
                {
                    "authorId": "1584940075",
                    "name": "Angelina McMillan-Major"
                },
                {
                    "authorId": "46181066",
                    "name": "Iz Beltagy"
                },
                {
                    "authorId": "2168170616",
                    "name": "Huu Nguyen"
                },
                {
                    "authorId": "2113836860",
                    "name": "Lucile Saulnier"
                },
                {
                    "authorId": "145814654",
                    "name": "Samson Tan"
                },
                {
                    "authorId": "147846651",
                    "name": "Pedro Ortiz Suarez"
                },
                {
                    "authorId": "2285868436",
                    "name": "Victor Sanh"
                },
                {
                    "authorId": "2172404846",
                    "name": "Hugo Laurenccon"
                },
                {
                    "authorId": "2262249",
                    "name": "Yacine Jernite"
                },
                {
                    "authorId": "143945447",
                    "name": "Julien Launay"
                },
                {
                    "authorId": "49501003",
                    "name": "Margaret Mitchell"
                },
                {
                    "authorId": "2402716",
                    "name": "Colin Raffel"
                },
                {
                    "authorId": "2273789852",
                    "name": "Aaron Gokaslan"
                },
                {
                    "authorId": "2183598223",
                    "name": "Adi Simhi"
                },
                {
                    "authorId": "2078619062",
                    "name": "Aitor Soroa Etxabe"
                },
                {
                    "authorId": "8129718",
                    "name": "Alham Fikri Aji"
                },
                {
                    "authorId": "73769093",
                    "name": "Amit Alfassy"
                },
                {
                    "authorId": "145046059",
                    "name": "Anna Rogers"
                },
                {
                    "authorId": "2190281124",
                    "name": "Ariel Kreisberg Nitzav"
                },
                {
                    "authorId": "66247317",
                    "name": "Canwen Xu"
                },
                {
                    "authorId": "35966970",
                    "name": "Chenghao Mou"
                },
                {
                    "authorId": "1591176064",
                    "name": "Chris C. Emezue"
                },
                {
                    "authorId": "2261291789",
                    "name": "Christopher Klamm"
                },
                {
                    "authorId": "89269402",
                    "name": "Colin Leong"
                },
                {
                    "authorId": "71075073",
                    "name": "Daniel Alexander van Strien"
                },
                {
                    "authorId": "2518906",
                    "name": "David Ifeoluwa Adelani"
                },
                {
                    "authorId": "9215251",
                    "name": "Dragomir R. Radev"
                },
                {
                    "authorId": "79512668",
                    "name": "E. G. Ponferrada"
                },
                {
                    "authorId": "2190281122",
                    "name": "Efrat Levkovizh"
                },
                {
                    "authorId": "2047591327",
                    "name": "Ethan Kim"
                },
                {
                    "authorId": "2088048322",
                    "name": "Eyal Natan"
                },
                {
                    "authorId": "2067891070",
                    "name": "F. Toni"
                },
                {
                    "authorId": "13656138",
                    "name": "G\u00e9rard Dupont"
                },
                {
                    "authorId": "2067996",
                    "name": "Germ\u00e1n Kruszewski"
                },
                {
                    "authorId": "2158858559",
                    "name": "Giada Pistilli"
                },
                {
                    "authorId": "2218938",
                    "name": "Hady ElSahar"
                },
                {
                    "authorId": "90563027",
                    "name": "Hamza Benyamina"
                },
                {
                    "authorId": "2057078797",
                    "name": "H. Tran"
                },
                {
                    "authorId": "47948569",
                    "name": "Ian Yu"
                },
                {
                    "authorId": "1429833598",
                    "name": "Idris Abdulmumin"
                },
                {
                    "authorId": "2060080508",
                    "name": "Isaac Johnson"
                },
                {
                    "authorId": "1404791152",
                    "name": "Itziar Gonzalez-Dios"
                },
                {
                    "authorId": "144979591",
                    "name": "Javier de la Rosa"
                },
                {
                    "authorId": "2164872258",
                    "name": "Jenny Chim"
                },
                {
                    "authorId": "34176020",
                    "name": "Jesse Dodge"
                },
                {
                    "authorId": "144549416",
                    "name": "Jian Zhu"
                },
                {
                    "authorId": "2116123009",
                    "name": "Jonathan Chang"
                },
                {
                    "authorId": "2146695800",
                    "name": "Jorg Frohberg"
                },
                {
                    "authorId": "2094755167",
                    "name": "Josephine Tobing"
                },
                {
                    "authorId": "143779690",
                    "name": "J. Bhattacharjee"
                },
                {
                    "authorId": "90615055",
                    "name": "Khalid Almubarak"
                },
                {
                    "authorId": "2157630500",
                    "name": "Kimbo Chen"
                },
                {
                    "authorId": "46258841",
                    "name": "Kyle Lo"
                },
                {
                    "authorId": "51128119",
                    "name": "L. V. Werra"
                },
                {
                    "authorId": "20308468",
                    "name": "Leon Weber"
                },
                {
                    "authorId": null,
                    "name": "Long Phan"
                },
                {
                    "authorId": "2190281230",
                    "name": "Loubna Ben Allal"
                },
                {
                    "authorId": "77970446",
                    "name": "Ludovic Tanguy"
                },
                {
                    "authorId": "1879591269",
                    "name": "Manan Dey"
                },
                {
                    "authorId": "115568186",
                    "name": "M. Mu\u00f1oz"
                },
                {
                    "authorId": "153528116",
                    "name": "Maraim Masoud"
                },
                {
                    "authorId": "2176184513",
                    "name": "Mar\u00eda Grandury"
                },
                {
                    "authorId": "2125821515",
                    "name": "Mario vSavsko"
                },
                {
                    "authorId": "2112504552",
                    "name": "Max Huang"
                },
                {
                    "authorId": "3443469",
                    "name": "Maximin Coavoux"
                },
                {
                    "authorId": "145431050",
                    "name": "Mayank Singh"
                },
                {
                    "authorId": "5745221",
                    "name": "Mike Tian-Jian Jiang"
                },
                {
                    "authorId": "1484109150",
                    "name": "Minh Chien Vu"
                },
                {
                    "authorId": "2097304324",
                    "name": "M. A. Jauhar"
                },
                {
                    "authorId": "2721586",
                    "name": "Mustafa Ghaleb"
                },
                {
                    "authorId": "34202134",
                    "name": "Nishant Subramani"
                },
                {
                    "authorId": "9529535",
                    "name": "Nora Kassner"
                },
                {
                    "authorId": "37441312",
                    "name": "Nurulaqilla Khamis"
                },
                {
                    "authorId": "2089233725",
                    "name": "Olivier Nguyen"
                },
                {
                    "authorId": "2190280842",
                    "name": "Omar Espejel"
                },
                {
                    "authorId": "51436367",
                    "name": "Ona de Gibert"
                },
                {
                    "authorId": "2176184659",
                    "name": "Paulo Villegas"
                },
                {
                    "authorId": "2071773966",
                    "name": "Peter Henderson"
                },
                {
                    "authorId": "46985469",
                    "name": "Pierre Colombo"
                },
                {
                    "authorId": "2190281321",
                    "name": "Priscilla Amuok"
                },
                {
                    "authorId": "2113836945",
                    "name": "Quentin Lhoest"
                },
                {
                    "authorId": "80858030",
                    "name": "Rheza Harliman"
                },
                {
                    "authorId": "150272855",
                    "name": "Rishi Bommasani"
                },
                {
                    "authorId": "116000979",
                    "name": "R. L'opez"
                },
                {
                    "authorId": null,
                    "name": "Rui Ribeiro"
                },
                {
                    "authorId": "1486204986",
                    "name": "Salomey Osei"
                },
                {
                    "authorId": "1708916",
                    "name": "S. Pyysalo"
                },
                {
                    "authorId": "47351277",
                    "name": "Sebastian Nagel"
                },
                {
                    "authorId": "2795685",
                    "name": "Shamik Bose"
                },
                {
                    "authorId": "7744881",
                    "name": "Shamsuddeen Hassan Muhammad"
                },
                {
                    "authorId": "1409842673",
                    "name": "S. Sharma"
                },
                {
                    "authorId": "29909347",
                    "name": "S. Longpre"
                },
                {
                    "authorId": "2099315138",
                    "name": "Somaieh Nikpoor"
                },
                {
                    "authorId": "82674724",
                    "name": "S. Silberberg"
                },
                {
                    "authorId": "2053516473",
                    "name": "S. Pai"
                },
                {
                    "authorId": "2074482488",
                    "name": "S. Zink"
                },
                {
                    "authorId": "46308692",
                    "name": "Tiago Timponi Torrent"
                },
                {
                    "authorId": "32246932",
                    "name": "Timo Schick"
                },
                {
                    "authorId": "1500242049",
                    "name": "Tristan Thrush"
                },
                {
                    "authorId": "3382327",
                    "name": "V. Danchev"
                },
                {
                    "authorId": "2841761",
                    "name": "Vassilina Nikoulina"
                },
                {
                    "authorId": "1796619",
                    "name": "Veronika Laippala"
                },
                {
                    "authorId": "2190280574",
                    "name": "Violette Lepercq"
                },
                {
                    "authorId": "2059767242",
                    "name": "V. Prabhu"
                },
                {
                    "authorId": "25098419",
                    "name": "Zaid Alyafeai"
                },
                {
                    "authorId": "2138053020",
                    "name": "Zeerak Talat"
                },
                {
                    "authorId": "2048082186",
                    "name": "Arun Raja"
                },
                {
                    "authorId": "2266692",
                    "name": "Benjamin Heinzerling"
                },
                {
                    "authorId": "152358188",
                    "name": "Chenglei Si"
                },
                {
                    "authorId": "3448427",
                    "name": "Elizabeth Salesky"
                },
                {
                    "authorId": "27689253",
                    "name": "Sabrina J. Mielke"
                },
                {
                    "authorId": "2183377987",
                    "name": "Wilson Y. Lee"
                },
                {
                    "authorId": "2051500420",
                    "name": "Abheesht Sharma"
                },
                {
                    "authorId": "2065039862",
                    "name": "Andrea Santilli"
                },
                {
                    "authorId": "2129106958",
                    "name": "Antoine Chaffin"
                },
                {
                    "authorId": "114762823",
                    "name": "Arnaud Stiegler"
                },
                {
                    "authorId": "2852125",
                    "name": "Debajyoti Datta"
                },
                {
                    "authorId": "50812522",
                    "name": "Eliza Szczechla"
                },
                {
                    "authorId": "1509809381",
                    "name": "Gunjan Chhablani"
                },
                {
                    "authorId": "144407394",
                    "name": "Han Wang"
                },
                {
                    "authorId": "144834468",
                    "name": "Harshit Pandey"
                },
                {
                    "authorId": "2879705",
                    "name": "Hendrik Strobelt"
                },
                {
                    "authorId": "31592365",
                    "name": "Jason Alan Fries"
                },
                {
                    "authorId": "120419790",
                    "name": "Jos Rozen"
                },
                {
                    "authorId": "2027599537",
                    "name": "Leo Gao"
                },
                {
                    "authorId": "35566806",
                    "name": "Lintang Sutawika"
                },
                {
                    "authorId": "31773000",
                    "name": "M Saiful Bari"
                },
                {
                    "authorId": "1752627730",
                    "name": "Maged S. Al-Shaibani"
                },
                {
                    "authorId": "35904689",
                    "name": "Matteo Manica"
                },
                {
                    "authorId": "22209084",
                    "name": "Nihal V. Nayak"
                },
                {
                    "authorId": "2131107966",
                    "name": "Ryan Teehan"
                },
                {
                    "authorId": "7641268",
                    "name": "Samuel Albanie"
                },
                {
                    "authorId": "2191455",
                    "name": "Sheng Shen"
                },
                {
                    "authorId": "2152318619",
                    "name": "Srulik Ben-David"
                },
                {
                    "authorId": "2870504",
                    "name": "Stephen H. Bach"
                },
                {
                    "authorId": "2111181991",
                    "name": "Taewoon Kim"
                },
                {
                    "authorId": "94251255",
                    "name": "T. Bers"
                },
                {
                    "authorId": "79215748",
                    "name": "Thibault F\u00e9vry"
                },
                {
                    "authorId": "10729963",
                    "name": "Trishala Neeraj"
                },
                {
                    "authorId": "70296695",
                    "name": "Urmish Thakker"
                },
                {
                    "authorId": "24025563",
                    "name": "Vikas Raunak"
                },
                {
                    "authorId": "2118488348",
                    "name": "Xiang Tang"
                },
                {
                    "authorId": "1725420331",
                    "name": "Zheng-Xin Yong"
                },
                {
                    "authorId": "48064856",
                    "name": "Zhiqing Sun"
                },
                {
                    "authorId": "1720739223",
                    "name": "Shaked Brody"
                },
                {
                    "authorId": "2101395835",
                    "name": "Y. Uri"
                },
                {
                    "authorId": "2190280874",
                    "name": "Hadar Tojarieh"
                },
                {
                    "authorId": "145625142",
                    "name": "Adam Roberts"
                },
                {
                    "authorId": "3351938",
                    "name": "Hyung Won Chung"
                },
                {
                    "authorId": "2112211652",
                    "name": "Jaesung Tae"
                },
                {
                    "authorId": "80842917",
                    "name": "Jason Phang"
                },
                {
                    "authorId": "40170001",
                    "name": "Ofir Press"
                },
                {
                    "authorId": "2609325",
                    "name": "Conglong Li"
                },
                {
                    "authorId": "22252150",
                    "name": "D. Narayanan"
                },
                {
                    "authorId": "2190280830",
                    "name": "Hatim Bourfoune"
                },
                {
                    "authorId": "48991386",
                    "name": "J. Casper"
                },
                {
                    "authorId": "3299496",
                    "name": "Jeff Rasley"
                },
                {
                    "authorId": "1491753352",
                    "name": "Max Ryabinin"
                },
                {
                    "authorId": "1381446720",
                    "name": "Mayank Mishra"
                },
                {
                    "authorId": "67016465",
                    "name": "Minjia Zhang"
                },
                {
                    "authorId": "1911755",
                    "name": "M. Shoeybi"
                },
                {
                    "authorId": "31758637",
                    "name": "Myriam Peyrounette"
                },
                {
                    "authorId": "31614549",
                    "name": "N. Patry"
                },
                {
                    "authorId": "2179884903",
                    "name": "Nouamane Tazi"
                },
                {
                    "authorId": "2186979509",
                    "name": "Omar Sanseviero"
                },
                {
                    "authorId": "138609838",
                    "name": "Patrick von Platen"
                },
                {
                    "authorId": "2190281218",
                    "name": "Pierre Cornette"
                },
                {
                    "authorId": "2190280981",
                    "name": "Pierre Franccois Lavall'ee"
                },
                {
                    "authorId": "31734741",
                    "name": "R. Lacroix"
                },
                {
                    "authorId": "32817044",
                    "name": "Samyam Rajbhandari"
                },
                {
                    "authorId": "2188737826",
                    "name": "Sanchit Gandhi"
                },
                {
                    "authorId": "2110486618",
                    "name": "Shaden Smith"
                },
                {
                    "authorId": "2293408",
                    "name": "S. Requena"
                },
                {
                    "authorId": "2147312210",
                    "name": "Suraj Patil"
                },
                {
                    "authorId": "3239480",
                    "name": "Tim Dettmers"
                },
                {
                    "authorId": "114850513",
                    "name": "Ahmed Baruwa"
                },
                {
                    "authorId": null,
                    "name": "Amanpreet Singh"
                },
                {
                    "authorId": "2190281235",
                    "name": "Anastasia Cheveleva"
                },
                {
                    "authorId": "1769176",
                    "name": "Anne-Laure Ligozat"
                },
                {
                    "authorId": "1677386832",
                    "name": "Arjun Subramonian"
                },
                {
                    "authorId": "2190281078",
                    "name": "Aur'elie N'ev'eol"
                },
                {
                    "authorId": "10727711",
                    "name": "Charles Lovering"
                },
                {
                    "authorId": "2758616",
                    "name": "Dan Garrette"
                },
                {
                    "authorId": "70209311",
                    "name": "D. Tunuguntla"
                },
                {
                    "authorId": "144568312",
                    "name": "Ehud Reiter"
                },
                {
                    "authorId": "2051713939",
                    "name": "Ekaterina Taktasheva"
                },
                {
                    "authorId": "2135526571",
                    "name": "E. Voloshina"
                },
                {
                    "authorId": "2158860079",
                    "name": "Eli Bogdanov"
                },
                {
                    "authorId": "9162688",
                    "name": "Genta Indra Winata"
                },
                {
                    "authorId": "2184031883",
                    "name": "Hailey Schoelkopf"
                },
                {
                    "authorId": "3245041",
                    "name": "Jan-Christoph Kalo"
                },
                {
                    "authorId": "2848048",
                    "name": "Jekaterina Novikova"
                },
                {
                    "authorId": "39774809",
                    "name": "J. Forde"
                },
                {
                    "authorId": "47274259",
                    "name": "Xiangru Tang"
                },
                {
                    "authorId": "11348687",
                    "name": "Jungo Kasai"
                },
                {
                    "authorId": "50106621",
                    "name": "Ken Kawamura"
                },
                {
                    "authorId": "2047711867",
                    "name": "Liam Hazan"
                },
                {
                    "authorId": "2954727",
                    "name": "Marine Carpuat"
                },
                {
                    "authorId": "2029314697",
                    "name": "Miruna Clinciu"
                },
                {
                    "authorId": "8756748",
                    "name": "Najoung Kim"
                },
                {
                    "authorId": "15590401",
                    "name": "Newton Cheng"
                },
                {
                    "authorId": "1799401599",
                    "name": "Oleg Serikov"
                },
                {
                    "authorId": "2132545395",
                    "name": "Omer Antverg"
                },
                {
                    "authorId": "1986356851",
                    "name": "Oskar van der Wal"
                },
                {
                    "authorId": "15176410",
                    "name": "Rui Zhang"
                },
                {
                    "authorId": "49775305",
                    "name": "Ruochen Zhang"
                },
                {
                    "authorId": "3159346",
                    "name": "Sebastian Gehrmann"
                },
                {
                    "authorId": "8963527",
                    "name": "Shachar Mirkin"
                },
                {
                    "authorId": "3097741",
                    "name": "S. Pais"
                },
                {
                    "authorId": "2134610800",
                    "name": "Tatiana Shavrina"
                },
                {
                    "authorId": "90745780",
                    "name": "Thomas Scialom"
                },
                {
                    "authorId": "2127600348",
                    "name": "Tian Yun"
                },
                {
                    "authorId": "1666636295",
                    "name": "Tomasz Limisiewicz"
                },
                {
                    "authorId": "1681799",
                    "name": "Verena Rieser"
                },
                {
                    "authorId": "2135362820",
                    "name": "Vitaly Protasov"
                },
                {
                    "authorId": "51259225",
                    "name": "V. Mikhailov"
                },
                {
                    "authorId": "100984698",
                    "name": "Yada Pruksachatkun"
                },
                {
                    "authorId": "2083259",
                    "name": "Yonatan Belinkov"
                },
                {
                    "authorId": "2190281071",
                    "name": "Zachary Bamberger"
                },
                {
                    "authorId": "2343772132",
                    "name": "Zden\u02c7ek Kasner"
                },
                {
                    "authorId": "1805991958",
                    "name": "Zden\u011bk Kasner"
                },
                {
                    "authorId": "2190281954",
                    "name": "A. Pestana"
                },
                {
                    "authorId": "15845853",
                    "name": "A. Feizpour"
                },
                {
                    "authorId": "2190399370",
                    "name": "Ammar Khan"
                },
                {
                    "authorId": "2190281566",
                    "name": "Amy Faranak"
                },
                {
                    "authorId": "2148971654",
                    "name": "A. Santos"
                },
                {
                    "authorId": "1739149407",
                    "name": "Anthony Hevia"
                },
                {
                    "authorId": "2190281069",
                    "name": "Antigona Unldreaj"
                },
                {
                    "authorId": "115638227",
                    "name": "Arash Aghagol"
                },
                {
                    "authorId": "2361305",
                    "name": "Arezoo Abdollahi"
                },
                {
                    "authorId": "101302626",
                    "name": "A. Tammour"
                },
                {
                    "authorId": "3110645",
                    "name": "A. HajiHosseini"
                },
                {
                    "authorId": "2190281564",
                    "name": "Bahareh Behroozi"
                },
                {
                    "authorId": "83263885",
                    "name": "Benjamin Ayoade Ajibade"
                },
                {
                    "authorId": "31577522",
                    "name": "B. Saxena"
                },
                {
                    "authorId": "2005399190",
                    "name": "Carlos Mu\u00f1oz Ferrandis"
                },
                {
                    "authorId": "2075459",
                    "name": "Danish Contractor"
                },
                {
                    "authorId": "144635557",
                    "name": "D. Lansky"
                },
                {
                    "authorId": "2058260775",
                    "name": "Davis David"
                },
                {
                    "authorId": "2111313627",
                    "name": "Douwe Kiela"
                },
                {
                    "authorId": "5943347",
                    "name": "D. A. Nguyen"
                },
                {
                    "authorId": "47654100",
                    "name": "Edward Tan"
                },
                {
                    "authorId": "2026649806",
                    "name": "Emi Baylor"
                },
                {
                    "authorId": "2190281502",
                    "name": "Ezinwanne Ozoani"
                },
                {
                    "authorId": "35330153",
                    "name": "F. Mirza"
                },
                {
                    "authorId": "2190281922",
                    "name": "Frankline Ononiwu"
                },
                {
                    "authorId": "123343513",
                    "name": "Habib Rezanejad"
                },
                {
                    "authorId": "2119822136",
                    "name": "H.A. Jones"
                },
                {
                    "authorId": "2105001416",
                    "name": "Indrani Bhattacharya"
                },
                {
                    "authorId": "1404060690",
                    "name": "Irene Solaiman"
                },
                {
                    "authorId": "2190281379",
                    "name": "Irina Sedenko"
                },
                {
                    "authorId": "3163125",
                    "name": "Isar Nejadgholi"
                },
                {
                    "authorId": "145629075",
                    "name": "J. Passmore"
                },
                {
                    "authorId": "150162316",
                    "name": "Joshua Seltzer"
                },
                {
                    "authorId": "97979993",
                    "name": "Julio Bonis Sanz"
                },
                {
                    "authorId": null,
                    "name": "Karen Fort"
                },
                {
                    "authorId": "3530609",
                    "name": "L\u00edvia Dutra"
                },
                {
                    "authorId": "2190281373",
                    "name": "Mairon Samagaio"
                },
                {
                    "authorId": "2190281500",
                    "name": "Maraim Elbadri"
                },
                {
                    "authorId": "2921990",
                    "name": "Margot Mieskes"
                },
                {
                    "authorId": "151492708",
                    "name": "Marissa Gerchick"
                },
                {
                    "authorId": "2190281205",
                    "name": "Martha Akinlolu"
                },
                {
                    "authorId": "2060092577",
                    "name": "Michael McKenna"
                },
                {
                    "authorId": "2056851511",
                    "name": "Mike Qiu"
                },
                {
                    "authorId": "144449938",
                    "name": "M. Ghauri"
                },
                {
                    "authorId": "2190281203",
                    "name": "Mykola Burynok"
                },
                {
                    "authorId": "1401945312",
                    "name": "Nafis Abrar"
                },
                {
                    "authorId": "8937909",
                    "name": "Nazneen Rajani"
                },
                {
                    "authorId": "2190281555",
                    "name": "Nour Elkott"
                },
                {
                    "authorId": "1992948200",
                    "name": "N. Fahmy"
                },
                {
                    "authorId": "2164156047",
                    "name": "Olanrewaju Samuel"
                },
                {
                    "authorId": "2061141169",
                    "name": "Ran An"
                },
                {
                    "authorId": "9294251",
                    "name": "R. Kromann"
                },
                {
                    "authorId": "2137183106",
                    "name": "Ryan Hao"
                },
                {
                    "authorId": "4279554",
                    "name": "S. Alizadeh"
                },
                {
                    "authorId": "2190281531",
                    "name": "Sarmad Shubber"
                },
                {
                    "authorId": "2116420702",
                    "name": "Silas L. Wang"
                },
                {
                    "authorId": "2109853801",
                    "name": "Sourav Roy"
                },
                {
                    "authorId": "10726201",
                    "name": "S. Viguier"
                },
                {
                    "authorId": "2153620715",
                    "name": "Thanh-Cong Le"
                },
                {
                    "authorId": "2190281729",
                    "name": "Tobi Oyebade"
                },
                {
                    "authorId": "2153620985",
                    "name": "T. Le"
                },
                {
                    "authorId": "2190429590",
                    "name": "Yoyo Yang"
                },
                {
                    "authorId": "2297189567",
                    "name": "Zach Nguyen"
                },
                {
                    "authorId": "41124383",
                    "name": "Abhinav Ramesh Kashyap"
                },
                {
                    "authorId": "2318515251",
                    "name": "A. Palasciano"
                },
                {
                    "authorId": "2840689",
                    "name": "A. Callahan"
                },
                {
                    "authorId": "2042747208",
                    "name": "Anima Shukla"
                },
                {
                    "authorId": "1414073449",
                    "name": "Antonio Miranda-Escalada"
                },
                {
                    "authorId": "2110183222",
                    "name": "A. Singh"
                },
                {
                    "authorId": "1379935164",
                    "name": "Benjamin Beilharz"
                },
                {
                    "authorId": "2165371942",
                    "name": "Bo Wang"
                },
                {
                    "authorId": "144972524",
                    "name": "C. Brito"
                },
                {
                    "authorId": "2111169784",
                    "name": "Chenxi Zhou"
                },
                {
                    "authorId": "50732716",
                    "name": "Chirag Jain"
                },
                {
                    "authorId": "2158158973",
                    "name": "Chuxin Xu"
                },
                {
                    "authorId": "2080941785",
                    "name": "Cl\u00e9mentine Fourrier"
                },
                {
                    "authorId": "2174177869",
                    "name": "Daniel Le'on Perin'an"
                },
                {
                    "authorId": "2082057793",
                    "name": "Daniel Molano"
                },
                {
                    "authorId": "150978762",
                    "name": "Dian Yu"
                },
                {
                    "authorId": "24907368",
                    "name": "Enrique Manjavacas"
                },
                {
                    "authorId": "2139792578",
                    "name": "Fabio Barth"
                },
                {
                    "authorId": "2190281754",
                    "name": "Florian Fuhrimann"
                },
                {
                    "authorId": "2165227550",
                    "name": "Gabriel Altay"
                },
                {
                    "authorId": "2166224123",
                    "name": "Giyaseddin Bayrak"
                },
                {
                    "authorId": null,
                    "name": "Gully Burns"
                },
                {
                    "authorId": "88811067",
                    "name": "Helena U. Vrabec"
                },
                {
                    "authorId": "121044523",
                    "name": "I. Bello"
                },
                {
                    "authorId": "93460753",
                    "name": "Isha Dash"
                },
                {
                    "authorId": "72725318",
                    "name": "J. Kang"
                },
                {
                    "authorId": "37585306",
                    "name": "John Giorgi"
                },
                {
                    "authorId": "144983077",
                    "name": "Jonas Golde"
                },
                {
                    "authorId": "2066514466",
                    "name": "J. Posada"
                },
                {
                    "authorId": "1601562797",
                    "name": "Karthi Sivaraman"
                },
                {
                    "authorId": "2190281314",
                    "name": "Lokesh Bulchandani"
                },
                {
                    "authorId": "2145287083",
                    "name": "Lu Liu"
                },
                {
                    "authorId": "2100596120",
                    "name": "Luisa Shinzato"
                },
                {
                    "authorId": "2190281960",
                    "name": "Madeleine Hahn de Bykhovetz"
                },
                {
                    "authorId": "2068853922",
                    "name": "Maiko Takeuchi"
                },
                {
                    "authorId": "1850527789",
                    "name": "Marc P\u00e0mies"
                },
                {
                    "authorId": "87956698",
                    "name": "M. A. Castillo"
                },
                {
                    "authorId": "2174178585",
                    "name": "Marianna Nezhurina"
                },
                {
                    "authorId": "1879523878",
                    "name": "Mario Sanger"
                },
                {
                    "authorId": "3004898",
                    "name": "M. Samwald"
                },
                {
                    "authorId": "120397552",
                    "name": "Michael Cullan"
                },
                {
                    "authorId": "50564168",
                    "name": "Michael Weinberg"
                },
                {
                    "authorId": "2072502429",
                    "name": "M. Wolf"
                },
                {
                    "authorId": "2190280864",
                    "name": "Mina Mihaljcic"
                },
                {
                    "authorId": "2112211627",
                    "name": "Minna Liu"
                },
                {
                    "authorId": "1397064923",
                    "name": "M. Freidank"
                },
                {
                    "authorId": "4981508",
                    "name": "Myungsun Kang"
                },
                {
                    "authorId": "12046785",
                    "name": "Natasha Seelam"
                },
                {
                    "authorId": "48948105",
                    "name": "N. Dahlberg"
                },
                {
                    "authorId": "40208102",
                    "name": "N. Broad"
                },
                {
                    "authorId": "70256289",
                    "name": "N. Muellner"
                },
                {
                    "authorId": "40539650",
                    "name": "Pascale Fung"
                },
                {
                    "authorId": "2097023671",
                    "name": "Patricia Haller"
                },
                {
                    "authorId": "2298902857",
                    "name": "Patrick Haller"
                },
                {
                    "authorId": "115525190",
                    "name": "R. Eisenberg"
                },
                {
                    "authorId": "2111138678",
                    "name": "Robert Martin"
                },
                {
                    "authorId": "2291171257",
                    "name": "Rodrigo Canalli"
                },
                {
                    "authorId": "2190282202",
                    "name": "Rosaline Su"
                },
                {
                    "authorId": "153083809",
                    "name": "Ruisi Su"
                },
                {
                    "authorId": "66986482",
                    "name": "Samuel Cahyawijaya"
                },
                {
                    "authorId": "51878929",
                    "name": "Samuele Garda"
                },
                {
                    "authorId": "2174177330",
                    "name": "Shlok S Deshmukh"
                },
                {
                    "authorId": "2112134590",
                    "name": "Shubhanshu Mishra"
                },
                {
                    "authorId": "39620434",
                    "name": "Sid Kiblawi"
                },
                {
                    "authorId": "119994729",
                    "name": "Simon Ott"
                },
                {
                    "authorId": "2190281679",
                    "name": "Sinee Sang-aroonsiri"
                },
                {
                    "authorId": "120438284",
                    "name": "Srishti Kumar"
                },
                {
                    "authorId": "134757625",
                    "name": "Stefan Schweter"
                },
                {
                    "authorId": "8723233",
                    "name": "S. Bharati"
                },
                {
                    "authorId": "103242455",
                    "name": "Tanmay Laud"
                },
                {
                    "authorId": "2174176862",
                    "name": "Th\u00e9o Gigant"
                },
                {
                    "authorId": "2190281376",
                    "name": "Tomoya Kainuma"
                },
                {
                    "authorId": "50320098",
                    "name": "Wojciech Kusa"
                },
                {
                    "authorId": "2139767217",
                    "name": "Yanis Labrak"
                },
                {
                    "authorId": "1572961212",
                    "name": "Yashasvi Bajaj"
                },
                {
                    "authorId": "2051879548",
                    "name": "Y. Venkatraman"
                },
                {
                    "authorId": "2110154622",
                    "name": "Yifan Xu"
                },
                {
                    "authorId": "2118670234",
                    "name": "Ying Xu"
                },
                {
                    "authorId": "2142717873",
                    "name": "Yu Xu"
                },
                {
                    "authorId": "1643680733",
                    "name": "Z. Tan"
                },
                {
                    "authorId": "79110285",
                    "name": "Zhongli Xie"
                },
                {
                    "authorId": "2114134227",
                    "name": "Zifan Ye"
                },
                {
                    "authorId": "2065370401",
                    "name": "M. Bras"
                },
                {
                    "authorId": "2037496520",
                    "name": "Younes Belkada"
                },
                {
                    "authorId": "50335211",
                    "name": "Thomas Wolf"
                }
            ],
            "abstract": "Large language models (LLMs) have been shown to be able to perform new tasks based on a few demonstrations or natural language instructions. While these capabilities have led to widespread adoption, most LLMs are developed by resource-rich organizations and are frequently kept from the public. As a step towards democratizing this powerful technology, we present BLOOM, a 176B-parameter open-access language model designed and built thanks to a collaboration of hundreds of researchers. BLOOM is a decoder-only Transformer language model that was trained on the ROOTS corpus, a dataset comprising hundreds of sources in 46 natural and 13 programming languages (59 in total). We find that BLOOM achieves competitive performance on a wide variety of benchmarks, with stronger results after undergoing multitask prompted finetuning. To facilitate future research and applications using LLMs, we publicly release our models and code under the Responsible AI License.",
            "corpus_id": 253420279,
            "sentences": [
                {
                    "corpus_id": "253420279",
                    "title": "BLOOM: A 176B-Parameter Open-Access Multilingual Language Model",
                    "text": "Although most modern language models are based on the Transformer architecture, there are significant deviations between architectural implementations. Notably, while the original Transformer is based on an encoder-decoder architecture, many popular models have opted for encoder-only (e.g. BERT, (Devlin et al., 2019)) or decoder-only (e.g. GPT, (Radford et al., 2018)) approaches. Currently, all state-of-the-art language models over 100 billion parameters are causal decoder-only models Rae et al., 2021;Chowdhery et al., 2022). This is in opposition to the findings of Raffel et al. (2020), in which encoderdecoder models significantly outperform decoder-only models for transfer learning.\n\nPrior to our work, the literature was lacking a systematic evaluation of the zero-shot generalization capabilities of different architectures and pretraining objectives. We explored this question in Wang et al. (2022a) where we evaluated encoder-decoder and decoder-only architectures and their interactions with causal, prefix, and masked language modeling pretraining objectives. Our results show that immediately after pretraining, causal decoderonly models performed best -validating the choice of state-of-the-art LLMs. Furthermore, they can be more efficiently adapted after pretraining to a non-causal architecture and objective-an approach which has been further explored and confirmed by .",
                    "score": 0.6411111600450696,
                    "section_title": "Architecture and Pretraining Objective",
                    "char_start_offset": 30941,
                    "sentence_offsets": [],
                    "ref_mentions": [
                        {
                            "start": 297,
                            "end": 318,
                            "matchedPaperCorpusId": "52967399"
                        },
                        {
                            "start": 573,
                            "end": 593,
                            "matchedPaperCorpusId": "204838007"
                        },
                        {
                            "start": 894,
                            "end": 913,
                            "matchedPaperCorpusId": "248118752"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.74072265625
                }
            ],
            "relevance_judgement": 0.74072265625,
            "relevance_judgment_input_expanded": "# Title: BLOOM: A 176B-Parameter Open-Access Multilingual Language Model\n# Venue: arXiv.org\n# Authors: Teven Le Scao, Angela Fan, Christopher Akiki, Ellie Pavlick, Suzana Ili'c, Daniel Hesslow, Roman Castagn'e, A. Luccioni, Fran\u00e7ois Yvon, Matthias Gall\u00e9, J. Tow, Alexander M. Rush, Stella Biderman, Albert Webson, Pawan Sasanka Ammanamanchi, Thomas Wang, Beno\u00eet Sagot, Niklas Muennighoff, Albert Villanova del Moral, Olatunji Ruwase, Rachel Bawden, Stas Bekman, Angelina McMillan-Major, Iz Beltagy, Huu Nguyen, Lucile Saulnier, Samson Tan, Pedro Ortiz Suarez, Victor Sanh, Hugo Laurenccon, Yacine Jernite, Julien Launay, Margaret Mitchell, Colin Raffel, Aaron Gokaslan, Adi Simhi, Aitor Soroa Etxabe, Alham Fikri Aji, Amit Alfassy, Anna Rogers, Ariel Kreisberg Nitzav, Canwen Xu, Chenghao Mou, Chris C. Emezue, Christopher Klamm, Colin Leong, Daniel Alexander van Strien, David Ifeoluwa Adelani, Dragomir R. Radev, E. G. Ponferrada, Efrat Levkovizh, Ethan Kim, Eyal Natan, F. Toni, G\u00e9rard Dupont, Germ\u00e1n Kruszewski, Giada Pistilli, Hady ElSahar, Hamza Benyamina, H. Tran, Ian Yu, Idris Abdulmumin, Isaac Johnson, Itziar Gonzalez-Dios, Javier de la Rosa, Jenny Chim, Jesse Dodge, Jian Zhu, Jonathan Chang, Jorg Frohberg, Josephine Tobing, J. Bhattacharjee, Khalid Almubarak, Kimbo Chen, Kyle Lo, L. V. Werra, Leon Weber, Long Phan, Loubna Ben Allal, Ludovic Tanguy, Manan Dey, M. Mu\u00f1oz, Maraim Masoud, Mar\u00eda Grandury, Mario vSavsko, Max Huang, Maximin Coavoux, Mayank Singh, Mike Tian-Jian Jiang, Minh Chien Vu, M. A. Jauhar, Mustafa Ghaleb, Nishant Subramani, Nora Kassner, Nurulaqilla Khamis, Olivier Nguyen, Omar Espejel, Ona de Gibert, Paulo Villegas, Peter Henderson, Pierre Colombo, Priscilla Amuok, Quentin Lhoest, Rheza Harliman, Rishi Bommasani, R. L'opez, Rui Ribeiro, Salomey Osei, S. Pyysalo, Sebastian Nagel, Shamik Bose, Shamsuddeen Hassan Muhammad, S. Sharma, S. Longpre, Somaieh Nikpoor, S. Silberberg, S. Pai, S. Zink, Tiago Timponi Torrent, Timo Schick, Tristan Thrush, V. Danchev, Vassilina Nikoulina, Veronika Laippala, Violette Lepercq, V. Prabhu, Zaid Alyafeai, Zeerak Talat, Arun Raja, Benjamin Heinzerling, Chenglei Si, Elizabeth Salesky, Sabrina J. Mielke, Wilson Y. Lee, Abheesht Sharma, Andrea Santilli, Antoine Chaffin, Arnaud Stiegler, Debajyoti Datta, Eliza Szczechla, Gunjan Chhablani, Han Wang, Harshit Pandey, Hendrik Strobelt, Jason Alan Fries, Jos Rozen, Leo Gao, Lintang Sutawika, M Saiful Bari, Maged S. Al-Shaibani, Matteo Manica, Nihal V. Nayak, Ryan Teehan, Samuel Albanie, Sheng Shen, Srulik Ben-David, Stephen H. Bach, Taewoon Kim, T. Bers, Thibault F\u00e9vry, Trishala Neeraj, Urmish Thakker, Vikas Raunak, Xiang Tang, Zheng-Xin Yong, Zhiqing Sun, Shaked Brody, Y. Uri, Hadar Tojarieh, Adam Roberts, Hyung Won Chung, Jaesung Tae, Jason Phang, Ofir Press, Conglong Li, D. Narayanan, Hatim Bourfoune, J. Casper, Jeff Rasley, Max Ryabinin, Mayank Mishra, Minjia Zhang, M. Shoeybi, Myriam Peyrounette, N. Patry, Nouamane Tazi, Omar Sanseviero, Patrick von Platen, Pierre Cornette, Pierre Franccois Lavall'ee, R. Lacroix, Samyam Rajbhandari, Sanchit Gandhi, Shaden Smith, S. Requena, Suraj Patil, Tim Dettmers, Ahmed Baruwa, Amanpreet Singh, Anastasia Cheveleva, Anne-Laure Ligozat, Arjun Subramonian, Aur'elie N'ev'eol, Charles Lovering, Dan Garrette, D. Tunuguntla, Ehud Reiter, Ekaterina Taktasheva, E. Voloshina, Eli Bogdanov, Genta Indra Winata, Hailey Schoelkopf, Jan-Christoph Kalo, Jekaterina Novikova, J. Forde, Xiangru Tang, Jungo Kasai, Ken Kawamura, Liam Hazan, Marine Carpuat, Miruna Clinciu, Najoung Kim, Newton Cheng, Oleg Serikov, Omer Antverg, Oskar van der Wal, Rui Zhang, Ruochen Zhang, Sebastian Gehrmann, Shachar Mirkin, S. Pais, Tatiana Shavrina, Thomas Scialom, Tian Yun, Tomasz Limisiewicz, Verena Rieser, Vitaly Protasov, V. Mikhailov, Yada Pruksachatkun, Yonatan Belinkov, Zachary Bamberger, Zden\u02c7ek Kasner, Zden\u011bk Kasner, A. Pestana, A. Feizpour, Ammar Khan, Amy Faranak, A. Santos, Anthony Hevia, Antigona Unldreaj, Arash Aghagol, Arezoo Abdollahi, A. Tammour, A. HajiHosseini, Bahareh Behroozi, Benjamin Ayoade Ajibade, B. Saxena, Carlos Mu\u00f1oz Ferrandis, Danish Contractor, D. Lansky, Davis David, Douwe Kiela, D. A. Nguyen, Edward Tan, Emi Baylor, Ezinwanne Ozoani, F. Mirza, Frankline Ononiwu, Habib Rezanejad, H.A. Jones, Indrani Bhattacharya, Irene Solaiman, Irina Sedenko, Isar Nejadgholi, J. Passmore, Joshua Seltzer, Julio Bonis Sanz, Karen Fort, L\u00edvia Dutra, Mairon Samagaio, Maraim Elbadri, Margot Mieskes, Marissa Gerchick, Martha Akinlolu, Michael McKenna, Mike Qiu, M. Ghauri, Mykola Burynok, Nafis Abrar, Nazneen Rajani, Nour Elkott, N. Fahmy, Olanrewaju Samuel, Ran An, R. Kromann, Ryan Hao, S. Alizadeh, Sarmad Shubber, Silas L. Wang, Sourav Roy, S. Viguier, Thanh-Cong Le, Tobi Oyebade, T. Le, Yoyo Yang, Zach Nguyen, Abhinav Ramesh Kashyap, A. Palasciano, A. Callahan, Anima Shukla, Antonio Miranda-Escalada, A. Singh, Benjamin Beilharz, Bo Wang, C. Brito, Chenxi Zhou, Chirag Jain, Chuxin Xu, Cl\u00e9mentine Fourrier, Daniel Le'on Perin'an, Daniel Molano, Dian Yu, Enrique Manjavacas, Fabio Barth, Florian Fuhrimann, Gabriel Altay, Giyaseddin Bayrak, Gully Burns, Helena U. Vrabec, I. Bello, Isha Dash, J. Kang, John Giorgi, Jonas Golde, J. Posada, Karthi Sivaraman, Lokesh Bulchandani, Lu Liu, Luisa Shinzato, Madeleine Hahn de Bykhovetz, Maiko Takeuchi, Marc P\u00e0mies, M. A. Castillo, Marianna Nezhurina, Mario Sanger, M. Samwald, Michael Cullan, Michael Weinberg, M. Wolf, Mina Mihaljcic, Minna Liu, M. Freidank, Myungsun Kang, Natasha Seelam, N. Dahlberg, N. Broad, N. Muellner, Pascale Fung, Patricia Haller, Patrick Haller, R. Eisenberg, Robert Martin, Rodrigo Canalli, Rosaline Su, Ruisi Su, Samuel Cahyawijaya, Samuele Garda, Shlok S Deshmukh, Shubhanshu Mishra, Sid Kiblawi, Simon Ott, Sinee Sang-aroonsiri, Srishti Kumar, Stefan Schweter, S. Bharati, Tanmay Laud, Th\u00e9o Gigant, Tomoya Kainuma, Wojciech Kusa, Yanis Labrak, Yashasvi Bajaj, Y. Venkatraman, Yifan Xu, Ying Xu, Yu Xu, Z. Tan, Zhongli Xie, Zifan Ye, M. Bras, Younes Belkada, Thomas Wolf\n## Abstract\nLarge language models (LLMs) have been shown to be able to perform new tasks based on a few demonstrations or natural language instructions. While these capabilities have led to widespread adoption, most LLMs are developed by resource-rich organizations and are frequently kept from the public. As a step towards democratizing this powerful technology, we present BLOOM, a 176B-parameter open-access language model designed and built thanks to a collaboration of hundreds of researchers. BLOOM is a decoder-only Transformer language model that was trained on the ROOTS corpus, a dataset comprising hundreds of sources in 46 natural and 13 programming languages (59 in total). We find that BLOOM achieves competitive performance on a wide variety of benchmarks, with stronger results after undergoing multitask prompted finetuning. To facilitate future research and applications using LLMs, we publicly release our models and code under the Responsible AI License.\n## Architecture and Pretraining Objective\nAlthough most modern language models are based on the Transformer architecture, there are significant deviations between architectural implementations. Notably, while the original Transformer is based on an encoder-decoder architecture, many popular models have opted for encoder-only (e.g. BERT, (Devlin et al., 2019)) or decoder-only (e.g. GPT, (Radford et al., 2018)) approaches. Currently, all state-of-the-art language models over 100 billion parameters are causal decoder-only models Rae et al., 2021;Chowdhery et al., 2022). This is in opposition to the findings of Raffel et al. (2020), in which encoderdecoder models significantly outperform decoder-only models for transfer learning.\n\nPrior to our work, the literature was lacking a systematic evaluation of the zero-shot generalization capabilities of different architectures and pretraining objectives. We explored this question in Wang et al. (2022a) where we evaluated encoder-decoder and decoder-only architectures and their interactions with causal, prefix, and masked language modeling pretraining objectives. Our results show that immediately after pretraining, causal decoderonly models performed best -validating the choice of state-of-the-art LLMs. Furthermore, they can be more efficiently adapted after pretraining to a non-causal architecture and objective-an approach which has been further explored and confirmed by .",
            "reference_string": "[253420279 | Scao et al. | 2022 | Citations: 2393]"
        },
        {
            "title": "Optimal word order for non-causal text generation with Large Language Models: The Spanish case",
            "venue": "Pattern Recognition Letters",
            "year": 2025,
            "reference_count": 57,
            "citation_count": 0,
            "influential_citation_count": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": "CLOSED",
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2502.14451, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2222734467",
                    "name": "Andrea Busto-Casti\u00f1eira"
                },
                {
                    "authorId": "1405165681",
                    "name": "Silvia Garc\u00eda-M\u00e9ndez"
                },
                {
                    "authorId": "2326130687",
                    "name": "Francisco de Arriba-P\u00e9rez"
                },
                {
                    "authorId": "1395988865",
                    "name": "F. Gonz\u00e1lez-Casta\u00f1o"
                }
            ],
            "abstract": null,
            "corpus_id": 276423946,
            "sentences": [
                {
                    "corpus_id": "276423946",
                    "title": "Optimal word order for non-causal text generation with Large Language Models: The Spanish case",
                    "text": "Transformers are unsupervised learners thanks to their selfattention mechanism [48], which controls the impact of the context on the model's output. The original transformer architecture is composed of an encoder and a decoder. While the encoder's attention is bidirectional, the decoder has a masked multi-head attention block that masks non-causal context and a bidirectional multi-head attention block that receives noncausal information from the encoder. \n\nAlthough the encoder-decoder architecture is widely used in some nlp applications like machine translation [21,29], other transformer-based models only use one of these two components. By excluding the encoder, we eliminate all non-causal contextual dependencies, thus using only the masked attention of the decoder. Currently, decoder-only transformers are the most effective task-agnostic nlg systems. \n\nWhile open domain nlg is mainly causal, there are a few noncausal nlg solutions. Most non-causal nlg systems are focused on particular tasks such as speech recognition [5,50], style transfer and grammar correction [20], textual data augmentation [33], and dialog systems [56,51]. \n\nNon-causal language models can also be trained for masked Language Modeling (mlm) [57]. mlm is an nlg task consisting of predicting masked words within a sentence. Some generative systems use bidirectional transformers trained on this task to recursively generate and fill masked tokens [38]. As these can be filled in any location within the text, these models can produce text in a non-causal way. \n\nNon-causal nlg strategies perform much worse in English than their causal counterparts [49]. However, to our knowledge, no prior research has been conducted on non-causal nlg in languages other than English. This work aims to evaluate whether bidirectional transformers trained on the mlm task could be successfully exploited in Spanish nlg.",
                    "score": 0.5235644749640453,
                    "section_title": "Causality in generative transformer language models",
                    "char_start_offset": 6569,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 148
                        },
                        {
                            "start": 149,
                            "end": 227
                        },
                        {
                            "start": 228,
                            "end": 458
                        },
                        {
                            "start": 461,
                            "end": 645
                        },
                        {
                            "start": 646,
                            "end": 777
                        },
                        {
                            "start": 778,
                            "end": 864
                        },
                        {
                            "start": 867,
                            "end": 947
                        },
                        {
                            "start": 948,
                            "end": 1146
                        },
                        {
                            "start": 1149,
                            "end": 1236
                        },
                        {
                            "start": 1237,
                            "end": 1312
                        },
                        {
                            "start": 1313,
                            "end": 1441
                        },
                        {
                            "start": 1442,
                            "end": 1548
                        },
                        {
                            "start": 1551,
                            "end": 1643
                        },
                        {
                            "start": 1644,
                            "end": 1758
                        },
                        {
                            "start": 1759,
                            "end": 1892
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 79,
                            "end": 83,
                            "matchedPaperCorpusId": "13756489"
                        },
                        {
                            "start": 568,
                            "end": 572,
                            "matchedPaperCorpusId": "231645376"
                        },
                        {
                            "start": 572,
                            "end": 575,
                            "matchedPaperCorpusId": "234785837"
                        },
                        {
                            "start": 1035,
                            "end": 1038,
                            "matchedPaperCorpusId": "231715684"
                        },
                        {
                            "start": 1038,
                            "end": 1041,
                            "matchedPaperCorpusId": "247126308"
                        },
                        {
                            "start": 1081,
                            "end": 1085,
                            "matchedPaperCorpusId": "218487230"
                        },
                        {
                            "start": 1113,
                            "end": 1117,
                            "matchedPaperCorpusId": "208224776"
                        },
                        {
                            "start": 1138,
                            "end": 1142,
                            "matchedPaperCorpusId": "212657570"
                        },
                        {
                            "start": 1142,
                            "end": 1145,
                            "matchedPaperCorpusId": "267201220"
                        },
                        {
                            "start": 1231,
                            "end": 1235,
                            "matchedPaperCorpusId": "265629619"
                        },
                        {
                            "start": 1436,
                            "end": 1440,
                            "matchedPaperCorpusId": "211069634"
                        },
                        {
                            "start": 1638,
                            "end": 1642,
                            "matchedPaperCorpusId": "60441316"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.71875
                }
            ],
            "relevance_judgement": 0.71875,
            "relevance_judgment_input_expanded": "# Title: Optimal word order for non-causal text generation with Large Language Models: The Spanish case\n# Venue: Pattern Recognition Letters\n# Authors: Andrea Busto-Casti\u00f1eira, Silvia Garc\u00eda-M\u00e9ndez, Francisco de Arriba-P\u00e9rez, F. Gonz\u00e1lez-Casta\u00f1o\n## Abstract\nNone\n## Causality in generative transformer language models\nTransformers are unsupervised learners thanks to their selfattention mechanism [48], which controls the impact of the context on the model's output. The original transformer architecture is composed of an encoder and a decoder. While the encoder's attention is bidirectional, the decoder has a masked multi-head attention block that masks non-causal context and a bidirectional multi-head attention block that receives noncausal information from the encoder. \n\nAlthough the encoder-decoder architecture is widely used in some nlp applications like machine translation [21,29], other transformer-based models only use one of these two components. By excluding the encoder, we eliminate all non-causal contextual dependencies, thus using only the masked attention of the decoder. Currently, decoder-only transformers are the most effective task-agnostic nlg systems. \n\nWhile open domain nlg is mainly causal, there are a few noncausal nlg solutions. Most non-causal nlg systems are focused on particular tasks such as speech recognition [5,50], style transfer and grammar correction [20], textual data augmentation [33], and dialog systems [56,51]. \n\nNon-causal language models can also be trained for masked Language Modeling (mlm) [57]. mlm is an nlg task consisting of predicting masked words within a sentence. Some generative systems use bidirectional transformers trained on this task to recursively generate and fill masked tokens [38]. As these can be filled in any location within the text, these models can produce text in a non-causal way. \n\nNon-causal nlg strategies perform much worse in English than their causal counterparts [49]. However, to our knowledge, no prior research has been conducted on non-causal nlg in languages other than English. This work aims to evaluate whether bidirectional transformers trained on the mlm task could be successfully exploited in Spanish nlg.",
            "reference_string": "[276423946 | Busto-Castineira et al. | 2025 | Citations: 0]"
        },
        {
            "title": "Aligning the Objective of LLM-based Program Repair",
            "venue": "",
            "year": 2024,
            "reference_count": 76,
            "citation_count": 8,
            "influential_citation_count": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2404.08877, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2186630124",
                    "name": "Junjielong Xu"
                },
                {
                    "authorId": "2296726489",
                    "name": "Ying Fu"
                },
                {
                    "authorId": "2297427913",
                    "name": "Shin Hwei Tan"
                },
                {
                    "authorId": "2265706586",
                    "name": "Pinjia He"
                }
            ],
            "abstract": "Large language models (LLMs) have achieved decent results on automated program repair (APR). However, the next token prediction training objective of decoder-only LLMs (e.g., GPT-4) is misaligned with the masked span prediction objective of current infilling-style methods, which impedes LLMs from fully leveraging pre-trained knowledge for program repair. In addition, while some LLMs can locate and repair bugs in certain functions using the related artifacts (e.g., test cases), existing methods still depend on statement-level fault localization methods to provide a list of buggy hunks for repair. This restriction hinders LLMs from exploring potential patches beyond the given locations. In this paper, we investigate a new approach to adapt LLMs to program repair. Our core insight is that LLM's APR capability can be greatly improved by simply aligning the output to their training objective and allowing them to refine the whole program without first identifying faulty statements. Based on this insight, we designed D4C, a straightforward prompting framework for APR. D4C can repair 180 bugs correctly in Defects4J, with each patch being sampled only 10 times. This surpasses the SOTA APR methods with perfect fault localization by 10% and reduces the patch sampling number by 90%. Our findings reveal that (1) objective alignment is crucial for fully exploiting LLM's pre-trained capability, and (2) replacing the traditional localize-buggy-hunks-then-repair workflow with direct debugging is more effective for LLM-based APR methods. Thus, we believe this paper introduces a new mindset for harnessing LLMs in APR.",
            "corpus_id": 269148552,
            "sentences": [
                {
                    "corpus_id": "269148552",
                    "title": "Aligning the Objective of LLM-based Program Repair",
                    "text": "Background: A large language model (LLM) is a language model consisting of a neural network with many parameters (typically billions of weights or more) trained on large quantities of unlabelled corpus using self-supervised learning [44]. The LLMs usually adopt the Transformer [45] architecture or one of its sub-structures (i.e., encoder or decoder). The encoder usually consists of feed-forward networks with selfattention [45], while the decoder usually consists of feedforward networks with cross-attention [45]. Thus, LLMs can be categorized into three types: encoder-only, decoder-only, and encoder-decoder LLMs. \n\nEncoder-only LLMs, such as BERT [23] and its variants like CodeBERT [46], have a bidirectional transformer encoder structure. They are typically trained on the masked language modeling objective (i.e., MLM), aiming to denoise and reconstruct the masked tokens via understanding the surrounding context (Fig. 1). As shown in Eq. 1, the loss of MLM training objective can be explicitly represented as the log-sum of the conditional probabilities of generating the masked token when all the unmasked tokens are known. \n\nwhere M is the total number of masked tokens. Decoder-only LLMs, including GPT series [13], [27] and LLaMA series [47], have an autoregressive transformer decoder structure. They are mainly trained on the causal language modeling objective (i.e., CLM), aiming to predict and complete next tokens via following the prefix context (Fig. 1). As shown in Eq. 2, the loss of CLM training objective can be explicitly represented as the log-sum of the conditional probabilities of generating the next token when all the preceeding tokens are known. \n\nwhere N is the total number of the input tokens. Encoder-decoder LLMs, such as T5 [24] and its variants like CodeT5 [48], have a complete transformer structure.",
                    "score": 0.4775922423194968,
                    "section_title": "II. BACKGROUND AND MOTIVATION A. LLM Architectures and Training Objectives",
                    "char_start_offset": 6433,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 238
                        },
                        {
                            "start": 239,
                            "end": 352
                        },
                        {
                            "start": 353,
                            "end": 517
                        },
                        {
                            "start": 518,
                            "end": 619
                        },
                        {
                            "start": 622,
                            "end": 747
                        },
                        {
                            "start": 748,
                            "end": 933
                        },
                        {
                            "start": 934,
                            "end": 1136
                        },
                        {
                            "start": 1139,
                            "end": 1184
                        },
                        {
                            "start": 1185,
                            "end": 1312
                        },
                        {
                            "start": 1313,
                            "end": 1477
                        },
                        {
                            "start": 1478,
                            "end": 1680
                        },
                        {
                            "start": 1683,
                            "end": 1731
                        },
                        {
                            "start": 1732,
                            "end": 1843
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 278,
                            "end": 282,
                            "matchedPaperCorpusId": "13756489"
                        },
                        {
                            "start": 426,
                            "end": 430,
                            "matchedPaperCorpusId": "13756489"
                        },
                        {
                            "start": 512,
                            "end": 516,
                            "matchedPaperCorpusId": "13756489"
                        },
                        {
                            "start": 1225,
                            "end": 1229,
                            "matchedPaperCorpusId": "218971783"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.71240234375
                }
            ],
            "relevance_judgement": 0.71240234375,
            "relevance_judgment_input_expanded": "# Title: Aligning the Objective of LLM-based Program Repair\n# Venue: \n# Authors: Junjielong Xu, Ying Fu, Shin Hwei Tan, Pinjia He\n## Abstract\nLarge language models (LLMs) have achieved decent results on automated program repair (APR). However, the next token prediction training objective of decoder-only LLMs (e.g., GPT-4) is misaligned with the masked span prediction objective of current infilling-style methods, which impedes LLMs from fully leveraging pre-trained knowledge for program repair. In addition, while some LLMs can locate and repair bugs in certain functions using the related artifacts (e.g., test cases), existing methods still depend on statement-level fault localization methods to provide a list of buggy hunks for repair. This restriction hinders LLMs from exploring potential patches beyond the given locations. In this paper, we investigate a new approach to adapt LLMs to program repair. Our core insight is that LLM's APR capability can be greatly improved by simply aligning the output to their training objective and allowing them to refine the whole program without first identifying faulty statements. Based on this insight, we designed D4C, a straightforward prompting framework for APR. D4C can repair 180 bugs correctly in Defects4J, with each patch being sampled only 10 times. This surpasses the SOTA APR methods with perfect fault localization by 10% and reduces the patch sampling number by 90%. Our findings reveal that (1) objective alignment is crucial for fully exploiting LLM's pre-trained capability, and (2) replacing the traditional localize-buggy-hunks-then-repair workflow with direct debugging is more effective for LLM-based APR methods. Thus, we believe this paper introduces a new mindset for harnessing LLMs in APR.\n## II. BACKGROUND AND MOTIVATION A. LLM Architectures and Training Objectives\nBackground: A large language model (LLM) is a language model consisting of a neural network with many parameters (typically billions of weights or more) trained on large quantities of unlabelled corpus using self-supervised learning [44]. The LLMs usually adopt the Transformer [45] architecture or one of its sub-structures (i.e., encoder or decoder). The encoder usually consists of feed-forward networks with selfattention [45], while the decoder usually consists of feedforward networks with cross-attention [45]. Thus, LLMs can be categorized into three types: encoder-only, decoder-only, and encoder-decoder LLMs. \n\nEncoder-only LLMs, such as BERT [23] and its variants like CodeBERT [46], have a bidirectional transformer encoder structure. They are typically trained on the masked language modeling objective (i.e., MLM), aiming to denoise and reconstruct the masked tokens via understanding the surrounding context (Fig. 1). As shown in Eq. 1, the loss of MLM training objective can be explicitly represented as the log-sum of the conditional probabilities of generating the masked token when all the unmasked tokens are known. \n\nwhere M is the total number of masked tokens. Decoder-only LLMs, including GPT series [13], [27] and LLaMA series [47], have an autoregressive transformer decoder structure. They are mainly trained on the causal language modeling objective (i.e., CLM), aiming to predict and complete next tokens via following the prefix context (Fig. 1). As shown in Eq. 2, the loss of CLM training objective can be explicitly represented as the log-sum of the conditional probabilities of generating the next token when all the preceeding tokens are known. \n\nwhere N is the total number of the input tokens. Encoder-decoder LLMs, such as T5 [24] and its variants like CodeT5 [48], have a complete transformer structure.",
            "reference_string": "[269148552 | Xu et al. | 2024 | Citations: 8]"
        },
        {
            "title": "ContraCLM: Contrastive Learning For Causal Language Model",
            "venue": "Annual Meeting of the Association for Computational Linguistics",
            "year": 2022,
            "reference_count": 64,
            "citation_count": 16,
            "influential_citation_count": 2,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://aclanthology.org/2023.acl-long.355.pdf",
                "status": "HYBRID",
                "license": "CCBY",
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2210.01185, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2146677401",
                    "name": "Nihal Jain"
                },
                {
                    "authorId": "2358258",
                    "name": "Dejiao Zhang"
                },
                {
                    "authorId": "38123220",
                    "name": "Wasi Uddin Ahmad"
                },
                {
                    "authorId": "50219006",
                    "name": "Zijian Wang"
                },
                {
                    "authorId": "144647318",
                    "name": "Feng Nan"
                },
                {
                    "authorId": "2187045812",
                    "name": "Xiaopeng Li"
                },
                {
                    "authorId": "144745483",
                    "name": "Ming Tan"
                },
                {
                    "authorId": "1701451",
                    "name": "Ramesh Nallapati"
                },
                {
                    "authorId": "31631000",
                    "name": "Baishakhi Ray"
                },
                {
                    "authorId": "50339091",
                    "name": "Parminder Bhatia"
                },
                {
                    "authorId": "47646605",
                    "name": "Xiaofei Ma"
                },
                {
                    "authorId": "144028698",
                    "name": "Bing Xiang"
                }
            ],
            "abstract": "Despite exciting progress in causal language models, the expressiveness of their representations is largely limited due to poor discrimination ability. To remedy this issue, we present CONTRACLM, a novel contrastive learning framework at both the token-level and the sequence-level. We assess CONTRACLM on a variety of downstream tasks. We show that CONTRACLM enhances the discrimination of representations and bridges the gap with encoder-only models, which makes causal language models better suited for tasks beyond language generation. Specifically, we attain 44% relative improvement on the Semantic Textual Similarity tasks and 34% on Code-to-Code Search tasks. Furthermore, by improving the expressiveness of representations, CONTRACLM also boosts the source code generation capability with 9% relative improvement on execution accuracy on the HumanEval benchmark.",
            "corpus_id": 258461112,
            "sentences": [
                {
                    "corpus_id": "258461112",
                    "title": "ContraCLM: Contrastive Learning For Causal Language Model",
                    "text": "Compared to the causal (left-to-right) attention mechanism of the decoder-only models, the bidirectional attention mechanism in both encoder-only and encoder-decoder models allows for better leverage of the context of the sequence and hence leads to better representations. \n\nTaking the encoder-only models in Table 7a for illustration, on average, BERT-Base (Devlin et al., 2019) and Roberta-Base (Liu et al., 2019)   between CodeGen and the BERT models trained on programming languages, i.e., CodeBERT (Feng et al., 2020) and GraphCodeBERT (Guo et al., 2021), decreases or even diminishes when evaluated on the code search tasks, the performance gap is still significant as both the model size and pretraining data in CodeGen are much larger than those used by the encoder-only models in Table 7b. Similar trends were observed in the performance gap between the decoder-only and encoderdecoder models on both natural language (Lewis et al., 2020;Raffel et al., 2020) and programming language (Ahmad et al., 2021;Wang et al., 2021). The large performance gap severely limits the decoder-only models used in many discriminative tasks. To this end, contrastive learning shows the promise to largely bridge the gap. As seen in Table 7a, on STS, CONTRACLM reduces the relative performance gap from 67.24% (absolute 21.12%) to 16.17% (absolute 7.33%) regarding BERT-Base, and from 84.62% (absolute 26.64%) to 28.24% (absolute 12.8%). Similarly, Table 7b shows that CONTRACLM outperforms encoder-decoder models and performs comparably to the encoder-only model, GraphCodeBERT. Gao et al. (2021) showed that the dropout-based augmentation is an effective strategy for unsupervised contrastive learning, and the follow-up works (Chuang et al., 2022;Wu et al., 2022) endorse the effectiveness.",
                    "score": 0.49778243774477504,
                    "section_title": "D.1 Bridge the Gap on Discriminative Tasks",
                    "char_start_offset": 38682,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 273
                        },
                        {
                            "start": 276,
                            "end": 799
                        },
                        {
                            "start": 800,
                            "end": 1033
                        },
                        {
                            "start": 1034,
                            "end": 1134
                        },
                        {
                            "start": 1135,
                            "end": 1213
                        },
                        {
                            "start": 1214,
                            "end": 1429
                        },
                        {
                            "start": 1430,
                            "end": 1571
                        },
                        {
                            "start": 1572,
                            "end": 1785
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 359,
                            "end": 380,
                            "matchedPaperCorpusId": "52967399"
                        },
                        {
                            "start": 504,
                            "end": 523,
                            "matchedPaperCorpusId": "211171605"
                        },
                        {
                            "start": 542,
                            "end": 560,
                            "matchedPaperCorpusId": "221761146"
                        },
                        {
                            "start": 928,
                            "end": 948,
                            "matchedPaperCorpusId": "204960716"
                        },
                        {
                            "start": 994,
                            "end": 1014,
                            "matchedPaperCorpusId": "232185260"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.705078125
                }
            ],
            "relevance_judgement": 0.705078125,
            "relevance_judgment_input_expanded": "# Title: ContraCLM: Contrastive Learning For Causal Language Model\n# Venue: Annual Meeting of the Association for Computational Linguistics\n# Authors: Nihal Jain, Dejiao Zhang, Wasi Uddin Ahmad, Zijian Wang, Feng Nan, Xiaopeng Li, Ming Tan, Ramesh Nallapati, Baishakhi Ray, Parminder Bhatia, Xiaofei Ma, Bing Xiang\n## Abstract\nDespite exciting progress in causal language models, the expressiveness of their representations is largely limited due to poor discrimination ability. To remedy this issue, we present CONTRACLM, a novel contrastive learning framework at both the token-level and the sequence-level. We assess CONTRACLM on a variety of downstream tasks. We show that CONTRACLM enhances the discrimination of representations and bridges the gap with encoder-only models, which makes causal language models better suited for tasks beyond language generation. Specifically, we attain 44% relative improvement on the Semantic Textual Similarity tasks and 34% on Code-to-Code Search tasks. Furthermore, by improving the expressiveness of representations, CONTRACLM also boosts the source code generation capability with 9% relative improvement on execution accuracy on the HumanEval benchmark.\n## D.1 Bridge the Gap on Discriminative Tasks\nCompared to the causal (left-to-right) attention mechanism of the decoder-only models, the bidirectional attention mechanism in both encoder-only and encoder-decoder models allows for better leverage of the context of the sequence and hence leads to better representations. \n\nTaking the encoder-only models in Table 7a for illustration, on average, BERT-Base (Devlin et al., 2019) and Roberta-Base (Liu et al., 2019)   between CodeGen and the BERT models trained on programming languages, i.e., CodeBERT (Feng et al., 2020) and GraphCodeBERT (Guo et al., 2021), decreases or even diminishes when evaluated on the code search tasks, the performance gap is still significant as both the model size and pretraining data in CodeGen are much larger than those used by the encoder-only models in Table 7b. Similar trends were observed in the performance gap between the decoder-only and encoderdecoder models on both natural language (Lewis et al., 2020;Raffel et al., 2020) and programming language (Ahmad et al., 2021;Wang et al., 2021). The large performance gap severely limits the decoder-only models used in many discriminative tasks. To this end, contrastive learning shows the promise to largely bridge the gap. As seen in Table 7a, on STS, CONTRACLM reduces the relative performance gap from 67.24% (absolute 21.12%) to 16.17% (absolute 7.33%) regarding BERT-Base, and from 84.62% (absolute 26.64%) to 28.24% (absolute 12.8%). Similarly, Table 7b shows that CONTRACLM outperforms encoder-decoder models and performs comparably to the encoder-only model, GraphCodeBERT. Gao et al. (2021) showed that the dropout-based augmentation is an effective strategy for unsupervised contrastive learning, and the follow-up works (Chuang et al., 2022;Wu et al., 2022) endorse the effectiveness.",
            "reference_string": "[258461112 | Jain et al. | 2022 | Citations: 16]"
        },
        {
            "title": "GOLLuM: Gaussian Process Optimized LLMs - Reframing LLM Finetuning through Bayesian Optimization",
            "venue": "arXiv.org",
            "year": 2025,
            "reference_count": 101,
            "citation_count": 1,
            "influential_citation_count": 1,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2504.06265, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2219925647",
                    "name": "Bojana Rankovi'c"
                },
                {
                    "authorId": "2239074343",
                    "name": "Philippe Schwaller"
                }
            ],
            "abstract": "Large Language Models (LLMs) can encode complex relationships in their latent spaces, yet harnessing them for optimization under uncertainty remains challenging. We address this gap with a novel architecture that reframes LLM finetuning as Gaussian process (GP) marginal likelihood optimization via deep kernel methods. We introduce LLM-based deep kernels, jointly optimized with GPs to preserve the benefits of both - LLMs to provide a rich and flexible input space for Bayesian optimization and - GPs to model this space with predictive uncertainty for more efficient sampling. Applied to Buchwald-Hartwig reaction optimization, our method nearly doubles the discovery rate of high-performing reactions compared to static LLM embeddings (from 24% to 43% coverage of the top 5% reactions in just 50 optimization iterations). We also observe a 14% improvement over domain-specific representations without requiring specialized features. Extensive empirical evaluation across 19 benchmarks - ranging from general chemistry to reaction and molecular property optimization - demonstrates our method's robustness, generality, and consistent improvements across: (1) tasks, (2) LLM architectures (encoder, decoder, encoder-decoder), (3) pretraining domains (chemistry-related or general-purpose) and (4) hyperparameter settings (tuned once on a single dataset). Finally, we explain these improvements: joint LLM-GP optimization through marginal likelihood implicitly performs contrastive learning, aligning representations to produce (1) better-structured embedding spaces, (2) improved uncertainty calibration, and (3) more efficient sampling - without requiring any external loss. This work provides both practical advances in sample-efficient optimization and insights into what makes effective Bayesian optimization.",
            "corpus_id": 277626915,
            "sentences": [
                {
                    "corpus_id": "277626915",
                    "title": "GOLLuM: Gaussian Process Optimized LLMs - Reframing LLM Finetuning through Bayesian Optimization",
                    "text": "LLMs process textual inputs by converting them into dense vector representations through a sequence of tokenization , embedding and attention-based transformations. Tokenization involves the process of splitting the input text into subword units (tokens) using a model-specific vocabulary (e.g., SentencePiece 84 , Byte-Pair Encoding 85 ). The tokens are mapped to continuous vectors via learned embedding layers and passed through multiple self-attention layers that capture contextual relationships between tokens. \n\nLLMs can follow different architectural designs: encoder-only (e.g., BERT 2 ), decoder-only (e.g., Qwen 52 ), and encoder-decoder (e.g., T5 49 ). Encoder-based models process the full input bidirectionally and are suited for classification and regression . Decoder-only models generate text autoregressively with causal masking . Encoder-decoder models combine both components and are often used for sequence-to-sequence tasks . The architecture choices impact the structure and pooling strategies used to extract unified representations from the variable-length token sequences. \n\nPooling refers to the process of aggregating a sequence of token-level representations produced by a language model into a single fixed-dimensional embedding. Encoder-based models often use the hidden state corresponding to the special [CLS] token or apply mean-pooling across token embeddings. Decoder-only models typically use the final hidden state of the last non-padding token. For encoder-decoder models, pooling is applied over the encoder-side hidden states.",
                    "score": 0.5138235671466846,
                    "section_title": "B.5 Large Language Models",
                    "char_start_offset": 39674,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 164
                        },
                        {
                            "start": 165,
                            "end": 339
                        },
                        {
                            "start": 340,
                            "end": 516
                        },
                        {
                            "start": 519,
                            "end": 664
                        },
                        {
                            "start": 665,
                            "end": 775
                        },
                        {
                            "start": 776,
                            "end": 848
                        },
                        {
                            "start": 849,
                            "end": 947
                        },
                        {
                            "start": 948,
                            "end": 1098
                        },
                        {
                            "start": 1101,
                            "end": 1259
                        },
                        {
                            "start": 1260,
                            "end": 1395
                        },
                        {
                            "start": 1396,
                            "end": 1483
                        },
                        {
                            "start": 1484,
                            "end": 1567
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 334,
                            "end": 336,
                            "matchedPaperCorpusId": "160025533"
                        },
                        {
                            "start": 659,
                            "end": 661,
                            "matchedPaperCorpusId": "204838007"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.703125
                }
            ],
            "relevance_judgement": 0.703125,
            "relevance_judgment_input_expanded": "# Title: GOLLuM: Gaussian Process Optimized LLMs - Reframing LLM Finetuning through Bayesian Optimization\n# Venue: arXiv.org\n# Authors: Bojana Rankovi'c, Philippe Schwaller\n## Abstract\nLarge Language Models (LLMs) can encode complex relationships in their latent spaces, yet harnessing them for optimization under uncertainty remains challenging. We address this gap with a novel architecture that reframes LLM finetuning as Gaussian process (GP) marginal likelihood optimization via deep kernel methods. We introduce LLM-based deep kernels, jointly optimized with GPs to preserve the benefits of both - LLMs to provide a rich and flexible input space for Bayesian optimization and - GPs to model this space with predictive uncertainty for more efficient sampling. Applied to Buchwald-Hartwig reaction optimization, our method nearly doubles the discovery rate of high-performing reactions compared to static LLM embeddings (from 24% to 43% coverage of the top 5% reactions in just 50 optimization iterations). We also observe a 14% improvement over domain-specific representations without requiring specialized features. Extensive empirical evaluation across 19 benchmarks - ranging from general chemistry to reaction and molecular property optimization - demonstrates our method's robustness, generality, and consistent improvements across: (1) tasks, (2) LLM architectures (encoder, decoder, encoder-decoder), (3) pretraining domains (chemistry-related or general-purpose) and (4) hyperparameter settings (tuned once on a single dataset). Finally, we explain these improvements: joint LLM-GP optimization through marginal likelihood implicitly performs contrastive learning, aligning representations to produce (1) better-structured embedding spaces, (2) improved uncertainty calibration, and (3) more efficient sampling - without requiring any external loss. This work provides both practical advances in sample-efficient optimization and insights into what makes effective Bayesian optimization.\n## B.5 Large Language Models\nLLMs process textual inputs by converting them into dense vector representations through a sequence of tokenization , embedding and attention-based transformations. Tokenization involves the process of splitting the input text into subword units (tokens) using a model-specific vocabulary (e.g., SentencePiece 84 , Byte-Pair Encoding 85 ). The tokens are mapped to continuous vectors via learned embedding layers and passed through multiple self-attention layers that capture contextual relationships between tokens. \n\nLLMs can follow different architectural designs: encoder-only (e.g., BERT 2 ), decoder-only (e.g., Qwen 52 ), and encoder-decoder (e.g., T5 49 ). Encoder-based models process the full input bidirectionally and are suited for classification and regression . Decoder-only models generate text autoregressively with causal masking . Encoder-decoder models combine both components and are often used for sequence-to-sequence tasks . The architecture choices impact the structure and pooling strategies used to extract unified representations from the variable-length token sequences. \n\nPooling refers to the process of aggregating a sequence of token-level representations produced by a language model into a single fixed-dimensional embedding. Encoder-based models often use the hidden state corresponding to the special [CLS] token or apply mean-pooling across token embeddings. Decoder-only models typically use the final hidden state of the last non-padding token. For encoder-decoder models, pooling is applied over the encoder-side hidden states.",
            "reference_string": "[277626915 | Rankovi'c et al. | 2025 | Citations: 1]"
        },
        {
            "title": "Decoder-Only or Encoder-Decoder? Interpreting Language Model as a Regularized Encoder-Decoder",
            "venue": "arXiv.org",
            "year": 2023,
            "reference_count": 73,
            "citation_count": 43,
            "influential_citation_count": 1,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "http://arxiv.org/pdf/2304.04052",
                "status": "CLOSED",
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2304.04052, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "1646716323",
                    "name": "Z. Fu"
                },
                {
                    "authorId": "1380007189",
                    "name": "W. Lam"
                },
                {
                    "authorId": "144873019",
                    "name": "Qian Yu"
                },
                {
                    "authorId": "1734000",
                    "name": "A. M. So"
                },
                {
                    "authorId": "1576223501",
                    "name": "Shengding Hu"
                },
                {
                    "authorId": null,
                    "name": "Zhiyuan Liu"
                },
                {
                    "authorId": "50638196",
                    "name": "Nigel Collier"
                }
            ],
            "abstract": "The sequence-to-sequence (seq2seq) task aims at generating the target sequence based on the given input source sequence. Traditionally, most of the seq2seq task is resolved by the Encoder-Decoder framework which requires an encoder to encode the source sequence and a decoder to generate the target text. Recently, a bunch of new approaches have emerged that apply decoder-only language models directly to the seq2seq task. Despite the significant advancements in applying language models to the seq2seq task, there is still a lack of thorough analysis on the effectiveness of the decoder-only language model architecture. This paper aims to address this gap by conducting a detailed comparison between the encoder-decoder architecture and the decoder-only language model framework through the analysis of a regularized encoder-decoder structure. This structure is designed to replicate all behaviors in the classical decoder-only language model but has an encoder and a decoder making it easier to be compared with the classical encoder-decoder structure. Based on the analysis, we unveil the attention degeneration problem in the language model, namely, as the generation step number grows, less and less attention is focused on the source sequence. To give a quantitative understanding of this problem, we conduct a theoretical sensitivity analysis of the attention output with respect to the source input. Grounded on our analysis, we propose a novel partial attention language model to solve the attention degeneration problem. Experimental results on machine translation, summarization, and data-to-text generation tasks support our analysis and demonstrate the effectiveness of our proposed model.",
            "corpus_id": 258049081,
            "sentences": [
                {
                    "corpus_id": "258049081",
                    "title": "Decoder-Only or Encoder-Decoder? Interpreting Language Model as a Regularized Encoder-Decoder",
                    "text": "Though the decoder-only Language Model (LM) is simply a decoder, it is still difficult to be compared with an Encoder-Decoder (ED) structure because this decoder handles both the source sequence and the target sequence together. To facilitate the comparison between the ED and LM structure, we propose to analyze a Regularized Encoder-Decoder (RED) framework as illustrated in Figure 2. It is a variant of the traditional ED framework while replicating the behaviors of an LM. Compared with the traditional ED structure, the RED framework mainly has the following different components: An unidirectional cross attention attends to both the source matrix and the target matrix simultaneously; a source auto-encoder recovers the input source; a parameter sharing mechanism shares the parameters between the encoder and the decoder; a layer-wise coordination component makes each decoder layer attending to the corresponding encoder layer output; a consecutive positional encoding utilizes a positional encoding starting from the length of the source tokens in the decoder. \n\nUnidirectional Cross Attention. The main difference between the ED framework and the LM is how the input source information is merged into the decoder. As illustrated in Figure 2, the ED framework first uses multiple Transformer blocks to extract features H E \u22121 from the source sequence s. Afterwards, it utilizes a self attention ATT D l to get the feature matrix G D l . It then uses an encoder attention ATT J l to take G D l as query and uses the encoder's final output H E \u22121 as the key and value to calculate Q D l . On the other hand, an LM uses an unidirectional attention to handle the concatenated features. To simulate this mechanism in the LM, as illustrated in Figure 2, the RED framework uses unidirectional cross attention ATT l which attends to both the source matrix G E l and the target matrix G D l simultaneously. Since it attends to all features with one attention, the output matrix Q D l of the attention layer becomes less sensitive to the input source matrix G E l especially when it has already generated many words and G D l becomes relatively long.",
                    "score": 0.46296184880055835,
                    "section_title": "Regularized Encoder-Decoder",
                    "char_start_offset": 9858,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 228
                        },
                        {
                            "start": 229,
                            "end": 476
                        },
                        {
                            "start": 477,
                            "end": 1070
                        },
                        {
                            "start": 1073,
                            "end": 1104
                        },
                        {
                            "start": 1105,
                            "end": 1224
                        },
                        {
                            "start": 1225,
                            "end": 1363
                        },
                        {
                            "start": 1364,
                            "end": 1446
                        },
                        {
                            "start": 1447,
                            "end": 1596
                        },
                        {
                            "start": 1597,
                            "end": 1691
                        },
                        {
                            "start": 1692,
                            "end": 1907
                        },
                        {
                            "start": 1908,
                            "end": 2150
                        }
                    ],
                    "ref_mentions": [],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.70263671875
                }
            ],
            "relevance_judgement": 0.70263671875,
            "relevance_judgment_input_expanded": "# Title: Decoder-Only or Encoder-Decoder? Interpreting Language Model as a Regularized Encoder-Decoder\n# Venue: arXiv.org\n# Authors: Z. Fu, W. Lam, Qian Yu, A. M. So, Shengding Hu, Zhiyuan Liu, Nigel Collier\n## Abstract\nThe sequence-to-sequence (seq2seq) task aims at generating the target sequence based on the given input source sequence. Traditionally, most of the seq2seq task is resolved by the Encoder-Decoder framework which requires an encoder to encode the source sequence and a decoder to generate the target text. Recently, a bunch of new approaches have emerged that apply decoder-only language models directly to the seq2seq task. Despite the significant advancements in applying language models to the seq2seq task, there is still a lack of thorough analysis on the effectiveness of the decoder-only language model architecture. This paper aims to address this gap by conducting a detailed comparison between the encoder-decoder architecture and the decoder-only language model framework through the analysis of a regularized encoder-decoder structure. This structure is designed to replicate all behaviors in the classical decoder-only language model but has an encoder and a decoder making it easier to be compared with the classical encoder-decoder structure. Based on the analysis, we unveil the attention degeneration problem in the language model, namely, as the generation step number grows, less and less attention is focused on the source sequence. To give a quantitative understanding of this problem, we conduct a theoretical sensitivity analysis of the attention output with respect to the source input. Grounded on our analysis, we propose a novel partial attention language model to solve the attention degeneration problem. Experimental results on machine translation, summarization, and data-to-text generation tasks support our analysis and demonstrate the effectiveness of our proposed model.\n## Regularized Encoder-Decoder\nThough the decoder-only Language Model (LM) is simply a decoder, it is still difficult to be compared with an Encoder-Decoder (ED) structure because this decoder handles both the source sequence and the target sequence together. To facilitate the comparison between the ED and LM structure, we propose to analyze a Regularized Encoder-Decoder (RED) framework as illustrated in Figure 2. It is a variant of the traditional ED framework while replicating the behaviors of an LM. Compared with the traditional ED structure, the RED framework mainly has the following different components: An unidirectional cross attention attends to both the source matrix and the target matrix simultaneously; a source auto-encoder recovers the input source; a parameter sharing mechanism shares the parameters between the encoder and the decoder; a layer-wise coordination component makes each decoder layer attending to the corresponding encoder layer output; a consecutive positional encoding utilizes a positional encoding starting from the length of the source tokens in the decoder. \n\nUnidirectional Cross Attention. The main difference between the ED framework and the LM is how the input source information is merged into the decoder. As illustrated in Figure 2, the ED framework first uses multiple Transformer blocks to extract features H E \u22121 from the source sequence s. Afterwards, it utilizes a self attention ATT D l to get the feature matrix G D l . It then uses an encoder attention ATT J l to take G D l as query and uses the encoder's final output H E \u22121 as the key and value to calculate Q D l . On the other hand, an LM uses an unidirectional attention to handle the concatenated features. To simulate this mechanism in the LM, as illustrated in Figure 2, the RED framework uses unidirectional cross attention ATT l which attends to both the source matrix G E l and the target matrix G D l simultaneously. Since it attends to all features with one attention, the output matrix Q D l of the attention layer becomes less sensitive to the input source matrix G E l especially when it has already generated many words and G D l becomes relatively long.",
            "reference_string": "[258049081 | Fu et al. | 2023 | Citations: 43]"
        },
        {
            "title": "Attention Mechanisms Don't Learn Additive Models: Rethinking Feature Importance for Transformers",
            "venue": "Trans. Mach. Learn. Res.",
            "year": 2024,
            "reference_count": 60,
            "citation_count": 5,
            "influential_citation_count": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2405.13536, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2130899453",
                    "name": "Tobias Leemann"
                },
                {
                    "authorId": "2302795616",
                    "name": "Alina Fastowski"
                },
                {
                    "authorId": "2317114785",
                    "name": "Felix Pfeiffer"
                },
                {
                    "authorId": "1686448",
                    "name": "Gjergji Kasneci"
                }
            ],
            "abstract": "We address the critical challenge of applying feature attribution methods to the transformer architecture, which dominates current applications in natural language processing and beyond. Traditional attribution methods to explainable AI (XAI) explicitly or implicitly rely on linear or additive surrogate models to quantify the impact of input features on a model's output. In this work, we formally prove an alarming incompatibility: transformers are structurally incapable of representing linear or additive surrogate models used for feature attribution, undermining the grounding of these conventional explanation methodologies. To address this discrepancy, we introduce the Softmax-Linked Additive Log Odds Model (SLALOM), a novel surrogate model specifically designed to align with the transformer framework. SLALOM demonstrates the capacity to deliver a range of insightful explanations with both synthetic and real-world datasets. We highlight SLALOM's unique efficiency-quality curve by showing that SLALOM can produce explanations with substantially higher fidelity than competing surrogate models or provide explanations of comparable quality at a fraction of their computational costs. We release code for SLALOM as an open-source project online at https://github.com/tleemann/slalom_explanations.",
            "corpus_id": 269982953,
            "sentences": [
                {
                    "corpus_id": "269982953",
                    "title": "Attention Mechanisms Don't Learn Additive Models: Rethinking Feature Importance for Transformers",
                    "text": "Practical implementations introduce subtle modifications into the process described previously.The most relevant distinction is made between encoder-only models, that include BERT [15] and its variants, and decoder-only models such as the GPT models [38,39].\n\nEncoder-only models.Considering BERT as an example of an encoder-only model, the first token is used for the classification, i.e, r = 1.Usually, a special token [CLS] is prepended to the text at position 1, however this is not strictly necessary for the functioning of the model.\n\nDecoder-only models.In contrast, decoder-models like GPT-2 [39] add the classification head on top of the last token for classification, i.e., r = |t|.A key difference is that in GPT-2 and other decoder-only models, a causal mask is laid over the attention matrix, resulting in \u03b1 i,j = 0 for j > i.This encodes the constraint that tokens can only attend to themselves or to previous ones.",
                    "score": 0.6261452571458622,
                    "section_title": "Encoder-Only and Decoder-Only models",
                    "char_start_offset": 11540,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 95
                        },
                        {
                            "start": 95,
                            "end": 258
                        },
                        {
                            "start": 260,
                            "end": 280
                        },
                        {
                            "start": 280,
                            "end": 396
                        },
                        {
                            "start": 396,
                            "end": 539
                        },
                        {
                            "start": 541,
                            "end": 561
                        },
                        {
                            "start": 561,
                            "end": 692
                        },
                        {
                            "start": 692,
                            "end": 839
                        },
                        {
                            "start": 839,
                            "end": 929
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 254,
                            "end": 257,
                            "matchedPaperCorpusId": "160025533"
                        },
                        {
                            "start": 600,
                            "end": 604,
                            "matchedPaperCorpusId": "160025533"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.69287109375
                }
            ],
            "relevance_judgement": 0.69287109375,
            "relevance_judgment_input_expanded": "# Title: Attention Mechanisms Don't Learn Additive Models: Rethinking Feature Importance for Transformers\n# Venue: Trans. Mach. Learn. Res.\n# Authors: Tobias Leemann, Alina Fastowski, Felix Pfeiffer, Gjergji Kasneci\n## Abstract\nWe address the critical challenge of applying feature attribution methods to the transformer architecture, which dominates current applications in natural language processing and beyond. Traditional attribution methods to explainable AI (XAI) explicitly or implicitly rely on linear or additive surrogate models to quantify the impact of input features on a model's output. In this work, we formally prove an alarming incompatibility: transformers are structurally incapable of representing linear or additive surrogate models used for feature attribution, undermining the grounding of these conventional explanation methodologies. To address this discrepancy, we introduce the Softmax-Linked Additive Log Odds Model (SLALOM), a novel surrogate model specifically designed to align with the transformer framework. SLALOM demonstrates the capacity to deliver a range of insightful explanations with both synthetic and real-world datasets. We highlight SLALOM's unique efficiency-quality curve by showing that SLALOM can produce explanations with substantially higher fidelity than competing surrogate models or provide explanations of comparable quality at a fraction of their computational costs. We release code for SLALOM as an open-source project online at https://github.com/tleemann/slalom_explanations.\n## Encoder-Only and Decoder-Only models\nPractical implementations introduce subtle modifications into the process described previously.The most relevant distinction is made between encoder-only models, that include BERT [15] and its variants, and decoder-only models such as the GPT models [38,39].\n\nEncoder-only models.Considering BERT as an example of an encoder-only model, the first token is used for the classification, i.e, r = 1.Usually, a special token [CLS] is prepended to the text at position 1, however this is not strictly necessary for the functioning of the model.\n\nDecoder-only models.In contrast, decoder-models like GPT-2 [39] add the classification head on top of the last token for classification, i.e., r = |t|.A key difference is that in GPT-2 and other decoder-only models, a causal mask is laid over the attention matrix, resulting in \u03b1 i,j = 0 for j > i.This encodes the constraint that tokens can only attend to themselves or to previous ones.",
            "reference_string": "[269982953 | Leemann et al. | 2024 | Citations: 5]"
        },
        {
            "title": "Predictability and Causality in Spanish and English Natural Language Generation",
            "venue": "IEEE Access",
            "year": 2024,
            "reference_count": 58,
            "citation_count": 1,
            "influential_citation_count": 0,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://doi.org/10.1109/access.2024.3420710",
                "status": "GOLD",
                "license": "CCBY",
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2408.14283, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2222734467",
                    "name": "Andrea Busto-Casti\u00f1eira"
                },
                {
                    "authorId": "2323809078",
                    "name": "Francisco Javier Gonz\u00e1lez-Casta\u00f1o"
                },
                {
                    "authorId": "1405165681",
                    "name": "Silvia Garc\u00eda-M\u00e9ndez"
                },
                {
                    "authorId": "2034282614",
                    "name": "Francisco de Arriba-P\u00e9rez"
                }
            ],
            "abstract": "In recent years, the field of Natural Language Generation (NLG) has been boosted by the recent advances in deep learning technologies. Nonetheless, these new data-intensive methods introduce language-dependent disparities in NLG as the main training data sets are in English. Also, most neural NLG systems use decoder-only (causal) transformer language models, which work well for English, but were not designed with other languages in mind. In this work we depart from the hypothesis that they may introduce generation bias in target languages with less rigid word ordering, subject omission, or different attachment preferences for relative clauses, so that for these target languages other language generation strategies may be more desirable. This paper first compares causal and non-causal language modeling for English and Spanish, two languages with different grammatical structures and over 1.5 billion and 0.5 billion speakers, respectively. For this purpose, we define a novel metric of average causal and non-causal context-conditioned entropy of the grammatical category distribution for both languages as an information-theoretic a priori approach. The evaluation of natural text sources (such as training data) in both languages reveals lower average non-causal conditional entropy in Spanish and lower causal conditional entropy in English. According to this experiment, Spanish is more predictable than English given a non-causal context. Then, by applying a conditional relative entropy metric to text generation experiments, we obtain as insights that the best performance is respectively achieved with causal NLG in English, and with non-causal NLG in Spanish. These insights support further research in NLG in Spanish using bidirectional transformer language models.",
            "corpus_id": 270832367,
            "sentences": [
                {
                    "corpus_id": "270832367",
                    "title": "Predictability and Causality in Spanish and English Natural Language Generation",
                    "text": "The contextual awareness of a transformer is controlled by self-attention. The base concept behind this attention mechanism is a mapping of a query (q) into pairs of keys (k) and values (v). By respectively denoting the queries', keys', and value sets' matrices as Q, K and V, we define self-attention as: \n\nTransformers, rather than a single attention function, project queries, keys, and values onto h separate heads. This is called multi-head attention: \n\n(2) By denoting each head attention function as: \n\nwhere W Q i , W K i , W V i and W O are parameter projection matrices for the queries, keys, values, and output respectively. This attention mechanism is present in all the layers of both the encoder and the decoder, if present. While the encoder's attention is bidirectional, the decoder has two different types of attention: (i) a masked multi-head attention block that masks non-causal context and (ii) a bidirectional multihead attention block that receives non-causal information from the encoder. \n\nEven though this encoder-decoder architecture is popular in some NLP tasks such as machine translation [20], [21], \n\n[22], [23], several transformer-based models only have one of these components. By omitting the encoder in decoder-only transformers, all non-causal contextual dependencies are removed by exclusively using masked attention. Decoder-only transformers are nowadays the best performing task-agnostic NLG systems. Nevertheless, there exist some state-of-theart non-causal NLG solutions. For example, non-causal language models can be trained for the Masked Language Modeling (MLM) objective, a task in which the language model predicts masked words within a sentence [24]. Typically, non-causal NLG systems are focused on particular tasks such as speech recognition [25], [26], [27], style transfer and grammar correction [28], textual data augmentation [29], and task-specific dialog systems [30], [31].",
                    "score": 0.4827920080482846,
                    "section_title": "A. CAUSALITY IN GENERATIVE TRANSFORMER LANGUAGE MODELS",
                    "char_start_offset": 5984,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 74
                        },
                        {
                            "start": 75,
                            "end": 190
                        },
                        {
                            "start": 191,
                            "end": 305
                        },
                        {
                            "start": 308,
                            "end": 419
                        },
                        {
                            "start": 420,
                            "end": 456
                        },
                        {
                            "start": 459,
                            "end": 507
                        },
                        {
                            "start": 510,
                            "end": 635
                        },
                        {
                            "start": 636,
                            "end": 738
                        },
                        {
                            "start": 739,
                            "end": 1012
                        },
                        {
                            "start": 1015,
                            "end": 1129
                        },
                        {
                            "start": 1132,
                            "end": 1211
                        },
                        {
                            "start": 1212,
                            "end": 1355
                        },
                        {
                            "start": 1356,
                            "end": 1441
                        },
                        {
                            "start": 1442,
                            "end": 1514
                        },
                        {
                            "start": 1515,
                            "end": 1700
                        },
                        {
                            "start": 1701,
                            "end": 1932
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 1118,
                            "end": 1122,
                            "matchedPaperCorpusId": "219463386"
                        },
                        {
                            "start": 1124,
                            "end": 1128,
                            "matchedPaperCorpusId": "209832341"
                        },
                        {
                            "start": 1138,
                            "end": 1142,
                            "matchedPaperCorpusId": "234785837"
                        },
                        {
                            "start": 1695,
                            "end": 1699,
                            "matchedPaperCorpusId": "265629619"
                        },
                        {
                            "start": 1794,
                            "end": 1798,
                            "matchedPaperCorpusId": "231924507"
                        },
                        {
                            "start": 1800,
                            "end": 1804,
                            "matchedPaperCorpusId": "231715684"
                        },
                        {
                            "start": 1806,
                            "end": 1810,
                            "matchedPaperCorpusId": "247126308"
                        },
                        {
                            "start": 1850,
                            "end": 1854,
                            "matchedPaperCorpusId": "218487230"
                        },
                        {
                            "start": 1882,
                            "end": 1886,
                            "matchedPaperCorpusId": "208224776"
                        },
                        {
                            "start": 1921,
                            "end": 1925,
                            "matchedPaperCorpusId": "210839508"
                        },
                        {
                            "start": 1927,
                            "end": 1931,
                            "matchedPaperCorpusId": "212657570"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.69140625
                }
            ],
            "relevance_judgement": 0.69140625,
            "relevance_judgment_input_expanded": "# Title: Predictability and Causality in Spanish and English Natural Language Generation\n# Venue: IEEE Access\n# Authors: Andrea Busto-Casti\u00f1eira, Francisco Javier Gonz\u00e1lez-Casta\u00f1o, Silvia Garc\u00eda-M\u00e9ndez, Francisco de Arriba-P\u00e9rez\n## Abstract\nIn recent years, the field of Natural Language Generation (NLG) has been boosted by the recent advances in deep learning technologies. Nonetheless, these new data-intensive methods introduce language-dependent disparities in NLG as the main training data sets are in English. Also, most neural NLG systems use decoder-only (causal) transformer language models, which work well for English, but were not designed with other languages in mind. In this work we depart from the hypothesis that they may introduce generation bias in target languages with less rigid word ordering, subject omission, or different attachment preferences for relative clauses, so that for these target languages other language generation strategies may be more desirable. This paper first compares causal and non-causal language modeling for English and Spanish, two languages with different grammatical structures and over 1.5 billion and 0.5 billion speakers, respectively. For this purpose, we define a novel metric of average causal and non-causal context-conditioned entropy of the grammatical category distribution for both languages as an information-theoretic a priori approach. The evaluation of natural text sources (such as training data) in both languages reveals lower average non-causal conditional entropy in Spanish and lower causal conditional entropy in English. According to this experiment, Spanish is more predictable than English given a non-causal context. Then, by applying a conditional relative entropy metric to text generation experiments, we obtain as insights that the best performance is respectively achieved with causal NLG in English, and with non-causal NLG in Spanish. These insights support further research in NLG in Spanish using bidirectional transformer language models.\n## A. CAUSALITY IN GENERATIVE TRANSFORMER LANGUAGE MODELS\nThe contextual awareness of a transformer is controlled by self-attention. The base concept behind this attention mechanism is a mapping of a query (q) into pairs of keys (k) and values (v). By respectively denoting the queries', keys', and value sets' matrices as Q, K and V, we define self-attention as: \n\nTransformers, rather than a single attention function, project queries, keys, and values onto h separate heads. This is called multi-head attention: \n\n(2) By denoting each head attention function as: \n\nwhere W Q i , W K i , W V i and W O are parameter projection matrices for the queries, keys, values, and output respectively. This attention mechanism is present in all the layers of both the encoder and the decoder, if present. While the encoder's attention is bidirectional, the decoder has two different types of attention: (i) a masked multi-head attention block that masks non-causal context and (ii) a bidirectional multihead attention block that receives non-causal information from the encoder. \n\nEven though this encoder-decoder architecture is popular in some NLP tasks such as machine translation [20], [21], \n\n[22], [23], several transformer-based models only have one of these components. By omitting the encoder in decoder-only transformers, all non-causal contextual dependencies are removed by exclusively using masked attention. Decoder-only transformers are nowadays the best performing task-agnostic NLG systems. Nevertheless, there exist some state-of-theart non-causal NLG solutions. For example, non-causal language models can be trained for the Masked Language Modeling (MLM) objective, a task in which the language model predicts masked words within a sentence [24]. Typically, non-causal NLG systems are focused on particular tasks such as speech recognition [25], [26], [27], style transfer and grammar correction [28], textual data augmentation [29], and task-specific dialog systems [30], [31].",
            "reference_string": "[270832367 | Busto-Castineira et al. | 2024 | Citations: 1]"
        },
        {
            "title": "Warm-Starting for Improving the Novelty of Abstractive Summarization",
            "venue": "IEEE Access",
            "year": 2023,
            "reference_count": 48,
            "citation_count": 1,
            "influential_citation_count": 0,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://doi.org/10.1109/access.2023.3322226",
                "status": "GOLD",
                "license": "CCBYNCND",
                "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.1109/ACCESS.2023.3322226?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1109/ACCESS.2023.3322226, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2130042263",
                    "name": "Ayham Alomari"
                },
                {
                    "authorId": "1403466899",
                    "name": "A. S. Al-Shamayleh"
                },
                {
                    "authorId": "36826893",
                    "name": "N. Idris"
                },
                {
                    "authorId": "2049063550",
                    "name": "Aznul Qalid Md Sabri"
                },
                {
                    "authorId": "1770016",
                    "name": "I. Alsmadi"
                },
                {
                    "authorId": "2182454665",
                    "name": "Danah Omary"
                }
            ],
            "abstract": "Abstractive summarization is distinguished by using novel phrases that are not found in the source text. However, most previous research ignores this feature in favour of enhancing syntactical similarity with the reference. To improve novelty aspects, we have used multiple warm-started models with varying encoder and decoder checkpoints and vocabulary. These models are then adapted to the paraphrasing task and the sampling decoding strategy to further boost the levels of novelty and quality. In addition, to avoid relying only on the syntactical similarity assessment, two additional abstractive summarization metrics are introduced: 1) NovScore: a new novelty metric that delivers a summary novelty score; and 2) NSSF: a new comprehensive metric that ensembles Novelty, Syntactic, Semantic, and Faithfulness features into a single score to simulate human assessment in providing a reliable evaluation. Finally, we compare our models to the state-of-the-art sequence-to-sequence models using the current and the proposed metrics. As a result, warm-starting, sampling, and paraphrasing improve novelty degrees by 2%, 5%, and 14%, respectively, while maintaining comparable scores on other metrics.",
            "corpus_id": 263729712,
            "sentences": [
                {
                    "corpus_id": "263729712",
                    "title": "Warm-Starting for Improving the Novelty of Abstractive Summarization",
                    "text": "For the decoder part, we leverage GPT2, BERT, RoBERTa, and Ernie 2.0. GPT2 is trained on open-ended autoregressive generation with the objective of predicting the next word in a sequence of a few token starters (causal language modeling) utilizing only the Transformer's unidirectional ''left-to-right'' self-attention decoder. Theoretically, GPT2 is claimed to be the optimal design for use as a decoder. However, with a few adjustments, bi-directional encoderonly pretrained models such as BERT, RoBERTa, and Ernie 2.0 may simply be adapted as decoders. To compare the encoder and decoder architectures of the Transformer, encoder blocks consist solely of a bi-directional self-attention layer and two feed-forward layers. By contrast, decoder blocks have a unidirectional self-attention layer, a crossattention layer, and two feed-forward layers. In addition, a language model head layer follows the decoder blocks, converting the last decoder block's output vectors to logit vectors. As a result, the following steps have been taken to enable an encoder to function as a decoder: First, we alter the self-attention layers to operate unidirectionally, similar to the decoder, and initialize them with the weights from the encoder's self-attention layer. A cross-attention layer is then added between the self-attention layer and the two feed-forward layers. As suggested by [6], we randomize the initialization of this layer's weights, which are subsequently trained while finetuning the model on the summarization task. Finally, we add a language model head layer on top of the last block of the decoder and initialize it with the weights of the encoder's word embeddings. Table 3 highlights the necessary adjustments for this approach. It is worth mentioning that the hidden size of the encoder and decoder in warm-started models must match in order for them to communicate and perform dot products on their respective vectors.",
                    "score": 0.5063729849034757,
                    "section_title": "2) DECODERS",
                    "char_start_offset": 21434,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 69
                        },
                        {
                            "start": 70,
                            "end": 327
                        },
                        {
                            "start": 328,
                            "end": 405
                        },
                        {
                            "start": 406,
                            "end": 555
                        },
                        {
                            "start": 556,
                            "end": 724
                        },
                        {
                            "start": 725,
                            "end": 849
                        },
                        {
                            "start": 850,
                            "end": 987
                        },
                        {
                            "start": 988,
                            "end": 1256
                        },
                        {
                            "start": 1257,
                            "end": 1360
                        },
                        {
                            "start": 1361,
                            "end": 1523
                        },
                        {
                            "start": 1524,
                            "end": 1676
                        },
                        {
                            "start": 1677,
                            "end": 1740
                        },
                        {
                            "start": 1741,
                            "end": 1932
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 1377,
                            "end": 1380,
                            "matchedPaperCorpusId": "198967997"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.6669921875
                }
            ],
            "relevance_judgement": 0.6669921875,
            "relevance_judgment_input_expanded": "# Title: Warm-Starting for Improving the Novelty of Abstractive Summarization\n# Venue: IEEE Access\n# Authors: Ayham Alomari, A. S. Al-Shamayleh, N. Idris, Aznul Qalid Md Sabri, I. Alsmadi, Danah Omary\n## Abstract\nAbstractive summarization is distinguished by using novel phrases that are not found in the source text. However, most previous research ignores this feature in favour of enhancing syntactical similarity with the reference. To improve novelty aspects, we have used multiple warm-started models with varying encoder and decoder checkpoints and vocabulary. These models are then adapted to the paraphrasing task and the sampling decoding strategy to further boost the levels of novelty and quality. In addition, to avoid relying only on the syntactical similarity assessment, two additional abstractive summarization metrics are introduced: 1) NovScore: a new novelty metric that delivers a summary novelty score; and 2) NSSF: a new comprehensive metric that ensembles Novelty, Syntactic, Semantic, and Faithfulness features into a single score to simulate human assessment in providing a reliable evaluation. Finally, we compare our models to the state-of-the-art sequence-to-sequence models using the current and the proposed metrics. As a result, warm-starting, sampling, and paraphrasing improve novelty degrees by 2%, 5%, and 14%, respectively, while maintaining comparable scores on other metrics.\n## 2) DECODERS\nFor the decoder part, we leverage GPT2, BERT, RoBERTa, and Ernie 2.0. GPT2 is trained on open-ended autoregressive generation with the objective of predicting the next word in a sequence of a few token starters (causal language modeling) utilizing only the Transformer's unidirectional ''left-to-right'' self-attention decoder. Theoretically, GPT2 is claimed to be the optimal design for use as a decoder. However, with a few adjustments, bi-directional encoderonly pretrained models such as BERT, RoBERTa, and Ernie 2.0 may simply be adapted as decoders. To compare the encoder and decoder architectures of the Transformer, encoder blocks consist solely of a bi-directional self-attention layer and two feed-forward layers. By contrast, decoder blocks have a unidirectional self-attention layer, a crossattention layer, and two feed-forward layers. In addition, a language model head layer follows the decoder blocks, converting the last decoder block's output vectors to logit vectors. As a result, the following steps have been taken to enable an encoder to function as a decoder: First, we alter the self-attention layers to operate unidirectionally, similar to the decoder, and initialize them with the weights from the encoder's self-attention layer. A cross-attention layer is then added between the self-attention layer and the two feed-forward layers. As suggested by [6], we randomize the initialization of this layer's weights, which are subsequently trained while finetuning the model on the summarization task. Finally, we add a language model head layer on top of the last block of the decoder and initialize it with the weights of the encoder's word embeddings. Table 3 highlights the necessary adjustments for this approach. It is worth mentioning that the hidden size of the encoder and decoder in warm-started models must match in order for them to communicate and perform dot products on their respective vectors.",
            "reference_string": "[263729712 | Alomari et al. | 2023 | Citations: 1]"
        },
        {
            "title": "A Unified Cascaded Encoder ASR Model for Dynamic Model Sizes",
            "venue": "Interspeech",
            "year": 2022,
            "reference_count": 30,
            "citation_count": 17,
            "influential_citation_count": 0,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://arxiv.org/pdf/2204.06164",
                "status": "GREEN",
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2204.06164, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "51267247",
                    "name": "Shaojin Ding"
                },
                {
                    "authorId": "2117928344",
                    "name": "Weiran Wang"
                },
                {
                    "authorId": "47783130",
                    "name": "Ding Zhao"
                },
                {
                    "authorId": "1784851",
                    "name": "Tara N. Sainath"
                },
                {
                    "authorId": "2145999837",
                    "name": "Yanzhang He"
                },
                {
                    "authorId": "2061545932",
                    "name": "R. David"
                },
                {
                    "authorId": "2004995",
                    "name": "Rami Botros"
                },
                {
                    "authorId": "2153688851",
                    "name": "Xin Wang"
                },
                {
                    "authorId": "1679451",
                    "name": "R. Panigrahy"
                },
                {
                    "authorId": "2055746849",
                    "name": "Qiao Liang"
                },
                {
                    "authorId": "2241835900",
                    "name": "Dongseong Hwang"
                },
                {
                    "authorId": "143685627",
                    "name": "Ian McGraw"
                },
                {
                    "authorId": "2557391",
                    "name": "Rohit Prabhavalkar"
                },
                {
                    "authorId": "2985957",
                    "name": "Trevor Strohman"
                }
            ],
            "abstract": "In this paper, we propose a dynamic cascaded encoder Automatic Speech Recognition (ASR) model, which unifies models for different deployment scenarios. Moreover, the model can significantly reduce model size and power consumption without loss of quality. Namely, with the dynamic cascaded encoder model, we explore three techniques to maximally boost the performance of each model size: 1) Use separate decoders for each sub-model while sharing the encoders; 2) Use funnel-pooling to improve the encoder efficiency; 3) Balance the size of causal and non-causal encoders to improve quality and fit deployment constraints. Overall, the proposed large-medium model has 30% smaller size and reduces power consumption by 33%, compared to the baseline cascaded encoder model. The triple-size model that unifies the large, medium, and small models achieves 37% total size reduction with minimal quality loss, while substantially reducing the engineering efforts of having separate models.",
            "corpus_id": 248157514,
            "sentences": [
                {
                    "corpus_id": "248157514",
                    "title": "A Unified Cascaded Encoder ASR Model for Dynamic Model Sizes",
                    "text": "The original cascaded encoder model [22] uses a shared RNN-T decoder. The decoder works with a causal encoder in the first pass to provide streaming recognition results, and works with an additional non-causal encoder that sits on top of the causal encoder to provide more accurate final results, leveraging audio right context extracted by the noncausal encoder. Therefore, the same decoder has to deal with features of different context, and we observe tension between the performance of the passes as we try to reduce the model size, i.e., as we assign more loss weights for the causal pass to satisfy WER target, the accuracy of the non-causal pass degrades. \n\nIn this work, we propose to use smaller separate decoders in each sub-model, to better cope with the different context, and this significantly alleviates the tension between different submodels:",
                    "score": 0.49048942896748615,
                    "section_title": "Separate decoders",
                    "char_start_offset": 6492,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 69
                        },
                        {
                            "start": 70,
                            "end": 363
                        },
                        {
                            "start": 364,
                            "end": 662
                        },
                        {
                            "start": 665,
                            "end": 859
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 36,
                            "end": 40,
                            "matchedPaperCorpusId": "225094578"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.66259765625
                }
            ],
            "relevance_judgement": 0.66259765625,
            "relevance_judgment_input_expanded": "# Title: A Unified Cascaded Encoder ASR Model for Dynamic Model Sizes\n# Venue: Interspeech\n# Authors: Shaojin Ding, Weiran Wang, Ding Zhao, Tara N. Sainath, Yanzhang He, R. David, Rami Botros, Xin Wang, R. Panigrahy, Qiao Liang, Dongseong Hwang, Ian McGraw, Rohit Prabhavalkar, Trevor Strohman\n## Abstract\nIn this paper, we propose a dynamic cascaded encoder Automatic Speech Recognition (ASR) model, which unifies models for different deployment scenarios. Moreover, the model can significantly reduce model size and power consumption without loss of quality. Namely, with the dynamic cascaded encoder model, we explore three techniques to maximally boost the performance of each model size: 1) Use separate decoders for each sub-model while sharing the encoders; 2) Use funnel-pooling to improve the encoder efficiency; 3) Balance the size of causal and non-causal encoders to improve quality and fit deployment constraints. Overall, the proposed large-medium model has 30% smaller size and reduces power consumption by 33%, compared to the baseline cascaded encoder model. The triple-size model that unifies the large, medium, and small models achieves 37% total size reduction with minimal quality loss, while substantially reducing the engineering efforts of having separate models.\n## Separate decoders\nThe original cascaded encoder model [22] uses a shared RNN-T decoder. The decoder works with a causal encoder in the first pass to provide streaming recognition results, and works with an additional non-causal encoder that sits on top of the causal encoder to provide more accurate final results, leveraging audio right context extracted by the noncausal encoder. Therefore, the same decoder has to deal with features of different context, and we observe tension between the performance of the passes as we try to reduce the model size, i.e., as we assign more loss weights for the causal pass to satisfy WER target, the accuracy of the non-causal pass degrades. \n\nIn this work, we propose to use smaller separate decoders in each sub-model, to better cope with the different context, and this significantly alleviates the tension between different submodels:",
            "reference_string": "[248157514 | Ding et al. | 2022 | Citations: 17]"
        },
        {
            "title": "A Survey on Large Language Models for Code Generation",
            "venue": "arXiv.org",
            "year": 2024,
            "reference_count": 245,
            "citation_count": 197,
            "influential_citation_count": 8,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2406.00515, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2294682530",
                    "name": "Juyong Jiang"
                },
                {
                    "authorId": "2304542351",
                    "name": "Fan Wang"
                },
                {
                    "authorId": "2305041631",
                    "name": "Jiasi Shen"
                },
                {
                    "authorId": "2304525068",
                    "name": "Sungju Kim"
                },
                {
                    "authorId": "2257349580",
                    "name": "Sunghun Kim"
                }
            ],
            "abstract": "Large Language Models (LLMs) have garnered remarkable advancements across diverse code-related tasks, known as Code LLMs, particularly in code generation that generates source code with LLM from natural language descriptions. This burgeoning field has captured significant interest from both academic researchers and industry professionals due to its practical significance in software development, e.g., GitHub Copilot. Despite the active exploration of LLMs for a variety of code tasks, either from the perspective of natural language processing (NLP) or software engineering (SE) or both, there is a noticeable absence of a comprehensive and up-to-date literature review dedicated to LLM for code generation. In this survey, we aim to bridge this gap by providing a systematic literature review that serves as a valuable reference for researchers investigating the cutting-edge progress in LLMs for code generation. We introduce a taxonomy to categorize and discuss the recent developments in LLMs for code generation, covering aspects such as data curation, latest advances, performance evaluation, ethical implications, environmental impact, and real-world applications. In addition, we present a historical overview of the evolution of LLMs for code generation and offer an empirical comparison using the HumanEval, MBPP, and BigCodeBench benchmarks across various levels of difficulty and types of programming tasks to highlight the progressive enhancements in LLM capabilities for code generation. We identify critical challenges and promising opportunities regarding the gap between academia and practical development. Furthermore, we have established a dedicated resource GitHub page (https://github.com/juyongjiang/CodeLLMSurvey) to continuously document and disseminate the most recent advances in the field.",
            "corpus_id": 270214176,
            "sentences": [
                {
                    "corpus_id": "270214176",
                    "title": "A Survey on Large Language Models for Code Generation",
                    "text": "where x < represents the sequence of preceding tokens { 1 , . . .,   \u22121 } before x  in the input,  denotes the model parameters.The conditional probability   (  |x < )) is modeled by adding a causal attention mask to the multi-head self-attention matrix of each Transformer block.To be specific, causal attention masking is implemented by setting the lower triangular part of the matrix to 0 and the remaining elements to \u2212\u221e, ensuring that each token   attends only to its predecessors and itself.On the contrary, in encoder-decoder LLMs, a pivot token   is randomly selected in a sequence of tokens and then regarding the context before it as the source sequence x  = { 1 , . . .,   } of the encoder and the sequence after it as the target output x  = { +1 , . . .,   } of decoder.Formally, the causal language modeling objective for training encoder-decoder LLMs is to minimize loss function as follows:\n\nwhere x \u2264 is the source sequence input and x < denotes the target sequence autoregressively generated so far.During the inference phase, pre-trained LLMs that have been trained on largescale code corpus can generate code in a zero-shot manner without the need for fine-tuning.This is achieved through the technique of prompt engineering, which guides the model to produce the desired output11 [31,186].Additionally, recent studies have explored the use of few-shot learning, also referred to as in-context learning, to enhance model performance further [131,178].Denoising Autoencoding.In addition to causal language modeling (CLM), the denoising autoencoding (DAE) task has been extensively applied in pre-training encoder-decoder architectures for code generation, such as PLBART [6], CodeT5 [234], and its enhanced successor, CodeT5+ [232].Following T5 [189] and CodeT5 [234], the DAE refers to initially perturbing the source sequence by introducing randomly masked spans of varying lengths.This corrupted sequence serves as the input for the encoder.",
                    "score": 0.47604966510043667,
                    "section_title": "Pre-training",
                    "char_start_offset": 42181,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 65
                        },
                        {
                            "start": 65,
                            "end": 128
                        },
                        {
                            "start": 128,
                            "end": 280
                        },
                        {
                            "start": 280,
                            "end": 497
                        },
                        {
                            "start": 497,
                            "end": 680
                        },
                        {
                            "start": 680,
                            "end": 765
                        },
                        {
                            "start": 765,
                            "end": 782
                        },
                        {
                            "start": 782,
                            "end": 905
                        },
                        {
                            "start": 907,
                            "end": 1016
                        },
                        {
                            "start": 1016,
                            "end": 1183
                        },
                        {
                            "start": 1183,
                            "end": 1309
                        },
                        {
                            "start": 1309,
                            "end": 1470
                        },
                        {
                            "start": 1470,
                            "end": 1493
                        },
                        {
                            "start": 1493,
                            "end": 1750
                        },
                        {
                            "start": 1750,
                            "end": 1902
                        },
                        {
                            "start": 1902,
                            "end": 1962
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 1300,
                            "end": 1304,
                            "matchedPaperCorpusId": "218971783"
                        },
                        {
                            "start": 1304,
                            "end": 1308,
                            "matchedPaperCorpusId": "160025533"
                        },
                        {
                            "start": 1701,
                            "end": 1706,
                            "matchedPaperCorpusId": "237386541"
                        },
                        {
                            "start": 1744,
                            "end": 1749,
                            "matchedPaperCorpusId": "258685677"
                        },
                        {
                            "start": 1763,
                            "end": 1768,
                            "matchedPaperCorpusId": "204838007"
                        },
                        {
                            "start": 1780,
                            "end": 1785,
                            "matchedPaperCorpusId": "237386541"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.66064453125
                }
            ],
            "relevance_judgement": 0.66064453125,
            "relevance_judgment_input_expanded": "# Title: A Survey on Large Language Models for Code Generation\n# Venue: arXiv.org\n# Authors: Juyong Jiang, Fan Wang, Jiasi Shen, Sungju Kim, Sunghun Kim\n## Abstract\nLarge Language Models (LLMs) have garnered remarkable advancements across diverse code-related tasks, known as Code LLMs, particularly in code generation that generates source code with LLM from natural language descriptions. This burgeoning field has captured significant interest from both academic researchers and industry professionals due to its practical significance in software development, e.g., GitHub Copilot. Despite the active exploration of LLMs for a variety of code tasks, either from the perspective of natural language processing (NLP) or software engineering (SE) or both, there is a noticeable absence of a comprehensive and up-to-date literature review dedicated to LLM for code generation. In this survey, we aim to bridge this gap by providing a systematic literature review that serves as a valuable reference for researchers investigating the cutting-edge progress in LLMs for code generation. We introduce a taxonomy to categorize and discuss the recent developments in LLMs for code generation, covering aspects such as data curation, latest advances, performance evaluation, ethical implications, environmental impact, and real-world applications. In addition, we present a historical overview of the evolution of LLMs for code generation and offer an empirical comparison using the HumanEval, MBPP, and BigCodeBench benchmarks across various levels of difficulty and types of programming tasks to highlight the progressive enhancements in LLM capabilities for code generation. We identify critical challenges and promising opportunities regarding the gap between academia and practical development. Furthermore, we have established a dedicated resource GitHub page (https://github.com/juyongjiang/CodeLLMSurvey) to continuously document and disseminate the most recent advances in the field.\n## Pre-training\nwhere x < represents the sequence of preceding tokens { 1 , . . .,   \u22121 } before x  in the input,  denotes the model parameters.The conditional probability   (  |x < )) is modeled by adding a causal attention mask to the multi-head self-attention matrix of each Transformer block.To be specific, causal attention masking is implemented by setting the lower triangular part of the matrix to 0 and the remaining elements to \u2212\u221e, ensuring that each token   attends only to its predecessors and itself.On the contrary, in encoder-decoder LLMs, a pivot token   is randomly selected in a sequence of tokens and then regarding the context before it as the source sequence x  = { 1 , . . .,   } of the encoder and the sequence after it as the target output x  = { +1 , . . .,   } of decoder.Formally, the causal language modeling objective for training encoder-decoder LLMs is to minimize loss function as follows:\n\nwhere x \u2264 is the source sequence input and x < denotes the target sequence autoregressively generated so far.During the inference phase, pre-trained LLMs that have been trained on largescale code corpus can generate code in a zero-shot manner without the need for fine-tuning.This is achieved through the technique of prompt engineering, which guides the model to produce the desired output11 [31,186].Additionally, recent studies have explored the use of few-shot learning, also referred to as in-context learning, to enhance model performance further [131,178].Denoising Autoencoding.In addition to causal language modeling (CLM), the denoising autoencoding (DAE) task has been extensively applied in pre-training encoder-decoder architectures for code generation, such as PLBART [6], CodeT5 [234], and its enhanced successor, CodeT5+ [232].Following T5 [189] and CodeT5 [234], the DAE refers to initially perturbing the source sequence by introducing randomly masked spans of varying lengths.This corrupted sequence serves as the input for the encoder.",
            "reference_string": "[270214176 | Jiang et al. | 2024 | Citations: 197]"
        },
        {
            "title": "Representation Learning for Stack Overflow Posts: How Far Are We?",
            "venue": "ACM Transactions on Software Engineering and Methodology",
            "year": 2023,
            "reference_count": 71,
            "citation_count": 20,
            "influential_citation_count": 0,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://dl.acm.org/doi/pdf/10.1145/3635711",
                "status": "HYBRID",
                "license": "other-oa",
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2303.06853, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2158107537",
                    "name": "Junda He"
                },
                {
                    "authorId": "2211431558",
                    "name": "Zhou Xin"
                },
                {
                    "authorId": "2203459",
                    "name": "Bowen Xu"
                },
                {
                    "authorId": "2146322053",
                    "name": "Ting Zhang"
                },
                {
                    "authorId": "35276441",
                    "name": "Kisub Kim"
                },
                {
                    "authorId": "2139059234",
                    "name": "Zhou Yang"
                },
                {
                    "authorId": "2121315",
                    "name": "Ferdian Thung"
                },
                {
                    "authorId": "9223952",
                    "name": "I. Irsan"
                },
                {
                    "authorId": "2150912791",
                    "name": "David Lo"
                }
            ],
            "abstract": "The tremendous success of Stack Overflow has accumulated an extensive corpus of software engineering knowledge, thus motivating researchers to propose various solutions for analyzing its content. The performance of such solutions hinges significantly on the selection of representation models for Stack Overflow posts. As the volume of literature on Stack Overflow continues to burgeon, it highlights the need for a powerful Stack Overflow post representation model and drives researchers\u2019 interest in developing specialized representation models that can adeptly capture the intricacies of Stack Overflow posts. The state-of-the-art (SOTA) Stack Overflow post representation models are Post2Vec and BERTOverflow, which are built upon neural networks such as convolutional neural network and transformer architecture (e.g., BERT). Despite their promising results, these representation methods have not been evaluated in the same experimental setting. To fill the research gap, we first empirically compare the performance of the representation models designed specifically for Stack Overflow posts (Post2Vec and BERTOverflow) in a wide range of related tasks (i.e., tag recommendation, relatedness prediction, and API recommendation). The results show that Post2Vec cannot further improve the SOTA techniques of the considered downstream tasks, and BERTOverflow shows surprisingly poor performance. To find more suitable representation models for the posts, we further explore a diverse set of transformer-based models, including (1) general domain language models (RoBERTa, Longformer, and GPT2) and (2) language models built with software engineering related textual artifacts (CodeBERT, GraphCodeBERT, seBERT, CodeT5, PLBart, and CodeGen). This exploration shows that models like CodeBERT and RoBERTa are suitable for representing Stack Overflow posts. However, it also illustrates the \u201cNo Silver Bullet\u201d concept, as none of the models consistently wins against all the others. Inspired by the findings, we propose SOBERT, which employs a simple yet effective strategy to improve the representation models of Stack Overflow posts by continuing the pre-training phase with the textual artifact from Stack Overflow. The overall experimental results demonstrate that SOBERT can consistently outperform the considered models and increase the SOTA performance significantly for all the downstream tasks.",
            "corpus_id": 257496129,
            "sentences": [
                {
                    "corpus_id": "257496129",
                    "title": "Representation Learning for Stack Overflow Posts: How Far Are We?",
                    "text": "Transformer-based language models have revolutionized the landscape of representation learning in natural language processing (NLP) [10,26,36]. Their efficacy in capturing text semantics has led to unparalleled performance in various applications, such as sentiment analysis [43], POS tagging [45], and question answering [34]. The vanilla transformer architecture [48] is composed of the encoder and decoder components. Based on the usage of these components, transformer-based language models can be categorized into three types: encoder-only, decoder-only, and encoderdecoder models. \n\nEncoder-only models exclusively leverage the encoder stacks of the vanilla transformer [48] architecture. BERT [10] stands as a prominent encoder-only representation model, which learns a bidirectional contextual representation of text. BERT proposes the Masked Language Modeling (MLM) task at the pre-training phase. In MLM, the input data is corrupted by randomly masking 15% of the tokens, and then the BERT model learns to reconstruct the original data by predicting the masked words. BERT is extensively pre-trained on large-scale datasets, which learn a meaningful representation that is reusable for various tasks, thus eliminating the process of training language models from scratch and saving time and resources. \n\nIn contrast, Decoder-only models consist solely of the decoder components of the original transformer architecture. A notable instance of such models is the GPT [35], GPT operates under a causal language modeling (CLM) framework during its training phase. CLM is a strategy where the model predicts the next token in a sequence while only considering preceding tokens. In other words, this design restricts the model from accessing future tokens in the sequence. \n\nBridging the above approaches, textitEncoder-decoder models integrate both the encoder and decoder components of the transformer architecture. Popular encoder-decoder models involve T5 [37] and BART [23]. The T5 model [37] advocates a unified text-to-text framework that converts various language tasks into a consistent text-to-text format.",
                    "score": 0.4778044410451196,
                    "section_title": "Transformer-based Language Models",
                    "char_start_offset": 6666,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 143
                        },
                        {
                            "start": 144,
                            "end": 327
                        },
                        {
                            "start": 328,
                            "end": 420
                        },
                        {
                            "start": 421,
                            "end": 586
                        },
                        {
                            "start": 589,
                            "end": 694
                        },
                        {
                            "start": 695,
                            "end": 825
                        },
                        {
                            "start": 826,
                            "end": 906
                        },
                        {
                            "start": 907,
                            "end": 1077
                        },
                        {
                            "start": 1078,
                            "end": 1311
                        },
                        {
                            "start": 1314,
                            "end": 1429
                        },
                        {
                            "start": 1430,
                            "end": 1569
                        },
                        {
                            "start": 1570,
                            "end": 1682
                        },
                        {
                            "start": 1683,
                            "end": 1776
                        },
                        {
                            "start": 1779,
                            "end": 1921
                        },
                        {
                            "start": 1922,
                            "end": 1983
                        },
                        {
                            "start": 1984,
                            "end": 2120
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 132,
                            "end": 136,
                            "matchedPaperCorpusId": "52967399"
                        },
                        {
                            "start": 139,
                            "end": 142,
                            "matchedPaperCorpusId": "160025533"
                        },
                        {
                            "start": 275,
                            "end": 279,
                            "matchedPaperCorpusId": "85459677"
                        },
                        {
                            "start": 293,
                            "end": 297,
                            "matchedPaperCorpusId": "202122780"
                        },
                        {
                            "start": 322,
                            "end": 326,
                            "matchedPaperCorpusId": "153312701"
                        },
                        {
                            "start": 365,
                            "end": 369,
                            "matchedPaperCorpusId": "13756489"
                        },
                        {
                            "start": 676,
                            "end": 680,
                            "matchedPaperCorpusId": "13756489"
                        },
                        {
                            "start": 700,
                            "end": 704,
                            "matchedPaperCorpusId": "52967399"
                        },
                        {
                            "start": 1964,
                            "end": 1968,
                            "matchedPaperCorpusId": "204838007"
                        },
                        {
                            "start": 1978,
                            "end": 1982,
                            "matchedPaperCorpusId": "204960716"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.66015625
                }
            ],
            "relevance_judgement": 0.66015625,
            "relevance_judgment_input_expanded": "# Title: Representation Learning for Stack Overflow Posts: How Far Are We?\n# Venue: ACM Transactions on Software Engineering and Methodology\n# Authors: Junda He, Zhou Xin, Bowen Xu, Ting Zhang, Kisub Kim, Zhou Yang, Ferdian Thung, I. Irsan, David Lo\n## Abstract\nThe tremendous success of Stack Overflow has accumulated an extensive corpus of software engineering knowledge, thus motivating researchers to propose various solutions for analyzing its content. The performance of such solutions hinges significantly on the selection of representation models for Stack Overflow posts. As the volume of literature on Stack Overflow continues to burgeon, it highlights the need for a powerful Stack Overflow post representation model and drives researchers\u2019 interest in developing specialized representation models that can adeptly capture the intricacies of Stack Overflow posts. The state-of-the-art (SOTA) Stack Overflow post representation models are Post2Vec and BERTOverflow, which are built upon neural networks such as convolutional neural network and transformer architecture (e.g., BERT). Despite their promising results, these representation methods have not been evaluated in the same experimental setting. To fill the research gap, we first empirically compare the performance of the representation models designed specifically for Stack Overflow posts (Post2Vec and BERTOverflow) in a wide range of related tasks (i.e., tag recommendation, relatedness prediction, and API recommendation). The results show that Post2Vec cannot further improve the SOTA techniques of the considered downstream tasks, and BERTOverflow shows surprisingly poor performance. To find more suitable representation models for the posts, we further explore a diverse set of transformer-based models, including (1) general domain language models (RoBERTa, Longformer, and GPT2) and (2) language models built with software engineering related textual artifacts (CodeBERT, GraphCodeBERT, seBERT, CodeT5, PLBart, and CodeGen). This exploration shows that models like CodeBERT and RoBERTa are suitable for representing Stack Overflow posts. However, it also illustrates the \u201cNo Silver Bullet\u201d concept, as none of the models consistently wins against all the others. Inspired by the findings, we propose SOBERT, which employs a simple yet effective strategy to improve the representation models of Stack Overflow posts by continuing the pre-training phase with the textual artifact from Stack Overflow. The overall experimental results demonstrate that SOBERT can consistently outperform the considered models and increase the SOTA performance significantly for all the downstream tasks.\n## Transformer-based Language Models\nTransformer-based language models have revolutionized the landscape of representation learning in natural language processing (NLP) [10,26,36]. Their efficacy in capturing text semantics has led to unparalleled performance in various applications, such as sentiment analysis [43], POS tagging [45], and question answering [34]. The vanilla transformer architecture [48] is composed of the encoder and decoder components. Based on the usage of these components, transformer-based language models can be categorized into three types: encoder-only, decoder-only, and encoderdecoder models. \n\nEncoder-only models exclusively leverage the encoder stacks of the vanilla transformer [48] architecture. BERT [10] stands as a prominent encoder-only representation model, which learns a bidirectional contextual representation of text. BERT proposes the Masked Language Modeling (MLM) task at the pre-training phase. In MLM, the input data is corrupted by randomly masking 15% of the tokens, and then the BERT model learns to reconstruct the original data by predicting the masked words. BERT is extensively pre-trained on large-scale datasets, which learn a meaningful representation that is reusable for various tasks, thus eliminating the process of training language models from scratch and saving time and resources. \n\nIn contrast, Decoder-only models consist solely of the decoder components of the original transformer architecture. A notable instance of such models is the GPT [35], GPT operates under a causal language modeling (CLM) framework during its training phase. CLM is a strategy where the model predicts the next token in a sequence while only considering preceding tokens. In other words, this design restricts the model from accessing future tokens in the sequence. \n\nBridging the above approaches, textitEncoder-decoder models integrate both the encoder and decoder components of the transformer architecture. Popular encoder-decoder models involve T5 [37] and BART [23]. The T5 model [37] advocates a unified text-to-text framework that converts various language tasks into a consistent text-to-text format.",
            "reference_string": "[257496129 | He et al. | 2023 | Citations: 20]"
        },
        {
            "title": "Exploring the Roles of Large Language Models in Reshaping Transportation Systems: A Survey, Framework, and Roadmap",
            "venue": "arXiv.org",
            "year": 2025,
            "reference_count": 284,
            "citation_count": 4,
            "influential_citation_count": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2503.21411, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "94168461",
                    "name": "Tong Nie"
                },
                {
                    "authorId": "2028643500",
                    "name": "Jiangming Sun"
                },
                {
                    "authorId": "2277421553",
                    "name": "Wei Ma"
                }
            ],
            "abstract": "Modern transportation systems face pressing challenges due to increasing demand, dynamic environments, and heterogeneous information integration. The rapid evolution of Large Language Models (LLMs) offers transformative potential to address these challenges. Extensive knowledge and high-level capabilities derived from pretraining evolve the default role of LLMs as text generators to become versatile, knowledge-driven task solvers for intelligent transportation systems. This survey first presents LLM4TR, a novel conceptual framework that systematically categorizes the roles of LLMs in transportation into four synergetic dimensions: information processors, knowledge encoders, component generators, and decision facilitators. Through a unified taxonomy, we systematically elucidate how LLMs bridge fragmented data pipelines, enhance predictive analytics, simulate human-like reasoning, and enable closed-loop interactions across sensing, learning, modeling, and managing tasks in transportation systems. For each role, our review spans diverse applications, from traffic prediction and autonomous driving to safety analytics and urban mobility optimization, highlighting how emergent capabilities of LLMs such as in-context learning and step-by-step reasoning can enhance the operation and management of transportation systems. We further curate practical guidance, including available resources and computational guidelines, to support real-world deployment. By identifying challenges in existing LLM-based solutions, this survey charts a roadmap for advancing LLM-driven transportation research, positioning LLMs as central actors in the next generation of cyber-physical-social mobility ecosystems. Online resources can be found in the project page: https://github.com/tongnie/awesome-llm4tr.",
            "corpus_id": 277349741,
            "sentences": [
                {
                    "corpus_id": "277349741",
                    "title": "Exploring the Roles of Large Language Models in Reshaping Transportation Systems: A Survey, Framework, and Roadmap",
                    "text": "Models like T5 (Raffel et al., 2020) and BART (Lewis et al., 2019) utilize both encoding and decoding mechanisms, enabling them to perform a wide range of tasks, including translation and summarization. The encoder applies stacked self-attention layers to encode the input sequence, and the decoder performs cross-attention on these representations and autoregressively generates the output. \n\n\u2022 Causal decoder. As a representative decoder-only architecture, causal decoder models introduce the unidirectional attention mask to ensure that each input token can only attend to the past tokens and itself. This mechanism makes them suitable for text generation tasks. Prominent examples are the GPTseries models (Radford et al., 2018(Radford et al., , 2019;;Brown et al., 2020). \n\n\u2022 Non-causal decoder. Another kind of decoder-only architecture is the non-casual structure. This architecture performs bidirectional attention on prefix tokens and unidirectional attention only on generated tokens. One representative prefix decoder LLMs is GLM (Zeng et al., 2022). \n\nTo scale the capacity of LLMs efficiently, the Mixture of Experts (MoE) technique can be exploited to combine the above architectures, such as in Swich Transformer (Fedus et al., 2022) and GLaM (Du et al., 2022). MoE involves sparsely activating a subset of model parameters (the \"experts\") for each input, allowing the model to handle a vast number of parameters without incurring prohibitive computational costs. This is achieved by employing a trainable gating mechanism to route each input token to the most relevant subset of experts. \n\nApart from the mainstream Transformer architecture, there are also emerging architectures proposed to alleviate the inherent issues of Transformers (e.g., the quadratic complexity) such as State-Space Models (SSMs) (Gu et al., 2021), Mamba (Gu and Dao, 2023), and RWKV (Peng et al., 2023).",
                    "score": 0.569274676786655,
                    "section_title": "Architecture",
                    "char_start_offset": 34135,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 202
                        },
                        {
                            "start": 203,
                            "end": 391
                        },
                        {
                            "start": 394,
                            "end": 411
                        },
                        {
                            "start": 412,
                            "end": 603
                        },
                        {
                            "start": 604,
                            "end": 665
                        },
                        {
                            "start": 666,
                            "end": 776
                        },
                        {
                            "start": 779,
                            "end": 800
                        },
                        {
                            "start": 801,
                            "end": 871
                        },
                        {
                            "start": 872,
                            "end": 994
                        },
                        {
                            "start": 995,
                            "end": 1061
                        },
                        {
                            "start": 1064,
                            "end": 1276
                        },
                        {
                            "start": 1277,
                            "end": 1478
                        },
                        {
                            "start": 1479,
                            "end": 1603
                        },
                        {
                            "start": 1606,
                            "end": 1895
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 15,
                            "end": 36,
                            "matchedPaperCorpusId": "204838007"
                        },
                        {
                            "start": 731,
                            "end": 756,
                            "matchedPaperCorpusId": "160025533"
                        },
                        {
                            "start": 756,
                            "end": 775,
                            "matchedPaperCorpusId": "218971783"
                        },
                        {
                            "start": 1228,
                            "end": 1248,
                            "matchedPaperCorpusId": "231573431"
                        },
                        {
                            "start": 1258,
                            "end": 1275,
                            "matchedPaperCorpusId": "245124124"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.658203125
                }
            ],
            "relevance_judgement": 0.658203125,
            "relevance_judgment_input_expanded": "# Title: Exploring the Roles of Large Language Models in Reshaping Transportation Systems: A Survey, Framework, and Roadmap\n# Venue: arXiv.org\n# Authors: Tong Nie, Jiangming Sun, Wei Ma\n## Abstract\nModern transportation systems face pressing challenges due to increasing demand, dynamic environments, and heterogeneous information integration. The rapid evolution of Large Language Models (LLMs) offers transformative potential to address these challenges. Extensive knowledge and high-level capabilities derived from pretraining evolve the default role of LLMs as text generators to become versatile, knowledge-driven task solvers for intelligent transportation systems. This survey first presents LLM4TR, a novel conceptual framework that systematically categorizes the roles of LLMs in transportation into four synergetic dimensions: information processors, knowledge encoders, component generators, and decision facilitators. Through a unified taxonomy, we systematically elucidate how LLMs bridge fragmented data pipelines, enhance predictive analytics, simulate human-like reasoning, and enable closed-loop interactions across sensing, learning, modeling, and managing tasks in transportation systems. For each role, our review spans diverse applications, from traffic prediction and autonomous driving to safety analytics and urban mobility optimization, highlighting how emergent capabilities of LLMs such as in-context learning and step-by-step reasoning can enhance the operation and management of transportation systems. We further curate practical guidance, including available resources and computational guidelines, to support real-world deployment. By identifying challenges in existing LLM-based solutions, this survey charts a roadmap for advancing LLM-driven transportation research, positioning LLMs as central actors in the next generation of cyber-physical-social mobility ecosystems. Online resources can be found in the project page: https://github.com/tongnie/awesome-llm4tr.\n## Architecture\nModels like T5 (Raffel et al., 2020) and BART (Lewis et al., 2019) utilize both encoding and decoding mechanisms, enabling them to perform a wide range of tasks, including translation and summarization. The encoder applies stacked self-attention layers to encode the input sequence, and the decoder performs cross-attention on these representations and autoregressively generates the output. \n\n\u2022 Causal decoder. As a representative decoder-only architecture, causal decoder models introduce the unidirectional attention mask to ensure that each input token can only attend to the past tokens and itself. This mechanism makes them suitable for text generation tasks. Prominent examples are the GPTseries models (Radford et al., 2018(Radford et al., , 2019;;Brown et al., 2020). \n\n\u2022 Non-causal decoder. Another kind of decoder-only architecture is the non-casual structure. This architecture performs bidirectional attention on prefix tokens and unidirectional attention only on generated tokens. One representative prefix decoder LLMs is GLM (Zeng et al., 2022). \n\nTo scale the capacity of LLMs efficiently, the Mixture of Experts (MoE) technique can be exploited to combine the above architectures, such as in Swich Transformer (Fedus et al., 2022) and GLaM (Du et al., 2022). MoE involves sparsely activating a subset of model parameters (the \"experts\") for each input, allowing the model to handle a vast number of parameters without incurring prohibitive computational costs. This is achieved by employing a trainable gating mechanism to route each input token to the most relevant subset of experts. \n\nApart from the mainstream Transformer architecture, there are also emerging architectures proposed to alleviate the inherent issues of Transformers (e.g., the quadratic complexity) such as State-Space Models (SSMs) (Gu et al., 2021), Mamba (Gu and Dao, 2023), and RWKV (Peng et al., 2023).",
            "reference_string": "[277349741 | Nie et al. | 2025 | Citations: 4]"
        },
        {
            "title": "DLIP: Distilling Language-Image Pre-training",
            "venue": "arXiv.org",
            "year": 2023,
            "reference_count": 58,
            "citation_count": 4,
            "influential_citation_count": 0,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://arxiv.org/pdf/2308.12956",
                "status": "CLOSED",
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2308.12956, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "1380215283",
                    "name": "Huafeng Kuang"
                },
                {
                    "authorId": "2118432533",
                    "name": "Jie Wu"
                },
                {
                    "authorId": "51056401",
                    "name": "Xiawu Zheng"
                },
                {
                    "authorId": "2150654378",
                    "name": "Ming Li"
                },
                {
                    "authorId": "2118724465",
                    "name": "Xuefeng Xiao"
                },
                {
                    "authorId": "39077217",
                    "name": "Rui Wang"
                },
                {
                    "authorId": "2026322565",
                    "name": "Minghang Zheng"
                },
                {
                    "authorId": "1572139630",
                    "name": "Rongrong Ji"
                }
            ],
            "abstract": "Vision-Language Pre-training (VLP) shows remarkable progress with the assistance of extremely heavy parameters, which challenges deployment in real applications. Knowledge distillation is well recognized as the essential procedure in model compression. However, existing knowledge distillation techniques lack an in-depth investigation and analysis of VLP, and practical guidelines for VLP-oriented distillation are still not yet explored. In this paper, we present DLIP, a simple yet efficient Distilling Language-Image Pre-training framework, through which we investigate how to distill a light VLP model. Specifically, we dissect the model distillation from multiple dimensions, such as the architecture characteristics of different modules and the information transfer of different modalities. We conduct comprehensive experiments and provide insights on distilling a light but performant VLP model. Experimental results reveal that DLIP can achieve a state-of-the-art accuracy/efficiency trade-off across diverse cross-modal tasks, e.g., image-text retrieval, image captioning and visual question answering. For example, DLIP compresses BLIP by 1.9x, from 213M to 108M parameters, while achieving comparable or better performance. Furthermore, DLIP succeeds in retaining more than 95% of the performance with 22.4% parameters and 24.8% FLOPs compared to the teacher model and accelerates inference speed by 2.7x.",
            "corpus_id": 261101164,
            "sentences": [
                {
                    "corpus_id": "261101164",
                    "title": "DLIP: Distilling Language-Image Pre-training",
                    "text": "Task Decoder. The decoder module is optional in the VLP model. Many VLP models adopt the encoder-only architecture, where the cross-modal representations are directly fed into an output layer to generate the final outputs. For transformer encoder-decoder architecture, the cross-modal representations are fed into a decoder and then to an output layer. More importantly, the encoder-decoder architecture is more flexible, as it can perform tasks such as image captioning, which may not be that straightforward for an encoder-only model to be applied to. Therefore, we take the decoder into account in our DLIP framework. The structure of the decoder is similar to the text encoder, and the difference is that it replaces the bidirectional self-attention layers with causal self-attention layers. \n\nThere are many different model designs under the DLIP framework, in order to train a light and efficient model to better adapt to different downstream tasks, we use the multimodal mixture of encoder-decoder (MED) [25] as our basic architecture, which can flexibly operate as a unimodal encoder or a multimodal encoder based on cross-attention, or a multimodal decoder.",
                    "score": 0.44704181785821356,
                    "section_title": "Model",
                    "char_start_offset": 9611,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 13
                        },
                        {
                            "start": 14,
                            "end": 62
                        },
                        {
                            "start": 63,
                            "end": 222
                        },
                        {
                            "start": 223,
                            "end": 352
                        },
                        {
                            "start": 353,
                            "end": 553
                        },
                        {
                            "start": 554,
                            "end": 620
                        },
                        {
                            "start": 621,
                            "end": 795
                        },
                        {
                            "start": 798,
                            "end": 1166
                        }
                    ],
                    "ref_mentions": [],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.650390625
                }
            ],
            "relevance_judgement": 0.650390625,
            "relevance_judgment_input_expanded": "# Title: DLIP: Distilling Language-Image Pre-training\n# Venue: arXiv.org\n# Authors: Huafeng Kuang, Jie Wu, Xiawu Zheng, Ming Li, Xuefeng Xiao, Rui Wang, Minghang Zheng, Rongrong Ji\n## Abstract\nVision-Language Pre-training (VLP) shows remarkable progress with the assistance of extremely heavy parameters, which challenges deployment in real applications. Knowledge distillation is well recognized as the essential procedure in model compression. However, existing knowledge distillation techniques lack an in-depth investigation and analysis of VLP, and practical guidelines for VLP-oriented distillation are still not yet explored. In this paper, we present DLIP, a simple yet efficient Distilling Language-Image Pre-training framework, through which we investigate how to distill a light VLP model. Specifically, we dissect the model distillation from multiple dimensions, such as the architecture characteristics of different modules and the information transfer of different modalities. We conduct comprehensive experiments and provide insights on distilling a light but performant VLP model. Experimental results reveal that DLIP can achieve a state-of-the-art accuracy/efficiency trade-off across diverse cross-modal tasks, e.g., image-text retrieval, image captioning and visual question answering. For example, DLIP compresses BLIP by 1.9x, from 213M to 108M parameters, while achieving comparable or better performance. Furthermore, DLIP succeeds in retaining more than 95% of the performance with 22.4% parameters and 24.8% FLOPs compared to the teacher model and accelerates inference speed by 2.7x.\n## Model\nTask Decoder. The decoder module is optional in the VLP model. Many VLP models adopt the encoder-only architecture, where the cross-modal representations are directly fed into an output layer to generate the final outputs. For transformer encoder-decoder architecture, the cross-modal representations are fed into a decoder and then to an output layer. More importantly, the encoder-decoder architecture is more flexible, as it can perform tasks such as image captioning, which may not be that straightforward for an encoder-only model to be applied to. Therefore, we take the decoder into account in our DLIP framework. The structure of the decoder is similar to the text encoder, and the difference is that it replaces the bidirectional self-attention layers with causal self-attention layers. \n\nThere are many different model designs under the DLIP framework, in order to train a light and efficient model to better adapt to different downstream tasks, we use the multimodal mixture of encoder-decoder (MED) [25] as our basic architecture, which can flexibly operate as a unimodal encoder or a multimodal encoder based on cross-attention, or a multimodal decoder.",
            "reference_string": "[261101164 | Kuang et al. | 2023 | Citations: 4]"
        },
        {
            "title": "Leveraging Biomolecule and Natural Language through Multi-Modal Learning: A Survey",
            "venue": "arXiv.org",
            "year": 2024,
            "reference_count": 179,
            "citation_count": 18,
            "influential_citation_count": 1,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2403.01528, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2171652249",
                    "name": "Qizhi Pei"
                },
                {
                    "authorId": "47767791",
                    "name": "Lijun Wu"
                },
                {
                    "authorId": "1944690382",
                    "name": "Kaiyuan Gao"
                },
                {
                    "authorId": "151068900",
                    "name": "Jinhua Zhu"
                },
                {
                    "authorId": "2290062348",
                    "name": "Yue Wang"
                },
                {
                    "authorId": "2290024261",
                    "name": "Zun Wang"
                },
                {
                    "authorId": "2267250090",
                    "name": "Tao Qin"
                },
                {
                    "authorId": "2257028545",
                    "name": "Rui Yan"
                }
            ],
            "abstract": "The integration of biomolecular modeling with natural language (BL) has emerged as a promising interdisciplinary area at the intersection of artificial intelligence, chemistry and biology. This approach leverages the rich, multifaceted descriptions of biomolecules contained within textual data sources to enhance our fundamental understanding and enable downstream computational tasks such as biomolecule property prediction. The fusion of the nuanced narratives expressed through natural language with the structural and functional specifics of biomolecules described via various molecular modeling techniques opens new avenues for comprehensively representing and analyzing biomolecules. By incorporating the contextual language data that surrounds biomolecules into their modeling, BL aims to capture a holistic view encompassing both the symbolic qualities conveyed through language as well as quantitative structural characteristics. In this review, we provide an extensive analysis of recent advancements achieved through cross modeling of biomolecules and natural language. (1) We begin by outlining the technical representations of biomolecules employed, including sequences, 2D graphs, and 3D structures. (2) We then examine in depth the rationale and key objectives underlying effective multi-modal integration of language and molecular data sources. (3) We subsequently survey the practical applications enabled to date in this developing research area. (4) We also compile and summarize the available resources and datasets to facilitate future work. (5) Looking ahead, we identify several promising research directions worthy of further exploration and investment to continue advancing the field. The related resources and contents are updating in \\url{https://github.com/QizhiPei/Awesome-Biomolecule-Language-Cross-Modeling}.",
            "corpus_id": 268247581,
            "sentences": [
                {
                    "corpus_id": "268247581",
                    "title": "Leveraging Biomolecule and Natural Language through Multi-Modal Learning: A Survey",
                    "text": "The Transformer model can be specialized into encoder-only (Figure 6a) and decoder-only (Figure 6c) designs to suit different purposes. Encoder-only models [25] specialize in processing input sequences of biomolecules and text through bi-directional self-attention, making them highly effective for tasks that require an in-depth understanding of the input, such as sentiment analysis and feature extraction in NLP. Thereby in biomolecule domain, encoder-only models establish a bi-directional association between biotokens and text tokens for predictive tasks [24]. Encoder-only models are typically designed for representation learning objective obtain strong representations for text and biomolecule. In contrast, decoder-only models [18], [138] employ causal attention to focus on the sequence of previous tokens. This architecture is typically utilized in generative tasks, such as generating text descriptions that match the given molecule or for the reverse task [54]. Thanks to the autoregressive generation property, decoder-only models are well suitable for instruction following and assistant/agent objectives.",
                    "score": 0.4717201581820769,
                    "section_title": "Encoder/Decoder-only",
                    "char_start_offset": 32051,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 135
                        },
                        {
                            "start": 136,
                            "end": 415
                        },
                        {
                            "start": 416,
                            "end": 566
                        },
                        {
                            "start": 567,
                            "end": 703
                        },
                        {
                            "start": 704,
                            "end": 817
                        },
                        {
                            "start": 818,
                            "end": 975
                        },
                        {
                            "start": 976,
                            "end": 1121
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 156,
                            "end": 160,
                            "matchedPaperCorpusId": "52967399"
                        },
                        {
                            "start": 561,
                            "end": 565,
                            "matchedPaperCorpusId": "246815222"
                        },
                        {
                            "start": 970,
                            "end": 974,
                            "matchedPaperCorpusId": "258762343"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.64990234375
                }
            ],
            "relevance_judgement": 0.64990234375,
            "relevance_judgment_input_expanded": "# Title: Leveraging Biomolecule and Natural Language through Multi-Modal Learning: A Survey\n# Venue: arXiv.org\n# Authors: Qizhi Pei, Lijun Wu, Kaiyuan Gao, Jinhua Zhu, Yue Wang, Zun Wang, Tao Qin, Rui Yan\n## Abstract\nThe integration of biomolecular modeling with natural language (BL) has emerged as a promising interdisciplinary area at the intersection of artificial intelligence, chemistry and biology. This approach leverages the rich, multifaceted descriptions of biomolecules contained within textual data sources to enhance our fundamental understanding and enable downstream computational tasks such as biomolecule property prediction. The fusion of the nuanced narratives expressed through natural language with the structural and functional specifics of biomolecules described via various molecular modeling techniques opens new avenues for comprehensively representing and analyzing biomolecules. By incorporating the contextual language data that surrounds biomolecules into their modeling, BL aims to capture a holistic view encompassing both the symbolic qualities conveyed through language as well as quantitative structural characteristics. In this review, we provide an extensive analysis of recent advancements achieved through cross modeling of biomolecules and natural language. (1) We begin by outlining the technical representations of biomolecules employed, including sequences, 2D graphs, and 3D structures. (2) We then examine in depth the rationale and key objectives underlying effective multi-modal integration of language and molecular data sources. (3) We subsequently survey the practical applications enabled to date in this developing research area. (4) We also compile and summarize the available resources and datasets to facilitate future work. (5) Looking ahead, we identify several promising research directions worthy of further exploration and investment to continue advancing the field. The related resources and contents are updating in \\url{https://github.com/QizhiPei/Awesome-Biomolecule-Language-Cross-Modeling}.\n## Encoder/Decoder-only\nThe Transformer model can be specialized into encoder-only (Figure 6a) and decoder-only (Figure 6c) designs to suit different purposes. Encoder-only models [25] specialize in processing input sequences of biomolecules and text through bi-directional self-attention, making them highly effective for tasks that require an in-depth understanding of the input, such as sentiment analysis and feature extraction in NLP. Thereby in biomolecule domain, encoder-only models establish a bi-directional association between biotokens and text tokens for predictive tasks [24]. Encoder-only models are typically designed for representation learning objective obtain strong representations for text and biomolecule. In contrast, decoder-only models [18], [138] employ causal attention to focus on the sequence of previous tokens. This architecture is typically utilized in generative tasks, such as generating text descriptions that match the given molecule or for the reverse task [54]. Thanks to the autoregressive generation property, decoder-only models are well suitable for instruction following and assistant/agent objectives.",
            "reference_string": "[268247581 | Pei et al. | 2024 | Citations: 18]"
        },
        {
            "title": "Towards an Understanding of Large Language Models in Software Engineering Tasks",
            "venue": "Empirical Software Engineering",
            "year": 2023,
            "reference_count": 226,
            "citation_count": 76,
            "influential_citation_count": 2,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://arxiv.org/pdf/2308.11396",
                "status": "CLOSED",
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2308.11396, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2148256392",
                    "name": "Zibin Zheng"
                },
                {
                    "authorId": "2115304",
                    "name": "Kai-Chun Ning"
                },
                {
                    "authorId": "2254800142",
                    "name": "Jiachi Chen"
                },
                {
                    "authorId": "2214155529",
                    "name": "Yanlin Wang"
                },
                {
                    "authorId": "2274095496",
                    "name": "Wenqing Chen"
                },
                {
                    "authorId": "2217902484",
                    "name": "Lianghong Guo"
                },
                {
                    "authorId": "2233023641",
                    "name": "Weicheng Wang"
                }
            ],
            "abstract": "Large Language Models (LLMs) have drawn widespread attention and research due to their astounding performance in text generation and reasoning tasks. Derivative products, like ChatGPT, have been extensively deployed and highly sought after. Meanwhile, the evaluation and optimization of LLMs in software engineering tasks, such as code generation, have become a research focus. However, there is still a lack of systematic research on applying and evaluating LLMs in software engineering. Therefore, this paper comprehensively investigate and collate the research and products combining LLMs with software engineering, aiming to answer two questions: (1) What are the current integrations of LLMs with software engineering? (2) Can LLMs effectively handle software engineering tasks? To find the answers, we have collected related literature as extensively as possible from seven mainstream databases and selected 123 timely papers published starting from 2022 for analysis. We have categorized these papers in detail and reviewed the current research status of LLMs from the perspective of seven major software engineering tasks, hoping this will help researchers better grasp the research trends and address the issues when applying LLMs. Meanwhile, we have also organized and presented papers with evaluation content to reveal the performance and effectiveness of LLMs in various software engineering tasks, guiding researchers and developers to optimize.",
            "corpus_id": 261064777,
            "sentences": [
                {
                    "corpus_id": "261064777",
                    "title": "Towards an Understanding of Large Language Models in Software Engineering Tasks",
                    "text": "The Transformer architecture, proposed by Vaswani et al. in 2017(Vaswani et al., 2017), has emerged as the leading choice for developing large language models (LLMs) due to its exceptional parallelizability and capacity (Zhao et al., 2023b). This scalability allows language models to be expanded to include hundreds or even thousands of billions of parameters, enabling them to capture more complex language patterns and improve performance on various tasks. In general, large language models can be categorized into three main architecture types: encoder-decoder structures, causal-decoder, and prefix decoder (Zhao et al., 2023b), each with its own characteristics and applications. \n\nEncoder-decoder Architecture: The vanilla Transformer proposed in (Vaswani et al., 2017) is based on encoder-decoder architecture, which comprises separate encoder and decoder components. The encoder processes the input sequence and captures its latent representation, which is then used by the decoder to generate the output sequence autoregressively. This architecture is well-suited for tasks involving sequence-to-sequence mapping, such as machine translation, text summarization, and dialogue generation. Encoder-decoder pretrained model. Encoder-decoder pretrained models, such as BART (Lewis et al., 2019) and T5 (Raffel et al., 2020), have demonstrated excellent performance across various downstream tasks. However, with the development of LLM there are only a few large language models based on the encoder-decoder architecture, such as Flan-T5 (Chung et al., 2022) and CodeT5+ (Wang et al., 2023f). \n\nCausal Decoder Architecture: The causal decoder architecture is commonly implemented as a stack of decoder layers. It utilizes a diagonal mask matrix, allowing each token to only have access to information from preceding tokens. This constraint ensures a unidirectional and autoregressive generation process.",
                    "score": 0.5076780060333468,
                    "section_title": "Model Architecture",
                    "char_start_offset": 10774,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 241
                        },
                        {
                            "start": 242,
                            "end": 459
                        },
                        {
                            "start": 460,
                            "end": 685
                        },
                        {
                            "start": 688,
                            "end": 875
                        },
                        {
                            "start": 876,
                            "end": 1040
                        },
                        {
                            "start": 1041,
                            "end": 1197
                        },
                        {
                            "start": 1198,
                            "end": 1231
                        },
                        {
                            "start": 1232,
                            "end": 1403
                        },
                        {
                            "start": 1404,
                            "end": 1597
                        },
                        {
                            "start": 1600,
                            "end": 1714
                        },
                        {
                            "start": 1715,
                            "end": 1828
                        },
                        {
                            "start": 1829,
                            "end": 1908
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 1308,
                            "end": 1329,
                            "matchedPaperCorpusId": "204838007"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.64697265625
                }
            ],
            "relevance_judgement": 0.64697265625,
            "relevance_judgment_input_expanded": "# Title: Towards an Understanding of Large Language Models in Software Engineering Tasks\n# Venue: Empirical Software Engineering\n# Authors: Zibin Zheng, Kai-Chun Ning, Jiachi Chen, Yanlin Wang, Wenqing Chen, Lianghong Guo, Weicheng Wang\n## Abstract\nLarge Language Models (LLMs) have drawn widespread attention and research due to their astounding performance in text generation and reasoning tasks. Derivative products, like ChatGPT, have been extensively deployed and highly sought after. Meanwhile, the evaluation and optimization of LLMs in software engineering tasks, such as code generation, have become a research focus. However, there is still a lack of systematic research on applying and evaluating LLMs in software engineering. Therefore, this paper comprehensively investigate and collate the research and products combining LLMs with software engineering, aiming to answer two questions: (1) What are the current integrations of LLMs with software engineering? (2) Can LLMs effectively handle software engineering tasks? To find the answers, we have collected related literature as extensively as possible from seven mainstream databases and selected 123 timely papers published starting from 2022 for analysis. We have categorized these papers in detail and reviewed the current research status of LLMs from the perspective of seven major software engineering tasks, hoping this will help researchers better grasp the research trends and address the issues when applying LLMs. Meanwhile, we have also organized and presented papers with evaluation content to reveal the performance and effectiveness of LLMs in various software engineering tasks, guiding researchers and developers to optimize.\n## Model Architecture\nThe Transformer architecture, proposed by Vaswani et al. in 2017(Vaswani et al., 2017), has emerged as the leading choice for developing large language models (LLMs) due to its exceptional parallelizability and capacity (Zhao et al., 2023b). This scalability allows language models to be expanded to include hundreds or even thousands of billions of parameters, enabling them to capture more complex language patterns and improve performance on various tasks. In general, large language models can be categorized into three main architecture types: encoder-decoder structures, causal-decoder, and prefix decoder (Zhao et al., 2023b), each with its own characteristics and applications. \n\nEncoder-decoder Architecture: The vanilla Transformer proposed in (Vaswani et al., 2017) is based on encoder-decoder architecture, which comprises separate encoder and decoder components. The encoder processes the input sequence and captures its latent representation, which is then used by the decoder to generate the output sequence autoregressively. This architecture is well-suited for tasks involving sequence-to-sequence mapping, such as machine translation, text summarization, and dialogue generation. Encoder-decoder pretrained model. Encoder-decoder pretrained models, such as BART (Lewis et al., 2019) and T5 (Raffel et al., 2020), have demonstrated excellent performance across various downstream tasks. However, with the development of LLM there are only a few large language models based on the encoder-decoder architecture, such as Flan-T5 (Chung et al., 2022) and CodeT5+ (Wang et al., 2023f). \n\nCausal Decoder Architecture: The causal decoder architecture is commonly implemented as a stack of decoder layers. It utilizes a diagonal mask matrix, allowing each token to only have access to information from preceding tokens. This constraint ensures a unidirectional and autoregressive generation process.",
            "reference_string": "[261064777 | Zheng et al. | 2023 | Citations: 76]"
        },
        {
            "title": "TURNA: A Turkish Encoder-Decoder Language Model for Enhanced Understanding and Generation",
            "venue": "Annual Meeting of the Association for Computational Linguistics",
            "year": 2024,
            "reference_count": 66,
            "citation_count": 12,
            "influential_citation_count": 1,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2401.14373, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2281033147",
                    "name": "Gokcce Uludougan"
                },
                {
                    "authorId": "2281033141",
                    "name": "Zeynep Yirmibecsouglu Balal"
                },
                {
                    "authorId": "2174736343",
                    "name": "Furkan Akkurt"
                },
                {
                    "authorId": "2281033264",
                    "name": "Melikcsah Turker"
                },
                {
                    "authorId": "9179697",
                    "name": "Onur Gungor"
                },
                {
                    "authorId": "66493576",
                    "name": "S. Uskudarli"
                }
            ],
            "abstract": "The recent advances in natural language processing have predominantly favored well-resourced English-centric models, resulting in a significant gap with low-resource languages. In this work, we introduce the language model TURNA, which is developed for the low-resource language Turkish and is capable of both natural language understanding and generation tasks. TURNA is pretrained with an encoder-decoder architecture based on the unified framework UL2 with a diverse corpus that we specifically curated for this purpose. We evaluated TURNA with three generation tasks and five understanding tasks for Turkish. The results show that TURNA outperforms several multilingual models in both understanding and generation tasks, and competes with monolingual Turkish models in understanding tasks. TURNA is made available at https://huggingface.co/boun-tabi-LMG/TURNA .",
            "corpus_id": 267211690,
            "sentences": [
                {
                    "corpus_id": "267211690",
                    "title": "TURNA: A Turkish Encoder-Decoder Language Model for Enhanced Understanding and Generation",
                    "text": "Recently, pretrained language models based on transformers have been dominant in the NLP field, exhibiting variations in both components and objectives. Models that exclusively employ encoders, typically trained with denoising objectives, are geared toward understanding tasks, as exemplified in works such as Devlin et al. (2019) and Clark et al. (2020). Conversely, models that exclusively use decoders are designed for generation tasks, employing causal language modeling, as demonstrated in various studies (Radford et al., 2019;Brown et al., 2020;Touvron et al., 2023). The Text-to-Text Transformer (T5) (Raffel et al., 2020), on the other hand, employs an encoder-decoder architecture and undergoes pretraining with a denoising objective referred to as span corruption. UniLM (Dong et al., 2019) is also an encoder-decoder model, but pre-trained using unidirectional, bidirectional, and sequence-to-sequence language modeling. This can be seen as a combination of causal and denoising objectives. Recently, Tay et al., 2023 proposed that various pretraining objectives can be recast as each other. They introduced the UL2 framework based on a pretraining objective called Mixture-of-Denoisers (MoD), which combines different pretraining paradigms. They compared decoder-only and encoder-decoder models trained with the MoD objective and found that encoder-decoder models often perform better. Notably, by using the MoD objective and moderately scaling up the model, they achieved state-of-the-art performance on a diverse set of NLP tasks including understanding and generation tasks.",
                    "score": 0.5050583677006911,
                    "section_title": "Pretraining objectives",
                    "char_start_offset": 5796,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 152
                        },
                        {
                            "start": 153,
                            "end": 355
                        },
                        {
                            "start": 356,
                            "end": 574
                        },
                        {
                            "start": 575,
                            "end": 775
                        },
                        {
                            "start": 776,
                            "end": 932
                        },
                        {
                            "start": 933,
                            "end": 1002
                        },
                        {
                            "start": 1003,
                            "end": 1103
                        },
                        {
                            "start": 1104,
                            "end": 1253
                        },
                        {
                            "start": 1254,
                            "end": 1398
                        },
                        {
                            "start": 1399,
                            "end": 1590
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 310,
                            "end": 330,
                            "matchedPaperCorpusId": "52967399"
                        },
                        {
                            "start": 335,
                            "end": 354,
                            "matchedPaperCorpusId": "208229926"
                        },
                        {
                            "start": 511,
                            "end": 533,
                            "matchedPaperCorpusId": "160025533"
                        },
                        {
                            "start": 609,
                            "end": 630,
                            "matchedPaperCorpusId": "204838007"
                        },
                        {
                            "start": 782,
                            "end": 801,
                            "matchedPaperCorpusId": "147704286"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.64208984375
                }
            ],
            "relevance_judgement": 0.64208984375,
            "relevance_judgment_input_expanded": "# Title: TURNA: A Turkish Encoder-Decoder Language Model for Enhanced Understanding and Generation\n# Venue: Annual Meeting of the Association for Computational Linguistics\n# Authors: Gokcce Uludougan, Zeynep Yirmibecsouglu Balal, Furkan Akkurt, Melikcsah Turker, Onur Gungor, S. Uskudarli\n## Abstract\nThe recent advances in natural language processing have predominantly favored well-resourced English-centric models, resulting in a significant gap with low-resource languages. In this work, we introduce the language model TURNA, which is developed for the low-resource language Turkish and is capable of both natural language understanding and generation tasks. TURNA is pretrained with an encoder-decoder architecture based on the unified framework UL2 with a diverse corpus that we specifically curated for this purpose. We evaluated TURNA with three generation tasks and five understanding tasks for Turkish. The results show that TURNA outperforms several multilingual models in both understanding and generation tasks, and competes with monolingual Turkish models in understanding tasks. TURNA is made available at https://huggingface.co/boun-tabi-LMG/TURNA .\n## Pretraining objectives\nRecently, pretrained language models based on transformers have been dominant in the NLP field, exhibiting variations in both components and objectives. Models that exclusively employ encoders, typically trained with denoising objectives, are geared toward understanding tasks, as exemplified in works such as Devlin et al. (2019) and Clark et al. (2020). Conversely, models that exclusively use decoders are designed for generation tasks, employing causal language modeling, as demonstrated in various studies (Radford et al., 2019;Brown et al., 2020;Touvron et al., 2023). The Text-to-Text Transformer (T5) (Raffel et al., 2020), on the other hand, employs an encoder-decoder architecture and undergoes pretraining with a denoising objective referred to as span corruption. UniLM (Dong et al., 2019) is also an encoder-decoder model, but pre-trained using unidirectional, bidirectional, and sequence-to-sequence language modeling. This can be seen as a combination of causal and denoising objectives. Recently, Tay et al., 2023 proposed that various pretraining objectives can be recast as each other. They introduced the UL2 framework based on a pretraining objective called Mixture-of-Denoisers (MoD), which combines different pretraining paradigms. They compared decoder-only and encoder-decoder models trained with the MoD objective and found that encoder-decoder models often perform better. Notably, by using the MoD objective and moderately scaling up the model, they achieved state-of-the-art performance on a diverse set of NLP tasks including understanding and generation tasks.",
            "reference_string": "[267211690 | Uludougan et al. | 2024 | Citations: 12]"
        },
        {
            "title": "Parallel Sentence-Level Explanation Generation for Real-World Low-Resource Scenarios",
            "venue": "IEEE International Conference on Acoustics, Speech, and Signal Processing",
            "year": 2023,
            "reference_count": 26,
            "citation_count": 4,
            "influential_citation_count": 1,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://arxiv.org/pdf/2302.10707",
                "status": "GREEN",
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2302.10707, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "1679704",
                    "name": "Y. Liu"
                },
                {
                    "authorId": "2292210488",
                    "name": "Xiaokang Chen"
                },
                {
                    "authorId": "2072900666",
                    "name": "Qianwen Dai"
                }
            ],
            "abstract": "In order to reveal the rationale behind model predictions, many works have exploited providing explanations in various forms. Recently, to further guarantee readability, more and more works turn to generate sentence-level human language explanations. However, current works pursuing sentence- level explanations rely heavily on annotated training data, which limits the development of interpretability to only a few tasks. As far as we know, this paper is the first to explore this problem smoothly from weak-supervised learning to unsupervised learning. Besides, we also notice the high latency of autoregressive sentence-level explanation generation, which leads to asynchronous interpretability after prediction. Therefore, we propose a non-autoregressive interpretable model to facilitate parallel explanation generation and simultaneous prediction. Through extensive experiments on Natural Language Inference task and Spouse Prediction task, we find that users are able to train classifiers with comparable performance 10 \u2212 15\u00d7 faster with parallel explanation generation using only a few or no annotated training data.",
            "corpus_id": 257050658,
            "sentences": [
                {
                    "corpus_id": "257050658",
                    "title": "Parallel Sentence-Level Explanation Generation for Real-World Low-Resource Scenarios",
                    "text": "We adopt the Transformer [22] as the backbone. To enable non-autoregressive interpretation, following [20], the decoder is modified in three aspects: input sequence, self-attention mask, and positional encoding. For input sequence modification, because previously generated tokens are unavailable under the non-autoregressive setting, we use a fertility predictor first to predict the length of the target explanation and produce decoder input with the tokens copied from the encoder input. \n\nFor the modification of the self-attention mask, because the decoder input is the copied sequence of encoder input, the self-attention module is allowed to attend all positions, rather than only left positions in the conventional Transformer decoder. Therefore, the self-attention mask is replaced with a non-causal mask in our non-autoregressive decoder. For positional encoding modification, different from the self-attention module, the positional attention module uses positional encoding as the query and key, and the hidden representations from the previous layer as the value.",
                    "score": 0.478695108027087,
                    "section_title": "Encoder and Decoder",
                    "char_start_offset": 3841,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 46
                        },
                        {
                            "start": 47,
                            "end": 211
                        },
                        {
                            "start": 212,
                            "end": 490
                        },
                        {
                            "start": 493,
                            "end": 743
                        },
                        {
                            "start": 744,
                            "end": 848
                        },
                        {
                            "start": 849,
                            "end": 1076
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 25,
                            "end": 29,
                            "matchedPaperCorpusId": "13756489"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.638671875
                }
            ],
            "relevance_judgement": 0.638671875,
            "relevance_judgment_input_expanded": "# Title: Parallel Sentence-Level Explanation Generation for Real-World Low-Resource Scenarios\n# Venue: IEEE International Conference on Acoustics, Speech, and Signal Processing\n# Authors: Y. Liu, Xiaokang Chen, Qianwen Dai\n## Abstract\nIn order to reveal the rationale behind model predictions, many works have exploited providing explanations in various forms. Recently, to further guarantee readability, more and more works turn to generate sentence-level human language explanations. However, current works pursuing sentence- level explanations rely heavily on annotated training data, which limits the development of interpretability to only a few tasks. As far as we know, this paper is the first to explore this problem smoothly from weak-supervised learning to unsupervised learning. Besides, we also notice the high latency of autoregressive sentence-level explanation generation, which leads to asynchronous interpretability after prediction. Therefore, we propose a non-autoregressive interpretable model to facilitate parallel explanation generation and simultaneous prediction. Through extensive experiments on Natural Language Inference task and Spouse Prediction task, we find that users are able to train classifiers with comparable performance 10 \u2212 15\u00d7 faster with parallel explanation generation using only a few or no annotated training data.\n## Encoder and Decoder\nWe adopt the Transformer [22] as the backbone. To enable non-autoregressive interpretation, following [20], the decoder is modified in three aspects: input sequence, self-attention mask, and positional encoding. For input sequence modification, because previously generated tokens are unavailable under the non-autoregressive setting, we use a fertility predictor first to predict the length of the target explanation and produce decoder input with the tokens copied from the encoder input. \n\nFor the modification of the self-attention mask, because the decoder input is the copied sequence of encoder input, the self-attention module is allowed to attend all positions, rather than only left positions in the conventional Transformer decoder. Therefore, the self-attention mask is replaced with a non-causal mask in our non-autoregressive decoder. For positional encoding modification, different from the self-attention module, the positional attention module uses positional encoding as the query and key, and the hidden representations from the previous layer as the value.",
            "reference_string": "[257050658 | Liu et al. | 2023 | Citations: 4]"
        },
        {
            "title": "Adapting Decoder-Based Language Models for Diverse Encoder Downstream Tasks",
            "venue": "arXiv.org",
            "year": 2025,
            "reference_count": 14,
            "citation_count": 1,
            "influential_citation_count": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2503.02656, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "1658871094",
                    "name": "P. Suganthan"
                },
                {
                    "authorId": "2165469946",
                    "name": "Fedor Moiseev"
                },
                {
                    "authorId": "2348489099",
                    "name": "Le Yan"
                },
                {
                    "authorId": "2261361394",
                    "name": "Junru Wu"
                },
                {
                    "authorId": "2348507846",
                    "name": "Jianmo Ni"
                },
                {
                    "authorId": "2348488953",
                    "name": "Jay Han"
                },
                {
                    "authorId": "1954563",
                    "name": "I. Zitouni"
                },
                {
                    "authorId": "1727837",
                    "name": "Enrique Alfonseca"
                },
                {
                    "authorId": "2348422460",
                    "name": "Xuanhui Wang"
                },
                {
                    "authorId": "2349772191",
                    "name": "Zhe Dong"
                }
            ],
            "abstract": "Decoder-based transformers, while revolutionizing language modeling and scaling to immense sizes, have not completely overtaken encoder-heavy architectures in natural language processing. Specifically, encoder-only models remain dominant in tasks like classification, regression, and ranking. This is primarily due to the inherent structure of decoder-based models, which limits their direct applicability to these tasks. In this paper, we introduce Gemma Encoder, adapting the powerful Gemma decoder model to an encoder architecture, thereby unlocking its potential for a wider range of non-generative applications. To optimize the adaptation from decoder to encoder, we systematically analyze various pooling strategies, attention mechanisms, and hyperparameters (e.g., dropout rate). Furthermore, we benchmark Gemma Encoder against established approaches on the GLUE benchmarks, and MS MARCO ranking benchmark, demonstrating its effectiveness and versatility.",
            "corpus_id": 276771845,
            "sentences": [
                {
                    "corpus_id": "276771845",
                    "title": "Adapting Decoder-Based Language Models for Diverse Encoder Downstream Tasks",
                    "text": "Decoder-based language models like Gemma (Gemma Team, 2024a,b) and Gemini (Gemini Team, 2023) have demonstrated remarkable language understanding capabilities. Yet, for many downstream tasks such as classification, regression, and ranking, encoderbased models, particularly those derived from BERT (Devlin et al., 2019) or T5's encoder (Raffel et al., 2020), remain the dominant choice. A key question thus arises: can we effectively adapt the powerful knowledge embedded in decoder-only models to excel in these encoder-centric tasks? \n\nThis work addresses this gap by introducing Gemma Encoder, a novel adaptation of the Gemma decoder model designed for encoder-only architectures. We leverage Gemma's pre-trained weights as a strong initialization point, and then strategically modify the architecture and training procedure to optimize performance on downstream tasks. Our approach centers on three key innovations: \n\nFirst, we augment the model with task-specific pooling and Multi-Layer Perceptron (MLP) layers, exploring various pooling strategies to determine the optimal architecture. Second, we address the critical impact of at-tention mechanisms. Gemma's causal attention, ideal for generative tasks, inherently limits its applicability to encoder-based tasks. We demonstrate that simply enabling bidirectional attention during fine-tuning dramatically improves performance. \n\nThird, we investigate the role of dropout. While often omitted during pre-training of modern decoder models, our empirical analysis reveals that incorporating dropout during fine-tuning significantly enhances Gemma Encoder's robustness and generalization ability. We also analyze different padding strategies to understand the effect on Encoder models. \n\nIn the rest of this paper, we describe the technical details of these modifications1 . To validate the effectiveness of our Gemma Encoder, we conduct our experiments on GLUE benchmarks (Wang et al., 2019(Wang et al., , 2020)), for classification and regression tasks, and the MSMARCO benchmark (Bajaj et al., 2018) for ranking tasks. Our results show that our Gemma Encoder models are able to outperform competitive baselines on these benchmark tasks.",
                    "score": 0.46748109429837814,
                    "section_title": "Introduction",
                    "char_start_offset": 15,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 159
                        },
                        {
                            "start": 160,
                            "end": 386
                        },
                        {
                            "start": 387,
                            "end": 535
                        },
                        {
                            "start": 538,
                            "end": 683
                        },
                        {
                            "start": 684,
                            "end": 872
                        },
                        {
                            "start": 873,
                            "end": 919
                        },
                        {
                            "start": 922,
                            "end": 1093
                        },
                        {
                            "start": 1094,
                            "end": 1158
                        },
                        {
                            "start": 1159,
                            "end": 1272
                        },
                        {
                            "start": 1273,
                            "end": 1386
                        },
                        {
                            "start": 1389,
                            "end": 1431
                        },
                        {
                            "start": 1432,
                            "end": 1652
                        },
                        {
                            "start": 1653,
                            "end": 1741
                        },
                        {
                            "start": 1744,
                            "end": 1830
                        },
                        {
                            "start": 1831,
                            "end": 2077
                        },
                        {
                            "start": 2078,
                            "end": 2195
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 298,
                            "end": 319,
                            "matchedPaperCorpusId": "52967399"
                        },
                        {
                            "start": 336,
                            "end": 357,
                            "matchedPaperCorpusId": "204838007"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.63134765625
                }
            ],
            "relevance_judgement": 0.63134765625,
            "relevance_judgment_input_expanded": "# Title: Adapting Decoder-Based Language Models for Diverse Encoder Downstream Tasks\n# Venue: arXiv.org\n# Authors: P. Suganthan, Fedor Moiseev, Le Yan, Junru Wu, Jianmo Ni, Jay Han, I. Zitouni, Enrique Alfonseca, Xuanhui Wang, Zhe Dong\n## Abstract\nDecoder-based transformers, while revolutionizing language modeling and scaling to immense sizes, have not completely overtaken encoder-heavy architectures in natural language processing. Specifically, encoder-only models remain dominant in tasks like classification, regression, and ranking. This is primarily due to the inherent structure of decoder-based models, which limits their direct applicability to these tasks. In this paper, we introduce Gemma Encoder, adapting the powerful Gemma decoder model to an encoder architecture, thereby unlocking its potential for a wider range of non-generative applications. To optimize the adaptation from decoder to encoder, we systematically analyze various pooling strategies, attention mechanisms, and hyperparameters (e.g., dropout rate). Furthermore, we benchmark Gemma Encoder against established approaches on the GLUE benchmarks, and MS MARCO ranking benchmark, demonstrating its effectiveness and versatility.\n## Introduction\nDecoder-based language models like Gemma (Gemma Team, 2024a,b) and Gemini (Gemini Team, 2023) have demonstrated remarkable language understanding capabilities. Yet, for many downstream tasks such as classification, regression, and ranking, encoderbased models, particularly those derived from BERT (Devlin et al., 2019) or T5's encoder (Raffel et al., 2020), remain the dominant choice. A key question thus arises: can we effectively adapt the powerful knowledge embedded in decoder-only models to excel in these encoder-centric tasks? \n\nThis work addresses this gap by introducing Gemma Encoder, a novel adaptation of the Gemma decoder model designed for encoder-only architectures. We leverage Gemma's pre-trained weights as a strong initialization point, and then strategically modify the architecture and training procedure to optimize performance on downstream tasks. Our approach centers on three key innovations: \n\nFirst, we augment the model with task-specific pooling and Multi-Layer Perceptron (MLP) layers, exploring various pooling strategies to determine the optimal architecture. Second, we address the critical impact of at-tention mechanisms. Gemma's causal attention, ideal for generative tasks, inherently limits its applicability to encoder-based tasks. We demonstrate that simply enabling bidirectional attention during fine-tuning dramatically improves performance. \n\nThird, we investigate the role of dropout. While often omitted during pre-training of modern decoder models, our empirical analysis reveals that incorporating dropout during fine-tuning significantly enhances Gemma Encoder's robustness and generalization ability. We also analyze different padding strategies to understand the effect on Encoder models. \n\nIn the rest of this paper, we describe the technical details of these modifications1 . To validate the effectiveness of our Gemma Encoder, we conduct our experiments on GLUE benchmarks (Wang et al., 2019(Wang et al., , 2020)), for classification and regression tasks, and the MSMARCO benchmark (Bajaj et al., 2018) for ranking tasks. Our results show that our Gemma Encoder models are able to outperform competitive baselines on these benchmark tasks.",
            "reference_string": "[276771845 | Suganthan et al. | 2025 | Citations: 1]"
        },
        {
            "title": "What Language Model to Train if You Have One Million GPU Hours?",
            "venue": "Conference on Empirical Methods in Natural Language Processing",
            "year": 2022,
            "reference_count": 88,
            "citation_count": 109,
            "influential_citation_count": 5,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://arxiv.org/pdf/2210.15424",
                "status": "GREEN",
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2210.15424, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "1379806208",
                    "name": "Teven Le Scao"
                },
                {
                    "authorId": "2135734748",
                    "name": "Thomas Wang"
                },
                {
                    "authorId": "80424302",
                    "name": "Daniel Hesslow"
                },
                {
                    "authorId": "2113836860",
                    "name": "Lucile Saulnier"
                },
                {
                    "authorId": "32136590",
                    "name": "Stas Bekman"
                },
                {
                    "authorId": "2054179756",
                    "name": "Saiful Bari"
                },
                {
                    "authorId": "103476203",
                    "name": "Stella Biderman"
                },
                {
                    "authorId": "2218938",
                    "name": "Hady ElSahar"
                },
                {
                    "authorId": "2037383772",
                    "name": "Niklas Muennighoff"
                },
                {
                    "authorId": "80842917",
                    "name": "Jason Phang"
                },
                {
                    "authorId": "40170001",
                    "name": "Ofir Press"
                },
                {
                    "authorId": "2402716",
                    "name": "Colin Raffel"
                },
                {
                    "authorId": "2285868436",
                    "name": "Victor Sanh"
                },
                {
                    "authorId": "2191455",
                    "name": "Sheng Shen"
                },
                {
                    "authorId": "35566806",
                    "name": "Lintang Sutawika"
                },
                {
                    "authorId": "2112211652",
                    "name": "Jaesung Tae"
                },
                {
                    "authorId": "1725420331",
                    "name": "Zheng-Xin Yong"
                },
                {
                    "authorId": "143945447",
                    "name": "Julien Launay"
                },
                {
                    "authorId": "46181066",
                    "name": "Iz Beltagy"
                }
            ],
            "abstract": "The crystallization of modeling methods around the Transformer architecture has been a boon for practitioners. Simple, well-motivated architectural variations can transfer across tasks and scale, increasing the impact of modeling research. However, with the emergence of state-of-the-art 100B+ parameters models, large language models are increasingly expensive to accurately design and train. Notably, it can be difficult to evaluate how modeling decisions may impact emergent capabilities, given that these capabilities arise mainly from sheer scale alone. In the process of building BLOOM--the Big Science Large Open-science Open-access Multilingual language model--our goal is to identify an architecture and training setup that makes the best use of our 1,000,000 A100-GPU-hours budget. Specifically, we perform an ablation study at the billion-parameter scale comparing different modeling practices and their impact on zero-shot generalization. In addition, we study the impact of various popular pre-training corpora on zero-shot generalization. We also study the performance of a multilingual model and how it compares to the English-only one. Finally, we consider the scaling behaviour of Transformers to choose the target model size, shape, and training setup. All our models and code are open-sourced at https://huggingface.co/bigscience .",
            "corpus_id": 247625205,
            "sentences": [
                {
                    "corpus_id": "247625205",
                    "title": "What Language Model to Train if You Have One Million GPU Hours?",
                    "text": "In this paper, we base all models on a decoder-only Transformer pretrained with an autoregressive language modeling objective. This is a popular choice for large language models (Brown et al., 2020;Rae et al., 2021;Thoppilan et al., 2022), possibly because it lends itself to zero-shot application to many downstream tasks (Radford et al., 2019). Alternatives include encoder-decoder models trained with a span-corruption objective (e.g., T5 Raffel et al. (2019)), as well as non-causal decoders models with visibility over a prefix (so-called Prefix LMs, Liu et al. (2018); Dong et al. (2019)). \n\nOur decision is motivated by the findings of Wang et al. (2022), which showed that decoderonly models combined with an autoregressive language modeling objective provide the best zeroshot generalization abilities immediately after pretraining. Although multitask finetuning (Sanh et al., 2021;Wei et al., 2021) will instead favor an encoder-decoder with span corruption for best zero-shot generalization, Wang et al. (2022) found a compromise between these two practices. Following autoregressive pretraining, decoder-only models can be efficiently adapted into non-causal decoders, simply by extending pretraining with span corruption. This adaptation produces a second model, which can provide excellent zero-shot generalization after multitask finetuning. Accordingly, we follow their recommendation, and train an autoregressive decoder-only model first which we will later consider adapting and finetuning.",
                    "score": 0.5789497204013925,
                    "section_title": "Architecture and Pretraining Objective",
                    "char_start_offset": 2960,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 126
                        },
                        {
                            "start": 127,
                            "end": 346
                        },
                        {
                            "start": 347,
                            "end": 595
                        },
                        {
                            "start": 598,
                            "end": 841
                        },
                        {
                            "start": 842,
                            "end": 1069
                        },
                        {
                            "start": 1070,
                            "end": 1234
                        },
                        {
                            "start": 1235,
                            "end": 1356
                        },
                        {
                            "start": 1357,
                            "end": 1508
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 178,
                            "end": 198,
                            "matchedPaperCorpusId": "218971783"
                        },
                        {
                            "start": 556,
                            "end": 573,
                            "matchedPaperCorpusId": "3608234"
                        },
                        {
                            "start": 575,
                            "end": 593,
                            "matchedPaperCorpusId": "147704286"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.6279296875
                }
            ],
            "relevance_judgement": 0.6279296875,
            "relevance_judgment_input_expanded": "# Title: What Language Model to Train if You Have One Million GPU Hours?\n# Venue: Conference on Empirical Methods in Natural Language Processing\n# Authors: Teven Le Scao, Thomas Wang, Daniel Hesslow, Lucile Saulnier, Stas Bekman, Saiful Bari, Stella Biderman, Hady ElSahar, Niklas Muennighoff, Jason Phang, Ofir Press, Colin Raffel, Victor Sanh, Sheng Shen, Lintang Sutawika, Jaesung Tae, Zheng-Xin Yong, Julien Launay, Iz Beltagy\n## Abstract\nThe crystallization of modeling methods around the Transformer architecture has been a boon for practitioners. Simple, well-motivated architectural variations can transfer across tasks and scale, increasing the impact of modeling research. However, with the emergence of state-of-the-art 100B+ parameters models, large language models are increasingly expensive to accurately design and train. Notably, it can be difficult to evaluate how modeling decisions may impact emergent capabilities, given that these capabilities arise mainly from sheer scale alone. In the process of building BLOOM--the Big Science Large Open-science Open-access Multilingual language model--our goal is to identify an architecture and training setup that makes the best use of our 1,000,000 A100-GPU-hours budget. Specifically, we perform an ablation study at the billion-parameter scale comparing different modeling practices and their impact on zero-shot generalization. In addition, we study the impact of various popular pre-training corpora on zero-shot generalization. We also study the performance of a multilingual model and how it compares to the English-only one. Finally, we consider the scaling behaviour of Transformers to choose the target model size, shape, and training setup. All our models and code are open-sourced at https://huggingface.co/bigscience .\n## Architecture and Pretraining Objective\nIn this paper, we base all models on a decoder-only Transformer pretrained with an autoregressive language modeling objective. This is a popular choice for large language models (Brown et al., 2020;Rae et al., 2021;Thoppilan et al., 2022), possibly because it lends itself to zero-shot application to many downstream tasks (Radford et al., 2019). Alternatives include encoder-decoder models trained with a span-corruption objective (e.g., T5 Raffel et al. (2019)), as well as non-causal decoders models with visibility over a prefix (so-called Prefix LMs, Liu et al. (2018); Dong et al. (2019)). \n\nOur decision is motivated by the findings of Wang et al. (2022), which showed that decoderonly models combined with an autoregressive language modeling objective provide the best zeroshot generalization abilities immediately after pretraining. Although multitask finetuning (Sanh et al., 2021;Wei et al., 2021) will instead favor an encoder-decoder with span corruption for best zero-shot generalization, Wang et al. (2022) found a compromise between these two practices. Following autoregressive pretraining, decoder-only models can be efficiently adapted into non-causal decoders, simply by extending pretraining with span corruption. This adaptation produces a second model, which can provide excellent zero-shot generalization after multitask finetuning. Accordingly, we follow their recommendation, and train an autoregressive decoder-only model first which we will later consider adapting and finetuning.",
            "reference_string": "[247625205 | Scao et al. | 2022 | Citations: 109]"
        }
    ],
    "retrieved": [
        {
            "corpus_id": "248118752",
            "title": "What Language Model Architecture and Pretraining Objective Work Best for Zero-Shot Generalization?",
            "text": "Notable pretrained language models using an encoder-decoder architecture include BART [Lewis et al., 2019] and T5 [Raffel et al., 2020]. T5 in particular was recently used as the foundation for the T0 model [Sanh et al., 2021], which leveraged large-scale multitask finetuning to achieve strong zero-shot generalization, outperforming decoder-only models an order of magnitude larger. \n\nCausal decoder-only. Although the encoder-decoder is the original Transformer variant, most recent LLMs use a decoder-only architecture. These models can be trained as a traditional language model (i.e. to predict the next token in a sequence). Decoder-only models have no independent means of processing or representing the input sequence and target sequence differently-all tokens are processed in an equivalent fashion, and, because of the causal masking pattern, conditioning is simply based on past tokens (see Figure 2, on the left). On the one hand, this means that the representation for any conditioning text is inherently weaker; on the other hand, it yields a simpler architecture that is naturally suited to a standard autoregressive next-step-prediction pretraining objective. We refer to this architecture as causal decoder-only (CD) . Most notably, the CD architecture makes up the backbone of the GPT series of models [Radford et al., 2018, 2019, Brown et al., 2020] as well as many other recent record-breaking LLMs [Zeng et al., 2021, Kim et al., 2021, Smith et al., 2022, Thoppilan et al., 2022, Rae et al., 2021, Hoffmann et al., 2022, Chowdhery et al., 2022]. \n\nNon-causal decoder-only. To allow decoder-only models to build richer non-causal representations of the input/conditioning text, it has been proposed to simply modify the attention mask used. Specifically, the self-attention masking pattern can be changed so that the region of the input sequence corresponding to conditioning information has a non-causal mask (i.e.",
            "score": 0.7331205509213734,
            "section_title": "Architectures",
            "char_start_offset": 10211,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 136
                },
                {
                    "start": 137,
                    "end": 384
                },
                {
                    "start": 387,
                    "end": 407
                },
                {
                    "start": 408,
                    "end": 523
                },
                {
                    "start": 524,
                    "end": 631
                },
                {
                    "start": 632,
                    "end": 926
                },
                {
                    "start": 927,
                    "end": 1176
                },
                {
                    "start": 1177,
                    "end": 1236
                },
                {
                    "start": 1237,
                    "end": 1567
                },
                {
                    "start": 1570,
                    "end": 1594
                },
                {
                    "start": 1595,
                    "end": 1761
                },
                {
                    "start": 1762,
                    "end": 1936
                }
            ],
            "ref_mentions": [
                {
                    "start": 114,
                    "end": 135,
                    "matchedPaperCorpusId": "204838007"
                },
                {
                    "start": 1321,
                    "end": 1342,
                    "matchedPaperCorpusId": "49313245"
                },
                {
                    "start": 1438,
                    "end": 1456,
                    "matchedPaperCorpusId": "237485423"
                },
                {
                    "start": 1476,
                    "end": 1500,
                    "matchedPaperCorpusId": "238582964"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.794921875
        },
        {
            "corpus_id": "277626724",
            "title": "Encoder-Decoder Gemma: Improving the Quality-Efficiency Trade-Off via Adaptation",
            "text": "Decoder-only pretraining often adopts causal language modeling on a single sequence. In contrast, encoder-decoder adaptation requires separate input and target sequences to be fed to the encoder and decoder separately. We explore two classical pretraining objectives for encoder-decoder modeling: prefix language modeling (PrefixLM) and UL2 (Tay et al., 2022;Wang et al., 2022). \n\nPrefixLM behaves similar to causal language modeling except for its prefix condition. To simplify the preprocessing, we split a sequence equally into two halves, the first half used as input and the second one as target. This also eases the adoption of knowledge distillation from decoder-only models. UL2 is more complicated. It is composed of several denoising tasks at different levels of complexity. We prepare UL2 data following Tay et al. (2022). We compare their performance in experiments.",
            "score": 0.702513057489843,
            "section_title": "Pretraining Objective",
            "char_start_offset": 9025,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 84
                },
                {
                    "start": 85,
                    "end": 218
                },
                {
                    "start": 219,
                    "end": 378
                },
                {
                    "start": 381,
                    "end": 466
                },
                {
                    "start": 467,
                    "end": 601
                },
                {
                    "start": 602,
                    "end": 682
                },
                {
                    "start": 683,
                    "end": 707
                },
                {
                    "start": 708,
                    "end": 784
                },
                {
                    "start": 785,
                    "end": 833
                },
                {
                    "start": 834,
                    "end": 878
                }
            ],
            "ref_mentions": [
                {
                    "start": 341,
                    "end": 359,
                    "matchedPaperCorpusId": "252780443"
                },
                {
                    "start": 359,
                    "end": 377,
                    "matchedPaperCorpusId": "248118752"
                },
                {
                    "start": 815,
                    "end": 832,
                    "matchedPaperCorpusId": "252780443"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.8017578125
        },
        {
            "corpus_id": "258947629",
            "title": "How Powerful are Decoder-Only Transformer Neural Models?",
            "text": "To create a decoder-only model, the vanilla architecture is modified in two ways. First, the connection to the encoder is removed. Second, the cross-attention which allows the decoder to conditionally attend to the encoder output at each layer of the decoder is eliminated. These, along with the entire encoder, are surrounded by a dashed yellow line in 1 to visualize what is eliminated. As mentioned previously, this superficially suggests that encoder-only and decoder-only architectures are identical as seen in 2. \n\n2) Differentiating Encoder-only and Decoder-only Models: Decoder-only models have three necessary characteristics which are derived from their function in the vanilla transformer. The decoder must (1) provide a means of autoregressively predicting the next token based on the tokens generated so far given the encoder input as contextualization. In 2 this is shown as the recursive red connection mapping the output vector back into the last element of the input sequence of vectors. To be suited to this task, decoder-only models must (2) not see future values when evaluating a query on the input sequence of vectors. This is why decoder-only models are often referred to as causal language models (CLM). In 2, we refer to the decoder attention heads as causal attention heads rather than masked attention heads as they are called in [1]. The model must be (3) trained to predict the next token given the current input sequence of vectors. This training method coupled with recursion allows decoder-only models to autoregressively generate arbitrarily long (up to the max size of the input vector sequence) sequences. \n\nIf any of the above are violated, the model can't be reasonably considered a decoder-only model as it is no longer capable of auto-regressive next token prediction.",
            "score": 0.6859072080984483,
            "section_title": "1) Modifying the Vanilla Transformer to form a Decoderonly Model:",
            "char_start_offset": 3733,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 81
                },
                {
                    "start": 82,
                    "end": 130
                },
                {
                    "start": 131,
                    "end": 273
                },
                {
                    "start": 274,
                    "end": 388
                },
                {
                    "start": 389,
                    "end": 518
                },
                {
                    "start": 521,
                    "end": 700
                },
                {
                    "start": 701,
                    "end": 866
                },
                {
                    "start": 867,
                    "end": 1004
                },
                {
                    "start": 1005,
                    "end": 1140
                },
                {
                    "start": 1141,
                    "end": 1227
                },
                {
                    "start": 1228,
                    "end": 1361
                },
                {
                    "start": 1362,
                    "end": 1462
                },
                {
                    "start": 1463,
                    "end": 1640
                },
                {
                    "start": 1643,
                    "end": 1807
                }
            ],
            "ref_mentions": [
                {
                    "start": 1357,
                    "end": 1360,
                    "matchedPaperCorpusId": "13756489"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.8037109375
        },
        {
            "corpus_id": "246035820",
            "title": "CM3: A Causal Masked Multimodal Model of the Internet",
            "text": "For example, masked encoder-only models such as BERT (Devlin et al., 2018) and RoBERTa (Liu et al., 2019) excel in non-generative fine-tuning tasks. Masked encoder-decoder models such as BART (Lewis et al., 2019) and T5 (Raffel et al., 2019) excel in both discriminative and generative fine-tuning. Brown et al. (2020) on the other hand, showed that causal language models (de-facto, decoder only) are capable of non-trivial performance without the need of fine-tuning by simply prompting with appropriate string to control the generated outputs Radford et al. (2019); Brown et al. (2020); Artetxe et al. (2021). \n\nThere are pros and cons to both masked and causal language modeling in the context of prompting. Masking offers the critical ability to encode bi-directionality within the prompts at the cost of only decoding roughly 15% of the tokens of the input sequence during training (Devlin et al., 2018;Liu et al., 2019;Lewis et al., 2019). Conversely, decoder-only causal language models decode every token in the input sequence in the training but are typically limited to left-only contexts. Empirically, more work has also been done on scaling causal decoder-only rather than their counterparts. \n\nIn an effort to get most of the best of both worlds, we introduce a novel objective that combines the benefit of per-token generation with optional bi-directionality specifically tailored to prompting. For a document of size s we select n \u223c Clamp(Poisson(1), 1, 16) masks and for each of those masks we select span m \u223c (U nif orm(0, s), U nif orm(0, s)) which does not intersect with any other m. These values are chosen to, on average, select relatively few relatively long spans, which we expect will allow the model to learn to infill long spans.",
            "score": 0.6554380011827385,
            "section_title": "INTRODUCTION",
            "char_start_offset": 1832,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 148
                },
                {
                    "start": 149,
                    "end": 298
                },
                {
                    "start": 299,
                    "end": 612
                },
                {
                    "start": 615,
                    "end": 711
                },
                {
                    "start": 712,
                    "end": 946
                },
                {
                    "start": 947,
                    "end": 1100
                },
                {
                    "start": 1101,
                    "end": 1205
                },
                {
                    "start": 1208,
                    "end": 1409
                },
                {
                    "start": 1410,
                    "end": 1604
                },
                {
                    "start": 1605,
                    "end": 1757
                }
            ],
            "ref_mentions": [
                {
                    "start": 546,
                    "end": 567,
                    "matchedPaperCorpusId": "160025533"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.48388671875
        },
        {
            "corpus_id": "273025546",
            "title": "ENTP: Encoder-only Next Token Prediction",
            "text": "In contrast, the causal decoder-only model (Brown et al., 2020;Chowdhery et al., 2023) uses only the Transformer decoder and applies causal attention to all tokens to perform nexttoken prediction, ensuring that each token attends only to previous tokens. The prefix decoder-only model (Raffel et al., 2020;Wu et al., 2021) is similar to the causal decoder-only model but differs in that it applies non-causal attention (i.e., full self-attention) to the input sequence (see Figure 8 for visualizations of the attention patterns in these variants). \n\nWith the development of these models, recent studies have investigated the performance of each variant across various tasks. Notably, Wang et al. (2022) examined the zero-shot generalization performance of each model along with various objectives, and Ding et al. (2024) analyzed the performance of causal decoder-only and prefix decoder-only models in in-context learning. However, despite these diverse studies, there is a lack of research on encoder-only models that do not impose the constraint of causal attention for every next token prediction. Therefore, in this work, we analyze the characteristics of encoder-only next-token prediction (ENTP), comparing them with decoder-only models.",
            "score": 0.6511514326530388,
            "section_title": "RELATED WORK",
            "char_start_offset": 5409,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 254
                },
                {
                    "start": 255,
                    "end": 547
                },
                {
                    "start": 550,
                    "end": 674
                },
                {
                    "start": 675,
                    "end": 923
                },
                {
                    "start": 924,
                    "end": 1101
                },
                {
                    "start": 1102,
                    "end": 1244
                }
            ],
            "ref_mentions": [
                {
                    "start": 43,
                    "end": 63,
                    "matchedPaperCorpusId": "218971783"
                },
                {
                    "start": 63,
                    "end": 86,
                    "matchedPaperCorpusId": "247951931"
                },
                {
                    "start": 285,
                    "end": 306,
                    "matchedPaperCorpusId": "204838007"
                },
                {
                    "start": 684,
                    "end": 702,
                    "matchedPaperCorpusId": "248118752"
                },
                {
                    "start": 802,
                    "end": 820,
                    "matchedPaperCorpusId": "260887420"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.904296875
        },
        {
            "corpus_id": "253420279",
            "title": "BLOOM: A 176B-Parameter Open-Access Multilingual Language Model",
            "text": "Although most modern language models are based on the Transformer architecture, there are significant deviations between architectural implementations. Notably, while the original Transformer is based on an encoder-decoder architecture, many popular models have opted for encoder-only (e.g. BERT, (Devlin et al., 2019)) or decoder-only (e.g. GPT, (Radford et al., 2018)) approaches. Currently, all state-of-the-art language models over 100 billion parameters are causal decoder-only models Rae et al., 2021;Chowdhery et al., 2022). This is in opposition to the findings of Raffel et al. (2020), in which encoderdecoder models significantly outperform decoder-only models for transfer learning.\n\nPrior to our work, the literature was lacking a systematic evaluation of the zero-shot generalization capabilities of different architectures and pretraining objectives. We explored this question in Wang et al. (2022a) where we evaluated encoder-decoder and decoder-only architectures and their interactions with causal, prefix, and masked language modeling pretraining objectives. Our results show that immediately after pretraining, causal decoderonly models performed best -validating the choice of state-of-the-art LLMs. Furthermore, they can be more efficiently adapted after pretraining to a non-causal architecture and objective-an approach which has been further explored and confirmed by .",
            "score": 0.6411111600450696,
            "section_title": "Architecture and Pretraining Objective",
            "char_start_offset": 30941,
            "sentence_offsets": [],
            "ref_mentions": [
                {
                    "start": 297,
                    "end": 318,
                    "matchedPaperCorpusId": "52967399"
                },
                {
                    "start": 573,
                    "end": 593,
                    "matchedPaperCorpusId": "204838007"
                },
                {
                    "start": 894,
                    "end": 913,
                    "matchedPaperCorpusId": "248118752"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.74072265625
        },
        {
            "corpus_id": "248118752",
            "title": "What Language Model Architecture and Pretraining Objective Work Best for Zero-Shot Generalization?",
            "text": ". First, we propose to pretrain a non-causal decoder model with an MLM objective and then further train the model as a causal decoder with a FLM objective (language modeling adaptation). This conversion is simple, as the parameters and overall architecture can be kept the same, and only the attention mask needs to be switched. We note that we also attempted this adaptation from the decoder portion of an encoder-decoder model, but it performed significantly worse than training from scratch, as discussed in Appendix E.4. \n\nValidations losses are plotted in Figure 6, on the left. Starting from an MLM-pretrained non-causal decoder model speeds up convergence significantly compared to training a causal-decoder model with an FLM objective from scratch. To achieve a loss comparable to the one achieved after 168B tokens of FLM pretraining, language modeling adaptation requires only 105B additional tokens (a 1.6\u00d7 speed-up). This makes it possible to obtain both a high-quality zero-shot model and a good generative model, for only 1.6\u00d7 the cost of training a single model. A causal decoder-only pretrained with FLM from scratch (Causal FLM) compared to a model being adapted with FLM following 168B pretraining tokens as a non-causal masked language model (LMadapted Non-causal MLM). The adaptation requires 63% of the tokens (1.6\u00d7 speedup) versus training from scratch. Right: Causal and non-causal decoder-only masked language models (Causal MLM, Non-causal MLM) trained from scratch compared to a model being adapted to a non-causal MLM following 168B pretraining tokens as a causal FLM (MLM-adapted Causal FLM). The adaptation requires 30% of the tokens (3.3\u00d7 speedup) versus training the non-causal MLM from scratch. \n\nNon-causal masked language modeling adaptation (NC-A). To investigate alternative avenues for adaptation, we now introduce non-causal masked language modeling adaptation: starting from a causal decoder model pretrained with FLM as the objective, we then continue training the model as a non-causal decoder using an MLM objective.",
            "score": 0.6302885311351464,
            "section_title": "Language modeling adaptation (LM-A)",
            "char_start_offset": 35459,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 186
                },
                {
                    "start": 187,
                    "end": 328
                },
                {
                    "start": 329,
                    "end": 524
                },
                {
                    "start": 527,
                    "end": 583
                },
                {
                    "start": 584,
                    "end": 756
                },
                {
                    "start": 757,
                    "end": 928
                },
                {
                    "start": 929,
                    "end": 1077
                },
                {
                    "start": 1078,
                    "end": 1288
                },
                {
                    "start": 1289,
                    "end": 1375
                },
                {
                    "start": 1376,
                    "end": 1620
                },
                {
                    "start": 1621,
                    "end": 1726
                },
                {
                    "start": 1729,
                    "end": 1783
                },
                {
                    "start": 1784,
                    "end": 2058
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.71435546875
        },
        {
            "corpus_id": "248118752",
            "title": "What Language Model Architecture and Pretraining Objective Work Best for Zero-Shot Generalization?",
            "text": "When studying adaptation and the conversion from one architecture to another, we also considered converting to and from encoder-decoder models. Conversion across causal and non-causal decoder-only models is straightforward, simply by switching the attention mask; for encoder-decoder, parameters have to be either pruned or added for both the entire encoder, and for the cross-attention in the decoder. Results from one of our attempt to convert an encoder-decoder into a causal decoder are reported in Figure 10. While converting across causal/non-causal decoder provides an improvement over training from scratch, this is not the case here. Validation loss when adapting an encoder-decoder pretrained with MLM to a causal decoder-only using FLM. We adapted a pretrained (for 168B tokens) encoder-decoder model to decoder-only by feeding an empty input into the encoder and causally training with a FLM objective on the decoder. We stopped this adaptation once it was clear the performance would not match that of a causal FLM trained from scratch, in contrast with the other adaptations we studied.",
            "score": 0.6298750196169863,
            "section_title": "E.4 Adaptation from an encoder-decoder",
            "char_start_offset": 46680,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 143
                },
                {
                    "start": 144,
                    "end": 402
                },
                {
                    "start": 403,
                    "end": 513
                },
                {
                    "start": 514,
                    "end": 642
                },
                {
                    "start": 643,
                    "end": 747
                },
                {
                    "start": 748,
                    "end": 929
                },
                {
                    "start": 930,
                    "end": 1100
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.84423828125
        },
        {
            "corpus_id": "269982953",
            "title": "Attention Mechanisms Don't Learn Additive Models: Rethinking Feature Importance for Transformers",
            "text": "Practical implementations introduce subtle modifications into the process described previously.The most relevant distinction is made between encoder-only models, that include BERT [15] and its variants, and decoder-only models such as the GPT models [38,39].\n\nEncoder-only models.Considering BERT as an example of an encoder-only model, the first token is used for the classification, i.e, r = 1.Usually, a special token [CLS] is prepended to the text at position 1, however this is not strictly necessary for the functioning of the model.\n\nDecoder-only models.In contrast, decoder-models like GPT-2 [39] add the classification head on top of the last token for classification, i.e., r = |t|.A key difference is that in GPT-2 and other decoder-only models, a causal mask is laid over the attention matrix, resulting in \u03b1 i,j = 0 for j > i.This encodes the constraint that tokens can only attend to themselves or to previous ones.",
            "score": 0.6261452571458622,
            "section_title": "Encoder-Only and Decoder-Only models",
            "char_start_offset": 11540,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 95
                },
                {
                    "start": 95,
                    "end": 258
                },
                {
                    "start": 260,
                    "end": 280
                },
                {
                    "start": 280,
                    "end": 396
                },
                {
                    "start": 396,
                    "end": 539
                },
                {
                    "start": 541,
                    "end": 561
                },
                {
                    "start": 561,
                    "end": 692
                },
                {
                    "start": 692,
                    "end": 839
                },
                {
                    "start": 839,
                    "end": 929
                }
            ],
            "ref_mentions": [
                {
                    "start": 254,
                    "end": 257,
                    "matchedPaperCorpusId": "160025533"
                },
                {
                    "start": 600,
                    "end": 604,
                    "matchedPaperCorpusId": "160025533"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.69287109375
        },
        {
            "corpus_id": "248118752",
            "title": "What Language Model Architecture and Pretraining Objective Work Best for Zero-Shot Generalization?",
            "text": "The complete set of results across all checkpoints obtained through this study is made available in Appendix E.2. We are first interested in the architecture and objective achieving the best zero-shot performance after unsupervised pretraining only. For this, we only consider the full/prefix language modeling objectives since masked language modeling does not yield a model appropriate for zero-shot prompted evaluation on its own. This is validated with early checkpoints in Appendix E.1. \n\nWe present our main full/prefix language modeling pretraining results in Table 3. On both our evaluation benchmarks, the causal decoder architecture systematically outperforms the other architectures when using language modeling pretraining alone. The non-causal decoder remains within a percent of the causal decoder performance, but the encoder-decoder performance lags far behind. Finally, we note that the performances on T0-Eval are close to the random baseline, while performance differences on EAI-Eval are significant enough to make comparison across experiments. \n\nFinding 1. Causal decoder-only models pretrained with a full language modeling objective achieve best zero-shot generalization when evaluated immediately after unsupervised pretraining , in line with current common practices for large language models.",
            "score": 0.6160019241073809,
            "section_title": "Evaluation",
            "char_start_offset": 28247,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 113
                },
                {
                    "start": 114,
                    "end": 249
                },
                {
                    "start": 250,
                    "end": 433
                },
                {
                    "start": 434,
                    "end": 491
                },
                {
                    "start": 494,
                    "end": 575
                },
                {
                    "start": 576,
                    "end": 741
                },
                {
                    "start": 742,
                    "end": 877
                },
                {
                    "start": 878,
                    "end": 1065
                },
                {
                    "start": 1068,
                    "end": 1319
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.51123046875
        },
        {
            "corpus_id": "221702979",
            "title": "Current Limitations of Language Models: What You Need is Retrieval",
            "text": "We consider some of the successful recent language model approaches to improve the performancecomputes trade-off of language model (Kaplan et al., 2020) and classify them for the ease of our argument. Each category tends to be orthogonal to each other, so that they can often be used in combination for further gain. Fig. 1 summarizes our classification. There are four major categories: non-causal models, extension of batch length with efficient attention, memory and retrieval. We note that batch length in this paper is defined to be the length of a minibatch across timestep dimension. We avoid calling it context length to be more specific. In this paper, the default modality of our consideration is text unless specified otherwise. Also, memory and retriever are defined to possess a certain property addressed later in this section, so their usage may differ from the usual one. Efficient attention also refers to non-attention alternatives with better complexity, such as variants of convolution, for the convenience of our argument. \n\nNon-causal models in this paper are defined to be the model that predicts a token using future information. The example includes various BERT variants, including BART and T5 (Devlin et al., 2018;Raffel et al., 2019;Yang et al., 2019;Liu et al., 2019;Beltagy et al., 2020;Zaheer et al., 2020;Lewis et al., 2019;Hendrycks et al., 2020). On the other hand, causal models include the original Transformer encoder-decoder (Vaswani et al., 2017) and GPT-2/3. Though we usually consider the decoder-only model as the latter for a causal model, it should be clear from the context whether a causal model refers to the former, the latter or both.",
            "score": 0.6043837683365122,
            "section_title": "Classification of Recent Language Model Approaches",
            "char_start_offset": 53,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 200
                },
                {
                    "start": 201,
                    "end": 316
                },
                {
                    "start": 317,
                    "end": 354
                },
                {
                    "start": 355,
                    "end": 480
                },
                {
                    "start": 481,
                    "end": 590
                },
                {
                    "start": 591,
                    "end": 646
                },
                {
                    "start": 647,
                    "end": 739
                },
                {
                    "start": 740,
                    "end": 887
                },
                {
                    "start": 888,
                    "end": 1043
                },
                {
                    "start": 1046,
                    "end": 1153
                },
                {
                    "start": 1154,
                    "end": 1380
                },
                {
                    "start": 1381,
                    "end": 1498
                },
                {
                    "start": 1499,
                    "end": 1683
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.258056640625
        },
        {
            "corpus_id": "277244603",
            "title": "FutureGen: LLM-RAG Approach to Generate the Future Work of Scientific Article",
            "text": "We collected 200 random samples from the dataset and, for each sample, evaluated three key questions using Ground Truth, LLM-generated text, and LLM-regenerated text after feedback. future work could focus on exploring novel architectures that combine the strengths of both encoderonly and decoder-only language models to improve semantic understanding in natural language processing tasks. Additionally, research could investigate alternative training methods or prompting techniques to enhance the capabilities of decoder-only models in comprehending word meaning. Furthermore, expanding the study to include languages other than English and evaluating a wider range of language models could provide a more comprehensive understanding of the performance differences between encoder-only and decoder-only architectures. Investigating the impact of model size and training data on semantic understanding could also be a valuable direction for future research. \n\n0.87 0.90 0.88",
            "score": 0.6008233994157317,
            "section_title": "A.1 Human Evaluation",
            "char_start_offset": 32119,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 181
                },
                {
                    "start": 182,
                    "end": 390
                },
                {
                    "start": 391,
                    "end": 566
                },
                {
                    "start": 567,
                    "end": 820
                },
                {
                    "start": 821,
                    "end": 959
                },
                {
                    "start": 962,
                    "end": 976
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.203857421875
        },
        {
            "corpus_id": "250113742",
            "title": "Improving Deliberation by Text-Only and Semi-Supervised Training",
            "text": "Our model is illustrated in Fig. 1. Note that different from [9,10], the deliberation decoder is based on the non-causal encoder [2] instead of a causal encoder. The decoder attends to both the non-causal encoder output (e) and hypotheses (yr) from the non-causal path, i.e., decoded using non-causal encoder. The non-causal encoder often has a right-context for better recognition quality [2]. We use a conformer encoder [28] as the text encoder. A two-source attention LAS decoder is used as the deliberation decoder, similar to [9]. The decoder can be used for either re-decoding or rescoring. The deliberation model in this work does not stream compared to [29], as we focus on text-only and semi-supervised training.",
            "score": 0.5973825861675406,
            "section_title": "Model Overview",
            "char_start_offset": 4458,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 35
                },
                {
                    "start": 36,
                    "end": 161
                },
                {
                    "start": 162,
                    "end": 309
                },
                {
                    "start": 310,
                    "end": 394
                },
                {
                    "start": 395,
                    "end": 447
                },
                {
                    "start": 448,
                    "end": 535
                },
                {
                    "start": 536,
                    "end": 596
                },
                {
                    "start": 597,
                    "end": 721
                }
            ],
            "ref_mentions": [
                {
                    "start": 61,
                    "end": 64,
                    "matchedPaperCorpusId": "212747696"
                },
                {
                    "start": 64,
                    "end": 67,
                    "matchedPaperCorpusId": "231718654"
                },
                {
                    "start": 129,
                    "end": 132,
                    "matchedPaperCorpusId": "225094578"
                },
                {
                    "start": 390,
                    "end": 393,
                    "matchedPaperCorpusId": "225094578"
                },
                {
                    "start": 531,
                    "end": 534,
                    "matchedPaperCorpusId": "212747696"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.197265625
        },
        {
            "corpus_id": "249626024",
            "title": "Language Models are General-Purpose Interfaces",
            "text": "Moreover, in terms of architecture, we directly feed the outputs of bidirectional encoders into the causal decoder, rather than relying on cross attention (Vaswani et al., 2017). Besides, multiple bidirectional encoders can be mounted to the causal language model, but the encoder-decoder architecture usually has only one encoder. \n\nNon-causal encoders as System 1, and causal language models as System 2. Cognition is usually categorized into two levels (Kahneman, 2011;Bengio, 2019): System 1 (i.e., intuitive, and unconscious) and System 2 (i.e., sequential, conscious, planning, and reasoning). In the proposed framework, the modules can be regarded as an implementation of these two levels, respectively. To be specific, non-causal encoders pretrained by masked data modeling, such as BERT (Devlin et al., 2019) and BEiT (Bao et al., 2022), are used as a perception layer to encode various input modalities. The encoding modules can be viewed as System 1. After we obtain the input representations, we  feed them to the causal language model, which has shown promising performance on commonsense reasoning (Chowdhery et al., 2022) and planning (Huang et al., 2022). The universal task layer is designed to play a role of System 2 in our method. \n\nNatural language interface between users and pretrained models. The universal task layer based on causal language modeling enables users to interact with pretrained non-causal encoders using natural language. First, language can be used as a programming language for the underlying pretrained or finetuned models, which is compiled by the universal interface. For example, we can write text-based instructions (Ouyang et al., 2022) and explanations (Wei et al., 2022) to repurpose and guide the model behaviors. Second, the universal interface enables the models to present the results using free texts, making predictions directly understandable and explainable. Third, the proposed framework natively supports multi-turn conversational interactions. In each turn, we can feed the encoded input to the interface layer and then generate response results in a semi-causal manner.",
            "score": 0.5950525413144571,
            "section_title": "body",
            "char_start_offset": 3968,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 178
                },
                {
                    "start": 179,
                    "end": 331
                },
                {
                    "start": 334,
                    "end": 599
                },
                {
                    "start": 600,
                    "end": 710
                },
                {
                    "start": 711,
                    "end": 913
                },
                {
                    "start": 914,
                    "end": 961
                },
                {
                    "start": 962,
                    "end": 1171
                },
                {
                    "start": 1172,
                    "end": 1250
                },
                {
                    "start": 1253,
                    "end": 1316
                },
                {
                    "start": 1317,
                    "end": 1461
                },
                {
                    "start": 1462,
                    "end": 1612
                },
                {
                    "start": 1613,
                    "end": 1764
                },
                {
                    "start": 1765,
                    "end": 1916
                },
                {
                    "start": 1917,
                    "end": 2004
                },
                {
                    "start": 2005,
                    "end": 2131
                }
            ],
            "ref_mentions": [
                {
                    "start": 796,
                    "end": 817,
                    "matchedPaperCorpusId": "52967399"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.56689453125
        },
        {
            "corpus_id": "268157336",
            "title": "A Review of Current Trends, Techniques, and Challenges in Large Language Models (LLMs)",
            "text": "Prefix language models are also decoder-only-based models but differ in the masking mechanism. Instead of a causal mask, a fully visible mask is used for the prefix part of the input sequence, and a causal mask is used for the target sequence. \n\nFor example, to translate an English sentence \"I am doing well\" to French, the model would apply a fully visible mask to the prefix \"translate English to French: I am doing well. Target:\", followed by causal masking while predicting the target \"je vais bien\". Also, unlike causal language models where the targets-only paradigm is used, the prefix language model uses the input-to-target paradigm. Both causal and prefix model architectures are autoregressive as the objective is to predict the next token. However, the causal model uses a unidirectional attention mask, while the prefix model modifies the masking mechanism to employ bidirectional attention over prefix tokens. Figure 4 demonstrates the mechanism of the above architectures. The lines represent the attention visibility. Dark lines represent the fully visible masking (bidirectional attention), and light gray lines represent causal masking (unidirectional attention). As shown in Figure 4, in the encoder-decoder architecture, fully visible masking is used in the encoder and causal masking is used in the decoder. In a decoder-only model, the input and target are concatenated, and then a causal mask is used throughout. A decoder-only model with a prefix allows fully visible masking over part of the input token (prefix), followed by causal masking on the rest of the sequence. In general, autoencoding models learn bidirectional contextualized representation suited for NLU tasks, whereas autoregressive models learn to generate the next token and hence are suited for NLG tasks.",
            "score": 0.5904376792323018,
            "section_title": "Prefix (Non-Causal) Language Model",
            "char_start_offset": 19013,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 94
                },
                {
                    "start": 95,
                    "end": 243
                },
                {
                    "start": 246,
                    "end": 424
                },
                {
                    "start": 425,
                    "end": 505
                },
                {
                    "start": 506,
                    "end": 643
                },
                {
                    "start": 644,
                    "end": 752
                },
                {
                    "start": 753,
                    "end": 924
                },
                {
                    "start": 925,
                    "end": 988
                },
                {
                    "start": 989,
                    "end": 1034
                },
                {
                    "start": 1035,
                    "end": 1182
                },
                {
                    "start": 1183,
                    "end": 1329
                },
                {
                    "start": 1330,
                    "end": 1436
                },
                {
                    "start": 1437,
                    "end": 1595
                },
                {
                    "start": 1596,
                    "end": 1798
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.77587890625
        },
        {
            "corpus_id": "249626024",
            "title": "Language Models are General-Purpose Interfaces",
            "text": "For example, we can use task instructions (Ouyang et al., 2022) to repurpose the model, and use demonstrations of some examples to conduct few-shot learning. \n\nNon-causal modeling (i.e., bidirectional encoder) is conducive to transfer across tasks, languages, and modalities. Although causal language models are good at zero-and few-shot generalization, BERT (Devlin et al., 2019) and T5 (Raffel et al., 2020) show that having bidirectional encoders pretrained by masked language modeling achieves much better finetuning performance. Once the whole input is given, non-causal modeling is quite rational for encoding data. Because all the context can access each other, while causal modeling can only make use of history tokens one by one. The advantage of finetuning is helpful for the data-rich setting where there are many annotated data available. In addition, non-causal encoder pretrained by the masked language modeling objective achieves competitive performance on cross-lingual transfer (Conneau et al., 2020), which makes it effective to adapt models to the multilingual setting. \n\nSemi-causal language modeling as a meta-pretraining task. Semi-causal language modeling plays the role of linking together non-causal encoders and the causal language model. It is a meta task in the sense of universal interface pretraining of pretrained encoders. Specifically, non-causal encoders learn to represent various input data, and a causal language model serves as a universal task layer. Non-causal encoders dock with a causal language model, so that we can benefit from both modeling methods described as above. In comparison with previous encoder-decoder pretraining (such as prefix language modeling, and T5; Raffel et al. 2020), our task non-causally encodes random spans of the whole sequence, while generating the rest via causal language modeling. Moreover, in terms of architecture, we directly feed the outputs of bidirectional encoders into the causal decoder, rather than relying on cross attention (Vaswani et al., 2017).",
            "score": 0.5883874687556997,
            "section_title": "body",
            "char_start_offset": 2111,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 157
                },
                {
                    "start": 160,
                    "end": 275
                },
                {
                    "start": 276,
                    "end": 533
                },
                {
                    "start": 534,
                    "end": 621
                },
                {
                    "start": 622,
                    "end": 738
                },
                {
                    "start": 739,
                    "end": 850
                },
                {
                    "start": 851,
                    "end": 1088
                },
                {
                    "start": 1091,
                    "end": 1148
                },
                {
                    "start": 1149,
                    "end": 1264
                },
                {
                    "start": 1265,
                    "end": 1354
                },
                {
                    "start": 1355,
                    "end": 1489
                },
                {
                    "start": 1490,
                    "end": 1614
                },
                {
                    "start": 1615,
                    "end": 1856
                },
                {
                    "start": 1857,
                    "end": 2035
                }
            ],
            "ref_mentions": [
                {
                    "start": 359,
                    "end": 380,
                    "matchedPaperCorpusId": "52967399"
                },
                {
                    "start": 388,
                    "end": 409,
                    "matchedPaperCorpusId": "204838007"
                },
                {
                    "start": 995,
                    "end": 1017,
                    "matchedPaperCorpusId": "207880568"
                },
                {
                    "start": 1714,
                    "end": 1733,
                    "matchedPaperCorpusId": "204838007"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.6142578125
        },
        {
            "corpus_id": "248118752",
            "title": "What Language Model Architecture and Pretraining Objective Work Best for Zero-Shot Generalization?",
            "text": "We consider causal decoder (CD) , encoder-decoder (ED) , and non-causal decoder (ND) architectures. All models share the basic configuration outlined in Table 1. For fair comparison across architectures, we aim to approximately match pretraining compute budget; accordingly, our encoder-decoder models have twice as many layers as the decoder-only models. This results in encoder-decoder models with 11B parameters and decoder-only models with 4.8B parameters. We note that due to the cross-attention layers, encoder-decoder models are approximately \u223c 10% more computationally expensive to run than the decoder-only models we consider.",
            "score": 0.5834455430461523,
            "section_title": "Architecture",
            "char_start_offset": 21774,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 99
                },
                {
                    "start": 100,
                    "end": 161
                },
                {
                    "start": 162,
                    "end": 355
                },
                {
                    "start": 356,
                    "end": 460
                },
                {
                    "start": 461,
                    "end": 635
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.83642578125
        },
        {
            "corpus_id": "247625205",
            "title": "What Language Model to Train if You Have One Million GPU Hours?",
            "text": "In this paper, we base all models on a decoder-only Transformer pretrained with an autoregressive language modeling objective. This is a popular choice for large language models (Brown et al., 2020;Rae et al., 2021;Thoppilan et al., 2022), possibly because it lends itself to zero-shot application to many downstream tasks (Radford et al., 2019). Alternatives include encoder-decoder models trained with a span-corruption objective (e.g., T5 Raffel et al. (2019)), as well as non-causal decoders models with visibility over a prefix (so-called Prefix LMs, Liu et al. (2018); Dong et al. (2019)). \n\nOur decision is motivated by the findings of Wang et al. (2022), which showed that decoderonly models combined with an autoregressive language modeling objective provide the best zeroshot generalization abilities immediately after pretraining. Although multitask finetuning (Sanh et al., 2021;Wei et al., 2021) will instead favor an encoder-decoder with span corruption for best zero-shot generalization, Wang et al. (2022) found a compromise between these two practices. Following autoregressive pretraining, decoder-only models can be efficiently adapted into non-causal decoders, simply by extending pretraining with span corruption. This adaptation produces a second model, which can provide excellent zero-shot generalization after multitask finetuning. Accordingly, we follow their recommendation, and train an autoregressive decoder-only model first which we will later consider adapting and finetuning.",
            "score": 0.5789497204013925,
            "section_title": "Architecture and Pretraining Objective",
            "char_start_offset": 2960,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 126
                },
                {
                    "start": 127,
                    "end": 346
                },
                {
                    "start": 347,
                    "end": 595
                },
                {
                    "start": 598,
                    "end": 841
                },
                {
                    "start": 842,
                    "end": 1069
                },
                {
                    "start": 1070,
                    "end": 1234
                },
                {
                    "start": 1235,
                    "end": 1356
                },
                {
                    "start": 1357,
                    "end": 1508
                }
            ],
            "ref_mentions": [
                {
                    "start": 178,
                    "end": 198,
                    "matchedPaperCorpusId": "218971783"
                },
                {
                    "start": 556,
                    "end": 573,
                    "matchedPaperCorpusId": "3608234"
                },
                {
                    "start": 575,
                    "end": 593,
                    "matchedPaperCorpusId": "147704286"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.6279296875
        },
        {
            "corpus_id": "248779924",
            "title": "Zero- and Few-Shot NLP with Pretrained Language Models",
            "text": "The following section will focus on the factor underlying the success of these methods-language model pretraining. First, we will provide a review of popular language model pretraining objectives and architectures. Topics will include: causal (Radford et al., 2019) vs. masked (Devlin et al., 2019) pretraining, encoder-only (Devlin et al., 2019;Liu et al., 2019) vs. decoder-only (Radford et al., 2019) vs. encoderdecoder architectures (Lewis et al., 2020;Raffel et al., 2020), and the impact of training data (Aghajanyan et al., 2021;Saxton et al., 2019;Gao et al., 2021a).",
            "score": 0.5772838689354339,
            "section_title": "Pretraining -(20 minutes)",
            "char_start_offset": 4523,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 114
                },
                {
                    "start": 115,
                    "end": 214
                },
                {
                    "start": 215,
                    "end": 575
                }
            ],
            "ref_mentions": [
                {
                    "start": 243,
                    "end": 265,
                    "matchedPaperCorpusId": "160025533"
                },
                {
                    "start": 277,
                    "end": 298,
                    "matchedPaperCorpusId": "52967399"
                },
                {
                    "start": 325,
                    "end": 346,
                    "matchedPaperCorpusId": "52967399"
                },
                {
                    "start": 381,
                    "end": 403,
                    "matchedPaperCorpusId": "160025533"
                },
                {
                    "start": 437,
                    "end": 457,
                    "matchedPaperCorpusId": "204960716"
                },
                {
                    "start": 457,
                    "end": 477,
                    "matchedPaperCorpusId": "204838007"
                },
                {
                    "start": 511,
                    "end": 536,
                    "matchedPaperCorpusId": "235899205"
                },
                {
                    "start": 536,
                    "end": 556,
                    "matchedPaperCorpusId": "85504763"
                },
                {
                    "start": 556,
                    "end": 574,
                    "matchedPaperCorpusId": "230435736"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.43310546875
        },
        {
            "corpus_id": "258685677",
            "title": "CodeT5+: Open Code Large Language Models for Code Understanding and Generation",
            "text": "Following the success of large language models (LLMs) such as BERT [Devlin et al., 2019] and GPT [Radford et al., 2019] in natural language processing (NLP), recent years witness a surge of research work of LLMs in the code domain, leading to new SoTA results on a wide spectrum of code-related tasks. Typically, code-based LLMs can be categorized into three architectures: encoderonly models [Feng et al., 2020, Guo et al., 2021, Wang et al., 2022a], decoder-only models [Lu et al., 2021, Chen et al., 2021, Fried et al., 2022, Nijkamp et al., 2023b], and encoder-decoder models [Ahmad et al., 2021, Wang et al., 2021b, Niu et al., 2022, Chakraborty et al., 2022]. For encoder-only and decoder-only models, they are often ideal for either understanding tasks such as code retrieval [Husain et al., 2019] or generation tasks such as code synthesis [Chen et al., 2021, Hendrycks et al., 2021] respectively. For encoder-decoder models, they can be adapted to both code understanding and generation but do not always achieve better performance [Wang et al., 2021b, Ahmad et al., 2021] than decoder-only or encoder-only models. In this work, we propose a new family of encoder-decoder code large language models that can flexibly operate in various modes, including encoder-only, decoder-only, and encoder-decoder models. \n\nPrior code LLMs are also limited by their pretraining tasks, which are not perfect to transfer the models to some downstream tasks.",
            "score": 0.571953001480261,
            "section_title": "Related Work",
            "char_start_offset": 6554,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 301
                },
                {
                    "start": 302,
                    "end": 665
                },
                {
                    "start": 666,
                    "end": 905
                },
                {
                    "start": 906,
                    "end": 1123
                },
                {
                    "start": 1124,
                    "end": 1317
                },
                {
                    "start": 1320,
                    "end": 1451
                }
            ],
            "ref_mentions": [
                {
                    "start": 67,
                    "end": 88,
                    "matchedPaperCorpusId": "52967399"
                },
                {
                    "start": 97,
                    "end": 119,
                    "matchedPaperCorpusId": "160025533"
                },
                {
                    "start": 393,
                    "end": 411,
                    "matchedPaperCorpusId": "211171605"
                },
                {
                    "start": 429,
                    "end": 450,
                    "matchedPaperCorpusId": "248512635"
                },
                {
                    "start": 472,
                    "end": 488,
                    "matchedPaperCorpusId": "231855531"
                },
                {
                    "start": 527,
                    "end": 551,
                    "matchedPaperCorpusId": "252668917"
                },
                {
                    "start": 580,
                    "end": 599,
                    "matchedPaperCorpusId": "232185260"
                },
                {
                    "start": 599,
                    "end": 619,
                    "matchedPaperCorpusId": "237386541"
                },
                {
                    "start": 619,
                    "end": 637,
                    "matchedPaperCorpusId": "246077487"
                },
                {
                    "start": 637,
                    "end": 664,
                    "matchedPaperCorpusId": "249674577"
                },
                {
                    "start": 1041,
                    "end": 1060,
                    "matchedPaperCorpusId": "237386541"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.34912109375
        },
        {
            "corpus_id": "248118752",
            "title": "What Language Model Architecture and Pretraining Objective Work Best for Zero-Shot Generalization?",
            "text": "Specifically, the self-attention masking pattern can be changed so that the region of the input sequence corresponding to conditioning information has a non-causal mask (i.e. attention in this region is not restricted to past tokens, see middle of Figure 2), as in the encoder of an encoder-decoder architecture. We refer to this architecture as non-causal decoder-only (ND) . Sometimes called a prefix language model, this approach was introduced by [Liu et al., 2018] and was later explored as an architectural variant by [Raffel et al., 2020, Wu et al., 2021]. Despite single-task finetuning performance nearly on par with encoder-decoder models [Raffel et al., 2020], it has seen limited adoption in the literature. \n\nEncoder-only. As an aside, we note that another popular architectural variant is to only use a Transformer encoder layer stack. This model architecture underlies the ubiquitous BERT [Devlin et al., 2018] and its derivatives. However, this architecture is limited to producing the same number of tokens as it was fed as input, considerably limiting its applicability and making it only rarely used in the zero-shot setting [Tamborrino et al., 2020]. We therefore omit it from consideration. For full language modeling, all tokens in a sequence are used during training. For prefix language modeling, we randomly select a prefix size, and hence only half of the tokens are used on average to derive the loss. At inference time, the prefix would be over the input/conditioning information. Finally, for masked language modeling, we mask 15% of the tokens, in spans of 3 tokens on average. We use sentinel tokens to replace spans (not represented here), and the model outputs subsequently each sentinel followed by its prediction of the content masked by the sentinel. \n\nComparisons across architectures. Decoder-only models process a single sequence consisting of the concatenation of the input and target text. On the other hand, in an encoder-decoder, the encoder processes only the input and the decoder processes only the target.",
            "score": 0.5715552541593032,
            "section_title": "Architectures",
            "char_start_offset": 11973,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 174
                },
                {
                    "start": 175,
                    "end": 312
                },
                {
                    "start": 313,
                    "end": 376
                },
                {
                    "start": 377,
                    "end": 563
                },
                {
                    "start": 564,
                    "end": 719
                },
                {
                    "start": 722,
                    "end": 735
                },
                {
                    "start": 736,
                    "end": 849
                },
                {
                    "start": 850,
                    "end": 946
                },
                {
                    "start": 947,
                    "end": 1170
                },
                {
                    "start": 1171,
                    "end": 1211
                },
                {
                    "start": 1212,
                    "end": 1290
                },
                {
                    "start": 1291,
                    "end": 1428
                },
                {
                    "start": 1429,
                    "end": 1508
                },
                {
                    "start": 1509,
                    "end": 1607
                },
                {
                    "start": 1608,
                    "end": 1786
                },
                {
                    "start": 1789,
                    "end": 1822
                },
                {
                    "start": 1823,
                    "end": 1930
                },
                {
                    "start": 1931,
                    "end": 2052
                }
            ],
            "ref_mentions": [
                {
                    "start": 451,
                    "end": 469,
                    "matchedPaperCorpusId": "3608234"
                },
                {
                    "start": 524,
                    "end": 544,
                    "matchedPaperCorpusId": "204838007"
                },
                {
                    "start": 649,
                    "end": 670,
                    "matchedPaperCorpusId": "204838007"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.62841796875
        },
        {
            "corpus_id": "258461229",
            "title": "CodeGen2: Lessons for Training LLMs on Programming and Natural Languages",
            "text": "LLMs on Code Transformers capture dependency among sequence elements through attention mechanism (Bahdanau et al., 2014) and are highly scalable, as shown in natural language processing (Devlin et al., 2019;Lewis et al., 2020;Raffel et al., 2020). Several efforts explore these models for program synthesis (Chen et al., 2021a;Austin et al., 2021;Li et al., 2022;Fried et al., 2022;Nijkamp et al., 2022;Allal et al., 2023) and its effectiveness (Vaithilingam et al., 2022). \n\nAblation Studies Raffel et al. (2020) introduce the concept of non-causal decoder in the form of a Prefix-LM with favorable performance over causal decoders after fine-tuning on down-stream tasks. \n\nThe performance in few-shot generative tasks was not evaluated. Wang et al. (2022) conduct an extensive ablation study over architectures and objectives with the conclusion that decoder-only models with causal language modeling exhibit the strongest zero-shot generalization. Therefore, we limit our investigation to causal and non-causal decoders. Tay et al. (2022a) compare encoder-decoder, decoder-only, and Prefix-LM architectures and report the beneficial performance of encoder-decoder models, while zero-shot generation tasks are not evaluated. The authors later adopt Prefix-LM instead of encoder-decoder in (Tay et al., 2022b). \n\nData Mixtures LaMDA (Thoppilan et al., 2022) was trained on a mixture of various data sources including dialogues, code documents, Q&A data, tutorials, and, Wikipedia. However, the impact of this mixture and the specific sources are unclear. Xie et al. ( 2023) introduces a data selection method based on importance resampling which allows to mix datasets of various sizes, however, the evaluation only covers encoder-only models.",
            "score": 0.5703271664332408,
            "section_title": "RELATED WORK",
            "char_start_offset": 5422,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 247
                },
                {
                    "start": 248,
                    "end": 473
                },
                {
                    "start": 476,
                    "end": 672
                },
                {
                    "start": 675,
                    "end": 738
                },
                {
                    "start": 739,
                    "end": 950
                },
                {
                    "start": 951,
                    "end": 1023
                },
                {
                    "start": 1024,
                    "end": 1226
                },
                {
                    "start": 1227,
                    "end": 1311
                },
                {
                    "start": 1314,
                    "end": 1481
                },
                {
                    "start": 1482,
                    "end": 1555
                },
                {
                    "start": 1556,
                    "end": 1744
                }
            ],
            "ref_mentions": [
                {
                    "start": 186,
                    "end": 207,
                    "matchedPaperCorpusId": "52967399"
                },
                {
                    "start": 207,
                    "end": 226,
                    "matchedPaperCorpusId": "204960716"
                },
                {
                    "start": 226,
                    "end": 246,
                    "matchedPaperCorpusId": "204838007"
                },
                {
                    "start": 347,
                    "end": 363,
                    "matchedPaperCorpusId": "246527904"
                },
                {
                    "start": 445,
                    "end": 472,
                    "matchedPaperCorpusId": "247255943"
                },
                {
                    "start": 493,
                    "end": 513,
                    "matchedPaperCorpusId": "204838007"
                },
                {
                    "start": 739,
                    "end": 757,
                    "matchedPaperCorpusId": "248118752"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.50927734375
        },
        {
            "corpus_id": "277349741",
            "title": "Exploring the Roles of Large Language Models in Reshaping Transportation Systems: A Survey, Framework, and Roadmap",
            "text": "Models like T5 (Raffel et al., 2020) and BART (Lewis et al., 2019) utilize both encoding and decoding mechanisms, enabling them to perform a wide range of tasks, including translation and summarization. The encoder applies stacked self-attention layers to encode the input sequence, and the decoder performs cross-attention on these representations and autoregressively generates the output. \n\n\u2022 Causal decoder. As a representative decoder-only architecture, causal decoder models introduce the unidirectional attention mask to ensure that each input token can only attend to the past tokens and itself. This mechanism makes them suitable for text generation tasks. Prominent examples are the GPTseries models (Radford et al., 2018(Radford et al., , 2019;;Brown et al., 2020). \n\n\u2022 Non-causal decoder. Another kind of decoder-only architecture is the non-casual structure. This architecture performs bidirectional attention on prefix tokens and unidirectional attention only on generated tokens. One representative prefix decoder LLMs is GLM (Zeng et al., 2022). \n\nTo scale the capacity of LLMs efficiently, the Mixture of Experts (MoE) technique can be exploited to combine the above architectures, such as in Swich Transformer (Fedus et al., 2022) and GLaM (Du et al., 2022). MoE involves sparsely activating a subset of model parameters (the \"experts\") for each input, allowing the model to handle a vast number of parameters without incurring prohibitive computational costs. This is achieved by employing a trainable gating mechanism to route each input token to the most relevant subset of experts. \n\nApart from the mainstream Transformer architecture, there are also emerging architectures proposed to alleviate the inherent issues of Transformers (e.g., the quadratic complexity) such as State-Space Models (SSMs) (Gu et al., 2021), Mamba (Gu and Dao, 2023), and RWKV (Peng et al., 2023).",
            "score": 0.569274676786655,
            "section_title": "Architecture",
            "char_start_offset": 34135,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 202
                },
                {
                    "start": 203,
                    "end": 391
                },
                {
                    "start": 394,
                    "end": 411
                },
                {
                    "start": 412,
                    "end": 603
                },
                {
                    "start": 604,
                    "end": 665
                },
                {
                    "start": 666,
                    "end": 776
                },
                {
                    "start": 779,
                    "end": 800
                },
                {
                    "start": 801,
                    "end": 871
                },
                {
                    "start": 872,
                    "end": 994
                },
                {
                    "start": 995,
                    "end": 1061
                },
                {
                    "start": 1064,
                    "end": 1276
                },
                {
                    "start": 1277,
                    "end": 1478
                },
                {
                    "start": 1479,
                    "end": 1603
                },
                {
                    "start": 1606,
                    "end": 1895
                }
            ],
            "ref_mentions": [
                {
                    "start": 15,
                    "end": 36,
                    "matchedPaperCorpusId": "204838007"
                },
                {
                    "start": 731,
                    "end": 756,
                    "matchedPaperCorpusId": "160025533"
                },
                {
                    "start": 756,
                    "end": 775,
                    "matchedPaperCorpusId": "218971783"
                },
                {
                    "start": 1228,
                    "end": 1248,
                    "matchedPaperCorpusId": "231573431"
                },
                {
                    "start": 1258,
                    "end": 1275,
                    "matchedPaperCorpusId": "245124124"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.658203125
        },
        {
            "corpus_id": "265985194",
            "title": "Too Much Information: Keeping Training Simple for BabyLMs",
            "text": "We opted to use encoder-only models. We initially experimented with encoder-decoder models, but found that the evaluation metrics for this shared task being non-generative gave encoder-only models an advantage, as it allows for full attention, rather than only causal attention. In terms of specific model selection, we opted for RoBERTa-base (Liu et al., 2019) in order to directly compare with the provided baseline. We also experimented with (and ultimately submitted) DeBERTa-large (He et al., 2021) as it is a larger model and considered state-of-the-art for encoder-only models.",
            "score": 0.5662406946562063,
            "section_title": "Model Choice",
            "char_start_offset": 4521,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 36
                },
                {
                    "start": 37,
                    "end": 278
                },
                {
                    "start": 279,
                    "end": 418
                },
                {
                    "start": 419,
                    "end": 584
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.35205078125
        },
        {
            "corpus_id": "254069435",
            "title": "BARTSmiles: Generative Masked Language Models for Molecular Representations",
            "text": "Wang et al. (2022b) argues that for zeroshot or k-shot prompting, causal language models with uni-directional attention is optimal, while bidirectionality (in both context and attention masks) is the primary driver of success in the fine-tuning setting. \n\nWithin the fine-tuning setting, there are different objectives conditioned on model type. For encoderbased models, the masked language modeling objective as initially proposed in Devlin et al. (2018) and further refined in Liu et al. (2019). The downside of encoder models is the inability to do generative fine-tuning, which led to the introduction of the denoising model for the encoder-decoder models Lewis et al. (2019). Decoder causal models are problematic because they are not bidirectional, although recently proposed objectives such as causal masking in Aghajanyan et al. (2022) are bidirectional in context but not in attention. \n\nFor BARTSmiles we select both the denoising objective and architecture from Lewis et al. (2019) while previous works have focused on encoder-only (Wang et al., 2019;Chithrananda et al., 2020).",
            "score": 0.5624562778319434,
            "section_title": "RELATED WORK",
            "char_start_offset": 5057,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 253
                },
                {
                    "start": 256,
                    "end": 345
                },
                {
                    "start": 346,
                    "end": 497
                },
                {
                    "start": 498,
                    "end": 680
                },
                {
                    "start": 681,
                    "end": 894
                },
                {
                    "start": 897,
                    "end": 1089
                }
            ],
            "ref_mentions": [
                {
                    "start": 1043,
                    "end": 1062,
                    "matchedPaperCorpusId": "202159174"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.49267578125
        },
        {
            "corpus_id": "267211690",
            "title": "TURNA: A Turkish Encoder-Decoder Language Model for Enhanced Understanding and Generation",
            "text": "Recent advances in natural language processing (NLP) have predominantly produced Englishcentric models (Devlin et al., 2019;Clark et al., 2020;Radford et al., 2019;Brown et al., 2020;Touvron et al., 2023;Jiang et al., 2023). Englishcentric models have benefited from the vast amount of training data gathered from an abundance of English resources present on the web. As such, these models become utilized in applications and fuel an abundance of further research leading to the stateof-the-art performances across various tasks (Touvron et al., 2023;Jiang et al., 2023). On the other hand, low-resource languages face challenges due to the lack of data and limited computational resources, leading to a significant gap between models trained on well-resourced languages and those focusing on low-resource languages. Multilingual models have been proposed that attempt to bridge this gap (Devlin et al., 2019;Conneau et al., 2020;Xue et al., 2021;Liu et al., 2020). However, they often do not perform well in tasks requiring a deep understanding of language-specific nuances, such as dependency parsing and named entity recognition (Virtanen et al., 2019;Baumann, 2019;Tanvir et al., 2021) and lag behind monolingual models of the same scale (Rust et al., 2021;Nozza et al., 2020). \n\nRecently, pretrained language models built upon transformers (Vaswani et al., 2017) have dominated NLP. These models vary in terms of their architectures and objectives (i.e., causal language modeling and denoising objectives). The architectures are commonly classified as encoder-only, decoder-only, or encoder-decoder models. Encoder-only models are typically trained with denoising objectives and focus on understanding tasks (Devlin et al., 2019;Clark et al., 2020). Decoder-only models are designed for generation tasks with causal language modeling (Radford et al., 2019;Brown et al., 2020;Touvron et al., 2023).",
            "score": 0.5622314575716991,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 224
                },
                {
                    "start": 225,
                    "end": 367
                },
                {
                    "start": 368,
                    "end": 571
                },
                {
                    "start": 572,
                    "end": 816
                },
                {
                    "start": 817,
                    "end": 965
                },
                {
                    "start": 966,
                    "end": 1281
                },
                {
                    "start": 1284,
                    "end": 1387
                },
                {
                    "start": 1388,
                    "end": 1511
                },
                {
                    "start": 1512,
                    "end": 1611
                },
                {
                    "start": 1612,
                    "end": 1754
                },
                {
                    "start": 1755,
                    "end": 1902
                }
            ],
            "ref_mentions": [
                {
                    "start": 103,
                    "end": 124,
                    "matchedPaperCorpusId": "52967399"
                },
                {
                    "start": 124,
                    "end": 143,
                    "matchedPaperCorpusId": "208229926"
                },
                {
                    "start": 143,
                    "end": 164,
                    "matchedPaperCorpusId": "160025533"
                },
                {
                    "start": 204,
                    "end": 223,
                    "matchedPaperCorpusId": "221006090"
                },
                {
                    "start": 551,
                    "end": 570,
                    "matchedPaperCorpusId": "221006090"
                },
                {
                    "start": 888,
                    "end": 909,
                    "matchedPaperCorpusId": "52967399"
                },
                {
                    "start": 909,
                    "end": 930,
                    "matchedPaperCorpusId": "207880568"
                },
                {
                    "start": 1155,
                    "end": 1169,
                    "matchedPaperCorpusId": "208131025"
                },
                {
                    "start": 1242,
                    "end": 1261,
                    "matchedPaperCorpusId": "229924220"
                },
                {
                    "start": 1345,
                    "end": 1367,
                    "matchedPaperCorpusId": "13756489"
                },
                {
                    "start": 1713,
                    "end": 1734,
                    "matchedPaperCorpusId": "52967399"
                },
                {
                    "start": 1734,
                    "end": 1753,
                    "matchedPaperCorpusId": "208229926"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.32763671875
        },
        {
            "corpus_id": "273025546",
            "title": "ENTP: Encoder-only Next Token Prediction",
            "text": "For any L \u2265 2 and D \u2265 1, there exists a position-free decoder D that has L-layers and embedding dimension D, such that for any encoder E, there exists some input sequence \n\nTheorem 2. For any L \u2265 2 and D \u2265 1, there exists a position-free encoder E that has L-layers and embedding dimension D, such that for any decoder D with positional embeddings satisfying p 1 \u0338 = p 2 , there exists some input sequence (x 1 , x 2 , . . . \n\nThese theorems are existential in nature. Informally, Theorem 1 says that if we consider causal model defined over the entirety of R D as its vocabulary, we can find some decoder, for which any encoder will differ from it on some input sequence. Theorem 2 makes a similar (albeit weaker statement) in the other direction; namely the existence of a causal function computable by an encoder, but not by any decoder that uses \"non-trivial\" positional embeddings (e.g. embeddings for different positions are unique). Detailed proof of both theorems are deferred to Appendix A. \n\nOf course, the setting and assumptions of the above two statements are not necessarily very realistic. For one, they focus on general class of causal models rather than only auto-regressive ones. Furthermore, the results only pertain to exact realization and say nothing about approximation. The assumption of unbounded domain is also not realistic as in practice decoders are trained and used over a finite domain of tokens, each with some fixed embeddings. And specific to Theorem 2, no claim is made about decoders that do not use positional embeddings. But despite the limitations, these theorems give an indication that the expressive power of encoder and decoder model are different -despite the almost identical description modulo the attention mask. Changing the mask on the attention scores causes significant changes to the properties of the model. Thus, in the following sections we propose an auto-regressive tasks and run experiments comparing encoders and decoders that corroborates this view.",
            "score": 0.5620288621685603,
            "section_title": "EXPRESSIVE POWER OF ENCODER-ONLY VS. DECODER-ONLY TRANSFORMERS",
            "char_start_offset": 11805,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 170
                },
                {
                    "start": 173,
                    "end": 424
                },
                {
                    "start": 427,
                    "end": 468
                },
                {
                    "start": 469,
                    "end": 672
                },
                {
                    "start": 673,
                    "end": 891
                },
                {
                    "start": 892,
                    "end": 939
                },
                {
                    "start": 940,
                    "end": 999
                },
                {
                    "start": 1002,
                    "end": 1104
                },
                {
                    "start": 1105,
                    "end": 1197
                },
                {
                    "start": 1198,
                    "end": 1293
                },
                {
                    "start": 1294,
                    "end": 1460
                },
                {
                    "start": 1461,
                    "end": 1558
                },
                {
                    "start": 1559,
                    "end": 1759
                },
                {
                    "start": 1760,
                    "end": 1860
                },
                {
                    "start": 1861,
                    "end": 2009
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.72265625
        },
        {
            "corpus_id": "270702559",
            "title": "CrisisSense-LLM: Instruction Fine-Tuned Large Language Model for Multi-label Social Media Text Classification in Disaster Informatics",
            "text": "Typical architectures for LLMs can be categorized as encoder-decoder, causal decoder, and prefix decoder (Wang and Hesslow, 2022;Zhao et al., 2023).Among them, the causal decoder architecture is the most frequently used by various LLMs, such as OPT (Susan Zhang et al., 2023), LLAMA (Touvron et al., 2023a), BLOOM (Scao et al., 2023) due to its superior zero-shot and fewshot generalization capacity (Wang and Hesslow, 2022) and the effectiveness of scaling law (Brown et al., 2020;Kaplan et al., 2020).The causal decoder architecture employs a unidirectional attention mask, ensuring that each input token can only attend to preceding tokens and itself.Both input and output tokens are processed identically through the decoder (Zhao et al., 2023).\n\nThe prefix decoder architecture, also known as the non-causal decoder, modifies the masking mechanism used in causal decoders (Zhang et al., 2020).This adjustment allows for bidirectional attention over the prefix tokens while restricting attention to unidirectional generated tokens (Dong et al., 2019).Similar to the encoder-decoder architecture, prefix decoders can bidirectionally encode the prefix sequence and autoregressively predict the output tokens sequentially (Zhao et al., 2023).The same parameters are shared during both encoding and decoding processes.Representative LLMs based on prefix decoders include GLM-130B (Zeng et al., 2023) and U-PaLM (Tay et al., 2022).\n\nThe vanilla Transformer model is constructed on the encoder-decoder architecture (Vaswani et al., 2017), comprising two stacks of Transformer blocks designated as the encoder and decoder, respectively.The encoder utilizes stacked multi-head self-attention layers to encode the input sequence into its latent representations.Meanwhile, the decoder employs cross-attention mechanisms on these representations, and autoregressive generates the target sequence.",
            "score": 0.5614064964146818,
            "section_title": "Architecture of LLMs",
            "char_start_offset": 7065,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 148
                },
                {
                    "start": 148,
                    "end": 503
                },
                {
                    "start": 503,
                    "end": 654
                },
                {
                    "start": 654,
                    "end": 749
                },
                {
                    "start": 751,
                    "end": 898
                },
                {
                    "start": 898,
                    "end": 1055
                },
                {
                    "start": 1055,
                    "end": 1243
                },
                {
                    "start": 1243,
                    "end": 1318
                },
                {
                    "start": 1318,
                    "end": 1430
                },
                {
                    "start": 1432,
                    "end": 1633
                },
                {
                    "start": 1633,
                    "end": 1756
                },
                {
                    "start": 1756,
                    "end": 1889
                }
            ],
            "ref_mentions": [
                {
                    "start": 1513,
                    "end": 1535,
                    "matchedPaperCorpusId": "13756489"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.74267578125
        },
        {
            "corpus_id": "269188354",
            "title": "Small Language Models Are Good Too: An Empirical Study of Zero-Shot Classification",
            "text": "We limit this evaluation to simple prompting methods and hand-crafted, unoptimized prompts.We also provide a single prompt for each dataset.\n\nWe focused on causal-decoder-only and encoder-decoder models without comparing them with encoder-only or non-causal decoders as recently released models focused on those architectures.\n\nWe did not mention external factors such as pretraining time, data quality, or potential biases in the datasets.These external factors might impact the results or the generalizability of the conclusions.\n\nThe choice and assumptions of the statistical tools could influence the results.There might be newer or specialized models not included in this study, which could exhibit different behaviors.",
            "score": 0.5591334588915723,
            "section_title": "Limitations",
            "char_start_offset": 16952,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 91
                },
                {
                    "start": 91,
                    "end": 140
                },
                {
                    "start": 142,
                    "end": 326
                },
                {
                    "start": 328,
                    "end": 440
                },
                {
                    "start": 440,
                    "end": 531
                },
                {
                    "start": 533,
                    "end": 613
                },
                {
                    "start": 613,
                    "end": 724
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.3525390625
        },
        {
            "corpus_id": "270560675",
            "title": "Exploring the Role of Large Language Models in Prompt Encoding for Diffusion Models",
            "text": "As outlined in Sec. 1, we observe two discrepancies between decoder-only LLMs and encoder-decoder models: optimization objective and model architecture. Specifically, the decoder-only LLMs are typically optimized using the next token prediction task while the encoder-decoder models are trained with the masked language modeling task. Besides, the former tokens in a sequence cannot attend the latter tokens in decoder-only LLMs while every token in the sequence can attend each other in the encoder models. Based on the observations, we conduct elaborate experiments to investigate how such discrepancies affect the prompt encoding capacity of LLMs.",
            "score": 0.5588926847533273,
            "section_title": "Prompt Encoding with Language Models",
            "char_start_offset": 4842,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 152
                },
                {
                    "start": 153,
                    "end": 334
                },
                {
                    "start": 335,
                    "end": 507
                },
                {
                    "start": 508,
                    "end": 650
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.7783203125
        },
        {
            "corpus_id": "248118752",
            "title": "What Language Model Architecture and Pretraining Objective Work Best for Zero-Shot Generalization?",
            "text": "We find that the MLM-adapted model performs best by a significant margin and outperforms every other model we considered on EAI-Eval. Furthermore, the measured zero-shot generalization is in line with the MLM-pretrained non-causal decoder reported in Figure 4, though it still lags behind the MLM-pretrained encoder-decoder, despite the adapted models having seen 51 billion additional tokens. Finally, we note that performing non-causal multitask finetuning of the causal model produces no meaningful change in performance. \n\nFinding 3. Decoder-only models can be efficiently adapted from one architecture/objective prior to the other. Specifically, to obtain both a generative and a multitask model with the smallest total compute budget possible, we recommend starting with a causal decoder-only model, pretraining it with a full language modeling objective, and then using non-causal masked language modeling adaptation before taking it through multitask finetuning . Figure 7: Applying non-causal MLM adaptation to a causal decoder-only FLM before multitask finetuning improves zero-shot performance, even when controlling for additional LM pretraining for the same number of tokens. Zero-shot generalization on T0-Eval (left) and EAI-Eval (right), for the T5-LM and T0 baselines (grey), and for models from our study. Converting the model into a non-causal decoder for multitask finetuning only does not improve performance on T0-Eval. Results after adaptation are in line with non-causal decoder-only pretrained with MLM in Figure 4.",
            "score": 0.5567292838615006,
            "section_title": "Language modeling adaptation (LM-A)",
            "char_start_offset": 39174,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 133
                },
                {
                    "start": 134,
                    "end": 393
                },
                {
                    "start": 394,
                    "end": 524
                },
                {
                    "start": 527,
                    "end": 636
                },
                {
                    "start": 637,
                    "end": 971
                },
                {
                    "start": 972,
                    "end": 1188
                },
                {
                    "start": 1189,
                    "end": 1323
                },
                {
                    "start": 1324,
                    "end": 1441
                },
                {
                    "start": 1442,
                    "end": 1540
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.689453125
        },
        {
            "corpus_id": "268723594",
            "title": "RankMamba: Benchmarking Mamba's Document Ranking Performance in the Era of Transformers",
            "text": "We benchmark the performance of different models on three sizes: models with \u2248 110M parameters (e.g.bert-base-uncased); models with \u2248 330M parameters (e.g.bert-large-uncased) and models with > 700M parameters (e.g.T5-large and state-spaces/mamba-790m).\n\nFor encoder-only models, we choose BERT [Dev+19] and RoBERTa [Liu+19]; for decoder-only models, we include Pythia [Bid+23], OPT [Zha+22] and Mamba [GD23], and we opt for T5 [Raf+20] as the representative encoder-decoder models.The compared models are from different model families, varying in terms of pretraining objectives (e.g.causal language modeling, masked language modeling, denoising, next sentence prediction), information direction (bidirectional vs unidirectional), model structure (Attention vs Mamba) and Positional Encoding (no positional encoding, learned positional encoding, relative positional encoding and rotary positional encoding [Su+24]).An important note is that we do not include instruction-finetuned language models such as Flan-T5 models [Chu+22] for a fair comparison to other language model families.For a review of these models, refer to Table 1.",
            "score": 0.5557655231990845,
            "section_title": "Backbone Language Models",
            "char_start_offset": 10474,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 100
                },
                {
                    "start": 100,
                    "end": 155
                },
                {
                    "start": 155,
                    "end": 214
                },
                {
                    "start": 214,
                    "end": 252
                },
                {
                    "start": 254,
                    "end": 481
                },
                {
                    "start": 481,
                    "end": 584
                },
                {
                    "start": 584,
                    "end": 915
                },
                {
                    "start": 915,
                    "end": 1084
                },
                {
                    "start": 1084,
                    "end": 1131
                }
            ],
            "ref_mentions": [
                {
                    "start": 368,
                    "end": 376,
                    "matchedPaperCorpusId": "257921893"
                },
                {
                    "start": 427,
                    "end": 435,
                    "matchedPaperCorpusId": "204838007"
                },
                {
                    "start": 906,
                    "end": 913,
                    "matchedPaperCorpusId": "233307138"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.1375732421875
        },
        {
            "corpus_id": "235732358",
            "title": "Dual Causal/Non-Causal Self-Attention for Streaming End-to-End Speech Recognition",
            "text": "Finally, at the output of the encoder, the non-causal encoder sequence X nc E is forwarded to the CTC and decoder branches, while the causal sequence X c E is dropped. In order to enforce a consistency between the causal and non-causal sequences, we explored inplace knowledge distillation (KD) [27,28] between X c E (student) and X nc E (teacher) using the mean squared error (MSE) loss. In our experiments, the MSE loss is multiplied with a weight of 1.0 and added to Eq. ( 12), unless otherwise noted in the results section below. All model parameters are shared for processing the causal and non-causal frames in the encoder, except parameters of the normalization layers.",
            "score": 0.5556801820194495,
            "section_title": "Dual Causal/Non-Causal Self-Attention",
            "char_start_offset": 9440,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 167
                },
                {
                    "start": 168,
                    "end": 388
                },
                {
                    "start": 389,
                    "end": 533
                },
                {
                    "start": 534,
                    "end": 676
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.5029296875
        },
        {
            "corpus_id": "266755678",
            "title": "Understanding LLMs: A Comprehensive Overview from Training to Inference",
            "text": "LLMs with a Decoder-only architecture utilize the decoder component of the traditional Transformer architecture. Unlike the Encoder-Decoder architecture, which incorporates both an encoder and a decoder, the Decoder-only architecture is solely focused on the decoding process. In this configuration, the model sequentially generates tokens, attending to preceding tokens in the sequence. This architecture has been applied to various language generation tasks, showcasing its effectiveness in various tasks such as text generation without the need for an explicit encoding phase. The Decoder-only architecture can be further classified into two categories: the Causal Decoder architecture and the Prefix Decoder architecture. \n\nThe Causal Decoder Architecture: In the Causal Decoder architecture, each token in the model input sequence can only attend to past input tokens and itself during the decoding process. It achieves unidirectional attention to the input sequence by using a specific mask as shown in Figure 1. In fact, different architectures are mainly implemented by configuring different mask matrices. The figure illustrates a comparison of mask configurations between the Encoderdecoder and Decoder-only architectures (including Casual Decoder and Prefix Decoder). The representative LLMs for the Causal Decoder architecture are the GPT series [18; 7; 8; 93; 19]. The GPT series of LLMs are currently known for their superior performance, with their foundational Causal Decoder architecture widely applied in other LLMs such as BLOOM [38], OPT [83], Gopher [84], and LLaMA [9]. \n\nThe Prefix Decoder Architecture: The Prefix Decoder architecture combines the advantages of both the Encoderdecoder and Causal Decoder architectures. It leverages its unique mask configurations, as illustrated in Figure 1, enabling bidirectional attention for tokens in the prefix while maintaining unidirectional attention for generating subsequent tokens [54]. This design allows for the autoregressive generation of the output sequence with the flexibility to attend bi-directionally to the prefix tokens. Representative LLMs utilizing the Prefix Decoder architecture include PaLM [36] and GLM [37].",
            "score": 0.5546775486231706,
            "section_title": "Decoder-only Architecture",
            "char_start_offset": 35165,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 112
                },
                {
                    "start": 113,
                    "end": 276
                },
                {
                    "start": 277,
                    "end": 387
                },
                {
                    "start": 388,
                    "end": 579
                },
                {
                    "start": 580,
                    "end": 725
                },
                {
                    "start": 728,
                    "end": 912
                },
                {
                    "start": 913,
                    "end": 1018
                },
                {
                    "start": 1019,
                    "end": 1114
                },
                {
                    "start": 1115,
                    "end": 1278
                },
                {
                    "start": 1279,
                    "end": 1377
                },
                {
                    "start": 1378,
                    "end": 1591
                },
                {
                    "start": 1594,
                    "end": 1743
                },
                {
                    "start": 1744,
                    "end": 1956
                },
                {
                    "start": 1957,
                    "end": 2102
                },
                {
                    "start": 2103,
                    "end": 2196
                }
            ],
            "ref_mentions": [
                {
                    "start": 1951,
                    "end": 1955,
                    "matchedPaperCorpusId": "147704286"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.8115234375
        },
        {
            "corpus_id": "267095335",
            "title": "Transformer-Based Models Are Not Yet Perfect At Learning to Emulate Structural Recursion",
            "text": "We were initially not sure whether to evaluate the tasks of interest on an encoder-only, decoder-only, or encoder-decoder transformer model. To decide, we ran preliminary experiments to evaluate these three architectures under our setup for a related toy experiment and on one of our two tasks. \n\nOur toy experiment focused on string reversal. We trained models on strings ranging from 10 to 37 and tested up to length 50. We used three different architectures: a 2-layer encoder-decoder transformer model, a 4-layer encoder-only model, and a 4-layer decoder-only model. We trained until we achieved perfect accuracy on both the training and in-domain validation datasets. We employed the same \"random-padding\" strategy as in the main experiment. The encoder-decoder model exhibited the best extrapolation performance among the three architectures (Figure 25). Both the encoder-only and decoder-only models experienced a rapid decline in performance as the extrapolation length increased. Interestingly, we also observed that the model failed to extrapolate when trained on a fixed-length \"learnable token\" setup, even though we ensured that the positional embeddings were seen during training. Moreover, the model also struggled to extrapolate when padding was not applied. \n\nThe results of the encoder-only and decoder-only transformers on the natural order binary successor task (each with four layers) are presented in Figure 26. \n\nC Sketch of Proof: Single layer transformer can solve one-step reduction of binary successor task. \n\nLet us assume, for simplicity, 4 types of tokens: S, 01, X0, X1.",
            "score": 0.5474161509391786,
            "section_title": "B Extrapolation of Different Model Architectures",
            "char_start_offset": 92019,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 140
                },
                {
                    "start": 141,
                    "end": 294
                },
                {
                    "start": 297,
                    "end": 343
                },
                {
                    "start": 344,
                    "end": 422
                },
                {
                    "start": 423,
                    "end": 570
                },
                {
                    "start": 571,
                    "end": 672
                },
                {
                    "start": 673,
                    "end": 746
                },
                {
                    "start": 747,
                    "end": 860
                },
                {
                    "start": 861,
                    "end": 988
                },
                {
                    "start": 989,
                    "end": 1194
                },
                {
                    "start": 1195,
                    "end": 1274
                },
                {
                    "start": 1277,
                    "end": 1433
                },
                {
                    "start": 1436,
                    "end": 1534
                },
                {
                    "start": 1537,
                    "end": 1601
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.235107421875
        },
        {
            "corpus_id": "248118752",
            "title": "What Language Model Architecture and Pretraining Objective Work Best for Zero-Shot Generalization?",
            "text": "We now focus on the relatively new practice of multitask finetuning, where there has not yet been any systematic study of the influence of the architecture and training objective. Notably, the two main papers advocating this practice use completely different approaches: Sanh et al. [2021] finetunes an encoder-decoder model pretrained with span corruption, whereas Wei et al. [2021] finetunes a decoder-only pretrained with full language modeling. It is not immediately clear which approach is more natural: while decoder-only models trained with full language modeling are better at zero-shot generalization (as evidenced in Section 4.1), encoder-decoder models and masked language modeling pretraining have been shown to perform significantly better after finetuning [Raffel et al., 2020]. We therefore evaluate every architecture and objective combination after multitask finetuning. \n\nOur results are outlined in Figure 4. The encoder-decoder pretrained with span corruption offers the best performance after multitask finetuning. Specifically, on EAI-Eval, the best performance is achieved by the encoder-decoder with MLM, and the non-causal decoder with MLM comes in a close second. However, the difference is more significant on T0-Eval, where the encoder-decoder with MLM pretraining outperforms other models by a large margin. Finally, encoder-decoder pretrained with PLM and causal decoder with MLM achieve significantly worse performance than other models. These results are consistent across all levels of pretraining (see early checkpoints in Appendix D). \n\nFinding 2. Encoder-decoder models trained with masked language modeling achieve the best zero-shot performance after multitask finetuning . More broadly, approaches that perform well in the single-task finetuning setting perform well on multitask finetuning. \n\nTable 3: After full or prefix language modeling pretraining, the causal decoder (FLM) exhibits the best zero-shot generalization abilities, followed closely by the non-causal decoder (PLM).",
            "score": 0.5465808938586033,
            "section_title": "After multitask finetuning",
            "char_start_offset": 29597,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 179
                },
                {
                    "start": 180,
                    "end": 448
                },
                {
                    "start": 449,
                    "end": 792
                },
                {
                    "start": 793,
                    "end": 887
                },
                {
                    "start": 890,
                    "end": 927
                },
                {
                    "start": 928,
                    "end": 1035
                },
                {
                    "start": 1036,
                    "end": 1189
                },
                {
                    "start": 1190,
                    "end": 1336
                },
                {
                    "start": 1337,
                    "end": 1468
                },
                {
                    "start": 1469,
                    "end": 1569
                },
                {
                    "start": 1572,
                    "end": 1711
                },
                {
                    "start": 1712,
                    "end": 1830
                },
                {
                    "start": 1833,
                    "end": 2022
                }
            ],
            "ref_mentions": [
                {
                    "start": 770,
                    "end": 791,
                    "matchedPaperCorpusId": "204838007"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.7509765625
        },
        {
            "corpus_id": "259287245",
            "title": "Leveraging Cross-Utterance Context For ASR Decoding",
            "text": "While typically a transformer [15] may consist of a bidirectional encoder, followed by a causal decoder, for language modelling we use the decoder-only variant transformer. This consists of alternating multi-headed self-attention with a causal mask, and feed-forward modules. \n\nGiven a word sequence w = (w1, .., wT ) causal language models are trained to estimate the conditional probability of P (wt|w<t). Word sequence probabilities can then be obtained by an expansion resulting in P (w), when working with loglikelihoods this equates to: T t=1 log P (wt|w<t). For the purpose of decoding these likelihoods can be treated as scores and combined with the AM through a log-linear interpolation.",
            "score": 0.5414537269562241,
            "section_title": "Transformer Language Modelling",
            "char_start_offset": 3407,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 172
                },
                {
                    "start": 173,
                    "end": 275
                },
                {
                    "start": 278,
                    "end": 407
                },
                {
                    "start": 408,
                    "end": 564
                },
                {
                    "start": 565,
                    "end": 696
                }
            ],
            "ref_mentions": [
                {
                    "start": 30,
                    "end": 34,
                    "matchedPaperCorpusId": "13756489"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.5244140625
        },
        {
            "corpus_id": "258236098",
            "title": "Video Pre-trained Transformer: A Multimodal Mixture of Pre-trained Experts",
            "text": "In short, combining PaLM's observations with the Updated Scaling Laws proposed in Chinchilla (Hoffmann et al., (2022)), the best practice is to use a decoder-only causal language model architecture if your data size indicates your model should have more than 100 billion parameters. For smaller datasets, elect for encoder-decoder models. Currently, I am unsure of best-practices for encoder-only architectures.",
            "score": 0.5389783085384656,
            "section_title": "LLM Architecture Best Practices",
            "char_start_offset": 26288,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 282
                },
                {
                    "start": 283,
                    "end": 338
                },
                {
                    "start": 339,
                    "end": 411
                }
            ],
            "ref_mentions": [
                {
                    "start": 93,
                    "end": 117,
                    "matchedPaperCorpusId": "247778764"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.323974609375
        },
        {
            "corpus_id": "248118752",
            "title": "What Language Model Architecture and Pretraining Objective Work Best for Zero-Shot Generalization?",
            "text": "In this paper, we systematically studied the effects of pretraining objective and architecture choices on the zero-shot generalization abilities of large language models. Specifically, we compared language modeling and masked language modeling objectives applied to causal/non-causal decoder-only and encoder-decoder architectures. We also evaluated zero-shot performance with and without multitask finetuning. Notably, we found that the best objective and architecture is the opposite in these two settings: a causal decoder-only pretrained with full language modeling performs best if evaluated immediately after pretraining, whereas when adding a multitask finetuning step, an encoder-decoder pretrained with masked language modeling performs best. We therefore evaluate the practice of adaptation, to convert models across architectures and objectives. We found a simple efficient compromise, where a causal decoder-only model pretrained with full language modeling underwent additional masked language model training as a non-causal decoder-only model, yielding significant speedup in convergence over starting from scratch. This enables practitioners to get both an excellent generative model and a model that delivers good performance after multitask finetuning. Our results provide significant new insights into the design of LLMs. In the future, we are interested in work investigating architectures and objectives that perform well regardless of whether multitask finetuning is performed. To facilitate future work, we release all models, code, and data used in our study.",
            "score": 0.537863017077425,
            "section_title": "Conclusion",
            "char_start_offset": 40729,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 170
                },
                {
                    "start": 171,
                    "end": 331
                },
                {
                    "start": 332,
                    "end": 410
                },
                {
                    "start": 411,
                    "end": 751
                },
                {
                    "start": 752,
                    "end": 856
                },
                {
                    "start": 857,
                    "end": 1129
                },
                {
                    "start": 1130,
                    "end": 1269
                },
                {
                    "start": 1270,
                    "end": 1339
                },
                {
                    "start": 1340,
                    "end": 1498
                },
                {
                    "start": 1499,
                    "end": 1582
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.83203125
        },
        {
            "corpus_id": "276408714",
            "title": "The underlying structures of self-attention: symmetry, directionality, and emergent dynamics in Transformer training",
            "text": "This difference is consistent across multiple families of models and input modalities, such as BERT [Devlin et al., 2019], GPT [Radford et al., 2018, 2019], LLAMA3 [Touvron et al., 2023], Phi [Hughes, 2023, Abdin et al., 2024], MISTRAL [Jiang et al., 2023], ModernBERT [Warner et al., 2024], and many others (see Figure S1 for vision and audio models). Strikingly, we observe that decoder-only models have higher degrees of directionality than encoderonly models (Figure 2b). Again, this difference is consistent across all the models and input modalities we consider. We show in Figure S2 that a similar pattern is observed when including full encoderdecoder Transformers (e.g. the language T5 models [Xue et al., 2021]), despite these models having an overall lower degree of directionality.",
            "score": 0.5363789541679107,
            "section_title": "Symmetric and directional structures are predominant in Transformer models",
            "char_start_offset": 14397,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 352
                },
                {
                    "start": 353,
                    "end": 475
                },
                {
                    "start": 476,
                    "end": 568
                },
                {
                    "start": 569,
                    "end": 793
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.410888671875
        },
        {
            "corpus_id": "273025546",
            "title": "ENTP: Encoder-only Next Token Prediction",
            "text": "Inspired by the different computational models of encoder-only and decoder-only Transformers, we characterize the causal sequence functions learnable by encoders and decoders based on their required computational complexity. We give an informal comparison of encoders and decoders in terms of their required time and space complexities -both over the entire sequence and for each additional token. Using the \"gap\" between the complexity of encoders and decoders, we propose the Triplet-Counting task, which is feasible for an encoder but challenging for a decoder due to its limited computation complexity.",
            "score": 0.534856975221131,
            "section_title": "TIME AND SPACE COMPLEXITY COMPARISONS",
            "char_start_offset": 13856,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 224
                },
                {
                    "start": 225,
                    "end": 397
                },
                {
                    "start": 398,
                    "end": 606
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.6064453125
        },
        {
            "corpus_id": "277066758",
            "title": "Investigating Human-Aligned Large Language Model Uncertainty",
            "text": "Current LLM architectures are based broadly on the transformer introduced by Vaswani et al. (2017). While the original transformer architecture utilized a traditional encoder-decoder structure, current models are overwhelmingly encoder-only or decoder-only models. Encoder-only models, typified by BERT (Devlin et al., 2019), are largely relegated to masked language modeling tasks where one or more tokens at any position in a given context are masked or otherwise unavailable. The model's task is to predict the most appropriate tokens to insert in those masked positions. \n\nMost well-known LLMs are decoder-only models (Roberts, 2024). These are causal models which predict, given the current context, the most appropriate next token to add to the end of the current context. They are trained and used autoregressively by iteratively predicting a new token, adding that new token to the original context, and repeating the process until either a maximum number of tokens are generated or a special stop token is predicted. \n\nWe investigate uncertainty exclusively in decoder-only language models. Specifically, we examine the LLaMa 3.1 (Grattafiori et al., 2024), LLaMa 3.2 (AI, 2024), Mistral 0.1, and Mistral 0.3 (Jiang et al., 2023) models. For each of these, we investigate both the base completion and the instruction-finetuned (instruct) versions. The instruct models are finetuned to respond in a conversational manner and follow directional instructions in addition to standard next-word prediction. Both varieties of a given model see heavy usage in practice, with base models being used for specialized backend applications and instruct models being used in many user-facing applications. \n\nAnother important distinction in LLMs is between black box and white box models. Black box models, typified by OpenAI's ChatGPT (OpenAI, 2022) and Anthropic's Claude (Anthropic, 2024) models, do not provide access to a model's internal weights. Black box models often will only provide an output sequence and may not provide even the token probability distribution for each generation step.",
            "score": 0.5339358690225345,
            "section_title": "Large Language Models",
            "char_start_offset": 2703,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 99
                },
                {
                    "start": 100,
                    "end": 264
                },
                {
                    "start": 265,
                    "end": 478
                },
                {
                    "start": 479,
                    "end": 574
                },
                {
                    "start": 577,
                    "end": 638
                },
                {
                    "start": 639,
                    "end": 778
                },
                {
                    "start": 779,
                    "end": 1025
                },
                {
                    "start": 1028,
                    "end": 1099
                },
                {
                    "start": 1100,
                    "end": 1246
                },
                {
                    "start": 1247,
                    "end": 1356
                },
                {
                    "start": 1357,
                    "end": 1510
                },
                {
                    "start": 1511,
                    "end": 1701
                },
                {
                    "start": 1704,
                    "end": 1784
                },
                {
                    "start": 1785,
                    "end": 1948
                },
                {
                    "start": 1949,
                    "end": 2094
                }
            ],
            "ref_mentions": [
                {
                    "start": 77,
                    "end": 98,
                    "matchedPaperCorpusId": "13756489"
                },
                {
                    "start": 303,
                    "end": 324,
                    "matchedPaperCorpusId": "52967399"
                },
                {
                    "start": 622,
                    "end": 637,
                    "matchedPaperCorpusId": "258947629"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.58154296875
        },
        {
            "corpus_id": "271974388",
            "title": "Legilimens: Practical and Unified Content Moderation for Large Language Model Services",
            "text": "The input and output tokens are processed in the same fashion through the decoder. As representative language models of this architecture, the GPT series models [9,55,57] are developed based on the causal decoder architecture. So far, the causal decoders have been widely adopted as the architecture of LLMs by various existing LLMs, such as LLaMA [73,74], Dolly [17,18], and Falcon [61]. \n\nThe prefix decoder architecture (a.k.a., non-causal decoder) revises the masking mechanism of causal decoders, to enable performing bidirectional attention over the prefix tokens [23] and unidirectional attention only on generated tokens. In this way, like the encoder-decoder architecture, the prefix decoders can bidirectionally encode the prefix sequence and auto-regressively predict the output tokens one by one, where the same parameters are shared during encoding and decoding. Existing representative LLMs based on prefix decoders include GLM [24,89] and U-PaLM [72]. \n\nwhere \u2295 denotes concatenating the previous output tokens to the end of the input sequence until a special sentence ending token (usually denoted as [eos]) is generated. The first decoding method is greedy search, which predicts the most likely token at each step based on the previously generated tokens. The other decoding method is sampling, which randomly samples the next token based on the probability distribution to enhance the randomness and diversity during generation. \n\nFrom the inference process and the self-attention mechanism of LLMs we know that LLMs output the first token  1 of r leveraging the information of p, and output the last token (i.e., [eos]) with the information of both p and r, i.e., \n\nwhere H (\u2022) denotes the inference function of LLMs. We utilize this inference process as a feature extractor for the downstream content moderation task, which we elaborate on in \u00a74.",
            "score": 0.5322895950458684,
            "section_title": "Basic Component.",
            "char_start_offset": 11466,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 82
                },
                {
                    "start": 83,
                    "end": 226
                },
                {
                    "start": 227,
                    "end": 388
                },
                {
                    "start": 391,
                    "end": 629
                },
                {
                    "start": 630,
                    "end": 875
                },
                {
                    "start": 876,
                    "end": 966
                },
                {
                    "start": 969,
                    "end": 1137
                },
                {
                    "start": 1138,
                    "end": 1273
                },
                {
                    "start": 1274,
                    "end": 1447
                },
                {
                    "start": 1450,
                    "end": 1683
                },
                {
                    "start": 1686,
                    "end": 1737
                },
                {
                    "start": 1738,
                    "end": 1867
                }
            ],
            "ref_mentions": [
                {
                    "start": 161,
                    "end": 164,
                    "matchedPaperCorpusId": "218971783"
                },
                {
                    "start": 167,
                    "end": 170,
                    "matchedPaperCorpusId": "246426909"
                },
                {
                    "start": 570,
                    "end": 574,
                    "matchedPaperCorpusId": "147704286"
                },
                {
                    "start": 942,
                    "end": 946,
                    "matchedPaperCorpusId": "247519241"
                },
                {
                    "start": 961,
                    "end": 965,
                    "matchedPaperCorpusId": "253018395"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.35546875
        },
        {
            "corpus_id": "273025546",
            "title": "ENTP: Encoder-only Next Token Prediction",
            "text": "Traditionally, auto-regressive language modeling has relied on decoder-only Transformers (Vaswani et al., 2017) with causal attention, trained using the next-token prediction objective. Causal attention ensures that each token can only attend to previous tokens, preventing future tokens from influencing past outputs. This mechanism makes training and inference more efficient, as past keys and values do not need to be recomputed for each token. This efficiency enables the scaling of decoder-only Transformers, such as GPT-4 (Achiam et al., 2023) and Llama-3 (Dubey et al., 2024), up to billions of parameters using current hardware. \n\nHowever, causal attention also introduces artificial constraints. Given tokens x 1 , x 2 , ..., x n , the contextual embedding of x j (where j < n) can only attend to embeddings of earlier tokens, even when predicting x n+1 . While this constraint ensures a strict causal structure, it may not always be necessary or beneficial. In principle, there is no inherent reason that language models should be limited by this restriction. Encoder-only Transformers, which are typically used for tasks like classification, do not impose this causality constraint. Though traditionally not used for auto-regressive tasks, encoder-only architectures can be adapted for next-token prediction. When computing the output at the current time step, an encoder-only Transformer, or any sequence model, can be made causal by only providing inputs Decoders use a causal attention, ensuring that each token attends only to the preceding tokens. In contrast, encoders allow all tokens to attend to each other by performing attention computation from scratch for each token prediction. \n\nup to and including the current time step. Therefore, in this work, we investigate the idea of using encoder-only Transformers for next-token prediction. We summarize our findings below. \n\nFunctions expressible with Decoder-only and Encoder-only Transformers. We demonstrate that the sets of functions expressible by decoder-only and encoder-only Transformers are not comparable, which goes against intuition that the expressivity of encoders would subsume that of decoders.",
            "score": 0.5294747968085247,
            "section_title": "INTRODUCTION",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 185
                },
                {
                    "start": 186,
                    "end": 318
                },
                {
                    "start": 319,
                    "end": 447
                },
                {
                    "start": 448,
                    "end": 636
                },
                {
                    "start": 639,
                    "end": 704
                },
                {
                    "start": 705,
                    "end": 864
                },
                {
                    "start": 865,
                    "end": 967
                },
                {
                    "start": 968,
                    "end": 1069
                },
                {
                    "start": 1070,
                    "end": 1193
                },
                {
                    "start": 1194,
                    "end": 1319
                },
                {
                    "start": 1320,
                    "end": 1563
                },
                {
                    "start": 1564,
                    "end": 1702
                },
                {
                    "start": 1705,
                    "end": 1747
                },
                {
                    "start": 1748,
                    "end": 1858
                },
                {
                    "start": 1859,
                    "end": 1891
                },
                {
                    "start": 1894,
                    "end": 1964
                },
                {
                    "start": 1965,
                    "end": 2179
                }
            ],
            "ref_mentions": [
                {
                    "start": 89,
                    "end": 111,
                    "matchedPaperCorpusId": "13756489"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.9013671875
        },
        {
            "corpus_id": "250113742",
            "title": "Improving Deliberation by Text-Only and Semi-Supervised Training",
            "text": "Compared to cascaded encoder (B0), our WER improvement is up to 15%. For individual techniques, we see that JATD and semi-supervised training improves LT significantly by around 10% relative. BERT pretraining improves VS significantly (6% relative), and the lack of LT improvement is probably because JATD already does well in long-tail. \n\nIn Table 5, we also compare to a LM rescoring model (B2) similar to [1], which consists of 12 conformer layers. The LM has a model dimension of 384 and 3072-D feedforward layers, 4-headed self attention, and a left context of 31 tokens. Overall, the LM rescorer has 71M parameters. The LM is trained using the same text-only data used to train the BERT text encoder. During inference, the conformer LM is used to rescore the lattice after the non-causal cascaded encoders. Compared to LM rescoring in Table 5, deliberation with JATD (E1) performs similarly to LM rescoring (B2) in long-tail words, and 6% and 12% relatively better for VS and SxS test sets, respectively. Note that without BERT encoder the deliberation rescorer size of E1 is 57M, which is 20% smaller than LM (71M). When incorporating BERT and semi-supervised training, we achieve more significant and uniform improvements: VS (8.9%), longtail (1.8%), and SxS test set (15.6%), all in relative WER reductions. We further compare the proposed deliberation model to a stateof-the-art LM rescorer [42] in a decoding setup with endpointing. The baseline cascaded encoders in [42] consist of a small causal encoder and large non-causal encoder. The causal encoder consists of a 7-layer conformer and the non-causal encoder has a 10-layer right-context conformer with an overall right-context of 0.9s. The encoder output dimension is projected to 384 to reduce model size.",
            "score": 0.5281279573861105,
            "section_title": "JATD",
            "char_start_offset": 18388,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 68
                },
                {
                    "start": 69,
                    "end": 191
                },
                {
                    "start": 192,
                    "end": 337
                },
                {
                    "start": 340,
                    "end": 451
                },
                {
                    "start": 452,
                    "end": 576
                },
                {
                    "start": 577,
                    "end": 621
                },
                {
                    "start": 622,
                    "end": 706
                },
                {
                    "start": 707,
                    "end": 812
                },
                {
                    "start": 813,
                    "end": 1010
                },
                {
                    "start": 1011,
                    "end": 1122
                },
                {
                    "start": 1123,
                    "end": 1316
                },
                {
                    "start": 1317,
                    "end": 1443
                },
                {
                    "start": 1444,
                    "end": 1546
                },
                {
                    "start": 1547,
                    "end": 1702
                },
                {
                    "start": 1703,
                    "end": 1773
                }
            ],
            "ref_mentions": [
                {
                    "start": 1401,
                    "end": 1405,
                    "matchedPaperCorpusId": "249437304"
                },
                {
                    "start": 1478,
                    "end": 1482,
                    "matchedPaperCorpusId": "249437304"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.252197265625
        },
        {
            "corpus_id": "249626024",
            "title": "Language Models are General-Purpose Interfaces",
            "text": "As shown in Figure 3, we summarize the model architectures of three language model variants and the proposed semi-causal language model. First, causal language model (such as GPT; Brown et al. 2020) is a left-to-right Transformer decoder. Second, prefix language model uses the encoder-decoder architecture with cross-attention connections to complete the sequence. Third, non-causal language model is a bidirectional encoder, which is usually pretrained by masked language modeling (Devlin et al., 2019). Forth, the proposed semi-causal language model has a unidirectional Transformer decoder, and multiple bidirectional encoders that dock with the decoder. In other words, our model processes the whole session from left to right, while having some spans pre-encoded by non-causal encoders. \n\nBackbone Network We use Transformer (Vaswani et al., 2017) to build the models. Given an input sequence, we first pack their vector representations together. Then we feed the vectors into a multilayer Transformer, which encodes the input to contextualized representations. In each Transformer block, there is a multi-head self-attention layer and a feed-forward network layer that are used to aggregate the hidden states of the previous layer. Moreover, attention masks are used to control the context access. We use a triangular matrix as the attention mask for the universal task layer, so that it processes the input from left to right. For the bidirectional encoder, we allow all the tokens to access each other. After obtaining the output vectors of the universal task layer, we use a softmax classifier to predict over the vocabulary. The weight matrix is shared with the input token embeddings. \n\nConnector As shown in Figure 2, there is a connector layer between the universal task layer and various bidirectional encoders. The connectors project vector representations of bidirectional encoders before feeding them into the general-purpose interface. Moreover, the connectors are used to match the output dimensions of foundation models with the universal task layer. We empirically find that both linear projection and feed-forward network work well in our experiments.",
            "score": 0.5273576755839207,
            "section_title": "Model Architecture",
            "char_start_offset": 7682,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 136
                },
                {
                    "start": 137,
                    "end": 238
                },
                {
                    "start": 239,
                    "end": 365
                },
                {
                    "start": 366,
                    "end": 505
                },
                {
                    "start": 506,
                    "end": 658
                },
                {
                    "start": 659,
                    "end": 792
                },
                {
                    "start": 795,
                    "end": 874
                },
                {
                    "start": 875,
                    "end": 952
                },
                {
                    "start": 953,
                    "end": 1067
                },
                {
                    "start": 1068,
                    "end": 1238
                },
                {
                    "start": 1239,
                    "end": 1304
                },
                {
                    "start": 1305,
                    "end": 1434
                },
                {
                    "start": 1435,
                    "end": 1511
                },
                {
                    "start": 1512,
                    "end": 1635
                },
                {
                    "start": 1636,
                    "end": 1696
                },
                {
                    "start": 1699,
                    "end": 1826
                },
                {
                    "start": 1827,
                    "end": 1954
                },
                {
                    "start": 1955,
                    "end": 2071
                },
                {
                    "start": 2072,
                    "end": 2174
                }
            ],
            "ref_mentions": [
                {
                    "start": 180,
                    "end": 197,
                    "matchedPaperCorpusId": "218971783"
                },
                {
                    "start": 483,
                    "end": 504,
                    "matchedPaperCorpusId": "52967399"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.50439453125
        },
        {
            "corpus_id": "272910778",
            "title": "How Transliterations Improve Crosslingual Alignment",
            "text": "Models pretrained on a wide range of languages using self-supervised objectives, such as masked language modeling (MLM) (Devlin et al., 2019) or causal language modeling (Radford et al., 2019), are referred to as mPLMs. With respect to their use of the Transformer (Vaswani et al., 2017) architecture, these models can be categorized into encoder-only (Devlin et al., 2019;Conneau et al., 2020;Liang et al., 2023), encoder-decoder (Liu et al., 2020;Fan et al., 2021;Xue et al., 2021), and decoder-only models (Lin et al., 2022;Shliazhko et al., 2022;Scao et al., 2022). With the recent scale-up in both model and data size, decoderonly models, also known as large language models (LLMs) (Achiam et al., 2023;Touvron et al., 2023), can achieve impressive performance in various generation tasks across high-and medium-resource languages (Zhao et al., 2024a;\u00dcst\u00fcn et al., 2024;Zhao et al., 2024b). Parallel efforts have produced encoder-only models with very large language coverage, improving the situation for many lowresource or under-represented languages (Ogueji et al., 2021;Alabi et al., 2022;ImaniGooghari et al., 2023;Wang et al., 2023;Liu et al., 2024a). These encoder-only models excel in multiple tasks in the zero-shot crosslingual transfer manner (Huang et al., 2019;Artetxe and Schwenk, 2019;Hu et al., 2020;Zhang et al., 2024).",
            "score": 0.526831416602483,
            "section_title": "Multilingual Language Models",
            "char_start_offset": 4930,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 219
                },
                {
                    "start": 220,
                    "end": 569
                },
                {
                    "start": 570,
                    "end": 895
                },
                {
                    "start": 896,
                    "end": 1162
                },
                {
                    "start": 1163,
                    "end": 1341
                }
            ],
            "ref_mentions": [
                {
                    "start": 120,
                    "end": 141,
                    "matchedPaperCorpusId": "52967399"
                },
                {
                    "start": 170,
                    "end": 192,
                    "matchedPaperCorpusId": "160025533"
                },
                {
                    "start": 265,
                    "end": 287,
                    "matchedPaperCorpusId": "13756489"
                },
                {
                    "start": 352,
                    "end": 373,
                    "matchedPaperCorpusId": "52967399"
                },
                {
                    "start": 373,
                    "end": 394,
                    "matchedPaperCorpusId": "207880568"
                },
                {
                    "start": 394,
                    "end": 413,
                    "matchedPaperCorpusId": "256231072"
                },
                {
                    "start": 449,
                    "end": 466,
                    "matchedPaperCorpusId": "224814118"
                },
                {
                    "start": 1058,
                    "end": 1079,
                    "matchedPaperCorpusId": "240225648"
                },
                {
                    "start": 1079,
                    "end": 1098,
                    "matchedPaperCorpusId": "252088953"
                },
                {
                    "start": 1098,
                    "end": 1125,
                    "matchedPaperCorpusId": "258832427"
                },
                {
                    "start": 1125,
                    "end": 1143,
                    "matchedPaperCorpusId": "258426562"
                },
                {
                    "start": 1143,
                    "end": 1161,
                    "matchedPaperCorpusId": "265213028"
                },
                {
                    "start": 1259,
                    "end": 1279,
                    "matchedPaperCorpusId": "202541545"
                },
                {
                    "start": 1279,
                    "end": 1305,
                    "matchedPaperCorpusId": "56895585"
                },
                {
                    "start": 1305,
                    "end": 1321,
                    "matchedPaperCorpusId": "214641214"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.38623046875
        },
        {
            "corpus_id": "277857043",
            "title": "Integrating Structural and Semantic Signals in Text-Attributed Graphs with BiGTex",
            "text": "Continuing from the architectural foundations of large language models, it is important to distinguish between two major classes of transformer-based designs: encoder-only and decoder-only models. Each follows a unique training paradigm and serves different purposes in natural language understanding or generation tasks. \n\nEncoder-only models, such as BERT [18], are designed to generate holistic representations of input sequences. These models take an entire sequence as input and process it bidirectionally-each token has access to the full left and right context during training. This characteristic allows the model to deeply capture semantic dependencies across the input. \n\nThe standard training objective for such models is Masked Language Modelling (MLM), where a random subset of tokens is replaced with a special [MASK] token. The model is then trained to reconstruct the masked tokens using the surrounding unmasked tokens. This approach encourages the network to develop contextualized embeddings grounded in full-sequence comprehension. The loss function for MLM is typically expressed as: \n\nwhere \\M denotes the set of masked positions, x \\M is the unmasked sequence, and \u03b8 represents model parameters. \n\nIn contrast, decoder-only models, such as GPT [19], operate unidirectionally. They are trained in an autoregressive fashion, where each token is generated based only on preceding tokens. The model has no access to future inputs during training or inference, enforcing a strict left-to-right dependency. This makes them particularly well-suited for generative tasks such as open-ended text generation or dialogue modelling. \n\nThe pretraining objective for decoder-only models is Causal Language Modelling (CLM), where the model learns to predict the next token in a sequence, given all previous ones. The associated loss is defined as: \n\nHere, each token x t is conditioned solely on the sequence x <t , making the model capable of generating coherent text step by step. \n\nThe distinction between encoder-only and decoder-only models is foundational: the former are optimized for understanding input context and are widely used in classification and embedding tasks, while the latter are tailored for sequential prediction and text generation.",
            "score": 0.5254484805332319,
            "section_title": "Large language Models (LLMs) and Attention Mechanism",
            "char_start_offset": 11813,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 196
                },
                {
                    "start": 197,
                    "end": 321
                },
                {
                    "start": 324,
                    "end": 433
                },
                {
                    "start": 434,
                    "end": 584
                },
                {
                    "start": 585,
                    "end": 679
                },
                {
                    "start": 682,
                    "end": 838
                },
                {
                    "start": 839,
                    "end": 936
                },
                {
                    "start": 937,
                    "end": 1051
                },
                {
                    "start": 1052,
                    "end": 1104
                },
                {
                    "start": 1107,
                    "end": 1218
                },
                {
                    "start": 1221,
                    "end": 1298
                },
                {
                    "start": 1299,
                    "end": 1407
                },
                {
                    "start": 1408,
                    "end": 1523
                },
                {
                    "start": 1524,
                    "end": 1643
                },
                {
                    "start": 1646,
                    "end": 1820
                },
                {
                    "start": 1821,
                    "end": 1855
                },
                {
                    "start": 1858,
                    "end": 1990
                },
                {
                    "start": 1993,
                    "end": 2263
                }
            ],
            "ref_mentions": [
                {
                    "start": 1267,
                    "end": 1271,
                    "matchedPaperCorpusId": "218971783"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.810546875
        },
        {
            "corpus_id": "270832367",
            "title": "Predictability and Causality in Spanish and English Natural Language Generation",
            "text": "These generative models are exclusively causal, that is, they produce text from left to right by recursively feeding the model with previously generated sequences. As in the case of Recurrent Neural Networks (RNN), decoder-only transformers are expectation-based word predictors. These systems tend to favor structures in which related elements are close along the sequence, such as relative clause attachments to syntactically lower nominals in ambiguous contexts, which fits nicely into English syntax [17]. \n\nHowever, the mutually beneficial congruence between causal language modeling and English may not apply to other languages. Not only does Spanish prefer a higher nominal attachment in the resolution of ambiguous relative clauses, but its syntax is also highly flexible, even within declarative sentences [18]. This is strongly opposed to the more strict subject-verb-object structure of the English language, which allows for few inversion exceptions [19]. \n\nUnlike causal language models, encoder-only non-causal language models generate word embeddings using bidirectional contexts, which means that the model output can be conditioned by both left and right tokens. This eliminates the output sequence's sequential dependencies and allows alternative generation orders. \n\nIn light of this, we depart from the hypothesis that decoderonly (causal) transformer language models may introduce generation bias in target languages with less rigid word ordering than English, subject omission, or different attachment preferences for relative clauses, so that for these target languages other language generation strategies may be more desirable. To put this hypothesis to test, in addition to English, we consider and Spanish, a language with a different grammatical structure and also a broad base of speakers (these languages sum over 1.5 billion and 0.5 billion speakers, respectively, a substantial share of the world's population). However, the approaches in this work can be extended to obtain insights on other languages and NLP tasks. \n\nOur contributions are: A) First, we present a novel information-theoretic approach to study language predictability. We compare the causal context-conditioned entropy and the noncausal context-conditioned entropy of the grammatical category distribution of source natural texts to assess whether their language is more predictable from causal or non-causal language contexts.",
            "score": 0.5248599789957022,
            "section_title": "I. INTRODUCTION",
            "char_start_offset": 1949,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 163
                },
                {
                    "start": 164,
                    "end": 279
                },
                {
                    "start": 280,
                    "end": 509
                },
                {
                    "start": 512,
                    "end": 634
                },
                {
                    "start": 635,
                    "end": 820
                },
                {
                    "start": 821,
                    "end": 967
                },
                {
                    "start": 970,
                    "end": 1179
                },
                {
                    "start": 1180,
                    "end": 1283
                },
                {
                    "start": 1286,
                    "end": 1652
                },
                {
                    "start": 1653,
                    "end": 1943
                },
                {
                    "start": 1944,
                    "end": 2049
                },
                {
                    "start": 2052,
                    "end": 2168
                },
                {
                    "start": 2169,
                    "end": 2427
                }
            ],
            "ref_mentions": [
                {
                    "start": 504,
                    "end": 508,
                    "matchedPaperCorpusId": "218470598"
                },
                {
                    "start": 815,
                    "end": 819,
                    "matchedPaperCorpusId": "146854174"
                },
                {
                    "start": 962,
                    "end": 966,
                    "matchedPaperCorpusId": "235555573"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.476806640625
        },
        {
            "corpus_id": "276423946",
            "title": "Optimal word order for non-causal text generation with Large Language Models: The Spanish case",
            "text": "Transformers are unsupervised learners thanks to their selfattention mechanism [48], which controls the impact of the context on the model's output. The original transformer architecture is composed of an encoder and a decoder. While the encoder's attention is bidirectional, the decoder has a masked multi-head attention block that masks non-causal context and a bidirectional multi-head attention block that receives noncausal information from the encoder. \n\nAlthough the encoder-decoder architecture is widely used in some nlp applications like machine translation [21,29], other transformer-based models only use one of these two components. By excluding the encoder, we eliminate all non-causal contextual dependencies, thus using only the masked attention of the decoder. Currently, decoder-only transformers are the most effective task-agnostic nlg systems. \n\nWhile open domain nlg is mainly causal, there are a few noncausal nlg solutions. Most non-causal nlg systems are focused on particular tasks such as speech recognition [5,50], style transfer and grammar correction [20], textual data augmentation [33], and dialog systems [56,51]. \n\nNon-causal language models can also be trained for masked Language Modeling (mlm) [57]. mlm is an nlg task consisting of predicting masked words within a sentence. Some generative systems use bidirectional transformers trained on this task to recursively generate and fill masked tokens [38]. As these can be filled in any location within the text, these models can produce text in a non-causal way. \n\nNon-causal nlg strategies perform much worse in English than their causal counterparts [49]. However, to our knowledge, no prior research has been conducted on non-causal nlg in languages other than English. This work aims to evaluate whether bidirectional transformers trained on the mlm task could be successfully exploited in Spanish nlg.",
            "score": 0.5235644749640453,
            "section_title": "Causality in generative transformer language models",
            "char_start_offset": 6569,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 148
                },
                {
                    "start": 149,
                    "end": 227
                },
                {
                    "start": 228,
                    "end": 458
                },
                {
                    "start": 461,
                    "end": 645
                },
                {
                    "start": 646,
                    "end": 777
                },
                {
                    "start": 778,
                    "end": 864
                },
                {
                    "start": 867,
                    "end": 947
                },
                {
                    "start": 948,
                    "end": 1146
                },
                {
                    "start": 1149,
                    "end": 1236
                },
                {
                    "start": 1237,
                    "end": 1312
                },
                {
                    "start": 1313,
                    "end": 1441
                },
                {
                    "start": 1442,
                    "end": 1548
                },
                {
                    "start": 1551,
                    "end": 1643
                },
                {
                    "start": 1644,
                    "end": 1758
                },
                {
                    "start": 1759,
                    "end": 1892
                }
            ],
            "ref_mentions": [
                {
                    "start": 79,
                    "end": 83,
                    "matchedPaperCorpusId": "13756489"
                },
                {
                    "start": 568,
                    "end": 572,
                    "matchedPaperCorpusId": "231645376"
                },
                {
                    "start": 572,
                    "end": 575,
                    "matchedPaperCorpusId": "234785837"
                },
                {
                    "start": 1035,
                    "end": 1038,
                    "matchedPaperCorpusId": "231715684"
                },
                {
                    "start": 1038,
                    "end": 1041,
                    "matchedPaperCorpusId": "247126308"
                },
                {
                    "start": 1081,
                    "end": 1085,
                    "matchedPaperCorpusId": "218487230"
                },
                {
                    "start": 1113,
                    "end": 1117,
                    "matchedPaperCorpusId": "208224776"
                },
                {
                    "start": 1138,
                    "end": 1142,
                    "matchedPaperCorpusId": "212657570"
                },
                {
                    "start": 1142,
                    "end": 1145,
                    "matchedPaperCorpusId": "267201220"
                },
                {
                    "start": 1231,
                    "end": 1235,
                    "matchedPaperCorpusId": "265629619"
                },
                {
                    "start": 1436,
                    "end": 1440,
                    "matchedPaperCorpusId": "211069634"
                },
                {
                    "start": 1638,
                    "end": 1642,
                    "matchedPaperCorpusId": "60441316"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.71875
        },
        {
            "corpus_id": "267402678",
            "title": "The evolution, applications, and future prospects of large language models: An in-depth overview",
            "text": "As shown in Figure 3, similar to most seq2seq models, the Transformer architecture consists of an encoder and a decoder. Overall, LLM (Language Model) models can be categorized into three major types: Encoder-decoder Architecture, Causal Decoder Architecture, and Prefix Decoder Architecture [11]. The Encoder-decoder Architecture uses the most basic structure and was initially introduced by the Seq2Seq model to address sequence-to-sequence tasks, such as machine translation. It consists of an encoder and a decoder. The encoder is responsible for transforming the input sequence into a fixeddimensional semantic representation, while the decoder uses this semantic representation to generate the output sequence. Within the encoder-decoder structure, self-attention mechanisms are commonly employed for sequence modeling, enabling the model to handle variable-length input and output sequences. This architecture has proven to be highly effective in various sequence-to-sequence tasks, such as text translation, text summarization, and dialogue generation. Prominent examples of large language models following this architecture include ELMo, BERT, RoBERTa, among others [12]. \n\nCurrently, the most widely used architecture is the Causal Decoder, which is primarily employed for handling autoregressive generation tasks, where each element of the output sequence depends on previously generated elements. The Causal Decoder Architecture is an improvement over the Encoderdecoder structure, as it introduces an autoregressive mechanism in the decoder. This means that when generating the current element, the model only uses the elements generated before it. This ensures that the model does not have access to future information during the generation process, thereby preserving causality. The GPT series (e.g., GPT-3) is a typical example of models that use the Causal Decoder Architecture. These models generate text by sequentially producing words one by one, avoiding information leakage and enabling the generation of coherent and plausible text. \n\nCompared to the Encoder-decoder models, Decoder-only models offer several advantages due to their simpler structure, faster training and inference speed, suitability for pure generation tasks, and advantages in decoder self-supervision.",
            "score": 0.5231945909442883,
            "section_title": "Different structures for combining encoders and decoders.",
            "char_start_offset": 10124,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 120
                },
                {
                    "start": 121,
                    "end": 297
                },
                {
                    "start": 298,
                    "end": 478
                },
                {
                    "start": 479,
                    "end": 519
                },
                {
                    "start": 520,
                    "end": 716
                },
                {
                    "start": 717,
                    "end": 898
                },
                {
                    "start": 899,
                    "end": 1060
                },
                {
                    "start": 1061,
                    "end": 1180
                },
                {
                    "start": 1183,
                    "end": 1408
                },
                {
                    "start": 1409,
                    "end": 1554
                },
                {
                    "start": 1555,
                    "end": 1661
                },
                {
                    "start": 1662,
                    "end": 1793
                },
                {
                    "start": 1794,
                    "end": 1895
                },
                {
                    "start": 1896,
                    "end": 2055
                },
                {
                    "start": 2058,
                    "end": 2294
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.78076171875
        },
        {
            "corpus_id": "270514433",
            "title": "EDC: Effective and Efficient Dialog Comprehension For Dialog State Tracking",
            "text": "Pre-trained language models built on multi-head attention mechanism have achieved state-of-theart performance on various natural language understanding and generation problems.They can be divided into three major categories: encoderonly models (Kenton and Toutanova, 2019;Liu et al., 2019), decoder-only auto-regressive models (Radford et al., 2018(Radford et al., , 2019;;Brown et al., 2020) and encoder-decoder models (Vaswani et al., 2017;Lewis et al., 2020;Raffel et al., 2020).EDC is built on BART (Lewis et al., 2020), but can be extended to use any encoder-decoder language model as its backbone.It is also inspired by the causal attention mask construction of UniLM (Dong et al., 2019).",
            "score": 0.520815048103235,
            "section_title": "Pre-trained Language Models",
            "char_start_offset": 6602,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 176
                },
                {
                    "start": 176,
                    "end": 482
                },
                {
                    "start": 482,
                    "end": 603
                },
                {
                    "start": 603,
                    "end": 694
                }
            ],
            "ref_mentions": [
                {
                    "start": 348,
                    "end": 373,
                    "matchedPaperCorpusId": "160025533"
                },
                {
                    "start": 442,
                    "end": 461,
                    "matchedPaperCorpusId": "204960716"
                },
                {
                    "start": 461,
                    "end": 481,
                    "matchedPaperCorpusId": "204838007"
                },
                {
                    "start": 503,
                    "end": 523,
                    "matchedPaperCorpusId": "204960716"
                },
                {
                    "start": 674,
                    "end": 693,
                    "matchedPaperCorpusId": "147704286"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.473876953125
        },
        {
            "corpus_id": "271218377",
            "title": "Generative AI Systems: A Systems-based Perspective on Generative AI",
            "text": "Language Models are probabilistic models of a language, e.g., a natural language or a programming language.\n\nAs such, they take processed language (e.g., n-grams, a bag of words) as input and generate new content.Large Language Models (LLMs) are language models parameterized by neural networks, e.g., Recurrent Neural Networks (RNNs), transformers, or state space models.However, in fact, LLMs are not only language models, they consist of multiple modules.Each LLM requires a tokenizer to turn text into numbers (e.g., integers), and an embedding that changes tokenized text to real-valued vectors.Sometimes, both modules are treated as one (e.g., vectorizers in scikit-learn).A popular choice for a tokenizer these days is byte pair encoding which greedily merges commonly occurring sub-strings based on their frequency Gage (1994).\n\nThe embedding module serves only a single purpose, namely, to map a token represented as a one-hot vector to a real-valued vector of size D.Then, after processing embeddings using a neural network, the output must be de-tokenized to a string again.In general, we can distinguish three types of LLMs:\n\n1. Encoders take a piece of text (string) and return an encoding, i.e., a numerical representation of the input.Encoders can have access to the whole input at any point of processing and they do not require any specific constraints.They provide outputs in a single forward run both during training and at the inference time.\n\n2. Decoders are used for generating new texts (strings).They can be seen as autoregressive models and, as such, neural networks parameterizing them must be causal.For decoders, the sampling procedure is an iterative process, which is typically slow.\n\n3. Encoder-Decoders and Encoder-Encoders are LLMs that are conditioned on additional information.Therefore, an additional encoder is used to process conditioning, and then an encoder or a decoder provides an encoding of input text or generates new text, respectively.",
            "score": 0.5200547320002866,
            "section_title": "LLMs as systems",
            "char_start_offset": 10324,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 107
                },
                {
                    "start": 109,
                    "end": 213
                },
                {
                    "start": 213,
                    "end": 372
                },
                {
                    "start": 372,
                    "end": 458
                },
                {
                    "start": 458,
                    "end": 600
                },
                {
                    "start": 600,
                    "end": 679
                },
                {
                    "start": 679,
                    "end": 835
                },
                {
                    "start": 837,
                    "end": 977
                },
                {
                    "start": 977,
                    "end": 1085
                },
                {
                    "start": 1085,
                    "end": 1136
                },
                {
                    "start": 1138,
                    "end": 1250
                },
                {
                    "start": 1250,
                    "end": 1370
                },
                {
                    "start": 1370,
                    "end": 1462
                },
                {
                    "start": 1464,
                    "end": 1520
                },
                {
                    "start": 1520,
                    "end": 1627
                },
                {
                    "start": 1627,
                    "end": 1713
                },
                {
                    "start": 1715,
                    "end": 1812
                },
                {
                    "start": 1812,
                    "end": 1982
                }
            ],
            "ref_mentions": [
                {
                    "start": 823,
                    "end": 834,
                    "matchedPaperCorpusId": "59804030"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.313720703125
        },
        {
            "corpus_id": "269921730",
            "title": "Information Leakage from Embedding in Large Language Models",
            "text": "The model is a pre-trained model on the English language using a causal language modeling (CLM) objective.We also experimented with other pre-trained models with a similar number of parameters, such as the encoder-only model and encoder-decoder model, but in general, the decoder-only architecture performed best.\n\nWe used two datasets in the training process, one for training Embed Parrot and the other to validate the performance of Embed Parrot's reduced inputs.We use the cosine similarity of H i y and H 0 x to optimize the loss function of Embed Parrot.To enhance the fluency of the outputs generated by the language models, we appended to the loss function the Perplexity (PPL) stemming from the reconstructed sentences when used as inputs to LLMs.",
            "score": 0.5193075602699913,
            "section_title": "Embed Parrot",
            "char_start_offset": 13783,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 106
                },
                {
                    "start": 106,
                    "end": 313
                },
                {
                    "start": 315,
                    "end": 466
                },
                {
                    "start": 466,
                    "end": 560
                },
                {
                    "start": 560,
                    "end": 756
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.188720703125
        },
        {
            "corpus_id": "258947846",
            "title": "B2T Connection: Serving Stability and Performance in Deep Transformers",
            "text": "In addition to encoder-decoders, we investigate the effect of our B2T connection when used in the decoder side only, i.e., a neural language model. Because recent pre-trained models, such as the GPT series, are language models trained on a large amount of training data, experimental results in this section give an insight for pre-trained models.",
            "score": 0.5191928874176369,
            "section_title": "Language Model",
            "char_start_offset": 19687,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 147
                },
                {
                    "start": 148,
                    "end": 347
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.06500244140625
        },
        {
            "corpus_id": "257505035",
            "title": "The Life Cycle of Knowledge in Big Language Models: A Survey",
            "text": "Currently, pre-trained language models usually acquire various knowledge from pure text through self-supervised learning on a large-scale text corpus. In this section, we will first introduce several widely used learning objectives (Qiu et al., 2020), and then discuss the learning mechanisms behind them. \n\nCausal Language modeling aims to autoregressively predict the next token in the input sequence, which is the most popular pre-training tasks (Radford et al., 2019b;Brown et al., 2020;Ouyang et al., 2022;Scao et al., 2022) and has demonstrated excellent effectiveness in capturing context dependency and text generation paradigms. One limitation of causal language modeling is unidirectional, which can only capture contextual information from left to right. \n\nMasked Language Modeling aims to mask some tokens in the input randomly, and then predict the masked token conditioned on the rest of sequence (Devlin et al., 2019;Liu et al., 2019c). Unlike causal language modeling, which can only obtain information in a unidirectional manner, masked language modeling can capture contextual information from both left-to-right and right-to-left directions. \n\nSeq2seq Masked Language Modeling uses an encoder-decoder architecture for pre-training, which first feeds the encoder with masked sequence, and the decoder is supposed to predict the masked tokens autoregressively (Raffel et al., 2020;Song et al., 2019). \n\nDenoising Autoencoder first corrupts the input sequence with randomly mask symbols, then feed the input into a bidirectional encoder, and the likelihood of the whole original input is calculated with an auto-regressive decoder (Lewis et al., 2020a).",
            "score": 0.5177023092536437,
            "section_title": "Learning from Text Data",
            "char_start_offset": 12176,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 150
                },
                {
                    "start": 151,
                    "end": 305
                },
                {
                    "start": 308,
                    "end": 637
                },
                {
                    "start": 638,
                    "end": 765
                },
                {
                    "start": 768,
                    "end": 951
                },
                {
                    "start": 952,
                    "end": 1160
                },
                {
                    "start": 1163,
                    "end": 1417
                },
                {
                    "start": 1420,
                    "end": 1669
                }
            ],
            "ref_mentions": [
                {
                    "start": 232,
                    "end": 250,
                    "matchedPaperCorpusId": "212747830"
                },
                {
                    "start": 449,
                    "end": 472,
                    "matchedPaperCorpusId": "160025533"
                },
                {
                    "start": 472,
                    "end": 491,
                    "matchedPaperCorpusId": "218971783"
                },
                {
                    "start": 491,
                    "end": 511,
                    "matchedPaperCorpusId": "246426909"
                },
                {
                    "start": 911,
                    "end": 932,
                    "matchedPaperCorpusId": "52967399"
                },
                {
                    "start": 932,
                    "end": 950,
                    "matchedPaperCorpusId": "198953378"
                },
                {
                    "start": 1377,
                    "end": 1398,
                    "matchedPaperCorpusId": "204838007"
                },
                {
                    "start": 1398,
                    "end": 1416,
                    "matchedPaperCorpusId": "146808476"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.262939453125
        },
        {
            "corpus_id": "221702858",
            "title": "Efficient Transformers: A Survey",
            "text": "It is important to note the differences in how the Transformer blocks are used. Transformers can primarily be used in three ways, namely: (1) encoder-only (e.g., for classification), (2) decoder-only (e.g., for language modeling), and (3) encoder-decoder (e.g., for machine translation). In encoder-decoder mode, there are usually multiple multi-headed self-attention modules, including a standard self-attention in both the encoder and the decoder, along with an encoder-decoder cross-attention that allows the decoder to utilize information from the encoder. This influences the design of the self-attention mechanism. In the encoder mode, there is no restriction or constraint that the self-attention mechanism has to be causal, i.e., dependent solely on the present and past tokens. In the encoder-decoder setting, self-attention used in the decoder (i.e. across decoding positions) must be causal since each auto-regressive decoding step can only depend on previous tokens, whereas the selfattention used in the encoder need not. Fulfilling this requirement can prove challenging for many efficient self-attention designs. \n\nThe mode of usage of a Transformer model generally depends on the target application. Given an input sequence, the sequence is typically passed through an encoder stack. At this stage, there might be too options. For multi-class classification, a linear layer with Softmax outputs typically projects the sequence representation down to the number of classes. In the case of BERT (Devlin et al., 2018), this is a [CLS] token that is appended to the start of the sequence as a prefix. Recent work has also explored the usage of Encoder-Decoder architectures for classification, such as T5 (Raffel et al., 2019). Decoder-only models are typically used for generation and are trained using a language modeling objective (of predicting the next token). Due to the nature of the loss, these models are often superior for open ended generation (Brown et al., 2020).",
            "score": 0.5169425056444182,
            "section_title": "Transformer Mode",
            "char_start_offset": 9721,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 79
                },
                {
                    "start": 80,
                    "end": 287
                },
                {
                    "start": 288,
                    "end": 560
                },
                {
                    "start": 561,
                    "end": 620
                },
                {
                    "start": 621,
                    "end": 786
                },
                {
                    "start": 787,
                    "end": 859
                },
                {
                    "start": 860,
                    "end": 1034
                },
                {
                    "start": 1035,
                    "end": 1127
                },
                {
                    "start": 1130,
                    "end": 1215
                },
                {
                    "start": 1216,
                    "end": 1299
                },
                {
                    "start": 1300,
                    "end": 1342
                },
                {
                    "start": 1343,
                    "end": 1488
                },
                {
                    "start": 1489,
                    "end": 1612
                },
                {
                    "start": 1613,
                    "end": 1739
                },
                {
                    "start": 1740,
                    "end": 1877
                },
                {
                    "start": 1878,
                    "end": 1988
                }
            ],
            "ref_mentions": [
                {
                    "start": 1509,
                    "end": 1530,
                    "matchedPaperCorpusId": "52967399"
                },
                {
                    "start": 1717,
                    "end": 1738,
                    "matchedPaperCorpusId": "204838007"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.806640625
        },
        {
            "corpus_id": "235313355",
            "title": "Luna: Linear Unified Nested Attention",
            "text": "In the formulation of causal attention, P is expected to contain no information about X. Thus, we need to formulate P based on the usage mode of the causal attention. For the encoder-decoder mode in sequence-to-sequence modeling (e.g. for machine translation), we can use packed output from the Luna encoder as P . For the decoder-only mode (e.g. for language modeling), P might be formulated as a learnable parameter of each layer.",
            "score": 0.5164941647349144,
            "section_title": "Luna Causal Attention",
            "char_start_offset": 11132,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 88
                },
                {
                    "start": 89,
                    "end": 166
                },
                {
                    "start": 167,
                    "end": 314
                },
                {
                    "start": 315,
                    "end": 432
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.275146484375
        },
        {
            "corpus_id": "3480671",
            "title": "Non-Autoregressive Neural Machine Translation",
            "text": "In order to translate non-autoregressively and parallelize the decoding process, we modify the decoder stack as follows. \n\nDecoder Inputs Before decoding starts, the NAT needs to know how long the target sentence will be in order to generate all words in parallel. More crucially, we cannot use time-shifted target outputs (during training) or previously predicted outputs (during inference) as the inputs to the first decoder layer. Omitting inputs to the first decoder layer entirely, or using only positional embeddings, resulted in very poor performance. Instead, we initialize the decoding process using copied source inputs from the encoder side. As the source and target sentences are often of different lengths, we propose two methods: \n\n\u2022 Copy source inputs uniformly: Each decoder input t is a copy of the Round(T t/T )-th encoder input. This is equivalent to \"scanning\" source inputs from left to right with a constant \"speed,\" and results in a decoding process that is deterministic given a (predicted) target length. \u2022 Copy source inputs using fertilities: A more powerful way, depicted in Fig. 2 and discussed in more detail below, is to copy each encoder input as a decoder input zero or more times, with the number of times each input is copied referred to as that input word's \"fertility.\" In this case the source inputs are scanned from left to right at a \"speed\" that varies inversely with the fertility of each input; the decoding process is now conditioned on the sequence of fertilities, while the resulting output length is determined by the sum of all fertility values. \n\nNon-causal self-attention Without the constraint of an autoregressive factorization of the output distribution, we no longer need to prevent earlier decoding steps from accessing information from later steps. Thus we can avoid the causal mask used in the self-attention module of the conventional Transformer's decoder. Instead, we mask out each query position only from attending to itself, which we found to improve decoder performance relative to unmasked self-attention.",
            "score": 0.5156804214534315,
            "section_title": "DECODER STACK",
            "char_start_offset": 7700,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 120
                },
                {
                    "start": 123,
                    "end": 264
                },
                {
                    "start": 265,
                    "end": 433
                },
                {
                    "start": 434,
                    "end": 558
                },
                {
                    "start": 559,
                    "end": 652
                },
                {
                    "start": 653,
                    "end": 743
                },
                {
                    "start": 746,
                    "end": 847
                },
                {
                    "start": 848,
                    "end": 1029
                },
                {
                    "start": 1030,
                    "end": 1306
                },
                {
                    "start": 1307,
                    "end": 1593
                },
                {
                    "start": 1596,
                    "end": 1804
                },
                {
                    "start": 1805,
                    "end": 1915
                },
                {
                    "start": 1916,
                    "end": 2070
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.57958984375
        },
        {
            "corpus_id": "256416540",
            "title": "Towards Better Few-Shot and Finetuning Performance with Forgetful Causal Language Models",
            "text": "Language model (LM) pre-training has substantially advanced the state-of-the-art across a variety of natural language processing tasks (Peters et al., 2018;Devlin et al., 2018;Brown et al., 2020;Chowdhery et al., 2022) and related fields including image generation, reasoning, and code generation (Alayrac et al., 2022;Lewkowycz et al., 2022;Saharia et al., 2022;Chen et al., 2021). Prior work on pre-training have focused on mixing different choices of architecture (e.g., encoder-only, decoder-only, or encoder-decoder) with different objective functions (e.g., masking or causal language modeling). For example, masked encoder-only models such as BERT (Devlin et al., 2018) and RoBERTa (Liu et al., 2019) excel in discriminative finetuning tasks such as classification. Similarly, masked encoder-decoder models such as BART (Lewis et al., 2019) and T5 (Roberts et al., 2019) perform well on both discriminative and generative finetuning. While masked language modeling is effective for finetuning and removes the need for task-specific architectures, its major limitation is that there is still a need for task-specific datasets and taskspecific finetuning. On the other hand, decoder-only causal language models remove such limitations. In fact, they are capable of zero-shot and few-shot adaptation without the need for finetuning, by simply prompting the model with appropriate strings to control the generated outputs, as shown in GPT3 (Brown et al., 2020) and PaLM (Chowdhery et al., 2022).",
            "score": 0.5139832117907746,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 382
                },
                {
                    "start": 383,
                    "end": 601
                },
                {
                    "start": 602,
                    "end": 772
                },
                {
                    "start": 773,
                    "end": 940
                },
                {
                    "start": 941,
                    "end": 1160
                },
                {
                    "start": 1161,
                    "end": 1240
                },
                {
                    "start": 1241,
                    "end": 1498
                }
            ],
            "ref_mentions": [
                {
                    "start": 176,
                    "end": 195,
                    "matchedPaperCorpusId": "218971783"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.5869140625
        },
        {
            "corpus_id": "277626915",
            "title": "GOLLuM: Gaussian Process Optimized LLMs - Reframing LLM Finetuning through Bayesian Optimization",
            "text": "LLMs process textual inputs by converting them into dense vector representations through a sequence of tokenization , embedding and attention-based transformations. Tokenization involves the process of splitting the input text into subword units (tokens) using a model-specific vocabulary (e.g., SentencePiece 84 , Byte-Pair Encoding 85 ). The tokens are mapped to continuous vectors via learned embedding layers and passed through multiple self-attention layers that capture contextual relationships between tokens. \n\nLLMs can follow different architectural designs: encoder-only (e.g., BERT 2 ), decoder-only (e.g., Qwen 52 ), and encoder-decoder (e.g., T5 49 ). Encoder-based models process the full input bidirectionally and are suited for classification and regression . Decoder-only models generate text autoregressively with causal masking . Encoder-decoder models combine both components and are often used for sequence-to-sequence tasks . The architecture choices impact the structure and pooling strategies used to extract unified representations from the variable-length token sequences. \n\nPooling refers to the process of aggregating a sequence of token-level representations produced by a language model into a single fixed-dimensional embedding. Encoder-based models often use the hidden state corresponding to the special [CLS] token or apply mean-pooling across token embeddings. Decoder-only models typically use the final hidden state of the last non-padding token. For encoder-decoder models, pooling is applied over the encoder-side hidden states.",
            "score": 0.5138235671466846,
            "section_title": "B.5 Large Language Models",
            "char_start_offset": 39674,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 164
                },
                {
                    "start": 165,
                    "end": 339
                },
                {
                    "start": 340,
                    "end": 516
                },
                {
                    "start": 519,
                    "end": 664
                },
                {
                    "start": 665,
                    "end": 775
                },
                {
                    "start": 776,
                    "end": 848
                },
                {
                    "start": 849,
                    "end": 947
                },
                {
                    "start": 948,
                    "end": 1098
                },
                {
                    "start": 1101,
                    "end": 1259
                },
                {
                    "start": 1260,
                    "end": 1395
                },
                {
                    "start": 1396,
                    "end": 1483
                },
                {
                    "start": 1484,
                    "end": 1567
                }
            ],
            "ref_mentions": [
                {
                    "start": 334,
                    "end": 336,
                    "matchedPaperCorpusId": "160025533"
                },
                {
                    "start": 659,
                    "end": 661,
                    "matchedPaperCorpusId": "204838007"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.703125
        },
        {
            "corpus_id": "174800180",
            "title": "One Size Does Not Fit All: Comparing NMT Representations of Different Granularities",
            "text": "Comparing encoder representations to decoder representations, it is interesting to see that in several cases the decoder side representations performed better than the encoder side ones, even though the former were trained using a uni-directional LSTM. However, since there is no difference in the general trends between the encoder-and the decoder-side representations, below we focus on the encoder-side only.",
            "score": 0.5135810051815419,
            "section_title": "Morphological Tagging",
            "char_start_offset": 18988,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 252
                },
                {
                    "start": 253,
                    "end": 411
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.26806640625
        },
        {
            "corpus_id": "253080830",
            "title": "Is Encoder-Decoder Redundant for Neural Machine Translation?",
            "text": "In this work, we question the long-standing encoder-decoder architecture for neural machine translation. Through extensive experiments in various translation directions, considering backtranslation and multilingual translation, we find that an encoder-only model can perform as good as an encoder-decoder model. We further discuss implications and subtleties of such models to motivate further research into more compact models and more general neural network interfaces. Table 9: Grid search of four source-reconstruction-related hyperparameters on de-en and en-ro. LM means to shift the source-side outputs and the auxiliary task corresponds to autoregressive language modeling, and AE means to not shift and corresponds to an autoencoding task. Our interpretations of the table are given in Sec.4.1",
            "score": 0.5126372073897046,
            "section_title": "Conclusion",
            "char_start_offset": 25704,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 104
                },
                {
                    "start": 105,
                    "end": 311
                },
                {
                    "start": 312,
                    "end": 471
                },
                {
                    "start": 472,
                    "end": 566
                },
                {
                    "start": 567,
                    "end": 747
                },
                {
                    "start": 748,
                    "end": 801
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.13330078125
        },
        {
            "corpus_id": "252846620",
            "title": "Scaling Up Deliberation For Multilingual ASR",
            "text": "We use a baseline multilingual model similar to [11] which is language agnostic. The baseline model consists of a 12-layer causal conformer encoder and a 5-layer non-causal cascaded encoder. The causal encoder includes two blocks separated by a stacking layer. The first block consists of a input projection layer and 3 conformer layers. The stacking layer concatenates two neighboring encodings in time to form a 60-ms frame rate. Then the second block starts with a 1024-dim conformer layer, and then a projection layer to reduce the model dimension back to 512 for the rest of the layers. Note that the causal conformer layers uses causal convolution and left-context attention and is thus strictly causal. Secondly, the noncausal layers are cascaded [23] to the causal encoder output. The 5 layers of non-causal conformer layers have the same dimension of 512 as the causal encoder, and a total right-context of 0.9 sec. The outputs from causal and non-causal encoders are sampled with probabilities of 0.4, and 0.6, respectively, and fed to a transducer decoder during training [11]. \n\nThe transducer decoder consists of a prediction network and a joint network [31]. We use a 2-layer 2048D LSTM as the prediction network whose output is projected to 640 dimension for efficiency. The joint network is a single feed-forward layer of 640 units. A softmax is finally used to predict 16,384 wordpieces. We generate the wordpieces using mixed transcripts pooled from all languages. In summary, the baseline multilingual transducer model has a total of 173M parameters. \n\nOur transformer-based deliberation decoder attends to noncausal encoder outputs and hypotheses from the first-pass multilingual model decoded using the non-causal encoder. For efficiency, we sample from the softmax outputs to generate a single hypothesis token for each frame in both training and inference.",
            "score": 0.5119039116181626,
            "section_title": "Model Description",
            "char_start_offset": 9827,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 80
                },
                {
                    "start": 81,
                    "end": 190
                },
                {
                    "start": 191,
                    "end": 260
                },
                {
                    "start": 261,
                    "end": 337
                },
                {
                    "start": 338,
                    "end": 431
                },
                {
                    "start": 432,
                    "end": 591
                },
                {
                    "start": 592,
                    "end": 709
                },
                {
                    "start": 710,
                    "end": 788
                },
                {
                    "start": 789,
                    "end": 924
                },
                {
                    "start": 925,
                    "end": 1088
                },
                {
                    "start": 1091,
                    "end": 1172
                },
                {
                    "start": 1173,
                    "end": 1285
                },
                {
                    "start": 1286,
                    "end": 1348
                },
                {
                    "start": 1349,
                    "end": 1404
                },
                {
                    "start": 1405,
                    "end": 1482
                },
                {
                    "start": 1483,
                    "end": 1569
                },
                {
                    "start": 1572,
                    "end": 1743
                },
                {
                    "start": 1744,
                    "end": 1879
                }
            ],
            "ref_mentions": [
                {
                    "start": 754,
                    "end": 758,
                    "matchedPaperCorpusId": "225094578"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.43359375
        },
        {
            "corpus_id": "276902726",
            "title": "Beyond Decoder-only: Large Language Models Can be Good Encoders for Machine Translation",
            "text": "It is important to note that: (i) when encoding X, we can modify its original causal attention mask M c to be fully-visible mask M f , similar to used in the encoderdecoder architecture, thus creating a variant of the CausalLM model known as the prefixLM model (Dong et al., 2019;Raffel et al., 2020); (ii) the fusion of source and target information is achieved by computing attention on concatenated representations of both, distinct from cross-attention, which we refer to in this paper as concat attention; (iii) the interaction between X and Y is Layer-Wise, rather than using only the top-layer representation of X as in the encoder-decoder architecture (TopOnly). \n\nThe overall comparison of these architectures is shown in Table 5. In practice, some modern deployment frameworks of LLMs (decoder-only models) explicitly separate encoding (prefilling) and decoding processes across distinct computational resources, making the architecture structurally resemble encoder-decoder models (Zhong et al., 2024;Patel et al., 2024). In this light, the so-called decoder-only model can be considered a variant of the encoder-decoder model, wherein the encoding function is implicitly integrated through shared parameters with the decoder. Conversely, one may view the encoder-decoder model as an extension of PrefixLM, with a more explicit division between encoding and decoding stages.",
            "score": 0.5113385067502426,
            "section_title": "Limitations",
            "char_start_offset": 34034,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 670
                },
                {
                    "start": 673,
                    "end": 739
                },
                {
                    "start": 740,
                    "end": 1032
                },
                {
                    "start": 1033,
                    "end": 1237
                },
                {
                    "start": 1238,
                    "end": 1385
                }
            ],
            "ref_mentions": [
                {
                    "start": 261,
                    "end": 280,
                    "matchedPaperCorpusId": "147704286"
                },
                {
                    "start": 280,
                    "end": 300,
                    "matchedPaperCorpusId": "204838007"
                },
                {
                    "start": 992,
                    "end": 1012,
                    "matchedPaperCorpusId": "276409195"
                },
                {
                    "start": 1012,
                    "end": 1031,
                    "matchedPaperCorpusId": "265506047"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.62109375
        },
        {
            "corpus_id": "248118752",
            "title": "What Language Model Architecture and Pretraining Objective Work Best for Zero-Shot Generalization?",
            "text": "Adaptation extends pretraining with a different objective and/or architecture. In contrast with finetuning, no new downstream data is used, only additional pretraining data. Language modeling adaptation (LM-A) takes a model pretrained with MLM and extend its training with PLM or FLM. It has been used to convert encoder-decoder models pretrained with MLM, such as T5, into better generative models. Notably, it is used as a first step before prompt tuning [Lester et al., 2021] and also to prepare the model before multitask finetuning in T0 [Sanh et al., 2021]. When we perform language modeling adaptation on a non-causal decoder-only model, we convert it into a causal decoder-only by simply switching the attention mask. Furthermore, we propose to study the opposite adaptation: starting from a causal decoder pretrained with FLM, we cast the model into a non-causal decoder (again by switching the attention mask) and we extend pretraining with MLM. We call this approach non-causal MLM adaptation (NC-A) ; to our knowledge, this is an entirely novel practice.",
            "score": 0.5109290541766855,
            "section_title": "Model adaptation",
            "char_start_offset": 16725,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 78
                },
                {
                    "start": 79,
                    "end": 173
                },
                {
                    "start": 174,
                    "end": 284
                },
                {
                    "start": 285,
                    "end": 399
                },
                {
                    "start": 400,
                    "end": 563
                },
                {
                    "start": 564,
                    "end": 725
                },
                {
                    "start": 726,
                    "end": 955
                },
                {
                    "start": 956,
                    "end": 1066
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.52294921875
        },
        {
            "corpus_id": "248118752",
            "title": "What Language Model Architecture and Pretraining Objective Work Best for Zero-Shot Generalization?",
            "text": "Large pretrained Transformer language models have been shown to exhibit zero-shot generalization, i.e. they can perform a wide variety of tasks that they were not explicitly trained on. However, the architectures and pretraining objectives used across state-of-the-art models differ significantly, and there has been limited systematic comparison of these factors. In this work, we present a large-scale evaluation of modeling choices and their impact on zero-shot generalization. In particular, we focus on text-to-text models and experiment with three model architectures (causal/non-causal decoder-only and encoder-decoder), trained with two different pretraining objectives (autoregressive and masked language modeling), and evaluated with and without multitask prompted finetuning. We train models with over 5 billion parameters for more than 170 billion tokens, thereby increasing the likelihood that our conclusions will transfer to even larger scales. Our experiments show that causal decoder-only models trained on an autoregressive language modeling objective exhibit the strongest zero-shot generalization after purely unsupervised pretraining. However, models with non-causal visibility on their input trained with a masked language modeling objective followed by multitask finetuning perform the best among our experiments. We therefore consider the adaptation of pretrained models across architectures and objectives. We find that pretrained non-causal decoder models can be adapted into performant generative causal decoder models, using autoregressive language modeling as a downstream task. Furthermore, we find that pretrained causal decoder models can be efficiently adapted into non-causal decoder models, ultimately achieving competitive performance after multitask finetuning. Code and checkpoints are available at https://github.com/bigscience-workshop/architecture-objective.",
            "score": 0.5104326896624234,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.73388671875
        },
        {
            "corpus_id": "254044610",
            "title": "The Stack: 3 TB of permissively licensed source code",
            "text": "Code LLMs A growing body of research has trained large-scale transformer models on source code. Several groups have explored decoder-only models with a causal language modeling objective (Chen et al., 2021;Austin et al., 2021;Nijkamp et al., 2022;Christopoulou et al., 2022;Izadi et al., 2022;Xu et al., 2022) and generally found that larger models are increasingly capable of synthesizing programs from natural language descriptions. A few studies have used such decoder-only models for code-infilling tasks via a causal masking mechanism (Fried et al., 2022;Bavarian et al., 2022). Researchers have also investigated encoder masked language models (Feng et al., 2020;Kanade et al., 2020) and encoder-decoder architectures with various training objectives (Li et al., 2022;Ahmad et al., 2021;Wang et al., 2021;Roziere et al., 2021).",
            "score": 0.5101013344011618,
            "section_title": "Related Work",
            "char_start_offset": 5507,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 95
                },
                {
                    "start": 96,
                    "end": 434
                },
                {
                    "start": 435,
                    "end": 583
                },
                {
                    "start": 584,
                    "end": 833
                }
            ],
            "ref_mentions": [
                {
                    "start": 274,
                    "end": 293,
                    "matchedPaperCorpusId": "246823166"
                },
                {
                    "start": 293,
                    "end": 309,
                    "matchedPaperCorpusId": "247158549"
                },
                {
                    "start": 650,
                    "end": 669,
                    "matchedPaperCorpusId": "211171605"
                },
                {
                    "start": 669,
                    "end": 689,
                    "matchedPaperCorpusId": "220425306"
                },
                {
                    "start": 774,
                    "end": 793,
                    "matchedPaperCorpusId": "232185260"
                },
                {
                    "start": 793,
                    "end": 811,
                    "matchedPaperCorpusId": "237386541"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.15673828125
        },
        {
            "corpus_id": "258461229",
            "title": "CodeGen2: Lessons for Training LLMs on Programming and Natural Languages",
            "text": "In Table 2, Prefix-LM generally achieves better results over causal decoder trained with the same objective on our SuperGLUE tasks, which partially answers our question that bi-directional attention may yield informative representations. However, our 1B parameter model could not compete with much smaller encoder-only pre-trained models such as CodeBERT on CodeXGLUE or RoBERTa-large on SuperGLUE, which leads us to conclude that the representations are not sufficiently informative to justify the substitution of smaller scale encoder-only pretrained models with one Prefix-LM. For few-shot XSum, we did not observe meaningful differences between the two models, regardless of the number of exemplars in the non-causal part.",
            "score": 0.5100821770619725,
            "section_title": "Results and Findings",
            "char_start_offset": 18882,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 237
                },
                {
                    "start": 238,
                    "end": 579
                },
                {
                    "start": 580,
                    "end": 726
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.282470703125
        },
        {
            "corpus_id": "258461229",
            "title": "CodeGen2: Lessons for Training LLMs on Programming and Natural Languages",
            "text": "Model Architecture In representation learning with transformers (Devlin et al., 2019;Lewis et al., 2020;Raffel et al., 2020), two schemes of modeling are prevalent which differ in their attention masks for the contextualization of hidden vectors. For a sequence x = (x 1 , . . . , x n ) of n vectors, we differ: (1) bi-directional encoder-based representations in which each token vector x i can attend all other tokens {x j : i = 1, . . . , n}, (2) uni-directional decoder-based representations in which each token vector x i can only attend previous tokens {x j : j \u2264 i}. While encoder-based representations for which each hidden vector can contextualize with all other vectors may be desirable for understanding tasks, decoder-based representations with temporal causal masking are required for language modeling for which the joint density is factorized as the product of conditionals over time steps. To unify both schemes, we adopt the notion of prefix-based language modeling (Prefix-LM) (Raffel et al., 2020). For a prefix, we decompose the input sequence x into a prefix p and a context c. For the prefix p = (x 1 , . . . , x m ) where m < n, each token can attend over all other tokens in the prefix, which amounts to bi-directional representations. For the context c = (x m+1 , . . . , x n ), each token can only attend to previous tokens, which amounts to uni-directional decoder representations. This unifies bi-directional attention over the prefix with the requirement of causal masking to factorize the joint density over time. The hope is to achieve competitive auto-regressive sampling for synthesis tasks, while learning strong bi-directional representations for understanding tasks. \n\nLearning Algorithm The choice of encoder or decoder-based model architectures typically guides the selection of learning algorithms for language modeling. Encoder-based models may be trained with the task of masked language modeling in the form of denoising span corruptions (Devlin et al., 2019;Raffel et al., 2020).",
            "score": 0.509731871832832,
            "section_title": "COMPONENTS: ARCHITECTURE, OBJECTIVE, SAMPLING, DATA",
            "char_start_offset": 9494,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 246
                },
                {
                    "start": 247,
                    "end": 278
                },
                {
                    "start": 279,
                    "end": 439
                },
                {
                    "start": 440,
                    "end": 573
                },
                {
                    "start": 574,
                    "end": 905
                },
                {
                    "start": 906,
                    "end": 1017
                },
                {
                    "start": 1018,
                    "end": 1098
                },
                {
                    "start": 1099,
                    "end": 1130
                },
                {
                    "start": 1131,
                    "end": 1259
                },
                {
                    "start": 1260,
                    "end": 1294
                },
                {
                    "start": 1295,
                    "end": 1408
                },
                {
                    "start": 1409,
                    "end": 1543
                },
                {
                    "start": 1544,
                    "end": 1702
                },
                {
                    "start": 1705,
                    "end": 1859
                },
                {
                    "start": 1860,
                    "end": 2022
                }
            ],
            "ref_mentions": [
                {
                    "start": 64,
                    "end": 85,
                    "matchedPaperCorpusId": "52967399"
                },
                {
                    "start": 85,
                    "end": 104,
                    "matchedPaperCorpusId": "204960716"
                },
                {
                    "start": 104,
                    "end": 124,
                    "matchedPaperCorpusId": "204838007"
                },
                {
                    "start": 995,
                    "end": 1016,
                    "matchedPaperCorpusId": "204838007"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.5556640625
        },
        {
            "corpus_id": "259164739",
            "title": "Recipes for Sequential Pre-training of Multilingual Encoder and Seq2Seq Models",
            "text": "Pre-trained Transformer models (Vaswani et al., 2017) are commonly used in Natural Language Processing (NLP) for both transfer learning in downstream tasks (Devlin et al., 2019;Liu et al., 2019;Radford and Narasimhan, 2018;Radford et al., 2019) and for in-context learning (Brown et al., 2020). Transformers were originally designed as sequence-to-sequence (seq2seq) models with an encoder and a decoder component (Vaswani et al., 2017). However, all three obvious variants of this architecture are now common: encoder-only (Devlin et al., 2019), decoder-only (Radford and Narasimhan, 2018;Radford et al., 2019;Brown et al., 2020;Chowdhery et al., 2022;Zhang et al., 2022;Thoppilan et al., 2022) and seq2seq (Lewis et al., 2020;Raffel et al., 2020b;Sanh et al., 2021;Dong et al., 2019;Bao et al., 2020). \n\nCommonly, encoder transformer models are pre-trained using the MLM objective (Devlin et al., 2019). Decoders are pre-trained using a next-token left-to-right prediction (causal) language modeling objective (Radford and Narasimhan, 2018) or some version of autoregressive de-noising (Lewis et al., 2020). Seq2seq models often combine these objectives (Lewis et al., 2020;Bao et al., 2020). \n\nWe follow the multilingual approach of models such as XLM-RoBERTa (Conneau et al., 2020b) (encoder-only) and mT5/mBART (Xue et al., 2021;Liu et al., 2020) (seq2seq), where the model is pre-trained on data from multiple languages.",
            "score": 0.509278886846935,
            "section_title": "A Additional Related Work",
            "char_start_offset": 13138,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 294
                },
                {
                    "start": 295,
                    "end": 437
                },
                {
                    "start": 438,
                    "end": 803
                },
                {
                    "start": 806,
                    "end": 905
                },
                {
                    "start": 906,
                    "end": 1109
                },
                {
                    "start": 1110,
                    "end": 1194
                },
                {
                    "start": 1197,
                    "end": 1426
                }
            ],
            "ref_mentions": [
                {
                    "start": 156,
                    "end": 177,
                    "matchedPaperCorpusId": "52967399"
                },
                {
                    "start": 273,
                    "end": 293,
                    "matchedPaperCorpusId": "218971783"
                },
                {
                    "start": 524,
                    "end": 545,
                    "matchedPaperCorpusId": "52967399"
                },
                {
                    "start": 611,
                    "end": 630,
                    "matchedPaperCorpusId": "218971783"
                },
                {
                    "start": 749,
                    "end": 767,
                    "matchedPaperCorpusId": "239009562"
                },
                {
                    "start": 883,
                    "end": 904,
                    "matchedPaperCorpusId": "52967399"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.396728515625
        },
        {
            "corpus_id": "261064777",
            "title": "Towards an Understanding of Large Language Models in Software Engineering Tasks",
            "text": "The Transformer architecture, proposed by Vaswani et al. in 2017(Vaswani et al., 2017), has emerged as the leading choice for developing large language models (LLMs) due to its exceptional parallelizability and capacity (Zhao et al., 2023b). This scalability allows language models to be expanded to include hundreds or even thousands of billions of parameters, enabling them to capture more complex language patterns and improve performance on various tasks. In general, large language models can be categorized into three main architecture types: encoder-decoder structures, causal-decoder, and prefix decoder (Zhao et al., 2023b), each with its own characteristics and applications. \n\nEncoder-decoder Architecture: The vanilla Transformer proposed in (Vaswani et al., 2017) is based on encoder-decoder architecture, which comprises separate encoder and decoder components. The encoder processes the input sequence and captures its latent representation, which is then used by the decoder to generate the output sequence autoregressively. This architecture is well-suited for tasks involving sequence-to-sequence mapping, such as machine translation, text summarization, and dialogue generation. Encoder-decoder pretrained model. Encoder-decoder pretrained models, such as BART (Lewis et al., 2019) and T5 (Raffel et al., 2020), have demonstrated excellent performance across various downstream tasks. However, with the development of LLM there are only a few large language models based on the encoder-decoder architecture, such as Flan-T5 (Chung et al., 2022) and CodeT5+ (Wang et al., 2023f). \n\nCausal Decoder Architecture: The causal decoder architecture is commonly implemented as a stack of decoder layers. It utilizes a diagonal mask matrix, allowing each token to only have access to information from preceding tokens. This constraint ensures a unidirectional and autoregressive generation process.",
            "score": 0.5076780060333468,
            "section_title": "Model Architecture",
            "char_start_offset": 10774,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 241
                },
                {
                    "start": 242,
                    "end": 459
                },
                {
                    "start": 460,
                    "end": 685
                },
                {
                    "start": 688,
                    "end": 875
                },
                {
                    "start": 876,
                    "end": 1040
                },
                {
                    "start": 1041,
                    "end": 1197
                },
                {
                    "start": 1198,
                    "end": 1231
                },
                {
                    "start": 1232,
                    "end": 1403
                },
                {
                    "start": 1404,
                    "end": 1597
                },
                {
                    "start": 1600,
                    "end": 1714
                },
                {
                    "start": 1715,
                    "end": 1828
                },
                {
                    "start": 1829,
                    "end": 1908
                }
            ],
            "ref_mentions": [
                {
                    "start": 1308,
                    "end": 1329,
                    "matchedPaperCorpusId": "204838007"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.64697265625
        },
        {
            "corpus_id": "260886785",
            "title": "Token-Scaled Logit Distillation for Ternary Weight Generative Language Models",
            "text": "In fine-tuning processes, encoder-only models, often used in Natural Language Understanding (NLU), and decoder-only models, common in Natural Language Generation (NLG) with causal language modeling approach, exhibit different mechanisms for receiving ground truth loss. Fig. 1(b) illustrates the differences. In NLU tasks, encoder models leverage the representation of a unique special token, transformed into a logit vector, for calculating cross-entropy loss, with the number of classes usually being a few [12]. Conversely, decoder models in NLG tasks predict the subsequent token for each input token, transforming each token representation into a logit vector with a class size equivalent to the vocabulary size, often exceeding 50k [25]. Hence, while encoder models gain ground truth loss from a single special token's logit, decoder models derive it from the output logits of all input tokens, which could provide more detailed token-level prediction information. Motivation. The inherent nature of masked self-attention, where each token representation builds upon the representation of the previous tokens, presents previously unseen challenges when applying quantization to decoder models. For a clearer understanding of the decoder model, we conducted a comparative analysis with the encoder model to examine the impact of quantization error on the model. In Fig. 2 (a), the quantization error of the encoder self-attention map exhibits a widespread presence of errors due to the absence of masking in self-attention, and the per-token quantization errors along the layers also show irregular patterns depending on the token index. However, in Fig. 2 (b), the heat map of the decoder model reveals an increasing brightness of quantization errors as we move toward the later tokens. When examining the token index, the phenomenon of quantization errors accumulating toward the later tokens becomes even more pronounced. This previously unconsidered phenomenon of token quantization error accumulation in the decoder model is a crucial feature to consider in GLM QAT. Reflecting on this feature, we analyze the effectiveness of prior KD methods for language modeling and explore suitable KD approaches for the decoder model. \n\nComparison of KD Methods for Decoder QAT.",
            "score": 0.5074862452350472,
            "section_title": "Quantization Challenges on GLMs",
            "char_start_offset": 11276,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 269
                },
                {
                    "start": 270,
                    "end": 308
                },
                {
                    "start": 309,
                    "end": 514
                },
                {
                    "start": 515,
                    "end": 743
                },
                {
                    "start": 744,
                    "end": 970
                },
                {
                    "start": 971,
                    "end": 982
                },
                {
                    "start": 983,
                    "end": 1199
                },
                {
                    "start": 1200,
                    "end": 1366
                },
                {
                    "start": 1367,
                    "end": 1642
                },
                {
                    "start": 1643,
                    "end": 1792
                },
                {
                    "start": 1793,
                    "end": 1929
                },
                {
                    "start": 1930,
                    "end": 2076
                },
                {
                    "start": 2077,
                    "end": 2233
                },
                {
                    "start": 2236,
                    "end": 2277
                }
            ],
            "ref_mentions": [
                {
                    "start": 509,
                    "end": 513,
                    "matchedPaperCorpusId": "52967399"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.93896484375
        },
        {
            "corpus_id": "259360665",
            "title": "Lost in the Middle: How Language Models Use Long Contexts",
            "text": "The open models we evaluated are all decoder-only models-at each timestep, they may only attend to prior tokens. To better understand the potential effects of model architecture on how language model use context, we compare decoder-only and encoder-decoder language models. We experiment with Flan-T5-XXL (Raffel et al., 2020;Chung et al., 2022) and Flan-UL2 (Tay et al., 2023). Flan-T5-XXL is trained with a sequences of 512 tokens (encoder and decoder). Flan-UL2 is initially trained with sequences of 512 tokens (encoder and decoder), but is then pre-trained for an extra 100K steps with 1024 tokens (encoder and decoder) before instruction fine-tuning on sequences with 2048 tokens in the encoder and 512 tokens in the decoder. However, since these models use relative positional embeddings, they can (in principle) extrapolate beyond Figure 8: When encoder-decoder models (Flan-UL2 and Flan-T5-XXL) evaluated on sequences that are shorter than their encoder's training-time maximum sequence length (2048 and 512 tokens, respectively), they are relatively robust to changes in the position of relevant information within their input context (left subplot). In contrast, when these models are evaluated on sequences longer than those seen during training (center and right subplots), we observe a U-shaped performance curve-performance is higher when relevant information occurs at the beginning or end of the input context, as opposed to the middle of the input context. these maximum context lengths; Shaham et al. (2023) find that both models can perform well with sequences of up to 8K tokens. \n\nFigure 8 compares the performance of decoder-only and encoder-decoder models.",
            "score": 0.5068681850747085,
            "section_title": "Effect of Model Architecture",
            "char_start_offset": 19565,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 112
                },
                {
                    "start": 113,
                    "end": 273
                },
                {
                    "start": 274,
                    "end": 378
                },
                {
                    "start": 379,
                    "end": 455
                },
                {
                    "start": 456,
                    "end": 731
                },
                {
                    "start": 732,
                    "end": 1160
                },
                {
                    "start": 1161,
                    "end": 1474
                },
                {
                    "start": 1475,
                    "end": 1600
                },
                {
                    "start": 1603,
                    "end": 1680
                }
            ],
            "ref_mentions": [
                {
                    "start": 305,
                    "end": 326,
                    "matchedPaperCorpusId": "204838007"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.56494140625
        },
        {
            "corpus_id": "248118752",
            "title": "What Language Model Architecture and Pretraining Objective Work Best for Zero-Shot Generalization?",
            "text": "An important step in building LLMs is pretraining, where the model is trained on a large, unlabeled dataset via self-supervision. The choice of pretraining objective can have significant impact on the downstream usability of the LLM, and we therefore include objective choice as a factor in our empirical study. Figure 3 outlines the input and target tokens for the pretraining objectives considered. \n\nLanguage modeling. Since the advent of GPT-2 [Radford et al., 2019], large decoder-only models have generally been pretrained with an autoregressive language modeling objective [Brown et al., 2020, Wu et al., 2021, Rae et al., 2021]. Given previous tokens, the model is tasked with predicting the following one. We refer to this as full language modeling (FLM) . This objective is particularly efficient during pretraining: all tokens in a sequence can generate a loss signal in parallel. At inference time, the model is iteratively asked to predict the next token. \n\nPrefix language modeling. For encoder-decoder and non-causal decoder-only models to perform language modeling, one can define a prefix where the attention mask is allowed to be non-causal. Similar to standard language modeling, the model is tasked to predict each token outside the prefix given all previous tokens. We hereafter refer to this objective as prefix language modeling (PLM) . Loss on the prefix is ignored as tokens in the prefix can attend to their targets. For inference, the prefix is naturally the input text; during pretraining, it is usually chosen at random for each sample. \n\nMasked language modeling. Encoder-only models, such as BERT [Devlin et al., 2018], have typically been pretrained with a masked language modeling objective. Tokens or spans of tokens in the input text are replaced with a special mask token and the model is trained to predict the missing tokens. Raffel et al. [2020] introduced a version of this objective adapted to text-to-text models in the form of span corruption: sentinel tokens are used to flag masked spans of short random lengths, and, after processing the masked input, the model outputs the sentinels followed by their respective predicted content.",
            "score": 0.5068295059017026,
            "section_title": "Pretraining objectives",
            "char_start_offset": 14465,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 129
                },
                {
                    "start": 130,
                    "end": 311
                },
                {
                    "start": 312,
                    "end": 400
                },
                {
                    "start": 403,
                    "end": 421
                },
                {
                    "start": 422,
                    "end": 636
                },
                {
                    "start": 637,
                    "end": 714
                },
                {
                    "start": 715,
                    "end": 765
                },
                {
                    "start": 766,
                    "end": 891
                },
                {
                    "start": 892,
                    "end": 968
                },
                {
                    "start": 971,
                    "end": 996
                },
                {
                    "start": 997,
                    "end": 1159
                },
                {
                    "start": 1160,
                    "end": 1286
                },
                {
                    "start": 1287,
                    "end": 1359
                },
                {
                    "start": 1360,
                    "end": 1442
                },
                {
                    "start": 1443,
                    "end": 1565
                },
                {
                    "start": 1568,
                    "end": 1593
                },
                {
                    "start": 1594,
                    "end": 1724
                },
                {
                    "start": 1725,
                    "end": 1863
                },
                {
                    "start": 1864,
                    "end": 2177
                }
            ],
            "ref_mentions": [
                {
                    "start": 448,
                    "end": 470,
                    "matchedPaperCorpusId": "160025533"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.54345703125
        },
        {
            "corpus_id": "267061124",
            "title": "OrchMoE: Efficient Multi-Adapter Learning with Task-Skill Synergy",
            "text": "The Transformer [Vaswani et al., 2023] architecture constitutes the bedrock upon which the preponderance of contemporary large-scale models are constructed. Stemming from this versatile architecture, two principal model frameworks have emerged: the Encoder-Decoder and the Decoder-Only architectures. The Encoder-Decoder paradigm, exemplified by Google's T5 model [Raffel et al., 2023], has been celebrated for its proficiency in semantic comprehension, attributable to its cloze-style learning objectives. However, this architecture also exhibits certain limitations in generative tasks, an issue that becomes more pronounced in Encoder-Only models such as BERT [Devlin et al., 2019]. Conversely, Decoder-Only models, epitomized by GPT [Radford et al., 2019] and GLM [Du et al., 2022]   formidable strengths in both natural language generation and in-context learning. Their prowess is largely ascribed to the uniformity of autoregressive generation during both training and inference phases, offering a more coherent framework for these tasks.",
            "score": 0.5066659255572976,
            "section_title": "Large Language Model",
            "char_start_offset": 25146,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 156
                },
                {
                    "start": 157,
                    "end": 300
                },
                {
                    "start": 301,
                    "end": 506
                },
                {
                    "start": 507,
                    "end": 685
                },
                {
                    "start": 686,
                    "end": 869
                },
                {
                    "start": 870,
                    "end": 1045
                }
            ],
            "ref_mentions": [
                {
                    "start": 768,
                    "end": 785,
                    "matchedPaperCorpusId": "247519241"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.34228515625
        },
        {
            "corpus_id": "270832367",
            "title": "Predictability and Causality in Spanish and English Natural Language Generation",
            "text": "We compare the causal context-conditioned entropy and the noncausal context-conditioned entropy of the grammatical category distribution of source natural texts to assess whether their language is more predictable from causal or non-causal language contexts. This reveals lower average non-causal conditional entropy in Spanish and lower causal conditional entropy in English. According to this assessment, Spanish is more predictable than English given a non-causal context. B) Then, using both automatic (based on conditional relative entropy) and manual evaluation methodologies, we put decoder-only and encoder-only transformer language models to test to assess empirical causal and non-causal NLG performance, seeking to evaluate if the currently dominant causal NLG paradigm is adequate from a language-agnostic perspective or whether specific languages may benefit from other word generation orderings. We obtain as insights that the best performance is achieved with causal NLG in English and non-causal NLG in Spanish. These insights support further research in NLG in Spanish using bidirectional transformer language models instead of the dominant decoder-only ones. The rest of this paper is organized as follows. Section II reviews related work on both psycholinguistic language predictability and language model causality in NLG. Section III describes the proposed analytical methodology used for the experiments. Sections IV and V present the details and results of the assessments of predictability and text generation performance, respectively. Section VI summarizes and discusses the results obtained. Finally, Section VII concludes the paper.",
            "score": 0.5066301221820766,
            "section_title": "I. INTRODUCTION",
            "char_start_offset": 4118,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 258
                },
                {
                    "start": 259,
                    "end": 376
                },
                {
                    "start": 377,
                    "end": 475
                },
                {
                    "start": 476,
                    "end": 909
                },
                {
                    "start": 910,
                    "end": 1027
                },
                {
                    "start": 1028,
                    "end": 1176
                },
                {
                    "start": 1177,
                    "end": 1224
                },
                {
                    "start": 1225,
                    "end": 1342
                },
                {
                    "start": 1343,
                    "end": 1426
                },
                {
                    "start": 1427,
                    "end": 1560
                },
                {
                    "start": 1561,
                    "end": 1618
                },
                {
                    "start": 1619,
                    "end": 1660
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.4912109375
        },
        {
            "corpus_id": "263729712",
            "title": "Warm-Starting for Improving the Novelty of Abstractive Summarization",
            "text": "For the decoder part, we leverage GPT2, BERT, RoBERTa, and Ernie 2.0. GPT2 is trained on open-ended autoregressive generation with the objective of predicting the next word in a sequence of a few token starters (causal language modeling) utilizing only the Transformer's unidirectional ''left-to-right'' self-attention decoder. Theoretically, GPT2 is claimed to be the optimal design for use as a decoder. However, with a few adjustments, bi-directional encoderonly pretrained models such as BERT, RoBERTa, and Ernie 2.0 may simply be adapted as decoders. To compare the encoder and decoder architectures of the Transformer, encoder blocks consist solely of a bi-directional self-attention layer and two feed-forward layers. By contrast, decoder blocks have a unidirectional self-attention layer, a crossattention layer, and two feed-forward layers. In addition, a language model head layer follows the decoder blocks, converting the last decoder block's output vectors to logit vectors. As a result, the following steps have been taken to enable an encoder to function as a decoder: First, we alter the self-attention layers to operate unidirectionally, similar to the decoder, and initialize them with the weights from the encoder's self-attention layer. A cross-attention layer is then added between the self-attention layer and the two feed-forward layers. As suggested by [6], we randomize the initialization of this layer's weights, which are subsequently trained while finetuning the model on the summarization task. Finally, we add a language model head layer on top of the last block of the decoder and initialize it with the weights of the encoder's word embeddings. Table 3 highlights the necessary adjustments for this approach. It is worth mentioning that the hidden size of the encoder and decoder in warm-started models must match in order for them to communicate and perform dot products on their respective vectors.",
            "score": 0.5063729849034757,
            "section_title": "2) DECODERS",
            "char_start_offset": 21434,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 69
                },
                {
                    "start": 70,
                    "end": 327
                },
                {
                    "start": 328,
                    "end": 405
                },
                {
                    "start": 406,
                    "end": 555
                },
                {
                    "start": 556,
                    "end": 724
                },
                {
                    "start": 725,
                    "end": 849
                },
                {
                    "start": 850,
                    "end": 987
                },
                {
                    "start": 988,
                    "end": 1256
                },
                {
                    "start": 1257,
                    "end": 1360
                },
                {
                    "start": 1361,
                    "end": 1523
                },
                {
                    "start": 1524,
                    "end": 1676
                },
                {
                    "start": 1677,
                    "end": 1740
                },
                {
                    "start": 1741,
                    "end": 1932
                }
            ],
            "ref_mentions": [
                {
                    "start": 1377,
                    "end": 1380,
                    "matchedPaperCorpusId": "198967997"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.6669921875
        },
        {
            "corpus_id": "260203128",
            "title": "Exploiting the Potential of Seq2Seq Models as Robust Few-Shot Learners",
            "text": "According to Brown et al. (2020), pretrained decoder-only LLMs achieve proficiency in various tasks simply by prompting a few examples as input, without requiring any parameter updating. Unlike the many advancements of large decoder-only models (Rae et al., 2021;Zhang et al., 2022;Touvron et al., 2023;Hao et al., 2022), the mainstream approach for encoder-decoder LLMs to adapt to a specific task remained supervised instruction-tuning (Sanh et al., 2022;Wei et al., 2022a;Longpre et al., 2023). Recently, a few studies attempted to explore in-context learning; UL2 and AlexaTM reported zero-shot results on the SuperGLUE dataset, and T0 utilized multitask prompted training to enhance zero-shot performance. Some of the studies employed techniques that emulate decoder-only models. Patel et al. (2022) utilized decoder-only-style sequential autoregressive prompting. Tay et al. (2022) mixed causal language modeling with denoising objectives. However, these approaches are restricted to particular setups and there has been a lack of structured results regarding few-shot learning.",
            "score": 0.5062148462587657,
            "section_title": "Related work",
            "char_start_offset": 25049,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 186
                },
                {
                    "start": 187,
                    "end": 497
                },
                {
                    "start": 498,
                    "end": 710
                },
                {
                    "start": 711,
                    "end": 784
                },
                {
                    "start": 785,
                    "end": 869
                },
                {
                    "start": 870,
                    "end": 945
                },
                {
                    "start": 946,
                    "end": 1084
                }
            ],
            "ref_mentions": [
                {
                    "start": 13,
                    "end": 32,
                    "matchedPaperCorpusId": "218971783"
                },
                {
                    "start": 457,
                    "end": 475,
                    "matchedPaperCorpusId": "237416585"
                },
                {
                    "start": 785,
                    "end": 804,
                    "matchedPaperCorpusId": "252595927"
                },
                {
                    "start": 870,
                    "end": 887,
                    "matchedPaperCorpusId": "257219404"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.331298828125
        },
        {
            "corpus_id": "268876353",
            "title": "Large Language Model for Vulnerability Detection and Repair: Literature Review and the Road Ahead",
            "text": "The term Large Language Model (LLM) was introduced to distinguish language models based on their parameter size, specifically referring to large-sized pre-trained language models [127]. However, the literature lacks a formal consensus on the minimum parameter scale for LLMs [100]. In this paper, we adopt the LLM scope division and taxonomy introduced by Pan et al. [73] and categorize the mainstream LLMs into three groups according to their architectures: 1) encoderonly, 2) encoder-decoder, and 3) decoder-only LLMs. We will provide a brief introduction to some representative LLMs for each category due to the space limit. Encoder-only LLMs. Encoder-only LLMs are a type of neural network architecture that utilizes only the encoder component of the Transformer model [20]. In the SE domain, examples of encoder-only LLMs include CodeBERT [23], GraphCodeBERT [33], CuBERT [42], VulBERTa [34], CCBERT [131], SOBERT [36], and BERTOverflow [91]. Encoder-decoder LLMs. Encoder-decoder LLMs integrate both the encoder and decoder modules of the Transformer model [97]. The encoder processes the input sentence, while the decoder generates the target output text/code. Prominent examples of encoder-decoder LLMs include PLBART [10], T5 [79], CodeT5 [103], UniXcoder [32], and NatGen [14]. Decoder-only LLMs. Decoder-only LLMs exclusively utilize the decoder module of the Transformer model to generate the target output text/code. The GPT series, including GPT-2 [78], GPT-3 [13], GPT-3.5 [71], and GPT-4 [72], stand as prominent implementations of this model series. Additionally, in the SE domain, there are numerous decoder-only LLMs specialized for code as well.",
            "score": 0.5059209156903967,
            "section_title": "Large Language Models (LLMs)",
            "char_start_offset": 5797,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 185
                },
                {
                    "start": 186,
                    "end": 281
                },
                {
                    "start": 282,
                    "end": 520
                },
                {
                    "start": 521,
                    "end": 627
                },
                {
                    "start": 628,
                    "end": 646
                },
                {
                    "start": 647,
                    "end": 778
                },
                {
                    "start": 779,
                    "end": 947
                },
                {
                    "start": 948,
                    "end": 969
                },
                {
                    "start": 970,
                    "end": 1068
                },
                {
                    "start": 1069,
                    "end": 1167
                },
                {
                    "start": 1168,
                    "end": 1287
                },
                {
                    "start": 1288,
                    "end": 1306
                },
                {
                    "start": 1307,
                    "end": 1429
                },
                {
                    "start": 1430,
                    "end": 1566
                },
                {
                    "start": 1567,
                    "end": 1665
                }
            ],
            "ref_mentions": [
                {
                    "start": 877,
                    "end": 881,
                    "matchedPaperCorpusId": "220425306"
                },
                {
                    "start": 892,
                    "end": 896,
                    "matchedPaperCorpusId": "249062897"
                },
                {
                    "start": 905,
                    "end": 910,
                    "matchedPaperCorpusId": "263151902"
                },
                {
                    "start": 919,
                    "end": 923,
                    "matchedPaperCorpusId": "257496129"
                },
                {
                    "start": 942,
                    "end": 946,
                    "matchedPaperCorpusId": "218487168"
                },
                {
                    "start": 1235,
                    "end": 1239,
                    "matchedPaperCorpusId": "204838007"
                },
                {
                    "start": 1265,
                    "end": 1269,
                    "matchedPaperCorpusId": "247315559"
                },
                {
                    "start": 1282,
                    "end": 1286,
                    "matchedPaperCorpusId": "249674577"
                },
                {
                    "start": 1462,
                    "end": 1466,
                    "matchedPaperCorpusId": "160025533"
                },
                {
                    "start": 1474,
                    "end": 1478,
                    "matchedPaperCorpusId": "218971783"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.480224609375
        },
        {
            "corpus_id": "254044610",
            "title": "The Stack: 3 TB of permissively licensed source code",
            "text": "We experiment with decoder-only transformers trained via a causal language modeling objective. We opt for a 350M parameter model with 24 layers, a hidden dimension of 1024, 16 attention heads, and a sequence length of 2048. The model is trained for 300K iterations with a global batch size of 384 using Adam (Kingma & Ba, 2015) with \u03b2 1 = 0.9, \u03b2 2 = 0.95, = 10 \u22128 and a weight decay of 0.1. The learning rate set to 3 \u00d7 10 \u22124 is warmed up for 175 steps, then follows a cosine decay. The model processes 235.9B tokens during training. The Byte-Pair Encoding tokenizer was trained on a 50-50 mixture of the Pile (Gao et al., 2020) and Python files from The Stack. We use a fork13 of Megatron-LM (Shoeybi et al., 2019) for training.",
            "score": 0.5057263523284173,
            "section_title": "Training details",
            "char_start_offset": 27193,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 94
                },
                {
                    "start": 95,
                    "end": 223
                },
                {
                    "start": 224,
                    "end": 390
                },
                {
                    "start": 391,
                    "end": 482
                },
                {
                    "start": 483,
                    "end": 533
                },
                {
                    "start": 534,
                    "end": 661
                },
                {
                    "start": 662,
                    "end": 729
                }
            ],
            "ref_mentions": [
                {
                    "start": 308,
                    "end": 327,
                    "matchedPaperCorpusId": "6628106"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.0191192626953125
        },
        {
            "corpus_id": "267211690",
            "title": "TURNA: A Turkish Encoder-Decoder Language Model for Enhanced Understanding and Generation",
            "text": "Recently, pretrained language models based on transformers have been dominant in the NLP field, exhibiting variations in both components and objectives. Models that exclusively employ encoders, typically trained with denoising objectives, are geared toward understanding tasks, as exemplified in works such as Devlin et al. (2019) and Clark et al. (2020). Conversely, models that exclusively use decoders are designed for generation tasks, employing causal language modeling, as demonstrated in various studies (Radford et al., 2019;Brown et al., 2020;Touvron et al., 2023). The Text-to-Text Transformer (T5) (Raffel et al., 2020), on the other hand, employs an encoder-decoder architecture and undergoes pretraining with a denoising objective referred to as span corruption. UniLM (Dong et al., 2019) is also an encoder-decoder model, but pre-trained using unidirectional, bidirectional, and sequence-to-sequence language modeling. This can be seen as a combination of causal and denoising objectives. Recently, Tay et al., 2023 proposed that various pretraining objectives can be recast as each other. They introduced the UL2 framework based on a pretraining objective called Mixture-of-Denoisers (MoD), which combines different pretraining paradigms. They compared decoder-only and encoder-decoder models trained with the MoD objective and found that encoder-decoder models often perform better. Notably, by using the MoD objective and moderately scaling up the model, they achieved state-of-the-art performance on a diverse set of NLP tasks including understanding and generation tasks.",
            "score": 0.5050583677006911,
            "section_title": "Pretraining objectives",
            "char_start_offset": 5796,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 152
                },
                {
                    "start": 153,
                    "end": 355
                },
                {
                    "start": 356,
                    "end": 574
                },
                {
                    "start": 575,
                    "end": 775
                },
                {
                    "start": 776,
                    "end": 932
                },
                {
                    "start": 933,
                    "end": 1002
                },
                {
                    "start": 1003,
                    "end": 1103
                },
                {
                    "start": 1104,
                    "end": 1253
                },
                {
                    "start": 1254,
                    "end": 1398
                },
                {
                    "start": 1399,
                    "end": 1590
                }
            ],
            "ref_mentions": [
                {
                    "start": 310,
                    "end": 330,
                    "matchedPaperCorpusId": "52967399"
                },
                {
                    "start": 335,
                    "end": 354,
                    "matchedPaperCorpusId": "208229926"
                },
                {
                    "start": 511,
                    "end": 533,
                    "matchedPaperCorpusId": "160025533"
                },
                {
                    "start": 609,
                    "end": 630,
                    "matchedPaperCorpusId": "204838007"
                },
                {
                    "start": 782,
                    "end": 801,
                    "matchedPaperCorpusId": "147704286"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.64208984375
        },
        {
            "corpus_id": "268230771",
            "title": "DINER: Debiasing Aspect-based Sentiment Analysis with Multi-variable Causal Inference",
            "text": "Our method is model-agnostic. In the empirical study, we utilize two types of mainstream encoderonly model, RoBERTa (Liu et al., 2019b) 3 and BERT (Devlin et al., 2019) 4 as the backbone for our experiments. For comprehensive details on the hyperparameters employed in our experiments, refer to Appendix D.",
            "score": 0.5050389877887302,
            "section_title": "Implementations",
            "char_start_offset": 19437,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 29
                },
                {
                    "start": 30,
                    "end": 207
                },
                {
                    "start": 208,
                    "end": 306
                }
            ],
            "ref_mentions": [
                {
                    "start": 147,
                    "end": 168,
                    "matchedPaperCorpusId": "52967399"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.030670166015625
        },
        {
            "corpus_id": "248157514",
            "title": "A Unified Cascaded Encoder ASR Model for Dynamic Model Sizes",
            "text": "The baseline Conformer-based [21] cascaded encoder model [14] is comprised of a causal conformer encoder with N layers, followed by a non-causal conformer encoder [22] with M layers and an embedding RNN-T decoder [23]. To improve the flexibility in unifying different models, we reformulate the cascaded model architecture to allow easy extractions of models with different sizes, as shown in Figure 1. In our model, each causal layer can be connected to the decoder or the first non-causal layer. We also allow connections from any non-causal layer to the decoder. From the super-net, we extract K sub-models, each containing the first n k (0 \u2264 n k \u2264 N ) causal layers, and the first m k (0 \u2264 m k \u2264 M ) non-causal layers, which can be used under different model size and latency restrictions: \n\nwhere x and y k denote the input and output of the k-th submodel (all the sub-models have the same input). Enc c k is the causal encoder containing n k causal layers, Enc nc k is the noncausal encoder containing m k non-causal layers, and Dec is the shared decoder. Note that each of our sub-models does not have any dedicated encoder layer during training to minimize the total memory and storage cost in practice.",
            "score": 0.5046369926334167,
            "section_title": "Dynamic cascaded encoder model",
            "char_start_offset": 5259,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 218
                },
                {
                    "start": 219,
                    "end": 402
                },
                {
                    "start": 403,
                    "end": 497
                },
                {
                    "start": 498,
                    "end": 565
                },
                {
                    "start": 566,
                    "end": 793
                },
                {
                    "start": 796,
                    "end": 902
                },
                {
                    "start": 903,
                    "end": 1061
                },
                {
                    "start": 1062,
                    "end": 1211
                }
            ],
            "ref_mentions": [
                {
                    "start": 29,
                    "end": 33,
                    "matchedPaperCorpusId": "218674528"
                },
                {
                    "start": 57,
                    "end": 61,
                    "matchedPaperCorpusId": "249437304"
                },
                {
                    "start": 163,
                    "end": 167,
                    "matchedPaperCorpusId": "225094578"
                },
                {
                    "start": 213,
                    "end": 217,
                    "matchedPaperCorpusId": "237532186"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.54296875
        },
        {
            "corpus_id": "270521959",
            "title": "GenDistiller: Distilling Pre-trained Language Models based on an Autoregressive Generative Model",
            "text": "Language models are established to predict the embedding of the target token based on the context or the previous tokens of it.In this work, we use generative language model to predict the hidden layer embeddings of teacher model in the hope to involve the interaction of the intermediate layers and avoid utilizing the future information.Three generative architectures are considered: encoder-decoder, prefix decoder and causal decoder [24].The encoder-decoder architecture [25][26][27] consists of two stacks of Transformer blocks to construct encoder and decoder separately.The encoder encodes the input sequence into a common history and the decoder generates the target sequence based on the common history in an autoregressive way.The prefix decoder architecture [28,29] performs bidirectional attention over the prefix tokens and unidirectional attention on generated tokens.The causal decoder architecture [30][31][32][33] only attend to the past tokens of the input through a unidirectional attention mask.In our work, we select the causal decoder architecture as the backbone of our generative distiller since it is more concise and is capable to predict the hidden layer outputs of the teacher model autoregressively.",
            "score": 0.5046004740790092,
            "section_title": "Generative Language Models",
            "char_start_offset": 5733,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 127
                },
                {
                    "start": 127,
                    "end": 339
                },
                {
                    "start": 339,
                    "end": 442
                },
                {
                    "start": 442,
                    "end": 577
                },
                {
                    "start": 577,
                    "end": 737
                },
                {
                    "start": 737,
                    "end": 882
                },
                {
                    "start": 882,
                    "end": 1015
                },
                {
                    "start": 1015,
                    "end": 1228
                }
            ],
            "ref_mentions": [
                {
                    "start": 437,
                    "end": 441,
                    "matchedPaperCorpusId": "257900969"
                },
                {
                    "start": 479,
                    "end": 483,
                    "matchedPaperCorpusId": "204838007"
                },
                {
                    "start": 769,
                    "end": 773,
                    "matchedPaperCorpusId": "247951931"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.54248046875
        },
        {
            "corpus_id": "248266379",
            "title": "A Survey on Non-Autoregressive Generation for Neural Machine Translation and Beyond",
            "text": "Model Architecture. As for model architecture, both AT and NAT models take the encoder and decoder framework for translation. The encoder and decoder can be different neural networks, such as RNN [9], CNN [11], and Transformer [13]. Due to the superior performance of the Transformer network, we focus on the Transformer model for discussion in this survey. The encoder is used to encode the source sentences, while the decoder is utilized for decoding the target sentence. Compared to AT and NAT models, they adopt the same encoder architecture, and the differences are reflected in the decoders to match the specific training objective. (1) Specifically, AT models need to prevent earlier decoding steps from peeking at information from later steps. Therefore, the constraint of an autoregressive factorization of the output distribution is required, and they adopt the strict causal mask by applying a lower triangular matrix in the self-attention module of the conventional Transformer decoder [13]. (2) However, for NAT models, including the iteration-based NAT models, this constraint is no longer necessary, so they adopt the unmasked self-attention over all target tokens [16]. \n\n(3) As for SAT models, they adopt a coarse-grained lower triangular matrix as the causal mask, which means that they allow k tokens to peep later information in the same group while keeping the constraint between different groups. Inference Schedule. When going to the inference stage, the differences are as follows. (1) The AT models predict the target tokens in a one-by-one manner, and the tokens predicted previously are fed back into the decoder to generate the next token. (2) While SAT models predict a group of target tokens at one time, the previously generated groups of tokens are fed into the decoder to generate the next group of tokens, which is the same as the AT models. (3) For iterationbased NAT models, it needs k iterations for inference. The translated results of the previous iteration will be fed into the decoder again for refinements. (4) As for fully NAT models, they generate all predicted target tokens at only one step, which greatly speeds up inference.",
            "score": 0.5044484716249439,
            "section_title": "Comparison",
            "char_start_offset": 12320,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 19
                },
                {
                    "start": 20,
                    "end": 125
                },
                {
                    "start": 126,
                    "end": 232
                },
                {
                    "start": 233,
                    "end": 357
                },
                {
                    "start": 358,
                    "end": 473
                },
                {
                    "start": 474,
                    "end": 638
                },
                {
                    "start": 639,
                    "end": 751
                },
                {
                    "start": 752,
                    "end": 1003
                },
                {
                    "start": 1004,
                    "end": 1185
                },
                {
                    "start": 1188,
                    "end": 1418
                },
                {
                    "start": 1419,
                    "end": 1438
                },
                {
                    "start": 1439,
                    "end": 1505
                },
                {
                    "start": 1506,
                    "end": 1667
                },
                {
                    "start": 1668,
                    "end": 1875
                },
                {
                    "start": 1876,
                    "end": 1947
                },
                {
                    "start": 1948,
                    "end": 2048
                },
                {
                    "start": 2049,
                    "end": 2172
                }
            ],
            "ref_mentions": [
                {
                    "start": 196,
                    "end": 199,
                    "matchedPaperCorpusId": "17048224"
                },
                {
                    "start": 205,
                    "end": 209,
                    "matchedPaperCorpusId": "3648736"
                },
                {
                    "start": 227,
                    "end": 231,
                    "matchedPaperCorpusId": "13756489"
                },
                {
                    "start": 998,
                    "end": 1002,
                    "matchedPaperCorpusId": "13756489"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.75
        },
        {
            "corpus_id": "249626024",
            "title": "Language Models are General-Purpose Interfaces",
            "text": "Large-scale language model pretraining has achieved strong performance across various downstream tasks and aroused extensive research interest. The difference between the models mainly lies in the pretraining objective and model architecture. GPT (Radford et al., 2018;2019;Brown et al., 2020) pretrains causal language models with decoder-only Transformers, demonstrating intriguing properties of few-shot and in-context learning. Recent efforts (Rae et al., 2021;Du et al., 2021;Smith et al., 2022;Hoffmann et al., 2022;Thoppilan et al., 2022;Chowdhery et al., 2022) focus on scaling up in terms of data and model size. In order to implement bidirectional encoding, Devlin et al. (2019) propose the masked language modeling objective. Clark et al. (2020) introduce the replaced token detection task to improve pretraining efficiency. Furthermore, some efforts investigate frameworks that can handle both natural language understanding and generation tasks. T5 (Raffel et al., 2020) introduces an encoder-decoder framework that converts all tasks into a text-to-text format. BART (Lewis et al., 2020) is a sequence-to-sequence model pretrained by reconstructing the original text from corrupted documents. UniLM (Dong et al., 2019;Bao et al., 2020) presents to jointly optimize unidirectional, bidirectional and sequence-to-sequence language modeling objectives controlled by different self-attention masks. Wang et al. (2022b), Tay et al. (2022), andArtetxe et al. (2022) study the effects of different pretraining objectives and architectures on downstream generalization. Specifically, causal language models are good at zero-shot or in-context learning, while non-causal models perform better for finetuning. In our work, we combine the best of both worlds by introducing semi-causal language modeling. So we can obtain decent finetuning performance and benefit from the capability of in-context learning. Moreover, the unification enables us to build a general-purpose interface to various foundation models.",
            "score": 0.5043976554382644,
            "section_title": "Language Model Pretraining",
            "char_start_offset": 43537,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 143
                },
                {
                    "start": 144,
                    "end": 242
                },
                {
                    "start": 243,
                    "end": 431
                },
                {
                    "start": 432,
                    "end": 621
                },
                {
                    "start": 622,
                    "end": 736
                },
                {
                    "start": 737,
                    "end": 835
                },
                {
                    "start": 836,
                    "end": 958
                },
                {
                    "start": 959,
                    "end": 1075
                },
                {
                    "start": 1076,
                    "end": 1206
                },
                {
                    "start": 1207,
                    "end": 1408
                },
                {
                    "start": 1409,
                    "end": 1575
                },
                {
                    "start": 1576,
                    "end": 1713
                },
                {
                    "start": 1714,
                    "end": 1807
                },
                {
                    "start": 1808,
                    "end": 1910
                },
                {
                    "start": 1911,
                    "end": 2014
                }
            ],
            "ref_mentions": [
                {
                    "start": 274,
                    "end": 293,
                    "matchedPaperCorpusId": "218971783"
                },
                {
                    "start": 481,
                    "end": 500,
                    "matchedPaperCorpusId": "51876975"
                },
                {
                    "start": 668,
                    "end": 688,
                    "matchedPaperCorpusId": "52967399"
                },
                {
                    "start": 962,
                    "end": 983,
                    "matchedPaperCorpusId": "204838007"
                },
                {
                    "start": 1081,
                    "end": 1101,
                    "matchedPaperCorpusId": "204960716"
                },
                {
                    "start": 1232,
                    "end": 1249,
                    "matchedPaperCorpusId": "211572655"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.40283203125
        },
        {
            "corpus_id": "259047693",
            "title": "Natural Language Generation and Understanding of Big Code for AI-Assisted Programming: A Review",
            "text": "Encoder-decoder models [47] refer to sequence-to-sequence models, utilizing both components of the transformer architecture [48]. The encoder's attention layers can access all words in the input sentence at each stage, while the decoder's attention layers can only access the words preceding a given word in the input. Sequence-to-sequence models such as BART [49], T5 (Text-to-Text Transfer Transformer) [50], and TreeGen [51] are well-suited for tasks that involve generating new text based on an input, such as code generation, code refinement, defect detection, and clone detection, for AI-assisted programming tasks. \n\nEncoder-only models, also known as autoencoders, use only an encoder network to transform input data into a compressed representation. They are commonly used in unsupervised learning tasks such as dimensionality reduction and anomaly detection in NLP tasks. In the past, code embedding approaches could be utilized to obtain the representation from the input data such as Neural Network Language Model [52], Code2Vec [53], ELMo [54], TextRank [55], and GGNN [56]. For AI-assisted programming tasks, they are used for understanding tasks to learn useful representations with the The encoder considers all words in a sentence, while the decoder works sequentially. Once the initial words are predicted, they are used to generate subsequent words. The attention layers in the encoder consider all the words in a sentence, while the decoder works sequentially and can only focus on the words it has already translated. \n\nBERT [57] and RoBERTa [44] of data in an unsupervised manner, which can be used as features for downstream tasks such as code translation and code summarization. \n\nDecoder-only models, also known as autoregressive models, are a type of neural network architecture used in natural language processing tasks such as GPT-2 [58], GPT-3 [59], GPT-J [60], Reformer [61], and GPT-Neo [62], which use the decoder to predict the next token output given all previous tokens.",
            "score": 0.5043637316181604,
            "section_title": "Encoder-only Understanding",
            "char_start_offset": 8664,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 129
                },
                {
                    "start": 130,
                    "end": 318
                },
                {
                    "start": 319,
                    "end": 621
                },
                {
                    "start": 624,
                    "end": 758
                },
                {
                    "start": 759,
                    "end": 881
                },
                {
                    "start": 882,
                    "end": 1087
                },
                {
                    "start": 1088,
                    "end": 1286
                },
                {
                    "start": 1287,
                    "end": 1368
                },
                {
                    "start": 1369,
                    "end": 1538
                },
                {
                    "start": 1541,
                    "end": 1702
                },
                {
                    "start": 1705,
                    "end": 2005
                }
            ],
            "ref_mentions": [
                {
                    "start": 23,
                    "end": 27,
                    "matchedPaperCorpusId": "5590763"
                },
                {
                    "start": 124,
                    "end": 128,
                    "matchedPaperCorpusId": "13756489"
                },
                {
                    "start": 360,
                    "end": 364,
                    "matchedPaperCorpusId": "204960716"
                },
                {
                    "start": 405,
                    "end": 409,
                    "matchedPaperCorpusId": "204838007"
                },
                {
                    "start": 423,
                    "end": 427,
                    "matchedPaperCorpusId": "208248351"
                },
                {
                    "start": 1026,
                    "end": 1030,
                    "matchedPaperCorpusId": "1326925"
                },
                {
                    "start": 1052,
                    "end": 1056,
                    "matchedPaperCorpusId": "3626819"
                },
                {
                    "start": 1067,
                    "end": 1071,
                    "matchedPaperCorpusId": "577937"
                },
                {
                    "start": 1082,
                    "end": 1086,
                    "matchedPaperCorpusId": "3495200"
                },
                {
                    "start": 1546,
                    "end": 1550,
                    "matchedPaperCorpusId": "52967399"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.4599609375
        },
        {
            "corpus_id": "271769681",
            "title": "ARIES: A General Benchmark for Argument Relation Identification",
            "text": "In addition to the three natural language modelling approaches, we have also included the three main model architectures in state-of-the-art natural language processing. This way, we consider encoderonly (Devlin et al., 2019), decoder-only (Brown et al., 2020), and encoder-decoder (Vaswani et al., 2017) architectures. For the first two natural language modelling approaches (i.e., sequence and token classification), the ARIES benchmark considers the three possible architectures. However, for the sequence-to-sequence alignment approach, we can only rely on the encoder-decoder architecture, given its nature requiring both encoder and decoder (see Appendix A.2 for more details).",
            "score": 0.5038380539181918,
            "section_title": "Model Architectures",
            "char_start_offset": 16356,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 169
                },
                {
                    "start": 170,
                    "end": 319
                },
                {
                    "start": 320,
                    "end": 482
                },
                {
                    "start": 483,
                    "end": 683
                }
            ],
            "ref_mentions": [
                {
                    "start": 204,
                    "end": 225,
                    "matchedPaperCorpusId": "52967399"
                },
                {
                    "start": 240,
                    "end": 260,
                    "matchedPaperCorpusId": "218971783"
                },
                {
                    "start": 282,
                    "end": 304,
                    "matchedPaperCorpusId": "13756489"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.1348876953125
        },
        {
            "corpus_id": "270765454",
            "title": "Lexical Substitution as Causal Language Modeling",
            "text": "Let me <t> begin </t> again.\n\nFigure 1: Comparison between (a) GeneSis (Lacerra et al., 2021b) and (b) our proposed PromptSub.\n\nlexical simplification (Aumiller and Gertz, 2022), adversarial attacks and defenses (Li et al., 2021), semantic change detection (Card, 2023), and natural language watermarking (Yang et al., 2022).\n\nRecent prior work on LST leverages pre-trained language models (PLMs), specifically masked language models (MLMs) (Lin et al., 2022;Michalopoulos et al., 2022;Omarov and Kondrak, 2023), of which BERT (Devlin et al., 2019) is a well-known example.Since MLMs are trained on the task of predicting likely words in a context where a single word is masked, they seem to be a natural fit for LST.However, masking a word is an information-losing process.As a result, the predicted substitutes may fit the context well, but can significantly alter the original meaning of the sentence.\n\nAs an alternative to masked language modeling, we propose to employ causal language modeling instead.While MLMs first encode the entire context around the mask and then decode output from this encoding, causal language models (CLMs) are trained to predict the next token in a sequence given only the previous tokens as context (Radford et al., 2018).This linear processing of text is referred to as auto-regressive decoding; by eschewing the need for discrete encoding and decoding phases, these models can achieve high performance in generative tasks, without an encoder that increases the number of parameters.These decoder-only models include the well-known GPT series (Brown et al., 2020), which powers popular language generation tools such as ChatGPT (OpenAI, 2023).However, prior methods for applying a pre-trained CLM to LST go no further than simple prompting (Lee et al., 2021).",
            "score": 0.5037954489654204,
            "section_title": "(b) PromptSub",
            "char_start_offset": 813,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 28
                },
                {
                    "start": 30,
                    "end": 126
                },
                {
                    "start": 128,
                    "end": 325
                },
                {
                    "start": 327,
                    "end": 573
                },
                {
                    "start": 573,
                    "end": 717
                },
                {
                    "start": 717,
                    "end": 774
                },
                {
                    "start": 774,
                    "end": 904
                },
                {
                    "start": 906,
                    "end": 1007
                },
                {
                    "start": 1007,
                    "end": 1256
                },
                {
                    "start": 1256,
                    "end": 1518
                },
                {
                    "start": 1518,
                    "end": 1678
                },
                {
                    "start": 1678,
                    "end": 1794
                }
            ],
            "ref_mentions": [
                {
                    "start": 71,
                    "end": 94,
                    "matchedPaperCorpusId": "243865376"
                },
                {
                    "start": 151,
                    "end": 177,
                    "matchedPaperCorpusId": "255415873"
                },
                {
                    "start": 212,
                    "end": 229,
                    "matchedPaperCorpusId": "221739314"
                },
                {
                    "start": 257,
                    "end": 269,
                    "matchedPaperCorpusId": "259370664"
                },
                {
                    "start": 305,
                    "end": 324,
                    "matchedPaperCorpusId": "245144237"
                },
                {
                    "start": 441,
                    "end": 459,
                    "matchedPaperCorpusId": "248798641"
                },
                {
                    "start": 459,
                    "end": 486,
                    "matchedPaperCorpusId": "235794903"
                },
                {
                    "start": 486,
                    "end": 511,
                    "matchedPaperCorpusId": "259858942"
                },
                {
                    "start": 527,
                    "end": 547,
                    "matchedPaperCorpusId": "52967399"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.164306640625
        },
        {
            "corpus_id": "6589519",
            "title": "Causal and anti-causal learning in pattern recognition for neuroimaging",
            "text": "Pattern recognition in neuroimaging aims to provide insights into the neural basis of cognitive processes. Two types of models are used in this endeavor: encoding-and decoding models. Encoding models predict a subject's brain state for a given experimental condition, while decoding models aim to reconstruct experimental conditions from neuroimaging data. This difference has important consequences for the interpretation of brain state features that are found to be relevant in each type of model. \n\nIt has been argued that only encoding models can provide a complete functional description of a region of interest [1]. Decoding models, on the other hand, may determine brain state features as relevant that are statistically independent of the experimental condition [2]. While in linear decoding models potential misinterpretations can be avoided by converting them into encoding models [3], this is a substantially more difficult problem for non-linear decoding models. As decoding models are becoming ever more popular in the analysis of neuroimaging data [4], the correct interpretation of such models is of considerable importance. \n\nIn this paper, we argue that the distinction between encoding-and decoding models is not sufficient to determine the meaning of relevant features in each type of model: Pattern recognition models need to be further distinguished with respect to whether they learn causal-or anti-causal relations [5]. In general, neuroimaging studies are based on the following causal structure: stimulus \u2192 brain activity \u2192 response. We note that more complex experimental paradigms, in which responses again act as stimuli [6], can also be modeled in this way by considering time-resolved variables, e. g. stimulus \n\n. Depending on whether experimental conditions are chosen to represent stimuli or responses, encoding-and decoding models then model causalor anti-causal relations. In the following, we argue that this has important consequences for the interpretation of relevant features in each type of model. Furthermore, we argue that interpretation of neuroimaging data de facto requires causal inference problems to be solved. \n\nThe remainder of this article is organized as follows. In section II we introduce the necessary notation and terminology to formulate our proposed distinction of pattern recognition models in section II-D.",
            "score": 0.503337495863351,
            "section_title": "I. INTRODUCTION",
            "char_start_offset": 18,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 106
                },
                {
                    "start": 107,
                    "end": 183
                },
                {
                    "start": 184,
                    "end": 356
                },
                {
                    "start": 357,
                    "end": 499
                },
                {
                    "start": 502,
                    "end": 621
                },
                {
                    "start": 622,
                    "end": 774
                },
                {
                    "start": 775,
                    "end": 974
                },
                {
                    "start": 975,
                    "end": 1139
                },
                {
                    "start": 1142,
                    "end": 1442
                },
                {
                    "start": 1443,
                    "end": 1558
                },
                {
                    "start": 1559,
                    "end": 1740
                },
                {
                    "start": 1743,
                    "end": 1907
                },
                {
                    "start": 1908,
                    "end": 2038
                },
                {
                    "start": 2039,
                    "end": 2159
                },
                {
                    "start": 2162,
                    "end": 2216
                },
                {
                    "start": 2217,
                    "end": 2367
                }
            ],
            "ref_mentions": [
                {
                    "start": 617,
                    "end": 620,
                    "matchedPaperCorpusId": "27072109"
                },
                {
                    "start": 770,
                    "end": 773,
                    "matchedPaperCorpusId": "15154531"
                },
                {
                    "start": 891,
                    "end": 894,
                    "matchedPaperCorpusId": "4512713"
                },
                {
                    "start": 1062,
                    "end": 1065,
                    "matchedPaperCorpusId": "9431428"
                },
                {
                    "start": 1438,
                    "end": 1441,
                    "matchedPaperCorpusId": "17675972"
                },
                {
                    "start": 1649,
                    "end": 1652,
                    "matchedPaperCorpusId": "4712461"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.333740234375
        },
        {
            "corpus_id": "271693304",
            "title": "Reconsidering Degeneration of Token Embeddings with Definitions for Encoder-based Pre-trained Language Models",
            "text": "Limited Models. The scope of this paper is limited to encoder-based PLMs. Although DefinitionEMB can yield effective embeddings for these models by the reconstruction, its denoising autoencoder-based objective function makes it challenging to directly apply it to decoder-only PLMs. This is because decoder-only PLMs use standard causal language modeling (left-to-right), which causes different information flow across layers compared to the masked language modeling, utilized in DefinitionEMB (Voita et al., 2019). While we aimed to emphasize the effectiveness of definition datasets in addressing degeneration for PLMs, future work could explore methods that can be effectively applied to decoder-only PLMs, especially large-scale ones. \n\nLimited Understanding of PLMs' Robustness. One of our findings is that encoder-based PLMs do not degenerate into a cone shape during fine-tuning. While this finding aligns with recent studies that PLMs rarely alter their parameters during fine-tuning (Jain et al., 2024;Panigrahi et al., 2023), it tells only half the story -modifying model parameters before fine-tuning can lead to more effective performance. However, to fully understand why PLMs' embeddings are robust against degeneration during fine-tuning and to pinpoint improvements in the models, the interplay between embedding layers and other components of the model architecture, such as attention layers, could be further studied.",
            "score": 0.5028528622092864,
            "section_title": "Limitations",
            "char_start_offset": 26590,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 15
                },
                {
                    "start": 16,
                    "end": 73
                },
                {
                    "start": 74,
                    "end": 282
                },
                {
                    "start": 283,
                    "end": 515
                },
                {
                    "start": 516,
                    "end": 738
                },
                {
                    "start": 741,
                    "end": 783
                },
                {
                    "start": 784,
                    "end": 886
                },
                {
                    "start": 887,
                    "end": 1151
                },
                {
                    "start": 1152,
                    "end": 1435
                }
            ],
            "ref_mentions": [
                {
                    "start": 494,
                    "end": 514,
                    "matchedPaperCorpusId": "202541078"
                },
                {
                    "start": 992,
                    "end": 1011,
                    "matchedPaperCorpusId": "265308865"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.595703125
        },
        {
            "corpus_id": "268711097",
            "title": "Memory-Augmenting Decoder-Only Language Models through Encoders (Student Abstract)",
            "text": "Our approach consists of building on top of pretrained decoder-only models such as LLaMA (Touvron et al. 2023) and augmenting them with an encoder. This encoder will only take as input the memory (e.g. conversation history, new knowledge, etc.) and will output its encoded version. The decoder (plain LLaMA), will receive the regular input (e.g. a user query) and the output of the encoder (the memory). \n\nWe acknowledge that separately trained encoders may not be able to reach the same performance as models designed to have an encoder since the beginning of the training (Zhong, Lei, and Chen 2022), however we think this method (and our approach) represents a good trade-off between good performance and the computational costs of pretraining a model from scratch. \n\nOur approach is also similar in concept to works that use Adapters to fine-tune language models (Zhang et al. 2023) with the difference that our encoder (replacing the adapter) receives a different input compared to the decoder. While the decoder's goal is still to answer the original query, the encoder receives the memory and encodes it to influence the decoder's output.",
            "score": 0.5019327122938378,
            "section_title": "Methods",
            "char_start_offset": 1918,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 147
                },
                {
                    "start": 148,
                    "end": 201
                },
                {
                    "start": 202,
                    "end": 281
                },
                {
                    "start": 282,
                    "end": 403
                },
                {
                    "start": 406,
                    "end": 768
                },
                {
                    "start": 771,
                    "end": 999
                },
                {
                    "start": 1000,
                    "end": 1145
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.58837890625
        },
        {
            "corpus_id": "273403587",
            "title": "The Mystery of the Pathological Path-star Task for Language Models",
            "text": "We train a NAR model in Exp. 26x to contrast with Exp. 24x. Contrary to expectation, the performance of the NAR model is nearly as good as the IAR model, with D = 5 showing the only difference. We believe this result may not hold under increased task difficulty, especially as M grows, since more samples can offer ground-truth support while bypassing the CHC. As we have not formally defined sensitivity, this remains a conjecture only, with, potentially, some primary counter-evidence. \n\nOne open question is why the task's difficulty increases with D even though RASP analysis indicates the solutions do not depend on D. This may be explained by Z depending on D. It may also relate to the sensitivity conjecture, as D increases the number of possible targets for a given graph. \n\nAnother open question is why increasing the layers helps for the encoder-only model. As there were some successful trials with the smaller model, it can not be the case that the number of layers was a constraint on solving the task, however, it seems easier to find the solution with more layers. \n\nPerhaps the biggest open question is why only the encoder-only model consistently solves the task. If this is solely due to the non-causal parameterization, we would expect the encoder-encoder model to perform the same as the training method is identical (Exps. 19,20,19x vs. 24,25,24x). As this is not the case, there must be a significant difference between these two non-causal parameterizations. This was an unexpected finding. The encoder-only model is unique in that the source-side representation conditions and is dependent on the targetside. This is not true of the decoder-only model due to the causal constraint and not true of the encoder-decoder and encoder-encoder models as both employ a source-side encoder that isolates and prevents it from conditioning on the target-side. \n\nOne explanation for this behaviour is the encoder-only model may learn to write t into the correct target position and then condition on this latent variable when forming the representation of G.",
            "score": 0.4998250362230584,
            "section_title": "Conclusion",
            "char_start_offset": 31491,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 28
                },
                {
                    "start": 29,
                    "end": 54
                },
                {
                    "start": 55,
                    "end": 59
                },
                {
                    "start": 60,
                    "end": 193
                },
                {
                    "start": 194,
                    "end": 360
                },
                {
                    "start": 361,
                    "end": 487
                },
                {
                    "start": 490,
                    "end": 781
                },
                {
                    "start": 784,
                    "end": 868
                },
                {
                    "start": 869,
                    "end": 1080
                },
                {
                    "start": 1083,
                    "end": 1181
                },
                {
                    "start": 1182,
                    "end": 1370
                },
                {
                    "start": 1371,
                    "end": 1482
                },
                {
                    "start": 1483,
                    "end": 1514
                },
                {
                    "start": 1515,
                    "end": 1633
                },
                {
                    "start": 1634,
                    "end": 1873
                },
                {
                    "start": 1876,
                    "end": 2071
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.4541015625
        },
        {
            "corpus_id": "258461112",
            "title": "ContraCLM: Contrastive Learning For Causal Language Model",
            "text": "Compared to the causal (left-to-right) attention mechanism of the decoder-only models, the bidirectional attention mechanism in both encoder-only and encoder-decoder models allows for better leverage of the context of the sequence and hence leads to better representations. \n\nTaking the encoder-only models in Table 7a for illustration, on average, BERT-Base (Devlin et al., 2019) and Roberta-Base (Liu et al., 2019)   between CodeGen and the BERT models trained on programming languages, i.e., CodeBERT (Feng et al., 2020) and GraphCodeBERT (Guo et al., 2021), decreases or even diminishes when evaluated on the code search tasks, the performance gap is still significant as both the model size and pretraining data in CodeGen are much larger than those used by the encoder-only models in Table 7b. Similar trends were observed in the performance gap between the decoder-only and encoderdecoder models on both natural language (Lewis et al., 2020;Raffel et al., 2020) and programming language (Ahmad et al., 2021;Wang et al., 2021). The large performance gap severely limits the decoder-only models used in many discriminative tasks. To this end, contrastive learning shows the promise to largely bridge the gap. As seen in Table 7a, on STS, CONTRACLM reduces the relative performance gap from 67.24% (absolute 21.12%) to 16.17% (absolute 7.33%) regarding BERT-Base, and from 84.62% (absolute 26.64%) to 28.24% (absolute 12.8%). Similarly, Table 7b shows that CONTRACLM outperforms encoder-decoder models and performs comparably to the encoder-only model, GraphCodeBERT. Gao et al. (2021) showed that the dropout-based augmentation is an effective strategy for unsupervised contrastive learning, and the follow-up works (Chuang et al., 2022;Wu et al., 2022) endorse the effectiveness.",
            "score": 0.49778243774477504,
            "section_title": "D.1 Bridge the Gap on Discriminative Tasks",
            "char_start_offset": 38682,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 273
                },
                {
                    "start": 276,
                    "end": 799
                },
                {
                    "start": 800,
                    "end": 1033
                },
                {
                    "start": 1034,
                    "end": 1134
                },
                {
                    "start": 1135,
                    "end": 1213
                },
                {
                    "start": 1214,
                    "end": 1429
                },
                {
                    "start": 1430,
                    "end": 1571
                },
                {
                    "start": 1572,
                    "end": 1785
                }
            ],
            "ref_mentions": [
                {
                    "start": 359,
                    "end": 380,
                    "matchedPaperCorpusId": "52967399"
                },
                {
                    "start": 504,
                    "end": 523,
                    "matchedPaperCorpusId": "211171605"
                },
                {
                    "start": 542,
                    "end": 560,
                    "matchedPaperCorpusId": "221761146"
                },
                {
                    "start": 928,
                    "end": 948,
                    "matchedPaperCorpusId": "204960716"
                },
                {
                    "start": 994,
                    "end": 1014,
                    "matchedPaperCorpusId": "232185260"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.705078125
        },
        {
            "corpus_id": "270703043",
            "title": "Evaluation of Language Models in the Medical Context Under Resource-Constrained Settings",
            "text": "The emergence of pre-trained language models represented a paradigm shift, driving research toward designing more efficient architectures and refining pre-training strategies. These pre-trained models have been commonly adapted or specialized to downstream tasks via fine-tuning, which involves transferring knowledge by further training a model on new data. There are significant advantages demonstrated by these models in language understanding and model performance in various tasks [9,13]. \n\nELMo is one of the earliest attempts at pre-trained language models [15]. This model was developed to capture context-aware word representations by pre-training a bidirectional Long Short-Term Memory (biLSTM) network and fine-tuning it for subsequent downstream tasks. Later the Transformer architecture was introduced, revolutionizing the NLP field by offering highly parallelizable structures and self-attention mechanisms. The Transformer [6] follows the autoencoder archetype, from which three families of models arose: (1) BERT-family or encoder-only models, (2) GPT-family or decoder-only models, and (3) text-to-text or encoder-decoder models. In Fig. 1, the graphical representations of these  Encoder-only models are mainly used for discriminative tasks. Their input is tokenized, and some of these tokens are masked. They are then fed into Transformer blocks with self-attention to obtain contextualized output embeddings, which are further processed by next sentence prediction (NSP) and language model (LM) heads or used by downstream task-specific heads. Depending on the training objective, the NSP head may or may not be necessary. Decoder-only models focus on generation tasks. Their input is tokenized and fed to Transformer blocks with causal self-attention. The causal self-attention ensures that the information flows unidirectionally from left to right. Encoder-decoder models are used for text-to-text tasks. Their encoder processes the input text, similar to encoder-only models but excluding the NSP head, and flows information to the decoder via the cross-attention mechanism. This information is used with the target output so that the decoder learns to produce the latter generatively. families are shown.",
            "score": 0.4959167879024693,
            "section_title": "Pre-trained language models",
            "char_start_offset": 5957,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 175
                },
                {
                    "start": 176,
                    "end": 358
                },
                {
                    "start": 359,
                    "end": 493
                },
                {
                    "start": 496,
                    "end": 569
                },
                {
                    "start": 570,
                    "end": 764
                },
                {
                    "start": 765,
                    "end": 921
                },
                {
                    "start": 922,
                    "end": 1146
                },
                {
                    "start": 1147,
                    "end": 1259
                },
                {
                    "start": 1260,
                    "end": 1322
                },
                {
                    "start": 1323,
                    "end": 1563
                },
                {
                    "start": 1564,
                    "end": 1642
                },
                {
                    "start": 1643,
                    "end": 1689
                },
                {
                    "start": 1690,
                    "end": 1772
                },
                {
                    "start": 1773,
                    "end": 1870
                },
                {
                    "start": 1871,
                    "end": 1926
                },
                {
                    "start": 1927,
                    "end": 2097
                },
                {
                    "start": 2098,
                    "end": 2208
                },
                {
                    "start": 2209,
                    "end": 2228
                }
            ],
            "ref_mentions": [
                {
                    "start": 564,
                    "end": 568,
                    "matchedPaperCorpusId": "3626819"
                },
                {
                    "start": 938,
                    "end": 941,
                    "matchedPaperCorpusId": "13756489"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.5185546875
        },
        {
            "corpus_id": "261494010",
            "title": "Copiloting the Copilots: Fusing Large Language Models with Completion Engines for Automated Program Repair",
            "text": "Recent advances in Natural Language Processing (NLP) have empowered the idea of using Large Language Models (LLMs) that are pre-trained on enormous corpora of natural language and code for various code-related tasks [4,5,10,38,71]. LLMs are based on the transformer architecture [61] that can be categorized into encoderonly, decoder-only and encoder-decoder. Encoder-only models use only the encoder component by training using Masked Language Modeling (MLM) [14] objective where a small percentage (e.g., 15%) of the tokens are masked on. The goal of MLM is to recover these masked tokens given the surrounding context. Encoderonly models such as CodeBERT [15] and GraphCodeBERT [22] are designed to provide a representation of the input code to be used for downstream tasks such as code classification [72]. Decoder-only models, on the other hand, aim to autoregressively generate tokens based on all previously generated tokens. CodeGEN [51,52], Codex [10] and PolyCoder [71] are examples of decoder-only LLMs where they can be used for code autocompletion tasks. Different from encoder-and decoder-only LLMs, encoder-decoder models (e.g., CodeT5 [62,63] and PLBART [3]) combine both encoder and decoder together and jointly train both components together. A commonly used pre-training objective for encoder-decoder models is Masked Span Prediction (MSP) where random spans (multiple consecutive tokens) are replaced with single masked tokens and the models learn to fill in the masked span with the correct sequence of tokens. Furthermore, decoder-only models like InCoder [17] can also perform infilling by training through causal language modeling [2] objective. Instead of using the decoder to predict the next token in the original training data, similar to MSP, InCoder also replaces random spans with masked span tokens. During training, InCoder learns to autoregressively recover the original spans.",
            "score": 0.49589470358197923,
            "section_title": "BACKGROUND AND RELATED WORK 2.1 Large Language Models for Code",
            "char_start_offset": 9083,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 231
                },
                {
                    "start": 232,
                    "end": 359
                },
                {
                    "start": 360,
                    "end": 540
                },
                {
                    "start": 541,
                    "end": 621
                },
                {
                    "start": 622,
                    "end": 810
                },
                {
                    "start": 811,
                    "end": 932
                },
                {
                    "start": 933,
                    "end": 1067
                },
                {
                    "start": 1068,
                    "end": 1260
                },
                {
                    "start": 1261,
                    "end": 1531
                },
                {
                    "start": 1532,
                    "end": 1669
                },
                {
                    "start": 1670,
                    "end": 1831
                },
                {
                    "start": 1832,
                    "end": 1911
                }
            ],
            "ref_mentions": [
                {
                    "start": 219,
                    "end": 221,
                    "matchedPaperCorpusId": "250144196"
                },
                {
                    "start": 224,
                    "end": 227,
                    "matchedPaperCorpusId": "246527904"
                },
                {
                    "start": 227,
                    "end": 230,
                    "matchedPaperCorpusId": "247158549"
                },
                {
                    "start": 279,
                    "end": 283,
                    "matchedPaperCorpusId": "13756489"
                },
                {
                    "start": 460,
                    "end": 464,
                    "matchedPaperCorpusId": "52967399"
                },
                {
                    "start": 681,
                    "end": 685,
                    "matchedPaperCorpusId": "221761146"
                },
                {
                    "start": 975,
                    "end": 979,
                    "matchedPaperCorpusId": "247158549"
                },
                {
                    "start": 1155,
                    "end": 1158,
                    "matchedPaperCorpusId": "237386541"
                },
                {
                    "start": 1578,
                    "end": 1582,
                    "matchedPaperCorpusId": "248157108"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.75537109375
        },
        {
            "corpus_id": "211003686",
            "title": "Teaching Machines to Converse",
            "text": ".., y t\u22121 , x) (2.14)\n\nBy comparing Eq.2.14 with the conditional probability of language modeling in Eq. 2.1, we can see that the only difference between them is the additional consideration of the source input x.\n\nMore specifically, a standard SEQ2SEQ model consists of two key components, a encoder, which maps the source input x to a vector representation, and a decoder, which generates an output sequence based on the source sentence. Both the encoder and the decoder are multi-layer LSTMs. To enable the encoder to access information from the encoder, the last state memory of the encoder is passed to the decoder as the initial memory state, based on which words are sequentially predicted using a softmax function. Commonly, input and output use different LSTMs with separate compositional parameters to capture different compositional patterns.",
            "score": 0.49474632501742033,
            "section_title": "Sequence-to-Sequence Generation",
            "char_start_offset": 42596,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.360107421875
        },
        {
            "corpus_id": "207780015",
            "title": "On the Linguistic Representational Power of Neural Machine Translation Models",
            "text": "The decoder DEC is a crucial part in an MT system with access to both source-side representations and partially generated target-side representations, which it uses to generate the next target word. We now examine whether the representations learned on the decoder-side possess the same amount of morphological knowledge as the encoder side. To probe this, we flipped the language direction and trained NMT systems with English\u2192{German, Czech, Russian} configurations. Then, we use the trained model to encode a source sentence and generate features for words in the target sentence. These features are used to train a classifier on morphological tagging on the target side. Note that in this case the decoder is given the correct target words one by one, similar to the usual NMT training regime. The right-hand side of Figure 4 shows a similar performance trend as in the case of encoder-side representations, with character units performing the best and word units performing the worst. Again, morphological units performed better than the BPE-based units.\n\nComparing encoder representations with decoder representations, it is interesting to see that in several cases the decoder-side representations performed better than the encoder-side representations, even though they are trained using a unidirectional LSTM only. Because we did not see any notable trends in differences between encoder and decoder side representations, we only present the encoder-side results in the rest of the paper.",
            "score": 0.49393817564961706,
            "section_title": "Encoder versus Decoder Representations",
            "char_start_offset": 45322,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.197265625
        },
        {
            "corpus_id": "272969109",
            "title": "LangSAMP: Language-Script Aware Multilingual Pretraining",
            "text": "Multilingual pretrained language models (mPLMs) are models that are trained on many languages, with one or multiple self-supervised objectives, such as masked language modeling (MLM) (Devlin et al., 2019) or causal language modeling (Radford et al., 2019). These models can be generally classified as encoder-only (Devlin et al., 2019;Conneau et al., 2020;Liang et al., 2023), encoder-decoder (Liu et al., 2020;Fan et al., 2021;Xue et al., 2021), and decoder-only models (Lin et al., 2022;Shliazhko et al., 2022;Scao et al., 2022). Decoder-only models that have considerably many parameters and are pretrained on a lot of data are also referred to as large language models (LLMs) (Achiam et al., 2023;Touvron et al., 2023;\u00dcst\u00fcn et al., 2024), which are good at natural language generation tasks, typically for high-and medium-resource languages. In parallel, some recent encoder-only models attempt to scale horizontally, i.e., cover more languages, especially low-resource ones (Ogueji et al., 2021;Alabi et al., 2022;ImaniGooghari et al., 2023;Liu et al., 2024a). These highly multilingual encoder-only models are particularly good at understanding tasks in a zero-shot crosslingual fashion.",
            "score": 0.49288149970432804,
            "section_title": "Multilingual Pretrained Language Models",
            "char_start_offset": 5341,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 256
                },
                {
                    "start": 257,
                    "end": 531
                },
                {
                    "start": 532,
                    "end": 845
                },
                {
                    "start": 846,
                    "end": 1065
                },
                {
                    "start": 1066,
                    "end": 1193
                }
            ],
            "ref_mentions": [
                {
                    "start": 233,
                    "end": 255,
                    "matchedPaperCorpusId": "160025533"
                },
                {
                    "start": 471,
                    "end": 489,
                    "matchedPaperCorpusId": "248721770"
                },
                {
                    "start": 979,
                    "end": 1000,
                    "matchedPaperCorpusId": "240225648"
                },
                {
                    "start": 1000,
                    "end": 1019,
                    "matchedPaperCorpusId": "252088953"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.37353515625
        },
        {
            "corpus_id": "258309400",
            "title": "State Spaces Aren\u2019t Enough: Machine Translation Needs Attention",
            "text": "Following Gu et al. (2022), our architectures are based on the Transformer, but with the S4 block (Section 2) replacing self-attention. In our initial experiments, we intentionally omitted the use of crossattention in our models to determine whether S4's internal states alone suffice in capturing long-range dependencies for MT. We call the B consecutive S4 blocks together with the MLP layer, followed by a residual connection and normalization, one S4 layer. Gu et al. (2022) use B = 2. \n\nWe consider two approaches (Figure 1b): a decoderonly model (\u2205\u2212S4), and an encoder-decoder architecture (S4-S4). Our decoder-only model is based on Gu et al. (2022), which was shown to perform well in language modeling. This model is designed to predict the next target token by taking as input the concatenated source and the previously predicted target tokens. Our S4-S4 encoder-decoder architecture consists of L E S4 encoder layers and L D S4 decoder layers, without cross-attention. Instead, we use a simple method to propagate information between the encoder and the decoder: concatenating the encoder outputs with the shifted target sequence. This way, the decoder processes both the encoder outputs and the target tokens.2 \n\nFinally, for some of the latter experiments, we consider the case where encoder is bidirectional, which we will refer to as S4BI. In this configuration, the S4 blocks have two sets of parameters (A, B and C), one per direction.",
            "score": 0.4924720927268792,
            "section_title": "Base Architecture",
            "char_start_offset": 7409,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 135
                },
                {
                    "start": 136,
                    "end": 329
                },
                {
                    "start": 330,
                    "end": 461
                },
                {
                    "start": 462,
                    "end": 489
                },
                {
                    "start": 492,
                    "end": 604
                },
                {
                    "start": 605,
                    "end": 711
                },
                {
                    "start": 712,
                    "end": 854
                },
                {
                    "start": 855,
                    "end": 979
                },
                {
                    "start": 980,
                    "end": 1141
                },
                {
                    "start": 1142,
                    "end": 1222
                },
                {
                    "start": 1225,
                    "end": 1354
                },
                {
                    "start": 1355,
                    "end": 1452
                }
            ],
            "ref_mentions": [
                {
                    "start": 10,
                    "end": 26,
                    "matchedPaperCorpusId": "240354066"
                },
                {
                    "start": 462,
                    "end": 478,
                    "matchedPaperCorpusId": "240354066"
                },
                {
                    "start": 640,
                    "end": 656,
                    "matchedPaperCorpusId": "240354066"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.438720703125
        },
        {
            "corpus_id": "267301268",
            "title": "Looking Right is Sometimes Right: Investigating the Capabilities of Decoder-only LLMs for Sequence Labeling",
            "text": "Pre-trained language models (PLMs) built upon the Transformer architecture have demonstrated exceptional performance across many natural language understanding (NLU) tasks (Radford et al., 2018;Devlin et al., 2019;Yang et al., 2019;Clark et al., 2020;Raffel et al., 2020). Typically, achieving state-of-the-art (SOTA) results in tasks such as sequence classification and sequence labeling Figure 1: Layer-wise causal mask removal from decoder block groups in a decoder-only LLM. Here, the causal mask is removed from the top eight decoder blocks of the Llama2-7B model to enable bidirectionality during fine-tuning, which proves beneficial for many SL tasks. \n\ninvolves a two-step process: pre-training on unlabeled corpora, followed by fine-tuning on taskspecific data -a process often referred to as transfer learning (Ruder et al., 2019;Raffel et al., 2020). Two prevailing architectures emerged, each coupled with a compatible pre-training paradigm: (1) the decoder-only architecture, utilizing causal language modeling (CLM) for pre-training, and (2) the encoder-only architecture, with the masked language modeling (MLM) pre-training objective. \n\nIn transfer learning experiments that juxtapose models of a comparable number of parameters, MLM-based encoders consistently outperformed CLM-based decoders on NLU tasks (Devlin et al., 2019).2 However, a shift in strategy emerged within the NLP community when encoder models ceased being scaled up to the same magnitude of parameters and pre-training data as their decoder counterparts. Consequently, there has been a pronounced trend toward scaling decoder models to multiple billion parameters, leading to a proliferation of large language models (LLMs). Combining LLM text generation capabilities with various prompting strategies can boost the performance on many NLU tasks, eliminating the need for fine-tuning model parameters (Liu et al., 2023).",
            "score": 0.4923673418110025,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 272
                },
                {
                    "start": 273,
                    "end": 478
                },
                {
                    "start": 479,
                    "end": 658
                },
                {
                    "start": 661,
                    "end": 861
                },
                {
                    "start": 862,
                    "end": 1150
                },
                {
                    "start": 1153,
                    "end": 1346
                },
                {
                    "start": 1347,
                    "end": 1540
                },
                {
                    "start": 1541,
                    "end": 1710
                },
                {
                    "start": 1711,
                    "end": 1906
                }
            ],
            "ref_mentions": [
                {
                    "start": 194,
                    "end": 214,
                    "matchedPaperCorpusId": "52967399"
                },
                {
                    "start": 214,
                    "end": 232,
                    "matchedPaperCorpusId": "195069387"
                },
                {
                    "start": 251,
                    "end": 271,
                    "matchedPaperCorpusId": "204838007"
                },
                {
                    "start": 820,
                    "end": 840,
                    "matchedPaperCorpusId": "186206211"
                },
                {
                    "start": 840,
                    "end": 860,
                    "matchedPaperCorpusId": "204838007"
                },
                {
                    "start": 1323,
                    "end": 1344,
                    "matchedPaperCorpusId": "52967399"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.5615234375
        },
        {
            "corpus_id": "277626915",
            "title": "GOLLuM: Gaussian Process Optimized LLMs - Reframing LLM Finetuning through Bayesian Optimization",
            "text": "Decoder-only architectures, exemplified by models in the GPT 3 family, generate outputs autoregressively by predicting each token conditioned on all previous ones. While traditionally used for generation tasks, these models can also produce dense representations of input text by extracting hidden states from specific tokens (e.g., the final token or special marker tokens). Decoder-only models are typically pretrained with causal language modeling objectives and operate unidirectionally, which distinguishes their contextual encoding behavior from encoder-based models. \n\nIn this work, we evaluate several decoder-style models for embedding chemical procedures and reaction descriptions: OpenAI Embeddings 54 A widely-used commercial API that provides text embeddings via proprietary transformer models. While the architectural details are not public, we assign them to decoder-style GPT family. \n\nQwen2-7B-Instruct 52 A large-scale instruction-tuned language model from Alibaba, based on a decoder-only architecture. We use this model in embedding mode by extracting the hidden state of the last non-padding token. \n\nGTE-Qwen2-7B-Instruct 53 A retrieval-optimized variant of Qwen2, finetuned to produce sentencelevel embeddings with improved performance on similarity and ranking tasks. \n\nLLaMA 3-8B 50 Meta's open LLaMA 3 model in its original instruction-tuned form, without additional adaptation for embeddings. \n\nLLM2Vec Models 51 We also evaluate decoder-only LLMs adapted for embedding tasks using the LLM2Vec framework ? . These models, such as LLM2Vec-Meta-Llama-3 and LLM2Vec-Mistral-7B, are trained with masked next token prediction (MNTP) to enable bidirectional context modeling and use supervised mean pooling over selected internal layers. This adaptation allows decoder-only transformers to behave similarly to encoder models in embedding quality, while preserving their original architecture.",
            "score": 0.49208915367559586,
            "section_title": "C.3.4 Decoder-Only Models",
            "char_start_offset": 48827,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 163
                },
                {
                    "start": 164,
                    "end": 375
                },
                {
                    "start": 376,
                    "end": 573
                },
                {
                    "start": 576,
                    "end": 807
                },
                {
                    "start": 808,
                    "end": 899
                },
                {
                    "start": 902,
                    "end": 1021
                },
                {
                    "start": 1022,
                    "end": 1119
                },
                {
                    "start": 1122,
                    "end": 1291
                },
                {
                    "start": 1294,
                    "end": 1419
                },
                {
                    "start": 1422,
                    "end": 1534
                },
                {
                    "start": 1535,
                    "end": 1758
                },
                {
                    "start": 1759,
                    "end": 1913
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.611328125
        },
        {
            "corpus_id": "258309400",
            "title": "State Spaces Aren\u2019t Enough: Machine Translation Needs Attention",
            "text": "Let (x 1:n , y 1:m ) be a source and target sentence pair. The negative log-likelihood of y given x can be written as: \n\nwhere p(y i | x 1:n , y <i ) is modeled using a neural network. In encoder-decoder models, such as the Transformer (Vaswani et al., 2017), the model has two main components: an encoder, responsible for capturing source-side dependencies, and a decoder, which captures both target-side and source-target dependencies. \n\nAlternatively, MT can be treated as a Language Modeling task, where the (decoder-only) model is trained on the concatenated source and target sentences, separated with a special [SEP] token in between (Wang et al., 2021;Gao et al., 2022). Following this approach, the negative log-likelihood is written as: \n\nThe L AE term corresponds to the source reconstruction loss, while L M T is identical to Equation (6). \n\nSince our focus is on MT, we only need to optimize the second term, i.e., L M T . In our experiments, including both loss terms degraded translation quality (see Appendix A). Therefore, for our decoder-only models using only the second term, L M T .",
            "score": 0.49120816809851753,
            "section_title": "Machine Translation (MT)",
            "char_start_offset": 5669,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 58
                },
                {
                    "start": 59,
                    "end": 118
                },
                {
                    "start": 121,
                    "end": 184
                },
                {
                    "start": 185,
                    "end": 437
                },
                {
                    "start": 440,
                    "end": 678
                },
                {
                    "start": 679,
                    "end": 746
                },
                {
                    "start": 749,
                    "end": 851
                },
                {
                    "start": 854,
                    "end": 935
                },
                {
                    "start": 936,
                    "end": 1028
                },
                {
                    "start": 1029,
                    "end": 1103
                }
            ],
            "ref_mentions": [
                {
                    "start": 236,
                    "end": 258,
                    "matchedPaperCorpusId": "13756489"
                },
                {
                    "start": 660,
                    "end": 677,
                    "matchedPaperCorpusId": "253080830"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.260009765625
        },
        {
            "corpus_id": "248157514",
            "title": "A Unified Cascaded Encoder ASR Model for Dynamic Model Sizes",
            "text": "The original cascaded encoder model [22] uses a shared RNN-T decoder. The decoder works with a causal encoder in the first pass to provide streaming recognition results, and works with an additional non-causal encoder that sits on top of the causal encoder to provide more accurate final results, leveraging audio right context extracted by the noncausal encoder. Therefore, the same decoder has to deal with features of different context, and we observe tension between the performance of the passes as we try to reduce the model size, i.e., as we assign more loss weights for the causal pass to satisfy WER target, the accuracy of the non-causal pass degrades. \n\nIn this work, we propose to use smaller separate decoders in each sub-model, to better cope with the different context, and this significantly alleviates the tension between different submodels:",
            "score": 0.49048942896748615,
            "section_title": "Separate decoders",
            "char_start_offset": 6492,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 69
                },
                {
                    "start": 70,
                    "end": 363
                },
                {
                    "start": 364,
                    "end": 662
                },
                {
                    "start": 665,
                    "end": 859
                }
            ],
            "ref_mentions": [
                {
                    "start": 36,
                    "end": 40,
                    "matchedPaperCorpusId": "225094578"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.66259765625
        },
        {
            "corpus_id": "267751001",
            "title": "Metric-Learning Encoding Models Identify Processing Profiles of Linguistic Features in BERT's Representations",
            "text": "Decoding and encoding methods have different merits and limitations. One main limitation of decoding methods is that the decodability of a given feature does not guarantee its causal role. For instance, a certain feature can be decodable from neural activations not because it has a mechanistic role but only because it correlates with another feature that has such a role. In encoding models, this limitation can be addressed to some extent by introducing the feature of interest and confound features, testing their relative importance in predicting neural activations. However, a common limitation of encoding models is that they are uni-rather than multi-variate, where the goal is typically to predict the neural activity of one single unit of the model at a time, from a single electrode in the brain or from a single fMRI voxel. Encoding approaches are thus limited in their ability to study distributed representations across many units. \n\nHere, we introduce a simple approach, which preserves the good from both worlds, by extending encoding methods to the multivariate case within a metric-learning framework (Kulis, 2013). We call it Metric-Learning Encoding Models, or MLEMs for short. To study the neural encoding of linguistic features, we created four new datasets, whose stimuli contrast various linguistic features. We then presented stimuli from these datasets to BERT (Devlin et al., 2019), extracted its neural activations and studied them using MLEMs. \n\nThe main contributions of our study are: (1) A new framework to study neural encoding in large language models; (2) A new set of probing datasets with their corresponding generating codes; (3) Identification of orders among linguistic features, for all model layers, with respect to their dominance in the neural representations; (4) Identification of hierarchical patterns in the neural representations of linguistic information; (5) Identification of a strong disentanglement of linguistic features in layers of BERT, discovered by contrasting uni-and multivariate encoding models; (6) Demonstration of the limitation of multivariate decoding methods compared to encoding approaches.",
            "score": 0.48973020524518585,
            "section_title": "Introduction",
            "char_start_offset": 2011,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 68
                },
                {
                    "start": 69,
                    "end": 188
                },
                {
                    "start": 189,
                    "end": 373
                },
                {
                    "start": 374,
                    "end": 571
                },
                {
                    "start": 572,
                    "end": 835
                },
                {
                    "start": 836,
                    "end": 945
                },
                {
                    "start": 948,
                    "end": 1133
                },
                {
                    "start": 1134,
                    "end": 1197
                },
                {
                    "start": 1198,
                    "end": 1332
                },
                {
                    "start": 1333,
                    "end": 1472
                },
                {
                    "start": 1475,
                    "end": 2160
                }
            ],
            "ref_mentions": [
                {
                    "start": 1119,
                    "end": 1132,
                    "matchedPaperCorpusId": "55485900"
                },
                {
                    "start": 1387,
                    "end": 1408,
                    "matchedPaperCorpusId": "52967399"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.1229248046875
        },
        {
            "corpus_id": "3518190",
            "title": "Unsupervised Machine Translation Using Monolingual Corpora Only",
            "text": "The translation model we propose is composed of an encoder and a decoder, respectively responsible for encoding source and target sentences to a latent space, and to decode from that latent space to the source or the target domain. We use a single encoder and a single decoder for both domains (Johnson et al., 2016). The only difference when applying these modules to different languages is the choice of lookup tables. \n\nLet us denote by W S the set of words in the source domain associated with the (learned) words embeddings Z S = (z s 1 , ...., z s |W S | ), and by W T the set of words in the target domain associated with the embeddings Z T = (z t 1 , ...., z t |W T | ), Z being the set of all the embeddings. Given an input sentence of m words x = (x 1 , x 2 , ..., x m ) in a particular language , \u2208 {src, tgt}, an encoder e \u03b8enc,Z (x, ) computes a sequence of m hidden states z = (z 1 , z 2 , ..., z m ) by using the corresponding word embeddings, i.e. Z S if = src and Z T if = tgt; the other parameters \u03b8 enc are instead shared between the source and target languages. For the sake of simplicity, the encoder will be denoted as e(x, ) in the following. These hidden states are vectors in R n , n being the dimension of the latent space. \n\nA decoder d \u03b8 dec ,Z (z, ) takes as input z and a language , and generates an output sequence y = (y 1 , y 2 , ..., y k ), where each word y i is in the corresponding vocabulary W . This decoder makes use of the corresponding word embeddings, and it is otherwise parameterized by a vector \u03b8 dec that does not depend on the output language. It will thus be denoted d(z, ) in the following.",
            "score": 0.48948388559965766,
            "section_title": "NEURAL MACHINE TRANSLATION MODEL",
            "char_start_offset": 5969,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 231
                },
                {
                    "start": 232,
                    "end": 317
                },
                {
                    "start": 318,
                    "end": 420
                },
                {
                    "start": 423,
                    "end": 717
                },
                {
                    "start": 718,
                    "end": 963
                },
                {
                    "start": 964,
                    "end": 1081
                },
                {
                    "start": 1082,
                    "end": 1165
                },
                {
                    "start": 1166,
                    "end": 1249
                },
                {
                    "start": 1252,
                    "end": 1433
                },
                {
                    "start": 1434,
                    "end": 1591
                },
                {
                    "start": 1592,
                    "end": 1640
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.371826171875
        },
        {
            "corpus_id": "237635166",
            "title": "Unsupervised Translation of German\u2013Lower Sorbian: Exploring Training and Novel Transfer Methods on a Low-Resource Language",
            "text": "We used the MASS (Song et al., 2019) model, which is a 12-layer encoder-decoder (6 layers each) Transformer model identical to the XLM (Lample and Conneau, 2019) architecture. The difference comes in the training, using the MASS sequence masking (MA) objective allows both the encoder and decoder to be trained in the language model pretraining phase. This can be contrasted with XLM, which only pretrains the encoder.",
            "score": 0.48914676519470707,
            "section_title": "Architecture",
            "char_start_offset": 6302,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 175
                },
                {
                    "start": 176,
                    "end": 351
                },
                {
                    "start": 352,
                    "end": 418
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.0567626953125
        },
        {
            "corpus_id": "227127650",
            "title": "A Better and Faster end-to-end Model for Streaming ASR",
            "text": "A big goal of two-pass models is to make up for the quality degradation in the 1st-pass. To improve our current two-pass approach, which is to run rescoring, we look to run a beam search instead. One of the challenges with this beam search is that we need the model to be robust to decoding short search utterances as well as long caption utterances. Typically attention-based models do very poorly on long-form [21]. \n\nTo address this, we look at running a 2nd-pass beam search with RNN-T itself, which has shown better robustness to long-form [25]. The model explored, which we call Cascaded Encoders, is shown in Figure 2. To add the non-causal aspect of LAS (which improves quality) [17] into a streaming, causal RNN-T system, we add additional non-causal encoder layers on top of the causal encoder layers. The 1st-pass uses only the causal encoder and the RNN-T decoder. In the second pass, the additional non-causal layers take in both left and right context of the 1st-pass encoder outputs, and again feeds to the same decoder. A single RNN-T decoder is shared between the 1st and 2nd-passes for smaller model size and on-device benefits.",
            "score": 0.4886497652407017,
            "section_title": "Two-pass with Cascaded Encoders",
            "char_start_offset": 4458,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 88
                },
                {
                    "start": 89,
                    "end": 195
                },
                {
                    "start": 196,
                    "end": 350
                },
                {
                    "start": 351,
                    "end": 417
                },
                {
                    "start": 420,
                    "end": 550
                },
                {
                    "start": 551,
                    "end": 811
                },
                {
                    "start": 812,
                    "end": 876
                },
                {
                    "start": 877,
                    "end": 1035
                },
                {
                    "start": 1036,
                    "end": 1146
                }
            ],
            "ref_mentions": [
                {
                    "start": 412,
                    "end": 416,
                    "matchedPaperCorpusId": "207880479"
                },
                {
                    "start": 545,
                    "end": 549,
                    "matchedPaperCorpusId": "204900916"
                },
                {
                    "start": 687,
                    "end": 691,
                    "matchedPaperCorpusId": "201667597"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.29150390625
        },
        {
            "corpus_id": "220274104",
            "title": "Supertagging with CCG primitives",
            "text": "The most recent, highest-performing supertaggers are all based on bidirectional LSTM architectures. At each time step, the forward and backward LSTM outputs are combined and fed through a softmax layer to produce a distribution over categories. In order to construct a supertagger that works at the level of primitives, we propose a model that replaces the softmax prediction layer with a separate LSTM that predicts primitives in a manner similar to the decoder in RNN encoderdecoder architectures (Cho et al., 2014;Sutskever et al., 2014), or to how text is generated from neural language models. \n\nIn encoder-decoder LSTM models, an encoder LSTM is run over the input sequence. The final LSTM cell is used to initialize the decoder's LSTM cell, after which the decoder is trained to predict the output sequence. During training, the decoder receives as input the correct output for time t \u2212 1, and asked to predict the output for time t. During inference, the model makes its predictions autoregressively, since the correct previous output is unknown at test time. Output sequences are padded with [START] and [STOP] symbols: the former allows the model to learn a distribution over initial output symbols, as well as providing a means to trigger the output sequence prediction process (e.g., after an input sentence has been read by the encoder); the latter is how the decoder indicates its completion of the current sequence. \n\nStandard use cases for encoder-decoder models, such as machine translation, have the property that the output sequence lengths are not easily determinable from the input sequence lengths; nor is there an easy, strictly monotonic correspondence between input and output tokens. The usual application of encoder-decoder models handles this discrepancy by mostly separating the encoding and decoding parts of the model, leaving them connected only at their ends (i.e., via the copying of the encoder's hidden state to the decoder's).",
            "score": 0.4884083638034638,
            "section_title": "Decoding sequences of primitives",
            "char_start_offset": 13476,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 99
                },
                {
                    "start": 100,
                    "end": 244
                },
                {
                    "start": 245,
                    "end": 598
                },
                {
                    "start": 601,
                    "end": 680
                },
                {
                    "start": 681,
                    "end": 814
                },
                {
                    "start": 815,
                    "end": 940
                },
                {
                    "start": 941,
                    "end": 1067
                },
                {
                    "start": 1068,
                    "end": 1430
                },
                {
                    "start": 1433,
                    "end": 1709
                },
                {
                    "start": 1710,
                    "end": 1963
                }
            ],
            "ref_mentions": [
                {
                    "start": 499,
                    "end": 517,
                    "matchedPaperCorpusId": "5590763"
                },
                {
                    "start": 517,
                    "end": 540,
                    "matchedPaperCorpusId": "7961699"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.4453125
        },
        {
            "corpus_id": "273654074",
            "title": "Language Agents Meet Causality - Bridging LLMs and Causal World Models",
            "text": "We replace the action encoding R t in the CRL framework with a language-based representation L e (L t ), where L e embeds a natural language description L t . This is implemented using an encoder-only language model (Reimers & Gurevych, 2019) with a trainable head, replacing the original action encodings in the CRL framework's transition model R t = L e (L t ) (see also Section 4.2). \n\nDecoder The decoder G comprises two parts: the causal mapper and the state description generator. The causal mapper m \u03b8 extracts causal variables C from the learned disentangled representations z. It first identifies which latent dimensions z i are most predictive for each causal variable C j , then learns to perform the actual mapping. The state description generator s maps the estimated causal variables \u0108 to \u2113, a natural language description of the state. Detailed implementations of these components are provided in Appendix F and G respectively.",
            "score": 0.48794712131941664,
            "section_title": "Language-Based Action Representations",
            "char_start_offset": 14389,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 158
                },
                {
                    "start": 159,
                    "end": 386
                },
                {
                    "start": 389,
                    "end": 486
                },
                {
                    "start": 487,
                    "end": 585
                },
                {
                    "start": 586,
                    "end": 727
                },
                {
                    "start": 728,
                    "end": 850
                },
                {
                    "start": 851,
                    "end": 942
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.1976318359375
        },
        {
            "corpus_id": "258887689",
            "title": "Mixture-of-Expert Conformer for Streaming Multilingual ASR",
            "text": "We use a language agnostic multilingual model similar to [8] as the baseline. The baseline model consists of a 7-layer causal Conformer encoder and a 10-layer non-causal cascaded encoder. The causal encoder includes two blocks separated by a stacking layer. The first block consists of an input projection layer and 3 convolution layers. The stacking layer concatenates two neighboring encodings in time to form a 60-ms frame rate. The second block starts with a 1024-dim Conformer layer, and then a projection layer to reduce the model dimension back to 512 for the rest of the causal layers. Note that the causal Con- former layers uses causal convolution and left-context attention and is thus strictly causal. Secondly, the non-causal layers are cascaded [31] to the causal encoder output. The 10 layers of non-causal Conformer layers have a dimension of 640, and a total right-context of 0.9 sec. We use separate decoders for causal and non-causal encoders to achieve the best quality. \n\nEach transducer decoder consists of a prediction network and a joint network [28]. For the prediction network, we use two embedding layers to embed current and previous tokens separately and concatenate the embeddings as output. The joint network is a single feed-forward layer of 640 units. We use a hybrid autoregressive transducer (HAT) version of the decoder [32]. A softmax is used to predict 16,384 wordpieces. We generate the wordpieces using mixed transcripts pooled from all languages. The baseline multilingual transducer model has a total of 180M parameters.",
            "score": 0.4879088130629483,
            "section_title": "Baseline Multilingual Model",
            "char_start_offset": 8146,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 77
                },
                {
                    "start": 78,
                    "end": 187
                },
                {
                    "start": 188,
                    "end": 257
                },
                {
                    "start": 258,
                    "end": 337
                },
                {
                    "start": 338,
                    "end": 431
                },
                {
                    "start": 432,
                    "end": 593
                },
                {
                    "start": 594,
                    "end": 713
                },
                {
                    "start": 714,
                    "end": 793
                },
                {
                    "start": 794,
                    "end": 901
                },
                {
                    "start": 902,
                    "end": 990
                },
                {
                    "start": 993,
                    "end": 1075
                },
                {
                    "start": 1076,
                    "end": 1221
                },
                {
                    "start": 1222,
                    "end": 1284
                },
                {
                    "start": 1285,
                    "end": 1361
                },
                {
                    "start": 1362,
                    "end": 1409
                },
                {
                    "start": 1410,
                    "end": 1487
                },
                {
                    "start": 1488,
                    "end": 1562
                }
            ],
            "ref_mentions": [
                {
                    "start": 759,
                    "end": 763,
                    "matchedPaperCorpusId": "225094578"
                },
                {
                    "start": 1356,
                    "end": 1360,
                    "matchedPaperCorpusId": "212737031"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.55908203125
        },
        {
            "corpus_id": "269981935",
            "title": "Bitune: Bidirectional Instruction-Tuning",
            "text": "Our approach shares similarities with the concept of \"prefix language modeling\", which enables a decoder-only model to handle bidirectional context within a prefix (instruction) while maintaining causal generation for the output sequence.The prefix-LM architecture was introduced by Liu et al.\n\n(2018) and further explored and popularized by Raffel et al. (2020).In their work on T5, Raffel et al. (2020) pretrained the prefix-LM architecture alongside other architectures, such as encoder-decoder and decoder-only models, demonstrating that prefix-LM outperforms decoder-only models on both training objectives: denoising and language modeling.\n\nThe prefix-LM approach has been used in UniLM (Dong et al., 2019), which trains a single transformer on three types of language modeling tasks: unidirectional, bidirectional, and sequence-to-sequence prediction.UniLM employs a shared Transformer network and utilizes specific self-attention masks to control the context that predictions are conditioned on, where the sequence-to-sequence task is equivalent to the prefix-LM approach.\n\nAdditionally, UL2 (Tay et al., 2023) introduces a pretraining objective called \"Mixture of Denoisers\", which combines various denoising strategies, including the prefix-LM approach.Lastly, XLNet Yang et al. ( 2019) also allows for non-causal word ordering by allowing random permutations to be used with a next-token prediction objective.\n\nAll these works focused on the model pretraining.As for the utilization of pretrained causal language models, Springer et al. (2024) show in their work that simply repeating the input to these models improves the quality of token embeddings for text-retrieval.This work addresses the limitation that token embeddings in autoregressive models cannot contain information from tokens appearing later in the input.By repeating the input twice, the early tokens are allowed to encode information Table 6: Qualitative results on Llama3-8B.We show a response for LoRA and Bitune.\n\n[GSM8K] Question:\n\nJanet's ducks lay 16 eggs per day.",
            "score": 0.48679340259892623,
            "section_title": "Related Work",
            "char_start_offset": 19262,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 238
                },
                {
                    "start": 238,
                    "end": 293
                },
                {
                    "start": 295,
                    "end": 363
                },
                {
                    "start": 363,
                    "end": 645
                },
                {
                    "start": 647,
                    "end": 858
                },
                {
                    "start": 858,
                    "end": 1080
                },
                {
                    "start": 1082,
                    "end": 1263
                },
                {
                    "start": 1263,
                    "end": 1420
                },
                {
                    "start": 1422,
                    "end": 1471
                },
                {
                    "start": 1471,
                    "end": 1682
                },
                {
                    "start": 1682,
                    "end": 1832
                },
                {
                    "start": 1832,
                    "end": 1955
                },
                {
                    "start": 1955,
                    "end": 1994
                },
                {
                    "start": 1996,
                    "end": 2013
                },
                {
                    "start": 2015,
                    "end": 2049
                }
            ],
            "ref_mentions": [
                {
                    "start": 342,
                    "end": 362,
                    "matchedPaperCorpusId": "204838007"
                },
                {
                    "start": 384,
                    "end": 404,
                    "matchedPaperCorpusId": "204838007"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.29443359375
        },
        {
            "corpus_id": "276771845",
            "title": "Adapting Decoder-Based Language Models for Diverse Encoder Downstream Tasks",
            "text": "This work explores adapting decoder-only generative AI models, exemplified by GPT-4 (Ope-nAI, 2023), Gemini (Gemini Team, 2023), and Gemma (Gemma Team, 2024a,b), for encoderonly tasks such as classification, regression, and ranking. Our adaptation strategy, as shown in Figure 1, centers on three key architectural and training modifications: attention mechanism design, pooling strategies, and the application of dropout. \n\nWhile both decoder-only and encoder-only Transformers share a similar underlying layer design, their training objectives and downstream applications diverge significantly. Decoder-only models are optimized for next-token prediction, making them well-suited for generative tasks. In contrast, encoder-only models are trained to generate a comprehensive representation of the entire input sequence, empowering a diverse range of predictive tasks, including classification (label prediction), regression (score prediction), and ranking (relative ordering prediction). \n\nBy systematically evaluating these crucial modeling choices, we aim to identify the architec-ture that best captures the essential information embedded within the input sequence. This will ultimately optimize the performance of Gemma Encoder on downstream tasks. Our analysis provides valuable insights into the trade-offs inherent in various design decisions, offering guidance not only for adapting Gemma, but also for transforming other decoder-only models into effective encoders.",
            "score": 0.4850864007318094,
            "section_title": "Model Adaptation Choices",
            "char_start_offset": 2532,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 232
                },
                {
                    "start": 233,
                    "end": 422
                },
                {
                    "start": 425,
                    "end": 596
                },
                {
                    "start": 597,
                    "end": 703
                },
                {
                    "start": 704,
                    "end": 989
                },
                {
                    "start": 992,
                    "end": 1170
                },
                {
                    "start": 1171,
                    "end": 1254
                },
                {
                    "start": 1255,
                    "end": 1476
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.62548828125
        },
        {
            "corpus_id": "269188354",
            "title": "Small Language Models Are Good Too: An Empirical Study of Zero-Shot Classification",
            "text": "We compare the performance of the LLM models on several datasets, studying the correlation with the number of parameters, the impact of the architecture, and the type of training strategy (instruction or not).Then, for the two types of architectures (encoder-decoder & decoder-only), we study the impact of the instruction-tuning and the different scoring functions to understand the discriminating factors on performance.",
            "score": 0.4843290498313657,
            "section_title": "Results",
            "char_start_offset": 10005,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 209
                },
                {
                    "start": 209,
                    "end": 422
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.176513671875
        },
        {
            "corpus_id": "250311056",
            "title": "Improving Streaming End-to-End ASR on Transformer-based Causal Models with Encoder States Revision Strategies",
            "text": "However, this work only involves the decoder part without any modification to the encoder, and the mask-predict operation must be conducted after the entire block has been calculated. In [23], authors propose to use dynamic latency and revise the state of encoder and decoder so that both streaming and non-streaming models can apply incremental decoding [24]. This method revises the states that have not been completed for RNN-T models and can not change the previous information of the encoder either. In practical applications, to avoid distracting the user's attention, it is more desirable to ensure a fast response and previous stable outputs simultaneously [25]. The above methods will significantly change the previous outputs when the encoder states are inaccurate, which can not be used to improve causal models. \n\nIn this paper, we explore a new method to improve causal models, called encoder states revision strategy. Specifically, we first let the model calculate causally until the revision interval to revise the previous states and correct the decoding path. By applying revision, later outputs will be more accurate, and there is no need to change the hypothesis after all final outputs. Furthermore, we design a CTC spike position alignment decoding algorithm to reduce the computation costs, which is applicable to any revision methods that change the decoding path. Our experiments are based on the wav2vec2.0 [26] fine-tuning models using CTC and conducted on Librispeech benchmark [27]. Our best method can achieve 3.7/9.2 WERs which brings tremendous improvement for causal models and can be compared with chunk-based and knowledge distillation methods (23% and 6% relative reduction).",
            "score": 0.4838696724073879,
            "section_title": "Introduction",
            "char_start_offset": 1770,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 183
                },
                {
                    "start": 184,
                    "end": 360
                },
                {
                    "start": 361,
                    "end": 504
                },
                {
                    "start": 505,
                    "end": 670
                },
                {
                    "start": 671,
                    "end": 823
                },
                {
                    "start": 826,
                    "end": 931
                },
                {
                    "start": 932,
                    "end": 1076
                },
                {
                    "start": 1077,
                    "end": 1206
                },
                {
                    "start": 1207,
                    "end": 1387
                },
                {
                    "start": 1388,
                    "end": 1431
                },
                {
                    "start": 1432,
                    "end": 1510
                },
                {
                    "start": 1511,
                    "end": 1546
                },
                {
                    "start": 1547,
                    "end": 1710
                }
            ],
            "ref_mentions": [
                {
                    "start": 1432,
                    "end": 1436,
                    "matchedPaperCorpusId": "219966759"
                },
                {
                    "start": 1505,
                    "end": 1509,
                    "matchedPaperCorpusId": "2191379"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.305419921875
        },
        {
            "corpus_id": "273025546",
            "title": "ENTP: Encoder-only Next Token Prediction",
            "text": "Sequence-to-Token Functions and Autoregression. Given a vocabulary V (which we traditionally think of as some finite set, but could in general be any arbitrary perhaps uncountable set e.g. R d ), we can define a sequence over V as (x 1 , . . . , x n ) where x 1 , . . . , x n \u2208 V. Let V * = {(x 1 , . . . , x n ) : n \u2208 N; x i \u2208 V } be the set of all sequences generated by V . Then, we say that f : V * \u2192 V is a sequence-to-token function. \n\nWe can view a causal model as a map from an input sequence to an output sequence with the causality constraint that the i'th ouput token depends only on the first i input tokens. Mathematically, we enforce this causality contraint by characterizing our causal model, T f : V * \u2192 V * , with some sequence-to-token function f where on input sequence (x 1 , . . . , x n ) we have that \n\nObserve that a sequence-to-token function f can be used auto-regressively to generate tokens from some initial sequence (x 1 , . . . , x n ) via the following update rule: \n\nThis can also be viewed as a special case of the causal model, where the input sequence is chosen so that \n\nHence, if we are trying to learn a causal or auto-regressive model, it suffices to learn the sequence function that generates it. Thus in this paper, we focus on the type of sequence functions that encoders versus decoders can learn and express. \n\nEncoders and Decoders. We will use the letters E and D respectively to refer to encoders and decoders. In this paper, both models refer to variants of the Transformer architecture introduced in Vaswani et al. (2017), where the only difference lies in the masking used on the attention scores (decoder uses a causal mask while encoder allows full attention, as illustrated in Figure 1). The model size of a Transformer is determined by two parameters: \n\n\u2022 L: number of Transformer blocks. \n\n\u2022 D: embedding dimension.",
            "score": 0.4835049838407315,
            "section_title": "PRELIMINARIES",
            "char_start_offset": 6671,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 47
                },
                {
                    "start": 48,
                    "end": 188
                },
                {
                    "start": 189,
                    "end": 243
                },
                {
                    "start": 244,
                    "end": 269
                },
                {
                    "start": 270,
                    "end": 304
                },
                {
                    "start": 305,
                    "end": 376
                },
                {
                    "start": 377,
                    "end": 439
                },
                {
                    "start": 442,
                    "end": 620
                },
                {
                    "start": 621,
                    "end": 802
                },
                {
                    "start": 803,
                    "end": 823
                },
                {
                    "start": 826,
                    "end": 958
                },
                {
                    "start": 959,
                    "end": 997
                },
                {
                    "start": 1000,
                    "end": 1105
                },
                {
                    "start": 1108,
                    "end": 1237
                },
                {
                    "start": 1238,
                    "end": 1353
                },
                {
                    "start": 1356,
                    "end": 1378
                },
                {
                    "start": 1379,
                    "end": 1458
                },
                {
                    "start": 1459,
                    "end": 1741
                },
                {
                    "start": 1742,
                    "end": 1806
                },
                {
                    "start": 1809,
                    "end": 1843
                },
                {
                    "start": 1846,
                    "end": 1871
                }
            ],
            "ref_mentions": [
                {
                    "start": 1550,
                    "end": 1571,
                    "matchedPaperCorpusId": "13756489"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.5634765625
        },
        {
            "corpus_id": "268157336",
            "title": "A Review of Current Trends, Techniques, and Challenges in Large Language Models (LLMs)",
            "text": "An example of encoder-decoder architecture is the transformer model proposed in [24]. Its encoder and decoder blocks are stacked with multiple layers. As shown in Figure 3, the transformer encoder layer consists of a self-attention layer and a position-wise feed-forward layer. In addition to these two layers, decoder consists of a third cross-attention layer, which is responsible for attending to encoder output. \n\nEncoder-decoder models adopt bidirectional attention for the encoder, unidirectional attention for the decoder, and a cross-attention mechanism between them. Cross-attention in the decoder has access only to the fully processed encoder output and is responsible for connecting input tokens to target tokens. The encoder-decoder-based models are pretrained for seq2seq tasks. They can also be pretrained on conditional generation tasks, where the output is generated in regard to the given input, for example in summarizing, question answering, and translation tasks. T5 [25] uses encoder-decoder architecture. As stated in T5, using encoder-decoder structure helped to achieve good performance regarding classification as well as for generative tasks. \n\nAlthough encoder-decoder models end up having twice as many parameters as their decoder-only or encoder-only counterparts, they still have similar computational cost. Compared to PrefixLM models where the parameters are shared, here, the input and target are independently processed and use separate sets of parameters. Unlike decoder-only language models that are trained to generate the input, encoder-decoder models output target tokens. \n\nThe original transformer consisted of encoder-decoder blocks and was initially used for sequence-to-sequence tasks, such as NMT. However, it was discovered that, with the change in how the input is fed to the model, the single-stack (decoder or encoder) could also complete sequence-sequence model tasks. As a result, the subsequent models started containing either an encoder or decoder architecture. Below, we discuss these architectural variants of the original transformer model.",
            "score": 0.48287359742819547,
            "section_title": "Encoder-Decoder-Based Model",
            "char_start_offset": 15589,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 85
                },
                {
                    "start": 86,
                    "end": 150
                },
                {
                    "start": 151,
                    "end": 277
                },
                {
                    "start": 278,
                    "end": 415
                },
                {
                    "start": 418,
                    "end": 575
                },
                {
                    "start": 576,
                    "end": 725
                },
                {
                    "start": 726,
                    "end": 792
                },
                {
                    "start": 793,
                    "end": 984
                },
                {
                    "start": 985,
                    "end": 1027
                },
                {
                    "start": 1028,
                    "end": 1169
                },
                {
                    "start": 1172,
                    "end": 1338
                },
                {
                    "start": 1339,
                    "end": 1491
                },
                {
                    "start": 1492,
                    "end": 1612
                },
                {
                    "start": 1615,
                    "end": 1743
                },
                {
                    "start": 1744,
                    "end": 1919
                },
                {
                    "start": 1920,
                    "end": 2016
                },
                {
                    "start": 2017,
                    "end": 2098
                }
            ],
            "ref_mentions": [
                {
                    "start": 988,
                    "end": 992,
                    "matchedPaperCorpusId": "204838007"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.68017578125
        },
        {
            "corpus_id": "270832367",
            "title": "Predictability and Causality in Spanish and English Natural Language Generation",
            "text": "The contextual awareness of a transformer is controlled by self-attention. The base concept behind this attention mechanism is a mapping of a query (q) into pairs of keys (k) and values (v). By respectively denoting the queries', keys', and value sets' matrices as Q, K and V, we define self-attention as: \n\nTransformers, rather than a single attention function, project queries, keys, and values onto h separate heads. This is called multi-head attention: \n\n(2) By denoting each head attention function as: \n\nwhere W Q i , W K i , W V i and W O are parameter projection matrices for the queries, keys, values, and output respectively. This attention mechanism is present in all the layers of both the encoder and the decoder, if present. While the encoder's attention is bidirectional, the decoder has two different types of attention: (i) a masked multi-head attention block that masks non-causal context and (ii) a bidirectional multihead attention block that receives non-causal information from the encoder. \n\nEven though this encoder-decoder architecture is popular in some NLP tasks such as machine translation [20], [21], \n\n[22], [23], several transformer-based models only have one of these components. By omitting the encoder in decoder-only transformers, all non-causal contextual dependencies are removed by exclusively using masked attention. Decoder-only transformers are nowadays the best performing task-agnostic NLG systems. Nevertheless, there exist some state-of-theart non-causal NLG solutions. For example, non-causal language models can be trained for the Masked Language Modeling (MLM) objective, a task in which the language model predicts masked words within a sentence [24]. Typically, non-causal NLG systems are focused on particular tasks such as speech recognition [25], [26], [27], style transfer and grammar correction [28], textual data augmentation [29], and task-specific dialog systems [30], [31].",
            "score": 0.4827920080482846,
            "section_title": "A. CAUSALITY IN GENERATIVE TRANSFORMER LANGUAGE MODELS",
            "char_start_offset": 5984,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 74
                },
                {
                    "start": 75,
                    "end": 190
                },
                {
                    "start": 191,
                    "end": 305
                },
                {
                    "start": 308,
                    "end": 419
                },
                {
                    "start": 420,
                    "end": 456
                },
                {
                    "start": 459,
                    "end": 507
                },
                {
                    "start": 510,
                    "end": 635
                },
                {
                    "start": 636,
                    "end": 738
                },
                {
                    "start": 739,
                    "end": 1012
                },
                {
                    "start": 1015,
                    "end": 1129
                },
                {
                    "start": 1132,
                    "end": 1211
                },
                {
                    "start": 1212,
                    "end": 1355
                },
                {
                    "start": 1356,
                    "end": 1441
                },
                {
                    "start": 1442,
                    "end": 1514
                },
                {
                    "start": 1515,
                    "end": 1700
                },
                {
                    "start": 1701,
                    "end": 1932
                }
            ],
            "ref_mentions": [
                {
                    "start": 1118,
                    "end": 1122,
                    "matchedPaperCorpusId": "219463386"
                },
                {
                    "start": 1124,
                    "end": 1128,
                    "matchedPaperCorpusId": "209832341"
                },
                {
                    "start": 1138,
                    "end": 1142,
                    "matchedPaperCorpusId": "234785837"
                },
                {
                    "start": 1695,
                    "end": 1699,
                    "matchedPaperCorpusId": "265629619"
                },
                {
                    "start": 1794,
                    "end": 1798,
                    "matchedPaperCorpusId": "231924507"
                },
                {
                    "start": 1800,
                    "end": 1804,
                    "matchedPaperCorpusId": "231715684"
                },
                {
                    "start": 1806,
                    "end": 1810,
                    "matchedPaperCorpusId": "247126308"
                },
                {
                    "start": 1850,
                    "end": 1854,
                    "matchedPaperCorpusId": "218487230"
                },
                {
                    "start": 1882,
                    "end": 1886,
                    "matchedPaperCorpusId": "208224776"
                },
                {
                    "start": 1921,
                    "end": 1925,
                    "matchedPaperCorpusId": "210839508"
                },
                {
                    "start": 1927,
                    "end": 1931,
                    "matchedPaperCorpusId": "212657570"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.69140625
        },
        {
            "corpus_id": "274992300",
            "title": "Segment-Based Attention Masking for GPTs",
            "text": "Encoder-only models like BERT (Devlin, 2018) are primarily designed for bidirectional understanding tasks and excel in applications such as classification and question answering. While BERT can generate text autoregressively, each newly generated token changes the attention computation, requiring all dependent hidden states to be recomputed. Unlike decoder-only models, which use a key-value (KV) cache to efficiently generate multiple tokens during inference, BERT's bidirectional design prevents such optimization. This makes BERT impractical for token-by-token decoding. \n\nThe T5 framework (Raffel et al., 2020), with its encoder-decoder architecture, is effective for many NLP tasks due to its ability to incorporate bidirectional context during encoding and causal generation during decoding. However, SOTA and efficient performances in text generation are dominated by decoder-only models with causal masking, such as GPT-based architectures (Brown et al., 2020;Jiang et al., 2023;Yang et al., 2024;Dubey et al., 2024;Abdin et al., 2024). These models are highly efficient for token-by-token generation but cannot fully utilize input prompt information in their current design. \n\nThe most closely related work to our approach, PrefixLM, was explored in the T5 framework (Raffel et al., 2020). PrefixLM operates within a unified decoder-only architecture but enables bidirectional attention over a designated prefix of the input sequence while maintaining causal attention for the remainder. However, PrefixLM requires training from scratch and is limited to single-turn inputs, overlooking scenarios with multiple prefill phases, as often encountered in chat-based systems. \n\nIn contrast, our approach enables the easy enhancement of SOTA decoder-only models by unlocking the potential of bidirectional attention in non-generated segments through lightweight finetuning. Trained on massive corpora with causal masking, these models can be enhanced with limited hardware and just a few hours of fine-tuning, enabling them to effectively use bidirectional attention during the prefill phase.",
            "score": 0.48201720260494585,
            "section_title": "Related Work",
            "char_start_offset": 3399,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 178
                },
                {
                    "start": 179,
                    "end": 343
                },
                {
                    "start": 344,
                    "end": 518
                },
                {
                    "start": 519,
                    "end": 575
                },
                {
                    "start": 578,
                    "end": 799
                },
                {
                    "start": 800,
                    "end": 1046
                },
                {
                    "start": 1047,
                    "end": 1185
                },
                {
                    "start": 1188,
                    "end": 1300
                },
                {
                    "start": 1301,
                    "end": 1498
                },
                {
                    "start": 1499,
                    "end": 1681
                },
                {
                    "start": 1684,
                    "end": 1878
                },
                {
                    "start": 1879,
                    "end": 2097
                }
            ],
            "ref_mentions": [
                {
                    "start": 595,
                    "end": 616,
                    "matchedPaperCorpusId": "204838007"
                },
                {
                    "start": 950,
                    "end": 970,
                    "matchedPaperCorpusId": "218971783"
                },
                {
                    "start": 1278,
                    "end": 1299,
                    "matchedPaperCorpusId": "204838007"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.60400390625
        },
        {
            "corpus_id": "233004659",
            "title": "ASSEM-VC: Realistic Voice Conversion by Assembling Modern Speech Synthesis Techniques",
            "text": "Causal Decoder. Out of all the models we deal with, only Mellotron-VC used a causal decoder. During the decoding step, the output is generated in an autoregressive manner by using the linguistic encoding, the intonation encoding, and the previous mel frames. This architecture is capable of generating higher quality mel spectrograms than non-causal decoders. However, since a causal decoder is trained through teacher forcing, it may learn to cheat off of previous mel spectrogram frames, which are highly dependent on the source speaker. Thus, speaker disentanglement may not be achieved even if the speaker-independent features are used. \n\nNon-causal Decoder. A fully convolutional non-causal decoder is used in PPG-VC, Cotatron-VC, and Assem-VC. The structure of the decoder is the same as Cotatron-VC [11] except the speaker conditioning method. Instead of using speaker embedding for conditioning, we use an additional speaker encoder to capture the variation of target speech in the entire corpus. Extracted speaker representation conditions the decoder via conditional batch normalization layer [17].",
            "score": 0.4820032114381637,
            "section_title": "Decoder",
            "char_start_offset": 5246,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 15
                },
                {
                    "start": 16,
                    "end": 92
                },
                {
                    "start": 93,
                    "end": 258
                },
                {
                    "start": 259,
                    "end": 359
                },
                {
                    "start": 360,
                    "end": 539
                },
                {
                    "start": 540,
                    "end": 640
                },
                {
                    "start": 643,
                    "end": 662
                },
                {
                    "start": 663,
                    "end": 749
                },
                {
                    "start": 750,
                    "end": 850
                },
                {
                    "start": 851,
                    "end": 1004
                },
                {
                    "start": 1005,
                    "end": 1108
                }
            ],
            "ref_mentions": [
                {
                    "start": 806,
                    "end": 810,
                    "matchedPaperCorpusId": "218537963"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.293212890625
        },
        {
            "corpus_id": "53596423",
            "title": "An Analysis of Encoder Representations in Transformer-Based Machine Translation",
            "text": "We aim at analyzing the encoder representation of different models by assessing their quality through several experiments: i) by visualizing the attention weights (Section 5), ii) by inducing tree structure from the encoder weights (Section 6), iii) by probing the encoder as input representation for various prediction tasks (Section 7), and iv) by transferring the knowledge of one encoder to another (Section 8). We start by looking for linguistic patterns through the visualization of the heat-maps of the encoder weights. Next, we use the softmax weights extracted from the multi-head attention to build maximum spanning trees from the input sentences, assessing the quality of the induced tree through dependency parsing. Additionally, we evaluate the ability of the decoder, using a fixed encoder representation as input, on several sequence labeling tasks, measuring how important the input features are for various tasks. As test bed we use four dif-  The assumption is that if a property is well encoded in the input representation then it is easy for the decoder to predict that property. In practice, after training the MT system, we freeze the encoder parameters, and train one decoder layer for each task. The decoder layer is simpler than the original one used for MT; it consists only of one attention head and one feedforward layer with ReLU activation. Moreover, in order to output the right amount of labels, the decoder also has to learn implicitly the length of the input sentence. Note that our goal is not to beat the state of the art in a given task but rather to analyze the representation of an encoder trained for MT on different tasks referring to different linguistic properties. Finally, to assess whether the knowledge captured within an encoder is general enough to also be used for other models, we test a transfer learning scenario in which we use the encoder representation of a high resource language pair to initialize the encoder of a low resource language pair. Here, we assume that a model is better at encoding abstract linguistic properties if it can share useful information to enhance another weaker model.",
            "score": 0.48165110824422724,
            "section_title": "Methodology",
            "char_start_offset": 6742,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.162353515625
        },
        {
            "corpus_id": "271227251",
            "title": "A Survey on Symbolic Knowledge Distillation of Large Language Models",
            "text": "The GPTseries models, including GPT-1 [59], GPT-2 [60], and GPT-3 [96], are representative language models of this architecture. Many other LLMs, such as OPT [108], BLOOM [133], and Gopher [104], have also adopted the causal decoder architecture. \n\nPrefix Decoder Architecture: The prefix decoder architecture, also known as a non-causal decoder, is another type of decoder-only architecture which revises the masking mechanism of causal decoders to enable bidirectional attention over  Pre-trainning:Pre-training LLMs involves training on extensive unlabeled text datasets to learn general language patterns and insights. The success of pre-training hinges on both the scale and quality of the training corpus, with large, diverse datasets allowing models to capture a wide array of language patterns and generalize effectively to new data. \n\nThe pre-training process unfolds in phases, starting with data collection, which is divided into general and specialized data sources. General data encompasses a wide range of text, including webpages, conversations, Q&A portals, and books, while specialized data targets more niche content like research papers, code, and multilingual texts. The second phase, data pre-processing, focuses on refining the dataset by eliminating noisy, redundant, and irrelevant content. Techniques employed include quality filtering, deduplication (at sentence, document, and dataset levels), privacy protection (removing personal information), and tokenization (splitting text into manageable units for the model). Given that LLMs are not typically retrained frequently, the pre-training phase must be approached with precision, prioritizing a balanced mix of source materials [104], and ensuring both the quantity [110] and quality [136] of the data are optimal. Pre-training tasks may involve language modeling [95], favored by decoder-only architectures for predicting subsequent tokens, or de-noising autoencoding [132], which focuses on correcting or replacing corrupted tokens.",
            "score": 0.48135082959171055,
            "section_title": "B. Large Language Models",
            "char_start_offset": 27151,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 128
                },
                {
                    "start": 129,
                    "end": 246
                },
                {
                    "start": 249,
                    "end": 622
                },
                {
                    "start": 623,
                    "end": 841
                },
                {
                    "start": 844,
                    "end": 978
                },
                {
                    "start": 979,
                    "end": 1186
                },
                {
                    "start": 1187,
                    "end": 1314
                },
                {
                    "start": 1315,
                    "end": 1543
                },
                {
                    "start": 1544,
                    "end": 1792
                },
                {
                    "start": 1793,
                    "end": 2012
                }
            ],
            "ref_mentions": [
                {
                    "start": 66,
                    "end": 70,
                    "matchedPaperCorpusId": "218971783"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.1533203125
        },
        {
            "corpus_id": "253018395",
            "title": "Transcending Scaling Laws with 0.1% Extra Compute",
            "text": "Unified language learner (UL2) The UL2 (Tay et al., 2022b) model is a state-of-the-art model that bridges both generative causal language models and bidirectional language models. UL2 proposes a mixture-ofdenoiser objective that mixes prefix (non-causal) language modeling and infilling (span corruption) within the same model and leverages mode prompts to switch between modes during downstream tasks. UL2 is architecture agnostic in which the authors argue that the choice of decoder-only versus encoder-decoder models is largely an efficiency trade-off. In (Tay et al., 2022b), the final UL2 model was trained as a 20B encoder-decoder model, which achieves very compelling performance on both finetuning and in-context learning.",
            "score": 0.4811900621992207,
            "section_title": "Continued Training of Language Models",
            "char_start_offset": 10168,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 179
                },
                {
                    "start": 180,
                    "end": 402
                },
                {
                    "start": 403,
                    "end": 556
                },
                {
                    "start": 557,
                    "end": 731
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.3515625
        },
        {
            "corpus_id": "237416525",
            "title": "Language Modeling, Lexical Translation, Reordering: The Training Process of NMT through the Lens of Classical SMT",
            "text": "In this section, we compare different architectures within the same encoder-decoder framework (Transformer vs LSTM), and different frameworks with the Transformer architecture (encoder-decoder vs decoder-only). Overall, we find that all models follow the behavior described in Section 4.4; here we discuss some of their differences.\n\nTransformer vs LSTM. As might be expected from the low BLEU scores (Table 1), LSTM translations are simpler than the Transformer ones. We see that they are less surprising according to the target-side language modeling scores (Figure 7a 5 )  and have more monotonic alignments (Figure 7b). Regarding the latter, it is not clear whether this is because of the lower model capacity or because LSTM has an inductive bias towards more monotonic alignments; we leave this to future work.\n\nEncoder-decoder vs decoder-only. Table 1 shows that decoder-only (LM-style) NMT is not much worse than the standard encoder-decoder model, especially in the higher-resource setting (e.g., En-De). However, the decoder-only model has much simpler reordering patterns compared to the standard Transformer: its reordering scores are very close to the much weaker LSTM model ( Figure 7b). One possible explanation is that the bidirectional nature of Transformer's encoder facilitates learning more complicated reorderings.",
            "score": 0.48051836594576036,
            "section_title": "Other NMT Models",
            "char_start_offset": 19566,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.5439453125
        },
        {
            "corpus_id": "244715077",
            "title": "Question Answering for Complex Electronic Health Records Database using Unified Encoder-Decoder Architecture",
            "text": "Inspired by Dong et al. (2019), we propose an Encoder-as-Decoder model suited for the NLQ2Query task. To the best of our knowledge, this is the first attempt to adapt the encoder-as-decoder framework into the NLQ2Query task. Denoting the input embeddings from Section 3.2 as H 0 , they are encoded into contextual representations at different levels of hidden outputs H l using an L-layer Transformer encoders et al., 2017). \n\nIn this unified architecture, decoding is performed similar to Dong et al. (2019); At inference, given the input sequence (Q, [SEP], [MASK] 1 ), the model predicts \u01771 , the identity of [MASK] 1 . Then [MASK] 1 is replaced with \u01771 and we attach [MASK] 2 to the previous input sequence and repeat this process until [SEP] token is predicted. \n\nIn a typical Encoder-to-Decoder architecture, the decoder can only access the fully contextualized input embeddings (i.e. the output of the encoder), thereby limiting the model's ability to consider the input tokens during the decoding process. On the other hand, the Encoder-as-Decoder architecture allows the decoding process to access the input tokens at every layer of the encoder, thus improving the decoding capacity. Moreover, thanks to both encoding and decoding trained with [MASK] reconstruction (unlike Encoder-to-Decoder where the decoder is trained autoregressively), Encoder-as-Decoder can be naturally initialized with pre-trained language models such as BERT (Devlin et al., 2019).",
            "score": 0.48004348120654594,
            "section_title": "Encoder-as-Decoder Architecture",
            "char_start_offset": 7844,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 101
                },
                {
                    "start": 102,
                    "end": 224
                },
                {
                    "start": 225,
                    "end": 424
                },
                {
                    "start": 427,
                    "end": 622
                },
                {
                    "start": 623,
                    "end": 766
                },
                {
                    "start": 769,
                    "end": 1013
                },
                {
                    "start": 1014,
                    "end": 1192
                },
                {
                    "start": 1193,
                    "end": 1466
                }
            ],
            "ref_mentions": [
                {
                    "start": 12,
                    "end": 30,
                    "matchedPaperCorpusId": "147704286"
                },
                {
                    "start": 490,
                    "end": 508,
                    "matchedPaperCorpusId": "147704286"
                },
                {
                    "start": 1444,
                    "end": 1465,
                    "matchedPaperCorpusId": "52967399"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.135986328125
        },
        {
            "corpus_id": "13747425",
            "title": "The Best of Both Worlds: Combining Recent Advances in Neural Machine Translation",
            "text": "In an encoder-decoder architecture, a natural assumption is that the role of an encoder is to build feature representations that can best encode the meaning of the source sequence, while a decoder should be able to process and interpret the representations from the encoder and, at the same time, track the current target history. Decoding is inherently auto-regressive, and keeping track of the state information should therefore be intuitively beneficial for conditional generation. We set out to study which family of encoders is more suitable to extract rich representations from a given input sequence, and which family of decoders can make the best of such rich representations. We start by combining the encoder and decoder from different model families. Since it takes a significant amount of time for a ConvS2S model to converge, and because the final translation quality was not on par with the other models, we focus on two types of hybrids only: From Table 5, it is clear that the Transformer encoder is better at encoding or feature extraction than the RNMT+ encoder, whereas RNMT+ is better at decoding or conditional language modeling, confirming our intuition that a stateful decoder is beneficial for conditional language generation.",
            "score": 0.47987212460889583,
            "section_title": "Assessing Individual Encoders and Decoders",
            "char_start_offset": 19019,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 330
                },
                {
                    "start": 331,
                    "end": 484
                },
                {
                    "start": 485,
                    "end": 684
                },
                {
                    "start": 685,
                    "end": 761
                },
                {
                    "start": 762,
                    "end": 1250
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.4521484375
        },
        {
            "corpus_id": "259833781",
            "title": "Knowledge Graph-augmented Language Models for Complex Question Answering",
            "text": "We evaluate our method using language models from four families of models:\n\n\u2022 Flan-T5 (Chung et al., 2022) models are an extension of T5 encoder-decoder models that have been instruction tuned on a large set of instructions that were automatically generated using existing datasets and templates. We use the Flan-T5 Small (80M parameters), XL (3B), and XXL (11B) models.\n\n\u2022 T0 (Sanh et al., 2022) models are encoderdecoder models that are trained on a variety of prompts, which are automatically built from supervised datasets using templates. We use the T0 (11B) and T0 3B (3B) models.\n\n\u2022 OPT (Zhang et al., 2022) models are large, open-source, decoder-only models that have been trained to roughly match the performance of GPT-3 models. We use the 13B parameter version of OPT.\n\n\u2022 AlexaTM (Soltan et al., 2022) is a 20 billion parameter encoder-decoder model trained on publicly available data in multiple languages.",
            "score": 0.47979578804890566,
            "section_title": "Language Models",
            "char_start_offset": 8155,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.14501953125
        },
        {
            "corpus_id": "257050658",
            "title": "Parallel Sentence-Level Explanation Generation for Real-World Low-Resource Scenarios",
            "text": "We adopt the Transformer [22] as the backbone. To enable non-autoregressive interpretation, following [20], the decoder is modified in three aspects: input sequence, self-attention mask, and positional encoding. For input sequence modification, because previously generated tokens are unavailable under the non-autoregressive setting, we use a fertility predictor first to predict the length of the target explanation and produce decoder input with the tokens copied from the encoder input. \n\nFor the modification of the self-attention mask, because the decoder input is the copied sequence of encoder input, the self-attention module is allowed to attend all positions, rather than only left positions in the conventional Transformer decoder. Therefore, the self-attention mask is replaced with a non-causal mask in our non-autoregressive decoder. For positional encoding modification, different from the self-attention module, the positional attention module uses positional encoding as the query and key, and the hidden representations from the previous layer as the value.",
            "score": 0.478695108027087,
            "section_title": "Encoder and Decoder",
            "char_start_offset": 3841,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 46
                },
                {
                    "start": 47,
                    "end": 211
                },
                {
                    "start": 212,
                    "end": 490
                },
                {
                    "start": 493,
                    "end": 743
                },
                {
                    "start": 744,
                    "end": 848
                },
                {
                    "start": 849,
                    "end": 1076
                }
            ],
            "ref_mentions": [
                {
                    "start": 25,
                    "end": 29,
                    "matchedPaperCorpusId": "13756489"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.638671875
        },
        {
            "corpus_id": "257496129",
            "title": "Representation Learning for Stack Overflow Posts: How Far Are We?",
            "text": "Transformer-based language models have revolutionized the landscape of representation learning in natural language processing (NLP) [10,26,36]. Their efficacy in capturing text semantics has led to unparalleled performance in various applications, such as sentiment analysis [43], POS tagging [45], and question answering [34]. The vanilla transformer architecture [48] is composed of the encoder and decoder components. Based on the usage of these components, transformer-based language models can be categorized into three types: encoder-only, decoder-only, and encoderdecoder models. \n\nEncoder-only models exclusively leverage the encoder stacks of the vanilla transformer [48] architecture. BERT [10] stands as a prominent encoder-only representation model, which learns a bidirectional contextual representation of text. BERT proposes the Masked Language Modeling (MLM) task at the pre-training phase. In MLM, the input data is corrupted by randomly masking 15% of the tokens, and then the BERT model learns to reconstruct the original data by predicting the masked words. BERT is extensively pre-trained on large-scale datasets, which learn a meaningful representation that is reusable for various tasks, thus eliminating the process of training language models from scratch and saving time and resources. \n\nIn contrast, Decoder-only models consist solely of the decoder components of the original transformer architecture. A notable instance of such models is the GPT [35], GPT operates under a causal language modeling (CLM) framework during its training phase. CLM is a strategy where the model predicts the next token in a sequence while only considering preceding tokens. In other words, this design restricts the model from accessing future tokens in the sequence. \n\nBridging the above approaches, textitEncoder-decoder models integrate both the encoder and decoder components of the transformer architecture. Popular encoder-decoder models involve T5 [37] and BART [23]. The T5 model [37] advocates a unified text-to-text framework that converts various language tasks into a consistent text-to-text format.",
            "score": 0.4778044410451196,
            "section_title": "Transformer-based Language Models",
            "char_start_offset": 6666,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 143
                },
                {
                    "start": 144,
                    "end": 327
                },
                {
                    "start": 328,
                    "end": 420
                },
                {
                    "start": 421,
                    "end": 586
                },
                {
                    "start": 589,
                    "end": 694
                },
                {
                    "start": 695,
                    "end": 825
                },
                {
                    "start": 826,
                    "end": 906
                },
                {
                    "start": 907,
                    "end": 1077
                },
                {
                    "start": 1078,
                    "end": 1311
                },
                {
                    "start": 1314,
                    "end": 1429
                },
                {
                    "start": 1430,
                    "end": 1569
                },
                {
                    "start": 1570,
                    "end": 1682
                },
                {
                    "start": 1683,
                    "end": 1776
                },
                {
                    "start": 1779,
                    "end": 1921
                },
                {
                    "start": 1922,
                    "end": 1983
                },
                {
                    "start": 1984,
                    "end": 2120
                }
            ],
            "ref_mentions": [
                {
                    "start": 132,
                    "end": 136,
                    "matchedPaperCorpusId": "52967399"
                },
                {
                    "start": 139,
                    "end": 142,
                    "matchedPaperCorpusId": "160025533"
                },
                {
                    "start": 275,
                    "end": 279,
                    "matchedPaperCorpusId": "85459677"
                },
                {
                    "start": 293,
                    "end": 297,
                    "matchedPaperCorpusId": "202122780"
                },
                {
                    "start": 322,
                    "end": 326,
                    "matchedPaperCorpusId": "153312701"
                },
                {
                    "start": 365,
                    "end": 369,
                    "matchedPaperCorpusId": "13756489"
                },
                {
                    "start": 676,
                    "end": 680,
                    "matchedPaperCorpusId": "13756489"
                },
                {
                    "start": 700,
                    "end": 704,
                    "matchedPaperCorpusId": "52967399"
                },
                {
                    "start": 1964,
                    "end": 1968,
                    "matchedPaperCorpusId": "204838007"
                },
                {
                    "start": 1978,
                    "end": 1982,
                    "matchedPaperCorpusId": "204960716"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.66015625
        },
        {
            "corpus_id": "269009682",
            "title": "LLM2Vec: Large Language Models Are Secretly Powerful Text Encoders",
            "text": "in an unsupervised manner using only a set of unordered sentences. These unsupervised approaches typically create two different representations of the same sentence for contrastive learning. The methods vary in how they form these representations -perturbing the input sentence (Wu et al., 2020), or using different model instances (Carlsson et al., 2021). SimCSE (Gao et al., 2021), the approach used in this work, generates two representations of the same sentence by passing it through the model twice with different dropout masks. \n\nTurning decoder-only LLMs into text encoders While decoder-only LLMs have outperformed bidirectional encoders across a large variety of language understanding tasks (Brown et al., 2020;Touvron et al., 2023;Jiang et al., 2023a, inter alia), their impact on sentence representation learning remains limited. The most common approaches in literature use the final hidden state of the last token as the sentence embedding (Neelakantan et al., 2022;Ma et al., 2023;Wang et al., 2023). \n\nThere are few works that explore the limitations of using a causal attention mask when adapting decoder-only LLMs for text classification and sentence representation tasks. Li et al. (2023b) experiment with removing the causal mask of Llama-2 during supervised fine-tuning for text classification and NER tasks. Similarly, Duki\u0107 & \u0160najder (2024) enable bidirectional attention for a group of layers during supervised fine-tuning on NER and chunking. In the context of sentence representation learning, Li & Li (2024) explore enabling bidirectional attention in the last layer of a decoder-only model during supervised contrastive fine-tuning on STS tasks. \n\nConcurrent to our work, several works have focused on converting decoder-only-LLMs to text encoders in supervised and unsupervised manner. Jiang et al. (2023b) and Lei et al. (2024) prompt the language model to summarize the input text in one word, and take the last layer's hidden embedding for the last token as the text's representation.",
            "score": 0.477713523720629,
            "section_title": "Unsupervised text encoders Another line of work has explored training text embedders",
            "char_start_offset": 22008,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 66
                },
                {
                    "start": 67,
                    "end": 190
                },
                {
                    "start": 191,
                    "end": 356
                },
                {
                    "start": 357,
                    "end": 534
                },
                {
                    "start": 537,
                    "end": 842
                },
                {
                    "start": 843,
                    "end": 1016
                },
                {
                    "start": 1019,
                    "end": 1191
                },
                {
                    "start": 1192,
                    "end": 1330
                },
                {
                    "start": 1331,
                    "end": 1468
                },
                {
                    "start": 1469,
                    "end": 1674
                },
                {
                    "start": 1677,
                    "end": 1815
                },
                {
                    "start": 1816,
                    "end": 2017
                }
            ],
            "ref_mentions": [
                {
                    "start": 332,
                    "end": 355,
                    "matchedPaperCorpusId": "235613354"
                },
                {
                    "start": 364,
                    "end": 382,
                    "matchedPaperCorpusId": "233296292"
                },
                {
                    "start": 702,
                    "end": 722,
                    "matchedPaperCorpusId": "218971783"
                },
                {
                    "start": 981,
                    "end": 997,
                    "matchedPaperCorpusId": "16299141"
                },
                {
                    "start": 1521,
                    "end": 1535,
                    "matchedPaperCorpusId": "265066823"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.28125
        },
        {
            "corpus_id": "266335547",
            "title": "On the compression of shallow non-causal ASR models using knowledge distillation and tied-and-reduced decoder for low-latency on-device speech recognition",
            "text": "The non-causal encoder uses beam search for decoding, which causes an increase in second pass decoding latency. Therefore, we developed shallow cascaded model by reducing the size of non-causal encoder and applied KD on non-causal encoder. Two different KD models are trained. One involves using KD on the TAR cascaded Conformer-T model. The second involves applying KD to a regular cascaded Conformer-T model, which serves as the compression baseline for our experiments. We scaled down the Conformer model by decreasing hidden units and/or layers of the both causal/non-causal encoder network. The model parameters of each of the employed teacher and student models can be found in Table 1. Other attributes remain the same across all models. We trained each model for 200 epochs and then published the results for the epoch that produced the best WER. According to our initial studies, the optimum outcomes for knowledge distillation come from a distillation loss weight of 0.02. The temperature was set to 1.0 for all our teacher and student model experiments.",
            "score": 0.4776968777954965,
            "section_title": "Knowledge distillation on shallow cascaded model",
            "char_start_offset": 11317,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 111
                },
                {
                    "start": 112,
                    "end": 239
                },
                {
                    "start": 240,
                    "end": 276
                },
                {
                    "start": 277,
                    "end": 337
                },
                {
                    "start": 338,
                    "end": 472
                },
                {
                    "start": 473,
                    "end": 595
                },
                {
                    "start": 596,
                    "end": 692
                },
                {
                    "start": 693,
                    "end": 744
                },
                {
                    "start": 745,
                    "end": 854
                },
                {
                    "start": 855,
                    "end": 982
                },
                {
                    "start": 983,
                    "end": 1064
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.177001953125
        },
        {
            "corpus_id": "269148552",
            "title": "Aligning the Objective of LLM-based Program Repair",
            "text": "Background: A large language model (LLM) is a language model consisting of a neural network with many parameters (typically billions of weights or more) trained on large quantities of unlabelled corpus using self-supervised learning [44]. The LLMs usually adopt the Transformer [45] architecture or one of its sub-structures (i.e., encoder or decoder). The encoder usually consists of feed-forward networks with selfattention [45], while the decoder usually consists of feedforward networks with cross-attention [45]. Thus, LLMs can be categorized into three types: encoder-only, decoder-only, and encoder-decoder LLMs. \n\nEncoder-only LLMs, such as BERT [23] and its variants like CodeBERT [46], have a bidirectional transformer encoder structure. They are typically trained on the masked language modeling objective (i.e., MLM), aiming to denoise and reconstruct the masked tokens via understanding the surrounding context (Fig. 1). As shown in Eq. 1, the loss of MLM training objective can be explicitly represented as the log-sum of the conditional probabilities of generating the masked token when all the unmasked tokens are known. \n\nwhere M is the total number of masked tokens. Decoder-only LLMs, including GPT series [13], [27] and LLaMA series [47], have an autoregressive transformer decoder structure. They are mainly trained on the causal language modeling objective (i.e., CLM), aiming to predict and complete next tokens via following the prefix context (Fig. 1). As shown in Eq. 2, the loss of CLM training objective can be explicitly represented as the log-sum of the conditional probabilities of generating the next token when all the preceeding tokens are known. \n\nwhere N is the total number of the input tokens. Encoder-decoder LLMs, such as T5 [24] and its variants like CodeT5 [48], have a complete transformer structure.",
            "score": 0.4775922423194968,
            "section_title": "II. BACKGROUND AND MOTIVATION A. LLM Architectures and Training Objectives",
            "char_start_offset": 6433,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 238
                },
                {
                    "start": 239,
                    "end": 352
                },
                {
                    "start": 353,
                    "end": 517
                },
                {
                    "start": 518,
                    "end": 619
                },
                {
                    "start": 622,
                    "end": 747
                },
                {
                    "start": 748,
                    "end": 933
                },
                {
                    "start": 934,
                    "end": 1136
                },
                {
                    "start": 1139,
                    "end": 1184
                },
                {
                    "start": 1185,
                    "end": 1312
                },
                {
                    "start": 1313,
                    "end": 1477
                },
                {
                    "start": 1478,
                    "end": 1680
                },
                {
                    "start": 1683,
                    "end": 1731
                },
                {
                    "start": 1732,
                    "end": 1843
                }
            ],
            "ref_mentions": [
                {
                    "start": 278,
                    "end": 282,
                    "matchedPaperCorpusId": "13756489"
                },
                {
                    "start": 426,
                    "end": 430,
                    "matchedPaperCorpusId": "13756489"
                },
                {
                    "start": 512,
                    "end": 516,
                    "matchedPaperCorpusId": "13756489"
                },
                {
                    "start": 1225,
                    "end": 1229,
                    "matchedPaperCorpusId": "218971783"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.71240234375
        },
        {
            "corpus_id": "263671583",
            "title": "DecoderLens: Layerwise Interpretation of Encoder-Decoder Transformers",
            "text": "Many methods for interpreting the internal states of neural language models -and in particular Transformer-based models -have been proposed in the last few years (for a review, see Lyu et al., 2024). Such methods operate at many different levels of granularity, ranging from model-agnostic attribution methods that treat models as black-boxes, to probing methods that assess whether specific information is decodable from model representations, to fine-grained techniques aiming to causally link highly localized circuits to model behavior. These latter techniques (often referred to as 'mechanistic interpretability', Elhage et al., 2021, or 'causal abstractions', Geiger et al., 2021) are often strongly tied to model-specific components, and are likely to provide more faithful insight into how these models operate. \n\nIn this paper, we propose DecoderLens, a method aimed at exploiting the decoder module Model Output encoder \n\nWhat is the capital of Spain? \n\nThe capital of Spain is the capital of Spain decoder \n\nThe is the of of of of \n\ndecoder What is the capital of Spain? of encoder-decoder Transformers as a \"lens\" to explain the evolution of representations throughout model layers in these model architectures. Our method is directly inspired by the LogitLens (nostalgebraist, 2020), which leverages the residual stream1 present in Transformer architectures. The LogitLens, however, is defined only for decoderonly Transformers, and is unable to explain how representations evolve in the encoder of encoderdecoder models. \n\nConcretely, DecoderLens forces the decoder module of an encoder-decoder model to crossattend intermediate encoder activations. As a consequence, its generations can be seen as sequences of vocabulary projections depending only on partiallyformed source-side representations. Such adaptation is necessary as LogitLens requires the presence of a residual stream, which is not found between encoder and decoder modules. Contrary to common probing methods, DecoderLens operates without any additional training, letting the model \"explain itself\" by producing natural generations in a humaninterpretable vocabulary space. Figure 1 provides a graphical overview of our approach.",
            "score": 0.4767611899494618,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 199
                },
                {
                    "start": 200,
                    "end": 540
                },
                {
                    "start": 541,
                    "end": 819
                },
                {
                    "start": 822,
                    "end": 929
                },
                {
                    "start": 932,
                    "end": 961
                },
                {
                    "start": 964,
                    "end": 1016
                },
                {
                    "start": 1019,
                    "end": 1041
                },
                {
                    "start": 1044,
                    "end": 1223
                },
                {
                    "start": 1224,
                    "end": 1371
                },
                {
                    "start": 1372,
                    "end": 1534
                },
                {
                    "start": 1537,
                    "end": 1663
                },
                {
                    "start": 1664,
                    "end": 1811
                },
                {
                    "start": 1812,
                    "end": 1953
                },
                {
                    "start": 1954,
                    "end": 2153
                },
                {
                    "start": 2154,
                    "end": 2209
                }
            ],
            "ref_mentions": [
                {
                    "start": 181,
                    "end": 198,
                    "matchedPaperCorpusId": "252519203"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.3134765625
        },
        {
            "corpus_id": "250072778",
            "title": "Pruned RNN-T for fast, memory-efficient ASR training",
            "text": "Since (5) makes it natural to separate the encoder (acoustic) and decoder (language-model/LM) parts of L trivial (t, u, v), we decided to try interpolating the trivial joiner with even-moretrivial versions of the joiner network: specifically, versions where we use encoder-only and decoder-only versions of the probabilities. Let LogSoftmax be the log-softmax operation, applied along the appropriate axis (the v axis). So the version of the log-likelihoods we use in the recursion would be: \n\nwhere: \n\nand L avg dec (u, v) takes the role of a unigram language-model prior: \n\nThe reason for the asymmetry between the encoder and decoder here is that we want the decoder log-probs to be independently interpretable as language-model probabilities; this will be more convenient in case we need to access the language model probabilities independently for some reason later on.",
            "score": 0.4764331218054072,
            "section_title": "Smoothed trivial joiner",
            "char_start_offset": 11429,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 325
                },
                {
                    "start": 326,
                    "end": 419
                },
                {
                    "start": 420,
                    "end": 491
                },
                {
                    "start": 494,
                    "end": 500
                },
                {
                    "start": 503,
                    "end": 573
                },
                {
                    "start": 576,
                    "end": 874
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.1026611328125
        },
        {
            "corpus_id": "270214176",
            "title": "A Survey on Large Language Models for Code Generation",
            "text": "where x < represents the sequence of preceding tokens { 1 , . . .,   \u22121 } before x  in the input,  denotes the model parameters.The conditional probability   (  |x < )) is modeled by adding a causal attention mask to the multi-head self-attention matrix of each Transformer block.To be specific, causal attention masking is implemented by setting the lower triangular part of the matrix to 0 and the remaining elements to \u2212\u221e, ensuring that each token   attends only to its predecessors and itself.On the contrary, in encoder-decoder LLMs, a pivot token   is randomly selected in a sequence of tokens and then regarding the context before it as the source sequence x  = { 1 , . . .,   } of the encoder and the sequence after it as the target output x  = { +1 , . . .,   } of decoder.Formally, the causal language modeling objective for training encoder-decoder LLMs is to minimize loss function as follows:\n\nwhere x \u2264 is the source sequence input and x < denotes the target sequence autoregressively generated so far.During the inference phase, pre-trained LLMs that have been trained on largescale code corpus can generate code in a zero-shot manner without the need for fine-tuning.This is achieved through the technique of prompt engineering, which guides the model to produce the desired output11 [31,186].Additionally, recent studies have explored the use of few-shot learning, also referred to as in-context learning, to enhance model performance further [131,178].Denoising Autoencoding.In addition to causal language modeling (CLM), the denoising autoencoding (DAE) task has been extensively applied in pre-training encoder-decoder architectures for code generation, such as PLBART [6], CodeT5 [234], and its enhanced successor, CodeT5+ [232].Following T5 [189] and CodeT5 [234], the DAE refers to initially perturbing the source sequence by introducing randomly masked spans of varying lengths.This corrupted sequence serves as the input for the encoder.",
            "score": 0.47604966510043667,
            "section_title": "Pre-training",
            "char_start_offset": 42181,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 65
                },
                {
                    "start": 65,
                    "end": 128
                },
                {
                    "start": 128,
                    "end": 280
                },
                {
                    "start": 280,
                    "end": 497
                },
                {
                    "start": 497,
                    "end": 680
                },
                {
                    "start": 680,
                    "end": 765
                },
                {
                    "start": 765,
                    "end": 782
                },
                {
                    "start": 782,
                    "end": 905
                },
                {
                    "start": 907,
                    "end": 1016
                },
                {
                    "start": 1016,
                    "end": 1183
                },
                {
                    "start": 1183,
                    "end": 1309
                },
                {
                    "start": 1309,
                    "end": 1470
                },
                {
                    "start": 1470,
                    "end": 1493
                },
                {
                    "start": 1493,
                    "end": 1750
                },
                {
                    "start": 1750,
                    "end": 1902
                },
                {
                    "start": 1902,
                    "end": 1962
                }
            ],
            "ref_mentions": [
                {
                    "start": 1300,
                    "end": 1304,
                    "matchedPaperCorpusId": "218971783"
                },
                {
                    "start": 1304,
                    "end": 1308,
                    "matchedPaperCorpusId": "160025533"
                },
                {
                    "start": 1701,
                    "end": 1706,
                    "matchedPaperCorpusId": "237386541"
                },
                {
                    "start": 1744,
                    "end": 1749,
                    "matchedPaperCorpusId": "258685677"
                },
                {
                    "start": 1763,
                    "end": 1768,
                    "matchedPaperCorpusId": "204838007"
                },
                {
                    "start": 1780,
                    "end": 1785,
                    "matchedPaperCorpusId": "237386541"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.66064453125
        },
        {
            "corpus_id": "248512883",
            "title": "Anticipation-Free Training for Simultaneous Machine Translation",
            "text": "All SimulMT models use causal encoders. During inference, the encoder states are computed incrementally after each read, similar to (Elbayad et al., 2020). The causal encoder models follow a similar training process to non-autoregressive translation (NAT) (Gu et al., 2018;Libovick\u00fd and Helcl, 2018;Lee et al., 2018;Zhou et al., 2020). We adopt sequence level knowledge distillation (Seq-KD) (Kim and Rush, 2016) for all systems. The combination of Seq-KD and CTC loss has been shown to achieve state-of-the-art performance (Gu and Kong, 2021) and could deal with the reordering problem (Chuang et al., 2021). Specifically, we first train a full-sentence model as a teacher model on the original dataset, then we use beam search with beam width 5 to decode the Seq-KD set. We use the Seq-KD set in subsequent experiments. We list the Transformer and ASN hyperparameters separately in Appendix C and D.\n\nWe use Adam (Kingma and Ba, 2015) with an inverse square root schedule for the optimizer. The max learning rate is 5e-4 with 4000 warm-up steps. We use gradient accumulation to achieve an effective batch size of 128K tokens for the teacher model and 32K for others. We optimize the model with the 300K steps. Early stopping is applied when the validation BLEU does not improve within 25K steps. Label smoothing (Szegedy et al., 2016) with 3 We use casia2015, casict2011, casict2015, neu2017. \u03f5 ls = 0.1 is applied on cross-entropy and CTC loss. For CTC, this reduces excessive blank symbol predictions (Kim et al., 2018). Random seeds are set in training scripts in our source code. For the hardware information and environment settings, see Appendix E.\n\nFor latency evaluation, we use SimulEval (Ma et al., 2020a) to compute Average Lagging (AL) (Ma et al., 2019) and",
            "score": 0.4757898620487193,
            "section_title": "Experimental Setup",
            "char_start_offset": 13411,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.0863037109375
        },
        {
            "corpus_id": "277128409",
            "title": "A review on the applications of Transformer-based language models for nucleotide sequence analysis",
            "text": "The functioning of the Decoder has many parts in common with the Encoder, yet there are some major differences. In a nutshell, the Decoder processes the output component of the input-output pairs of the training data; encodes such information and produces its own embedding using a multi-head attention scheme similar to the Encoder, but: \n\n1. once the output has been encoded, it is combined with the  and  matrices coming from the Encoder. This is the step where Transformers learn the relation between the input of the training (e.g., the question) and the output (e.g., the answer to the question); considering the question-answer example, the Decoder performs cross-attention on the Encoder output (which represents the question) while processing the answer; 2. at the end of the process a linear layer has a number of output neurons equal to the size of the vocabulary; such network uses a softmax function to produce likelihood for each term in the vocabulary. Then, the term with the highest likelihood is the output of the Decoder i.e. the predicted next word; 3. The Decoder produces the output one word at a time; in the training phase, knowing the correct word, the error is computed and used to drive the backpropagation step and the correction of the weights to reduce the error. \n\nThe steps as mentioned above can be visualised in Fig. 1(c). To fully appreciate the subtleties of the Encoder-Decoder mechanism, one should be aware that Encoder and Decoder are trained simultaneously, both in pretraining and in finetuning. Both strategies depend on the model type. For example, the weights obtained in pretraining for Encoder-only model like BERT, are based on randomly masked words; on the other hand, for Decoder-only model like GPT, causal masking is used, where only the future tokens are masked, and the model predicts the next token given past tokens. In finetuning, the pretrained weights are refined when input-output training pairs are presented to the system.",
            "score": 0.47549074833648897,
            "section_title": "Encoding and decoding",
            "char_start_offset": 16803,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 111
                },
                {
                    "start": 112,
                    "end": 338
                },
                {
                    "start": 341,
                    "end": 441
                },
                {
                    "start": 442,
                    "end": 967
                },
                {
                    "start": 968,
                    "end": 1293
                },
                {
                    "start": 1296,
                    "end": 1356
                },
                {
                    "start": 1357,
                    "end": 1537
                },
                {
                    "start": 1538,
                    "end": 1579
                },
                {
                    "start": 1580,
                    "end": 1872
                },
                {
                    "start": 1873,
                    "end": 1984
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.83349609375
        },
        {
            "corpus_id": "254854120",
            "title": "Multistep Multiappliance Load Prediction",
            "text": "Encoder-decoder models, such as the sequence-to-sequence model, likewise originate from language translation [Sutskever et al., 2014] and prove powerful in time series forecasting [Du et al., 2018], [Sehovac and Grolinger, 2020b] and [Fadlallah et al., 2013]. An encoder-decoder architecture comprises two network parts, an encoder network and a decoder network. The encoder calculates a hidden representation of the inputs encoded in hidden states and passes them on to the decoder network for calculating predictions. The difference to standard neural network training is the transfer of states instead of layer outputs between the encoder and decoder and often the two parts are The second variation S2S context uses no pre-training, instead, input information is separated into past observations of the target sequence and additional features. The encoder calculates a hidden state representation of the additional features as a sort of context. Transferring these states initializes the decoder network. The decoder uses the past target sequence as input together with the encoder weights to predict the next output sequence. This variation trains only one network and reduces computation times compared to S2S reversed. Both networks use standard LSTM-layers within the encoder and decoder parts.",
            "score": 0.47463219598637024,
            "section_title": "Encoder-Decoder Networks",
            "char_start_offset": 25228,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 259
                },
                {
                    "start": 260,
                    "end": 362
                },
                {
                    "start": 363,
                    "end": 519
                },
                {
                    "start": 520,
                    "end": 847
                },
                {
                    "start": 848,
                    "end": 949
                },
                {
                    "start": 950,
                    "end": 1008
                },
                {
                    "start": 1009,
                    "end": 1130
                },
                {
                    "start": 1131,
                    "end": 1225
                },
                {
                    "start": 1226,
                    "end": 1302
                }
            ],
            "ref_mentions": [
                {
                    "start": 109,
                    "end": 133,
                    "matchedPaperCorpusId": "7961699"
                },
                {
                    "start": 180,
                    "end": 197,
                    "matchedPaperCorpusId": "145051220"
                },
                {
                    "start": 199,
                    "end": 229,
                    "matchedPaperCorpusId": "212647795"
                },
                {
                    "start": 234,
                    "end": 257,
                    "matchedPaperCorpusId": "12598891"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.4892578125
        },
        {
            "corpus_id": "214161830",
            "title": "RobuTrans: A Robust Transformer-Based Text-to-Speech Model",
            "text": "As discussed in Section 2.1, the encoder-decoder attention mechanism is a crucial factor for the instability. However, simply removing this attention will also discard the advantages it brings to the TTS model. The advantages can be considered as the following two aspects. On the one hand, the encoder-decoder attention provides a holistic view of input sequence for the decoder, while on the other hand, it composes frame-level context vectors according to decoder inputs (which are mel frames). These two advantages make great contribution to the decoding procedure, and we propose \"pseudo non-causal attention\" (PNCA) to replace the causal self-attention layers as shown in Figure 4, which not only inherits the two features above, but also makes the decoding procedure robust. \n\nLet T be the total length of mel spectrum to be decoded, x l i be the autoregressive output of step i and layer l, h i be the tiled encoder hidden state of step i. For the time step t, the PNCA of layer l takes x l\u22121 1 , x l\u22121 2 , ..., x l\u22121 t3 and [h t , h t+1 ...h T ] as input. Specifically, let Attention(Q, K) be the multi-head attention, Figure 4: Phoneme-level to frame-level conversion and pseudo non-causal attention (PNCA). The left part of PNCA is causal self-attention, which takes the encoder hidden states fused with padded mel spectrum frames by a linear projection as input, while the right part consumes only the encoder hidden states. \n\nThen y l t is added to x l\u22121 t and consumed by FFN and following residual connection to obtain x l t .",
            "score": 0.4745377088700461,
            "section_title": "Pseudo Non-causal Attention",
            "char_start_offset": 12308,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 109
                },
                {
                    "start": 110,
                    "end": 210
                },
                {
                    "start": 211,
                    "end": 273
                },
                {
                    "start": 274,
                    "end": 497
                },
                {
                    "start": 498,
                    "end": 781
                },
                {
                    "start": 784,
                    "end": 947
                },
                {
                    "start": 948,
                    "end": 1064
                },
                {
                    "start": 1065,
                    "end": 1217
                },
                {
                    "start": 1218,
                    "end": 1436
                },
                {
                    "start": 1439,
                    "end": 1541
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.439453125
        },
        {
            "corpus_id": "243865137",
            "title": "Exploring Non-Autoregressive Text Style Transfer",
            "text": "All the NAR models adopt the same Transformer based encoder-decoder architecture, and the BaseAR model only differs from the NAR models by the following three differences discussed in Section 3.1. \n\n\u2022 The NAR model discards the autoregressive mask in the self-attention layer. Since the NAR model removes the conditional dependency among the output tokens, the causal mask where the position t can only attend to positions 1 . . . t \u2212 1 is no longer needed. Following Gu et al. (2018), we set the masks to prevent a position from attending to itself. \n\n\u2022 The NAR model incorporates a positionalattention layer in the decoder, which has been shown to facilitate local reordering in decoding (Gu et al., 2018). The positional-attention layer, placed between the self-attention layer and the inter-attention layer, takes the position embeddings as queries and keys while the decoder states as values. \n\n\u2022 The NAR model uniformly maps the source words as the decoder input to enrich the information on the decoder side. Specifically, position t in the decoder input takes the word embedding of the source token in position i = round( Tx Ty \u2022 t), where T x and T y denote the lengths of source input and target output, respectively. \n\nBoth the encoder and the decoder use a Transformer structure with d model = d hidden = 128, n head = 4, n layer = 2. Following existing works (Lample et al., 2019), the target style is treated as a special start token in decoder. Both the style classifier for automatic evaluation and the pretrained p \u03c8 in the style compatibility loss follow the TextCNN (Kim, 2014) architecture but are independently trained. To backpropagate the gradients from p \u03c8 to \u03b8, we approximate y in Eq. 4 with the softmax distribution sequence from which y should be sampled.",
            "score": 0.474383303749919,
            "section_title": "C.1 Model Architecture",
            "char_start_offset": 16259,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 196
                },
                {
                    "start": 199,
                    "end": 276
                },
                {
                    "start": 277,
                    "end": 430
                },
                {
                    "start": 431,
                    "end": 457
                },
                {
                    "start": 458,
                    "end": 550
                },
                {
                    "start": 553,
                    "end": 708
                },
                {
                    "start": 709,
                    "end": 897
                },
                {
                    "start": 900,
                    "end": 1015
                },
                {
                    "start": 1016,
                    "end": 1227
                },
                {
                    "start": 1230,
                    "end": 1459
                },
                {
                    "start": 1460,
                    "end": 1640
                },
                {
                    "start": 1641,
                    "end": 1783
                }
            ],
            "ref_mentions": [
                {
                    "start": 468,
                    "end": 484,
                    "matchedPaperCorpusId": "3480671"
                },
                {
                    "start": 690,
                    "end": 707,
                    "matchedPaperCorpusId": "3480671"
                },
                {
                    "start": 1585,
                    "end": 1596,
                    "matchedPaperCorpusId": "9672033"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.591796875
        },
        {
            "corpus_id": "44180499",
            "title": "The OSU Realizer for SRST \u201818: Neural Sequence-to-Sequence Inflection and Incremental Locality-Based Linearization",
            "text": "We define inference (the decoding step) of output y given input sequence x as a distribution of possible output strings: \n\nThis distribution is derived from the product of previous individual outputs y 1 , y 2 , ..., y t\u22121 up to the current time step t to produce the most likely output y t . Output y is also dependent on s t (the hidden state of the decoder) and context c t (the weighted sum of annotations produced by the encoder): \n\nWhere we calculate weights \u03b1 ij for h j as: \n\nWe used standard cross-entropy loss, 300 hidden units for both the encoder and decoder. We followed Kann and Sch\u00fctze by training the model using minibatches of 20 and Adadelta (Zeiler, 2012). For the datasets, we used the entirety of the supplied training data, but only used a random sample of 6000 items from the development set to speed up training. Models for each language were trained until wordform prediction accuracy on the development set was over 98% or up to 30 epochs with early stopping. Dropout was set to 0.5. \n\nTable 3 shows our system's performance in selecting fully inflected wordforms on the development set. We also supply two competing baselines as a point of comparison: one in which our system just copies the citation form supplied and one where it only selects the most common inflected wordform seen in training. By and large, we see tremendous improvements in selecting the correct wordform. \n\nOur final feature set included any features supplied by the data, in addition to features from immediate children and parents in the dependency tree. We made use of all features from a given",
            "score": 0.47414598110184564,
            "section_title": "Morphological Inflection",
            "char_start_offset": 6642,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 120
                },
                {
                    "start": 123,
                    "end": 292
                },
                {
                    "start": 293,
                    "end": 435
                },
                {
                    "start": 438,
                    "end": 481
                },
                {
                    "start": 484,
                    "end": 571
                },
                {
                    "start": 572,
                    "end": 675
                },
                {
                    "start": 676,
                    "end": 836
                },
                {
                    "start": 837,
                    "end": 985
                },
                {
                    "start": 986,
                    "end": 1009
                },
                {
                    "start": 1012,
                    "end": 1113
                },
                {
                    "start": 1114,
                    "end": 1324
                },
                {
                    "start": 1325,
                    "end": 1404
                },
                {
                    "start": 1407,
                    "end": 1556
                },
                {
                    "start": 1557,
                    "end": 1597
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.05499267578125
        },
        {
            "corpus_id": "260435365",
            "title": "Pre-Trained Language Models for Text Generation: A Survey",
            "text": "Then, GPT-2 [153] explored the transfer capacity of language models for zero-shot generation task, highlighting the significance of sufficient data. Furthermore, GPT-3 [14] showed that massive model parameters can significantly improve the downstream generation tasks, with a few examples or prompts. CTRL [90] is proposed as a conditional causal LM to generate text based on control codes that govern style, content, and task-specific behavior. Causal LMs are simple and straightforward for text generation, but they have several structural and algorithmic limitations: Causal LMs encode the tokens just from left to right, thus ignore the bidirectional information on the input side. Moreover, causal LMs are not specially designed for the sequence-to-sequence generation tasks, thus in practice they do not achieve high performance in tasks such as summarization and translation [153]. \n\n4.1.3 Prefix Language Models. Upon a single Transformer, prefix LMs adopt bidirectional encoding scheme in the input side and natural left-to-right generation pattern in the output side. By utilizing the mixture attention mask, the tokens in the input text  can attend to each other, while the tokens in the target text  can only attend to all input tokens and previous generated tokens. \n\nUniLM [36] was the first prefix LM. Compared to causal LMs, UniLM used prefix attention mask to solve conditional generation tasks, similar to the encoder-decoder architecture. UniLMv2 [5] and GLM [39] improved vanilla prefix masking strategy by introducing permuted language modeling in XLNet [198]. Although prefix LMs have several advantages, Raffel et al. [154] compared single-Transformer prefix LMs to Transformer-based encoder-decoder LMs and concluded that adding explicit encoder-decoder attention is more effective to capture conditional dependencies. \n\n4.1.4 Encoder-Decoder Language Models. Encoder-decoder LMs follow the standard Transformer architecture for text generation, consisting of stacks of both encoder and decoder layers.",
            "score": 0.47363667479172494,
            "section_title": "Standard Architecture",
            "char_start_offset": 34171,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 148
                },
                {
                    "start": 149,
                    "end": 300
                },
                {
                    "start": 301,
                    "end": 445
                },
                {
                    "start": 446,
                    "end": 685
                },
                {
                    "start": 686,
                    "end": 888
                },
                {
                    "start": 891,
                    "end": 896
                },
                {
                    "start": 897,
                    "end": 920
                },
                {
                    "start": 921,
                    "end": 1077
                },
                {
                    "start": 1078,
                    "end": 1278
                },
                {
                    "start": 1281,
                    "end": 1316
                },
                {
                    "start": 1317,
                    "end": 1457
                },
                {
                    "start": 1458,
                    "end": 1581
                },
                {
                    "start": 1582,
                    "end": 1842
                },
                {
                    "start": 1845,
                    "end": 1850
                },
                {
                    "start": 1851,
                    "end": 1883
                },
                {
                    "start": 1884,
                    "end": 2026
                }
            ],
            "ref_mentions": [
                {
                    "start": 12,
                    "end": 17,
                    "matchedPaperCorpusId": "160025533"
                },
                {
                    "start": 168,
                    "end": 172,
                    "matchedPaperCorpusId": "218971783"
                },
                {
                    "start": 882,
                    "end": 887,
                    "matchedPaperCorpusId": "160025533"
                },
                {
                    "start": 1466,
                    "end": 1469,
                    "matchedPaperCorpusId": "211572655"
                },
                {
                    "start": 1478,
                    "end": 1482,
                    "matchedPaperCorpusId": "232270067"
                },
                {
                    "start": 1575,
                    "end": 1580,
                    "matchedPaperCorpusId": "195069387"
                },
                {
                    "start": 1641,
                    "end": 1646,
                    "matchedPaperCorpusId": "204838007"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.54736328125
        },
        {
            "corpus_id": "244904514",
            "title": "A novel time\u2013frequency Transformer based on self\u2013attention mechanism and its application in fault diagnosis of rolling bearings",
            "text": "It is important to note the differences in the mode of usage of the Transformer block. Transformers generally can be divided into three categories, named: 1) encoder-only (e.g., for classification), 2) decoder-only (e.g., for language modeling), and 3) encoder-decoder (e.g., for machine translation). The vanilla Transformer proposed by Vaswani uses an encoder-decoder structure (as shown in Fig. 1) for machine translation, which is a Seq2Seq problem. The decoder of vanilla Transformer uses Transformer blocks that are different from the aforementioned those for the encoder. The proposed method in this paper, however, adopts an encoder-only structure, so details of decoders are not introduced.",
            "score": 0.47310150404166573,
            "section_title": "Transformer Mode",
            "char_start_offset": 13455,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 86
                },
                {
                    "start": 87,
                    "end": 301
                },
                {
                    "start": 302,
                    "end": 453
                },
                {
                    "start": 454,
                    "end": 578
                },
                {
                    "start": 579,
                    "end": 699
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.402587890625
        },
        {
            "corpus_id": "225094578",
            "title": "Cascaded Encoders for Unifying Streaming and Non-Streaming ASR",
            "text": "Tab. 1 shows results using the RNN-T baseline, and the model that uses LSTM-based cascaded encoders (CASE RNN-T). The causal encoder in CASE RNN-T model has the same architecture as the baseline RNN-T. The non-causal encoder is a 2-layer bidirectional LSTM, with \u223c10M parameters. Compared to the baseline RNN-T, CASE RNN-T in causal mode obtains very similar WERs. But when used in the non-causal mode, it improves WER on VS by 14% and NVS by 19%. On the long-form T-AB set, WER improves by 16%. A fully bidirectional RNN-T model obtains similar WERs as the non-causal CASE RNN-T on NVS and T-AB sets, but improves VS WER by another 6% relative. The results show that even with 2 layers of bidirectional LSTMs we can obtain WERs that are almost as good a fully bidirectional model using cascaded encoders.\n\nThe table also shows results using LAS-based [12] (LAS) and deliberation-based (LAS-Delib) [13] two-pass models models. Performance on VS improves compared to the RNN-T baseline, when using LAS or LAS-Delib in decode mode or rescore mode (first-pass hypotheses generated by RNN-T are rescored in this mode). But only LAS-Delib outperforms CASE RNN-T by 4%, when the latter is used in non-causal mode. And on the slightly longer NVS set, and the long-form T-AB set, non-causal CASE RNN-T outperforms LAS and LAS-Delib. As expected, on T-AB, LAS and LAS-Delib marginally worsens WERs when used in rescore mode, and performs poorly when used in decode-mode. The results clearly show the advantages of CASE RNN-T over LAS and LAS-Delib.  Table 2. WERs for RNN-T, C-T, cascaded encoders RNN-T when using conformer layers",
            "score": 0.47297035026262507,
            "section_title": "Cascaded Encoders RNN-T model",
            "char_start_offset": 14810,
            "sentence_offsets": [],
            "ref_mentions": [
                {
                    "start": 852,
                    "end": 856,
                    "matchedPaperCorpusId": "201667597"
                },
                {
                    "start": 898,
                    "end": 902,
                    "matchedPaperCorpusId": "212747696"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.230224609375
        },
        {
            "corpus_id": "260900427",
            "title": "Using Text Injection to Improve Recognition of Personal Identifiers in Speech",
            "text": "The text-injection model used in this work includes a speech encoder, a text encoder with a learned duration model, a shared encoder, decoder and an alignment decoder following [4]. The speech encoder consists of 4 causal conformer layers. The shared encoder consists of 3 causal conformer layers and 10 non-causal conformer layers, each with model dimension of 512. The text encoder contains 2 conformer layers and 4 lightweight convolutional upsampling layers [18]. HAT decoders [19] with v 2 embeddings [20] are used in both decoder and an alignment decoder with the distinction of the former produces word-piece models as text outputs and the latter uses phonemes as model units to get speech-text alignments. The overall model contains 165M parameters, with an additional 58M in the text encoder which is only used during training.",
            "score": 0.4726636062921852,
            "section_title": "Text-Injection and ASR Details",
            "char_start_offset": 5807,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 181
                },
                {
                    "start": 182,
                    "end": 239
                },
                {
                    "start": 240,
                    "end": 366
                },
                {
                    "start": 367,
                    "end": 467
                },
                {
                    "start": 468,
                    "end": 713
                },
                {
                    "start": 714,
                    "end": 836
                }
            ],
            "ref_mentions": [
                {
                    "start": 177,
                    "end": 180,
                    "matchedPaperCorpusId": "248006130"
                },
                {
                    "start": 462,
                    "end": 466,
                    "matchedPaperCorpusId": "225040157"
                },
                {
                    "start": 481,
                    "end": 485,
                    "matchedPaperCorpusId": "212737031"
                },
                {
                    "start": 506,
                    "end": 510,
                    "matchedPaperCorpusId": "216288689"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.304931640625
        },
        {
            "corpus_id": "251719567",
            "title": "PanDa: Prompt Transfer Meets Knowledge Distillation for Efficient Model Adaptation",
            "text": "Additionally, to combine the advantages of decoder-only PLMs and encoder-only PLMs, encoder-decoder PLMs are sequentially proposed (e.g., T5 [4] and BART [17]), which firstly employ a separate encoder to model the source text and then use a left-to-right LM to decode the conditional target text. The encoder-decoder paradigm makes these PLMs not only generally suitable for text generation, but also well for text understanding tasks [22]. \n\nIn this paper, we mainly focus on the adaptation of discriminative (encoder-only) language models and aim to improve its efficiency via a simple yet effective prompt-tuning method. Hence, we review the related work of prompt-tuning on discriminative language models in the following part.",
            "score": 0.47241593534196047,
            "section_title": "II. RELATED WORKS A. Pretrained Language Model",
            "char_start_offset": 6627,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 296
                },
                {
                    "start": 297,
                    "end": 440
                },
                {
                    "start": 443,
                    "end": 623
                },
                {
                    "start": 624,
                    "end": 731
                }
            ],
            "ref_mentions": [
                {
                    "start": 154,
                    "end": 158,
                    "matchedPaperCorpusId": "204960716"
                },
                {
                    "start": 435,
                    "end": 439,
                    "matchedPaperCorpusId": "249192274"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.52099609375
        },
        {
            "corpus_id": "273901601",
            "title": "How Are Metaphors Processed by Language Models? The Case of Analogies",
            "text": "For decoder-only LMs such as GPT (Radford et al.), we compute the perplexity of a tokenized sentence x \" rx 1 ...x m s as \n\nwhere P lm px|xq is the likelihood of the next token given the precedent tokens. For masked language models (MLM) such as BERT (Devlin et al., 2019), \n\nFigure 1: Medians of the ratios between the perplexities of the metaphoric and literal instances (solid lines) and between the anomalous and metaphoric instances (dashed lines) for decoder only models on the left, masked and encoder-decoder models on the right, for the Jankowiak dataset (upper plots) and Green dataset (lower plots). \n\npseudo-perplexity (Salazar et al., 2020) is used instead, which replaces the likelihood P in Equation 1 by P mask px j |x zj q, the pseudo-likelihood (Wang and Cho, 2019) to predict the masked token x j . For encoder-decoder LMs such as T5 (Raffel et al., 2020), we compute P lm on the decoder, which is conditioned by the encoder. We should emphasize that perplexity values are model-dependent. Thus, in this work we have not attempted to measure perplexity values across LMs, but only for comparing sentences within the same LM.6",
            "score": 0.4720029166883996,
            "section_title": "Models.",
            "char_start_offset": 14492,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 121
                },
                {
                    "start": 124,
                    "end": 204
                },
                {
                    "start": 205,
                    "end": 273
                },
                {
                    "start": 276,
                    "end": 610
                },
                {
                    "start": 613,
                    "end": 817
                },
                {
                    "start": 818,
                    "end": 944
                },
                {
                    "start": 945,
                    "end": 1008
                },
                {
                    "start": 1009,
                    "end": 1144
                }
            ],
            "ref_mentions": [
                {
                    "start": 251,
                    "end": 272,
                    "matchedPaperCorpusId": "52967399"
                },
                {
                    "start": 631,
                    "end": 653,
                    "matchedPaperCorpusId": "218628872"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.2218017578125
        },
        {
            "corpus_id": "268247581",
            "title": "Leveraging Biomolecule and Natural Language through Multi-Modal Learning: A Survey",
            "text": "The Transformer model can be specialized into encoder-only (Figure 6a) and decoder-only (Figure 6c) designs to suit different purposes. Encoder-only models [25] specialize in processing input sequences of biomolecules and text through bi-directional self-attention, making them highly effective for tasks that require an in-depth understanding of the input, such as sentiment analysis and feature extraction in NLP. Thereby in biomolecule domain, encoder-only models establish a bi-directional association between biotokens and text tokens for predictive tasks [24]. Encoder-only models are typically designed for representation learning objective obtain strong representations for text and biomolecule. In contrast, decoder-only models [18], [138] employ causal attention to focus on the sequence of previous tokens. This architecture is typically utilized in generative tasks, such as generating text descriptions that match the given molecule or for the reverse task [54]. Thanks to the autoregressive generation property, decoder-only models are well suitable for instruction following and assistant/agent objectives.",
            "score": 0.4717201581820769,
            "section_title": "Encoder/Decoder-only",
            "char_start_offset": 32051,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 135
                },
                {
                    "start": 136,
                    "end": 415
                },
                {
                    "start": 416,
                    "end": 566
                },
                {
                    "start": 567,
                    "end": 703
                },
                {
                    "start": 704,
                    "end": 817
                },
                {
                    "start": 818,
                    "end": 975
                },
                {
                    "start": 976,
                    "end": 1121
                }
            ],
            "ref_mentions": [
                {
                    "start": 156,
                    "end": 160,
                    "matchedPaperCorpusId": "52967399"
                },
                {
                    "start": 561,
                    "end": 565,
                    "matchedPaperCorpusId": "246815222"
                },
                {
                    "start": 970,
                    "end": 974,
                    "matchedPaperCorpusId": "258762343"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.64990234375
        },
        {
            "corpus_id": "271874360",
            "title": "Mamba Retriever: Utilizing Mamba for Effective and Efficient Dense Retrieval",
            "text": "In Section 3.1, we let Mamba be the base model  of retriever. As comparison, the base model  is changed from Mamba to other frequently-used base models, including the Transformer encoderonly models and decoder-only models. In this section, we analyze the differences between using Mamba and these models as . \n\nMamba and Decoder-only vs Encoder-only. Mamba and Transformer decoder-only models share similarities that distinguish them from Transformer encoder-only models. In terms of data, Mamba and decoder-only models are pre-trained on more data than most encoder-only models. In particular, Mamba and Pythia [2] are pre-trained on the same data. In terms of architecture, Mamba and decoder-only models have causal characteristics, which is not as suitable as encoder-only models with bi-directional attention for comprehension tasks like retrieval. Mamba can be reconstructed to be bi-directional, but this would lead to a decrease in efficiency. \n\nMamba vs Decoder-only. Intuitively, Transformer decoderonly model can capture long-term dependencies by self-attention mechanism, while Mamba may be limited by the maximum amount of information that can be compressed in latent states. \n\nHowever, some works [1,3,5,16] analyze that Mamba has some mechanism similar to or even surpassing Transformer: Mamba has implicit attention mechanism with good expressiveness; if each SSM is regarded as one head in multi-head self-attention mechanism, then Mamba has more heads than the Transformer; the softmax in self-attention can cause problems, such as over-smoothing, whereas Mamba does not use softmax and thus may better capture subtle differences between different tokens. \n\nIn addition, Mamba has an additional explicit process of summarizing previous information using the latent states. When calculating a token at position , decoder-only model uses the attention mechanism to access keys and values of all previous tokens.",
            "score": 0.4717176241371326,
            "section_title": "Base Model Comparison",
            "char_start_offset": 8036,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 61
                },
                {
                    "start": 62,
                    "end": 222
                },
                {
                    "start": 223,
                    "end": 308
                },
                {
                    "start": 311,
                    "end": 350
                },
                {
                    "start": 351,
                    "end": 471
                },
                {
                    "start": 472,
                    "end": 579
                },
                {
                    "start": 580,
                    "end": 649
                },
                {
                    "start": 650,
                    "end": 852
                },
                {
                    "start": 853,
                    "end": 950
                },
                {
                    "start": 953,
                    "end": 975
                },
                {
                    "start": 976,
                    "end": 1187
                },
                {
                    "start": 1190,
                    "end": 1672
                },
                {
                    "start": 1675,
                    "end": 1789
                },
                {
                    "start": 1790,
                    "end": 1926
                }
            ],
            "ref_mentions": [
                {
                    "start": 612,
                    "end": 615,
                    "matchedPaperCorpusId": "257921893"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.5302734375
        },
        {
            "corpus_id": "265018965",
            "title": "Sentiment Analysis through LLM Negotiations",
            "text": "(2) decoder-only models, which have a decoder and generate text conditioned on the input text like GPT-series models (Radford et al., 2019;Brown et al., 2020;Keskar et al., 2019;Radford et al., 2019;Chowdhery et al., 2022;Ouyang et al., 2022;Zhang et al., 2022a;Scao et al., 2022;Zeng et al., 2022b;Touvron et al., 2023a;Peng et al., 2023;OpenAI, 2023); and (3) encoder-decoder models, which have a pair of encoder-decoder and generate text conditioned on the input representation, such as T5 (Raffel et al., 2020) and its variants (Lewis et al., 2019;Xue et al., 2020). \n\nStarting with GPT-3 (Brown et al., 2020), LLMs have shown emerging capabilities (Wei et al., 2022a)",
            "score": 0.47108735754461806,
            "section_title": "Final Decision Undetermined",
            "char_start_offset": 6718,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 570
                },
                {
                    "start": 573,
                    "end": 672
                }
            ],
            "ref_mentions": [
                {
                    "start": 139,
                    "end": 158,
                    "matchedPaperCorpusId": "218971783"
                },
                {
                    "start": 493,
                    "end": 514,
                    "matchedPaperCorpusId": "204838007"
                },
                {
                    "start": 593,
                    "end": 613,
                    "matchedPaperCorpusId": "218971783"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.230712890625
        },
        {
            "corpus_id": "268724233",
            "title": "FoC: Figure out the Cryptographic Functions in Stripped Binaries with LLMs",
            "text": "Specifically, we apply mean pooling to the hidden states of each token in the last layer of the encoder, which is to obtain an embedding representing the input function. We then use cosine distance to measure the similarity between these embeddings. As shown in Equation 3, the cosine-similarity loss is used to optimize the model parameters, where the   and    represent the embeddings of the same function in source code and binary code, respectively: \n\nThis approach is based on insight from previous work [43,56], which indicates that the decoder is critical for complex causal-generation tasks and thus requires more careful training. Consequently, we configure the decoder with more layers than the encoder, i.e.,   >   , and the encoder has a smaller proportion of parameters in the overall model. Instead of training the entire large model, as depicted in Figure 5, we freeze the decoder and set only the encoder and the cross-attention layer as trainable, which significantly reduces a large number of trainable parameters for more efficient training.",
            "score": 0.47025812007292456,
            "section_title": "Golden Model Initialization.",
            "char_start_offset": 35751,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 169
                },
                {
                    "start": 170,
                    "end": 249
                },
                {
                    "start": 250,
                    "end": 453
                },
                {
                    "start": 456,
                    "end": 639
                },
                {
                    "start": 640,
                    "end": 804
                },
                {
                    "start": 805,
                    "end": 1060
                }
            ],
            "ref_mentions": [
                {
                    "start": 513,
                    "end": 516,
                    "matchedPaperCorpusId": "246527904"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.1903076171875
        },
        {
            "corpus_id": "241033425",
            "title": "An Empirical Study of Training End-to-End Vision-and-Language Transformers",
            "text": "both types of fusion modules in our new context.\n\nEncoder-Only vs. Encoder-Decoder. Many VLP models such as VisualBERT [24] adopt the encoder-only architecture, where the cross-modal representations are directly fed into an output layer to generate the final outputs. Recently, VL-T5 [6] and SimVLM [51], on the other hand, advocate the use of a transformer encoder-decoder architecture, where the cross-modal representations are first fed into a decoder and then to an output layer. In their models,  the decoder attends to both the encoder representations and the previously generated tokens, producing the outputs autoregressively. Figure 3 shows the difference between them when performing the masked language modeling task. For the encoder-decoder model, when performing classification tasks such as VQA, we feed the text inputs into its encoder and feed a classification token into the decoder, and the decoder then generates the output class accordingly.",
            "score": 0.47008898739948024,
            "section_title": "Mx",
            "char_start_offset": 10178,
            "sentence_offsets": [],
            "ref_mentions": [
                {
                    "start": 284,
                    "end": 287,
                    "matchedPaperCorpusId": "231802355"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.60107421875
        },
        {
            "corpus_id": "272525123",
            "title": "How Does Code Pretraining Affect Language Model Task Performance?",
            "text": "We use the datasets constructed in section 3 as pretraining corpora for causally-masked decoder-only transformer language models (Vaswani et al., 2017;Radford et al., 2019). We construct 12-layer decoder-only models in t5x Roberts et al. (2023). Model hyperparameters were chosen following the methodology of Wang et al. (2022) and Petty et al. (2024) to approximate decoder-only versions of T5-large, resulting in models with roughly 374 M parameters; see Appendix A for hyperparameter details. 2 We pretrain these models with a base natural language data volume of 132 B tokens. This means that all models in the competitive setting were trained with N total = 132 B tokens, while the models in the additive setting were trained with N lang = 132 B tokens, and hence N total varying between 132 B tokens and 264 B tokens depending on the mixture; we use a batch size of 128, meaning that models were trained for between 1 M and 2 M steps, depending on the mixture and setting. For each combination of code mixture and setting, we pretrain models from five different random seeds. We pretrain models on TPUs. We estimate that full replication of the pretraining procedure outlined here would take roughly 750 TPU-days of compute.",
            "score": 0.4699287065010589,
            "section_title": "Model Construction & Training",
            "char_start_offset": 14150,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 173
                },
                {
                    "start": 174,
                    "end": 245
                },
                {
                    "start": 246,
                    "end": 580
                },
                {
                    "start": 581,
                    "end": 978
                },
                {
                    "start": 979,
                    "end": 1081
                },
                {
                    "start": 1082,
                    "end": 1109
                },
                {
                    "start": 1110,
                    "end": 1230
                }
            ],
            "ref_mentions": [
                {
                    "start": 129,
                    "end": 151,
                    "matchedPaperCorpusId": "13756489"
                },
                {
                    "start": 223,
                    "end": 244,
                    "matchedPaperCorpusId": "247942109"
                },
                {
                    "start": 309,
                    "end": 327,
                    "matchedPaperCorpusId": "248118752"
                },
                {
                    "start": 332,
                    "end": 351,
                    "matchedPaperCorpusId": "264820247"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.08831787109375
        },
        {
            "corpus_id": "248118752",
            "title": "What Language Model Architecture and Pretraining Objective Work Best for Zero-Shot Generalization?",
            "text": "To investigate alternative avenues for adaptation, we now introduce non-causal masked language modeling adaptation: starting from a causal decoder model pretrained with FLM as the objective, we then continue training the model as a non-causal decoder using an MLM objective. This is essentially the reverse of the language modeling adaptation setup, and the conversion is as easily undertaken by switching the attention mask. \n\nValidation losses are plotted in Figure 6, on the right. Convergence on the MLM pretraining objective is significantly accelerated: by a factor of 3.3\u00d7 compared to training a non-causal decoder from scratch, and up to a factor 9.1\u00d7 compared to training a causal decoder from scratch (both with a masked language modeling objective). This is a significant improvement over even the previously considered language modeling adaptation, enabling one to obtain both a zero-shot model and an excellent generative model for only 1.3\u00d7 the cost of training a single model. \n\nFinally, we confirm that the improvement in validation loss also transfer to an improvement in zeroshot generalization. We evaluate the non-causal MLM adapted model, and check that it is better than the original causal decoder model pretrained with full language modeling, and control for the total number of training tokens. Specifically, we evaluate zero-shot performance after multitask finetuning in three settings: first, a causal decoder model pretrained with FLM for 219 billion tokens before being multitask finetuned; second, a causal decoder model pretrained with FLM for 219 billion tokens and then multitask finetuned as a non-causal decoder model; and, third, a causal decoder model first trained with FLM for 168 billion tokens, then MLM-adapted as an non-causal model for 51 billion tokens, and finally multitask finetuned. All three variants are multitask finetuned for 13 billion tokens. Results are presented Figure 7. We find that the MLM-adapted model performs best by a significant margin and outperforms every other model we considered on EAI-Eval.",
            "score": 0.4698338910811808,
            "section_title": "Language modeling adaptation (LM-A)",
            "char_start_offset": 37243,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 274
                },
                {
                    "start": 275,
                    "end": 425
                },
                {
                    "start": 428,
                    "end": 484
                },
                {
                    "start": 485,
                    "end": 760
                },
                {
                    "start": 761,
                    "end": 991
                },
                {
                    "start": 994,
                    "end": 1113
                },
                {
                    "start": 1114,
                    "end": 1319
                },
                {
                    "start": 1320,
                    "end": 1832
                },
                {
                    "start": 1833,
                    "end": 1898
                },
                {
                    "start": 1899,
                    "end": 1930
                },
                {
                    "start": 1931,
                    "end": 2064
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.435302734375
        },
        {
            "corpus_id": "264833196",
            "title": "What Formal Languages Can Transformers Express? A Survey",
            "text": "When a transformer decoder or encoder-decoder is run as a language recognizer, it allows for the possibility of inserting a number of intermediate time steps between the end of the input string and the decision. The encoder-decoder models above do this, as do some decoder-only models (Feng et al., 2023;Merrill and Sabharwal, 2024). As we will see ( \u00a76.1), intermediate steps vastly increase the model's power, which has also been observed in practice in the form of a \"scratchpad\" (Nye et al., 2022) or \"chain of thought\" (Wei et al., 2022b).",
            "score": 0.4695500983400669,
            "section_title": "Intermediate steps",
            "char_start_offset": 13974,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 211
                },
                {
                    "start": 212,
                    "end": 333
                },
                {
                    "start": 334,
                    "end": 544
                }
            ],
            "ref_mentions": [
                {
                    "start": 285,
                    "end": 304,
                    "matchedPaperCorpusId": "258865989"
                },
                {
                    "start": 304,
                    "end": 332,
                    "matchedPaperCorpusId": "263909434"
                },
                {
                    "start": 483,
                    "end": 501,
                    "matchedPaperCorpusId": "244773644"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.1605224609375
        },
        {
            "corpus_id": "270514399",
            "title": "SuperGLEBer: German Language Understanding Evaluation Benchmark",
            "text": "Depending on the of transformer architecture, we use different training approaches, each tailored to the specific model: we distinguish between encoder-only, decoder-only and encoder-decoder models and follow the established training approaches for the respective model as defined in the used library.For transformers following the encoder or decoder architecture, we finetune the text classification tasks using the standard approach of adding a linear layer on top of the output representation of the CLS token, while for sequence tagging tasks we use the same approach, but train the linear layer to predict the correct class on top of the output representation of each input token individually.\n\nFor the sentence similarity we follow the Sentence-BERT (Reimers and Gurevych, 2019) approach and finetune the model using a triplet loss with negative sampling on the mean-pooled final output representations of the model.When finetuning for extractive question answering, we again follow the standard approach of adding a linear layer on top of the output representations of the input tokens, and train the linear layer to predict the start and end token of the answer span.For transformer models following the encoder+decoder architecture, we adopt the practices in the respectively used library by discarding the model's decoder entirely for classification, sequence tagging and similarity tasks, and only finetune the encoder part of the model as described above and for question answering tasks we add the span extraction head on top of the decoder output.",
            "score": 0.4685406816867108,
            "section_title": "Training Methodology by LLM Type",
            "char_start_offset": 13853,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 301
                },
                {
                    "start": 301,
                    "end": 698
                },
                {
                    "start": 700,
                    "end": 922
                },
                {
                    "start": 922,
                    "end": 1175
                },
                {
                    "start": 1175,
                    "end": 1561
                }
            ],
            "ref_mentions": [
                {
                    "start": 756,
                    "end": 784,
                    "matchedPaperCorpusId": "201646309"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.166259765625
        },
        {
            "corpus_id": "260886785",
            "title": "Token-Scaled Logit Distillation for Ternary Weight Generative Language Models",
            "text": "In this section, we draw a comparison between the computations of Transformer encoders and decoders to deepen our understanding of the fresh challenges that surface within the realm of GLMs. \n\nCumulative Quantization Errors in Causal Attention. Causal attention, which integrates masking into self-attention to avoid referencing future tokens, is vital for text generation tasks. To comprehend the quantization characteristics of GLMs, we contrast the computation procedure of the self-attention map. For a Transformer encoder, the quantization error reflected on the self-attention map due to Q, K, and V computation is uniformly spread because of the simultaneous computing nature inherent in Transformer encoders. However, the mask in the causal attention accumulates quantization errors from each token, creating uneven errors in the output computation. \n\nTo elucidate, consider the encoder self-attention depicted at the top of Fig. 1(a). During the computation of the weighted sum of value representations for tokens 1 and 3, all attention probabilities in the self-attention map impacted by quantization errors are incorporated. On the other hand, in the decoder's causal attention, as illustrated at the bottom of Fig. 1(a), the output value representation is computed using only the self-attention probabilities of the current token and those before it. For example, while generating the output value representation for token 1 (highlighted in a blue box), only two attention probabilities affected by quantization error are used. In contrast, for token 3 (indicated by the red box), probabilities from all preceding tokens are incorporated. This observation underscores that the inherent design of causal attention results in an accumulation of quantization errors, especially towards the latter tokens. Given this characteristic, it becomes imperative to devise a decoder QAT approach that mitigates this uneven distribution of quantization errors within the causal attention mechanism. \n\nNecessity of Ground Truth Loss. In fine-tuning processes, encoder-only models, often used in Natural Language Understanding (NLU), and decoder-only models, common in Natural Language Generation (NLG) with causal language modeling approach, exhibit different mechanisms for receiving ground truth loss.",
            "score": 0.4684601268620513,
            "section_title": "Quantization Challenges on GLMs",
            "char_start_offset": 9244,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 190
                },
                {
                    "start": 193,
                    "end": 244
                },
                {
                    "start": 245,
                    "end": 379
                },
                {
                    "start": 380,
                    "end": 500
                },
                {
                    "start": 501,
                    "end": 716
                },
                {
                    "start": 717,
                    "end": 857
                },
                {
                    "start": 860,
                    "end": 943
                },
                {
                    "start": 944,
                    "end": 1135
                },
                {
                    "start": 1136,
                    "end": 1362
                },
                {
                    "start": 1363,
                    "end": 1539
                },
                {
                    "start": 1540,
                    "end": 1650
                },
                {
                    "start": 1651,
                    "end": 1813
                },
                {
                    "start": 1814,
                    "end": 1997
                },
                {
                    "start": 2000,
                    "end": 2031
                },
                {
                    "start": 2032,
                    "end": 2301
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.65283203125
        },
        {
            "corpus_id": "201698436",
            "title": "Latent Part-of-Speech Sequences for Neural Machine Translation",
            "text": "Our goal is to design a model that allows for exhaustive search over syntactic choices. We introduce LaSyn, a new latent model shown in Fig- ure 1c. The syntactic choices are modeled as true latent variables i.e., unobserved variables. Compared to the ideal model in Figure 1a, LaSyn includes two simplifications for tractability: (i) The dependence between successive syntactic choices is modeled indirectly, via the word choices made in the previous time steps. (ii) The word choice at each position depends only on the syntactic choice at the current position and the previous predicted words. Dependence on previous syntactic choices is modeled indirectly. \n\nUnder this model, the joint conditional probability of the target word y n together with its corresponding latent syntactic choice z n2 is given by: P (y n , z n |x, y <n ) = P (y n |z n , x, y <n ) \n\nWe implement LaSyn by modifying the Transformer-based encoder-decoder architecture (Vaswani et al., 2017). As shown in Figure 2, LaSyn consists of a shared encoder for encoding source sentence x and a hybrid decoder that manages the decoding of the latent sequence z (left branch) and the target sentence y (right branch) separately. \n\nThe encoder consists of the standard selfattention layer, which generates representations of each token in the source sentence x. The hybrid decoder consists of a self-attention layer that encodes the output generated thus far (i.e., the partial translation), followed by a inter-attention layer",
            "score": 0.4680037050933383,
            "section_title": "Model Description",
            "char_start_offset": 6263,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 87
                },
                {
                    "start": 88,
                    "end": 148
                },
                {
                    "start": 149,
                    "end": 235
                },
                {
                    "start": 236,
                    "end": 463
                },
                {
                    "start": 464,
                    "end": 596
                },
                {
                    "start": 597,
                    "end": 660
                },
                {
                    "start": 663,
                    "end": 861
                },
                {
                    "start": 864,
                    "end": 970
                },
                {
                    "start": 971,
                    "end": 1197
                },
                {
                    "start": 1200,
                    "end": 1329
                },
                {
                    "start": 1330,
                    "end": 1495
                }
            ],
            "ref_mentions": [
                {
                    "start": 947,
                    "end": 969,
                    "matchedPaperCorpusId": "13756489"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.045013427734375
        },
        {
            "corpus_id": "276771845",
            "title": "Adapting Decoder-Based Language Models for Diverse Encoder Downstream Tasks",
            "text": "Decoder-based language models like Gemma (Gemma Team, 2024a,b) and Gemini (Gemini Team, 2023) have demonstrated remarkable language understanding capabilities. Yet, for many downstream tasks such as classification, regression, and ranking, encoderbased models, particularly those derived from BERT (Devlin et al., 2019) or T5's encoder (Raffel et al., 2020), remain the dominant choice. A key question thus arises: can we effectively adapt the powerful knowledge embedded in decoder-only models to excel in these encoder-centric tasks? \n\nThis work addresses this gap by introducing Gemma Encoder, a novel adaptation of the Gemma decoder model designed for encoder-only architectures. We leverage Gemma's pre-trained weights as a strong initialization point, and then strategically modify the architecture and training procedure to optimize performance on downstream tasks. Our approach centers on three key innovations: \n\nFirst, we augment the model with task-specific pooling and Multi-Layer Perceptron (MLP) layers, exploring various pooling strategies to determine the optimal architecture. Second, we address the critical impact of at-tention mechanisms. Gemma's causal attention, ideal for generative tasks, inherently limits its applicability to encoder-based tasks. We demonstrate that simply enabling bidirectional attention during fine-tuning dramatically improves performance. \n\nThird, we investigate the role of dropout. While often omitted during pre-training of modern decoder models, our empirical analysis reveals that incorporating dropout during fine-tuning significantly enhances Gemma Encoder's robustness and generalization ability. We also analyze different padding strategies to understand the effect on Encoder models. \n\nIn the rest of this paper, we describe the technical details of these modifications1 . To validate the effectiveness of our Gemma Encoder, we conduct our experiments on GLUE benchmarks (Wang et al., 2019(Wang et al., , 2020)), for classification and regression tasks, and the MSMARCO benchmark (Bajaj et al., 2018) for ranking tasks. Our results show that our Gemma Encoder models are able to outperform competitive baselines on these benchmark tasks.",
            "score": 0.46748109429837814,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 159
                },
                {
                    "start": 160,
                    "end": 386
                },
                {
                    "start": 387,
                    "end": 535
                },
                {
                    "start": 538,
                    "end": 683
                },
                {
                    "start": 684,
                    "end": 872
                },
                {
                    "start": 873,
                    "end": 919
                },
                {
                    "start": 922,
                    "end": 1093
                },
                {
                    "start": 1094,
                    "end": 1158
                },
                {
                    "start": 1159,
                    "end": 1272
                },
                {
                    "start": 1273,
                    "end": 1386
                },
                {
                    "start": 1389,
                    "end": 1431
                },
                {
                    "start": 1432,
                    "end": 1652
                },
                {
                    "start": 1653,
                    "end": 1741
                },
                {
                    "start": 1744,
                    "end": 1830
                },
                {
                    "start": 1831,
                    "end": 2077
                },
                {
                    "start": 2078,
                    "end": 2195
                }
            ],
            "ref_mentions": [
                {
                    "start": 298,
                    "end": 319,
                    "matchedPaperCorpusId": "52967399"
                },
                {
                    "start": 336,
                    "end": 357,
                    "matchedPaperCorpusId": "204838007"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.63134765625
        },
        {
            "corpus_id": "254877381",
            "title": "SeqDiffuSeq: Text Diffusion with Encoder-Decoder Transformers",
            "text": "Denoising with Encoder-Decoder Framework Unlike DiffuSeq (Gong et al., 2022) using encoder-only Transformer architecture, we propose using an encoder-decoder Transformers architecture to model the input and output text sequences. For z 0 \u03b8 (z t , w x , t), we use the encoder to process the input sequences w x and use the decoder to model the noisy output sequence z t . Following the previous work (Li et al., 2022), we inject time step information t by adding time step embedding to z t . Using the encoder-decoder architecture has computational convenience during generation because the input sequences w x only require one forward computation through the encoder network during the whole reverse process. Considering the reverse process requires thousands of iterations to generate the output sequences of high quality, the saving of computational resources can be significant. \n\nDuring training and generation, the function z 0 \u03b8 generates denoised samples at the sequence level. Therefore making predictions from the denoising function z 0 \u03b8 resembles the non-autoregressive natural language generation. In this regard, we use a decoder with full attention matrices instead of causal attention matrices to model z t at the sequence level.",
            "score": 0.46631111876424947,
            "section_title": "Diffusion Model",
            "char_start_offset": 10965,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 229
                },
                {
                    "start": 230,
                    "end": 371
                },
                {
                    "start": 372,
                    "end": 491
                },
                {
                    "start": 492,
                    "end": 709
                },
                {
                    "start": 710,
                    "end": 882
                },
                {
                    "start": 885,
                    "end": 985
                },
                {
                    "start": 986,
                    "end": 1110
                },
                {
                    "start": 1111,
                    "end": 1245
                }
            ],
            "ref_mentions": [
                {
                    "start": 400,
                    "end": 417,
                    "matchedPaperCorpusId": "249192356"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.3154296875
        },
        {
            "corpus_id": "275544523",
            "title": "MAGNET: Augmenting Generative Decoders with Representation Learning and Infilling Capabilities",
            "text": "Representation Learning. Text representation learning focuses on understanding contextual relationships within sentences. Traditionally, encoder models dominated this field due to their bidirectional context modeling, using masked language modeling for token-level representations (Devlin et al., 2019;Liu et al., 2019;He et al., 2020;Clark et al., 2020;He et al., 2021) and special tokens with similarity-based optimization for sentencelevel understanding (Gunel et al., 2020;Reimers and Gurevych, 2019;Wu et al., 2020;Carlsson et al., 2021;Gao et al., 2021;Wei et al., 2020). Recent work has explored adapting decoder-only LLMs for text encoding through various methods, including introducing special tokens to the model's vocabulary (Zhang et al., 2024), using last-token or mean-pooled representations (Neelakantan et al., 2022;Wang et al., 2023), or fine-tuning with masked modeling (BehnamGhader et al., 2024) or label supervision (Li et al., 2023;Duki'c and vSnajder, 2024). While some approaches modify the decoder's causal attention to be bidirectional (BehnamGhader et al., 2024;Muennighoff et al., 2024;Li and Li, 2023;Duki'c and vSnajder, 2024;Man et al., 2024), this often compromises the model's text generation abilities. In contrast, MAGNET employs a hybrid attention mechanism that combines causal and bidirectional attention, enabling both robust representation learning and preserved generation capabilities. Text Infilling. Text infilling requires considering both left and right context when generating text in the middle of a sequence. Encoder-decoder models (Raffel et al., 2019;Lewis et al., 2019;Kalinsky et al., 2023) can handle this task by encoding available context and decoding infilled text.",
            "score": 0.46570281238068506,
            "section_title": "Related Works",
            "char_start_offset": 3793,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 24
                },
                {
                    "start": 25,
                    "end": 121
                },
                {
                    "start": 122,
                    "end": 577
                },
                {
                    "start": 578,
                    "end": 981
                },
                {
                    "start": 982,
                    "end": 1236
                },
                {
                    "start": 1237,
                    "end": 1427
                },
                {
                    "start": 1428,
                    "end": 1443
                },
                {
                    "start": 1444,
                    "end": 1557
                },
                {
                    "start": 1558,
                    "end": 1722
                }
            ],
            "ref_mentions": [
                {
                    "start": 281,
                    "end": 302,
                    "matchedPaperCorpusId": "52967399"
                },
                {
                    "start": 335,
                    "end": 354,
                    "matchedPaperCorpusId": "208229926"
                },
                {
                    "start": 477,
                    "end": 504,
                    "matchedPaperCorpusId": "201646309"
                },
                {
                    "start": 520,
                    "end": 542,
                    "matchedPaperCorpusId": "235613354"
                },
                {
                    "start": 542,
                    "end": 559,
                    "matchedPaperCorpusId": "233296292"
                },
                {
                    "start": 736,
                    "end": 756,
                    "matchedPaperCorpusId": "272524678"
                },
                {
                    "start": 954,
                    "end": 980,
                    "matchedPaperCorpusId": "267301268"
                },
                {
                    "start": 1130,
                    "end": 1156,
                    "matchedPaperCorpusId": "267301268"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.48388671875
        },
        {
            "corpus_id": "269626143",
            "title": "You Only Cache Once: Decoder-Decoder Architectures for Language Models",
            "text": "The decoder-only Transformer [VSP + 17] has become the de facto architecture for language models.Numerous efforts have continued to develop suitable architectures for language modeling.There have been main strands of explorations.First, encoder-only language models, such as BERT [DCLT19], bidirectionally encode the input sequence.Second, encoder-decoder models, such as T5 [RSR + 20], use a bidirectional encoder to encode input and a unidirectional decoder to generate output.Both of the above layouts struggle with autoregressive generation due to bidirectionality.Specifically, encoders have to encode the whole input and output tokens again for the next generation step.Although encoder-decoder can use only decoder to generate, the output tokens do not fully leverage the parameters of encoder, especially for multi-turn conversation.Third, decoder-only language models, such as GPT [BMR + 20], generate tokens autoregressively.By caching the previously computed key/value vectors, the model can reuse them for the current generation step.The key-value (KV) cache avoids encoding the history again for each token, greatly improving the inference speed.This compelling feature establishes the decoder-only language model as the standard option.\n\nHowever, as the number of serving tokens increases, the KV caches occupy a lot of GPU memory, rendering the inference of large language models memory-bounded [PDC + 22].For the example of a 65B-size language model (augmented with grouped-query attention [ALTdJ + 23] and 8-bit KV quantization), 512K tokens occupy about 86GB GPU memory, which is even larger than the capacity of one H100-80GB GPU.In addition, the prefilling latency of long-sequence input is extremely high.For instance, using four H100 GPUs, the 7B language model (augmented with Flash-Decoding [DHMS23] and kernel fusion) requires about 110 seconds to prefill 450K tokens, and 380 seconds for 1M length.",
            "score": 0.46564791410872164,
            "section_title": "Introduction",
            "char_start_offset": 398,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 97
                },
                {
                    "start": 97,
                    "end": 185
                },
                {
                    "start": 185,
                    "end": 230
                },
                {
                    "start": 230,
                    "end": 332
                },
                {
                    "start": 332,
                    "end": 479
                },
                {
                    "start": 479,
                    "end": 569
                },
                {
                    "start": 569,
                    "end": 676
                },
                {
                    "start": 676,
                    "end": 841
                },
                {
                    "start": 841,
                    "end": 935
                },
                {
                    "start": 935,
                    "end": 1046
                },
                {
                    "start": 1046,
                    "end": 1159
                },
                {
                    "start": 1159,
                    "end": 1250
                },
                {
                    "start": 1252,
                    "end": 1421
                },
                {
                    "start": 1421,
                    "end": 1649
                },
                {
                    "start": 1649,
                    "end": 1726
                },
                {
                    "start": 1726,
                    "end": 1924
                }
            ],
            "ref_mentions": [
                {
                    "start": 280,
                    "end": 288,
                    "matchedPaperCorpusId": "52967399"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.51513671875
        },
        {
            "corpus_id": "273963038",
            "title": "When are 1.58 bits enough? A Bottom-up Exploration of BitNet Quantization",
            "text": "Contemporary machine learning models, such as language models, are powerful, but come with immense resource requirements both at training and inference time. It has been shown that decoder-only language models can be trained to a competitive state with ternary weights (1.58 bits per weight), facilitating efficient inference. Here, we start our exploration with non-transformer model architectures, investigating 1.58-bit training for multi-layer perceptrons and graph neural networks. Then, we explore 1.58-bit training in other transformer-based language models, namely encoder-only and encoder-decoder models. Our results show that in all of these settings, 1.58-bit training is on par with or sometimes even better than the standard 32/16-bit models.",
            "score": 0.4643577374581579,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.0753173828125
        },
        {
            "corpus_id": "257019949",
            "title": "JEIT: Joint End-to-End Model and Internal Language Model Training for Speech Recognition",
            "text": "We train HAT and MHAT with 2-pass cascaded encoders and separate decoders as in [38,39]. They share the same front-end and encoder architecture. Specifically, 128-dim log Mel filterbanks are extracted from speech signal and are subsampled to form a 512-dim feature every 30 ms. Each speech feature is appended with a 16dim domain ID [40]. The causal encoder is a 7-layer conformer with causal convolution and left-context attention. The non-causal encoder is a 10-layer conformer with right-context attention that processes 900 ms of speech into the future. Each conformer layer uses a 512-dim 8-head self-attention and a convolution kernel of size 15. \n\nThe causal and non-causal decoders of HAT or MHAT decode using the outputs of the causal and non-causal encoders, respectively. The label decoders of HAT and MHAT are 2-layer LSTMs with 2048 hidden units in each layer. In HAT, the label decoder output passes through a 640-dim feedforward joint network before projected to 4096 output units representing word pieces [41]. In MHAT, the label decoder output is directly projected to the output layer of the same size. ILMs of HAT and MHAT have 30.7M and 30M parameters, respectively. The blank decoder of MHAT is a 320-dim V 2 embedding decoder [42] with a look-up table shared between the last 2 tokens and has 1.5M parameters. Overall, HAT and MHAT have in total 205M and 210M model parameters, respectively. We report only the 2nd pass WER in this paper. We train baselines with only audio-transcript pairs and show their WERs in Table 1. \n\nMoreover, we train a 12-layer conformer LM with 384-dim selfattention and 3072-dim feedforward layer [6]. The external LM has left attention context of 31 and has in total 70M parameters.",
            "score": 0.4643007952197857,
            "section_title": "Modeling",
            "char_start_offset": 9470,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 88
                },
                {
                    "start": 89,
                    "end": 144
                },
                {
                    "start": 145,
                    "end": 277
                },
                {
                    "start": 278,
                    "end": 338
                },
                {
                    "start": 339,
                    "end": 432
                },
                {
                    "start": 433,
                    "end": 557
                },
                {
                    "start": 558,
                    "end": 652
                },
                {
                    "start": 655,
                    "end": 782
                },
                {
                    "start": 783,
                    "end": 873
                },
                {
                    "start": 874,
                    "end": 1026
                },
                {
                    "start": 1027,
                    "end": 1120
                },
                {
                    "start": 1121,
                    "end": 1152
                },
                {
                    "start": 1153,
                    "end": 1186
                },
                {
                    "start": 1187,
                    "end": 1331
                },
                {
                    "start": 1332,
                    "end": 1413
                },
                {
                    "start": 1414,
                    "end": 1460
                },
                {
                    "start": 1461,
                    "end": 1544
                },
                {
                    "start": 1547,
                    "end": 1652
                },
                {
                    "start": 1653,
                    "end": 1734
                }
            ],
            "ref_mentions": [
                {
                    "start": 80,
                    "end": 84,
                    "matchedPaperCorpusId": "248157514"
                },
                {
                    "start": 84,
                    "end": 87,
                    "matchedPaperCorpusId": "225094578"
                },
                {
                    "start": 333,
                    "end": 337,
                    "matchedPaperCorpusId": "204900916"
                },
                {
                    "start": 1021,
                    "end": 1025,
                    "matchedPaperCorpusId": "22320655"
                },
                {
                    "start": 1248,
                    "end": 1252,
                    "matchedPaperCorpusId": "237532186"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.60791015625
        },
        {
            "corpus_id": "259950775",
            "title": "Integration of large language models and federated learning",
            "text": "Language Models (LMs) aim to predict the probability distribution of future tokens based on a given sequence of tokens [146]. As the size of model parameters and the amount of training data increase, LLMs have shown impressive capabilities in handling complex tasks, including In-context Learning (ICL) [19], instruction following [162,115,172], and step-by-step reasoning [174]. \n\nThe success of LLMs is not just due to their larger model sizes and extensive training data but also owes much to the Transformer architecture [161]. Existing LLMs primarily rely on two design architectures [207]: only-decoder, and encoder-decoder [161], with the only-decoder architectures further divided into causal decoder [123,19] and prefix decoder [204]. Causal decoder architectures, which combine a unidirectional attention mask to ensure each input token can only attend to past tokens and itself [122], have been widely adopted across various existing LLMs, offering significant advantages with massive training data. Specifically, GPT-3 [19] successfully demonstrated the effectiveness of this architecture. Zhao et. al. [207] outline three key stages of training LLMs: pre-training, instruction-tuning, and alignment-tuning. During the pre-training stage, LLMs learn basic language processing abilities and world knowledge across a broad corpus, such as grammar, syntax, and general knowledge. Instruction-tuning becomes crucial for refining LLMs' ability to handle new tasks effectively. It involves crafting precise task instructions or contextual learning strategies to bolster the model's adaptability to unseen tasks [172]. Despite the benefits, there's a risk of instruction-fine-tuned models generating harmful content due to potential misalignment with human values [14,175]. Therefore, aligning LLMs with human values, such as honesty and harmlessness, through alignment-tuning has become an important task. To this end, InstructGPT [115] proposes alignment training methods, including supervised fine-tuning and reinforcement learning from human feedback [115].",
            "score": 0.4642489844944859,
            "section_title": "Background 2.1 Large Language Models",
            "char_start_offset": 3948,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 125
                },
                {
                    "start": 126,
                    "end": 379
                },
                {
                    "start": 382,
                    "end": 531
                },
                {
                    "start": 532,
                    "end": 743
                },
                {
                    "start": 744,
                    "end": 1010
                },
                {
                    "start": 1011,
                    "end": 1101
                },
                {
                    "start": 1102,
                    "end": 1110
                },
                {
                    "start": 1111,
                    "end": 1219
                },
                {
                    "start": 1220,
                    "end": 1388
                },
                {
                    "start": 1389,
                    "end": 1483
                },
                {
                    "start": 1484,
                    "end": 1623
                },
                {
                    "start": 1624,
                    "end": 1778
                },
                {
                    "start": 1779,
                    "end": 1911
                },
                {
                    "start": 1912,
                    "end": 2066
                }
            ],
            "ref_mentions": [
                {
                    "start": 303,
                    "end": 307,
                    "matchedPaperCorpusId": "218971783"
                },
                {
                    "start": 331,
                    "end": 336,
                    "matchedPaperCorpusId": "239009562"
                },
                {
                    "start": 336,
                    "end": 340,
                    "matchedPaperCorpusId": "246426909"
                },
                {
                    "start": 340,
                    "end": 344,
                    "matchedPaperCorpusId": "237416585"
                },
                {
                    "start": 373,
                    "end": 378,
                    "matchedPaperCorpusId": "246411621"
                },
                {
                    "start": 709,
                    "end": 714,
                    "matchedPaperCorpusId": "160025533"
                },
                {
                    "start": 714,
                    "end": 717,
                    "matchedPaperCorpusId": "218971783"
                },
                {
                    "start": 737,
                    "end": 742,
                    "matchedPaperCorpusId": "246441975"
                },
                {
                    "start": 1031,
                    "end": 1035,
                    "matchedPaperCorpusId": "218971783"
                },
                {
                    "start": 1617,
                    "end": 1622,
                    "matchedPaperCorpusId": "237416585"
                },
                {
                    "start": 1769,
                    "end": 1773,
                    "matchedPaperCorpusId": "262580630"
                },
                {
                    "start": 1937,
                    "end": 1942,
                    "matchedPaperCorpusId": "246426909"
                },
                {
                    "start": 2060,
                    "end": 2065,
                    "matchedPaperCorpusId": "246426909"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.55810546875
        },
        {
            "corpus_id": "268041362",
            "title": "A Survey on Recent Advances in LLM-Based Multi-turn Dialogue Systems",
            "text": "LLMs are a class of extensive artificial intelligence models characterized by their massive scale with billions of parameters [14]. Scaling up LLMs allows them to learn more intricate and accurate language representations, resulting in improved performance across diverse downstream Natural Language Processing (NLP) tasks, particularly excelling in Natural Language Generation (NLG) challenges [15,16]. The brief comparison of different structures of the LLMs mentioned can be seen in Table 1. Fig. 1. The matrix comparison of attention mask patterns between decoder-only and encoder-decoder architectures. The matrix uses dark cells to allow for self-attention of input elements  at the output time step , while light cells restrict this attention. The left panel represents the full input attention, the middle panel refers to preventing future input reliance, and the right panel combines causal masking with a prefix for partial input sequence fully-visible masking. [18] The vanilla Transformer architecture [17], a sequence-to-sequence model, has emerged as a foundational framework for diverse LLMs, utilizing encoders and decoders with self-attention mechanisms as its core components, thanks to its exceptional parallelism and capacity. Based on the masking methods utilized by various attention mechanisms in the model, the current LLMs can be divided into three categories, i.e., Encoder-Decoder, Decoder-only, and Encoder-only. \n\nThe decoder-only category further includes distinctions such as causal decoders and prefix decoders, illustrated in Figure 1. \n\nIn the following subsection, we shall introduce different types of LLMs based on various Transformer architectures.",
            "score": 0.4639698015671688,
            "section_title": "GENERAL METHODS",
            "char_start_offset": 4546,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 131
                },
                {
                    "start": 132,
                    "end": 403
                },
                {
                    "start": 404,
                    "end": 494
                },
                {
                    "start": 495,
                    "end": 502
                },
                {
                    "start": 503,
                    "end": 607
                },
                {
                    "start": 608,
                    "end": 750
                },
                {
                    "start": 751,
                    "end": 971
                },
                {
                    "start": 972,
                    "end": 1246
                },
                {
                    "start": 1247,
                    "end": 1440
                },
                {
                    "start": 1443,
                    "end": 1568
                },
                {
                    "start": 1571,
                    "end": 1686
                }
            ],
            "ref_mentions": [
                {
                    "start": 399,
                    "end": 402,
                    "matchedPaperCorpusId": "212747830"
                },
                {
                    "start": 972,
                    "end": 976,
                    "matchedPaperCorpusId": "204838007"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.5517578125
        },
        {
            "corpus_id": "249431810",
            "title": "LegoNN: Building Modular Encoder-Decoder Models",
            "text": "The authors of [41], [42] have shown that ASR encoders trained with CTC loss can use language models trained with text-only resources to decode CTC distributions into words. These models are modular, but they are not trained end-to-end and cannot be conditioned on the error patterns of the encoder.\n\nFor comparison with the LegoNN encoder-decoder model, we decoded the CTC distributions of LegoNN encoders presented in Table II with a language model trained on the same data. The LegoNN encoder with WFST decoding using a language model [41] achieves 9.1% and 19.4% WER on the SWB and CH test sets averaged across 3 seeds, which is inferior to the 8.4% and 18.2% WER achieved by the LegoNN encoder-decoder model (Table II). We expect this gap to be greater when comparing the MT translation models, where the performance gap between LegoNN encoders and LegoNN encoder-decoder models is larger (Table I).\n\nAdditionally, we can still utilize external language models while decoding LegoNN encoder-decoder models via shallowfusion during beam-search [29] and other techniques such as back-translation [47]. For future directions, the ability to train decoder-only LegoNN modules can allow training decoder modules on text-only data. For example, simulating CTC-like distributions with text-only data can allow decoder modules to accept those distributions and train on text-only data.\n\nThe LegoNN approach also offers a novel way to use additional data from various different tasks and languages (Tables III  and IV). For example, a LegoNN encoder-decoder model with multiple encoders processing different modalities can train a decoder module with data from multiple tasks spanning various modalities.",
            "score": 0.46387415747473293,
            "section_title": "D. Incorporating Text-Only Resources in LegoNNs",
            "char_start_offset": 43205,
            "sentence_offsets": [],
            "ref_mentions": [
                {
                    "start": 15,
                    "end": 19,
                    "matchedPaperCorpusId": "206514100"
                },
                {
                    "start": 21,
                    "end": 25,
                    "matchedPaperCorpusId": "11590585"
                },
                {
                    "start": 538,
                    "end": 542,
                    "matchedPaperCorpusId": "206514100"
                },
                {
                    "start": 1048,
                    "end": 1052,
                    "matchedPaperCorpusId": "4556070"
                },
                {
                    "start": 1099,
                    "end": 1103,
                    "matchedPaperCorpusId": "15600925"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.22607421875
        },
        {
            "corpus_id": "271329037",
            "title": "A Comparison of Language Modeling and Translation as Multilingual Pretraining Objectives",
            "text": "To allow a systematic evaluation, we train models with various neural network architectures and learning objectives. All models are based on the transformer architecture (Vaswani et al., 2017) and implemented in fairseq (Ott et al., 2019). We consider both double-stacks (encoder-decoder) and single-stacks (encoder-only or decoder-only) models. \n\nThe two double-stack models are variants of the BART architecture of (Lewis et al., 2020); they are trained either on a straightforward machine translation (MT) objective, using language tokens to distinguish the source, or on the original denoising auto-encoder objective of Lewis et al.. We refer to these two models as 2-LM and 2-MT respectively. \n\nWe also consider three single-stack models: (i) an encoder-only model trained on the masked language modeling objective (MLM) of Devlin et al. (2019); (ii) an autoregressive causal language model (CLM), similar to Radford et al. (2019); and (iii) an autoregressive model trained to generate a sentence, followed by its translation in the language specified by a given control token, known as a translation language model (TLM) as proposed by Conneau and Lample (2019). 1 We provide an example datapoint for each pretraining objective in Table 3, Appendix A. \n\nPretraining conditions. Our core focus is on guaranteeing comparable conditions across the different pretraining objectives we consider. This entails that our datasets need to be doubly structured: both in documents for CLM pretraining; and as aligned bitexts for MT pretraining. Two datasets broadly match these criteria: the UNPC (Ziemski et al., 2016) and OpenSubtitles (OpSub; Tiedemann, 2012) corpora. The choice also narrows down the languages considered in this study: we take the set of languages present in both resources, namely the six languages in UNPC: Arabic (AR), Chinese (ZH), English (EN), French (FR), Russian (RU), and Spanish (ES).",
            "score": 0.4632773544709222,
            "section_title": "Models and objectives.",
            "char_start_offset": 3596,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 116
                },
                {
                    "start": 117,
                    "end": 239
                },
                {
                    "start": 240,
                    "end": 345
                },
                {
                    "start": 348,
                    "end": 637
                },
                {
                    "start": 638,
                    "end": 697
                },
                {
                    "start": 700,
                    "end": 1257
                },
                {
                    "start": 1260,
                    "end": 1283
                },
                {
                    "start": 1284,
                    "end": 1396
                },
                {
                    "start": 1397,
                    "end": 1539
                },
                {
                    "start": 1540,
                    "end": 1666
                },
                {
                    "start": 1667,
                    "end": 1911
                }
            ],
            "ref_mentions": [
                {
                    "start": 170,
                    "end": 192,
                    "matchedPaperCorpusId": "13756489"
                },
                {
                    "start": 220,
                    "end": 238,
                    "matchedPaperCorpusId": "91184134"
                },
                {
                    "start": 417,
                    "end": 437,
                    "matchedPaperCorpusId": "204960716"
                },
                {
                    "start": 829,
                    "end": 849,
                    "matchedPaperCorpusId": "52967399"
                },
                {
                    "start": 914,
                    "end": 935,
                    "matchedPaperCorpusId": "160025533"
                },
                {
                    "start": 1142,
                    "end": 1167,
                    "matchedPaperCorpusId": "58981712"
                },
                {
                    "start": 1592,
                    "end": 1614,
                    "matchedPaperCorpusId": "11644625"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.28857421875
        },
        {
            "corpus_id": "274150503",
            "title": "SynEHRgy: Synthesizing Mixed-Type Structured Electronic Health Records using Decoder-Only Transformers",
            "text": "Decoder-only Transformer models, such as the ones used in language modeling (e.g., GPT [41]), typically employ a causal language modeling (CLM) objective as their loss function. This objective is also known as autoregressive language modeling. The core idea is to predict the next token in a sequence given all previous tokens. The model generates a probability distribution over the vocabulary for the next token, and the loss function measures the discrepancy between the predicted distribution and the actual next token. For an tokenized patient sequence x = [x 1 , x 2 , . . . , x T ], the model predicts the probability distribution over the vocabulary for each token in the sequence, conditioned on all previous tokens: \n\nThe loss for a single token prediction is typically computed using cross-entropy between the predicted distribution and the one-hot encoded true next token. For the entire sequence, the loss function is:",
            "score": 0.46317732913747967,
            "section_title": "Model Architecture",
            "char_start_offset": 9402,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 177
                },
                {
                    "start": 178,
                    "end": 243
                },
                {
                    "start": 244,
                    "end": 327
                },
                {
                    "start": 328,
                    "end": 523
                },
                {
                    "start": 524,
                    "end": 580
                },
                {
                    "start": 581,
                    "end": 725
                },
                {
                    "start": 728,
                    "end": 884
                },
                {
                    "start": 885,
                    "end": 931
                }
            ],
            "ref_mentions": [
                {
                    "start": 87,
                    "end": 91,
                    "matchedPaperCorpusId": "160025533"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.06756591796875
        },
        {
            "corpus_id": "258049081",
            "title": "Decoder-Only or Encoder-Decoder? Interpreting Language Model as a Regularized Encoder-Decoder",
            "text": "Though the decoder-only Language Model (LM) is simply a decoder, it is still difficult to be compared with an Encoder-Decoder (ED) structure because this decoder handles both the source sequence and the target sequence together. To facilitate the comparison between the ED and LM structure, we propose to analyze a Regularized Encoder-Decoder (RED) framework as illustrated in Figure 2. It is a variant of the traditional ED framework while replicating the behaviors of an LM. Compared with the traditional ED structure, the RED framework mainly has the following different components: An unidirectional cross attention attends to both the source matrix and the target matrix simultaneously; a source auto-encoder recovers the input source; a parameter sharing mechanism shares the parameters between the encoder and the decoder; a layer-wise coordination component makes each decoder layer attending to the corresponding encoder layer output; a consecutive positional encoding utilizes a positional encoding starting from the length of the source tokens in the decoder. \n\nUnidirectional Cross Attention. The main difference between the ED framework and the LM is how the input source information is merged into the decoder. As illustrated in Figure 2, the ED framework first uses multiple Transformer blocks to extract features H E \u22121 from the source sequence s. Afterwards, it utilizes a self attention ATT D l to get the feature matrix G D l . It then uses an encoder attention ATT J l to take G D l as query and uses the encoder's final output H E \u22121 as the key and value to calculate Q D l . On the other hand, an LM uses an unidirectional attention to handle the concatenated features. To simulate this mechanism in the LM, as illustrated in Figure 2, the RED framework uses unidirectional cross attention ATT l which attends to both the source matrix G E l and the target matrix G D l simultaneously. Since it attends to all features with one attention, the output matrix Q D l of the attention layer becomes less sensitive to the input source matrix G E l especially when it has already generated many words and G D l becomes relatively long.",
            "score": 0.46296184880055835,
            "section_title": "Regularized Encoder-Decoder",
            "char_start_offset": 9858,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 228
                },
                {
                    "start": 229,
                    "end": 476
                },
                {
                    "start": 477,
                    "end": 1070
                },
                {
                    "start": 1073,
                    "end": 1104
                },
                {
                    "start": 1105,
                    "end": 1224
                },
                {
                    "start": 1225,
                    "end": 1363
                },
                {
                    "start": 1364,
                    "end": 1446
                },
                {
                    "start": 1447,
                    "end": 1596
                },
                {
                    "start": 1597,
                    "end": 1691
                },
                {
                    "start": 1692,
                    "end": 1907
                },
                {
                    "start": 1908,
                    "end": 2150
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.70263671875
        },
        {
            "corpus_id": "269791266",
            "title": "When LLMs step into the 3D World: A Survey and Meta-Analysis of 3D Tasks via Multi-modal Large Language Models",
            "text": "In the context of LLMs, the \"encoder-decoder\" and \"decoderonly\" architectures are prominently used for NLP tasks.\n\nEncoder-decoder architectures [95,118,119] consist of two main components: an encoder f enc and a decoder f dec .The encoder and decoder components are typically implemented using transformers [95], which employ attention mechanisms to capture long-range dependencies in the input and output sequences.The encoder takes the input sequence X = (x 1 , x 2 , . . ., x N ) and maps it into a sequence of latent representations H = (h 1 , h 2 , . . ., h N ) that capture the contextual information, and the decoder generates the output sequence Y = (y 1 , y 2 , . . ., y T ) based on H. Mathematically, the encoding process can be expressed as H = f enc (X), and the entire latent sequence H is generated at once from X.The decoder, however, generates the output sequence Y sequentially: y t = f dec (y <t , H) where y <t = (y 1 , y 2 , . . ., y t\u22121 ).\n\nDecoder-only architectures [120,121,122], on the other hand, are a variant of the transformer architecture that uses only the decoder component.It is particularly suitable for language modeling tasks, where the goal is to predict the next token given the previous tokens.The decoder-only architecture can be mathematically expressed as y t = f dec (y <t ).\n\nTokenization is a preprocessing method to break the input text into a sequence of tokens, the basic data unit in language models.The number of tokens is finite, and each token can correspond to a word, sub-word, or a single letter.During inference, the input text is converted to a sequence of tokens and fed to the models, which predict the output tokens that are then converted back to text.The tokenization has a great impact on the performance of the language models, as it affects how the models perceive text.",
            "score": 0.4628069578269912,
            "section_title": "LLM Architectures",
            "char_start_offset": 9206,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 113
                },
                {
                    "start": 115,
                    "end": 228
                },
                {
                    "start": 228,
                    "end": 417
                },
                {
                    "start": 417,
                    "end": 476
                },
                {
                    "start": 476,
                    "end": 560
                },
                {
                    "start": 560,
                    "end": 677
                },
                {
                    "start": 677,
                    "end": 830
                },
                {
                    "start": 830,
                    "end": 952
                },
                {
                    "start": 952,
                    "end": 962
                },
                {
                    "start": 964,
                    "end": 1108
                },
                {
                    "start": 1108,
                    "end": 1235
                },
                {
                    "start": 1235,
                    "end": 1320
                },
                {
                    "start": 1322,
                    "end": 1451
                },
                {
                    "start": 1451,
                    "end": 1553
                },
                {
                    "start": 1553,
                    "end": 1715
                },
                {
                    "start": 1715,
                    "end": 1837
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.57666015625
        },
        {
            "corpus_id": "261100939",
            "title": "Pre-Trained Model-Based Automated Software Vulnerability Repair: How Far are We?",
            "text": "Pre-trained models have significantly improved performance across a wide range of natural language processing (NLP) and code-related tasks such as machine translation, defect detection, and code classification [34], [35]. Typically, the models are pre-trained to derive generic vector representation by self-supervised training on a large-scale unlabeled corpus and then are transferred to benefit multiple downstream tasks by fine-tuning on a limited labeled corpus [36]. \n\nExisting pre-trained models generally adopt the encoderdecoder transformer architecture, which can be classified into three types: encoder-only, decoder-only, and encoderdecoder models. Encoder-only models (e.g., CodeBERT [22]) usually pre-train a bidirectional transformer where tokens can attend to each other. Encoder-only models are good at understanding tasks (e.g., code search), but their bidirectionality nature requires an additional decoder for generation tasks. Decoder-only models (e.g., GPT [37]) are pre-trained using unidirectional language modeling that only allows tokens to attend to the previous tokens and themselves to predict the next token. Decoder-only models are good at auto-regressive tasks like code completion, but the unidirectional framework is sub-optimal for understanding tasks. Encoder-decoder models (e.g., T5 [38]) often make use of denoising pre-training objectives that corrupt the source input and require the decoder to recover them. Compared to encoder-only and decoder-only models that favor understanding and auto-regressive tasks, encoder-decoder models can support generation tasks well like code summarization. \n\nIn the context of vulnerability repair, an encoder stack takes a sequence of code tokens as input to map a vulnerable function X i = [x 1 , . . . , x n ] into a fixed-length intermediate hidden state, while the decoder stack takes the hidden state vector as an input to generate the output sequence of tokens Y i = [y 1 , . . . , y n ].",
            "score": 0.4625399303172597,
            "section_title": "Pre-trained Model",
            "char_start_offset": 10761,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 221
                },
                {
                    "start": 222,
                    "end": 472
                },
                {
                    "start": 475,
                    "end": 660
                },
                {
                    "start": 661,
                    "end": 787
                },
                {
                    "start": 788,
                    "end": 947
                },
                {
                    "start": 948,
                    "end": 1138
                },
                {
                    "start": 1139,
                    "end": 1287
                },
                {
                    "start": 1288,
                    "end": 1449
                },
                {
                    "start": 1450,
                    "end": 1632
                },
                {
                    "start": 1635,
                    "end": 1780
                },
                {
                    "start": 1781,
                    "end": 1962
                },
                {
                    "start": 1963,
                    "end": 1971
                }
            ],
            "ref_mentions": [
                {
                    "start": 216,
                    "end": 220,
                    "matchedPaperCorpusId": "221761146"
                },
                {
                    "start": 979,
                    "end": 983,
                    "matchedPaperCorpusId": "218971783"
                },
                {
                    "start": 1321,
                    "end": 1325,
                    "matchedPaperCorpusId": "204838007"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.447265625
        },
        {
            "corpus_id": "258841100",
            "title": "Exploring Geometric Representational Disparities between Multilingual and Bilingual Translation Models",
            "text": "Our models cover at most 3 language families for the sake of controlled analysis when modern multilingual translation models cover many more. We think it is worthwhile to analyze models with larger coverage as future work. We focus on one-to-many models as they tend to fall behind other multilingual model types (Sachan and Neubig, 2018;Wang et al., 2018;Shaham et al., 2023). However, manyto-many models still have multilingual decoders but may have different behavior given their multilingual encoder state space. \n\nAdditionally, our conclusions focus on encoderdecoder models, but there is growing interest in decoder-only translation models whose isotropic behavior may differ. \n\nFinally, our work focuses only on the characterization of representational capacity differences between model types, and not on the improvement of representational capacity of one-to-many models. However, we hope this work provides insight into the development of future modeling techniques for models with multilingual decoders.",
            "score": 0.4624376331539921,
            "section_title": "Limitations",
            "char_start_offset": 26428,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 141
                },
                {
                    "start": 142,
                    "end": 222
                },
                {
                    "start": 223,
                    "end": 377
                },
                {
                    "start": 378,
                    "end": 516
                },
                {
                    "start": 519,
                    "end": 682
                },
                {
                    "start": 685,
                    "end": 880
                },
                {
                    "start": 881,
                    "end": 1014
                }
            ],
            "ref_mentions": [
                {
                    "start": 313,
                    "end": 338,
                    "matchedPaperCorpusId": "52154258"
                },
                {
                    "start": 338,
                    "end": 356,
                    "matchedPaperCorpusId": "53079244"
                },
                {
                    "start": 356,
                    "end": 376,
                    "matchedPaperCorpusId": "254685798"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.10430908203125
        },
        {
            "corpus_id": "221246122",
            "title": "Amortized learning of neural causal representations",
            "text": "There are 3 main components to our model as shown in Figure 2. First, we evaluate if the attentive relational model is necessary to capture causal relationships. Hence, we evaluate against a monolitic baseline model such as a fully-connected MLP, this then forms our LSTM baseline model. We then evaluate the amount of performance gain the model could achieve by receiving supervision signal from our graph decoder. Lastly, in order to verify that the information for the conditional graph decoder for decoding the graph is indeed contained in our model, we compare the performance of our model to one with zero information in its input to the graph decoder. The sequence model that accumulates information across time is fairly straightforward and hence we have not performed comparisons against this component. \n\nLSTM baseline. Our model uses a relatively sophisticated relational network to extract information in the input. In order to verify the necessity of this component, we compare our encoder to a LSTM with naive fully connected feed-forward neural network (MLP). For simplicity, we call this the LSTM baseline. For this baseline, we no longer need an autoencoder to process the input, as the autoencoder could just be an identity model. As there are no autoencoder (with its own decoder), the baseline is missing the unsupervised learning signal. To compensate for this, we pass the gradients Q4. Does the accuracy of the decoder reflect the model's ability to recover causal structures? Next, we verify that the belief state in our model contains useful information for decoding the structured causal graph. This baseline uses the same relational encoder-decoder and graph decoder from our original model; the only difference is that the belief state is set to zero. If this baseline could also learn to decode the structured causal graph, it would indicate that it was not the information in the belief state that helped to decode the structured causal graph, it was rather the graph decoder itself. Figure 5 shows that the decoder was not in fact able to decode the causal graph structure.",
            "score": 0.46200098820722313,
            "section_title": "BASELINE MODELS",
            "char_start_offset": 18504,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 161
                },
                {
                    "start": 162,
                    "end": 287
                },
                {
                    "start": 288,
                    "end": 415
                },
                {
                    "start": 416,
                    "end": 658
                },
                {
                    "start": 659,
                    "end": 812
                },
                {
                    "start": 815,
                    "end": 829
                },
                {
                    "start": 830,
                    "end": 927
                },
                {
                    "start": 928,
                    "end": 1074
                },
                {
                    "start": 1075,
                    "end": 1122
                },
                {
                    "start": 1123,
                    "end": 1248
                },
                {
                    "start": 1249,
                    "end": 1358
                },
                {
                    "start": 1359,
                    "end": 1408
                },
                {
                    "start": 1409,
                    "end": 1499
                },
                {
                    "start": 1500,
                    "end": 1620
                },
                {
                    "start": 1621,
                    "end": 1779
                },
                {
                    "start": 1780,
                    "end": 2013
                },
                {
                    "start": 2014,
                    "end": 2104
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.21337890625
        },
        {
            "corpus_id": "272827489",
            "title": "Scaling Laws of Decoder-Only Models on the Multilingual Machine Translation Task",
            "text": "As a consequence, it is more tedious to apply complex self-reasoning mechanisms, such as chain-of-thought (Wei et al., 2022), or to interface it with external tools (Schick et al., 2024), because the outputs of such method (the reasoning process) should, preferably, be treated as inputs of the model. For the same reasons, it is much more computationally expensive to rely on an encoder-decoder for conversational purposes, making this architecture less efficient for modern workflows such as iterative translation. Indeed, at each round (the user's query and the system's answer) should be appended to the input side, and reprocessed by the encoder for the next round. Decoder-only models support it by design, without needing to recompute the representation of the ever-growing inputs. While we do not explore these directions in this work, we do leverage the flexibility of the decoder architecture to include input-or-output parameters. As we are tackling the multilingual and multidomain machine translation task, the model needs input tokens to represent the language direction and the domain. We propose to train the model to predict the source language and the domain so that, during inference, they can be seamlessly predicted or provided by arXiv:2409.15051v1 [cs.CL] 23 Sep 2024 the user. \n\nGenerally speaking, decoder-only models simply expect the input to be the whole discussion and process it in a single forward step. Causal masking enable efficient caching of already computed keys and values so inference is much cheaper. The main downside is that the quality of the input representation might be inferior, as input tokens can attend only on past tokens. But it should not be a major issue, as generated tokens attend to the whole past sequence, they do have access to the same quantity of information as with an encoder-decoder model. In addition, previous work propose to update the attention mask so that input tokens can attend to all input tokens while generated tokens can attend only on past tokens (Tay et al., 2022;Raffel et al., 2020).",
            "score": 0.46198227352374216,
            "section_title": "Introduction",
            "char_start_offset": 1480,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 301
                },
                {
                    "start": 302,
                    "end": 516
                },
                {
                    "start": 517,
                    "end": 670
                },
                {
                    "start": 671,
                    "end": 788
                },
                {
                    "start": 789,
                    "end": 941
                },
                {
                    "start": 942,
                    "end": 1100
                },
                {
                    "start": 1101,
                    "end": 1270
                },
                {
                    "start": 1271,
                    "end": 1300
                },
                {
                    "start": 1303,
                    "end": 1434
                },
                {
                    "start": 1435,
                    "end": 1540
                },
                {
                    "start": 1541,
                    "end": 1673
                },
                {
                    "start": 1674,
                    "end": 1854
                },
                {
                    "start": 1855,
                    "end": 2064
                }
            ],
            "ref_mentions": [
                {
                    "start": 106,
                    "end": 124,
                    "matchedPaperCorpusId": "246411621"
                },
                {
                    "start": 165,
                    "end": 186,
                    "matchedPaperCorpusId": "256697342"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.40576171875
        },
        {
            "corpus_id": "269687065",
            "title": "Sign Language Recognition: A Comprehensive Review of Traditional and Deep Learning Approaches, Datasets, and Challenges",
            "text": "The Encoder-Decoder Network consists of two parts to match two sequences in an intermediary latent space.Encoder aims to encode the input sequence into a fixed size vector, and decoder aims to complete the alignment between input sequence and target sequence in the latent space and output the predicting results.This structure could deal with complicated seq2seq problem, so not only Encoder-Decoder Network could resolve SLR but even SLT, especially for Transformer (which is actually an Encoder-Decoder Network essentially) because it can calculate the attention globally instead of following the sequence order.Here again we emphasis that sign language has its unique grammatical structure as oppose to language, which may lead to the different orders of words in a language sequence.SLR is to output the language sequence with same words order in inputting video sequence while SLT transform the order of words in output language sequence into our spoken language form.In fact, some SLT based on encoder-decoder network incorporates the SLR process, so a uniform presentation is made here.In the early studies, the encoder-decoder network of SLR and SLT are all based on RNN [70], [94], [137].Camgoz et al. [94] embedded the frames in video sequence by 2D CNN and words in label sequence by linear projection and inputted them into the encoder-decoder network based on RNN.\n\n75048 VOLUME 12, 2024 Authorized licensed use limited to the terms of the applicable license agreement with IEEE.Restrictions apply.Guo et al. [137] constructed the hierarchical encoderdecodernetwork, which contains two layers of LSTM and an additional LSTM was used to select key clips from input videos.Puet al. [70] combined encoder-decoder network with CTC, designing a BiLSTM encoder, a LSTM decoder and a CTC decoder.",
            "score": 0.46190742876616236,
            "section_title": "b: ENCODER-DECODER NETWORK",
            "char_start_offset": 68144,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 105
                },
                {
                    "start": 105,
                    "end": 313
                },
                {
                    "start": 313,
                    "end": 615
                },
                {
                    "start": 615,
                    "end": 788
                },
                {
                    "start": 788,
                    "end": 974
                },
                {
                    "start": 974,
                    "end": 1094
                },
                {
                    "start": 1094,
                    "end": 1198
                },
                {
                    "start": 1198,
                    "end": 1378
                },
                {
                    "start": 1380,
                    "end": 1493
                },
                {
                    "start": 1493,
                    "end": 1512
                },
                {
                    "start": 1512,
                    "end": 1685
                },
                {
                    "start": 1685,
                    "end": 1803
                }
            ],
            "ref_mentions": [
                {
                    "start": 1180,
                    "end": 1184,
                    "matchedPaperCorpusId": "195443370"
                },
                {
                    "start": 1186,
                    "end": 1190,
                    "matchedPaperCorpusId": "4724109"
                },
                {
                    "start": 1192,
                    "end": 1197,
                    "matchedPaperCorpusId": "19178535"
                },
                {
                    "start": 1212,
                    "end": 1216,
                    "matchedPaperCorpusId": "4724109"
                },
                {
                    "start": 1523,
                    "end": 1528,
                    "matchedPaperCorpusId": "19178535"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.1357421875
        },
        {
            "corpus_id": "252780443",
            "title": "UL2: Unifying Language Learning Paradigms",
            "text": "The line between decoder-only and encoder-decoder models is less clear. PrefixLM models are almost encoder-decoder models with shared parameters (but not quite). From an inductive bias point of view, there are multiple differences. Encoder-Decoder models process input and targets independently with a different set of parameters. This is a form of sparsity where different set of parameters are used for different tokens. Encoder-Decoder models also have a cross attention component that connects input tokens to target tokens. Meanwhile, decoder-only models process inputs and targets by concatenating them. Hence, the representations of inputs and targets are concurrently build layer by layer as the input/targets propagate up the network. Conversely, the decoder in Encoder-decoder models generally only looks at the fully processed encoder input. Overall, the inductive bias of PrefixLM decoder-only models and Encoder-Decoder models could be pretty similar modulo the subtle differences stated above. The distinct property is that Encoder-Decoder models are generally approximately 2x parameters of a decoder-only model when compute-matched. \n\nSparse Models On a side note, there have also been also an emerging trend of sparse pretrained models that achieve state-of-the-art performance. Sparse mixture-of-expert models such as the Switch Transformer (Fedus et al., 2021), GLaM (Du et al., 2021) and/or GShard (Lepikhin et al., 2020) have also demonstrated a lot of promise. While orthogonal to the topic of pretraining objectives, sparse models achieve a very different flop-per-parameter ratio compared to dense models -a core recurring motif in the debate surrounding encoder-decoder models vs decoder-only models.",
            "score": 0.46186137106523967,
            "section_title": "Decoder-only vs Encoder-Decoder",
            "char_start_offset": 10300,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 71
                },
                {
                    "start": 72,
                    "end": 161
                },
                {
                    "start": 162,
                    "end": 231
                },
                {
                    "start": 232,
                    "end": 330
                },
                {
                    "start": 331,
                    "end": 422
                },
                {
                    "start": 423,
                    "end": 528
                },
                {
                    "start": 529,
                    "end": 609
                },
                {
                    "start": 610,
                    "end": 743
                },
                {
                    "start": 744,
                    "end": 852
                },
                {
                    "start": 853,
                    "end": 1007
                },
                {
                    "start": 1008,
                    "end": 1148
                },
                {
                    "start": 1151,
                    "end": 1295
                },
                {
                    "start": 1296,
                    "end": 1482
                },
                {
                    "start": 1483,
                    "end": 1725
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.84814453125
        },
        {
            "corpus_id": "258461229",
            "title": "CodeGen2: Lessons for Training LLMs on Programming and Natural Languages",
            "text": "Encoder-based models may be trained with the task of masked language modeling in the form of denoising span corruptions (Devlin et al., 2019;Raffel et al., 2020). Decoder-based models may be trained as density language modeling in the form of a next-token-prediction task (Radford et al., 2018). For encoder-based models, the prevalent algorithms are variants of token reconstructions or denoising task for which spans of tokens undergo a corruption or perturbation. For a sequence x = (x 1 , . . . , x n ) of n tokens, a perturbation x = (x 1 , m 1 , x 5 , x 6 , m 2 , x 7 , . . . , x n ) replaces spans of tokens with special mask tokens (m 1 , m 2 , . . .). The learning task is to recover the original sequence x from the perturbation x. \n\nDenoising-based learning objectives have been shown to be highly efficient for language understanding tasks. For decoder-based models, the prevalent algorithm is maximum likelihood-based learning of causal language modeling in the form of a next token prediction task (Radford et al., 2018). For a sequence x = (x 1 , . . . , x n ) of n tokens, the task is to predict the token x i given previous tokens (x j : j < i). In this work, we explore a learning algorithm as a mixture of both causal language modeling objectives and span corruption. We postulate for such a mixture the task-specific prior information should be minimized to avoid over-fitting to specific tasks. That is, ideally, the distributions over mixture ratio of tasks, length of the prefixes, and length of spans are uniform. \n\nSampling Procedure Program synthesis in the form of auto-regressive sampling from a language model has been established as a predominant method. While left-to-right sampling can only take previous tokens into account, often when editing existing files of code, conditioning the sampling on context before and after the current position within a file is desirable.",
            "score": 0.46124375562373165,
            "section_title": "COMPONENTS: ARCHITECTURE, OBJECTIVE, SAMPLING, DATA",
            "char_start_offset": 11354,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 162
                },
                {
                    "start": 163,
                    "end": 295
                },
                {
                    "start": 296,
                    "end": 466
                },
                {
                    "start": 467,
                    "end": 498
                },
                {
                    "start": 499,
                    "end": 581
                },
                {
                    "start": 582,
                    "end": 656
                },
                {
                    "start": 657,
                    "end": 660
                },
                {
                    "start": 661,
                    "end": 741
                },
                {
                    "start": 744,
                    "end": 852
                },
                {
                    "start": 853,
                    "end": 1035
                },
                {
                    "start": 1036,
                    "end": 1067
                },
                {
                    "start": 1068,
                    "end": 1162
                },
                {
                    "start": 1163,
                    "end": 1286
                },
                {
                    "start": 1287,
                    "end": 1415
                },
                {
                    "start": 1416,
                    "end": 1537
                },
                {
                    "start": 1540,
                    "end": 1684
                },
                {
                    "start": 1685,
                    "end": 1903
                }
            ],
            "ref_mentions": [
                {
                    "start": 120,
                    "end": 141,
                    "matchedPaperCorpusId": "52967399"
                },
                {
                    "start": 141,
                    "end": 161,
                    "matchedPaperCorpusId": "204838007"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.329833984375
        },
        {
            "corpus_id": "259376468",
            "title": "DCU at SemEval-2023 Task 10: A Comparative Analysis of Encoder-only and Decoder-only Language Models with Insights into Interpretability",
            "text": "We experimented with two encoder-only and one decoder-only language models (LMs) for the shared-task, including BERT (Devlin et al., 2019), HateBERT (Caselli et al., 2021), andOPT (Zhang et al., 2022). We also explored continued pretraining strategies for encoder-only LMs, namely BERT and HateBERT. Our observations support the finding by Liu et al. (2019) that removing the next sentence prediction task during pre-training improves the LM's performance. Our results indi-cate that both decoder-only and encoder-only LMs, namely HateBERT mlm+ and OPT-1.3B, yield similar results when fine-tuned by placing a fully connected layer over the [CLS] output for binary classification. Moreover, we improved Subtask A performance with a simple voting mechanism, building an ensemble of both the encoder-and decoderbased classifiers. In Subtask B, we find a surprisingly large performance difference between finetuning HateBERT and fine-tuning OPT in our basic one-vs-all approach, with OPT having difficulties in all four binary classification sub-problems. Additionally, we found that our approach of fusing traditional machine learning algorithms for classification with document representations obtained with encoder-only LMs works well for Subtask C. Although this approach is simple, our rank indicates that it can be further improved upon. \n\nFurthermore, our study highlights tance of interpretability and explainability in detection systems, particularly for sensitive issues such as sexism. By using a combination of methods such as input prompts, annotator feedback, and gradient importance scores, we may gain a better understanding of how these systems make decisions and identify areas for improvement. This can ultimately lead to more reliable and accurate detection of sexism and other forms of discrimination. \n\nConference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers), pages 1415-1420, Minneapolis, Minnesota. Association for Computational Linguistics.",
            "score": 0.46102378671252603,
            "section_title": "Conclusion",
            "char_start_offset": 22222,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 201
                },
                {
                    "start": 202,
                    "end": 299
                },
                {
                    "start": 300,
                    "end": 456
                },
                {
                    "start": 457,
                    "end": 558
                },
                {
                    "start": 559,
                    "end": 680
                },
                {
                    "start": 681,
                    "end": 827
                },
                {
                    "start": 828,
                    "end": 1052
                },
                {
                    "start": 1053,
                    "end": 1249
                },
                {
                    "start": 1250,
                    "end": 1340
                },
                {
                    "start": 1343,
                    "end": 1493
                },
                {
                    "start": 1494,
                    "end": 1709
                },
                {
                    "start": 1710,
                    "end": 1819
                },
                {
                    "start": 1822,
                    "end": 2016
                },
                {
                    "start": 2017,
                    "end": 2059
                }
            ],
            "ref_mentions": [
                {
                    "start": 117,
                    "end": 138,
                    "matchedPaperCorpusId": "52967399"
                },
                {
                    "start": 149,
                    "end": 176,
                    "matchedPaperCorpusId": "225062242"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.281982421875
        },
        {
            "corpus_id": "271709868",
            "title": "PTM4Tag+: Tag Recommendation of Stack Overflow Posts with Pre-trained Models",
            "text": "Encoder-Decoder Fig. 1: The architecture of encoder-only and encoder-decoder models models learn practical and generic language representations which could achieve outstanding performance in various downstream tasks simply by fine-tuning them on a smaller dataset, i.e., without training a new model from scratch (Jin et al., 2020;Lin et al., 2021;Qu et al., 2019). With proper training manner, the model can effectively capture the semantics of individual words based on their surrounding context and reflect the meaning of the whole sentence. \n\nA major drawback of Post2Vec is that its underlying neural network (i.e., CNN) has limitations in modeling long input sequences. CNN requires large receptive fields to model long-range dependencies (Schmidhuber, 2015). However, increasing the receptive field dramatically reduces computational efficiency. We addressed this limitation by leveraging the Transformer-based PTMs, which enhanced the architecture with a self-attention mechanism and pre-trained knowledge obtained from other datasets. We categorized the considered PTMs in this paper into two types: the encoder-only PTMs and the encoder-decoder PTMs. The architectural difference between these two types of PTMs is illustrated in Fig 1 . In Table 1, we summarize the architecture, pre-training tasks, downstream tasks from the original paper, and language type of the PTMs used in this paper. Table 2 and 3 presents the details of the abbreviation used in Table 1.",
            "score": 0.46078208738038395,
            "section_title": "Encoder-only",
            "char_start_offset": 13904,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 365
                },
                {
                    "start": 366,
                    "end": 544
                },
                {
                    "start": 547,
                    "end": 675
                },
                {
                    "start": 676,
                    "end": 765
                },
                {
                    "start": 766,
                    "end": 852
                },
                {
                    "start": 853,
                    "end": 1043
                },
                {
                    "start": 1044,
                    "end": 1160
                },
                {
                    "start": 1161,
                    "end": 1247
                },
                {
                    "start": 1248,
                    "end": 1402
                },
                {
                    "start": 1403,
                    "end": 1474
                }
            ],
            "ref_mentions": [
                {
                    "start": 313,
                    "end": 331,
                    "matchedPaperCorpusId": "202539059"
                },
                {
                    "start": 331,
                    "end": 348,
                    "matchedPaperCorpusId": "231846746"
                },
                {
                    "start": 348,
                    "end": 364,
                    "matchedPaperCorpusId": "153312701"
                },
                {
                    "start": 745,
                    "end": 764,
                    "matchedPaperCorpusId": "11715509"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.4267578125
        },
        {
            "corpus_id": "251223807",
            "title": "Learning from flowsheets: A generative transformer model for autocompletion of flowsheets",
            "text": "Besides the original architecture, depending on the application, encoder and decoder stacks can be modified or left out entirely. Auto-regressive models for text generation, also called causal language modeling, typically only use the decoder part of the original transformer, as shown in Figure 2. Auto-regressive means that previously generated tokens are added to the input sequence for the next token generation. More specifically, at each time step, the decoder-only transformer model outputs probabilities for different tokens suitable as the next token in the sequence. The selected token is then added to the previous input sequence (dashed line in Figure 2) before the decoder computes the following outputs. \n\nIn recent years, many model architectures for text generation and pre-trained models were published, such as GPT-2 [33], Transformer-XL [34], and XLNet [35].",
            "score": 0.46010634505885945,
            "section_title": "Auto-regressive transformer for text generation",
            "char_start_offset": 10071,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 129
                },
                {
                    "start": 130,
                    "end": 416
                },
                {
                    "start": 417,
                    "end": 576
                },
                {
                    "start": 577,
                    "end": 717
                },
                {
                    "start": 720,
                    "end": 877
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.5205078125
        },
        {
            "corpus_id": "253237572",
            "title": "Modular Hybrid Autoregressive Transducer",
            "text": "From the speech signal, we extract 128-dim log Mel filterbank features with a 32 ms window and a 10 ms stride. Four adjacent features are stacked to form a 512-dim feature, which is then down-sampled to a frame rate of 30 ms. We append each speech feature with a 16dim domain identifier before passing it to MHAT. MHAT has 2-pass cascaded encoders [63]: the 1st pass causal encoder processes the input speech features and the 2nd pass non-causal encoder operates on the output of the causal encoder. Both label and blank decoders of a cascaded MHAT have to decode either using the output of the causal or non-causal encoder. The causal encoder has 7 conformer layers with only left-context attention to prevent the model from accessing future inputs. The non-causal encoder has 10 conformer layers with additional right-context attention processing 900 ms of speech into the future. A 512-dim self-attention with 8 heads and a convolution kernel of size 15 are used in each conformer layer. To learn from much larger amount of multi-domain text, we increase the capacity of the label decoder to a 2-layer LSTM with 1600 hidden units in each layer. The blank decoder remains to be an embedding decoder with an embedding dimension of 320 and with a look-up table shared between yu\u22122 and yu\u22121. The label decoder output is projected to 4095-dim output layer corresponding to the same word pieces. The label and blank decoder have 43M and 1.3M parameters, respectively, and the entire cascaded MHAT has 155M parameters. During ILMA, the KL regularization weight \u03c1 is set to 0.5 since we want to keep source-domain Voice Search performance unchanged. As a comparison, we also train a cascaded HAT with exactly the same input features and causal encoder as those of MHAT. HAT has an embedding label decoder with an embedding dimension of 640. The cascaded HAT has in total 155M parameters. To match the size of MHAT internal LM, the external LM described in Section 4.2.1 is a 2-layer LSTM",
            "score": 0.45963781091213224,
            "section_title": "Modeling",
            "char_start_offset": 26734,
            "sentence_offsets": [],
            "ref_mentions": [
                {
                    "start": 348,
                    "end": 352,
                    "matchedPaperCorpusId": "225094578"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.29443359375
        },
        {
            "corpus_id": "252846620",
            "title": "Scaling Up Deliberation For Multilingual ASR",
            "text": "Deliberation has been shown to significantly improve monolingual English first-pass models [14,24,34] but its effectiveness remains unknown to multilingual models. When one applies deliberation to a transducer model with cascaded encoder [11], the deliberation decoder can attend to either hypotheses decoded using the causal encoder, or the non-causal encoder. In Table 2, we deliberate on the causal decoding results for better latency. We show in Table 2 that deliberation (E0) improves the baseline (B0) average WER by around 5% relative. We note the improvement is uniform and significant for all languages. Note that our deliberation decoder does not explicitly use any languages information and is thus truly multilingual. The improvement shows that deliberation works effectively for multilingual models, and the improvement in Table 2 presumably comes from both bidirectional encoding of the hypothesis and a second-pass rescoring using both hypotheses and acoustic encoding.",
            "score": 0.4595462930358656,
            "section_title": "Deliberation using Causal Encoder",
            "char_start_offset": 13781,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 163
                },
                {
                    "start": 164,
                    "end": 361
                },
                {
                    "start": 362,
                    "end": 438
                },
                {
                    "start": 439,
                    "end": 542
                },
                {
                    "start": 543,
                    "end": 612
                },
                {
                    "start": 613,
                    "end": 729
                },
                {
                    "start": 730,
                    "end": 984
                }
            ],
            "ref_mentions": [
                {
                    "start": 91,
                    "end": 95,
                    "matchedPaperCorpusId": "212747696"
                },
                {
                    "start": 95,
                    "end": 98,
                    "matchedPaperCorpusId": "231718654"
                },
                {
                    "start": 98,
                    "end": 101,
                    "matchedPaperCorpusId": "249436472"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.1181640625
        },
        {
            "corpus_id": "251135268",
            "title": "Efficient Training of Language Models to Fill in the Middle",
            "text": "We use 8 causal transformer decoder models [Vaswani et al., 2017] with similar architecture, optimization hyperparameters, and encoding to Codex and GPT-3 [Chen et al., 2021, Brown et al., 2020]. The main architectural details of our models are summarized in Table 3. The only architectural modification we introduce is the use of relative attention [Shaw et al., 2018, Dai et al., 2019] rather than learned positional embeddings. This increases the parameter count negligibly but leads to improved performance. We also increase the learning rates of our three largest models by a factor of 2 for improved final performance, as it is known that GPT-3 series of models use rather conservative choices of learning rates. The context size for all the models is 2048. \n\nWe train our code models on the same dataset that was used to train Codex, which is a 159 GB Python dataset scraped in May 2020. As such, we expect no train set contamination from the subsequent public release of HumanEval. Similar to GPT-3 and unlike Codex, we train our models from scratch from a random initialization. All models from the main scans are trained for 100B tokens irrespective of size. Due to this fixed token budget, we expect our largest models to be undertrained [Hoffmann et al., 2022] and to benefit significantly from longer training. For our natural language models, we use the same dataset as was used in GPT-3 [Brown et al., 2020] Table 3: The model architecture for our suite of models. The 6 largest models follow similar architecture as models Small to 6.7B in the GPT-3 paper. The differences in the tables are due to minor calculation errors and typos in Table 2.1 of that paper. The n param column has the total number parameters in each model while n ne column has the number of parameters excluding the embedding and unembedding layers. Following [Kaplan et al., 2020], we use the number of non-embedding parameters in our scaling plots. We do not tie the weights in the embedding and unembedding layers.",
            "score": 0.45919703697989755,
            "section_title": "A Architecture and datasets",
            "char_start_offset": 50203,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 195
                },
                {
                    "start": 196,
                    "end": 267
                },
                {
                    "start": 268,
                    "end": 430
                },
                {
                    "start": 431,
                    "end": 511
                },
                {
                    "start": 512,
                    "end": 718
                },
                {
                    "start": 719,
                    "end": 763
                },
                {
                    "start": 766,
                    "end": 894
                },
                {
                    "start": 895,
                    "end": 989
                },
                {
                    "start": 990,
                    "end": 1087
                },
                {
                    "start": 1088,
                    "end": 1168
                },
                {
                    "start": 1169,
                    "end": 1323
                },
                {
                    "start": 1324,
                    "end": 1479
                },
                {
                    "start": 1480,
                    "end": 1572
                },
                {
                    "start": 1573,
                    "end": 1676
                },
                {
                    "start": 1677,
                    "end": 1836
                },
                {
                    "start": 1837,
                    "end": 1937
                },
                {
                    "start": 1938,
                    "end": 2004
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.04351806640625
        },
        {
            "corpus_id": "264405877",
            "title": "GenDistiller: Distilling Pre-trained Language Models based on Generative Models",
            "text": "The language models can predict the embedding of the target token based on the context or the previous tokens of it. In this work, we use generative language model to predict the hidden layer outputs of teacher network in the hope to take the interaction of the hidden layers into account and avoid seeing the future information. Three language model architectures are considered: encoder-decoder, causal decoder, and prefix decoder [27]. The encoder-decoder architecture [3,28,29] consists of two stacks of Transformer blocks as the encoder and decoder, in which encoder is to encode the input sequence as a common history for the generated sequence and decoder generates the target sequence autoregressively. The causal decoder architecture [30][31][32][33] only attend to the past tokens of the input and itself through a unidirectional attention mask. The prefix decoder architecture [34,35] performs bidirectional attention over the prefix tokens and unidirectional attention only on generated tokens. In our work, we treat the original feature as the prefix tokens and the target hidden layers as the sequence to be generated one-by-one. Therefore, we select the prefix decoder architecture and modify it to some extend to build our distiller which can bidirectionally encode the input features and predict the output layers autoregressively, meanwhile, the required parameters are less than the encoder-decoder architecture.",
            "score": 0.4590277557378998,
            "section_title": "Generative Language Models",
            "char_start_offset": 6038,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 116
                },
                {
                    "start": 117,
                    "end": 329
                },
                {
                    "start": 330,
                    "end": 438
                },
                {
                    "start": 439,
                    "end": 710
                },
                {
                    "start": 711,
                    "end": 855
                },
                {
                    "start": 856,
                    "end": 1006
                },
                {
                    "start": 1007,
                    "end": 1143
                },
                {
                    "start": 1144,
                    "end": 1431
                }
            ],
            "ref_mentions": [
                {
                    "start": 433,
                    "end": 437,
                    "matchedPaperCorpusId": "257900969"
                },
                {
                    "start": 472,
                    "end": 475,
                    "matchedPaperCorpusId": "204838007"
                },
                {
                    "start": 475,
                    "end": 478,
                    "matchedPaperCorpusId": "13756489"
                },
                {
                    "start": 478,
                    "end": 481,
                    "matchedPaperCorpusId": "204960716"
                },
                {
                    "start": 751,
                    "end": 755,
                    "matchedPaperCorpusId": "218971783"
                },
                {
                    "start": 892,
                    "end": 895,
                    "matchedPaperCorpusId": "253018395"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.625
        },
        {
            "corpus_id": "257833842",
            "title": "BloombergGPT: A Large Language Model for Finance",
            "text": "Our model is a decoder-only causal language model based on BLOOM (Scao et al., 2022). We present an overview of the architecture, with full details in Appendix A. The model contains 70 layers of transformer decoder blocks defined as follows:\n\nwhere SA is multi-head self-attention, LN is layer-normalization, and FFN is a feed-forward network with 1-hidden layer. Inside FFN, the non-linear function is GELU (Hendrycks and Gimpel, 2016). ALiBi positional encoding is applied through additive biases at the selfattention component of the transformer network (Le Scao et al., 2022). The input token embeddings are tied to the linear mapping before the final softmax. Following Le Scao et al. (2022) and first used in Dettmers et al. (2022), the model has an additional layer normalization after token embeddings, formally:\n\nwhere h 0 is the initial token embedding and LN em is the new component of embedding layernormalization. Notice that the second term includes two consecutive layer-normalizations.",
            "score": 0.45887231487251623,
            "section_title": "Architecture",
            "char_start_offset": 20051,
            "sentence_offsets": [],
            "ref_mentions": [
                {
                    "start": 557,
                    "end": 579,
                    "matchedPaperCorpusId": "247625205"
                },
                {
                    "start": 678,
                    "end": 696,
                    "matchedPaperCorpusId": "247625205"
                },
                {
                    "start": 715,
                    "end": 737,
                    "matchedPaperCorpusId": "238408308"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.0457763671875
        },
        {
            "corpus_id": "204904179",
            "title": "Multitask Learning For Different Subword Segmentations In Neural Machine Translation",
            "text": "We experiment with two variants of the BMTL model. BMTL1 has inputs of BPE10K and decoders of BPE300, BPE1K and BPE10K (as seen in Figure 1). BMTL2 has inputs of BPE32K and decoders of BPE10K, BPE16K and BPE32K. We experiment with different input segmentations to show that our architecture shows improvements irrespective of the input unit. For each BMTL model, we also train three baseline encoder-decoder models -each with the same input units as BMTL and an output corresponding to one of the BMTL decoders. For instance, we compare the output of BMTL1's BPE300 decoder with an encoder-decoder model that has input units of BPE10K and output units of BPE300. \n\nTable 1 shows the results of our experiments on the BMTL1 and BMTL2 models, as well as the baseline models. We observe that almost all of our BMTL decoders (in both BMTL1 and BMTL2) outperform the corresponding baseline models across all three languages, with an improvement of upto 2 BLEU points. This exhibits our architecture's ability to learn more robust encoded representations, irrespective of language, input units, and combination of output segmentations. 3 hese improvements are on models that have the same size as the baselines. Although at training time, the model includes multiple decoders and a shared encoder, while testing, we need to utilize only a single decoder and encoder, thus making it comparable to the baseline models. Each of our BMTL decoders also converges faster than the corresponding individual baseline models (Figure 2).",
            "score": 0.45881248015599696,
            "section_title": "Results",
            "char_start_offset": 10584,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 50
                },
                {
                    "start": 51,
                    "end": 141
                },
                {
                    "start": 142,
                    "end": 211
                },
                {
                    "start": 212,
                    "end": 341
                },
                {
                    "start": 342,
                    "end": 511
                },
                {
                    "start": 512,
                    "end": 662
                },
                {
                    "start": 665,
                    "end": 772
                },
                {
                    "start": 773,
                    "end": 962
                },
                {
                    "start": 963,
                    "end": 1131
                },
                {
                    "start": 1132,
                    "end": 1205
                },
                {
                    "start": 1206,
                    "end": 1410
                },
                {
                    "start": 1411,
                    "end": 1520
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.1214599609375
        },
        {
            "corpus_id": "239009900",
            "title": "Guiding Visual Question Generation",
            "text": "al. 2020). Following standard sequence-to-sequence causal decoding practices, our decoder receives some encoder outputs, and auto-regressively samples the next token, for use in the next decoding timestep. Since our encoder outputs are the concatenation (the ; operator) of our textual and vision modality representation: X = [S; i] \u2208 R (T +ko)\u00d7d , our decoder thus takes on the form:\n\nwhereq is the predicted question.\n\nIn this work, we primarily focus on a set-to-sequence problem as opposed to a sequence-to-sequence problem. That is, our textual input is not a natural language sequence, rather an unordered set comprising of tokens from the answer category, the object labels, and the caption. How this set is obtained is discussed in following section. Noteworthily, due to the set input, we disable positional encoding on the PLM encoder.",
            "score": 0.4581651138298162,
            "section_title": "Methodology",
            "char_start_offset": 10850,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.133056640625
        },
        {
            "corpus_id": "264426794",
            "title": "Which Prompts Make The Difference? Data Prioritization For Efficient Human LLM Evaluation",
            "text": "We assemble an evaluation pool comprised of several state-of-the-art and widely used language models including Flan-T5 [20] and Dolly-V2 [23] families, as well as MPT-7b-instruct [74] and falcon-7b-instruct [56]. Each of these models, ranging from 3 billion to 12 billion parameters, has been selected for their demonstrated capabilities across various NLP tasks and the diversity they bring to our pool regarding architectural differences and training methodologies. While Flan-T5 is an encoder-decoder model, all other models are decoder only. The base models also differ between models, ranging from Pythia [8] to T5 [21]. Table 1 provides additional information about each of the models.",
            "score": 0.45806909664174744,
            "section_title": "Models",
            "char_start_offset": 14375,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 212
                },
                {
                    "start": 213,
                    "end": 467
                },
                {
                    "start": 468,
                    "end": 545
                },
                {
                    "start": 546,
                    "end": 625
                },
                {
                    "start": 626,
                    "end": 691
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.089111328125
        },
        {
            "corpus_id": "247011991",
            "title": "SOTitle: A Transformer-based Post Title Generation Approach for Stack Overflow",
            "text": "The structure of the decoder is similar to the encoder. The difference is that it uses a standard attention mechanism to focus on the encoder output after each self-attentive layer. The self-attention mechanism in the decoder also uses a type of autoregressive or causal self-attention that allows the model to only focus on the past outputs.",
            "score": 0.45746462729163007,
            "section_title": "C. Transformer-based Autoregressive Decoder",
            "char_start_offset": 16115,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 55
                },
                {
                    "start": 56,
                    "end": 181
                },
                {
                    "start": 182,
                    "end": 342
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.4619140625
        },
        {
            "corpus_id": "268157336",
            "title": "A Review of Current Trends, Techniques, and Challenges in Large Language Models (LLMs)",
            "text": "Many language tasks, such as natural language inference (NLI) and question answering, require understanding of the relationship between sentences. The NSP objective is used to capture such relationships, where, given an input sentence, the goal is to predict the following sentence. The NSP task takes two sequences (S1 and S2) as input to predict whether S2 is the direct continuation of S1 or not. Table 2 details different objectives, datasets, and tokens and/or corpus sizes used during the pretraining of prominent LLM models. Transformer-base [24] encoder-decoder Transformer-big [24] encoder-decoder MLM, NSP WMT 2014 -BERT-base [26] Encoder-only BERT-large [26] Encoder-only MLM, NSP BooksCorpus, English Wikipedia 137 B, -GPT-1 [27] Decoder-only Causal/LTR-LM BooksCorpus, 1B Word Benchmark -GPT-2 [28] Decoder   In multitask learning (MTL), parameters are shared between multiple tasks during pretraining. This leads to better generalization and performance improvement of related tasks. MTL helps to improve performance on new domains by leveraging the knowledge and representation learned from related tasks during pretraining. MTL uses a single model to perform many downstream tasks simultaneously. However, unlike the adapter layers, MTL requires simultaneous access to the tasks during pretraining. The networks' lower MTL layers (and their weights) are shared among the tasks, using specialized higher layers based on the downstream tasks. \n\nDuring 'multitask learning', datasets from different tasks are mixed and used. As experimented in T5 [25], multitask learning involves pretraining the model on multiple tasks simultaneously. Although multiple tasks were used during pretraining, the T5 model was finetuned separately on supervised downstream tasks. One crucial factor to consider in multitask learning is how much data the model should be trained on from each task. \n\nThere needs to be a proper balance where the model sees enough data to perform well regarding the task while not exposing it to more data such that it starts memorizing (overfitting) the dataset.",
            "score": 0.4558917879263753,
            "section_title": "Next-Sentence Prediction (NSP) Objective",
            "char_start_offset": 27984,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 146
                },
                {
                    "start": 147,
                    "end": 282
                },
                {
                    "start": 283,
                    "end": 399
                },
                {
                    "start": 400,
                    "end": 531
                },
                {
                    "start": 532,
                    "end": 915
                },
                {
                    "start": 916,
                    "end": 997
                },
                {
                    "start": 998,
                    "end": 1139
                },
                {
                    "start": 1140,
                    "end": 1212
                },
                {
                    "start": 1213,
                    "end": 1314
                },
                {
                    "start": 1315,
                    "end": 1456
                },
                {
                    "start": 1459,
                    "end": 1537
                },
                {
                    "start": 1538,
                    "end": 1649
                },
                {
                    "start": 1650,
                    "end": 1773
                },
                {
                    "start": 1774,
                    "end": 1890
                },
                {
                    "start": 1893,
                    "end": 2088
                }
            ],
            "ref_mentions": [
                {
                    "start": 807,
                    "end": 811,
                    "matchedPaperCorpusId": "160025533"
                },
                {
                    "start": 1560,
                    "end": 1564,
                    "matchedPaperCorpusId": "204838007"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.242919921875
        },
        {
            "corpus_id": "258841633",
            "title": "Modular Domain Adaptation for Conformer-Based Streaming ASR",
            "text": "The backbone model is a Conformer transducer model [24]. The encoder is based on the cascaded structure [34,35] which consists of 7 causal Conformer blocks without future context followed by 10 non-causal Conformer blocks that use 900 ms of future acoustic context. The first two causal blocks do not have MHSA modules. The model dimensions are 512 and 640 for causal and non-causal encoders. For all Conformer blocks, MHSA modules have 8 attention heads; Conv modules have a convolution kernel size of 15; and the dimension of the FFN module is four times the model dimension. The causal and noncausal encoders have 47M and 99M parameters respectively. The outputs of both encoders are projected to 384 dimensions. There are two separate hybrid autoregressive transducer (HAT) decoders [36] for causal and non-causal encoders. The output vocabulary has 4096 wordpieces [37]. The joint network that combines features from the encoder and the embedding prediction network [38] has 640 units. Each decoder has 9.5M parameters. The total number of parameters of the entire model is 165M. During training, two encoders are selected randomly with the same probability and FastEmit loss [39] is applied with a scale of 0.005. All models were trained using the Lingvo toolkit [40] for 300k steps on 8 \u00d7 8 tensor processing units (TPUs) with a batch size of 4096.",
            "score": 0.4553791844402421,
            "section_title": "Model",
            "char_start_offset": 8332,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 56
                },
                {
                    "start": 57,
                    "end": 265
                },
                {
                    "start": 266,
                    "end": 319
                },
                {
                    "start": 320,
                    "end": 392
                },
                {
                    "start": 393,
                    "end": 577
                },
                {
                    "start": 578,
                    "end": 653
                },
                {
                    "start": 654,
                    "end": 715
                },
                {
                    "start": 716,
                    "end": 827
                },
                {
                    "start": 828,
                    "end": 875
                },
                {
                    "start": 876,
                    "end": 990
                },
                {
                    "start": 991,
                    "end": 1024
                },
                {
                    "start": 1025,
                    "end": 1084
                },
                {
                    "start": 1085,
                    "end": 1219
                },
                {
                    "start": 1220,
                    "end": 1355
                }
            ],
            "ref_mentions": [
                {
                    "start": 51,
                    "end": 55,
                    "matchedPaperCorpusId": "218674528"
                },
                {
                    "start": 104,
                    "end": 108,
                    "matchedPaperCorpusId": "225094578"
                },
                {
                    "start": 108,
                    "end": 111,
                    "matchedPaperCorpusId": "249437304"
                },
                {
                    "start": 787,
                    "end": 791,
                    "matchedPaperCorpusId": "212737031"
                },
                {
                    "start": 870,
                    "end": 874,
                    "matchedPaperCorpusId": "22320655"
                },
                {
                    "start": 971,
                    "end": 975,
                    "matchedPaperCorpusId": "237532186"
                },
                {
                    "start": 1181,
                    "end": 1185,
                    "matchedPaperCorpusId": "224814344"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.427978515625
        },
        {
            "corpus_id": "258461112",
            "title": "ContraCLM: Contrastive Learning For Causal Language Model",
            "text": "We provide a comparison between encoder-only (Feng et al., 2020;Guo et al., 2021), encoderdecoder (Ahmad et al., 2021;Wang et al., 2021), and decoder-only models (main focus of this work) on the zero-shot code-to-code search task in Table 7b. We see that CONTRACLM-TOK and CONTR-ACLM outperform the encoder-only model Code-BERT and both the encoder-decoder models. It is important to note that the comparison across these models is not apple-to-apple as these models differ in size, the scale of pretraining, and language settings. This comparison's purpose is to show the promise of decoder-only models being used in discriminative tasks like code search. We further break down the code search performances based on edit similarities and length differences between query code and their relevant code fragments. We present the results in Figure 7 and 8. We observe a similar performance trend in all three languages, although cross-lingual search performance still needs to improve. Nonetheless, the objective of this performance analysis is to show that sequence overlap or length are not the reasons for improvements in CONTRACLM-TOK. Instead, a finer-grained understanding of code tokens due to the token-level contrastive learning makes CONTRACLM-TOK more effective.",
            "score": 0.4553647108091705,
            "section_title": "C.2.3 Detailed Code Search Results",
            "char_start_offset": 37332,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 242
                },
                {
                    "start": 243,
                    "end": 364
                },
                {
                    "start": 365,
                    "end": 531
                },
                {
                    "start": 532,
                    "end": 656
                },
                {
                    "start": 657,
                    "end": 811
                },
                {
                    "start": 812,
                    "end": 853
                },
                {
                    "start": 854,
                    "end": 982
                },
                {
                    "start": 983,
                    "end": 1136
                },
                {
                    "start": 1137,
                    "end": 1270
                }
            ],
            "ref_mentions": [
                {
                    "start": 45,
                    "end": 64,
                    "matchedPaperCorpusId": "211171605"
                },
                {
                    "start": 64,
                    "end": 81,
                    "matchedPaperCorpusId": "221761146"
                },
                {
                    "start": 98,
                    "end": 118,
                    "matchedPaperCorpusId": "232185260"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.6162109375
        },
        {
            "corpus_id": "268363781",
            "title": "Beyond Memorization: The Challenge of Random Memory Access in Language Models",
            "text": "In this work, we mainly explore the memory access pattern of decoder-only language models.Future research is needed to understand whether our conclusions apply to other types of language models based on transformers such as encoder-only models and encoder-decoder models.Furthermore, we do not extend our study to larger models beyond 7 billion parameters due to computing resource constraints.It might be worthwhile to explore further scaling behavior of memory access patterns in even larger language models.In addition, we mainly conduct controlled experiments on a text corpus of fixed size.Further investigation may be needed to explore how the findings can apply to large-scale pretraining corpus and their implications on pretrained language models.",
            "score": 0.4552230790484416,
            "section_title": "Limitation",
            "char_start_offset": 30397,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 90
                },
                {
                    "start": 90,
                    "end": 271
                },
                {
                    "start": 271,
                    "end": 394
                },
                {
                    "start": 394,
                    "end": 510
                },
                {
                    "start": 510,
                    "end": 595
                },
                {
                    "start": 595,
                    "end": 756
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.293701171875
        },
        {
            "corpus_id": "273811349",
            "title": "Birdie: Advancing State Space Models with Reward-Driven Objectives and Curricula",
            "text": "Encoder-Decoder models encode a prefix sequence into a single vector (or set of hidden states, for cross-attention) to generate a suffix from (Kalchbrenner and Blunsom, 2013;Cho et al., 2014;Sutskever et al., 2014;Raffel et al., 2020). The prefix-LM architecture, introduced in T5 and UniLM (Raffel et al., 2020;Dong et al., 2019), simplifies this two-stage approach by instead only using self-attention layers with specific masking to allow for bidirectional processing in the prefix and causality in the suffix regions. We adapt this prefix-LM approach for recurrent models, which allows us to transfer information between the prefix and suffix regions for every layer, in contrast to the single-vector bottleneck with Encoder-Decoders. \n\nThis adaptation ensures our bidirectional SSMs maintain computational and parameter efficiency, enabling fair comparisons with unidirectional models. \n\nTo adapt prefix-LM to our models, we first split the recurrent state into forward and reverse components. The forward components are processed without modification, enabling our bidirectional layers to transmit information from the prefix to the suffix via the forward state dimensions. This contrasts with the bidirectional Encoder layers in Encoder-Decoder models, which are constrained to operate only within the prefix. The reverse components are modified in the suffix region to maintain causality; specifically, we zero out the forget gate (A t ) dimensions. This approach prevents the state from propagating information backward in causal areas. A mathematical description follows2 , and an additional example is included in Appendix Section A.4. \n\nCompared to causal SSMs, our bidirectional SSMs trained with Birdie show greatly enhanced capabilities on question-answering retrieval with information-dense Wikipedia articles in SQuAD V2, shown in Figure 3. As we allocate half of the state for the reverse direction, we conservatively only allow half of Birdie's layers to be bidirectional.",
            "score": 0.455056537615097,
            "section_title": "Bidirectional Processing",
            "char_start_offset": 12929,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 235
                },
                {
                    "start": 236,
                    "end": 521
                },
                {
                    "start": 522,
                    "end": 738
                },
                {
                    "start": 741,
                    "end": 890
                },
                {
                    "start": 893,
                    "end": 998
                },
                {
                    "start": 999,
                    "end": 1179
                },
                {
                    "start": 1180,
                    "end": 1316
                },
                {
                    "start": 1317,
                    "end": 1457
                },
                {
                    "start": 1458,
                    "end": 1545
                },
                {
                    "start": 1546,
                    "end": 1646
                },
                {
                    "start": 1649,
                    "end": 1857
                },
                {
                    "start": 1858,
                    "end": 1991
                }
            ],
            "ref_mentions": [
                {
                    "start": 142,
                    "end": 174,
                    "matchedPaperCorpusId": "12639289"
                },
                {
                    "start": 191,
                    "end": 214,
                    "matchedPaperCorpusId": "250920512"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.406982421875
        },
        {
            "corpus_id": "262084016",
            "title": "A Paradigm Shift in Machine Translation: Boosting Translation Performance of Large Language Models",
            "text": "We evaluate three potential training objectives for decoder-only LLM in machine translation. \n\nCausal Language Modeling (CLM) We first consider a standard language modeling loss that predicts the next token based on all prior tokens. \n\nPrefix Language Modeling (Prefix LM) For decoder-only models, a prefix can be defined with a non-causal attention mask. Analogous to standard language modeling, the model predicts each token outside the prefix based on previous tokens. In the context of machine translation, the provided prompt serves as the prefix, as depicted in Figure 2. \n\nMixture-of-Denoisers (MoD) The UL2 model (Tay et al., 2022a) introduces a unified approach to masking methods, utilizing a mixture-of-denoisers (MoD) strategy, which has also been implemented in the fine-tuning of PaLM (Tay et al., 2022b). This strategy is grounded in three objectives: \n\n\u2022 Regular Denoising: In this approach, noise is sampled in spans and replaced with sentinel tokens, aligning with the standard span corruption technique delineated in Raffel et al. (2020). The parameters set for this objective include a mean of 3 and a corruption rate of 15 \u2022 Extreme Denoising: This method amplifies the noise to a comparatively 'extreme' level, characterized by a mean length of 32 and a corruption rate reaching up to 25 \u2022 Sequential Denoising: This is known as the Prefix LM objective previously mentioned. \n\nIn our training process, we allocate a 25% probability each for both regular and extreme denoising, and a 50% probability for sequential denoising. \n\nWe employ the MPT-7B as our backbone model. Our investigation considers four distinct training data sizes: 0 (zero-shot), 100K, 1M, and 5M, with translation directed from Russian to English. We use the parallel dataset previously described in Section 3.1. For each data size, the MPT-7B is fine-tuned using the corresponding training objective, noting that all trainings utilize full-weight fine-tuning. \n\nThe results of the comparison between training objectives can be viewed in Figure 6.",
            "score": 0.4543551083996152,
            "section_title": "A COMPARING LLM TRAINING OBJECTIVES FOR MACHINE TRANSLATION",
            "char_start_offset": 24856,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 92
                },
                {
                    "start": 95,
                    "end": 233
                },
                {
                    "start": 236,
                    "end": 355
                },
                {
                    "start": 356,
                    "end": 471
                },
                {
                    "start": 472,
                    "end": 577
                },
                {
                    "start": 580,
                    "end": 819
                },
                {
                    "start": 820,
                    "end": 866
                },
                {
                    "start": 869,
                    "end": 1057
                },
                {
                    "start": 1058,
                    "end": 1396
                },
                {
                    "start": 1399,
                    "end": 1546
                },
                {
                    "start": 1549,
                    "end": 1592
                },
                {
                    "start": 1593,
                    "end": 1739
                },
                {
                    "start": 1740,
                    "end": 1804
                },
                {
                    "start": 1805,
                    "end": 1952
                },
                {
                    "start": 1955,
                    "end": 2039
                }
            ],
            "ref_mentions": [
                {
                    "start": 621,
                    "end": 640,
                    "matchedPaperCorpusId": "252780443"
                },
                {
                    "start": 1036,
                    "end": 1056,
                    "matchedPaperCorpusId": "204838007"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.286376953125
        },
        {
            "corpus_id": "253581364",
            "title": "NANSY++: Unified Voice Synthesis with Neural Analysis and Synthesis",
            "text": "We assume that this embedding already contains enough information about the speaker, so the singer-ID encoder is designed as a simple fully-connected layer with ReLU activation. Decoder Each decoder block of NANSY-SVS consists of casual and non-causal decoders and upsamplers as shown in Figure 16-(d). Both the casual and non-causal decoders (Figure 16-(a, b)) consist of 10 ConvGLU blocks and the last convolutional layer after PreConv module, and only the causality of ConvGLU is different. Singer-ID embedding is input as a condition to all ConvGLU blocks, and is reflected through the conditional layer norm layer. The upsampler (Figure 16-(c)) consists of the nearest upsample layers following the ConvGLU block to upsample the time-resolution of the input by 4 times. In the linguistic feature decoder block, the output of {Ling., MIDI-pitch, phoneme} encoder and singer-ID embedding are input to the causal decoder, and the output of the phoneme encoder and singer-ID embedding are input to the non-causal decoder. Similarly, in the residual-F 0 decoder block, the output of the {Residual-F 0 , phoneme, MIDI-pitch} encoder and singer embedding are input to the causal decoder, and the output of the MIDI-pitch encoder and singer embedding are input to the non-causal decoder. Finally, the model is trained so that the sum of the causal decoder and the non-causal decoder equals to the downsampled target feature, and the sum of the outputs of the two upsamplers equals to the original time-resolution target feature. Amplitude Predictor The amplitude predictor has the same structure as Figure 8-(b) Non-causal decoder, except that singer embedding is not used as an conditional input. The predicted linguistic feature and residual-F 0 are concatenated and used as input, and P amplitude and Ap amplitude, one of the analysis features of the NANSY++ backbone, are predicted.",
            "score": 0.45404169649560017,
            "section_title": "Duration Predictor",
            "char_start_offset": 41527,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 177
                },
                {
                    "start": 178,
                    "end": 302
                },
                {
                    "start": 303,
                    "end": 493
                },
                {
                    "start": 494,
                    "end": 619
                },
                {
                    "start": 620,
                    "end": 774
                },
                {
                    "start": 775,
                    "end": 1022
                },
                {
                    "start": 1023,
                    "end": 1284
                },
                {
                    "start": 1285,
                    "end": 1525
                },
                {
                    "start": 1526,
                    "end": 1694
                },
                {
                    "start": 1695,
                    "end": 1883
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.436279296875
        },
        {
            "corpus_id": "257557735",
            "title": "Exploring Distributional Shifts in Large Language Models for Code Analysis",
            "text": "We experiment with three large language models: (1) CodeT5 (Wang et al., 2021), which is an encoder-decoder model based on T5 (Raffel et al., 2019), ( 2) Codex (Chen et al., 2021), which is a decoder only model based on GPT-3 (Brown et al., 2020) and ( 3) ChatGPT (gpt-3.5-turbo) which is the chat optimized version of Instruct-GPT (Ouyang et al., 2022) which is fine-tuned with Reinforcement Learning with Human Feedback(RLHF) (Christiano et al., 2017). The models vary in size: CodeT5 utilizes the T5-large architecture with 700 million parameters, while the Codex model employs the GPT-3 architecture with over 100 billion parameters. Although the architecture of ChatGPT has not been disclosed, it is presumed to have billions of parameters. A more detailed discussion of these models is provided in the Appendix, Section 7.4.",
            "score": 0.45341446731021534,
            "section_title": "Models",
            "char_start_offset": 11829,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 279
                },
                {
                    "start": 280,
                    "end": 454
                },
                {
                    "start": 455,
                    "end": 637
                },
                {
                    "start": 638,
                    "end": 745
                },
                {
                    "start": 746,
                    "end": 830
                }
            ],
            "ref_mentions": [
                {
                    "start": 226,
                    "end": 246,
                    "matchedPaperCorpusId": "218971783"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.07708740234375
        },
        {
            "corpus_id": "254812339",
            "title": "A Framework and Operational Procedures for Metaverses-Based Industrial Foundation Models",
            "text": "Transformer [8], the sole attention-based architecture, is first proposed in the NLP area, which has then become the main framework used for constructing NLP big models. Typical structures of big NLP models include encoder-based models, decoder-based models, and encoder-decoder models which are paired with different training strategies and used for different tasks. Encoder-based NLP models, such as BERT [9], UniLM [10], XLM [11], ELECTRA [12], and so on, simultaneously consider the context features for each token and adopt autoencoding (AE) objective to train the model with masked language modeling-based self-supervision. Different from the bidirectional design in encoder-based models, decoder-based models only consider tokens before the current position and use autoregression (AR) objectives during the training process. Typical decoder-based NLP models include GPT [13], GPT-2 [14], GPT-3 [15], ELMo [16], CPM-1 [17], and so on. Encoder-based models have advantages in the modeling of context features and are widely used in language understanding. But due to the masking operations, there are gaps between the inputs at the pretraining stage and finetuning stages. Decoder-based methods adopt unidirectional designs, which are suitable for generative language tasks. XLNET [18] explores the combination of advantages from both AE and AR by token permutation. Encoder-decoder frameworks combine the representation and task-specific modules in series, which are widely used for sequence-to-sequence tasks, such as question answering and machine translation. T5 [19], Switch-Transformer [20], and BART [21] are typical encoder-decoder-based big language models.",
            "score": 0.4531468013326493,
            "section_title": "A. NLP Models",
            "char_start_offset": 5707,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 169
                },
                {
                    "start": 170,
                    "end": 367
                },
                {
                    "start": 368,
                    "end": 629
                },
                {
                    "start": 630,
                    "end": 832
                },
                {
                    "start": 833,
                    "end": 941
                },
                {
                    "start": 942,
                    "end": 1061
                },
                {
                    "start": 1062,
                    "end": 1178
                },
                {
                    "start": 1179,
                    "end": 1280
                },
                {
                    "start": 1281,
                    "end": 1372
                },
                {
                    "start": 1373,
                    "end": 1569
                },
                {
                    "start": 1570,
                    "end": 1672
                }
            ],
            "ref_mentions": [
                {
                    "start": 12,
                    "end": 15,
                    "matchedPaperCorpusId": "13756489"
                },
                {
                    "start": 418,
                    "end": 422,
                    "matchedPaperCorpusId": "147704286"
                },
                {
                    "start": 428,
                    "end": 432,
                    "matchedPaperCorpusId": "58981712"
                },
                {
                    "start": 890,
                    "end": 894,
                    "matchedPaperCorpusId": "160025533"
                },
                {
                    "start": 902,
                    "end": 906,
                    "matchedPaperCorpusId": "218971783"
                },
                {
                    "start": 913,
                    "end": 917,
                    "matchedPaperCorpusId": "3626819"
                },
                {
                    "start": 925,
                    "end": 929,
                    "matchedPaperCorpusId": "227238757"
                },
                {
                    "start": 1287,
                    "end": 1291,
                    "matchedPaperCorpusId": "195069387"
                },
                {
                    "start": 1573,
                    "end": 1577,
                    "matchedPaperCorpusId": "204838007"
                },
                {
                    "start": 1598,
                    "end": 1602,
                    "matchedPaperCorpusId": "231573431"
                },
                {
                    "start": 1613,
                    "end": 1617,
                    "matchedPaperCorpusId": "204960716"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.55419921875
        },
        {
            "corpus_id": "265607901",
            "title": "Alignment via Mutual Information",
            "text": "The first component of INFOALIGN is a joint probabilistic model of source and target spans in context. Let x i and y i be spans of sequences x and y. For convenience, let us define x \u2212i = x \\ x i (a version x with the span x i masked out; see Fig. 1). We may define y \u2212i correspondingly. Then, to compute the PMI between two spans in context (via Eq. ( 3)), we must compute the following three quantities: \n\nEach of these probability distributions is a kind of masked language model of a kind well-studied in the NLP literature: like the T5 and BART language models (Raffel et al., 2020;Lewis et al., 2019), all three quantities represent distributions over variable-length spans occurring in the middle of input sequences; like forgetful causal models (Liu et al., 2022), the latter two quantities mask multiple spans but predict only a subset. For large datasets, these distributions may be represented approximately using neural language models (Bengio et al., 2000). For small datasets, it is even possible to represent them using explicit frequency counts (Och and Ney, 2000). Indeed, it is possible to view Eqs. (4-6) as special kinds of skip-gram model (Huang et al., 1993) of a kind formerly popular in speech recognition and machine translation. \n\nIn practice, given a training set of paired sequences, we sample uniformly from the set of all maskings and train models to predict each of the three quantities above. We use encoder-decoder models, which generate x, y or both autoregressively (like T5 and BART) to avoid the indepen-dence assumptions made by masked language models (like BERT). 1 As a concrete example, each term in the bottom right of Fig. 1 shows an example of an input-output pair used for training (or querying) these models. Inputs may contain [MASK], [HIDE] and [SEP] tokens, while outputs contain one prediction for each [MASK]ed span, delimited with [SEP] tokens if multiple [MASK]s are present.",
            "score": 0.45300730246147697,
            "section_title": "Masked Span Modeling",
            "char_start_offset": 8982,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 102
                },
                {
                    "start": 103,
                    "end": 149
                },
                {
                    "start": 150,
                    "end": 251
                },
                {
                    "start": 252,
                    "end": 287
                },
                {
                    "start": 288,
                    "end": 405
                },
                {
                    "start": 408,
                    "end": 845
                },
                {
                    "start": 846,
                    "end": 970
                },
                {
                    "start": 971,
                    "end": 1081
                },
                {
                    "start": 1082,
                    "end": 1117
                },
                {
                    "start": 1118,
                    "end": 1254
                },
                {
                    "start": 1257,
                    "end": 1424
                },
                {
                    "start": 1425,
                    "end": 1754
                },
                {
                    "start": 1755,
                    "end": 1928
                }
            ],
            "ref_mentions": [
                {
                    "start": 566,
                    "end": 587,
                    "matchedPaperCorpusId": "204838007"
                },
                {
                    "start": 587,
                    "end": 606,
                    "matchedPaperCorpusId": "204960716"
                },
                {
                    "start": 948,
                    "end": 969,
                    "matchedPaperCorpusId": "221275765"
                },
                {
                    "start": 1061,
                    "end": 1080,
                    "matchedPaperCorpusId": "8031067"
                },
                {
                    "start": 1160,
                    "end": 1180,
                    "matchedPaperCorpusId": "16717715"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.040374755859375
        },
        {
            "corpus_id": "253255423",
            "title": "Unified End-to-End Speech Recognition and Endpointing for Fast and Efficient Speech Systems",
            "text": "As our ASR model, we use a \u223c150M parameter streaming cascaded conformer-transducer (Conf-T) [24,13], which features a causal conformer encoder with 7 layers and attention dimension 512, and a V 2 embedding decoder (i.e., the prediction network computes embeddings based on the two most recent non-blank tokens). As this is a streaming model [25], the causal encoder is given only left-context during recognition. The encoder contains a time-reduction stacking layer after the second conformer layer, which down-samples the input by a factor of 2. \n\nAs this is a cascaded-encoder setup [26], outputs from the causal encoder are passed to a second encoder which receives limited right context, composed of 6 conformer layers with dimension 384. The V 2 embedding decoder two produces predictions based on both the causal and non-causal encoder features; the word-error rates (WER) reported in Section 5 are obtained from the non-causal pathway. This model also features an E2E-EP [8], wherein the ASR search space is augmented with an </s> token that signals EOQ (see \u00a74.4).",
            "score": 0.45290622624355675,
            "section_title": "ASR Model",
            "char_start_offset": 13366,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 311
                },
                {
                    "start": 312,
                    "end": 412
                },
                {
                    "start": 413,
                    "end": 546
                },
                {
                    "start": 549,
                    "end": 742
                },
                {
                    "start": 743,
                    "end": 942
                },
                {
                    "start": 943,
                    "end": 1072
                }
            ],
            "ref_mentions": [
                {
                    "start": 92,
                    "end": 96,
                    "matchedPaperCorpusId": "218674528"
                },
                {
                    "start": 96,
                    "end": 99,
                    "matchedPaperCorpusId": "249437304"
                },
                {
                    "start": 341,
                    "end": 345,
                    "matchedPaperCorpusId": "211066611"
                },
                {
                    "start": 585,
                    "end": 589,
                    "matchedPaperCorpusId": "225094578"
                },
                {
                    "start": 978,
                    "end": 981,
                    "matchedPaperCorpusId": "146098760"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.3505859375
        },
        {
            "corpus_id": "271693742",
            "title": "Leveraging encoder-only large language models for mobile app review feature extraction",
            "text": "Stemming from recent literature reviews in the field of LLMs (Hou et al., 2024;Naveed et al., 2024;Zhao et al., 2023), we compared different encoderonly LLMs suitable for our evaluation and comparative analysis. We focused on encoder-only architecture due to their inherent suitability for classification tasks (Hou et al., 2024). In addition, we also excluded decoder-only models (also known as generative models) due to their size and resource consumption. These models present limited applicability in large-scale contexts such as user review mining, especially in terms of memory, computational resources and time constraints. Particularly, in this study, we selected the following models: \n\n-BERT, considered the first encoder-only LLM, is renowned for its advanced contextual understanding due to its bidirectional nature (Devlin et al., 2019). It is pre-trained using the MLM objective, which enhances its ability to grasp context from both directions, making it effective for token-level tasks such as NER (Broscheit, 2019). For these reasons, we use BERT as a baseline LLM for NER tasks. -RoBERTa improves upon BERT's design and training methods through extended pre-training on a larger dataset with additional data, resulting in stronger language representations (Liu et al., 2019). It also uses MLM for pre-training but outperforms BERT in many cases (Liu et al., 2019). We include RoBERTa in our model evaluation due to its enhanced performance over BERT. -XLNet uses a unique approach by combining autoregressive and bidirectional training, considering all possible word permutations during pretraining (Yang et al., 2019). This improves its ability to understand context and model token dependencies more effectively than traditional models. Unlike BERT and RoBERTa, XLNet employs a PLM training objective. Consequently, token dependencies are modelled differently. We evaluate XLNet's performance to compare its innovative training method against the MLM objectives of BERT and RoBERTa. \n\nEncoder-only models have not significantly evolved over the past few years.",
            "score": 0.452900172993852,
            "section_title": "Model fine-tuning",
            "char_start_offset": 25993,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 211
                },
                {
                    "start": 212,
                    "end": 330
                },
                {
                    "start": 331,
                    "end": 458
                },
                {
                    "start": 459,
                    "end": 630
                },
                {
                    "start": 631,
                    "end": 693
                },
                {
                    "start": 696,
                    "end": 850
                },
                {
                    "start": 851,
                    "end": 1032
                },
                {
                    "start": 1033,
                    "end": 1096
                },
                {
                    "start": 1097,
                    "end": 1293
                },
                {
                    "start": 1294,
                    "end": 1382
                },
                {
                    "start": 1383,
                    "end": 1468
                },
                {
                    "start": 1469,
                    "end": 1637
                },
                {
                    "start": 1638,
                    "end": 1756
                },
                {
                    "start": 1757,
                    "end": 1821
                },
                {
                    "start": 1822,
                    "end": 1880
                },
                {
                    "start": 1881,
                    "end": 2002
                },
                {
                    "start": 2005,
                    "end": 2080
                }
            ],
            "ref_mentions": [
                {
                    "start": 99,
                    "end": 117,
                    "matchedPaperCorpusId": "30075992"
                },
                {
                    "start": 828,
                    "end": 849,
                    "matchedPaperCorpusId": "52967399"
                },
                {
                    "start": 1014,
                    "end": 1031,
                    "matchedPaperCorpusId": "208191596"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.23681640625
        },
        {
            "corpus_id": "272826808",
            "title": "Machine Translation with Large Language Models: Decoder Only vs. Encoder-Decoder",
            "text": "The proposed methodology aims to evaluate and compare the performance of Encoder-Decoder and Decoder-only models in natural language processing tasks. This methodology is structured into several key phases, including data preparation, model design, training, and evaluation. Each phase is",
            "score": 0.45283379289724596,
            "section_title": "Proposed Methodology and Experimental Results",
            "char_start_offset": 4446,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 150
                },
                {
                    "start": 151,
                    "end": 274
                },
                {
                    "start": 275,
                    "end": 288
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.29833984375
        },
        {
            "corpus_id": "268819689",
            "title": "From Robustness to Improved Generalization and Calibration in Pre-trained Language Models",
            "text": "For our investigation, we opted to evaluate a variety of decoder-based language models due to their increasing prominence.We explored a series of decoder-based OPT models across three distinct sizes: 125 m, 1.3 b, and 6.7 b parameters.We also included the LLaMA-2 variant with 7 b parameters.As a baseline comparison, we included BERT, an encoder-based model.",
            "score": 0.45283379289724596,
            "section_title": "Model Selection",
            "char_start_offset": 18397,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 122
                },
                {
                    "start": 122,
                    "end": 235
                },
                {
                    "start": 235,
                    "end": 292
                },
                {
                    "start": 292,
                    "end": 359
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.1500244140625
        },
        {
            "corpus_id": "269009682",
            "title": "LLM2Vec: Large Language Models Are Secretly Powerful Text Encoders",
            "text": "Text embedding models aim to encode the semantic content of natural language text in vector representations which then facilitate various natural language processing (NLP) tasks, such as semantic textual similarity, information retrieval, and clustering. For many years, the dominating paradigm for building such models relied on pre-trained bidirectional encoders or encoder-decoders such as BERT (Devlin et al., 2019) and T5 (Raffel et al., 2020), which are typically adapted for text embedding tasks by following a multi-step training pipeline consisting of weakly-and fully-supervised contrastive training (Ni et al., 2022;Li et al., 2023a;Xiao et al., 2023, inter alia). Only recently, the community started to adopt decoder-only LLMs for embedding text (Muennighoff, 2022;Ma et al., 2023;Wang et al., 2023;Springer et al., 2024;Li & Li, 2024). \n\nWe speculate that the slow adoption of decoder-only LLMs for text embedding tasks is partly due to their causal attention mechanism, which inherently limits their ability to produce rich contextualized representations. At any given layer, causal attention limits token interactions, ensuring that the representation of a token at position i is influenced solely by the representations of preceding tokens at positions 0, 1, . . . , i \u2212 1. Although this limitation is necessary for generative capabilities, it is sub-optimal for text embeddings as it prevents the representations from capturing information across the entire input sequence. \n\nOvercoming this architectural limitation of decoder-only LLMs for text embedding tasks is highly appealing as these models come with several advantages compared to their encoder-",
            "score": 0.45222521246792,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 254
                },
                {
                    "start": 255,
                    "end": 675
                },
                {
                    "start": 676,
                    "end": 849
                },
                {
                    "start": 852,
                    "end": 1070
                },
                {
                    "start": 1071,
                    "end": 1281
                },
                {
                    "start": 1282,
                    "end": 1290
                },
                {
                    "start": 1291,
                    "end": 1491
                },
                {
                    "start": 1494,
                    "end": 1672
                }
            ],
            "ref_mentions": [
                {
                    "start": 398,
                    "end": 419,
                    "matchedPaperCorpusId": "52967399"
                },
                {
                    "start": 427,
                    "end": 448,
                    "matchedPaperCorpusId": "204838007"
                },
                {
                    "start": 610,
                    "end": 627,
                    "matchedPaperCorpusId": "245144556"
                },
                {
                    "start": 778,
                    "end": 794,
                    "matchedPaperCorpusId": "16299141"
                },
                {
                    "start": 834,
                    "end": 848,
                    "matchedPaperCorpusId": "265066823"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.5234375
        },
        {
            "corpus_id": "259138434",
            "title": "Rethinking Translation Memory Augmented Neural Machine Translation",
            "text": "To study the effect of the proposed methods in \u00a74, we implement a series of model variants by using the fairseq toolkit (Ott et al., 2019). #1 vanilla NMT without TM (Vaswani et al., 2017). We remove the model components related to TM, and only employ the encoder-decoder architecture for NMT. #2 Default TM-augmented NMT with top-5 TMs. We use top-5 TMs to train and test the model. Note that this is also a baseline model in Cai et al. (2021)",
            "score": 0.4521028572688058,
            "section_title": "Models",
            "char_start_offset": 17189,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 139
                },
                {
                    "start": 140,
                    "end": 189
                },
                {
                    "start": 190,
                    "end": 293
                },
                {
                    "start": 294,
                    "end": 337
                },
                {
                    "start": 338,
                    "end": 383
                },
                {
                    "start": 384,
                    "end": 444
                }
            ],
            "ref_mentions": [
                {
                    "start": 166,
                    "end": 188,
                    "matchedPaperCorpusId": "13756489"
                },
                {
                    "start": 427,
                    "end": 444,
                    "matchedPaperCorpusId": "235166182"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.0246124267578125
        },
        {
            "corpus_id": "258479712",
            "title": "Learning Language-Specific Layers for Multilingual Machine Translation",
            "text": "by ran-domly sampling 1.5M sentences per language.\n\nTagging We found it helpful to make the model aware of the corpus by training with corpus labels. Similarly to NLLB Team et al. (2022), we add a tag (e.g. <HQ> or <LQ>) to the beginning of the source sentence, so that the model can learn to distinguish between higher quality (WMT21, Opus-100, and Tatoeba) and lower quality examples (CCMatrix). During inference, we always use the high quality (<HQ>) tag. Additionally, we append source and target language tags to the end of the sentence.\n\nArchitecture In our experiments, we use a deep encoder, shallow decoder architecture (Kasai et al., 2021) with 16 encoder layers and 3 decoder layers. We share token embeddings between the encoder, decoder, and output layer (Press and Wolf, 2017). In our experiments we consider two kinds of models: those with target language-specific decoders, following Dong et al. (2015), on which we conduct most of our experiments, and those with a shared decoder. The encoder is always shared, with the exception of the LSLs. In the baseline models, the encoder consists only of \"regular\" Transformer Layers, and so it is fully shared. In this work, we only consider adding LSLs to the encoder. In preliminary experiments with LSLs in the decoder, our selection criteria picked target-specific LSLs for all decoder layers, effectively choosing a separate decoder architecture. We tried different placements of the layers in the decoder, but did not achieve any improvements. We leave a deeper analysis to future work.\n\nHyperparameters All experiments are implemented using FAIRSEQ (Ott et al., 2019). We use ADAM (Kingma and Ba, 2015) for optimization, due to its robustness (Schmidt et al., 2021) and popularity, with a learning rate of 0.0004. We train for 150k steps, by which point our models had converged, with 4000 warm-up steps, and an inverse square root learning rate scheduler (Vaswani et al.",
            "score": 0.4516692023519563,
            "section_title": "Experimental Setup",
            "char_start_offset": 13092,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.06500244140625
        },
        {
            "corpus_id": "6359641",
            "title": "Multi-Way, Multilingual Neural Machine Translation with a Shared Attention Mechanism",
            "text": "Each symbol, either source or target, is projected on a 620-dimensional space. The encoder is a bidirectional recurrent neural network with 1,000 gated recurrent units (GRU) in each direction, and the decoder is a recurrent neural network with also 1,000 GRU's. The decoder's output function g k from Eq. ( 7) is a feedforward network with 1,000 tanh hidden units. The dimensionalities of the context vector h n t in Eq. ( 8), the attention-specific context vector hn t in Eq. ( 9) and the attention-specific decoder hidden state hm t\u22121 in Eq. ( 11) are all set to 1,200. We use the same type of encoder for every source language, and the same type of decoder for every target language. The only difference between the single-pair models and the proposed multilingual ones is the numbers of encoders N and decoders M . We leave those multilingual translation specific components, such as the ones in Eqs. ( 8)-( 11), in the single-pair models in order to keep the number of shared parameters constant.",
            "score": 0.4516602957488313,
            "section_title": "Model Architecture",
            "char_start_offset": 22484,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 78
                },
                {
                    "start": 79,
                    "end": 261
                },
                {
                    "start": 262,
                    "end": 364
                },
                {
                    "start": 365,
                    "end": 571
                },
                {
                    "start": 572,
                    "end": 686
                },
                {
                    "start": 687,
                    "end": 818
                },
                {
                    "start": 819,
                    "end": 1001
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.10595703125
        },
        {
            "corpus_id": "271903271",
            "title": "FASST: Fast LLM-based Simultaneous Speech Translation",
            "text": "As shown in Figure 2, our model is composed of a speech encoder, an adapter and a LLM decoder. \n\nBlockwise-Causal Speech Encoder (BCSE) extracts contextualized acoustic features from the raw waveform incrementally. It consists of several casual convolutional layers as the audio feature extractor and a blockwise-causal Transformer Encoder as the contextual encoder. \n\nOur causal convolutional layers are built upon non-causal ones. Denote H in \u2208 R l\u00d7d as the input vectors to non-causal convolution Conv(\u2022) with kernel size w. We add additional zero padding Pad \u2208 R (w/2\u22121)\u00d7d to its left so that each output vector only depends on input vectors to its left, and remove the last w/2 \u2212 1 states to keep its output length the same as before, \n\n(2) \n\nBesides, we apply blockwise-causal masking to Transformer Encoder. Define attention mask M of speech encoder as follows \n\nwhere b is the block size, i.e., the number of hidden states of the speech encoder corresponding to one segment, and j Q , j K are row indices of query matrix Q and key matrix K. The attention output of speech encoder during training can then be written as \n\nwhere V is the value matrix. \n\nAdapter receives speech encoder outputs and converts them to the LLM embedding space. It consists of two causal convolutional layers to reduce the length of speech encoder outputs by four and one linear layer to project features into the LLM embedding space. We call the adapter outputs as speech embeddings, \n\nLLM receives speech embeddings and embeddings of previously generated tokens to decode autoregressively according to a wait-k-stride-n policy \u03c0. Wait-k-stride-n policy waits for k speech segments at the beginning and then alternate between generating n words and reading new segment. Figure 3 shows an example of wait-1-stride-2.",
            "score": 0.4514633035407789,
            "section_title": "Model Architecture",
            "char_start_offset": 3810,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 94
                },
                {
                    "start": 97,
                    "end": 214
                },
                {
                    "start": 215,
                    "end": 366
                },
                {
                    "start": 369,
                    "end": 432
                },
                {
                    "start": 433,
                    "end": 527
                },
                {
                    "start": 528,
                    "end": 739
                },
                {
                    "start": 742,
                    "end": 745
                },
                {
                    "start": 748,
                    "end": 814
                },
                {
                    "start": 815,
                    "end": 867
                },
                {
                    "start": 870,
                    "end": 1048
                },
                {
                    "start": 1049,
                    "end": 1126
                },
                {
                    "start": 1129,
                    "end": 1157
                },
                {
                    "start": 1160,
                    "end": 1245
                },
                {
                    "start": 1246,
                    "end": 1418
                },
                {
                    "start": 1419,
                    "end": 1468
                },
                {
                    "start": 1471,
                    "end": 1615
                },
                {
                    "start": 1616,
                    "end": 1754
                },
                {
                    "start": 1755,
                    "end": 1800
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.429443359375
        },
        {
            "corpus_id": "22352154",
            "title": "Encoding and Decoding Models in Cognitive Electrophysiology",
            "text": "It is possible for decoding models to be constructed with a regression framework, similarly to how encoding models operate. For example, in Mesgarani and Chang (2012) and , the experimenters fit one model for each stimulus feature being decoded. This amounts to simply reversing the terms in the standard regression equations:\n\nIt is tempting in this case to collect the coefficients of each decoding model and interpret this as if they came from an encoding model. However, it's important to note that a primary role of regression is to account for correlations between input features when estimating model coefficients. As explained in detail in Weichwald et al. (2015), if a stimulus feature X i causally influences a neural feature Y i , , and if the stimulus feature X i is correlated with another stimulus feature X j (for example, if they share correlated noise, or if the stimulus features are naturally correlated), the decoder will give significant weights for both X i and X j , even though it is only X i that influences the neural signal. This fact has important implications in the interpretation of model weights.\n\nConsider the case of receptive field modeling, in which auditory stimuli are presented to the individual, and a model is fit to uncover the spectral features to which the neural activity responds. In the encoding model, correlations between stimulus features are explicitly accounted for (X T X), while in the decoding model, correlations between the neural features are accounted for (Y T Y). While it is possible to retrieve a receptive field using a decoding paradigm (e.g., by fitting one decoding model for each frequency/time-lag and collecting coefficients into a STRF), correlations in the stimulus features will skew the distribution of model coefficients. This might result in a STRF that is smoothed over a local region in delay/frequency. An encoding model should (theoretically) take these stimulus correlations into account, and only assign non-zero coefficient values to the proper features (see Figure 5). In this case it is important to consider the regularizer used in fitting the model, as there are differences in how regularization techniques distribute model weights with correlated features (Mesgarani et al., 2009). FIGURE 5 | Comparing encoding and decoding weights. An example of how using an encoding",
            "score": 0.4511241499718934,
            "section_title": "Differences in Regression",
            "char_start_offset": 98913,
            "sentence_offsets": [],
            "ref_mentions": [
                {
                    "start": 140,
                    "end": 166,
                    "matchedPaperCorpusId": "4320045"
                },
                {
                    "start": 648,
                    "end": 671,
                    "matchedPaperCorpusId": "6003605"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.1297607421875
        },
        {
            "corpus_id": "261530180",
            "title": "One Wide Feedforward Is All You Need",
            "text": "We conduct thorough experiments with different configurations of the Transformer, across different language pairs, including a low resource language pair and multilingual. In addition, we investigate the effect of the FFN in a decoder-only Transformer-based model. We find that a considerable level of redundancy exists between the encoder and decoder FFNs. As a result, we are able to eliminate the decoder FFN and share a single FFN across the encoder without significantly compromising the model's accuracy. This step leads not only to significant parameter savings but also opens up opportunities for further improvements. We also suggest using wider FFNs in the encoder while dropping the decoder's FFN, which results in a model with a similar size, but improved accuracy and reduced latency. \n\nFinally we conduct a fine-grained analysis of the representational similarity between the original model, using one independent FFN per layer, and various models with shared FFNs. Our results reveal that both model accuracy and the internal representation of Transformer blocks remain stable when sharing the FFN.",
            "score": 0.4508922716462122,
            "section_title": "Introduction",
            "char_start_offset": 1929,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 171
                },
                {
                    "start": 172,
                    "end": 264
                },
                {
                    "start": 265,
                    "end": 357
                },
                {
                    "start": 358,
                    "end": 510
                },
                {
                    "start": 511,
                    "end": 626
                },
                {
                    "start": 627,
                    "end": 797
                },
                {
                    "start": 800,
                    "end": 979
                },
                {
                    "start": 980,
                    "end": 1113
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.139404296875
        },
        {
            "corpus_id": "267060969",
            "title": "Structured Code Representations Enable Data-Efficient Adaptation of Code Language Models",
            "text": "For decoder-only models, we find that it is effective to simply reuse the causal language modeling objective over the serialized tree z: \n\nWhen there exist natural language \u2194 code pairs, we replace z in the above formula by z \u2032 = [x : z]; that is, concatenating text and code. Compared with the specialized objectives of MSP and MNP for encoder-decoder models, here we require the model to reconstruct all tokens (both terminal and nonterminal nodes) in an autoregressive manner. We highlight that for all three tasks -and especially for the MBPP dataset -we model the problem as a strict Text2Code task, where the input is the natural language description and the output is the complete code. This differs from MBPP task formulations in prior works, where the input is the natural language description and the Python function signature and docstring and the model generates the function body. Figure 3 shows typical results (averaged over 3 random seeds) for all of them, with more results on other evaluation metrics left to the Appendix. We observe that the structured models always outperform the base counterparts. ing of these nodes. Then, adding non-terminal tokens to the tokenizer gains back the performance. Still, for Code2Text, the performance is not as competitive as the base model. Finally, using continual pre-training (which leads to our structured model) yields additional gains and beats the base model by a large margin. Additionally, we investigate the importance of the proposed training objectives for encoder-decoder models. Table 1 shows four scenarios, each using only one or two of the objectives for training. The performance of CodeT5-S suffers in all these scenarios, but the best among them is still better than the base CodeT5, corroborating the advantage of leveraging structures.",
            "score": 0.4508650795214759,
            "section_title": "Decoder-Only Models",
            "char_start_offset": 13956,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 136
                },
                {
                    "start": 139,
                    "end": 276
                },
                {
                    "start": 277,
                    "end": 479
                },
                {
                    "start": 480,
                    "end": 693
                },
                {
                    "start": 694,
                    "end": 893
                },
                {
                    "start": 894,
                    "end": 1040
                },
                {
                    "start": 1041,
                    "end": 1119
                },
                {
                    "start": 1120,
                    "end": 1139
                },
                {
                    "start": 1140,
                    "end": 1217
                },
                {
                    "start": 1218,
                    "end": 1296
                },
                {
                    "start": 1297,
                    "end": 1440
                },
                {
                    "start": 1441,
                    "end": 1548
                },
                {
                    "start": 1549,
                    "end": 1637
                },
                {
                    "start": 1638,
                    "end": 1813
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.41357421875
        },
        {
            "corpus_id": "252907291",
            "title": "JOIST: A Joint Speech and Text Streaming Model for ASR",
            "text": "The model is trained by jointly optimizing both decoders using audio-text pairs in addition to the unpaired text. If we denote LC(y, x) = \u2212 log PC(y|x) and LNC(y, x) = \u2212 log PNC(y|x) as the negative log likelihood of the causal and the non-causal decoders, respectively, we define the overall loss as: \n\nwhere, \u03bb1 is the weight corresponding to the paired audio-text data and \u03bb2 is the weight on the unpaired text-only data. As can be seen in ( 1), we weight the casual and non-causal decoders equally in the loss function. In practice, the losses are computed over a mini-batch of examples; in training, we use 50% paired audio-text and unpaired 50% text examples in each mini-batch. Unlike previous work, we do not add additional MLM or consistency losses from the text encoder [15,17,19] which simplifies the overall training procedure. Evaluations of the impact of these and other losses in the JOIST framework are left as future work.",
            "score": 0.4504620461578838,
            "section_title": "Loss Computation",
            "char_start_offset": 12354,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 113
                },
                {
                    "start": 114,
                    "end": 301
                },
                {
                    "start": 304,
                    "end": 424
                },
                {
                    "start": 425,
                    "end": 523
                },
                {
                    "start": 524,
                    "end": 684
                },
                {
                    "start": 685,
                    "end": 839
                },
                {
                    "start": 840,
                    "end": 939
                }
            ],
            "ref_mentions": [
                {
                    "start": 784,
                    "end": 787,
                    "matchedPaperCorpusId": "248119033"
                },
                {
                    "start": 787,
                    "end": 790,
                    "matchedPaperCorpusId": "248006130"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.2242431640625
        },
        {
            "corpus_id": "723169",
            "title": "Information Embedding on Actions",
            "text": "The system model for the set-up in Fig. 4, is similar to the one described in Sec. II-A with the only difference the decoding function for Decoder 2 a time i is given as \n\nwhich maps the strictly causally observed sequence f \n\nwhere the mutual information is evaluated with respect to the joint pmf for some pmf p(x 2 , a, u|x) such that the inequalities \n\nand The only difference between the rate-distortion-cost function of Proposition 1 with non-causal action observation with respect to the case with strictly causal action observation of Proposition 2 is the constraint (13c). Recall that the latter is needed to ensure that Decoder 2 is able to recover the reconstruction X2 . As detailed below, the strict causality of the observation of the action at Decoder 2 calls for a block-based encoding in which the actions carries information about the source sequence as observed in two different blocks, namely the current block for Decoder 1 and the future block for Decoder 2. This additional requirement causes the conditioning on X2 in (13c), which generally increases the rate (11) with respect to the counterpart (20) achievable with non-causal action observation. A sketch of the achievability proof is provided below and is based on the techniques proposed in [24], [20] (see also [21]). The proof of the converse is provided in Appendix B.",
            "score": 0.4503981593502345,
            "section_title": "C. Strictly Causal Action Observation",
            "char_start_offset": 15778,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 82
                },
                {
                    "start": 83,
                    "end": 169
                },
                {
                    "start": 172,
                    "end": 224
                },
                {
                    "start": 227,
                    "end": 354
                },
                {
                    "start": 357,
                    "end": 581
                },
                {
                    "start": 582,
                    "end": 682
                },
                {
                    "start": 683,
                    "end": 1172
                },
                {
                    "start": 1173,
                    "end": 1297
                },
                {
                    "start": 1298,
                    "end": 1350
                }
            ],
            "ref_mentions": [
                {
                    "start": 1270,
                    "end": 1274,
                    "matchedPaperCorpusId": "14361332"
                },
                {
                    "start": 1291,
                    "end": 1295,
                    "matchedPaperCorpusId": "2282680"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.197265625
        },
        {
            "corpus_id": "9402389",
            "title": "On Successive Refinement for the Kaspi/Heegard\u2013Berger Problem",
            "text": "Similarly, with non-causal SI, decoder Z uses the encoder transmission and Z in its entirety and reproduces X = ( X1 , ..., XN ) \u2208 X N , while in the case of causal SI, only the bitstream and Z i 1 are used for reproduction of Xi . The quality of reconstruction at each of the decoders is judged in terms of the expectations of additive distortion measures d y,1 (X, X) = 1 \n\n, where d y,1 (X, X) and d z,1 (X, X), X \u2208 X , X \u2208 X , X \u2208 X , are non-negative, bounded distortion measures. At the second stage, the encoder sends, at rate R 2 \u2212R 1 , an additional information about the source sequence to both decoders, also in the form of a binary bitstream, this time of length \n\n, taking values in {0, 1, ..., 2 N (R 2 \u2212R 1 ) \u22121} . The decoders reconstruct the source sequence with better accuracy (in terms of the distortion measures) according to both transmissions of the encoder and the individual SI's. The distortions measures used at the decoders Y and Z at this stage are also additive, d y,2 (X, X) = 1 \n\nwhere d y,2 (X, X) and d z,2 (X, X), X \u2208 X , X \u2208 X , X \u2208 X , are non-negative, bounded distortion measures. This setting can be straightforwardly extended to any number of refinement stages as well as any number of decoders at each stage. We confine ourselves to the case of two decoders and two stages. \n\nWe begin with the case of non-causal SI. \n\n) source code for a single encoder, two decoders and two-stage successive refinement with non-causal SI at the decoders, for the source P XY Z , consists of a first-stage encoder-decoder triplet (f 1 , g y,1 , g z,1 ): \n\nand a second-stage encoder-decoder triplet (f 2 , g y,2 , g z,2 ):",
            "score": 0.4503471094601445,
            "section_title": "System Description and Problem Definition",
            "char_start_offset": 14592,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 231
                },
                {
                    "start": 232,
                    "end": 373
                },
                {
                    "start": 376,
                    "end": 485
                },
                {
                    "start": 486,
                    "end": 674
                },
                {
                    "start": 677,
                    "end": 729
                },
                {
                    "start": 730,
                    "end": 905
                },
                {
                    "start": 906,
                    "end": 1009
                },
                {
                    "start": 1012,
                    "end": 1119
                },
                {
                    "start": 1120,
                    "end": 1250
                },
                {
                    "start": 1251,
                    "end": 1315
                },
                {
                    "start": 1318,
                    "end": 1358
                },
                {
                    "start": 1361,
                    "end": 1579
                },
                {
                    "start": 1582,
                    "end": 1648
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.59228515625
        },
        {
            "corpus_id": "269804069",
            "title": "On the Adaptation of Unlimiformer for Decoder-Only Transformers",
            "text": "In contrast to encoder-decoder models, decoderonly transformers use causal (unidirectional) attention.This difference means that a token has seen enough contextual information if a certain number of tokens are behind it.As a result, instead of only storing the hidden states of the middle half tokens, we can keep all the non-overlapping ones.This will allow us to be slightly more efficient when processing long documents.Note that only the first instance of overlapping tokens is added to the index, as illustrated by orange tokens in Figure 1.",
            "score": 0.44994319942167155,
            "section_title": "Chunks Encoding",
            "char_start_offset": 6884,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 102
                },
                {
                    "start": 102,
                    "end": 220
                },
                {
                    "start": 220,
                    "end": 343
                },
                {
                    "start": 343,
                    "end": 423
                },
                {
                    "start": 423,
                    "end": 546
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.515625
        },
        {
            "corpus_id": "259211816",
            "title": "Investigating Pre-trained Language Models on Cross-Domain Datasets, a Step Closer to General AI",
            "text": "In this section we further investigate the large improvement shown in 4.1. In order to ensure that the improvement is due to pre-training on language and not due to the features of the architectures of the respective model, we test four diverse pre-trained models instantiating different architectures including T5, BART, BERT, and GPT-2. Table 2 shows the number of parameters, the type, and the training strategy of each model. We investigate four diverse pre-trained language models with different training strategies and different sizes. T5 and BERT are both bidirectional models where the representation of the words are derived from both its left and right context. The difference between T5 and BERT in the training strategy is that BERT masks a token for each word, whereas T5 replaces multiple consecutive tokens with a single mask. It is up to the model to figure out how many tokens are missing in the sentence. Also, T5 is similar to the original transformer in architecture. It has encoder and decoder, whereas BERT is an encoder only model. Thus, T5 has 220 million parameters, whereas BERT has only 110 millions. GPT-2 is an auto-regressive model and the representation of the word is only derived from the left context. It is known that this type of models usually do not perform well on tasks that require knowledge of the whole sequence. GPT-2 has around 125 million parameters. Finally, similar to T5, BART is an encoder-decoder model that combines both BERT and GPT-2 strategies by having a bidirectional encoder and auto-regressive decoder. It has around 140 million parameters. \n\nWe test the four models on Listops and CIFAR10-LRA, but it is important to note that in this experiment and for simplicity we use Listops sequence length of 200-500 instead of 500-2000 used in section 4.1. Table 3 shows that all of the pre-trained models score higher than the best performing transformer trained from scratch. The performance of the pre-trained language models is similar where T5 scores the highest on Listops(200-500) with an accuracy of 65.4% compared to 39.5% for the best transformer trained from scratch.",
            "score": 0.44947674577639907,
            "section_title": "Does the architecture of pre-trained models make a difference?",
            "char_start_offset": 16995,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 74
                },
                {
                    "start": 75,
                    "end": 338
                },
                {
                    "start": 339,
                    "end": 429
                },
                {
                    "start": 430,
                    "end": 541
                },
                {
                    "start": 542,
                    "end": 671
                },
                {
                    "start": 672,
                    "end": 841
                },
                {
                    "start": 842,
                    "end": 922
                },
                {
                    "start": 923,
                    "end": 987
                },
                {
                    "start": 988,
                    "end": 1054
                },
                {
                    "start": 1055,
                    "end": 1127
                },
                {
                    "start": 1128,
                    "end": 1235
                },
                {
                    "start": 1236,
                    "end": 1355
                },
                {
                    "start": 1356,
                    "end": 1396
                },
                {
                    "start": 1397,
                    "end": 1561
                },
                {
                    "start": 1562,
                    "end": 1599
                },
                {
                    "start": 1602,
                    "end": 1807
                },
                {
                    "start": 1808,
                    "end": 1928
                },
                {
                    "start": 1929,
                    "end": 2129
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.1884765625
        },
        {
            "corpus_id": "239049412",
            "title": "Improving Non-autoregressive Generation with Mixup Training",
            "text": "Pre-trained language models Pre-trained language models bring significant improvement for both natural language understanding and generation tasks. These models are trained with a large amount of unlabeled data to understand the language and improve the results on small down-stream datasets. For example, BERT (Devlin et al. 2018) pre-trains a encoder based model with masked language task and next sentence prediction task. It significantly improves the performance on natural language understanding tasks, but it is not suitable for generation tasks. UniLM (Dong et al. 2019;Bao et al. 2020) pre-trains encoder based model with three tasks: unidirectional, bidirectional, and prediction, which allow it can be fine-tuned for both natural language understanding and generation tasks. For the encoder-decoder based models (Song et al. 2019;Qi et al. 2020b;Song et al. 2019), these models are pre-trained with sequence-to-sequence tasks to help the down-stream generation tasks. \n\nNon-autoregressive generation Many works (Gu et al. 2017;Kasai et al. 2020) have been proposed to decrease the huge latency in autoregressive generation. The most popular way is to generate tokens in parallel called nonautoregressive generation. However, these works (Gu et al. 2017;Gu and Kong 2020;Qian et al. 2020) mostly focus on translation, and cannot achieve reasonable results on tasks like summarization or question generation. BANG (Qi et al. 2020a) achieves significantly improvement by bridging autoregressive and non-autoregressive generation with large scale pre-training on these tasks. For model architecture, BANG has several differences compared to previous methods: 1) BANG only uses [MASK] tokens with fixed length as the input of decoder. 2) BANG does not predict target tokens length directly, but treats the first [SEP] token as the end of sequences like autoregressive generation.",
            "score": 0.44875719935731107,
            "section_title": "Related Work",
            "char_start_offset": 3741,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 147
                },
                {
                    "start": 148,
                    "end": 292
                },
                {
                    "start": 293,
                    "end": 425
                },
                {
                    "start": 426,
                    "end": 553
                },
                {
                    "start": 554,
                    "end": 785
                },
                {
                    "start": 786,
                    "end": 978
                },
                {
                    "start": 981,
                    "end": 1134
                },
                {
                    "start": 1135,
                    "end": 1226
                },
                {
                    "start": 1227,
                    "end": 1417
                },
                {
                    "start": 1418,
                    "end": 1582
                },
                {
                    "start": 1583,
                    "end": 1740
                },
                {
                    "start": 1741,
                    "end": 1885
                }
            ],
            "ref_mentions": [
                {
                    "start": 560,
                    "end": 578,
                    "matchedPaperCorpusId": "147704286"
                },
                {
                    "start": 578,
                    "end": 593,
                    "matchedPaperCorpusId": "211572655"
                },
                {
                    "start": 823,
                    "end": 841,
                    "matchedPaperCorpusId": "207880694"
                },
                {
                    "start": 841,
                    "end": 857,
                    "matchedPaperCorpusId": "210164665"
                },
                {
                    "start": 857,
                    "end": 874,
                    "matchedPaperCorpusId": "207880694"
                },
                {
                    "start": 1423,
                    "end": 1439,
                    "matchedPaperCorpusId": "210164665"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.1876220703125
        },
        {
            "corpus_id": "271769522",
            "title": "Super donors and super recipients: Studying cross-lingual transfer between high-resource and low-resource languages",
            "text": "In our experiments, we utilize the widely used pretrained multilingual language model mT57 (Xue et al., 2021). It is an encoder-decoder model trained on 101 languages from the mC4 dataset. It was originally pretrained in the transfer learning procedure and has shown itself well in transferring knowledge. We think the encoder-decoder architecture is more flexible and has more possible applications for future works than only encoder or decoder-based models. Due to its multitask finetuning, we decided not to use its another version, mT0 (Muennighoff et al., 2023). Considering our lack of labeled data, exploring its multitask zeroshot performance is unnecessary here.",
            "score": 0.44873508934918305,
            "section_title": "Base model",
            "char_start_offset": 8815,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 110
                },
                {
                    "start": 111,
                    "end": 188
                },
                {
                    "start": 189,
                    "end": 305
                },
                {
                    "start": 306,
                    "end": 459
                },
                {
                    "start": 460,
                    "end": 567
                },
                {
                    "start": 568,
                    "end": 671
                }
            ],
            "ref_mentions": [
                {
                    "start": 91,
                    "end": 109,
                    "matchedPaperCorpusId": "225040574"
                },
                {
                    "start": 540,
                    "end": 566,
                    "matchedPaperCorpusId": "253264914"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.06842041015625
        },
        {
            "corpus_id": "247476141",
            "title": "Hyperdecoders: Instance-specific decoders for multi-task NLP",
            "text": "We propose a novel method for generating adapters conditioned on a model's input and show that this improves performance in multi-task settings across a variety of tasks. We explore the effectiveness of our approach for sequence classification, QA, and summarisation tasks, and find that it often outperforms strong parameter-efficient baselines. Future work could examine applying our approach to other architectures (e.g. decoder-only models) or explore the tradeoffs between shared and generated parameters across different layers. An analysis of our approach suggests the primary benefits come from improved control of the encoder over the decoder, enhancing the effects of positive transfer from the shared encoder. This allows our approach to efficiently adapt a pretrained language model to multiple tasks unseen during pretraining while still benefiting strongly from positive transfer.",
            "score": 0.44859432056306636,
            "section_title": "Conclusion",
            "char_start_offset": 23469,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 170
                },
                {
                    "start": 171,
                    "end": 346
                },
                {
                    "start": 347,
                    "end": 423
                },
                {
                    "start": 424,
                    "end": 534
                },
                {
                    "start": 535,
                    "end": 720
                },
                {
                    "start": 721,
                    "end": 894
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.230224609375
        },
        {
            "corpus_id": "222290948",
            "title": "What Do Position Embeddings Learn? An Empirical Study of Pre-Trained Language Model Positional Encoding",
            "text": "One of the main reason for the difference is the pre-training objectives. Pre-trained Transformer encoders minimize the masked language modeling loss, which can be formulated as \n\nwhere U = {u 1 , ...u l } is the pre-training corpus with the length l, and \u03b8 is the model parameters. \n\nFor Transformer decoders, the objective is the traditional autoregressive language modeling loss: \n\nTransformer encoders can predict tokens depending on the tokens in both directions, while decoder can only predict depending on the token in the past. With enough context information, it is believed that Transformer encoders can succeed to predict tokens by only performing attention on the tokens nearby. That is why position embeddings learned We infer that encoder position embeddings may capture the local position information, which can force the output capturing the positions nearby, especially BERT almost involving nothing about absolute positions. The inference makes the previous observations in sections 4.2.2 and 4.2.1 sensible and explainable, and we will verify this inference through empirical experiments in section 5.",
            "score": 0.44859210662687143,
            "section_title": "Pre-Training Objectives",
            "char_start_offset": 12332,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 73
                },
                {
                    "start": 74,
                    "end": 177
                },
                {
                    "start": 180,
                    "end": 282
                },
                {
                    "start": 285,
                    "end": 382
                },
                {
                    "start": 385,
                    "end": 535
                },
                {
                    "start": 536,
                    "end": 690
                },
                {
                    "start": 691,
                    "end": 942
                },
                {
                    "start": 943,
                    "end": 1120
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.2037353515625
        },
        {
            "corpus_id": "6589519",
            "title": "Causal and anti-causal learning in pattern recognition for neuroimaging",
            "text": "Considering both the distinction of encoding-and decoding models and the distinction of stimulus-and response-based experiments we obtain the following four types of models: A. Causal encoding models -p(X|S) B. Anti-causal decoding models -p(S|X) C. Anti-causal encoding models -p(X|R) D. Causal decoding models -p(R|X) \n\nIn the following section we provide theoretical justifications why this distinction needs to be considered before interpreting encoding-or decoding models. As we show, interpretability of relevant features depends on whether the model represents causal or anti-causal relations.",
            "score": 0.4483924175418692,
            "section_title": "D. Distinction of pattern recognition models",
            "char_start_offset": 6311,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 319
                },
                {
                    "start": 322,
                    "end": 477
                },
                {
                    "start": 478,
                    "end": 600
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.10284423828125
        },
        {
            "corpus_id": "258418307",
            "title": "FlowTransformer: A Transformer Framework for Flow-based Network Intrusion Detection Systems",
            "text": "This study compares four transformer architectures, namely shallow encoder-based, shallow decoder-based, deep encoderbased, and deep decoder-based transformers. The shallow models are based on the basic multi-head self-attention transformer architecture and comprise between 2 and 6 encoder or decoder blocks. Two specific deep transformer models, GPT 2.0 and unmasked BERT, are also considered. The difference between shallow and deep models lies primarily in their depth, number of attention heads, and internal size, while their core transformer block structure is the same. Although both GPT and BERT use scaled-dot-product attention, BERT's attention mechanism is bidirectional, considering tokens in both directions, unlike GPT, which only considers previous tokens. \n\n1) GPT 2.0 -Deep Decoder Transformer [5]: GPT 2.0 is a generative model that is trained on next-word prediction. Unlike traditional input-output models, it treats the input prompt as part of a sequence and generates an output by using each generated word as part of the context to predict the subsequent word. This approach allows GPT to use exclusively transformer decoder blocks, rather than the traditional encoderdecoder structure. The internal block structure is repeated, with smaller GPT 2.0 models having 12 blocks. The input sequence is passed through these decoder blocks one by one, with the output of each block being fed as input to the next block. By using a stack of decoder blocks, GPT can model the distribution of natural language more effectively than a traditional transformer model, because the decoder blocks learn to generate the next word based on a combination of the input sequence and the previously generated words, instead of using only the input sequence. GPT is an autoregressive model that considers tokens only to the left of the token it is generating, moving through a sequence one token at a time. \n\n2) BERT -Deep Encoder Transformer [6]: BERT, in contrast to GPT, is an encoder-only transformer model, consisting of repeated blocks of transformer encoders. This is because BERT was primarily designed for natural language understanding tasks, rather than text generation.",
            "score": 0.4482444928035104,
            "section_title": "B. Transformer Models",
            "char_start_offset": 22153,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 160
                },
                {
                    "start": 161,
                    "end": 309
                },
                {
                    "start": 310,
                    "end": 395
                },
                {
                    "start": 396,
                    "end": 577
                },
                {
                    "start": 578,
                    "end": 772
                },
                {
                    "start": 775,
                    "end": 887
                },
                {
                    "start": 888,
                    "end": 1084
                },
                {
                    "start": 1085,
                    "end": 1210
                },
                {
                    "start": 1211,
                    "end": 1298
                },
                {
                    "start": 1299,
                    "end": 1436
                },
                {
                    "start": 1437,
                    "end": 1760
                },
                {
                    "start": 1761,
                    "end": 1908
                },
                {
                    "start": 1911,
                    "end": 2068
                },
                {
                    "start": 2069,
                    "end": 2183
                }
            ],
            "ref_mentions": [
                {
                    "start": 1945,
                    "end": 1948,
                    "matchedPaperCorpusId": "52967399"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.396728515625
        },
        {
            "corpus_id": "257219934",
            "title": "Full Stack Optimization of Transformer Inference: a Survey",
            "text": "Decoder Block. In contrast to encoder-only models, the decoderonly models [22,173,174] that consist of repeated decoder blocks are auto-regressive in nature. This means that the output at a given time step is based on the outputs in the previous time steps. In other words, the model predicts a token in a sentence based on the previous tokens it has generated so far, and the inference must therefore be performed sequentially and iteratively, once for each output token. For instance, if the previously generated sequence is \"I am a\", the model takes this as input and may predict the next token \"student\". Then, in the next time step, the input to the model becomes \"I am a student\". Therefore, the decoder-only structure is suitable for natural language generation tasks. It is important to note that, in decoder-only models, the input prompt tokens can be consumed in parallel before the model begins to generate subsequent tokens. For this work, we only consider open-ended generation (i.e., assuming no input prompt). \n\nUnlike the encoder block, which operates on the entire input sequence, the decoder block is inferred one token at a time. This results in a sequence length of one for each time step. In the case of the projection layers, each token is independent of the previously generated token. Thus, the projection operations are solely applied to the input token, resulting in a matrix-vector multiplication and a constant cost. However, this does not hold for the act-to-act matmuls, as the input token is not independent of the previously generated tokens. Instead, it is required to attend to all of them. Consequently, these operations scale linearly with sequence length, implying that more compute is required to process a token in a larger time step than a token in a smaller time step. A key detail to note is that the full key and value activations must be present for the input token to attend to all previously generated tokens. A common optimization technique for token generation is to cache and reuse the intermediate key and value of the previously generated tokens in subsequent iterations, thus avoiding the need to recompute them for every iteration.",
            "score": 0.4481285593716679,
            "section_title": "2.1.2",
            "char_start_offset": 16466,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 14
                },
                {
                    "start": 15,
                    "end": 157
                },
                {
                    "start": 158,
                    "end": 257
                },
                {
                    "start": 258,
                    "end": 472
                },
                {
                    "start": 473,
                    "end": 608
                },
                {
                    "start": 609,
                    "end": 686
                },
                {
                    "start": 687,
                    "end": 775
                },
                {
                    "start": 776,
                    "end": 936
                },
                {
                    "start": 937,
                    "end": 1024
                },
                {
                    "start": 1027,
                    "end": 1148
                },
                {
                    "start": 1149,
                    "end": 1209
                },
                {
                    "start": 1210,
                    "end": 1308
                },
                {
                    "start": 1309,
                    "end": 1444
                },
                {
                    "start": 1445,
                    "end": 1574
                },
                {
                    "start": 1575,
                    "end": 1624
                },
                {
                    "start": 1625,
                    "end": 1809
                },
                {
                    "start": 1810,
                    "end": 1955
                },
                {
                    "start": 1956,
                    "end": 2184
                }
            ],
            "ref_mentions": [
                {
                    "start": 82,
                    "end": 86,
                    "matchedPaperCorpusId": "160025533"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.59765625
        },
        {
            "corpus_id": "231986066",
            "title": "Position Information in Transformers: An Overview",
            "text": "There are different set-ups for using a Transformer model. One common possibility is to have an encoder only. For example, BERT (Devlin et al. 2019) uses a Transformer model T(X) as encoder to perform masked language modeling. In contrast, a traditional sequence-to-sequence approach can be materialized by adding a decoder. The decoder works almost identically to the encoder with two exceptions: (1) The upper triangle of the attention matrix A is usually masked in order to block information flow from future positions during the decoding process. (2) The output of the encoder is integrated through a cross-attention layer inserted before the feed-forward layer. See Vaswani et al. (2017) for more details. The differences between an encoder and encoder-decoder architecture are mostly irrelevant for the injection of position information and many architectures rely just on encoder layers. Thus for the sake of simplicity we will talk about Transformer encoder blocks in general for the majority of the article. See \u00a74.4 for position encodings that are tailored for encoder-decoder architectures.",
            "score": 0.4479807895961707,
            "section_title": "Encoder-Decoder",
            "char_start_offset": 6676,
            "sentence_offsets": [],
            "ref_mentions": [
                {
                    "start": 128,
                    "end": 147,
                    "matchedPaperCorpusId": "52967399"
                },
                {
                    "start": 671,
                    "end": 692,
                    "matchedPaperCorpusId": "13756489"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.38232421875
        },
        {
            "corpus_id": "269502453",
            "title": "A Systematic Literature Review on Large Language Models for Automated Program Repair",
            "text": "Categories.The literature has seen a variety of LLMs supporting NLP and SE research, which can be categorized into three main categories based on their model architectures.(1) Encoder-only LLMs, such as CodeBERT [35], GraphCodeBERT [46], train the encoder part of the Transformer to generate a fixed-dimensional bidirectional representation with Masked Language Modeling (MLM) and Next Sentence Prediction (NSP).MLM aims to predict the original tokens that have been randomly masked out, and NSP predicts whether two given sentences actually follow each other in a text.(2) Decoder-only LLMs, such as CodeGPT [95], train the decoder part of the Transformer to support auto-regressive tasks with Causal Language Modeling (CLM), which aims to predict new tokens in a sequence based on previous tokens.(3) Encoder-decoder LLMs, such as CodeT5 [154], train both encoder and decoder parts of the Transformer to support sequence-to-sequence generation tasks with denoising objectives.We will summarize existing LLMs and how they are leveraged to support program repair in Section 5.1.",
            "score": 0.44796196156817536,
            "section_title": "Model",
            "char_start_offset": 6307,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 11
                },
                {
                    "start": 11,
                    "end": 172
                },
                {
                    "start": 172,
                    "end": 412
                },
                {
                    "start": 412,
                    "end": 570
                },
                {
                    "start": 570,
                    "end": 799
                },
                {
                    "start": 799,
                    "end": 978
                },
                {
                    "start": 978,
                    "end": 1078
                }
            ],
            "ref_mentions": [
                {
                    "start": 172,
                    "end": 175,
                    "matchedPaperCorpusId": "256503496"
                },
                {
                    "start": 232,
                    "end": 236,
                    "matchedPaperCorpusId": "221761146"
                },
                {
                    "start": 609,
                    "end": 613,
                    "matchedPaperCorpusId": "231855531"
                },
                {
                    "start": 840,
                    "end": 845,
                    "matchedPaperCorpusId": "237386541"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.55224609375
        },
        {
            "corpus_id": "221865540",
            "title": "Pre-training for Abstractive Document Summarization by Reinstating Source Text",
            "text": "This work proposes to pre-train the decoder together with the encoder and then initialize both the encoder and decoder of a summarization model with the pre-trained Transformer model. \n\nThere is also a line of work that bridges extractive and abstractive models with attention mechanisms (Gehrmann et al., 2018;Hsu et al., 2018) and reinforcement learning (Chen and Bansal, 2018), while our model is simpler. Pre-training Pre-training methods draw a lot of attentions recently. Peters et al. (2018) and Radford et al. (2019) pre-trained LSTM and Transformer using language modeling objectives. To leverage the context in both directions, BERT (Devlin et al., 2019) is trained with the masked language modeling and next sentence prediction objectives. SpanBERT (Joshi et al., 2020) applied only the masked language modeling objective that masks contiguous random spans, rather than random tokens. XLNet (Yang et al., 2019) proposed a permutation language modeling objective that removes the independence assumption of masked tokens in BERT. RoBERTa (Liu et al., 2019) extends BERT with more training data and better training strategies. The above models focus on pre-training an encoder or a decoder, while we propose methods to pre-train a SEQ2SEQ model (i.e., the encoder together with the decoder) for abstractive summarization. Dong et al. (2019) (UniLM) proposed a unified language model that can be used for both natural language understanding and generation tasks, which is pre-trained using masked, unidirectional and SEQ2SEQ language modeling objectives. The encoder and decoder parameters are shared. By contrast, we pre-train a SEQ2SEQ Transformer with separate parameters for the encoder and decoder. Song et al. (2019) (MASS) proposed a method to pre-train a SEQ2SEQ Transformer by masking a span of text and then predicting the masked tokens. Their pre-training task is similar to our MDG task, but we apply a different masking strategy and predict the original text.",
            "score": 0.4475996995793613,
            "section_title": "Related Work",
            "char_start_offset": 5223,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 183
                },
                {
                    "start": 186,
                    "end": 408
                },
                {
                    "start": 409,
                    "end": 477
                },
                {
                    "start": 478,
                    "end": 593
                },
                {
                    "start": 594,
                    "end": 750
                },
                {
                    "start": 751,
                    "end": 895
                },
                {
                    "start": 896,
                    "end": 1039
                },
                {
                    "start": 1040,
                    "end": 1135
                },
                {
                    "start": 1136,
                    "end": 1330
                },
                {
                    "start": 1331,
                    "end": 1562
                },
                {
                    "start": 1563,
                    "end": 1609
                },
                {
                    "start": 1610,
                    "end": 1711
                },
                {
                    "start": 1712,
                    "end": 1855
                },
                {
                    "start": 1856,
                    "end": 1980
                }
            ],
            "ref_mentions": [
                {
                    "start": 288,
                    "end": 311,
                    "matchedPaperCorpusId": "52144157"
                },
                {
                    "start": 311,
                    "end": 328,
                    "matchedPaperCorpusId": "21723747"
                },
                {
                    "start": 356,
                    "end": 379,
                    "matchedPaperCorpusId": "44129061"
                },
                {
                    "start": 478,
                    "end": 498,
                    "matchedPaperCorpusId": "3626819"
                },
                {
                    "start": 503,
                    "end": 524,
                    "matchedPaperCorpusId": "160025533"
                },
                {
                    "start": 643,
                    "end": 664,
                    "matchedPaperCorpusId": "52967399"
                },
                {
                    "start": 760,
                    "end": 780,
                    "matchedPaperCorpusId": "198229624"
                },
                {
                    "start": 902,
                    "end": 921,
                    "matchedPaperCorpusId": "201304248"
                },
                {
                    "start": 1331,
                    "end": 1349,
                    "matchedPaperCorpusId": "147704286"
                },
                {
                    "start": 1712,
                    "end": 1730,
                    "matchedPaperCorpusId": "146808476"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.0855712890625
        },
        {
            "corpus_id": "258461308",
            "title": "Text Diffusion Model with Encoder-Decoder Transformers for Sequence-to-Sequence Generation",
            "text": "Denoising with Encoder-Decoder Framework Unlike DiffuSeq (Gong et al., 2022) using encoderonly Transformer architecture, we propose using an encoder-decoder Transformers architecture to model the input and output text sequences. For z 0 \u03b8 (z t , w x , t), we use the encoder to process the input sequences w x and use the decoder to model the noisy output sequence z t . Following the previous work (Li et al., 2022), we inject time step information t by adding time step embedding to z t . Using the encoder-decoder architecture has computational convenience during generation because the input sequences w x only require one forward computation through the encoder network during the whole reverse process. Considering the reverse process requires thousands of iterations to generate the output sequences of high quality, the saving of computational resources can be significant. \n\nDuring training and generation, the function z 0 \u03b8 generates denoised samples at the sequence level. Therefore making predictions from the denoising function z 0 \u03b8 resembles the non-autoregressive natural language generation. In this regard, we use a decoder with full attention matrices instead of causal attention matrices to model z t at the sequence level.",
            "score": 0.4472268349427461,
            "section_title": "Diffusion Model",
            "char_start_offset": 10872,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 228
                },
                {
                    "start": 229,
                    "end": 370
                },
                {
                    "start": 371,
                    "end": 490
                },
                {
                    "start": 491,
                    "end": 708
                },
                {
                    "start": 709,
                    "end": 881
                },
                {
                    "start": 884,
                    "end": 984
                },
                {
                    "start": 985,
                    "end": 1109
                },
                {
                    "start": 1110,
                    "end": 1244
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.3681640625
        },
        {
            "corpus_id": "261101164",
            "title": "DLIP: Distilling Language-Image Pre-training",
            "text": "Task Decoder. The decoder module is optional in the VLP model. Many VLP models adopt the encoder-only architecture, where the cross-modal representations are directly fed into an output layer to generate the final outputs. For transformer encoder-decoder architecture, the cross-modal representations are fed into a decoder and then to an output layer. More importantly, the encoder-decoder architecture is more flexible, as it can perform tasks such as image captioning, which may not be that straightforward for an encoder-only model to be applied to. Therefore, we take the decoder into account in our DLIP framework. The structure of the decoder is similar to the text encoder, and the difference is that it replaces the bidirectional self-attention layers with causal self-attention layers. \n\nThere are many different model designs under the DLIP framework, in order to train a light and efficient model to better adapt to different downstream tasks, we use the multimodal mixture of encoder-decoder (MED) [25] as our basic architecture, which can flexibly operate as a unimodal encoder or a multimodal encoder based on cross-attention, or a multimodal decoder.",
            "score": 0.44704181785821356,
            "section_title": "Model",
            "char_start_offset": 9611,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 13
                },
                {
                    "start": 14,
                    "end": 62
                },
                {
                    "start": 63,
                    "end": 222
                },
                {
                    "start": 223,
                    "end": 352
                },
                {
                    "start": 353,
                    "end": 553
                },
                {
                    "start": 554,
                    "end": 620
                },
                {
                    "start": 621,
                    "end": 795
                },
                {
                    "start": 798,
                    "end": 1166
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.650390625
        },
        {
            "corpus_id": "253098321",
            "title": "EncT5: A Framework for Fine-tuning T5 as Non-autoregressive Models",
            "text": "In this work, we propose a framework, EncT5, which converts an encoder-decoder T5 model to a non-auto-regressive model for classification, multilabel, and structured prediction and results in better performance, efficiency and usability. We also show that with models of sizes in similar ballpark, EncT5 is a better choice than BERT. However, we \n\nhave not yet concluded that Encoder-decoder pretraining is better than Encoder-only pre-training when everything else are hold the same (dataset, number of tokens seen). We did attempt to pre-train BERT on C4 but realized that designing an apple to apple comparison is a topic of its own. For future work, there are two directions: (1) the encoder-only model direction; and (2) the decoderonly model direction. For (1), we would like to answer the question of whether the EncT5 gains over BERT comes from pre-training or fine-tuning. We can do this by also training BERT on C4 with a similar training schedule. For (2), as decoder-only model becomes prevalent (Brown et al., 2020a;Chowdhery et al., 2022), we would also like to study how to convert them to a non-auto-regressive one to tackle the tasks we listed in the paper. These tasks covers most of the use cases in NLP application and is worth investigating if auto-regressive decoding is really necessary. \n\nOur work builds on top of publicly available pretrained checkpoints and study the fine-tuning performance on top of these checkpoints. BERT and T5 are trained on different datasets with different objectives. Our work does not answer whether C4 is better than Wikipedia+Book Corpus nor whether span corruption (Encoder-Decoder) is better than next sentence prediction + masked language model (Encoder-only).",
            "score": 0.44645886835413523,
            "section_title": "Conclusion and Future Work",
            "char_start_offset": 25245,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 237
                },
                {
                    "start": 238,
                    "end": 333
                },
                {
                    "start": 334,
                    "end": 345
                },
                {
                    "start": 348,
                    "end": 517
                },
                {
                    "start": 518,
                    "end": 636
                },
                {
                    "start": 637,
                    "end": 758
                },
                {
                    "start": 759,
                    "end": 881
                },
                {
                    "start": 882,
                    "end": 958
                },
                {
                    "start": 959,
                    "end": 1174
                },
                {
                    "start": 1175,
                    "end": 1310
                },
                {
                    "start": 1313,
                    "end": 1447
                },
                {
                    "start": 1448,
                    "end": 1520
                },
                {
                    "start": 1521,
                    "end": 1719
                }
            ],
            "ref_mentions": [
                {
                    "start": 1008,
                    "end": 1029,
                    "matchedPaperCorpusId": "218971783"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.28759765625
        },
        {
            "corpus_id": "226965220",
            "title": "Language Models not just for Pre-training: Fast Online Neural Noisy Channel Modeling",
            "text": "Prior work on neural noisy channel used channel models which were of the same size as the direct model (Yu et al., 2017;Yee et al., 2019). \n\nThe most recent work uses standard Transformer models (Yee et al., 2019;Ng et al., 2019;Yu et al., 2020). In this study, we hypothesize that the primary role of the channel model is to avoid explaining away effects by the language model. This primarily entails assigning low scores to unrelated outputs, which may not require a very powerful model. In this case, we may be able to substantially decrease the size of the channel model at only a small loss in accuracy. \n\nRecent work demonstrates that direct models with shallow decoders can give comparable accuracy, while being faster at inference time, compared to models with deep decoders (Wu et al., 2019;Elbayad et al., 2020;Kasai et al., 2020;Fan et al., 2020). This is particularly attractive for direct models for which the decoder network accounts for most of the wall time during inference but the dynamics for channel models are different: the channel model repeatedly scores the entire input sequence given progressively larger target prefixes. Unlike for direct models, there is no straightforward way to reuse the encoder output between time-steps, and we opt to recompute the entire encoder and decoder of the channel model at every target time-step. Since the input sequence is given, channel model computation can be batched over all tokens in the target prefix and the input sequence. This implies that we are free to adjust both the encoder and decoder depth. \n\nWe pursue two strategies to reduce model size: first, we progressively reduce the model dimension of the base Transformer architecture, by first halving the model dimension from 512 to 256, as well as the feed forward dimension from 2048 to 1024 for the half model. The smallest configuration uses a model dimension of just 32 and a feed forward dimension of 128 (denoted as 16th model). Second, we consider models with only a single encoder block and a single decoder block. These models have a postfix 1 1, e.g., 16th 1 1.",
            "score": 0.4464192778418606,
            "section_title": "Reducing Channel Model Size",
            "char_start_offset": 6005,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 138
                },
                {
                    "start": 141,
                    "end": 246
                },
                {
                    "start": 247,
                    "end": 378
                },
                {
                    "start": 379,
                    "end": 489
                },
                {
                    "start": 490,
                    "end": 608
                },
                {
                    "start": 611,
                    "end": 858
                },
                {
                    "start": 859,
                    "end": 1147
                },
                {
                    "start": 1148,
                    "end": 1356
                },
                {
                    "start": 1357,
                    "end": 1493
                },
                {
                    "start": 1494,
                    "end": 1569
                },
                {
                    "start": 1572,
                    "end": 1837
                },
                {
                    "start": 1838,
                    "end": 1959
                },
                {
                    "start": 1960,
                    "end": 2047
                },
                {
                    "start": 2048,
                    "end": 2096
                }
            ],
            "ref_mentions": [
                {
                    "start": 103,
                    "end": 120,
                    "matchedPaperCorpusId": "15816492"
                },
                {
                    "start": 120,
                    "end": 137,
                    "matchedPaperCorpusId": "201058550"
                },
                {
                    "start": 195,
                    "end": 213,
                    "matchedPaperCorpusId": "201058550"
                },
                {
                    "start": 213,
                    "end": 229,
                    "matchedPaperCorpusId": "196621535"
                },
                {
                    "start": 783,
                    "end": 800,
                    "matchedPaperCorpusId": "59310641"
                },
                {
                    "start": 800,
                    "end": 821,
                    "matchedPaperCorpusId": "204824061"
                },
                {
                    "start": 840,
                    "end": 857,
                    "matchedPaperCorpusId": "202750230"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.09979248046875
        },
        {
            "corpus_id": "268857084",
            "title": "Multitask-Based Evaluation of Open-Source LLM on Software Vulnerability",
            "text": "Since the advancements in Natural Language Processing, Large Language Models (LLMs) [1] have seen widespread adoption due to their capacity to be effectively trained with billions of parameters and training samples, resulting in significant performance enhancements.LLMs can readily be applied to downstream tasks through either fine-tuning [2] or prompting [3].Their versatility stems from being trained to possess a broad understanding, enabling them to capture diverse knowledge across various domains.Fine-tuning involves updating the model parameters specifically for a given downstream task through iterative training on a specific dataset.In contrast, prompting allows for direct utilization by providing natural language descriptions or a few examples of the downstream task.Compared to prompting, fine-tuning is resource-intensive as it necessitates additional model training and is applicable in limited scenarios, particularly when adequate training datasets are unavailable.\n\nLLMs are usually built on the transformer architecture [23] and can be classified into three types of architectures: encoder-only, encoder-decoder, and decoderonly.Encoder-only (e.g., CodeBERT [24], GraphCode-BERT [25], and UniXcoder [26]) and Encoder-Decoder (e.g., PLBART [27], CodeT5 [7], and CodeT5+ [8]) models are trained using Masked Language Modeling (MLM) or Masked Span Prediction (MSP) objective, respectively, where a small portion (e.g., 15%) of the tokens are replaced with either masked tokens or masked span tokens, LLMs are trained to recover the masked tokens.These models are trained as general ones on the code-related data and then are fine-tuned for the downstream tasks to achieve superior performance.Decoder-only models also attract a small portion of people's attention and they are trained by using Causal Language Modeling objectives to predict the probability of the next token given all previous tokens.GPT [2] and its variants are the most representative models, which bring the large language models into practical usage.",
            "score": 0.44611409844974215,
            "section_title": "Large Language Model",
            "char_start_offset": 7526,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 266
                },
                {
                    "start": 266,
                    "end": 362
                },
                {
                    "start": 362,
                    "end": 505
                },
                {
                    "start": 505,
                    "end": 646
                },
                {
                    "start": 646,
                    "end": 783
                },
                {
                    "start": 783,
                    "end": 986
                },
                {
                    "start": 988,
                    "end": 1152
                },
                {
                    "start": 1152,
                    "end": 1566
                },
                {
                    "start": 1566,
                    "end": 1713
                },
                {
                    "start": 1713,
                    "end": 1921
                },
                {
                    "start": 1921,
                    "end": 2041
                }
            ],
            "ref_mentions": [
                {
                    "start": 84,
                    "end": 87,
                    "matchedPaperCorpusId": "218971783"
                },
                {
                    "start": 1043,
                    "end": 1047,
                    "matchedPaperCorpusId": "13756489"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.404296875
        },
        {
            "corpus_id": "268041362",
            "title": "A Survey on Recent Advances in LLM-Based Multi-turn Dialogue Systems",
            "text": "Unlike decoder-only and encoder-decoder LLMs that utilize autoregressive regression, the encoder-only LLMs emphasize comprehension of input content and generation of task-specific outputs. \n\n2.2.1 BERT Series. BERT [47], short for Bidirectional Encoder Representations from Transformers, is a language model based on the transformer architecture, known for its significant improvement over previous state-of-the-art models. \n\nThe brief illustration for BERT series is shown in Figure 5.  BERT. BERT [47] namely a bidirectional Transformer-based encoder, is a LLM introduced by the Google AI Language team [48]. It incorporates masked language modeling, allowing pre-training to capture interactions between left and right context words. Recent advancements, such as extended training duration, parameter tying across layers, and span masking instead of individual words, have demonstrated improved performance. Notably, BERT's auto-regressive predictions limit its effectiveness for generation tasks. \n\nRoBERTa. RoBERTa [49], namely Robustly optimized BERT [47] approach, improves upon BERT [47] through straightforward modifications, such as longer training with larger batches, removal of the next sentence prediction objective, and dynamic changes to the masking pattern, and employs a more extensive byte-level Byte Pair Encoding (BPE) vocabulary. However, unlike BERT, RoBERTa streamlines its training process by omitting the Next Sentence Prediction (NSP) task and focusing solely on optimizing the Masked Language Model (MLM) task. This approach enhances the model's ability to learn bidirectional contextual information in language. \n\n2.2.2 UNiLM. UNILM [50] is a unified pre-training model jointly optimized for multiple language modeling objectives with shared parameters, covering bidirectional, unidirectional, and sequence-to-sequence language models. The model undergoes pre-training using three types of language modeling tasks: unidirectional, bidirectional, and sequence-tosequence prediction. Unified modeling is achieved through a shared Transformer network, incorporating specific self-attention masks to control the contextual conditions for predictions.",
            "score": 0.44602701660362315,
            "section_title": "Encoder-only Transformer Architecture",
            "char_start_offset": 12669,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 188
                },
                {
                    "start": 191,
                    "end": 209
                },
                {
                    "start": 210,
                    "end": 423
                },
                {
                    "start": 426,
                    "end": 493
                },
                {
                    "start": 494,
                    "end": 610
                },
                {
                    "start": 611,
                    "end": 736
                },
                {
                    "start": 737,
                    "end": 910
                },
                {
                    "start": 911,
                    "end": 1000
                },
                {
                    "start": 1003,
                    "end": 1011
                },
                {
                    "start": 1012,
                    "end": 1351
                },
                {
                    "start": 1352,
                    "end": 1538
                },
                {
                    "start": 1539,
                    "end": 1640
                },
                {
                    "start": 1643,
                    "end": 1655
                },
                {
                    "start": 1656,
                    "end": 1864
                },
                {
                    "start": 1865,
                    "end": 2010
                },
                {
                    "start": 2011,
                    "end": 2175
                }
            ],
            "ref_mentions": [
                {
                    "start": 1662,
                    "end": 1666,
                    "matchedPaperCorpusId": "147704286"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.51025390625
        },
        {
            "corpus_id": "253368350",
            "title": "An Adversarial Perspective on Accuracy, Robustness, Fairness, and Privacy: Multilateral-Tradeoffs in Trustworthy ML",
            "text": "A feature is non-spurious if a change in that feature leads to a change in Y, efficient if a change in Y leads to a change in the feature; and features are disentangled if changing each feature has a differing effect on Y. \n\nOne approach towards causal representation learning is summarized in the encoder-decoder formalism of [76], which seeks to construct causal features by first using an encoder E to map the p input features X i to m p noise variables N j , then models the causal relationships between Z i using an SCM \n\nIntuitively, each mechanism f i should reflect an cause-effect relationship that is independent of the other cause-effect relationships governing the transformation X \u2192 Z. A decoder D is trained so that D(E(X)) \u2248 X on the training distribution. The resulting causal features Z i contain the same information as the raw features X i , but are lower-dimensional and have favorable causal structure. This approach is popular, but has the disadvantages of not connecting the causal features with the target Y, not learning explicitly useful causal relations; and in general, such models are not uniquely identifiable from data. \n\nInvariant risk minimization (IRM) is an alternative approach to causal learning which explicitly learns feature representations that encode environmental invariance [77]. In this paradigm, the data is collected from multiple training environments D e := {(X e i , Y e i )} governed by different laws P e (X e , Y e ), with the assumption that the mechanism relating Y e to X e is invariant across environments. IRM proposes to learn that representation by finding a data representation Z = f (X) that simultaneously minimizes the risks R e (f ) = E (X,Y)\u223cP e (g(f (X)), Y)in all environments; here the minimization is done over the representation f and the predictor g. The original work showed the existence of invariant predictors across all training environments f when both f and g are constrained to be linear and the training environments satisfy some diversity condition. \n\nThe IRM paradigm has been critiqued and expanded significantly, with the most powerful results so far due to [78].",
            "score": 0.44584443074677715,
            "section_title": "C. IMPOSING CAUSALITY: CAUSAL REPRESENTATION LEARNING",
            "char_start_offset": 45218,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 222
                },
                {
                    "start": 225,
                    "end": 524
                },
                {
                    "start": 527,
                    "end": 698
                },
                {
                    "start": 699,
                    "end": 771
                },
                {
                    "start": 772,
                    "end": 923
                },
                {
                    "start": 924,
                    "end": 1150
                },
                {
                    "start": 1153,
                    "end": 1323
                },
                {
                    "start": 1324,
                    "end": 1563
                },
                {
                    "start": 1564,
                    "end": 1822
                },
                {
                    "start": 1823,
                    "end": 2031
                },
                {
                    "start": 2034,
                    "end": 2148
                }
            ],
            "ref_mentions": [
                {
                    "start": 327,
                    "end": 331,
                    "matchedPaperCorpusId": "261325982"
                },
                {
                    "start": 1318,
                    "end": 1322,
                    "matchedPaperCorpusId": "211082853"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.44677734375
        },
        {
            "corpus_id": "278714682",
            "title": "Tracr-Injection: Distilling Algorithms into Pre-trained Language Models",
            "text": "A key motivation for our work is to enforce symbolic reasoning in a pre-trained language model by injecting explicit RASP variables into its activations. We first verify that the model encodes these variables in its residual stream: our learned linear layer maps the residual stream activations to the compiled model's representation, enabling us to decode intermediate variables using their known labels (see Figure 2 and Appendix E). \n\nTo assess their causal role, we perturb the symbolic subspace by injecting noise. This degradation in performance is significantly larger than that caused by perturbing random directions, indicating that the injected symbolic representations are causally influential in the model's predictions (see Appendix A). \n\nThe encoding of ground truth variables that the model causally uses for its predictions is an im- portant benefit of our proposed method. By being able to decode the residual stream activations into these interpretable variables, we are much closer to having a final model that we can understand how it solves the task. This is not the case with normal finetuning settings, including our baseline, as we have no strong methods to extract ground-truth representations which are causally relevant to solve the task at hand. The call for more transparent systems has been raised several times in recent years (Bengio et al., 2025;Casper et al., 2024), and our method is a step toward this goal.",
            "score": 0.4457640616124711,
            "section_title": "Symbolic Representations Learned",
            "char_start_offset": 12138,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 153
                },
                {
                    "start": 154,
                    "end": 435
                },
                {
                    "start": 438,
                    "end": 519
                },
                {
                    "start": 520,
                    "end": 749
                },
                {
                    "start": 752,
                    "end": 889
                },
                {
                    "start": 890,
                    "end": 1071
                },
                {
                    "start": 1072,
                    "end": 1273
                },
                {
                    "start": 1274,
                    "end": 1443
                }
            ],
            "ref_mentions": [
                {
                    "start": 1379,
                    "end": 1399,
                    "matchedPaperCorpusId": "267301601"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.01525115966796875
        },
        {
            "corpus_id": "248118752",
            "title": "What Language Model Architecture and Pretraining Objective Work Best for Zero-Shot Generalization?",
            "text": "Large language models (LLMs) pretrained on unstructured text data have been shown to be capable of performing a wide variety of text processing tasks without additional training. This ability has been referred to as zero-shot generalization since these models are typically pretrained with a self-supervised objective that is not specific to a downstream task. Zero-shot generalization is particularly useful because it does not require any additional data or training in order to enable the model to perform a given task. As such, there has been an explosion of work on developing LLMs and training techniques that produce strong zero-shot generalization [Brown et al., 2020, Wang and Komatsuzaki, 2021, Du et al., 2021, Lin et al., 2021, Rae et al., 2021, Hoffmann et al., 2022, Chowdhery et al., 2022]. One recent line of work [Sanh et al., 2021, Wei et al., 2021, Xu et al., 2022] has demonstrated that adding an explicit multitask finetuning stage on an ensemble of prompted tasks after pretraining can significantly boost the zero-shot capabilities of LLMs. \n\nModern LLMs are based on the Transformer architecture [Vaswani et al., 2017]. While the original Transformer included a separate encoder that processes input text and a decoder that generates target text, most recent LLMs are causal decoder-only (CD) models trained to autoregressively predict a text sequence [Liu et al., 2018, Radford et al., 2018, Al-Rfou et al., 2019]. In contrast with this trend, Raffel et al. [2020] has shown that encoder-decoder (ED) models outperform decoder-only LLMs for transfer learning (i.e. where a pretrained model is finetuned on a single downstream task). Non-causal decoders (ND) [Liu et al., 2018, Dong et al., 2019] use a modified attention mask to bridge the gap between decoder-only and encoder-decoder models. However, they have seen limited adoption.",
            "score": 0.44552282191581016,
            "section_title": "Introduction",
            "char_start_offset": 561,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 178
                },
                {
                    "start": 179,
                    "end": 360
                },
                {
                    "start": 361,
                    "end": 522
                },
                {
                    "start": 523,
                    "end": 805
                },
                {
                    "start": 806,
                    "end": 1063
                },
                {
                    "start": 1066,
                    "end": 1143
                },
                {
                    "start": 1144,
                    "end": 1439
                },
                {
                    "start": 1440,
                    "end": 1589
                },
                {
                    "start": 1590,
                    "end": 1657
                },
                {
                    "start": 1658,
                    "end": 1817
                },
                {
                    "start": 1818,
                    "end": 1859
                }
            ],
            "ref_mentions": [
                {
                    "start": 1120,
                    "end": 1142,
                    "matchedPaperCorpusId": "13756489"
                },
                {
                    "start": 1376,
                    "end": 1393,
                    "matchedPaperCorpusId": "3608234"
                },
                {
                    "start": 1393,
                    "end": 1415,
                    "matchedPaperCorpusId": "49313245"
                },
                {
                    "start": 1415,
                    "end": 1438,
                    "matchedPaperCorpusId": "52004855"
                },
                {
                    "start": 1469,
                    "end": 1489,
                    "matchedPaperCorpusId": "204838007"
                },
                {
                    "start": 1683,
                    "end": 1700,
                    "matchedPaperCorpusId": "3608234"
                },
                {
                    "start": 1700,
                    "end": 1720,
                    "matchedPaperCorpusId": "147704286"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.658203125
        },
        {
            "corpus_id": "201058686",
            "title": "Towards Making the Most of BERT in Neural Machine Translation",
            "text": "As shown in Table 2, pre-trained language model representations are most effective when supervised on the encoder part but less effective on the decoder part. As BERT contains bidirectional information, pre-training decoder may lead inconsistencies between the training and the inference. The GPT-2 Transformer uses constrained selfattention where every token can only attend to context to its left, thus it is natural to introduce GPT-2 to the NMT decoder. While there are still no more significant gains obtained in our experiments. One possible reason is that the decoder is not a typical language model, which contains the information from source attention. We will leave this issue in the future study.\n\nBERT v.s. GPT-2\n\nWe compare BERT with GPT-2 (Radford et al. 2019; on WMT 2014 English-German corpus. As shown in Table 2, BERT added encoder works better than GPT-2. The experiments suggest that bidirectional information plays an important role in the encoder of NMT models. While for the decoder part, GPT-2 is a more priority choice. In the following part, we choose BERT as the pre-trained LM and apply only for the encoder part.",
            "score": 0.4454713301375848,
            "section_title": "Models",
            "char_start_offset": 18798,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.1610107421875
        },
        {
            "corpus_id": "235097256",
            "title": "Multi-Style Transfer with Discriminative Feedback on Disjoint Corpus",
            "text": "This is not only computationally more efficient but it also closely ties the underlying language distribution of the two modules. This is expected to make the discriminative feedback more effective while fine tuning the transfer model for multiple styles. \n\nIn Syed et al. (2020)'s setup, both encoder and decoder in the style transfer module are initialized with the pre-trained language model (trained on MLM objective). Instead, we initialize the decoder with the language model fine-tuned with the target style using Causal Language Modeling (CLM) objective, before training the joint encoder-decoder model, as detailed in Section 3.2. The encoder is initialized with the pre-trained model directly. Aligning the decoder to the distribution of the tar- get style helps speed up the fine-tuning process as decoder is more adept at generating stylized outputs. This does not add to computational overhead as these fine-tuned models are repurposed as discriminators for stylistic feedback (Section 3.2). \n\nTo instill style-awareness to the encoder-decoder setup initialized with pre-trained Transformer models, we fine-tune it with Denoising Autoencoder (DAE) loss using the target-domain corpus. In case of multiple styles, we use a randomized mixture of target-domain corpus from each of the target styles. Under the DAE objective, the encoder takes a noisy masked version x of the text x as input and attempts to fill in the mask token as per the MLM objective that it was pre-trained on. In turn, the decoder re-creates stylistic version of original sentence from this noisy output from the encoder. The overall training objective is \n\nwhere \u03b8 G are the trainable parameters of the encoder-decoder model. The noisy version of sentence x from the target corpus T is obtained after dropping tokens from x with probability p drop and masking with a probability of p mask . In conjunction, the encoder and decoder enable style transfer to the target style. The noteworthy aspect here is that the model has no sense of source style and is trained to generate sentences to match the style of the target-domain corpus with which it is trained.",
            "score": 0.4452825297294284,
            "section_title": "Pre-trained LM as Encoder-Decoder",
            "char_start_offset": 11503,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 129
                },
                {
                    "start": 130,
                    "end": 255
                },
                {
                    "start": 258,
                    "end": 422
                },
                {
                    "start": 423,
                    "end": 639
                },
                {
                    "start": 640,
                    "end": 703
                },
                {
                    "start": 704,
                    "end": 862
                },
                {
                    "start": 863,
                    "end": 1004
                },
                {
                    "start": 1007,
                    "end": 1197
                },
                {
                    "start": 1198,
                    "end": 1309
                },
                {
                    "start": 1310,
                    "end": 1492
                },
                {
                    "start": 1493,
                    "end": 1604
                },
                {
                    "start": 1605,
                    "end": 1638
                },
                {
                    "start": 1641,
                    "end": 1709
                },
                {
                    "start": 1710,
                    "end": 1874
                },
                {
                    "start": 1875,
                    "end": 1957
                },
                {
                    "start": 1958,
                    "end": 2141
                }
            ],
            "ref_mentions": [
                {
                    "start": 261,
                    "end": 279,
                    "matchedPaperCorpusId": "202719307"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.310791015625
        },
        {
            "corpus_id": "265375729",
            "title": "From System Models to Class Models: An In-Context Learning Paradigm",
            "text": "An encoder-decoder Transformer similar to the one originally introduced for language translation in [25] and adapted to the model-free in-context simulation task is used. The overall architecture is visualized in Fig. 2 and consists in: (i) an encoder that processes u 1:m , y 1:m (without causality restriction) and generates an embedding sequence \u03b6 1:m ; (ii) a decoder that processes \u03b6 1:m and test input u m+1:N (the latter with causal restriction) to produce the sequence of predictions \u0177m+1:N . Similarly to the one-step-ahead prediction task discussed in Section 3.1, the standard encoder-decoder Transformer is modified to process real-valued input/output sequences. \n\nIn a model-based interpretation, the output of the encoder \u03b6 1:m may be seen as a hidden representation of the system S (i) that is used as an implicit Figure 1: GPT-like decoder-only Transformer for one-step-ahead prediction. Differences w.r.t. plain GPT-2 for text generation [22,15] are highlighted in pink. \n\n\"model parameter\" enabling the decoder to simulate the system's response to the sequence u m+1:N . \n\nSimilarly to one-step-ahead prediction case, the weights \u03d5 of the meta-model M \u03d5 are obtained by minimizing over \u03d5 the loss \n\nAs in ( 9), a sample-based approximation over systems S (i) and datasets D (i) is used to approximate the expected value ( 11): \n\nFigure 2: Encoder-decoder Transformer for multi-step-ahead simulation. Main differences w.r.t. the standard Transformer architecture for language translation [25] are highlighted in pink.",
            "score": 0.4452166914069319,
            "section_title": "Encoder-Decoder Transformer for simulation",
            "char_start_offset": 16742,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 170
                },
                {
                    "start": 171,
                    "end": 500
                },
                {
                    "start": 501,
                    "end": 674
                },
                {
                    "start": 677,
                    "end": 903
                },
                {
                    "start": 904,
                    "end": 922
                },
                {
                    "start": 923,
                    "end": 987
                },
                {
                    "start": 990,
                    "end": 1088
                },
                {
                    "start": 1091,
                    "end": 1214
                },
                {
                    "start": 1217,
                    "end": 1344
                },
                {
                    "start": 1347,
                    "end": 1417
                },
                {
                    "start": 1418,
                    "end": 1534
                }
            ],
            "ref_mentions": [
                {
                    "start": 955,
                    "end": 959,
                    "matchedPaperCorpusId": "160025533"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.5830078125
        },
        {
            "corpus_id": "271358641",
            "title": "Research on Machine Learning Program Generation Algorithm Based on AORBCO",
            "text": "In this paper, the CodeT5 family of models is used as the basic model for code generation, on the basis of which further innovations are made to obtain the CodeT5+ model. First, the model introduces a flexible mode selection mechanism, which enables it to run flexibly in encoder-only, decoder-only, or encoder-decoder modes according to the needs of different tasks. This design makes CodeT5+ more adaptable to different types of downstream tasks and improves the generality of the model. Second, CodeT5+ employs a multi-task pre-training strategy, including diverse tasks such as span denoising, causal language modeling (CLM), and text-code comparison learning. Such a set of pre-training tasks helps the model learn richer representations from both code and text data, allowing for better migration and adaptation in various applications. \n\nIn terms of model architecture, CodeT5+ adopts a \"shallow encoder and deep decoder\" architecture. The encoder and decoder get initialized by pre-training checkpoints and connected to the cross-attention layer. By freezing the deep decoder and training only the shallow encoder and the cross-attention layer, the computational efficiency is improved while the performance of the model is maintained. In addition, CodeT5+ introduces mechanisms for adjusting instructions to better align with natural language instructions. This mechanism makes the model more flexible in understanding and following natural language instructions, thus better meeting user expectations when generating code.",
            "score": 0.44472219732153634,
            "section_title": "1) Code generation module",
            "char_start_offset": 11429,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 170
                },
                {
                    "start": 171,
                    "end": 367
                },
                {
                    "start": 368,
                    "end": 489
                },
                {
                    "start": 490,
                    "end": 664
                },
                {
                    "start": 665,
                    "end": 842
                },
                {
                    "start": 845,
                    "end": 942
                },
                {
                    "start": 943,
                    "end": 1054
                },
                {
                    "start": 1055,
                    "end": 1243
                },
                {
                    "start": 1244,
                    "end": 1365
                },
                {
                    "start": 1366,
                    "end": 1532
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.278564453125
        },
        {
            "corpus_id": "268819250",
            "title": "An Empirical Study of Automated Vulnerability Localization with Large Language Models",
            "text": "3) Impact of Model Architectures: The results of Partial category, where all LLMs have the same input setting, allows us to make fair comparisons with respect to different architectures.Encoder-only architectures, exemplified by CodeBERT and GraphCodeBERT, achieve remarkable precision (90.0%This balance benefits from the synergistic work of the encoder and decoder during pre-training.The encoder contextualizes the input, and the decoder leverages this understanding to generate words, enabling the model to flexibly learn language structures.However, PLBART lags significantly behind other models, a discrepancy that can be attributed to its smaller volume of pre-training data and programming languages (i.e., only Java and Python).Decoder-only models, particularly advanced ones like CodeLlama-7B, present an interesting case.With an impressive F1-score of 87.3% in BV-LOC, these models affirm the strength of understanding capabilities even when primarily designed for generation tasks.Surprisingly, despite having much more parameters, CodeGen-6B trails behind encoderonly models such as CodeBERT.Given the fundamental differences in their architectures rather than pre-training data, it can be inferred that the masked self-attention mechanism limits their ability to capture bidirectional context.Consequently, a considerable number of vulnerabilities that require a comprehensive understanding of the context, such as useafter-free issues, may not be accurately identified.\n\nFinding 5: Encoder-based LLMs excel in capturing contextual semantics essential for accurately identifying vulnerabilities, whereas decoder-only models are limited in unidirectional context, even with similar pre-training data.",
            "score": 0.44428425926392134,
            "section_title": "B. RQ2: Effectiveness of Fine-Tuning LLMs",
            "char_start_offset": 28737,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 186
                },
                {
                    "start": 186,
                    "end": 292
                },
                {
                    "start": 292,
                    "end": 387
                },
                {
                    "start": 387,
                    "end": 546
                },
                {
                    "start": 546,
                    "end": 737
                },
                {
                    "start": 737,
                    "end": 832
                },
                {
                    "start": 832,
                    "end": 993
                },
                {
                    "start": 993,
                    "end": 1105
                },
                {
                    "start": 1105,
                    "end": 1307
                },
                {
                    "start": 1307,
                    "end": 1484
                },
                {
                    "start": 1486,
                    "end": 1713
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.439453125
        },
        {
            "corpus_id": "252118711",
            "title": "IDIAPers @ Causal News Corpus 2022: Extracting Cause-Effect-Signal Triplets via Pre-trained Autoregressive Language Model",
            "text": "We utilize T5 (Raffel et al., 2020), a pre-trained autoregressive transformer-based language model trained on a mixture of unsupervised and supervised tasks that require language understanding. The model is conditioned n\u00d73 times for each example, as there can be n CES triplets in one sentence (up to n = 4 triplets in training data). Each time, we condition the language model 3 times for every example and its corresponding CES triplet, generating a different triplet component (cause, effect, and signal) to learn to generate the entire CES triplet. As these triplets are unordered, we uniformly sample a random path among them (e.g., 2-3-1-4, for sample with four triplets) during training. We only train with as many triplets, as available in the training data. We now describe the input format, further illustrated in Appendix B. Firstly, the model's encoder is conditioned with sentence tokens <sentence> followed by the history of already generated CES triplets for this example (empty if there was none) as <sentence> _history : <history>.\n\nThe history is always prepended with _history: tokens. The content of the history are the already generated triplets. Each part of the triplet is prepended with its corresponding _cause:, or _effect:, or _signal: sequence. Concurrently, model's decoder is prefixed with _cause: sequence. In this case, the probability of cause sequence is maximized. Secondly, the model is conditioned with sentence tokens <sentence> and cause tokens <cause>, prepended with _cause: token as <sentence> _cause : <cause> _history : <history>.\n\nThis time, the decoder is prompted with _effect: prefix, and the probability of effect sequence is maximized. Thirdly, the model is conditioned with sentence tokens <sentence>, cause tokens <cause>, and effect tokens <effect> with _effect: token prepended as <sentence> _cause : <cause> _effect : <effect> _history : <history>.\n\nAnalogically, decoder is prompted with _signal: prefix and probability of signal sequence is",
            "score": 0.44405986133560865,
            "section_title": "Language Model Training",
            "char_start_offset": 5625,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.0304412841796875
        },
        {
            "corpus_id": "268513600",
            "title": "Causality from Bottom to Top: A Survey",
            "text": "Encoder-decoder is a common used architecture in DL.Encoder-decoder models have effectively been used for causality reasoning from natural language [183].Cause-and-Effect Pair Mining (CEPM) [184] is an unsupervised model that learns relationships without labels by generating candidate pairs with an encoder-decoder and scoring them.Causal Inference over Natural Language (COIN) [185] also uses an encoder-decoder to produce implications and explanations, representing causality as queryable graphs.Some models not only propose claims but validate them via an encoder-decoder rationale generation component.Additionally, counterfactual text can be created with encoder-decoders by suggesting alterations to a potential cause or effect [186].These techniques represent causality and help justify relationships, revealing how encoder-decoders are well-suited for understanding causality from language.ML algorithms enhance the interpretability of causal models by shedding light on the causal relationships between variables [187].Identifying the most impactful causal relationships can be accomplished by analyzing the magnitude of their effects.Time-series analysis benefits from ML algorithms to study causal relationships between variables that change over time [188].Economic data, for example, can be employed to predict the effect of policy interventions using ML algorithms.ML algorithms facilitate causal inference even with incomplete or missing data.By leveraging observed data patterns, missing data can be imputed using ML algorithms.The above mentioned approaches and examples exemplify how ML and DL can be harnessed to explore causality within data.\n\nIntegrating ML with causal inference allows for unveiling causal relationships between variables, making predictions regarding intervention effects, and applying these insights in various domains such as healthcare, finance, and economics.\n\nIn previous sections, we mentioned the gap between GA and causal inference approaches.While there are approaches that tries to connect between the two [189,190,191].\n\n13 Causality and Generative AI (GAI)\n\nCausality plays a crucial role in explaining past results, but it can also support generated data.The integration between causality and emerging GAI plays an important role in research and applications.",
            "score": 0.4437208191183021,
            "section_title": "Causality and Bigdata",
            "char_start_offset": 59777,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 52
                },
                {
                    "start": 52,
                    "end": 154
                },
                {
                    "start": 154,
                    "end": 333
                },
                {
                    "start": 333,
                    "end": 499
                },
                {
                    "start": 499,
                    "end": 607
                },
                {
                    "start": 607,
                    "end": 741
                },
                {
                    "start": 741,
                    "end": 899
                },
                {
                    "start": 899,
                    "end": 1029
                },
                {
                    "start": 1029,
                    "end": 1145
                },
                {
                    "start": 1145,
                    "end": 1270
                },
                {
                    "start": 1270,
                    "end": 1380
                },
                {
                    "start": 1380,
                    "end": 1459
                },
                {
                    "start": 1459,
                    "end": 1545
                },
                {
                    "start": 1545,
                    "end": 1663
                },
                {
                    "start": 1665,
                    "end": 1904
                },
                {
                    "start": 1906,
                    "end": 1992
                },
                {
                    "start": 1992,
                    "end": 2071
                },
                {
                    "start": 2073,
                    "end": 2109
                },
                {
                    "start": 2111,
                    "end": 2209
                },
                {
                    "start": 2209,
                    "end": 2313
                }
            ],
            "ref_mentions": [
                {
                    "start": 148,
                    "end": 153,
                    "matchedPaperCorpusId": "248119123"
                },
                {
                    "start": 190,
                    "end": 195,
                    "matchedPaperCorpusId": "199465904"
                },
                {
                    "start": 379,
                    "end": 384,
                    "matchedPaperCorpusId": "237386009"
                },
                {
                    "start": 1023,
                    "end": 1028,
                    "matchedPaperCorpusId": "212633782"
                },
                {
                    "start": 1264,
                    "end": 1269,
                    "matchedPaperCorpusId": "220309392"
                },
                {
                    "start": 2057,
                    "end": 2062,
                    "matchedPaperCorpusId": "204939013"
                },
                {
                    "start": 2062,
                    "end": 2066,
                    "matchedPaperCorpusId": "7517127"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.3310546875
        },
        {
            "corpus_id": "248118752",
            "title": "What Language Model Architecture and Pretraining Objective Work Best for Zero-Shot Generalization?",
            "text": "We consider full language modeling (FLM) , prefix language modeling (PLM) , and masked language modeling (MLM) (specifically, the span corruption objective of Raffel et al. [2020]). The choice of language modeling objective depends on the architecture: the causal decoder uses either FLM or MLM, while the non-causal decoder and the encoder-decoder use either PLM or MLM. \n\nAll of our models are pretrained on 168 billion tokens of the C4 dataset from Raffel et al. [2020]. \n\nWe use the Adafactor [Shazeer and Stern, 2018] optimizer with an inverse square root learning rate schedule, training on batches of 2,048 sequences of length 626 tokens (for a total of 131,072 training steps). Detailed pretraining hyperparameters can be found in Table 2: we based elements of our pretraining setup (such as Adafactor, GEGLU, and the use of an auxiliary Z loss L(Z) = 10 \u22124 * log 2 (Z) to stabilize training [Chowdhery et al., 2022]) on the popular T5.1.1 recipe. \n\nTo operate with a fixed compute budget, we match the amount of tokens seen during pretraining (which corresponds to the total computational cost), not the number of tokens trained on (i.e. on which a loss is calculated). Full language modeling computes a loss on all the tokens it sees, whereas prefix language modeling cannot train on the tokens in its prefix: on average, it will train on half as many tokens as full language modeling. We consider these to be inherent trade-offs in efficiency between training objectives. We concatenated and sampled text from documents in such a way that there was virtually no padding during pretraining. More specifically to each objective: \n\n\u2022 For full language modeling, the loss is computed for all 626 token in each sequence in parallel, making for the most efficient configuration (100% of tokens are trained on).",
            "score": 0.44334500429866,
            "section_title": "Pretraining",
            "char_start_offset": 22425,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 181
                },
                {
                    "start": 182,
                    "end": 371
                },
                {
                    "start": 374,
                    "end": 473
                },
                {
                    "start": 476,
                    "end": 685
                },
                {
                    "start": 686,
                    "end": 955
                },
                {
                    "start": 958,
                    "end": 1178
                },
                {
                    "start": 1179,
                    "end": 1395
                },
                {
                    "start": 1396,
                    "end": 1482
                },
                {
                    "start": 1483,
                    "end": 1600
                },
                {
                    "start": 1601,
                    "end": 1637
                },
                {
                    "start": 1640,
                    "end": 1815
                }
            ],
            "ref_mentions": [
                {
                    "start": 159,
                    "end": 179,
                    "matchedPaperCorpusId": "204838007"
                },
                {
                    "start": 452,
                    "end": 472,
                    "matchedPaperCorpusId": "204838007"
                },
                {
                    "start": 497,
                    "end": 522,
                    "matchedPaperCorpusId": "4786918"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.360107421875
        },
        {
            "corpus_id": "270062996",
            "title": "Disentangling and Integrating Relational and Sensory Information in Transformer Architectures",
            "text": "We now briefly discuss the differences in the corresponding model architectures. Altabaa et al. [21] propose an encoder-like module called the Abstractor which consists of essentially replacing selfattention in an Encoder with relational cross-attention. That is, it consists of iteratively performing RCA followed by an MLP. The paper proposes several ways to incorporate this into the broader Transformer architecture. For example, some of the experiments use a Encoder \u2192 Abstractor \u2192 Decoder architecture to perform a sequence-to-sequence task. Here, the output of a standard Transformer Encoder is fed into an Abstractor, and the Decoder cross-attends to the output of the Abstractor. In another sequence-to-sequence experiment, Altabaa et al. [21] use an architecture where the Decoder cross-attends to both the Encoder and the Abstractor, making use of both sensory and relational information. In particular, the standard encoder and decoder blocks are the same (focusing on sensory information), but an additional module is inserted in between with a relational inductive bias. \n\nBy contrast, our approach in this paper is to propose novel encoder and decoder architectures imbued with two distinct types of attention heads, one with an inductive bias for sensory information and the other with an inductive bias for relational information. This has several potential advantages. The first is versatility and generality. The Abstractor architectures that were explored in [21] only explicitly support sequence-to-sequence or discriminative tasks. For example, they do not support autoregressive models like modern decoder-only language models (e.g., of the form we experiment with in Section 4.4). Moreover, even in sequence-to-sequence tasks, Abstractor architectures only support relational processing over the input sequence, but they do not support relational processing over the target sequence (since the decoder does not have RCA). Another potential advantage of DAT is simplicity. The Abstractor paper proposes several architectures and configurations for the Encoder/Abstractor/Decoder modules, introducing several hyperparameters that are not trivial to choose.",
            "score": 0.4433436933380883,
            "section_title": "D.2 Comparison between DAT and the Abstractor",
            "char_start_offset": 64045,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 80
                },
                {
                    "start": 81,
                    "end": 254
                },
                {
                    "start": 255,
                    "end": 325
                },
                {
                    "start": 326,
                    "end": 420
                },
                {
                    "start": 421,
                    "end": 547
                },
                {
                    "start": 548,
                    "end": 688
                },
                {
                    "start": 689,
                    "end": 899
                },
                {
                    "start": 900,
                    "end": 1084
                },
                {
                    "start": 1087,
                    "end": 1347
                },
                {
                    "start": 1348,
                    "end": 1386
                },
                {
                    "start": 1387,
                    "end": 1427
                },
                {
                    "start": 1428,
                    "end": 1553
                },
                {
                    "start": 1554,
                    "end": 1704
                },
                {
                    "start": 1705,
                    "end": 1945
                },
                {
                    "start": 1946,
                    "end": 1995
                },
                {
                    "start": 1996,
                    "end": 2178
                }
            ],
            "ref_mentions": [
                {
                    "start": 96,
                    "end": 100,
                    "matchedPaperCorpusId": "257912616"
                },
                {
                    "start": 748,
                    "end": 752,
                    "matchedPaperCorpusId": "257912616"
                },
                {
                    "start": 1479,
                    "end": 1483,
                    "matchedPaperCorpusId": "257912616"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.5126953125
        },
        {
            "corpus_id": "266335547",
            "title": "On the compression of shallow non-causal ASR models using knowledge distillation and tied-and-reduced decoder for low-latency on-device speech recognition",
            "text": "Our ASR model is based on the cascaded Conformer-T architecture [13]. The cascaded architecture consists of first pass causal encoder for generating immediate streaming output and second pass non-causal encoder to emit final output. Both encoders are made up of multiple layers of Conformer blocks [2]. In addition, the cascaded model employs a shared transducer decoder that functions similarly to a language model. Both the causal and non-causal encoders directly connected to the shared decoder. During training, total loss is computed as weighted sum of output coming from shared decoder via causal and non-causal connections, respectively, as mentioned in [13]. During inference time, model can operate either in streaming or non-streaming mode depending on feature extracted from causal or non-causal encoder for decoding.",
            "score": 0.4432161056180912,
            "section_title": "Cascaded Conformer transducer",
            "char_start_offset": 3719,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 69
                },
                {
                    "start": 70,
                    "end": 232
                },
                {
                    "start": 233,
                    "end": 302
                },
                {
                    "start": 303,
                    "end": 416
                },
                {
                    "start": 417,
                    "end": 498
                },
                {
                    "start": 499,
                    "end": 666
                },
                {
                    "start": 667,
                    "end": 828
                }
            ],
            "ref_mentions": [
                {
                    "start": 64,
                    "end": 68,
                    "matchedPaperCorpusId": "225094578"
                },
                {
                    "start": 661,
                    "end": 665,
                    "matchedPaperCorpusId": "225094578"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.55810546875
        },
        {
            "corpus_id": "278033532",
            "title": "Breaking the Modality Barrier: Universal Embedding Learning with Multimodal LLMs",
            "text": "As large language models increasingly exhibit remarkable proficiency in natural language processing, recent research has pivoted towards harnessing decoder-only architectures for effective representation learning [4,12,26,35,45]. Previous work has adapted the prompt-based representation method for autoregressive models, enabling Large Language Models (LLMs) to perform in-context learning and scale to various model sizes. LLM2Vec [4] transforming pre-trained decoder-only LLMs into versatile text encoders by incorporating three principal advancements: bidirectional attention mechanisms, masked next-token prediction, and unsupervised contrastive alignment. Concurrently, NV-Embed [26] introduces a latent attention layer and eliminates the causal attention mask during contrastive training, substantially enhancing the efficiency of embeddings generated from decoder-only LLMs. While these approaches show promising embedding performance, their exclusive focus on text-only inputs fails to meet the growing demands of multimodal applications.",
            "score": 0.4430100711717086,
            "section_title": "LLMs for Representation Learning",
            "char_start_offset": 7360,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 229
                },
                {
                    "start": 230,
                    "end": 424
                },
                {
                    "start": 425,
                    "end": 661
                },
                {
                    "start": 662,
                    "end": 882
                },
                {
                    "start": 883,
                    "end": 1047
                }
            ],
            "ref_mentions": [
                {
                    "start": 216,
                    "end": 219,
                    "matchedPaperCorpusId": "275466373"
                },
                {
                    "start": 222,
                    "end": 225,
                    "matchedPaperCorpusId": "263908865"
                },
                {
                    "start": 225,
                    "end": 228,
                    "matchedPaperCorpusId": "277271833"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.393798828125
        },
        {
            "corpus_id": "248887685",
            "title": "Transformers as Neural Augmentors: Class Conditional Sentence Generation via Variational Bayes",
            "text": "According to the Equation (1); we desing the Transformer model, introduced in [1], by injecting conditional variational autoencoder between its encoder and decoder. To formalize the idea, given a sequence of N tokens, (t 1 , t 2 , ..., t N ), the encoder of the Transformer model is defined as Encoder : \n\nwhere D is the hidden dimension, and computes the contextual representation c i of each token \n\nIn the original Transformer model, contextual information from encoder flows through to the decoder's cross attention layer by mapping it to key and value. However, recent stateof-the-art language models [23], [24], [25] show that full sentence information can be obtained by only using contextual information from first token t 1 (correspondingly the [CLS] token). In the same way, we only use c 1 token representation. The idea of conditional variational autoencoder starts from vector c 1 . First, the probabilistic encoder p encoder (z | c 1 ) of CVAE encodes sentence representation to a latent vector z with dimension of L. Instead of conditioning only latent vector z, decoder is also conditioned to the class information of our sentence. This conditional objective should be done by interacting latent vector with conditional information. If we pass this class information to the decoder, the reconstruction is conditioned both on the latent vector and class labels. Prior work on conditional variational autoencoder sets an another condition vector [26] and replace entries with condition features [27]. Instead of using two vectors, we replace first entry of latent vector with class integer C to inject class information to decoding process. Denoting replaced vector z as z , decoder is reformulated as \n\nNormally, the decoder of Transformer requires a whole sequence, however we use only vector c 1 as input to VAE's encoder. After decoding in VAE, the reconstructed variable is copied N times to create a sequence like the output of encoder of Transformer. This sequence of vectors is passed to the decoder of Transformer as key and value.",
            "score": 0.4426902636591006,
            "section_title": "B. Modeling Class Conditional Variational Transformer",
            "char_start_offset": 5682,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 164
                },
                {
                    "start": 165,
                    "end": 303
                },
                {
                    "start": 306,
                    "end": 399
                },
                {
                    "start": 402,
                    "end": 557
                },
                {
                    "start": 558,
                    "end": 767
                },
                {
                    "start": 768,
                    "end": 822
                },
                {
                    "start": 823,
                    "end": 895
                },
                {
                    "start": 896,
                    "end": 1147
                },
                {
                    "start": 1148,
                    "end": 1248
                },
                {
                    "start": 1249,
                    "end": 1376
                },
                {
                    "start": 1377,
                    "end": 1514
                },
                {
                    "start": 1515,
                    "end": 1654
                },
                {
                    "start": 1655,
                    "end": 1715
                },
                {
                    "start": 1718,
                    "end": 1839
                },
                {
                    "start": 1840,
                    "end": 1971
                },
                {
                    "start": 1972,
                    "end": 2054
                }
            ],
            "ref_mentions": [
                {
                    "start": 606,
                    "end": 610,
                    "matchedPaperCorpusId": "52967399"
                },
                {
                    "start": 1509,
                    "end": 1513,
                    "matchedPaperCorpusId": "49273733"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.11279296875
        },
        {
            "corpus_id": "272827856",
            "title": "Can Language Model Understand Word Semantics as A Chatbot? An Empirical Study of Language Model Internal External Mismatch",
            "text": "Current common interactions with language models is through full inference. This approach may not necessarily align with the model's internal knowledge. Studies show discrepancies between prompts and internal representations. Most focus on sentence understanding. We study the discrepancy of word semantics understanding in internal and external mismatch across Encoder-only, Decoder-only, and Encoder-Decoder pre-trained language models.",
            "score": 0.4426756651302637,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.21337890625
        },
        {
            "corpus_id": "225094578",
            "title": "Cascaded Encoders for Unifying Streaming and Non-Streaming ASR",
            "text": "either in causal or noncausal mode, depending on the output encoder from which the features are fetched for decoding.\n\nCascaded encoders offer a few advantages over the LAS-based two-pass models. Training such models, typically, is a two-stage process, since the first and the second pass models are trained independently. But cascaded encoders can be trained in a single stage, simplifying the overall process. The model also use a shared RNN-T decoder, which is better suited to handle long-form speech than LAS [16]: LAS-based second pass models do not provide improvements over the first pass model in long-form conditions; it can only operate in rescoring mode when used with long-form audio. Cascaded encoders, on the other hand, can provide gains in all conditions, as we show in Sec. 4. A potential disadvantage of cascaded encoders is that it is more expensive to use the non-causal layers to do rescoring compared to an LAS-based model, because the RNN-T decoder has to marginalize over all possible alignments to compute the posterior of an output sequence that needs rescoring.",
            "score": 0.44266496285571827,
            "section_title": "Cascaded Encoders",
            "char_start_offset": 7298,
            "sentence_offsets": [],
            "ref_mentions": [
                {
                    "start": 514,
                    "end": 518,
                    "matchedPaperCorpusId": "207880479"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.3486328125
        },
        {
            "corpus_id": "261644305",
            "title": "Effects of Generative Chatbots in Higher Education",
            "text": "Generative models (GPT-style) emphasize modelling the joint distribution of inputs and outputs, generating realistic output data that aligns with the training data distribution. GPT-3, Wu Dao, LamDA, YaLM, PaLM, BLOOM, GLM, LLaMA, and GPT-4 are generative LLMs. \n\nUnsupervised pre-training objectives: Based on their approach for generating and reconstructing sequences, LLMs can be classified into three categories: encoder-only, decoderonly, and encoder-decoder language models. \n\nEncoder-only (auto-encoding) models aim to reconstruct the original input sequence from a compressed and abstract representation of that sequence. The encoding process captures contextual information bidirectionally, making auto-encoding models unsuitable for unconditional sequence generation. However, they are useful for tasks like language understanding, feature extraction, and text representation learning. BERT is an example of an encoder-only model. \n\nDecoder-only (autoregressive) models generate output sequences by predicting the next token given the preceding context. These models generate text sequentially, word by word, taking into account the entire history of the sequence generated so far. This approach allows for coherent and contextually appropriate generation but may be slower and less parallelizable due to the sequential nature of the generation process. GPT-3, GPT-4, PaLM, LaMDA, BLOOM, GLM, PanGu, YaLM, and LLaMa are examples of autoregressive models. \n\nAn encoder-decoder (sequence-to-sequence) model treats each task as sequence-tosequence conversion, which can involve text-to-text or even text-to-image or image-to-text generation. Encoder-decoder models are typically used for tasks that require both content understanding and generation, such as machine translation. XLNet is an example of an encoder-decoder model. \n\nModality: LLMs can be categorized into two groups, based on the number of modalities they operate with unimodal and multimodal language models.",
            "score": 0.44259967196263117,
            "section_title": "Large Language Models for NLP and Their Comparison",
            "char_start_offset": 21324,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 177
                },
                {
                    "start": 178,
                    "end": 261
                },
                {
                    "start": 264,
                    "end": 480
                },
                {
                    "start": 483,
                    "end": 629
                },
                {
                    "start": 630,
                    "end": 777
                },
                {
                    "start": 778,
                    "end": 895
                },
                {
                    "start": 896,
                    "end": 940
                },
                {
                    "start": 943,
                    "end": 1063
                },
                {
                    "start": 1064,
                    "end": 1191
                },
                {
                    "start": 1192,
                    "end": 1363
                },
                {
                    "start": 1364,
                    "end": 1464
                },
                {
                    "start": 1467,
                    "end": 1648
                },
                {
                    "start": 1649,
                    "end": 1785
                },
                {
                    "start": 1786,
                    "end": 1834
                },
                {
                    "start": 1837,
                    "end": 1980
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.5009765625
        },
        {
            "corpus_id": "273963063",
            "title": "Mamba-Based Decoder-Only Approach with Bidirectional Speech Modeling for Speech Recognition",
            "text": "Its architecture was similar to the decoder-only model, whereas we allowed non-causal attention for the speech tokens. In addition, we used the relative positional encoding presented in [43], because training of prefixLM failed without the positional encoding. \n\nTraining: The AED models were trained using multi-task learning with the CTC loss [21], where the weight for the CTC loss was 0.3. We performed inference with and without CTC for a fair comparison with the decoder-only models.",
            "score": 0.44221300048845513,
            "section_title": "Results",
            "char_start_offset": 17082,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 118
                },
                {
                    "start": 119,
                    "end": 260
                },
                {
                    "start": 263,
                    "end": 393
                },
                {
                    "start": 394,
                    "end": 489
                }
            ],
            "ref_mentions": [
                {
                    "start": 186,
                    "end": 190,
                    "matchedPaperCorpusId": "204838007"
                },
                {
                    "start": 345,
                    "end": 349,
                    "matchedPaperCorpusId": "2133607"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.1702880859375
        },
        {
            "corpus_id": "263829839",
            "title": "LLM for SoC Security: A Paradigm Shift",
            "text": "The encoder component transforms input tokens into vectors using embeddings and positional encodings, then applies multihead self-attention and feedforward networks. The decoder, starting similarly, incorporates masked self-attention and crossattention with the output of the encoder, ensuring alignment and preventing future word prediction. Sequence-to-sequence models often use this architecture, where the input and output sequences can be of different lengths. These models shine in tasks where there is a direct and complex transformation between inputs and outputs. Examples like machine translation and text summarization are prototypical, as they require the model to understand the input deeply and generate a coherent and contextually accurate output. BART [5], T5 [137], and UL2 [138] are a few well-known encoder-decoder models to be named. \n\nIn Context of SoC Security: The encoder-decoder architecture, renowned for its ability in natural language understanding tasks, exhibits versatility in SoC security. Its twostage process of encoding the input data and then decoding it to produce an output makes it suitable for tasks that require both comprehension and generation. This model is particularly adept at vulnerability mitigation, where understanding the context (encoder) and generating a solution (decoder) are both crucial. However, while it is also a good fit for tasks like vulnerability insertion, security verification, and assessment, it might not always be the optimal choice when the task leans heavily toward either comprehension or generation b) Decoder-Only: Decoder-only LLMs have established impressive benchmarks in numerous NLP tasks, especially in the generation of free-form text. In a decoder-only model, a sequence is fed into the model, which then directly predicts the next token or word in the sequence. It operates autoregressively, using its generated tokens as context for subsequent predictions. It has two variants: causal decoder and prefix decoder. In a standard decoder (causal decoder), the unidirectional attention masking ensures that a token attends only to previous tokens and itself. Prefix decoders permit bidirectional attention over prefix tokens while maintaining unidirectional attention to generated tokens.",
            "score": 0.4421994543458883,
            "section_title": "A. Preliminaries",
            "char_start_offset": 37836,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 165
                },
                {
                    "start": 166,
                    "end": 342
                },
                {
                    "start": 343,
                    "end": 465
                },
                {
                    "start": 466,
                    "end": 572
                },
                {
                    "start": 573,
                    "end": 762
                },
                {
                    "start": 763,
                    "end": 853
                },
                {
                    "start": 856,
                    "end": 1021
                },
                {
                    "start": 1022,
                    "end": 1187
                },
                {
                    "start": 1188,
                    "end": 1345
                },
                {
                    "start": 1346,
                    "end": 1718
                },
                {
                    "start": 1719,
                    "end": 1846
                },
                {
                    "start": 1847,
                    "end": 1942
                },
                {
                    "start": 1943,
                    "end": 1998
                },
                {
                    "start": 1999,
                    "end": 2140
                },
                {
                    "start": 2141,
                    "end": 2270
                }
            ],
            "ref_mentions": [
                {
                    "start": 776,
                    "end": 781,
                    "matchedPaperCorpusId": "204838007"
                },
                {
                    "start": 791,
                    "end": 796,
                    "matchedPaperCorpusId": "252780443"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.79833984375
        },
        {
            "paperId": "de3a0d172c7116336028d1b784f96b9eef5af49b",
            "corpusId": 274464654,
            "title": "AntLM: Bridging Causal and Masked Language Models",
            "venue": "arXiv.org",
            "year": 2024,
            "referenceCount": 34,
            "citationCount": 1,
            "influentialCitationCount": 1,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2412.03275, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2333893528",
                    "name": "Xinru Yu"
                },
                {
                    "authorId": "2334021829",
                    "name": "Bin Guo"
                },
                {
                    "authorId": "2333877832",
                    "name": "Shiwei Luo"
                },
                {
                    "authorId": "2297186132",
                    "name": "Jie Wang"
                },
                {
                    "authorId": "2309500964",
                    "name": "Tao Ji"
                },
                {
                    "authorId": "2309669217",
                    "name": "Yuanbin Wu"
                }
            ],
            "abstract": "Causal Language Modeling (CLM) and Masked Language Modeling (MLM) are two mainstream learning paradigms based on Transformer networks, specifically the Decoder-only and Encoder-only architectures. The strengths of each paradigm in downstream tasks have shown a mix of advantages and disadvantages. In the past BabyLM Challenge 2023, although the MLM paradigm achieved the best average performance, the CLM paradigm demonstrated significantly faster convergence rates. For the BabyLM Challenge 2024, we propose a novel language modeling paradigm named $\\textbf{AntLM}$, which integrates both CLM and MLM to leverage the advantages of these two classic paradigms. We chose the strict-small track and conducted experiments on two foundation models: BabyLlama, representing CLM, and LTG-BERT, representing MLM. During the training process for specific foundation models, we alternate between applying CLM or MLM training objectives and causal or bidirectional attention masks. Experimental results show that combining the two pretraining objectives leverages their strengths, enhancing overall training performance. Under the same epochs, $AntLM_{BabyLlama}$ improves Macro-average by 1%, and $AntLM_{LTG-BERT}$ achieves a 2.2% increase over the baselines.",
            "corpus_id": "274464654",
            "text": "Causal Language Modeling (CLM) and Masked Language Modeling (MLM) are two mainstream learning paradigms based on Transformer networks, specifically the Decoder-only and Encoder-only architectures. The strengths of each paradigm in downstream tasks have shown a mix of advantages and disadvantages. In the past BabyLM Challenge 2023, although the MLM paradigm achieved the best average performance, the CLM paradigm demonstrated significantly faster convergence rates. For the BabyLM Challenge 2024, we propose a novel language modeling paradigm named $\\textbf{AntLM}$, which integrates both CLM and MLM to leverage the advantages of these two classic paradigms. We chose the strict-small track and conducted experiments on two foundation models: BabyLlama, representing CLM, and LTG-BERT, representing MLM. During the training process for specific foundation models, we alternate between applying CLM or MLM training objectives and causal or bidirectional attention masks. Experimental results show that combining the two pretraining objectives leverages their strengths, enhancing overall training performance. Under the same epochs, $AntLM_{BabyLlama}$ improves Macro-average by 1%, and $AntLM_{LTG-BERT}$ achieves a 2.2% increase over the baselines.",
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "score": 0.0,
            "stype": "public_api",
            "pdf_hash": "",
            "rerank_score": 0.2242431640625
        },
        {
            "paperId": "cd7cff2580aeebdc1be895ec4d6b835d77fd0d1a",
            "corpusId": 274656362,
            "title": "Causal Graphical Models for Vision-Language Compositional Understanding",
            "venue": "International Conference on Learning Representations",
            "year": 2024,
            "referenceCount": 89,
            "citationCount": 0,
            "influentialCitationCount": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2412.09353, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2334865476",
                    "name": "Fiorenzo Parascandolo"
                },
                {
                    "authorId": "2202986093",
                    "name": "Nicholas Moratelli"
                },
                {
                    "authorId": "1716310",
                    "name": "E. Sangineto"
                },
                {
                    "authorId": "1843795",
                    "name": "L. Baraldi"
                },
                {
                    "authorId": "2303850502",
                    "name": "Rita Cucchiara"
                }
            ],
            "abstract": "Recent work has empirically shown that Vision-Language Models (VLMs) struggle to fully understand the compositional properties of the human language, usually modeling an image caption as a\"bag of words\". As a result, they perform poorly on compositional tasks, which require a deeper understanding of the different entities of a sentence (subject, verb, etc.) jointly with their mutual relationships in order to be solved. In this paper, we model the dependency relations among textual and visual tokens using a Causal Graphical Model (CGM), built using a dependency parser, and we train a decoder conditioned by the VLM visual encoder. Differently from standard autoregressive or parallel predictions, our decoder's generative process is partially-ordered following the CGM structure. This structure encourages the decoder to learn only the main causal dependencies in a sentence discarding spurious correlations. Using extensive experiments on five compositional benchmarks, we show that our method significantly outperforms all the state-of-the-art compositional approaches by a large margin, and it also improves over methods trained using much larger datasets.",
            "corpus_id": "274656362",
            "text": "Recent work has empirically shown that Vision-Language Models (VLMs) struggle to fully understand the compositional properties of the human language, usually modeling an image caption as a\"bag of words\". As a result, they perform poorly on compositional tasks, which require a deeper understanding of the different entities of a sentence (subject, verb, etc.) jointly with their mutual relationships in order to be solved. In this paper, we model the dependency relations among textual and visual tokens using a Causal Graphical Model (CGM), built using a dependency parser, and we train a decoder conditioned by the VLM visual encoder. Differently from standard autoregressive or parallel predictions, our decoder's generative process is partially-ordered following the CGM structure. This structure encourages the decoder to learn only the main causal dependencies in a sentence discarding spurious correlations. Using extensive experiments on five compositional benchmarks, we show that our method significantly outperforms all the state-of-the-art compositional approaches by a large margin, and it also improves over methods trained using much larger datasets.",
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "score": 0.0,
            "stype": "public_api",
            "pdf_hash": "",
            "rerank_score": 0.1134033203125
        },
        {
            "paperId": "3797b525b8e7f74fd0091b17968e4eefd138401d",
            "corpusId": 276482752,
            "title": "Multiscale Byte Language Models - A Hierarchical Architecture for Causal Million-Length Sequence Modeling",
            "venue": "arXiv.org",
            "year": 2025,
            "referenceCount": 0,
            "citationCount": 0,
            "influentialCitationCount": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2502.14553, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2346326178",
                    "name": "Eric Egli"
                },
                {
                    "authorId": "2287139831",
                    "name": "Matteo Manica"
                },
                {
                    "authorId": "2062641025",
                    "name": "Jannis Born"
                }
            ],
            "abstract": "Bytes form the basis of the digital world and thus are a promising building block for multimodal foundation models. Recently, Byte Language Models (BLMs) have emerged to overcome tokenization, yet the excessive length of bytestreams requires new architectural paradigms. Therefore, we present the Multiscale Byte Language Model (MBLM), a model-agnostic hierarchical decoder stack that allows training with context windows of $5$M bytes on single GPU in full model precision. We thoroughly examine MBLM's performance with Transformer and Mamba blocks on both unimodal and multimodal tasks. Our experiments demonstrate that hybrid architectures are efficient in handling extremely long byte sequences during training while achieving near-linear generational efficiency. To the best of our knowledge, we present the first evaluation of BLMs on visual Q\\&A tasks and find that, despite serializing images and the absence of an encoder, a MBLM with pure next token prediction can match custom CNN-LSTM architectures with designated classification heads. We show that MBLMs exhibit strong adaptability in integrating diverse data representations, including pixel and image filestream bytes, underlining their potential toward omnimodal foundation models. Source code is publicly available at: https://github.com/ai4sd/multiscale-byte-lm",
            "corpus_id": "276482752",
            "text": "Bytes form the basis of the digital world and thus are a promising building block for multimodal foundation models. Recently, Byte Language Models (BLMs) have emerged to overcome tokenization, yet the excessive length of bytestreams requires new architectural paradigms. Therefore, we present the Multiscale Byte Language Model (MBLM), a model-agnostic hierarchical decoder stack that allows training with context windows of $5$M bytes on single GPU in full model precision. We thoroughly examine MBLM's performance with Transformer and Mamba blocks on both unimodal and multimodal tasks. Our experiments demonstrate that hybrid architectures are efficient in handling extremely long byte sequences during training while achieving near-linear generational efficiency. To the best of our knowledge, we present the first evaluation of BLMs on visual Q\\&A tasks and find that, despite serializing images and the absence of an encoder, a MBLM with pure next token prediction can match custom CNN-LSTM architectures with designated classification heads. We show that MBLMs exhibit strong adaptability in integrating diverse data representations, including pixel and image filestream bytes, underlining their potential toward omnimodal foundation models. Source code is publicly available at: https://github.com/ai4sd/multiscale-byte-lm",
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "score": 0.0,
            "stype": "public_api",
            "pdf_hash": "",
            "rerank_score": 0.01242828369140625
        },
        {
            "paperId": "7b0f35b63637a6b308da104ff5065e0fa22090f1",
            "corpusId": 268987613,
            "title": "Dwell in the Beginning: How Language Models Embed Long Documents for Dense Retrieval",
            "venue": "Annual Meeting of the Association for Computational Linguistics",
            "year": 2024,
            "referenceCount": 49,
            "citationCount": 8,
            "influentialCitationCount": 1,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2404.04163, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2295512058",
                    "name": "Joao Coelho"
                },
                {
                    "authorId": "2283771934",
                    "name": "Bruno Martins"
                },
                {
                    "authorId": "2283772665",
                    "name": "Jo\u00e3o Magalh\u00e3es"
                },
                {
                    "authorId": "2310820252",
                    "name": "James P. Callan"
                },
                {
                    "authorId": "2295512254",
                    "name": "Chenyan Xiong"
                }
            ],
            "abstract": "This study investigates the existence of positional biases in Transformer-based models for text representation learning, particularly in the context of web document retrieval. We build on previous research that demonstrated loss of information in the middle of input sequences for causal language models, extending it to the domain of representation learning. We examine positional biases at various stages of training for an encoder-decoder model, including language model pre-training, contrastive pre-training, and contrastive fine-tuning. Experiments with the MS-MARCO document collection reveal that after contrastive pre-training the model already generates embeddings that better capture early contents of the input, with fine-tuning further aggravating this effect.",
            "corpus_id": "268987613",
            "text": "This study investigates the existence of positional biases in Transformer-based models for text representation learning, particularly in the context of web document retrieval. We build on previous research that demonstrated loss of information in the middle of input sequences for causal language models, extending it to the domain of representation learning. We examine positional biases at various stages of training for an encoder-decoder model, including language model pre-training, contrastive pre-training, and contrastive fine-tuning. Experiments with the MS-MARCO document collection reveal that after contrastive pre-training the model already generates embeddings that better capture early contents of the input, with fine-tuning further aggravating this effect.",
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "score": 0.0,
            "stype": "public_api",
            "pdf_hash": "",
            "rerank_score": 0.26953125
        },
        {
            "paperId": "f9bfc6d9ba1665b73af3323d46c7642b852759ef",
            "corpusId": 258832930,
            "title": "VideoLLM: Modeling Video Sequence with Large Language Models",
            "venue": "arXiv.org",
            "year": 2023,
            "referenceCount": 103,
            "citationCount": 84,
            "influentialCitationCount": 2,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "http://arxiv.org/pdf/2305.13292",
                "status": "CLOSED",
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2305.13292, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2155229619",
                    "name": "Guo Chen"
                },
                {
                    "authorId": "1391155455",
                    "name": "Yin-Dong Zheng"
                },
                {
                    "authorId": "2110182417",
                    "name": "Jiahao Wang"
                },
                {
                    "authorId": "3259789",
                    "name": "Jilan Xu"
                },
                {
                    "authorId": "48355651",
                    "name": "Yifei Huang"
                },
                {
                    "authorId": "7588865",
                    "name": "Junting Pan"
                },
                {
                    "authorId": "46393411",
                    "name": "Yi Wang"
                },
                {
                    "authorId": "47903936",
                    "name": "Yali Wang"
                },
                {
                    "authorId": "145858545",
                    "name": "Y. Qiao"
                },
                {
                    "authorId": "2113488744",
                    "name": "Tong Lu"
                },
                {
                    "authorId": "2141353278",
                    "name": "Limin Wang"
                }
            ],
            "abstract": "With the exponential growth of video data, there is an urgent need for automated technology to analyze and comprehend video content. However, existing video understanding models are often task-specific and lack a comprehensive capability of handling diverse tasks. The success of large language models (LLMs) like GPT has demonstrated their impressive abilities in sequence causal reasoning. Building upon this insight, we propose a novel framework called VideoLLM that leverages the sequence reasoning capabilities of pre-trained LLMs from natural language processing (NLP) for video sequence understanding. VideoLLM incorporates a carefully designed Modality Encoder and Semantic Translator, which convert inputs from various modalities into a unified token sequence. This token sequence is then fed into a decoder-only LLM. Subsequently, with the aid of a simple task head, our VideoLLM yields an effective unified framework for different kinds of video understanding tasks. To evaluate the efficacy of VideoLLM, we conduct extensive experiments using multiple LLMs and fine-tuning methods. We evaluate our VideoLLM on eight tasks sourced from four different datasets. The experimental results demonstrate that the understanding and reasoning capabilities of LLMs can be effectively transferred to video understanding tasks. We release the code at https://github.com/cg1177/VideoLLM.",
            "corpus_id": "258832930",
            "text": "With the exponential growth of video data, there is an urgent need for automated technology to analyze and comprehend video content. However, existing video understanding models are often task-specific and lack a comprehensive capability of handling diverse tasks. The success of large language models (LLMs) like GPT has demonstrated their impressive abilities in sequence causal reasoning. Building upon this insight, we propose a novel framework called VideoLLM that leverages the sequence reasoning capabilities of pre-trained LLMs from natural language processing (NLP) for video sequence understanding. VideoLLM incorporates a carefully designed Modality Encoder and Semantic Translator, which convert inputs from various modalities into a unified token sequence. This token sequence is then fed into a decoder-only LLM. Subsequently, with the aid of a simple task head, our VideoLLM yields an effective unified framework for different kinds of video understanding tasks. To evaluate the efficacy of VideoLLM, we conduct extensive experiments using multiple LLMs and fine-tuning methods. We evaluate our VideoLLM on eight tasks sourced from four different datasets. The experimental results demonstrate that the understanding and reasoning capabilities of LLMs can be effectively transferred to video understanding tasks. We release the code at https://github.com/cg1177/VideoLLM.",
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "score": 0.0,
            "stype": "public_api",
            "pdf_hash": "",
            "rerank_score": 0.055511474609375
        },
        {
            "paperId": "ac5e62fc86586950576429fda824b5e1e1661872",
            "corpusId": 278740637,
            "title": "Masking in Multi-hop QA: An Analysis of How Language Models Perform with Context Permutation",
            "venue": "",
            "year": 2025,
            "referenceCount": 31,
            "citationCount": 0,
            "influentialCitationCount": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2505.11754, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "1896870270",
                    "name": "Wenyu Huang"
                },
                {
                    "authorId": "7631872",
                    "name": "P. Vougiouklis"
                },
                {
                    "authorId": "1747893",
                    "name": "Mirella Lapata"
                },
                {
                    "authorId": "2311112356",
                    "name": "Jeff Z. Pan"
                }
            ],
            "abstract": "Multi-hop Question Answering (MHQA) adds layers of complexity to question answering, making it more challenging. When Language Models (LMs) are prompted with multiple search results, they are tasked not only with retrieving relevant information but also employing multi-hop reasoning across the information sources. Although LMs perform well on traditional question-answering tasks, the causal mask can hinder their capacity to reason across complex contexts. In this paper, we explore how LMs respond to multi-hop questions by permuting search results (retrieved documents) under various configurations. Our study reveals interesting findings as follows: 1) Encoder-decoder models, such as the ones in the Flan-T5 family, generally outperform causal decoder-only LMs in MHQA tasks, despite being significantly smaller in size; 2) altering the order of gold documents reveals distinct trends in both Flan T5 models and fine-tuned decoder-only models, with optimal performance observed when the document order aligns with the reasoning chain order; 3) enhancing causal decoder-only models with bi-directional attention by modifying the causal mask can effectively boost their end performance. In addition to the above, we conduct a thorough investigation of the distribution of LM attention weights in the context of MHQA. Our experiments reveal that attention weights tend to peak at higher values when the resulting answer is correct. We leverage this finding to heuristically improve LMs' performance on this task. Our code is publicly available at https://github.com/hwy9855/MultiHopQA-Reasoning.",
            "corpus_id": "278740637",
            "text": "Multi-hop Question Answering (MHQA) adds layers of complexity to question answering, making it more challenging. When Language Models (LMs) are prompted with multiple search results, they are tasked not only with retrieving relevant information but also employing multi-hop reasoning across the information sources. Although LMs perform well on traditional question-answering tasks, the causal mask can hinder their capacity to reason across complex contexts. In this paper, we explore how LMs respond to multi-hop questions by permuting search results (retrieved documents) under various configurations. Our study reveals interesting findings as follows: 1) Encoder-decoder models, such as the ones in the Flan-T5 family, generally outperform causal decoder-only LMs in MHQA tasks, despite being significantly smaller in size; 2) altering the order of gold documents reveals distinct trends in both Flan T5 models and fine-tuned decoder-only models, with optimal performance observed when the document order aligns with the reasoning chain order; 3) enhancing causal decoder-only models with bi-directional attention by modifying the causal mask can effectively boost their end performance. In addition to the above, we conduct a thorough investigation of the distribution of LM attention weights in the context of MHQA. Our experiments reveal that attention weights tend to peak at higher values when the resulting answer is correct. We leverage this finding to heuristically improve LMs' performance on this task. Our code is publicly available at https://github.com/hwy9855/MultiHopQA-Reasoning.",
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "score": 0.0,
            "stype": "public_api",
            "pdf_hash": "",
            "rerank_score": 0.5888671875
        },
        {
            "paperId": "111dfa76cfe55b0210bee5b8c46572f45f21cadf",
            "corpusId": 261196969,
            "title": "State Estimation Over Delayed and Lossy Channels: An Encoder\u2013Decoder Synthesis",
            "venue": "IEEE Transactions on Automatic Control",
            "year": 2024,
            "referenceCount": 41,
            "citationCount": 10,
            "influentialCitationCount": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": "CLOSED",
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.1109/TAC.2023.3308826?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1109/TAC.2023.3308826, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2345515",
                    "name": "T. Soleymani"
                },
                {
                    "authorId": "1737891",
                    "name": "J. Baras"
                },
                {
                    "authorId": "1679177",
                    "name": "K. Johansson"
                }
            ],
            "abstract": "State estimation over a communication channel, in which sensory information of a stochastic source is transmitted in real-time by an encoder to a decoder that estimates the state of the source, is one of the basic problems in networked control systems. In this article, we investigate the performance of state estimation under two primary network imperfections: packet loss and time delay. To that end, we make a causal frequency-distortion tradeoff that is defined between the packet rate and the mean square error, when the source and the channel are modeled by a partially observable Gauss\u2013Markov process and a fixed-delay packet-erasure channel, under two distinct communication protocols: one with and one without packet-loss detection. We prove the existence of a globally optimal policy profile, and show that this policy profile is composed of a symmetric threshold scheduling policy and a non-Gaussian linear estimation policy, which are used by the encoder and the decoder, respectively. Our structural results assert that the scheduling policy is expressible in terms of $\\boldsymbol{\\mathit {3d-1}}$ variables related to the source and the channel, where $\\boldsymbol{\\mathit {d}}$ is the time delay, and that the estimation policy incorporates no residuals related to signaling. The key finding is that packet-loss detection does not increase the performance of the underlying networked system in the sense of the causal frequency-distortion tradeoff. We prove this by showing that the globally optimal policy profile remains exactly the same under both of the communication protocols.",
            "corpus_id": "261196969",
            "text": "State estimation over a communication channel, in which sensory information of a stochastic source is transmitted in real-time by an encoder to a decoder that estimates the state of the source, is one of the basic problems in networked control systems. In this article, we investigate the performance of state estimation under two primary network imperfections: packet loss and time delay. To that end, we make a causal frequency-distortion tradeoff that is defined between the packet rate and the mean square error, when the source and the channel are modeled by a partially observable Gauss\u2013Markov process and a fixed-delay packet-erasure channel, under two distinct communication protocols: one with and one without packet-loss detection. We prove the existence of a globally optimal policy profile, and show that this policy profile is composed of a symmetric threshold scheduling policy and a non-Gaussian linear estimation policy, which are used by the encoder and the decoder, respectively. Our structural results assert that the scheduling policy is expressible in terms of $\\boldsymbol{\\mathit {3d-1}}$ variables related to the source and the channel, where $\\boldsymbol{\\mathit {d}}$ is the time delay, and that the estimation policy incorporates no residuals related to signaling. The key finding is that packet-loss detection does not increase the performance of the underlying networked system in the sense of the causal frequency-distortion tradeoff. We prove this by showing that the globally optimal policy profile remains exactly the same under both of the communication protocols.",
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "score": 0.0,
            "stype": "public_api",
            "pdf_hash": "",
            "rerank_score": 0.077392578125
        },
        {
            "paperId": "3307a0abe669692a717aa91c68b8d68494e02c35",
            "corpusId": 273549474,
            "title": "Future Token Prediction - Causal Language Modelling with Per-Token Semantic State Vector for Multi-Token Prediction",
            "venue": "arXiv.org",
            "year": 2024,
            "referenceCount": 21,
            "citationCount": 0,
            "influentialCitationCount": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2410.18160, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2327336111",
                    "name": "Nicholas Walker"
                }
            ],
            "abstract": "Causal decoder-only transformer models used for generative language modelling, such as Generative Pre-trained Transformers (GPT), are trained to predict the next token in a sequence based only on its previous tokens. Despite this simple training objective, they have proved to be powerful AI tools. However, only predicting the next token results in top layer embedding vectors that are highly token-focused. There may be benefits in generating embedding vectors at each token position that better capture the overall meaning of longer sequences of future text. Recent studies matching brain scans with deep language models suggest that humans also predict upcoming words when listening or reading but consider multiple future tokens rather than just one. This research investigates a new pretraining method called Future Token Prediction (FTP). In FTP, a large transformer encoder generates top layer embedding vectors for each token position, which, instead of being passed to a language head, are linearly and expansively projected to a pseudo-sequence, which is cross attended to by a small transformer decoder to predict the next N tokens forward from that position in the sequence. The top layer embedding vectors from FTP models exhibit distinct properties compared to those from standard GPT models, varying smoothly along a text sequence as measured by cosine similarity between adjacent tokens. Text generated by FTP models show improved topic coherence compared to standard GPT-like models trained with the same prediction perplexity for the next single token. The vectors are shown to better represent the topic of text based on the results of text classification examples. On a toy, but complex, coding problem, FTP networks produce significantly better results than GPT networks.",
            "corpus_id": "273549474",
            "text": "Causal decoder-only transformer models used for generative language modelling, such as Generative Pre-trained Transformers (GPT), are trained to predict the next token in a sequence based only on its previous tokens. Despite this simple training objective, they have proved to be powerful AI tools. However, only predicting the next token results in top layer embedding vectors that are highly token-focused. There may be benefits in generating embedding vectors at each token position that better capture the overall meaning of longer sequences of future text. Recent studies matching brain scans with deep language models suggest that humans also predict upcoming words when listening or reading but consider multiple future tokens rather than just one. This research investigates a new pretraining method called Future Token Prediction (FTP). In FTP, a large transformer encoder generates top layer embedding vectors for each token position, which, instead of being passed to a language head, are linearly and expansively projected to a pseudo-sequence, which is cross attended to by a small transformer decoder to predict the next N tokens forward from that position in the sequence. The top layer embedding vectors from FTP models exhibit distinct properties compared to those from standard GPT models, varying smoothly along a text sequence as measured by cosine similarity between adjacent tokens. Text generated by FTP models show improved topic coherence compared to standard GPT-like models trained with the same prediction perplexity for the next single token. The vectors are shown to better represent the topic of text based on the results of text classification examples. On a toy, but complex, coding problem, FTP networks produce significantly better results than GPT networks.",
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "score": 0.0,
            "stype": "public_api",
            "pdf_hash": "",
            "rerank_score": 0.2900390625
        },
        {
            "paperId": "2635c1aeee582dacb865f00d1289b443c3d96d02",
            "corpusId": 273186710,
            "title": "Mitigating Modality Prior-Induced Hallucinations in Multimodal Large Language Models via Deciphering Attention Causality",
            "venue": "arXiv.org",
            "year": 2024,
            "referenceCount": 41,
            "citationCount": 12,
            "influentialCitationCount": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2410.04780, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2325943818",
                    "name": "Guanyu Zhou"
                },
                {
                    "authorId": "2262511952",
                    "name": "Yibo Yan"
                },
                {
                    "authorId": "2316449100",
                    "name": "Xin Zou"
                },
                {
                    "authorId": "2324823188",
                    "name": "Kun Wang"
                },
                {
                    "authorId": "10017193",
                    "name": "Aiwei Liu"
                },
                {
                    "authorId": "2307036168",
                    "name": "Xuming Hu"
                }
            ],
            "abstract": "Multimodal Large Language Models (MLLMs) have emerged as a central focus in both industry and academia, but often suffer from biases introduced by visual and language priors, which can lead to multimodal hallucination. These biases arise from the visual encoder and the Large Language Model (LLM) backbone, affecting the attention mechanism responsible for aligning multimodal inputs. Existing decoding-based mitigation methods focus on statistical correlations and overlook the causal relationships between attention mechanisms and model output, limiting their effectiveness in addressing these biases. To tackle this issue, we propose a causal inference framework termed CausalMM that applies structural causal modeling to MLLMs, treating modality priors as a confounder between attention mechanisms and output. Specifically, by employing backdoor adjustment and counterfactual reasoning at both the visual and language attention levels, our method mitigates the negative effects of modality priors and enhances the alignment of MLLM's inputs and outputs, with a maximum score improvement of 65.3% on 6 VLind-Bench indicators and 164 points on MME Benchmark compared to conventional methods. Extensive experiments validate the effectiveness of our approach while being a plug-and-play solution. Our code is available at: https://github.com/The-Martyr/CausalMM",
            "corpus_id": "273186710",
            "text": "Multimodal Large Language Models (MLLMs) have emerged as a central focus in both industry and academia, but often suffer from biases introduced by visual and language priors, which can lead to multimodal hallucination. These biases arise from the visual encoder and the Large Language Model (LLM) backbone, affecting the attention mechanism responsible for aligning multimodal inputs. Existing decoding-based mitigation methods focus on statistical correlations and overlook the causal relationships between attention mechanisms and model output, limiting their effectiveness in addressing these biases. To tackle this issue, we propose a causal inference framework termed CausalMM that applies structural causal modeling to MLLMs, treating modality priors as a confounder between attention mechanisms and output. Specifically, by employing backdoor adjustment and counterfactual reasoning at both the visual and language attention levels, our method mitigates the negative effects of modality priors and enhances the alignment of MLLM's inputs and outputs, with a maximum score improvement of 65.3% on 6 VLind-Bench indicators and 164 points on MME Benchmark compared to conventional methods. Extensive experiments validate the effectiveness of our approach while being a plug-and-play solution. Our code is available at: https://github.com/The-Martyr/CausalMM",
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "score": 0.0,
            "stype": "public_api",
            "pdf_hash": "",
            "rerank_score": 0.0260009765625
        },
        {
            "paperId": "780e4de0b3a5e620e3875a54c9a27a070dd186be",
            "corpusId": 267523128,
            "title": "StableMask: Refining Causal Masking in Decoder-only Transformer",
            "venue": "International Conference on Machine Learning",
            "year": 2024,
            "referenceCount": 0,
            "citationCount": 12,
            "influentialCitationCount": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2402.04779, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2281644597",
                    "name": "Qingyu Yin"
                },
                {
                    "authorId": "2283262102",
                    "name": "Xuzheng He"
                },
                {
                    "authorId": "2231289043",
                    "name": "Zhuang Xiang"
                },
                {
                    "authorId": "2283195201",
                    "name": "Yu Zhao"
                },
                {
                    "authorId": "2238917409",
                    "name": "Jianhua Yao"
                },
                {
                    "authorId": "2283453441",
                    "name": "Xiaoyu Shen"
                },
                {
                    "authorId": "2254277130",
                    "name": "Qiang Zhang"
                }
            ],
            "abstract": "The decoder-only Transformer architecture with causal masking and relative position encoding (RPE) has become the de facto choice in language modeling. Despite its exceptional performance across various tasks, we have identified two limitations: First, it requires all attention scores to be non-zero and sum up to 1, even if the current embedding has sufficient self-contained information. This compels the model to assign disproportional excessive attention to specific tokens. Second, RPE-based Transformers are not universal approximators due to their limited capacity at encoding absolute positional information, which limits their application in position-critical tasks. In this work, we propose StableMask: a parameter-free method to address both limitations by refining the causal mask. It introduces pseudo-attention values to balance attention distributions and encodes absolute positional information via a progressively decreasing mask ratio. StableMask's effectiveness is validated both theoretically and empirically, showing significant enhancements in language models with parameter sizes ranging from 71M to 1.4B across diverse datasets and encoding methods. We further show that it naturally supports (1) efficient extrapolation without special tricks such as StreamingLLM and (2) easy integration with existing attention optimization techniques.",
            "corpus_id": "267523128",
            "text": "The decoder-only Transformer architecture with causal masking and relative position encoding (RPE) has become the de facto choice in language modeling. Despite its exceptional performance across various tasks, we have identified two limitations: First, it requires all attention scores to be non-zero and sum up to 1, even if the current embedding has sufficient self-contained information. This compels the model to assign disproportional excessive attention to specific tokens. Second, RPE-based Transformers are not universal approximators due to their limited capacity at encoding absolute positional information, which limits their application in position-critical tasks. In this work, we propose StableMask: a parameter-free method to address both limitations by refining the causal mask. It introduces pseudo-attention values to balance attention distributions and encodes absolute positional information via a progressively decreasing mask ratio. StableMask's effectiveness is validated both theoretically and empirically, showing significant enhancements in language models with parameter sizes ranging from 71M to 1.4B across diverse datasets and encoding methods. We further show that it naturally supports (1) efficient extrapolation without special tricks such as StreamingLLM and (2) easy integration with existing attention optimization techniques.",
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "score": 0.0,
            "stype": "public_api",
            "pdf_hash": "",
            "rerank_score": 0.139892578125
        },
        {
            "paperId": "a4f374a96e9d265a03e2e2cdd7e204e1e4e37416",
            "corpusId": 274762932,
            "title": "CosyVoice 2: Scalable Streaming Speech Synthesis with Large Language Models",
            "venue": "arXiv.org",
            "year": 2024,
            "referenceCount": 57,
            "citationCount": 48,
            "influentialCitationCount": 8,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2412.10117, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2325844336",
                    "name": "Zhihao Du"
                },
                {
                    "authorId": "2329305566",
                    "name": "Yuxuan Wang"
                },
                {
                    "authorId": "2277244282",
                    "name": "Qian Chen"
                },
                {
                    "authorId": "2239092198",
                    "name": "Xian Shi"
                },
                {
                    "authorId": "2154217273",
                    "name": "Xiang Lv"
                },
                {
                    "authorId": "2327844317",
                    "name": "Tianyu Zhao"
                },
                {
                    "authorId": "51259415",
                    "name": "Zhifu Gao"
                },
                {
                    "authorId": "2310385662",
                    "name": "Yexin Yang"
                },
                {
                    "authorId": "2310895561",
                    "name": "Changfeng Gao"
                },
                {
                    "authorId": "2335481933",
                    "name": "Hui Wang"
                },
                {
                    "authorId": "2239054821",
                    "name": "Fan Yu"
                },
                {
                    "authorId": "2166335547",
                    "name": "Huadai Liu"
                },
                {
                    "authorId": "2335447207",
                    "name": "Zhengyan Sheng"
                },
                {
                    "authorId": "2310862011",
                    "name": "Yue Gu"
                },
                {
                    "authorId": "2057947796",
                    "name": "Chong Deng"
                },
                {
                    "authorId": "2316242114",
                    "name": "Wen Wang"
                },
                {
                    "authorId": "2241056111",
                    "name": "Shiliang Zhang"
                },
                {
                    "authorId": "2143626826",
                    "name": "Zhijie Yan"
                },
                {
                    "authorId": "2331310825",
                    "name": "Jing-Ru Zhou"
                }
            ],
            "abstract": "In our previous work, we introduced CosyVoice, a multilingual speech synthesis model based on supervised discrete speech tokens. By employing progressive semantic decoding with two popular generative models, language models (LMs) and Flow Matching, CosyVoice demonstrated high prosody naturalness, content consistency, and speaker similarity in speech in-context learning. Recently, significant progress has been made in multi-modal large language models (LLMs), where the response latency and real-time factor of speech synthesis play a crucial role in the interactive experience. Therefore, in this report, we present an improved streaming speech synthesis model, CosyVoice 2, which incorporates comprehensive and systematic optimizations. Specifically, we introduce finite-scalar quantization to improve the codebook utilization of speech tokens. For the text-speech LM, we streamline the model architecture to allow direct use of a pre-trained LLM as the backbone. In addition, we develop a chunk-aware causal flow matching model to support various synthesis scenarios, enabling both streaming and non-streaming synthesis within a single model. By training on a large-scale multilingual dataset, CosyVoice 2 achieves human-parity naturalness, minimal response latency, and virtually lossless synthesis quality in the streaming mode. We invite readers to listen to the demos at https://funaudiollm.github.io/cosyvoice2.",
            "corpus_id": "274762932",
            "text": "In our previous work, we introduced CosyVoice, a multilingual speech synthesis model based on supervised discrete speech tokens. By employing progressive semantic decoding with two popular generative models, language models (LMs) and Flow Matching, CosyVoice demonstrated high prosody naturalness, content consistency, and speaker similarity in speech in-context learning. Recently, significant progress has been made in multi-modal large language models (LLMs), where the response latency and real-time factor of speech synthesis play a crucial role in the interactive experience. Therefore, in this report, we present an improved streaming speech synthesis model, CosyVoice 2, which incorporates comprehensive and systematic optimizations. Specifically, we introduce finite-scalar quantization to improve the codebook utilization of speech tokens. For the text-speech LM, we streamline the model architecture to allow direct use of a pre-trained LLM as the backbone. In addition, we develop a chunk-aware causal flow matching model to support various synthesis scenarios, enabling both streaming and non-streaming synthesis within a single model. By training on a large-scale multilingual dataset, CosyVoice 2 achieves human-parity naturalness, minimal response latency, and virtually lossless synthesis quality in the streaming mode. We invite readers to listen to the demos at https://funaudiollm.github.io/cosyvoice2.",
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "score": 0.0,
            "stype": "public_api",
            "pdf_hash": "",
            "rerank_score": 0.0161590576171875
        },
        {
            "paperId": "9f0128d21e2efb4f3e680583452a14699644efaa",
            "corpusId": 271881684,
            "title": "TooT-PLM-P2S: Incorporating Secondary Structure Information into Protein Language Models",
            "venue": "bioRxiv",
            "year": 2024,
            "referenceCount": 0,
            "citationCount": 0,
            "influentialCitationCount": 0,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://www.biorxiv.org/content/biorxiv/early/2024/08/13/2024.08.13.607781.full.pdf",
                "status": "GREEN",
                "license": "CCBY",
                "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.1101/2024.08.13.607781?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1101/2024.08.13.607781, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "52171658",
                    "name": "Hamed Ghazikhani"
                },
                {
                    "authorId": "2316206126",
                    "name": "Gregory Butler"
                }
            ],
            "abstract": "In bioinformatics, modeling the protein space to better predict function and structure has benefitted from Protein Language Models (PLMs). Their basis is the protein\u2019s amino acid sequence and self-supervised learning. Ankh is a prime example of such a PLM. While there has been some recent work on integrating structure with a PLM to enhance predictive performance, to date there has been no work on integrating secondary structure rather than three-dimensional structure. Here we present TooT-PLM-P2S that begins with the Ankh model pre-trained on 45 million proteins using self-supervised learning. TooT-PLM-P2S builds upon the Ankh model by initially using its pre-trained encoder and decoder. It then undergoes an additional training phase with approximately 10,000 proteins and their corresponding secondary structures. This retraining process modifies the encoder and decoder, resulting in the creation of TooT-PLM-P2S. We then assess the impact of integrating secondary structure information into the Ankh model by comparing Ankh and TooT-PLM-P2S on eight downstream tasks including fluorescence and solubility prediction, sub-cellular localization, and membrane protein classification. For both Ankh and TooT-PLM-P2S the downstream tasks required task-specific training. Few of the results showed statistically significant differences. Ankh outperformed on three of the eight tasks, TooT-PLM-P2S did not outperform on any task for the primary metric. TooT-PLM-P2S did outperform for the precision metric for the task of discriminating membrane proteins from non-membrane proteins. This study requires future work with expanded datasets and refined integration methods.",
            "corpus_id": "271881684",
            "text": "In bioinformatics, modeling the protein space to better predict function and structure has benefitted from Protein Language Models (PLMs). Their basis is the protein\u2019s amino acid sequence and self-supervised learning. Ankh is a prime example of such a PLM. While there has been some recent work on integrating structure with a PLM to enhance predictive performance, to date there has been no work on integrating secondary structure rather than three-dimensional structure. Here we present TooT-PLM-P2S that begins with the Ankh model pre-trained on 45 million proteins using self-supervised learning. TooT-PLM-P2S builds upon the Ankh model by initially using its pre-trained encoder and decoder. It then undergoes an additional training phase with approximately 10,000 proteins and their corresponding secondary structures. This retraining process modifies the encoder and decoder, resulting in the creation of TooT-PLM-P2S. We then assess the impact of integrating secondary structure information into the Ankh model by comparing Ankh and TooT-PLM-P2S on eight downstream tasks including fluorescence and solubility prediction, sub-cellular localization, and membrane protein classification. For both Ankh and TooT-PLM-P2S the downstream tasks required task-specific training. Few of the results showed statistically significant differences. Ankh outperformed on three of the eight tasks, TooT-PLM-P2S did not outperform on any task for the primary metric. TooT-PLM-P2S did outperform for the precision metric for the task of discriminating membrane proteins from non-membrane proteins. This study requires future work with expanded datasets and refined integration methods.",
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "score": 0.0,
            "stype": "public_api",
            "pdf_hash": "",
            "rerank_score": 0.042572021484375
        },
        {
            "paperId": "34e880e8a655d4c7bb75951a01ba1945acc288af",
            "corpusId": 272683927,
            "title": "A Comparison of Tokenization Impact in Attention Based and State Space Genomic Language Models",
            "venue": "bioRxiv",
            "year": 2024,
            "referenceCount": 43,
            "citationCount": 1,
            "influentialCitationCount": 0,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://doi.org/10.1101/2024.09.09.612081",
                "status": "GREEN",
                "license": "CCBYNCND",
                "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.1101/2024.09.09.612081?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1101/2024.09.09.612081, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2261142643",
                    "name": "LeAnn Lindsey"
                },
                {
                    "authorId": "4292373",
                    "name": "N. L. Pershing"
                },
                {
                    "authorId": "2299183021",
                    "name": "Anisa Habib"
                },
                {
                    "authorId": "2321354453",
                    "name": "W. Z. Stephens"
                },
                {
                    "authorId": "4549022",
                    "name": "A. Blaschke"
                },
                {
                    "authorId": "2261141461",
                    "name": "Hari Sundar"
                }
            ],
            "abstract": "Genomic language models have recently emerged as powerful tools to decode and interpret genetic sequences. Existing genomic language models have utilized various tokenization methods including character tokenization, overlapping and non-overlapping k-mer tokenization, and byte-pair encoding, a method widely used in natural language models. Genomic models have significant differences from natural language and protein language models because of their low character variability, complex and overlapping features, and inconsistent directionality. These differences make sub-word tokenization in genomic language models significantly different from traditional language models. This study explores the impact of tokenization in attention-based and state-space genomic language models by evaluating their downstream performance on various fine-tuning tasks. We propose new definitions for fertility, the token per word ratio, in the context of genomic language models, and introduce to-kenization parity, which measures how consistently a tokenizer parses homologous sequences. We also perform an ablation study on the state-space model, Mamba, to evaluate the impact of character-based tokenization compared to byte-pair encoding. Our results indicate that the choice of tokenizer significantly impacts model performance and that when experiments control for input sequence length, character tokenization is the best choice in state-space models for all evaluated task categories except epigenetic mark prediction.",
            "corpus_id": "272683927",
            "text": "Genomic language models have recently emerged as powerful tools to decode and interpret genetic sequences. Existing genomic language models have utilized various tokenization methods including character tokenization, overlapping and non-overlapping k-mer tokenization, and byte-pair encoding, a method widely used in natural language models. Genomic models have significant differences from natural language and protein language models because of their low character variability, complex and overlapping features, and inconsistent directionality. These differences make sub-word tokenization in genomic language models significantly different from traditional language models. This study explores the impact of tokenization in attention-based and state-space genomic language models by evaluating their downstream performance on various fine-tuning tasks. We propose new definitions for fertility, the token per word ratio, in the context of genomic language models, and introduce to-kenization parity, which measures how consistently a tokenizer parses homologous sequences. We also perform an ablation study on the state-space model, Mamba, to evaluate the impact of character-based tokenization compared to byte-pair encoding. Our results indicate that the choice of tokenizer significantly impacts model performance and that when experiments control for input sequence length, character tokenization is the best choice in state-space models for all evaluated task categories except epigenetic mark prediction.",
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "score": 0.0,
            "stype": "public_api",
            "pdf_hash": "",
            "rerank_score": 0.007038116455078125
        },
        {
            "paperId": "4f9b3e117fe712cf62be8eac474b43f934cf374b",
            "corpusId": 259316671,
            "title": "Conformer LLMs - Convolution Augmented Large Language Models",
            "venue": "arXiv.org",
            "year": 2023,
            "referenceCount": 29,
            "citationCount": 1,
            "influentialCitationCount": 0,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "http://arxiv.org/pdf/2307.00461",
                "status": "CLOSED",
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2307.00461, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "50882405",
                    "name": "Prateek Verma"
                }
            ],
            "abstract": "This work builds together two popular blocks of neural architecture, namely convolutional layers and Transformers, for large language models (LLMs). Non-causal conformers are used ubiquitously in automatic speech recognition. This work aims to adapt these architectures in a causal setup for training LLMs. Transformers decoders effectively capture long-range dependencies over several modalities and form a core backbone of modern advancements in machine learning. Convolutional architectures have been popular in extracting features in domains such as raw 1-D signals, speech, and images, to name a few. In this paper, by combining local and global dependencies over latent representations using causal convolutional filters and Transformer, we achieve significant gains in performance. This work showcases a robust speech architecture that can be integrated and adapted in a causal setup beyond speech applications for large-scale language modeling.",
            "corpus_id": "259316671",
            "text": "This work builds together two popular blocks of neural architecture, namely convolutional layers and Transformers, for large language models (LLMs). Non-causal conformers are used ubiquitously in automatic speech recognition. This work aims to adapt these architectures in a causal setup for training LLMs. Transformers decoders effectively capture long-range dependencies over several modalities and form a core backbone of modern advancements in machine learning. Convolutional architectures have been popular in extracting features in domains such as raw 1-D signals, speech, and images, to name a few. In this paper, by combining local and global dependencies over latent representations using causal convolutional filters and Transformer, we achieve significant gains in performance. This work showcases a robust speech architecture that can be integrated and adapted in a causal setup beyond speech applications for large-scale language modeling.",
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "score": 0.0,
            "stype": "public_api",
            "pdf_hash": "",
            "rerank_score": 0.1033935546875
        },
        {
            "paperId": "944165a43458f98e00f6a322ff8aefd08ed0fd64",
            "corpusId": 278595740,
            "title": "Token-Mol 1.0: tokenized drug design with large language models",
            "venue": "Nature Communications",
            "year": 2025,
            "referenceCount": 71,
            "citationCount": 1,
            "influentialCitationCount": 0,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://doi.org/10.1038/s41467-025-59628-y",
                "status": "GOLD",
                "license": "CCBYNCND",
                "disclaimer": "Notice: Paper or abstract available at https://pmc.ncbi.nlm.nih.gov/articles/PMC12075800, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2237735396",
                    "name": "Jike Wang"
                },
                {
                    "authorId": "2310701527",
                    "name": "Rui Qin"
                },
                {
                    "authorId": "2142970644",
                    "name": "Mingyang Wang"
                },
                {
                    "authorId": "2310703435",
                    "name": "Meijing Fang"
                },
                {
                    "authorId": "2189515733",
                    "name": "Yangyang Zhang"
                },
                {
                    "authorId": "2307563764",
                    "name": "Yuchen Zhu"
                },
                {
                    "authorId": "2072402373",
                    "name": "Qun Su"
                },
                {
                    "authorId": "2310700922",
                    "name": "Qiaolin Gou"
                },
                {
                    "authorId": "2275714426",
                    "name": "Chao Shen"
                },
                {
                    "authorId": "2220403357",
                    "name": "Odin Zhang"
                },
                {
                    "authorId": "2037330967",
                    "name": "Zhenxing Wu"
                },
                {
                    "authorId": "2086994527",
                    "name": "Dejun Jiang"
                },
                {
                    "authorId": "2045701981",
                    "name": "Xujun Zhang"
                },
                {
                    "authorId": "2258696885",
                    "name": "Huifeng Zhao"
                },
                {
                    "authorId": "2300944193",
                    "name": "Jingxuan Ge"
                },
                {
                    "authorId": "2310779796",
                    "name": "Zhourui Wu"
                },
                {
                    "authorId": "2320730031",
                    "name": "Yu Kang"
                },
                {
                    "authorId": "2257984296",
                    "name": "Chang-Yu Hsieh"
                },
                {
                    "authorId": "2329895453",
                    "name": "Tingjun Hou"
                }
            ],
            "abstract": "The integration of large language models (LLMs) into drug design is gaining momentum; however, existing approaches often struggle to effectively incorporate three-dimensional molecular structures. Here, we present Token-Mol, a token-only 3D drug design model that encodes both 2D and 3D structural information, along with molecular properties, into discrete tokens. Built on a transformer decoder and trained with causal masking, Token-Mol introduces a Gaussian cross-entropy loss function tailored for regression tasks, enabling superior performance across multiple downstream applications. The model surpasses existing methods, improving molecular conformation generation by over 10% and 20% across two datasets, while outperforming token-only models by 30% in property prediction. In pocket-based molecular generation, it enhances drug-likeness and synthetic accessibility by approximately 11% and 14%, respectively. Notably, Token-Mol operates 35 times faster than expert diffusion models. In real-world validation, it improves success rates and, when combined with reinforcement learning, further optimizes affinity and drug-likeness, advancing AI-driven drug discovery.",
            "corpus_id": "278595740",
            "text": "The integration of large language models (LLMs) into drug design is gaining momentum; however, existing approaches often struggle to effectively incorporate three-dimensional molecular structures. Here, we present Token-Mol, a token-only 3D drug design model that encodes both 2D and 3D structural information, along with molecular properties, into discrete tokens. Built on a transformer decoder and trained with causal masking, Token-Mol introduces a Gaussian cross-entropy loss function tailored for regression tasks, enabling superior performance across multiple downstream applications. The model surpasses existing methods, improving molecular conformation generation by over 10% and 20% across two datasets, while outperforming token-only models by 30% in property prediction. In pocket-based molecular generation, it enhances drug-likeness and synthetic accessibility by approximately 11% and 14%, respectively. Notably, Token-Mol operates 35 times faster than expert diffusion models. In real-world validation, it improves success rates and, when combined with reinforcement learning, further optimizes affinity and drug-likeness, advancing AI-driven drug discovery.",
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "score": 0.0,
            "stype": "public_api",
            "pdf_hash": "",
            "rerank_score": 0.03289794921875
        },
        {
            "paperId": "fd38e1f3c5fa7e770c327180fd68bf8bbfc33a70",
            "corpusId": 258556987,
            "title": "On the Usage of Continual Learning for Out-of-Distribution Generalization in Pre-trained Language Models of Code",
            "venue": "ESEC/SIGSOFT FSE",
            "year": 2023,
            "referenceCount": 77,
            "citationCount": 10,
            "influentialCitationCount": 0,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://ink.library.smu.edu.sg/context/sis_research/article/9577/viewcontent/On_the_Usage_of_Continual_Learning_for_Out_of_Distribution_Generalization_in_Pre_trained_Language_Models_of_Code.pdf",
                "status": "GREEN",
                "license": "CCBYNCND",
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2305.04106, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "1820831988",
                    "name": "M. Weyssow"
                },
                {
                    "authorId": "2148928671",
                    "name": "Xin Zhou"
                },
                {
                    "authorId": "35276441",
                    "name": "Kisub Kim"
                },
                {
                    "authorId": "2150912791",
                    "name": "David Lo"
                },
                {
                    "authorId": "9460712",
                    "name": "H. Sahraoui"
                }
            ],
            "abstract": "Pre-trained language models (PLMs) have become a prevalent technique in deep learning for code, utilizing a two-stage pre-training and fine-tuning procedure to acquire general knowledge about code and specialize in a variety of downstream tasks. However, the dynamic nature of software codebases poses a challenge to the effectiveness and robustness of PLMs. In particular, world-realistic scenarios potentially lead to significant differences between the distribution of the pre-training and test data, i.e., distribution shift, resulting in a degradation of the PLM's performance on downstream tasks. In this paper, we stress the need for adapting PLMs of code to software data whose distribution changes over time, a crucial problem that has been overlooked in previous works. The motivation of this work is to consider the PLM in a non-stationary environment, where fine-tuning data evolves over time according to a software evolution scenario. Specifically, we design a scenario where the model needs to learn from a stream of programs containing new, unseen APIs over time. We study two widely used PLM architectures, i.e., a GPT2 decoder and a RoBERTa encoder, on two downstream tasks, API call and API usage prediction. We demonstrate that the most commonly used fine-tuning technique from prior work is not robust enough to handle the dynamic nature of APIs, leading to the loss of previously acquired knowledge i.e., catastrophic forgetting. To address these issues, we implement five continual learning approaches, including replay-based and regularization-based methods. Our findings demonstrate that utilizing these straightforward methods effectively mitigates catastrophic forgetting in PLMs across both downstream tasks while achieving comparable or superior performance.",
            "corpus_id": "258556987",
            "text": "Pre-trained language models (PLMs) have become a prevalent technique in deep learning for code, utilizing a two-stage pre-training and fine-tuning procedure to acquire general knowledge about code and specialize in a variety of downstream tasks. However, the dynamic nature of software codebases poses a challenge to the effectiveness and robustness of PLMs. In particular, world-realistic scenarios potentially lead to significant differences between the distribution of the pre-training and test data, i.e., distribution shift, resulting in a degradation of the PLM's performance on downstream tasks. In this paper, we stress the need for adapting PLMs of code to software data whose distribution changes over time, a crucial problem that has been overlooked in previous works. The motivation of this work is to consider the PLM in a non-stationary environment, where fine-tuning data evolves over time according to a software evolution scenario. Specifically, we design a scenario where the model needs to learn from a stream of programs containing new, unseen APIs over time. We study two widely used PLM architectures, i.e., a GPT2 decoder and a RoBERTa encoder, on two downstream tasks, API call and API usage prediction. We demonstrate that the most commonly used fine-tuning technique from prior work is not robust enough to handle the dynamic nature of APIs, leading to the loss of previously acquired knowledge i.e., catastrophic forgetting. To address these issues, we implement five continual learning approaches, including replay-based and regularization-based methods. Our findings demonstrate that utilizing these straightforward methods effectively mitigates catastrophic forgetting in PLMs across both downstream tasks while achieving comparable or superior performance.",
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "score": 0.0,
            "stype": "public_api",
            "pdf_hash": "",
            "rerank_score": 0.017303466796875
        }
    ],
    "quotes": {
        "cost": 0.2667569999999999,
        "quotes": [
            {
                "idx": 0,
                "key": "[221702858 | Tay et al. | 2020 | Citations: 1128]",
                "snippets": "In encoder mode, there is no restriction or constraint that the self-attention mechanism has to be causal, i.e., dependent solely on the present and past tokens. In the encoder-decoder setting, self-attention used in the decoder (i.e. across decoding positions) must be causal since each auto-regressive decoding step can only depend on previous tokens, whereas the selfattention used in the encoder need not. Fulfilling this requirement can prove challenging for many efficient self-attention designs.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {},
                "metadata": [
                    {
                        "quote": "In encoder mode, there is no restriction or constraint that the self-attention mechanism has to be causal, i.e., dependent solely on the present and past tokens. In the encoder-decoder setting, self-attention used in the decoder (i.e. across decoding positions) must be causal since each auto-regressive decoding step can only depend on previous tokens, whereas the selfattention used in the encoder need not. Fulfilling this requirement can prove challenging for many efficient self-attention designs.",
                        "pdf_hash": ""
                    }
                ]
            },
            {
                "idx": 1,
                "key": "[247625205 | Scao et al. | 2022 | Citations: 109]",
                "snippets": "Alternatives include encoder-decoder models trained with a span-corruption objective (e.g., T5 Raffel et al. (2019)), as well as non-causal decoders models with visibility over a prefix (so-called Prefix LMs, (Liu et al., 2018); (Dong et al., 2019))...Following autoregressive pretraining, decoder-only models can be efficiently adapted into non-causal decoders, simply by extending pretraining with span corruption. This adaptation produces a second model, which can provide excellent zero-shot generalization after multitask finetuning.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {
                    "[147704286 | Dong et al. | 2019 | Citations: 1560]": "This paper presents a new Unified pre-trained Language Model (UniLM) that can be fine-tuned for both natural language understanding and generation tasks. The model is pre-trained using three types of language modeling tasks: unidirectional, bidirectional, and sequence-to-sequence prediction. The unified modeling is achieved by employing a shared Transformer network and utilizing specific self-attention masks to control what context the prediction conditions on. UniLM compares favorably with BERT on the GLUE benchmark, and the SQuAD 2.0 and CoQA question answering tasks. Moreover, UniLM achieves new state-of-the-art results on five natural language generation datasets, including improving the CNN/DailyMail abstractive summarization ROUGE-L to 40.51 (2.04 absolute improvement), the Gigaword abstractive summarization ROUGE-L to 35.75 (0.86 absolute improvement), the CoQA generative question answering F1 score to 82.5 (37.1 absolute improvement), the SQuAD question generation BLEU-4 to 22.12 (3.75 absolute improvement), and the DSTC7 document-grounded dialog response generation NIST-4 to 2.67 (human performance is 2.65). The code and pre-trained models are available at this https URL.",
                    "[3608234 | Liu et al. | 2018 | Citations: 801]": "We show that generating English Wikipedia articles can be approached as a multi- document summarization of source documents. We use extractive summarization to coarsely identify salient information and a neural abstractive model to generate the article. For the abstractive model, we introduce a decoder-only architecture that can scalably attend to very long sequences, much longer than typical encoder- decoder architectures used in sequence transduction. We show that this model can generate fluent, coherent multi-sentence paragraphs and even whole Wikipedia articles. When given reference documents, we show it can extract relevant factual information as reflected in perplexity, ROUGE scores and human evaluations."
                },
                "metadata": [
                    {
                        "section_title": "Architecture and Pretraining Objective",
                        "pdf_hash": "",
                        "start": 347,
                        "end": 594,
                        "sentence_offsets": [
                            {
                                "start": 347,
                                "end": 595
                            }
                        ],
                        "ref_mentions": [
                            "3608234",
                            "147704286"
                        ],
                        "quote": "Alternatives include encoder-decoder models trained with a span-corruption objective (e.g., T5 Raffel et al. (2019)), as well as non-causal decoders models with visibility over a prefix (so-called Prefix LMs, (Liu et al., 2018); (Dong et al., 2019))"
                    },
                    {
                        "section_title": "Architecture and Pretraining Objective",
                        "pdf_hash": "",
                        "start": 1070,
                        "end": 1357,
                        "sentence_offsets": [
                            {
                                "start": 1070,
                                "end": 1234
                            },
                            {
                                "start": 1235,
                                "end": 1356
                            }
                        ],
                        "ref_mentions": [],
                        "quote": "Following autoregressive pretraining, decoder-only models can be efficiently adapted into non-causal decoders, simply by extending pretraining with span corruption. This adaptation produces a second model, which can provide excellent zero-shot generalization after multitask finetuning."
                    }
                ]
            },
            {
                "idx": 2,
                "key": "[248118752 | Wang et al. | 2022 | Citations: 175]",
                "snippets": "Causal decoder-only. Although the encoder-decoder is the original Transformer variant, most recent LLMs use a decoder-only architecture. These models can be trained as a traditional language model (i.e. to predict the next token in a sequence). Decoder-only models have no independent means of processing or representing the input sequence and target sequence differently-all tokens are processed in an equivalent fashion, and, because of the causal masking pattern, conditioning is simply based on past tokens (see Figure 2, on the left). On the one hand, this means that the representation for any conditioning text is inherently weaker; on the other hand, it yields a simpler architecture that is naturally suited to a standard autoregressive next-step-prediction pretraining objective. We refer to this architecture as causal decoder-only (CD) .\n\nNon-causal decoder-only. To allow decoder-only models to build richer non-causal representations of the input/conditioning text, it has been proposed to simply modify the attention mask used. Specifically, the self-attention masking pattern can be changed so that the region of the input sequence corresponding to conditioning information has a non-causal mask (i.e. attention in this region is not restricted to past tokens, see middle of Figure 2), as in the encoder of an encoder-decoder architecture. We refer to this architecture as non-causal decoder-only (ND) .\n\nComparisons across architectures. Decoder-only models process a single sequence consisting of the concatenation of the input and target text. On the other hand, in an encoder-decoder, the encoder processes only the input and the decoder processes only the target.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {},
                "metadata": [
                    {
                        "quote": "Causal decoder-only. Although the encoder-decoder is the original Transformer variant, most recent LLMs use a decoder-only architecture. These models can be trained as a traditional language model (i.e. to predict the next token in a sequence). Decoder-only models have no independent means of processing or representing the input sequence and target sequence differently-all tokens are processed in an equivalent fashion, and, because of the causal masking pattern, conditioning is simply based on past tokens (see Figure 2, on the left). On the one hand, this means that the representation for any conditioning text is inherently weaker; on the other hand, it yields a simpler architecture that is naturally suited to a standard autoregressive next-step-prediction pretraining objective. We refer to this architecture as causal decoder-only (CD) .\n\nNon-causal decoder-only. To allow decoder-only models to build richer non-causal representations of the input/conditioning text, it has been proposed to simply modify the attention mask used. Specifically, the self-attention masking pattern can be changed so that the region of the input sequence corresponding to conditioning information has a non-causal mask (i.e. attention in this region is not restricted to past tokens, see middle of Figure 2), as in the encoder of an encoder-decoder architecture. We refer to this architecture as non-causal decoder-only (ND) .\n\nComparisons across architectures. Decoder-only models process a single sequence consisting of the concatenation of the input and target text. On the other hand, in an encoder-decoder, the encoder processes only the input and the decoder processes only the target.",
                        "pdf_hash": ""
                    }
                ]
            },
            {
                "idx": 3,
                "key": "[248157514 | Ding et al. | 2022 | Citations: 17]",
                "snippets": "The original cascaded encoder model (Narayanan et al., 2020) uses a shared RNN-T decoder. The decoder works with a causal encoder in the first pass to provide streaming recognition results, and works with an additional non-causal encoder that sits on top of the causal encoder to provide more accurate final results, leveraging audio right context extracted by the noncausal encoder.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {
                    "[225094578 | Narayanan et al. | 2020 | Citations: 86]": "End-to-end (E2E) automatic speech recognition (ASR) models, by now, have shown competitive performance on several benchmarks. These models are structured to either operate in streaming or non-streaming mode. This work presents cascaded encoders for building a single E2E ASR model that can operate in both these modes simultaneously. The proposed model consists of streaming and non-streaming encoders. Input features are first processed by the streaming encoder; the non-streaming encoder operates exclusively on the output of the streaming encoder. A single decoder then learns to decode either using the output of the streaming or the non-streaming encoder. Results show that this model achieves similar word error rates (WER) as a standalone streaming model when operating in streaming mode, and obtains 10% \u2013 27% relative improvement when operating in non-streaming mode. Our results also show that the proposed approach outperforms existing E2E two-pass models, especially on long-form speech."
                },
                "metadata": [
                    {
                        "section_title": "Separate decoders",
                        "pdf_hash": "",
                        "start": 0,
                        "end": 363,
                        "sentence_offsets": [
                            {
                                "start": 0,
                                "end": 69
                            },
                            {
                                "start": 70,
                                "end": 363
                            }
                        ],
                        "ref_mentions": [
                            "225094578"
                        ],
                        "quote": "The original cascaded encoder model (Narayanan et al., 2020) uses a shared RNN-T decoder. The decoder works with a causal encoder in the first pass to provide streaming recognition results, and works with an additional non-causal encoder that sits on top of the causal encoder to provide more accurate final results, leveraging audio right context extracted by the noncausal encoder."
                    }
                ]
            },
            {
                "idx": 4,
                "key": "[248266379 | Xiao et al. | 2022 | Citations: 87]",
                "snippets": "(1) Specifically, AT models need to prevent earlier decoding steps from peeking at information from later steps. Therefore, the constraint of an autoregressive factorization of the output distribution is required, and they adopt the strict causal mask by applying a lower triangular matrix in the self-attention module of the conventional Transformer decoder (Vaswani et al., 2017). (2) However, for NAT models, including the iteration-based NAT models, this constraint is no longer necessary, so they adopt the unmasked self-attention over all target tokens [16].",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {
                    "[13756489 | Vaswani et al. | 2017 | Citations: 132444]": "The dominant sequence transduction models are based on complex recurrent or convolutional neural networks in an encoder-decoder configuration. The best performing models also connect the encoder and decoder through an attention mechanism. We propose a new simple network architecture, the Transformer, based solely on attention mechanisms, dispensing with recurrence and convolutions entirely. Experiments on two machine translation tasks show these models to be superior in quality while being more parallelizable and requiring significantly less time to train. Our model achieves 28.4 BLEU on the WMT 2014 English-to-German translation task, improving over the existing best results, including ensembles by over 2 BLEU. On the WMT 2014 English-to-French translation task, our model establishes a new single-model state-of-the-art BLEU score of 41.8 after training for 3.5 days on eight GPUs, a small fraction of the training costs of the best models from the literature. We show that the Transformer generalizes well to other tasks by applying it successfully to English constituency parsing both with large and limited training data."
                },
                "metadata": [
                    {
                        "section_title": "Comparison",
                        "pdf_hash": "",
                        "start": 639,
                        "end": 1185,
                        "sentence_offsets": [
                            {
                                "start": 639,
                                "end": 751
                            },
                            {
                                "start": 752,
                                "end": 1003
                            },
                            {
                                "start": 1004,
                                "end": 1185
                            }
                        ],
                        "ref_mentions": [
                            "13756489"
                        ],
                        "quote": "(1) Specifically, AT models need to prevent earlier decoding steps from peeking at information from later steps. Therefore, the constraint of an autoregressive factorization of the output distribution is required, and they adopt the strict causal mask by applying a lower triangular matrix in the self-attention module of the conventional Transformer decoder (Vaswani et al., 2017). (2) However, for NAT models, including the iteration-based NAT models, this constraint is no longer necessary, so they adopt the unmasked self-attention over all target tokens [16]."
                    }
                ]
            },
            {
                "idx": 5,
                "key": "[252780443 | Tay et al. | 2022 | Citations: 313]",
                "snippets": "The line between decoder-only and encoder-decoder models is less clear. PrefixLM models are almost encoder-decoder models with shared parameters (but not quite). From an inductive bias point of view, there are multiple differences. Encoder-Decoder models process input and targets independently with a different set of parameters. This is a form of sparsity where different set of parameters are used for different tokens. Encoder-Decoder models also have a cross attention component that connects input tokens to target tokens. Meanwhile, decoder-only models process inputs and targets by concatenating them. Hence, the representations of inputs and targets are concurrently build layer by layer as the input/targets propagate up the network. Conversely, the decoder in Encoder-decoder models generally only looks at the fully processed encoder input. Overall, the inductive bias of PrefixLM decoder-only models and Encoder-Decoder models could be pretty similar modulo the subtle differences stated above. The distinct property is that Encoder-Decoder models are generally approximately 2x parameters of a decoder-only model when compute-matched.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {},
                "metadata": [
                    {
                        "section_title": "Decoder-only vs Encoder-Decoder",
                        "pdf_hash": "",
                        "start": 0,
                        "end": 1148,
                        "sentence_offsets": [
                            {
                                "start": 0,
                                "end": 71
                            },
                            {
                                "start": 72,
                                "end": 161
                            },
                            {
                                "start": 162,
                                "end": 231
                            },
                            {
                                "start": 232,
                                "end": 330
                            },
                            {
                                "start": 331,
                                "end": 422
                            },
                            {
                                "start": 423,
                                "end": 528
                            },
                            {
                                "start": 529,
                                "end": 609
                            },
                            {
                                "start": 610,
                                "end": 743
                            },
                            {
                                "start": 744,
                                "end": 852
                            },
                            {
                                "start": 853,
                                "end": 1007
                            },
                            {
                                "start": 1008,
                                "end": 1148
                            }
                        ],
                        "ref_mentions": [],
                        "quote": "The line between decoder-only and encoder-decoder models is less clear. PrefixLM models are almost encoder-decoder models with shared parameters (but not quite). From an inductive bias point of view, there are multiple differences. Encoder-Decoder models process input and targets independently with a different set of parameters. This is a form of sparsity where different set of parameters are used for different tokens. Encoder-Decoder models also have a cross attention component that connects input tokens to target tokens. Meanwhile, decoder-only models process inputs and targets by concatenating them. Hence, the representations of inputs and targets are concurrently build layer by layer as the input/targets propagate up the network. Conversely, the decoder in Encoder-decoder models generally only looks at the fully processed encoder input. Overall, the inductive bias of PrefixLM decoder-only models and Encoder-Decoder models could be pretty similar modulo the subtle differences stated above. The distinct property is that Encoder-Decoder models are generally approximately 2x parameters of a decoder-only model when compute-matched."
                    }
                ]
            },
            {
                "idx": 6,
                "key": "[253420279 | Scao et al. | 2022 | Citations: 2393]",
                "snippets": "Notably, while the original Transformer is based on an encoder-decoder architecture, many popular models have opted for encoder-only (e.g. BERT, (Devlin et al., 2019)) or decoder-only (e.g. GPT, (Radford et al., 2018)) approaches. Currently, all state-of-the-art language models over 100 billion parameters are causal decoder-only models Rae et al., 2021;Chowdhery et al., 2022). This is in opposition to the findings of (Raffel et al., 2019), in which encoderdecoder models significantly outperform decoder-only models for transfer learning.\n\nPrior to our work, the literature was lacking a systematic evaluation of the zero-shot generalization capabilities of different architectures and pretraining objectives. We explored this question in (Wang et al., 2022) where we evaluated encoder-decoder and decoder-only architectures and their interactions with causal, prefix, and masked language modeling pretraining objectives. Our results show that immediately after pretraining, causal decoderonly models performed best -validating the choice of state-of-the-art LLMs.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {
                    "[248118752 | Wang et al. | 2022 | Citations: 175]": "Large pretrained Transformer language models have been shown to exhibit zero-shot generalization, i.e. they can perform a wide variety of tasks that they were not explicitly trained on. However, the architectures and pretraining objectives used across state-of-the-art models differ significantly, and there has been limited systematic comparison of these factors. In this work, we present a large-scale evaluation of modeling choices and their impact on zero-shot generalization. In particular, we focus on text-to-text models and experiment with three model architectures (causal/non-causal decoder-only and encoder-decoder), trained with two different pretraining objectives (autoregressive and masked language modeling), and evaluated with and without multitask prompted finetuning. We train models with over 5 billion parameters for more than 170 billion tokens, thereby increasing the likelihood that our conclusions will transfer to even larger scales. Our experiments show that causal decoder-only models trained on an autoregressive language modeling objective exhibit the strongest zero-shot generalization after purely unsupervised pretraining. However, models with non-causal visibility on their input trained with a masked language modeling objective followed by multitask finetuning perform the best among our experiments. We therefore consider the adaptation of pretrained models across architectures and objectives. We find that pretrained non-causal decoder models can be adapted into performant generative causal decoder models, using autoregressive language modeling as a downstream task. Furthermore, we find that pretrained causal decoder models can be efficiently adapted into non-causal decoder models, ultimately achieving competitive performance after multitask finetuning. Code and checkpoints are available at https://github.com/bigscience-workshop/architecture-objective.",
                    "[204838007 | Raffel et al. | 2019 | Citations: 20336]": "Transfer learning, where a model is first pre-trained on a data-rich task before being fine-tuned on a downstream task, has emerged as a powerful technique in natural language processing (NLP). The effectiveness of transfer learning has given rise to a diversity of approaches, methodology, and practice. In this paper, we explore the landscape of transfer learning techniques for NLP by introducing a unified framework that converts every language problem into a text-to-text format. Our systematic study compares pre-training objectives, architectures, unlabeled datasets, transfer approaches, and other factors on dozens of language understanding tasks. By combining the insights from our exploration with scale and our new \"Colossal Clean Crawled Corpus\", we achieve state-of-the-art results on many benchmarks covering summarization, question answering, text classification, and more. To facilitate future work on transfer learning for NLP, we release our dataset, pre-trained models, and code.",
                    "[52967399 | Devlin et al. | 2019 | Citations: 95215]": "We introduce a new language representation model called BERT, which stands for Bidirectional Encoder Representations from Transformers. Unlike recent language representation models (Peters et al., 2018a; Radford et al., 2018), BERT is designed to pre-train deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers. As a result, the pre-trained BERT model can be fine-tuned with just one additional output layer to create state-of-the-art models for a wide range of tasks, such as question answering and language inference, without substantial task-specific architecture modifications. BERT is conceptually simple and empirically powerful. It obtains new state-of-the-art results on eleven natural language processing tasks, including pushing the GLUE score to 80.5 (7.7 point absolute improvement), MultiNLI accuracy to 86.7% (4.6% absolute improvement), SQuAD v1.1 question answering Test F1 to 93.2 (1.5 point absolute improvement) and SQuAD v2.0 Test F1 to 83.1 (5.1 point absolute improvement)."
                },
                "metadata": [
                    {
                        "section_title": "Architecture and Pretraining Objective",
                        "pdf_hash": "",
                        "start": 152,
                        "end": 1219,
                        "sentence_offsets": [],
                        "ref_mentions": [
                            "52967399",
                            "204838007",
                            "248118752"
                        ],
                        "quote": "Notably, while the original Transformer is based on an encoder-decoder architecture, many popular models have opted for encoder-only (e.g. BERT, (Devlin et al., 2019)) or decoder-only (e.g. GPT, (Radford et al., 2018)) approaches. Currently, all state-of-the-art language models over 100 billion parameters are causal decoder-only models Rae et al., 2021;Chowdhery et al., 2022). This is in opposition to the findings of (Raffel et al., 2019), in which encoderdecoder models significantly outperform decoder-only models for transfer learning.\n\nPrior to our work, the literature was lacking a systematic evaluation of the zero-shot generalization capabilities of different architectures and pretraining objectives. We explored this question in (Wang et al., 2022) where we evaluated encoder-decoder and decoder-only architectures and their interactions with causal, prefix, and masked language modeling pretraining objectives. Our results show that immediately after pretraining, causal decoderonly models performed best -validating the choice of state-of-the-art LLMs."
                    }
                ]
            },
            {
                "idx": 7,
                "key": "[257050658 | Liu et al. | 2023 | Citations: 4]",
                "snippets": "For the modification of the self-attention mask, because the decoder input is the copied sequence of encoder input, the self-attention module is allowed to attend all positions, rather than only left positions in the conventional Transformer decoder. Therefore, the self-attention mask is replaced with a non-causal mask in our non-autoregressive decoder.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {},
                "metadata": [
                    {
                        "section_title": "Encoder and Decoder",
                        "pdf_hash": "",
                        "start": 493,
                        "end": 848,
                        "sentence_offsets": [
                            {
                                "start": 493,
                                "end": 743
                            },
                            {
                                "start": 744,
                                "end": 848
                            }
                        ],
                        "ref_mentions": [],
                        "quote": "For the modification of the self-attention mask, because the decoder input is the copied sequence of encoder input, the self-attention module is allowed to attend all positions, rather than only left positions in the conventional Transformer decoder. Therefore, the self-attention mask is replaced with a non-causal mask in our non-autoregressive decoder."
                    }
                ]
            },
            {
                "idx": 8,
                "key": "[257496129 | He et al. | 2023 | Citations: 20]",
                "snippets": "Transformer-based language models can be categorized into three types: encoder-only, decoder-only, and encoderdecoder models. \n\nEncoder-only models exclusively leverage the encoder stacks of the vanilla transformer [48] architecture. BERT [10] stands as a prominent encoder-only representation model, which learns a bidirectional contextual representation of text.\n\nIn contrast, Decoder-only models consist solely of the decoder components of the original transformer architecture. A notable instance of such models is the GPT [35], GPT operates under a causal language modeling (CLM) framework during its training phase. CLM is a strategy where the model predicts the next token in a sequence while only considering preceding tokens. In other words, this design restricts the model from accessing future tokens in the sequence. \n\nBridging the above approaches, textitEncoder-decoder models integrate both the encoder and decoder components of the transformer architecture. Popular encoder-decoder models involve T5 [37] and BART [23].",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {},
                "metadata": [
                    {
                        "quote": "Transformer-based language models can be categorized into three types: encoder-only, decoder-only, and encoderdecoder models. \n\nEncoder-only models exclusively leverage the encoder stacks of the vanilla transformer [48] architecture. BERT [10] stands as a prominent encoder-only representation model, which learns a bidirectional contextual representation of text.\n\nIn contrast, Decoder-only models consist solely of the decoder components of the original transformer architecture. A notable instance of such models is the GPT [35], GPT operates under a causal language modeling (CLM) framework during its training phase. CLM is a strategy where the model predicts the next token in a sequence while only considering preceding tokens. In other words, this design restricts the model from accessing future tokens in the sequence. \n\nBridging the above approaches, textitEncoder-decoder models integrate both the encoder and decoder components of the transformer architecture. Popular encoder-decoder models involve T5 [37] and BART [23].",
                        "pdf_hash": ""
                    }
                ]
            },
            {
                "idx": 9,
                "key": "[258049081 | Fu et al. | 2023 | Citations: 43]",
                "snippets": "Traditionally, most of the seq2seq task is resolved by the Encoder-Decoder framework which requires an encoder to encode the source sequence and a decoder to generate the target text. Recently, a bunch of new approaches have emerged that apply decoder-only language models directly to the seq2seq task. Despite the significant advancements in applying language models to the seq2seq task, there is still a lack of thorough analysis on the effectiveness of the decoder-only language model architecture.\n\nThe main difference between the ED framework and the LM is how the input source information is merged into the decoder. As illustrated in Figure 2, the ED framework first uses multiple Transformer blocks to extract features H E \u22121 from the source sequence s. Afterwards, it utilizes a self attention ATT D l to get the feature matrix G D l . It then uses an encoder attention ATT J l to take G D l as query and uses the encoder's final output H E \u22121 as the key and value to calculate Q D l . On the other hand, an LM uses an unidirectional attention to handle the concatenated features.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {},
                "metadata": [
                    {
                        "quote": "Traditionally, most of the seq2seq task is resolved by the Encoder-Decoder framework which requires an encoder to encode the source sequence and a decoder to generate the target text. Recently, a bunch of new approaches have emerged that apply decoder-only language models directly to the seq2seq task. Despite the significant advancements in applying language models to the seq2seq task, there is still a lack of thorough analysis on the effectiveness of the decoder-only language model architecture.\n\nThe main difference between the ED framework and the LM is how the input source information is merged into the decoder. As illustrated in Figure 2, the ED framework first uses multiple Transformer blocks to extract features H E \u22121 from the source sequence s. Afterwards, it utilizes a self attention ATT D l to get the feature matrix G D l . It then uses an encoder attention ATT J l to take G D l as query and uses the encoder's final output H E \u22121 as the key and value to calculate Q D l . On the other hand, an LM uses an unidirectional attention to handle the concatenated features.",
                        "pdf_hash": ""
                    }
                ]
            },
            {
                "idx": 10,
                "key": "[258461112 | Jain et al. | 2022 | Citations: 16]",
                "snippets": "Compared to the causal (left-to-right) attention mechanism of the decoder-only models, the bidirectional attention mechanism in both encoder-only and encoder-decoder models allows for better leverage of the context of the sequence and hence leads to better representations.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {},
                "metadata": [
                    {
                        "section_title": "D.1 Bridge the Gap on Discriminative Tasks",
                        "pdf_hash": "",
                        "start": 0,
                        "end": 273,
                        "sentence_offsets": [
                            {
                                "start": 0,
                                "end": 273
                            }
                        ],
                        "ref_mentions": [],
                        "quote": "Compared to the causal (left-to-right) attention mechanism of the decoder-only models, the bidirectional attention mechanism in both encoder-only and encoder-decoder models allows for better leverage of the context of the sequence and hence leads to better representations."
                    }
                ]
            },
            {
                "idx": 11,
                "key": "[258947629 | Roberts | 2023 | Citations: 19]",
                "snippets": "In this article we prove that the general transformer neural model undergirding modern large language models (LLMs) is Turing complete under reasonable assumptions...Differentiating Encoder-only and Decoder-only Models: Decoder-only models have three necessary characteristics which are derived from their function in the vanilla transformer. The decoder must (1) provide a means of autoregressively predicting the next token based on the tokens generated so far given the encoder input as contextualization. In 2 this is shown as the recursive red connection mapping the output vector back into the last element of the input sequence of vectors. To be suited to this task, decoder-only models must (2) not see future values when evaluating a query on the input sequence of vectors. This is why decoder-only models are often referred to as causal language models (CLM). In 2, we refer to the decoder attention heads as causal attention heads rather than masked attention heads as they are called in (Vaswani et al., 2017). The model must be (3) trained to predict the next token given the current input sequence of vectors. This training method coupled with recursion allows decoder-only models to autoregressively generate arbitrarily long (up to the max size of the input vector sequence) sequences.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {
                    "[13756489 | Vaswani et al. | 2017 | Citations: 132444]": "The dominant sequence transduction models are based on complex recurrent or convolutional neural networks in an encoder-decoder configuration. The best performing models also connect the encoder and decoder through an attention mechanism. We propose a new simple network architecture, the Transformer, based solely on attention mechanisms, dispensing with recurrence and convolutions entirely. Experiments on two machine translation tasks show these models to be superior in quality while being more parallelizable and requiring significantly less time to train. Our model achieves 28.4 BLEU on the WMT 2014 English-to-German translation task, improving over the existing best results, including ensembles by over 2 BLEU. On the WMT 2014 English-to-French translation task, our model establishes a new single-model state-of-the-art BLEU score of 41.8 after training for 3.5 days on eight GPUs, a small fraction of the training costs of the best models from the literature. We show that the Transformer generalizes well to other tasks by applying it successfully to English constituency parsing both with large and limited training data."
                },
                "metadata": [
                    {
                        "quote": "In this article we prove that the general transformer neural model undergirding modern large language models (LLMs) is Turing complete under reasonable assumptions",
                        "pdf_hash": "",
                        "section_title": "abstract"
                    },
                    {
                        "section_title": "1) Modifying the Vanilla Transformer to form a Decoderonly Model:",
                        "pdf_hash": "",
                        "start": 524,
                        "end": 1641,
                        "sentence_offsets": [
                            {
                                "start": 521,
                                "end": 700
                            },
                            {
                                "start": 701,
                                "end": 866
                            },
                            {
                                "start": 867,
                                "end": 1004
                            },
                            {
                                "start": 1005,
                                "end": 1140
                            },
                            {
                                "start": 1141,
                                "end": 1227
                            },
                            {
                                "start": 1228,
                                "end": 1361
                            },
                            {
                                "start": 1362,
                                "end": 1462
                            },
                            {
                                "start": 1463,
                                "end": 1640
                            }
                        ],
                        "ref_mentions": [
                            "13756489"
                        ],
                        "quote": "Differentiating Encoder-only and Decoder-only Models: Decoder-only models have three necessary characteristics which are derived from their function in the vanilla transformer. The decoder must (1) provide a means of autoregressively predicting the next token based on the tokens generated so far given the encoder input as contextualization. In 2 this is shown as the recursive red connection mapping the output vector back into the last element of the input sequence of vectors. To be suited to this task, decoder-only models must (2) not see future values when evaluating a query on the input sequence of vectors. This is why decoder-only models are often referred to as causal language models (CLM). In 2, we refer to the decoder attention heads as causal attention heads rather than masked attention heads as they are called in (Vaswani et al., 2017). The model must be (3) trained to predict the next token given the current input sequence of vectors. This training method coupled with recursion allows decoder-only models to autoregressively generate arbitrarily long (up to the max size of the input vector sequence) sequences."
                    }
                ]
            },
            {
                "idx": 12,
                "key": "[260886785 | Kim et al. | 2023 | Citations: 15]",
                "snippets": "Cumulative Quantization Errors in Causal Attention. Causal attention, which integrates masking into self-attention to avoid referencing future tokens, is vital for text generation tasks. To comprehend the quantization characteristics of GLMs, we contrast the computation procedure of the self-attention map. For a Transformer encoder, the quantization error reflected on the self-attention map due to Q, K, and V computation is uniformly spread because of the simultaneous computing nature inherent in Transformer encoders. However, the mask in the causal attention accumulates quantization errors from each token, creating uneven errors in the output computation. \n\nTo elucidate, consider the encoder self-attention depicted at the top of Fig. 1(a). During the computation of the weighted sum of value representations for tokens 1 and 3, all attention probabilities in the self-attention map impacted by quantization errors are incorporated. On the other hand, in the decoder's causal attention, as illustrated at the bottom of Fig. 1(a), the output value representation is computed using only the self-attention probabilities of the current token and those before it. For example, while generating the output value representation for token 1 (highlighted in a blue box), only two attention probabilities affected by quantization error are used. In contrast, for token 3 (indicated by the red box), probabilities from all preceding tokens are incorporated. This observation underscores that the inherent design of causal attention results in an accumulation of quantization errors, especially towards the latter tokens. Given this characteristic, it becomes imperative to devise a decoder QAT approach that mitigates this uneven distribution of quantization errors within the causal attention mechanism. \n\nNecessity of Ground Truth Loss. In fine-tuning processes, encoder-only models, often used in Natural Language Understanding (NLU), and decoder-only models, common in Natural Language Generation (NLG) with causal language modeling approach, exhibit different mechanisms for receiving ground truth loss. Fig. 1(b) illustrates the differences. In NLU tasks, encoder models leverage the representation of a unique special token, transformed into a logit vector, for calculating cross-entropy loss, with the number of classes usually being a few [12]. Conversely, decoder models in NLG tasks predict the subsequent token for each input token, transforming each token representation into a logit vector with a class size equivalent to the vocabulary size, often exceeding 50k [25]. Hence, while encoder models gain ground truth loss from a single special token's logit, decoder models derive it from the output logits of all input tokens, which could provide more detailed token-level prediction information.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {},
                "metadata": [
                    {
                        "quote": "Cumulative Quantization Errors in Causal Attention. Causal attention, which integrates masking into self-attention to avoid referencing future tokens, is vital for text generation tasks. To comprehend the quantization characteristics of GLMs, we contrast the computation procedure of the self-attention map. For a Transformer encoder, the quantization error reflected on the self-attention map due to Q, K, and V computation is uniformly spread because of the simultaneous computing nature inherent in Transformer encoders. However, the mask in the causal attention accumulates quantization errors from each token, creating uneven errors in the output computation. \n\nTo elucidate, consider the encoder self-attention depicted at the top of Fig. 1(a). During the computation of the weighted sum of value representations for tokens 1 and 3, all attention probabilities in the self-attention map impacted by quantization errors are incorporated. On the other hand, in the decoder's causal attention, as illustrated at the bottom of Fig. 1(a), the output value representation is computed using only the self-attention probabilities of the current token and those before it. For example, while generating the output value representation for token 1 (highlighted in a blue box), only two attention probabilities affected by quantization error are used. In contrast, for token 3 (indicated by the red box), probabilities from all preceding tokens are incorporated. This observation underscores that the inherent design of causal attention results in an accumulation of quantization errors, especially towards the latter tokens. Given this characteristic, it becomes imperative to devise a decoder QAT approach that mitigates this uneven distribution of quantization errors within the causal attention mechanism. \n\nNecessity of Ground Truth Loss. In fine-tuning processes, encoder-only models, often used in Natural Language Understanding (NLU), and decoder-only models, common in Natural Language Generation (NLG) with causal language modeling approach, exhibit different mechanisms for receiving ground truth loss. Fig. 1(b) illustrates the differences. In NLU tasks, encoder models leverage the representation of a unique special token, transformed into a logit vector, for calculating cross-entropy loss, with the number of classes usually being a few [12]. Conversely, decoder models in NLG tasks predict the subsequent token for each input token, transforming each token representation into a logit vector with a class size equivalent to the vocabulary size, often exceeding 50k [25]. Hence, while encoder models gain ground truth loss from a single special token's logit, decoder models derive it from the output logits of all input tokens, which could provide more detailed token-level prediction information.",
                        "pdf_hash": ""
                    }
                ]
            },
            {
                "idx": 13,
                "key": "[261064777 | Zheng et al. | 2023 | Citations: 76]",
                "snippets": "Encoder-decoder Architecture: The vanilla Transformer proposed in (Vaswani et al., 2017) is based on encoder-decoder architecture, which comprises separate encoder and decoder components. The encoder processes the input sequence and captures its latent representation, which is then used by the decoder to generate the output sequence autoregressively. This architecture is well-suited for tasks involving sequence-to-sequence mapping, such as machine translation, text summarization, and dialogue generation.\n\nCausal Decoder Architecture: The causal decoder architecture is commonly implemented as a stack of decoder layers. It utilizes a diagonal mask matrix, allowing each token to only have access to information from preceding tokens. This constraint ensures a unidirectional and autoregressive generation process.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {},
                "metadata": [
                    {
                        "quote": "Encoder-decoder Architecture: The vanilla Transformer proposed in (Vaswani et al., 2017) is based on encoder-decoder architecture, which comprises separate encoder and decoder components. The encoder processes the input sequence and captures its latent representation, which is then used by the decoder to generate the output sequence autoregressively. This architecture is well-suited for tasks involving sequence-to-sequence mapping, such as machine translation, text summarization, and dialogue generation.\n\nCausal Decoder Architecture: The causal decoder architecture is commonly implemented as a stack of decoder layers. It utilizes a diagonal mask matrix, allowing each token to only have access to information from preceding tokens. This constraint ensures a unidirectional and autoregressive generation process.",
                        "pdf_hash": ""
                    }
                ]
            },
            {
                "idx": 14,
                "key": "[261101164 | Kuang et al. | 2023 | Citations: 4]",
                "snippets": "The structure of the decoder is similar to the text encoder, and the difference is that it replaces the bidirectional self-attention layers with causal self-attention layers.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {},
                "metadata": [
                    {
                        "section_title": "Model",
                        "pdf_hash": "",
                        "start": 621,
                        "end": 795,
                        "sentence_offsets": [
                            {
                                "start": 621,
                                "end": 795
                            }
                        ],
                        "ref_mentions": [],
                        "quote": "The structure of the decoder is similar to the text encoder, and the difference is that it replaces the bidirectional self-attention layers with causal self-attention layers."
                    }
                ]
            },
            {
                "idx": 15,
                "key": "[261494010 | Wei et al. | 2023 | Citations: 100]",
                "snippets": "Encoder-only models use only the encoder component by training using Masked Language Modeling (MLM) [14] objective where a small percentage (e.g., 15%) of the tokens are masked on. The goal of MLM is to recover these masked tokens given the surrounding context. Encoder-only models such as CodeBERT [15] and GraphCodeBERT [22] are designed to provide a representation of the input code to be used for downstream tasks such as code classification [72]. Decoder-only models, on the other hand, aim to autoregressively generate tokens based on all previously generated tokens. CodeGEN [51,52], Codex [10] and PolyCoder [71] are examples of decoder-only LLMs where they can be used for code autocompletion tasks. Different from encoder-and decoder-only LLMs, encoder-decoder models (e.g., CodeT5 [62,63] and PLBART [3]) combine both encoder and decoder together and jointly train both components together. A commonly used pre-training objective for encoder-decoder models is Masked Span Prediction (MSP) where random spans (multiple consecutive tokens) are replaced with single masked tokens and the models learn to fill in the masked span with the correct sequence of tokens. Furthermore, decoder-only models like InCoder [17] can also perform infilling by training through causal language modeling [2] objective.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {
                    "[221761146 | Guo et al. | 2020 | Citations: 1146]": "Pre-trained models for programming language have achieved dramatic empirical improvements on a variety of code-related tasks such as code search, code completion, code summarization, etc. However, existing pre-trained models regard a code snippet as a sequence of tokens, while ignoring the inherent structure of code, which provides crucial code semantics and would enhance the code understanding process. We present GraphCodeBERT, a pre-trained model for programming language that considers the inherent structure of code. Instead of taking syntactic-level structure of code like abstract syntax tree (AST), we use data flow in the pre-training stage, which is a semantic-level structure of code that encodes the relation of \"where-the-value-comes-from\" between variables. Such a semantic-level structure is neat and does not bring an unnecessarily deep hierarchy of AST, the property of which makes the model more efficient. We develop GraphCodeBERT based on Transformer. In addition to using the task of masked language modeling, we introduce two structure-aware pre-training tasks. One is to predict code structure edges, and the other is to align representations between source code and code structure. We implement the model in an efficient way with a graph-guided masked attention function to incorporate the code structure. We evaluate our model on four tasks, including code search, clone detection, code translation, and code refinement. Results show that code structure and newly introduced pre-training tasks can improve GraphCodeBERT and achieves state-of-the-art performance on the four downstream tasks. We further show that the model prefers structure-level attentions over token-level attentions in the task of code search.",
                    "[237386541 | Wang et al. | 2021 | Citations: 1586]": "Pre-trained models for Natural Languages (NL) like BERT and GPT have been recently shown to transfer well to Programming Languages (PL) and largely benefit a broad set of code-related tasks. Despite their success, most current methods either rely on an encoder-only (or decoder-only) pre-training that is suboptimal for generation (resp. understanding) tasks or process the code snippet in the same way as NL, neglecting the special characteristics of PL such as token types. We present CodeT5, a unified pre-trained encoder-decoder Transformer model that better leverages the code semantics conveyed from the developer-assigned identifiers. Our model employs a unified framework to seamlessly support both code understanding and generation tasks and allows for multi-task learning. Besides, we propose a novel identifier-aware pre-training task that enables the model to distinguish which code tokens are identifiers and to recover them when they are masked. Furthermore, we propose to exploit the user-written code comments with a bimodal dual generation task for better NL-PL alignment. Comprehensive experiments show that CodeT5 significantly outperforms prior methods on understanding tasks such as code defect detection and clone detection, and generation tasks across various directions including PL-NL, NL-PL, and PL-PL. Further analysis reveals that our model can better capture semantic information from code. Our code and pre-trained models are released at https://github.com/salesforce/CodeT5.",
                    "[247158549 | Xu et al. | 2022 | Citations: 654]": "Large language models (LMs) of code have recently shown tremendous promise in completing code and synthesizing code from natural language descriptions. However, the current state-of-the-art code LMs (e.g., Codex) are not publicly available, leaving many questions about their model and data design decisions. We aim to fill in some of these blanks through a systematic evaluation of the largest existing models: Codex, GPT-J, GPT-Neo, GPT-NeoX-20B, and CodeParrot, across various programming languages. Although Codex itself is not open-source, we find that existing opensource models do achieve close results in some programming languages, although targeted mainly for natural language modeling. We further identify an important missing piece in the form of a large open-source model trained exclusively on a multi-lingual corpus of code. We release a new model, PolyCoder, with 2.7B parameters based on the GPT-2 architecture, that was trained on 249GB of code across 12 programming languages on a single machine. In the C programming language, PolyCoder outperforms all models including Codex. Our trained models are open-source and publicly available at https://github.com/VHellendoorn/Code-LMs, which enables future research and application in this area. We have an online appendix at https://arxiv.org/abs/2202.13169.",
                    "[248157108 | Fried et al. | 2022 | Citations: 652]": "Code is seldom written in a single left-to-right pass and is instead repeatedly edited and refined. We introduce InCoder, a unified generative model that can perform program synthesis (via left-to-right generation) as well as editing (via infilling). InCoder is trained to generate code files from a large corpus of permissively licensed code, where regions of code have been randomly masked and moved to the end of each file, allowing code infilling with bidirectional context. Our model is the first generative model that is able to directly perform zero-shot code infilling, which we evaluate on challenging tasks such as type inference, comment generation, and variable re-naming. We find that the ability to condition on bidirectional context substantially improves performance on these tasks, while still performing comparably on standard program synthesis benchmarks in comparison to left-to-right only models pretrained at similar scale. The InCoder models and code are publicly released. https://sites.google.com/view/incoder-code-models",
                    "[52967399 | Devlin et al. | 2019 | Citations: 95215]": "We introduce a new language representation model called BERT, which stands for Bidirectional Encoder Representations from Transformers. Unlike recent language representation models (Peters et al., 2018a; Radford et al., 2018), BERT is designed to pre-train deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers. As a result, the pre-trained BERT model can be fine-tuned with just one additional output layer to create state-of-the-art models for a wide range of tasks, such as question answering and language inference, without substantial task-specific architecture modifications. BERT is conceptually simple and empirically powerful. It obtains new state-of-the-art results on eleven natural language processing tasks, including pushing the GLUE score to 80.5 (7.7 point absolute improvement), MultiNLI accuracy to 86.7% (4.6% absolute improvement), SQuAD v1.1 question answering Test F1 to 93.2 (1.5 point absolute improvement) and SQuAD v2.0 Test F1 to 83.1 (5.1 point absolute improvement)."
                },
                "metadata": [
                    {
                        "section_title": "BACKGROUND AND RELATED WORK 2.1 Large Language Models for Code",
                        "pdf_hash": "",
                        "start": 281,
                        "end": 1591,
                        "sentence_offsets": [
                            {
                                "start": 232,
                                "end": 359
                            },
                            {
                                "start": 360,
                                "end": 540
                            },
                            {
                                "start": 541,
                                "end": 621
                            },
                            {
                                "start": 622,
                                "end": 810
                            },
                            {
                                "start": 811,
                                "end": 932
                            },
                            {
                                "start": 933,
                                "end": 1067
                            },
                            {
                                "start": 1068,
                                "end": 1260
                            },
                            {
                                "start": 1261,
                                "end": 1531
                            },
                            {
                                "start": 1532,
                                "end": 1669
                            }
                        ],
                        "ref_mentions": [
                            "52967399",
                            "221761146",
                            "247158549",
                            "237386541",
                            "248157108"
                        ],
                        "quote": "Encoder-only models use only the encoder component by training using Masked Language Modeling (MLM) [14] objective where a small percentage (e.g., 15%) of the tokens are masked on. The goal of MLM is to recover these masked tokens given the surrounding context. Encoder-only models such as CodeBERT [15] and GraphCodeBERT [22] are designed to provide a representation of the input code to be used for downstream tasks such as code classification [72]. Decoder-only models, on the other hand, aim to autoregressively generate tokens based on all previously generated tokens. CodeGEN [51,52], Codex [10] and PolyCoder [71] are examples of decoder-only LLMs where they can be used for code autocompletion tasks. Different from encoder-and decoder-only LLMs, encoder-decoder models (e.g., CodeT5 [62,63] and PLBART [3]) combine both encoder and decoder together and jointly train both components together. A commonly used pre-training objective for encoder-decoder models is Masked Span Prediction (MSP) where random spans (multiple consecutive tokens) are replaced with single masked tokens and the models learn to fill in the masked span with the correct sequence of tokens. Furthermore, decoder-only models like InCoder [17] can also perform infilling by training through causal language modeling [2] objective."
                    }
                ]
            },
            {
                "idx": 16,
                "key": "[263729712 | Alomari et al. | 2023 | Citations: 1]",
                "snippets": "For the decoder part, we leverage GPT2, BERT, RoBERTa, and Ernie 2.0. GPT2 is trained on open-ended autoregressive generation with the objective of predicting the next word in a sequence of a few token starters (causal language modeling) utilizing only the Transformer's unidirectional ''left-to-right'' self-attention decoder. Theoretically, GPT2 is claimed to be the optimal design for use as a decoder. However, with a few adjustments, bi-directional encoderonly pretrained models such as BERT, RoBERTa, and Ernie 2.0 may simply be adapted as decoders. To compare the encoder and decoder architectures of the Transformer, encoder blocks consist solely of a bi-directional self-attention layer and two feed-forward layers. By contrast, decoder blocks have a unidirectional self-attention layer, a crossattention layer, and two feed-forward layers.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {},
                "metadata": [
                    {
                        "section_title": "2) DECODERS",
                        "pdf_hash": "",
                        "start": 0,
                        "end": 849,
                        "sentence_offsets": [
                            {
                                "start": 0,
                                "end": 69
                            },
                            {
                                "start": 70,
                                "end": 327
                            },
                            {
                                "start": 328,
                                "end": 405
                            },
                            {
                                "start": 406,
                                "end": 555
                            },
                            {
                                "start": 556,
                                "end": 724
                            },
                            {
                                "start": 725,
                                "end": 849
                            }
                        ],
                        "ref_mentions": [],
                        "quote": "For the decoder part, we leverage GPT2, BERT, RoBERTa, and Ernie 2.0. GPT2 is trained on open-ended autoregressive generation with the objective of predicting the next word in a sequence of a few token starters (causal language modeling) utilizing only the Transformer's unidirectional ''left-to-right'' self-attention decoder. Theoretically, GPT2 is claimed to be the optimal design for use as a decoder. However, with a few adjustments, bi-directional encoderonly pretrained models such as BERT, RoBERTa, and Ernie 2.0 may simply be adapted as decoders. To compare the encoder and decoder architectures of the Transformer, encoder blocks consist solely of a bi-directional self-attention layer and two feed-forward layers. By contrast, decoder blocks have a unidirectional self-attention layer, a crossattention layer, and two feed-forward layers."
                    }
                ]
            },
            {
                "idx": 17,
                "key": "[263829839 | Saha et al. | 2023 | Citations: 54]",
                "snippets": "The encoder component transforms input tokens into vectors using embeddings and positional encodings, then applies multihead self-attention and feedforward networks. The decoder, starting similarly, incorporates masked self-attention and crossattention with the output of the encoder, ensuring alignment and preventing future word prediction. Sequence-to-sequence models often use this architecture, where the input and output sequences can be of different lengths. These models shine in tasks where there is a direct and complex transformation between inputs and outputs. Examples like machine translation and text summarization are prototypical, as they require the model to understand the input deeply and generate a coherent and contextually accurate output. BART [5], T5 (Raffel et al., 2019), and UL2 (Tay et al., 2022) are a few well-known encoder-decoder models to be named....In a decoder-only model, a sequence is fed into the model, which then directly predicts the next token or word in the sequence. It operates autoregressively, using its generated tokens as context for subsequent predictions. It has two variants: causal decoder and prefix decoder. In a standard decoder (causal decoder), the unidirectional attention masking ensures that a token attends only to previous tokens and itself. Prefix decoders permit bidirectional attention over prefix tokens while maintaining unidirectional attention to generated tokens.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {
                    "[252780443 | Tay et al. | 2022 | Citations: 313]": "Existing pre-trained models are generally geared towards a particular class of problems. To date, there seems to be still no consensus on what the right architecture and pre-training setup should be. This paper presents a unified framework for pre-training models that are universally effective across datasets and setups. We begin by disentangling architectural archetypes with pre-training objectives -- two concepts that are commonly conflated. Next, we present a generalized&unified perspective for self-supervision in NLP and show how different pre-training objectives can be cast as one another and how interpolating between different objectives can be effective. We then propose Mixture-of-Denoisers (MoD), a pre-training objective that combines diverse pre-training paradigms together. We furthermore introduce a notion of mode switching, wherein downstream fine-tuning is associated with specific pre-training schemes. We conduct extensive ablative experiments to compare multiple pre-training objectives and find that our method pushes the Pareto-frontier by outperforming T5&GPT-like models across multiple diverse setups. By scaling our model up to 20B parameters, we achieve SOTA performance on 50 well-established supervised finetuning based NLP tasks. Our model also achieve strong results at in-context learning, outperforming 175B GPT-3 on zero-shot SuperGLUE and tripling the performance of T5-XXL on one-shot summarization. On 0-shot MMLU, UL2 20B outperforms T0 and T5 models. UL2 20B also works well with chain-of-thought prompting and reasoning, making it an appealing choice for research into reasoning at a small to medium scale of 20B parameters. Finally, we apply FLAN instruction tuning to the UL2 20B model, achieving MMLU and Big-Bench scores competitive to FLAN-PaLM 62B. We release Flax-based T5X checkpoints for the UL2 20B&Flan-UL2 20B.",
                    "[204838007 | Raffel et al. | 2019 | Citations: 20336]": "Transfer learning, where a model is first pre-trained on a data-rich task before being fine-tuned on a downstream task, has emerged as a powerful technique in natural language processing (NLP). The effectiveness of transfer learning has given rise to a diversity of approaches, methodology, and practice. In this paper, we explore the landscape of transfer learning techniques for NLP by introducing a unified framework that converts every language problem into a text-to-text format. Our systematic study compares pre-training objectives, architectures, unlabeled datasets, transfer approaches, and other factors on dozens of language understanding tasks. By combining the insights from our exploration with scale and our new \"Colossal Clean Crawled Corpus\", we achieve state-of-the-art results on many benchmarks covering summarization, question answering, text classification, and more. To facilitate future work on transfer learning for NLP, we release our dataset, pre-trained models, and code."
                },
                "metadata": [
                    {
                        "section_title": "A. Preliminaries",
                        "pdf_hash": "",
                        "start": 0,
                        "end": 856,
                        "sentence_offsets": [
                            {
                                "start": 0,
                                "end": 165
                            },
                            {
                                "start": 166,
                                "end": 342
                            },
                            {
                                "start": 343,
                                "end": 465
                            },
                            {
                                "start": 466,
                                "end": 572
                            },
                            {
                                "start": 573,
                                "end": 762
                            },
                            {
                                "start": 763,
                                "end": 853
                            }
                        ],
                        "ref_mentions": [
                            "204838007",
                            "252780443"
                        ],
                        "quote": "The encoder component transforms input tokens into vectors using embeddings and positional encodings, then applies multihead self-attention and feedforward networks. The decoder, starting similarly, incorporates masked self-attention and crossattention with the output of the encoder, ensuring alignment and preventing future word prediction. Sequence-to-sequence models often use this architecture, where the input and output sequences can be of different lengths. These models shine in tasks where there is a direct and complex transformation between inputs and outputs. Examples like machine translation and text summarization are prototypical, as they require the model to understand the input deeply and generate a coherent and contextually accurate output. BART [5], T5 (Raffel et al., 2019), and UL2 (Tay et al., 2022) are a few well-known encoder-decoder models to be named"
                    },
                    {
                        "section_title": "A. Preliminaries",
                        "pdf_hash": "",
                        "start": 1719,
                        "end": 2271,
                        "sentence_offsets": [
                            {
                                "start": 1719,
                                "end": 1846
                            },
                            {
                                "start": 1847,
                                "end": 1942
                            },
                            {
                                "start": 1943,
                                "end": 1998
                            },
                            {
                                "start": 1999,
                                "end": 2140
                            },
                            {
                                "start": 2141,
                                "end": 2270
                            }
                        ],
                        "ref_mentions": [],
                        "quote": ".In a decoder-only model, a sequence is fed into the model, which then directly predicts the next token or word in the sequence. It operates autoregressively, using its generated tokens as context for subsequent predictions. It has two variants: causal decoder and prefix decoder. In a standard decoder (causal decoder), the unidirectional attention masking ensures that a token attends only to previous tokens and itself. Prefix decoders permit bidirectional attention over prefix tokens while maintaining unidirectional attention to generated tokens."
                    }
                ]
            },
            {
                "idx": 18,
                "key": "[266755678 | Liu et al. | 2024 | Citations: 74]",
                "snippets": "LLMs with a Decoder-only architecture utilize the decoder component of the traditional Transformer architecture. Unlike the Encoder-Decoder architecture, which incorporates both an encoder and a decoder, the Decoder-only architecture is solely focused on the decoding process. In this configuration, the model sequentially generates tokens, attending to preceding tokens in the sequence. This architecture has been applied to various language generation tasks, showcasing its effectiveness in various tasks such as text generation without the need for an explicit encoding phase. The Decoder-only architecture can be further classified into two categories: the Causal Decoder architecture and the Prefix Decoder architecture. \n\nThe Causal Decoder Architecture: In the Causal Decoder architecture, each token in the model input sequence can only attend to past input tokens and itself during the decoding process. It achieves unidirectional attention to the input sequence by using a specific mask as shown in Figure 1. In fact, different architectures are mainly implemented by configuring different mask matrices. The figure illustrates a comparison of mask configurations between the Encoderdecoder and Decoder-only architectures (including Casual Decoder and Prefix Decoder).\n\nThe Prefix Decoder Architecture: The Prefix Decoder architecture combines the advantages of both the Encoderdecoder and Causal Decoder architectures. It leverages its unique mask configurations, as illustrated in Figure 1, enabling bidirectional attention for tokens in the prefix while maintaining unidirectional attention for generating subsequent tokens [54]. This design allows for the autoregressive generation of the output sequence with the flexibility to attend bi-directionally to the prefix tokens.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {},
                "metadata": [
                    {
                        "quote": "LLMs with a Decoder-only architecture utilize the decoder component of the traditional Transformer architecture. Unlike the Encoder-Decoder architecture, which incorporates both an encoder and a decoder, the Decoder-only architecture is solely focused on the decoding process. In this configuration, the model sequentially generates tokens, attending to preceding tokens in the sequence. This architecture has been applied to various language generation tasks, showcasing its effectiveness in various tasks such as text generation without the need for an explicit encoding phase. The Decoder-only architecture can be further classified into two categories: the Causal Decoder architecture and the Prefix Decoder architecture. \n\nThe Causal Decoder Architecture: In the Causal Decoder architecture, each token in the model input sequence can only attend to past input tokens and itself during the decoding process. It achieves unidirectional attention to the input sequence by using a specific mask as shown in Figure 1. In fact, different architectures are mainly implemented by configuring different mask matrices. The figure illustrates a comparison of mask configurations between the Encoderdecoder and Decoder-only architectures (including Casual Decoder and Prefix Decoder).\n\nThe Prefix Decoder Architecture: The Prefix Decoder architecture combines the advantages of both the Encoderdecoder and Causal Decoder architectures. It leverages its unique mask configurations, as illustrated in Figure 1, enabling bidirectional attention for tokens in the prefix while maintaining unidirectional attention for generating subsequent tokens [54]. This design allows for the autoregressive generation of the output sequence with the flexibility to attend bi-directionally to the prefix tokens.",
                        "pdf_hash": ""
                    }
                ]
            },
            {
                "idx": 19,
                "key": "[267211690 | Uludougan et al. | 2024 | Citations: 12]",
                "snippets": "Models that exclusively employ encoders, typically trained with denoising objectives, are geared toward understanding tasks, as exemplified in works such as (Devlin et al., 2019) and (208229926). Conversely, models that exclusively use decoders are designed for generation tasks, employing causal language modeling, as demonstrated in various studies (Radford et al., 2019)Brown et al., 2020;Touvron et al., 2023). The Text-to-Text Transformer (T5) (Raffel et al., 2019), on the other hand, employs an encoder-decoder architecture and undergoes pretraining with a denoising objective referred to as span corruption. UniLM (Dong et al., 2019) is also an encoder-decoder model, but pre-trained using unidirectional, bidirectional, and sequence-to-sequence language modeling. This can be seen as a combination of causal and denoising objectives.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {
                    "[147704286 | Dong et al. | 2019 | Citations: 1560]": "This paper presents a new Unified pre-trained Language Model (UniLM) that can be fine-tuned for both natural language understanding and generation tasks. The model is pre-trained using three types of language modeling tasks: unidirectional, bidirectional, and sequence-to-sequence prediction. The unified modeling is achieved by employing a shared Transformer network and utilizing specific self-attention masks to control what context the prediction conditions on. UniLM compares favorably with BERT on the GLUE benchmark, and the SQuAD 2.0 and CoQA question answering tasks. Moreover, UniLM achieves new state-of-the-art results on five natural language generation datasets, including improving the CNN/DailyMail abstractive summarization ROUGE-L to 40.51 (2.04 absolute improvement), the Gigaword abstractive summarization ROUGE-L to 35.75 (0.86 absolute improvement), the CoQA generative question answering F1 score to 82.5 (37.1 absolute improvement), the SQuAD question generation BLEU-4 to 22.12 (3.75 absolute improvement), and the DSTC7 document-grounded dialog response generation NIST-4 to 2.67 (human performance is 2.65). The code and pre-trained models are available at this https URL.",
                    "[204838007 | Raffel et al. | 2019 | Citations: 20336]": "Transfer learning, where a model is first pre-trained on a data-rich task before being fine-tuned on a downstream task, has emerged as a powerful technique in natural language processing (NLP). The effectiveness of transfer learning has given rise to a diversity of approaches, methodology, and practice. In this paper, we explore the landscape of transfer learning techniques for NLP by introducing a unified framework that converts every language problem into a text-to-text format. Our systematic study compares pre-training objectives, architectures, unlabeled datasets, transfer approaches, and other factors on dozens of language understanding tasks. By combining the insights from our exploration with scale and our new \"Colossal Clean Crawled Corpus\", we achieve state-of-the-art results on many benchmarks covering summarization, question answering, text classification, and more. To facilitate future work on transfer learning for NLP, we release our dataset, pre-trained models, and code.",
                    "[52967399 | Devlin et al. | 2019 | Citations: 95215]": "We introduce a new language representation model called BERT, which stands for Bidirectional Encoder Representations from Transformers. Unlike recent language representation models (Peters et al., 2018a; Radford et al., 2018), BERT is designed to pre-train deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers. As a result, the pre-trained BERT model can be fine-tuned with just one additional output layer to create state-of-the-art models for a wide range of tasks, such as question answering and language inference, without substantial task-specific architecture modifications. BERT is conceptually simple and empirically powerful. It obtains new state-of-the-art results on eleven natural language processing tasks, including pushing the GLUE score to 80.5 (7.7 point absolute improvement), MultiNLI accuracy to 86.7% (4.6% absolute improvement), SQuAD v1.1 question answering Test F1 to 93.2 (1.5 point absolute improvement) and SQuAD v2.0 Test F1 to 83.1 (5.1 point absolute improvement)."
                },
                "metadata": [
                    {
                        "section_title": "Pretraining objectives",
                        "pdf_hash": "",
                        "start": 153,
                        "end": 1002,
                        "sentence_offsets": [
                            {
                                "start": 153,
                                "end": 355
                            },
                            {
                                "start": 356,
                                "end": 574
                            },
                            {
                                "start": 575,
                                "end": 775
                            },
                            {
                                "start": 776,
                                "end": 932
                            },
                            {
                                "start": 933,
                                "end": 1002
                            }
                        ],
                        "ref_mentions": [
                            "52967399",
                            "208229926",
                            "160025533",
                            "204838007",
                            "147704286"
                        ],
                        "quote": "Models that exclusively employ encoders, typically trained with denoising objectives, are geared toward understanding tasks, as exemplified in works such as (Devlin et al., 2019) and (208229926). Conversely, models that exclusively use decoders are designed for generation tasks, employing causal language modeling, as demonstrated in various studies (Radford et al., 2019)Brown et al., 2020;Touvron et al., 2023). The Text-to-Text Transformer (T5) (Raffel et al., 2019), on the other hand, employs an encoder-decoder architecture and undergoes pretraining with a denoising objective referred to as span corruption. UniLM (Dong et al., 2019) is also an encoder-decoder model, but pre-trained using unidirectional, bidirectional, and sequence-to-sequence language modeling. This can be seen as a combination of causal and denoising objectives."
                    }
                ]
            },
            {
                "idx": 20,
                "key": "[267402678 | Li | 2024 | Citations: 2]",
                "snippets": "As shown in Figure 3, similar to most seq2seq models, the Transformer architecture consists of an encoder and a decoder. Overall, LLM (Language Model) models can be categorized into three major types: Encoder-decoder Architecture, Causal Decoder Architecture, and Prefix Decoder Architecture [11]. The Encoder-decoder Architecture uses the most basic structure and was initially introduced by the Seq2Seq model to address sequence-to-sequence tasks, such as machine translation. It consists of an encoder and a decoder. The encoder is responsible for transforming the input sequence into a fixeddimensional semantic representation, while the decoder uses this semantic representation to generate the output sequence. Within the encoder-decoder structure, self-attention mechanisms are commonly employed for sequence modeling, enabling the model to handle variable-length input and output sequences. This architecture has proven to be highly effective in various sequence-to-sequence tasks, such as text translation, text summarization, and dialogue generation. Prominent examples of large language models following this architecture include ELMo, BERT, RoBERTa, among others [12]. \n\nCurrently, the most widely used architecture is the Causal Decoder, which is primarily employed for handling autoregressive generation tasks, where each element of the output sequence depends on previously generated elements. The Causal Decoder Architecture is an improvement over the Encoderdecoder structure, as it introduces an autoregressive mechanism in the decoder. This means that when generating the current element, the model only uses the elements generated before it. This ensures that the model does not have access to future information during the generation process, thereby preserving causality. The GPT series (e.g., GPT-3) is a typical example of models that use the Causal Decoder Architecture. These models generate text by sequentially producing words one by one, avoiding information leakage and enabling the generation of coherent and plausible text. \n\nCompared to the Encoder-decoder models, Decoder-only models offer several advantages due to their simpler structure, faster training and inference speed, suitability for pure generation tasks, and advantages in decoder self-supervision.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {},
                "metadata": [
                    {
                        "section_title": "Different structures for combining encoders and decoders.",
                        "pdf_hash": "",
                        "start": 0,
                        "end": 2294,
                        "sentence_offsets": [
                            {
                                "start": 0,
                                "end": 120
                            },
                            {
                                "start": 121,
                                "end": 297
                            },
                            {
                                "start": 298,
                                "end": 478
                            },
                            {
                                "start": 479,
                                "end": 519
                            },
                            {
                                "start": 520,
                                "end": 716
                            },
                            {
                                "start": 717,
                                "end": 898
                            },
                            {
                                "start": 899,
                                "end": 1060
                            },
                            {
                                "start": 1061,
                                "end": 1180
                            },
                            {
                                "start": 1183,
                                "end": 1408
                            },
                            {
                                "start": 1409,
                                "end": 1554
                            },
                            {
                                "start": 1555,
                                "end": 1661
                            },
                            {
                                "start": 1662,
                                "end": 1793
                            },
                            {
                                "start": 1794,
                                "end": 1895
                            },
                            {
                                "start": 1896,
                                "end": 2055
                            },
                            {
                                "start": 2058,
                                "end": 2294
                            }
                        ],
                        "ref_mentions": [],
                        "quote": "As shown in Figure 3, similar to most seq2seq models, the Transformer architecture consists of an encoder and a decoder. Overall, LLM (Language Model) models can be categorized into three major types: Encoder-decoder Architecture, Causal Decoder Architecture, and Prefix Decoder Architecture [11]. The Encoder-decoder Architecture uses the most basic structure and was initially introduced by the Seq2Seq model to address sequence-to-sequence tasks, such as machine translation. It consists of an encoder and a decoder. The encoder is responsible for transforming the input sequence into a fixeddimensional semantic representation, while the decoder uses this semantic representation to generate the output sequence. Within the encoder-decoder structure, self-attention mechanisms are commonly employed for sequence modeling, enabling the model to handle variable-length input and output sequences. This architecture has proven to be highly effective in various sequence-to-sequence tasks, such as text translation, text summarization, and dialogue generation. Prominent examples of large language models following this architecture include ELMo, BERT, RoBERTa, among others [12]. \n\nCurrently, the most widely used architecture is the Causal Decoder, which is primarily employed for handling autoregressive generation tasks, where each element of the output sequence depends on previously generated elements. The Causal Decoder Architecture is an improvement over the Encoderdecoder structure, as it introduces an autoregressive mechanism in the decoder. This means that when generating the current element, the model only uses the elements generated before it. This ensures that the model does not have access to future information during the generation process, thereby preserving causality. The GPT series (e.g., GPT-3) is a typical example of models that use the Causal Decoder Architecture. These models generate text by sequentially producing words one by one, avoiding information leakage and enabling the generation of coherent and plausible text. \n\nCompared to the Encoder-decoder models, Decoder-only models offer several advantages due to their simpler structure, faster training and inference speed, suitability for pure generation tasks, and advantages in decoder self-supervision."
                    }
                ]
            },
            {
                "idx": 21,
                "key": "[268157336 | Patil et al. | 2024 | Citations: 80]",
                "snippets": "Encoder-decoder models adopt bidirectional attention for the encoder, unidirectional attention for the decoder, and a cross-attention mechanism between them. Cross-attention in the decoder has access only to the fully processed encoder output and is responsible for connecting input tokens to target tokens...Prefix language models are also decoder-only-based models but differ in the masking mechanism. Instead of a causal mask, a fully visible mask is used for the prefix part of the input sequence, and a causal mask is used for the target sequence...In a decoder-only model, the input and target are concatenated, and then a causal mask is used throughout. A decoder-only model with a prefix allows fully visible masking over part of the input token (prefix), followed by causal masking on the rest of the sequence. In general, autoencoding models learn bidirectional contextualized representation suited for NLU tasks, whereas autoregressive models learn to generate the next token and hence are suited for NLG tasks.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {},
                "metadata": [
                    {
                        "section_title": "Encoder-Decoder-Based Model",
                        "pdf_hash": "",
                        "start": 418,
                        "end": 724,
                        "sentence_offsets": [
                            {
                                "start": 418,
                                "end": 575
                            },
                            {
                                "start": 576,
                                "end": 725
                            }
                        ],
                        "ref_mentions": [],
                        "quote": "Encoder-decoder models adopt bidirectional attention for the encoder, unidirectional attention for the decoder, and a cross-attention mechanism between them. Cross-attention in the decoder has access only to the fully processed encoder output and is responsible for connecting input tokens to target tokens"
                    },
                    {
                        "section_title": "Prefix (Non-Causal) Language Model",
                        "pdf_hash": "",
                        "start": 0,
                        "end": 243,
                        "sentence_offsets": [
                            {
                                "start": 0,
                                "end": 94
                            },
                            {
                                "start": 95,
                                "end": 243
                            }
                        ],
                        "ref_mentions": [],
                        "quote": "Prefix language models are also decoder-only-based models but differ in the masking mechanism. Instead of a causal mask, a fully visible mask is used for the prefix part of the input sequence, and a causal mask is used for the target sequence"
                    },
                    {
                        "section_title": "Prefix (Non-Causal) Language Model",
                        "pdf_hash": "",
                        "start": 1330,
                        "end": 1799,
                        "sentence_offsets": [
                            {
                                "start": 1330,
                                "end": 1436
                            },
                            {
                                "start": 1437,
                                "end": 1595
                            },
                            {
                                "start": 1596,
                                "end": 1798
                            }
                        ],
                        "ref_mentions": [],
                        "quote": "In a decoder-only model, the input and target are concatenated, and then a causal mask is used throughout. A decoder-only model with a prefix allows fully visible masking over part of the input token (prefix), followed by causal masking on the rest of the sequence. In general, autoencoding models learn bidirectional contextualized representation suited for NLU tasks, whereas autoregressive models learn to generate the next token and hence are suited for NLG tasks."
                    }
                ]
            },
            {
                "idx": 22,
                "key": "[268247581 | Pei et al. | 2024 | Citations: 18]",
                "snippets": "Encoder-only models (Devlin et al., 2019) specialize in processing input sequences of biomolecules and text through bi-directional self-attention, making them highly effective for tasks that require an in-depth understanding of the input, such as sentiment analysis and feature extraction in NLP. Thereby in biomolecule domain, encoder-only models establish a bi-directional association between biotokens and text tokens for predictive tasks (Zeng et al., 2022)...In contrast, decoder-only models [18], [138] employ causal attention to focus on the sequence of previous tokens. This architecture is typically utilized in generative tasks, such as generating text descriptions that match the given molecule or for the reverse task (Liu et al., 2023). Thanks to the autoregressive generation property, decoder-only models are well suitable for instruction following and assistant/agent objectives.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {
                    "[246815222 | Zeng et al. | 2022 | Citations: 134]": "To accelerate biomedical research process, deep-learning systems are developed to automatically acquire knowledge about molecule entities by reading large-scale biomedical data. Inspired by humans that learn deep molecule knowledge from versatile reading on both molecule structure and biomedical text information, we propose a knowledgeable machine reading system that bridges both types of information in a unified deep-learning framework for comprehensive biomedical research assistance. We solve the problem that existing machine reading models can only process different types of data separately, and thus achieve a comprehensive and thorough understanding of molecule entities. By grasping meta-knowledge in an unsupervised fashion within and across different information sources, our system can facilitate various real-world biomedical applications, including molecular property prediction, biomedical relation extraction and so on. Experimental results show that our system even surpasses human professionals in the capability of molecular property comprehension, and also reveal its promising potential in facilitating automatic drug discovery and documentation in the future. To accelerate biomedical research process, deep-learning systems are developed to automatically acquire knowledge about molecule entities by reading large-scale biomedical data. Inspired by humans that learn deep molecule knowledge from both molecule structure and biomedical text information, the authors propose a machine reading system that bridges both types of information.",
                    "[258762343 | Liu et al. | 2023 | Citations: 75]": "Generative pre-trained Transformer (GPT) has demonstrates its great success in natural language processing and related techniques have been adapted into molecular modeling. Considering that text is the most important record for scientific discovery, in this paper, we propose MolXPT, a unified language model of text and molecules pre-trained on SMILES (a sequence representation of molecules) wrapped by text. Briefly, we detect the molecule names in each sequence and replace them to the corresponding SMILES. In this way, the SMILES could leverage the information from surrounding text, and vice versa. The above wrapped sequences, text sequences from PubMed and SMILES sequences from PubChem are all fed into a language model for pre-training. Experimental results demonstrate that MolXPT outperforms strong baselines of molecular property prediction on MoleculeNet, performs comparably to the best model in text-molecule translation while using less than half of its parameters, and enables zero-shot molecular generation without finetuning.",
                    "[52967399 | Devlin et al. | 2019 | Citations: 95215]": "We introduce a new language representation model called BERT, which stands for Bidirectional Encoder Representations from Transformers. Unlike recent language representation models (Peters et al., 2018a; Radford et al., 2018), BERT is designed to pre-train deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers. As a result, the pre-trained BERT model can be fine-tuned with just one additional output layer to create state-of-the-art models for a wide range of tasks, such as question answering and language inference, without substantial task-specific architecture modifications. BERT is conceptually simple and empirically powerful. It obtains new state-of-the-art results on eleven natural language processing tasks, including pushing the GLUE score to 80.5 (7.7 point absolute improvement), MultiNLI accuracy to 86.7% (4.6% absolute improvement), SQuAD v1.1 question answering Test F1 to 93.2 (1.5 point absolute improvement) and SQuAD v2.0 Test F1 to 83.1 (5.1 point absolute improvement)."
                },
                "metadata": [
                    {
                        "section_title": "Encoder/Decoder-only",
                        "pdf_hash": "",
                        "start": 136,
                        "end": 565,
                        "sentence_offsets": [
                            {
                                "start": 136,
                                "end": 415
                            },
                            {
                                "start": 416,
                                "end": 566
                            }
                        ],
                        "ref_mentions": [
                            "52967399",
                            "246815222"
                        ],
                        "quote": "Encoder-only models (Devlin et al., 2019) specialize in processing input sequences of biomolecules and text through bi-directional self-attention, making them highly effective for tasks that require an in-depth understanding of the input, such as sentiment analysis and feature extraction in NLP. Thereby in biomolecule domain, encoder-only models establish a bi-directional association between biotokens and text tokens for predictive tasks (Zeng et al., 2022)"
                    },
                    {
                        "section_title": "Encoder/Decoder-only",
                        "pdf_hash": "",
                        "start": 704,
                        "end": 1122,
                        "sentence_offsets": [
                            {
                                "start": 704,
                                "end": 817
                            },
                            {
                                "start": 818,
                                "end": 975
                            },
                            {
                                "start": 976,
                                "end": 1121
                            }
                        ],
                        "ref_mentions": [
                            "258762343"
                        ],
                        "quote": "In contrast, decoder-only models [18], [138] employ causal attention to focus on the sequence of previous tokens. This architecture is typically utilized in generative tasks, such as generating text descriptions that match the given molecule or for the reverse task (Liu et al., 2023). Thanks to the autoregressive generation property, decoder-only models are well suitable for instruction following and assistant/agent objectives."
                    }
                ]
            },
            {
                "idx": 23,
                "key": "[269148552 | Xu et al. | 2024 | Citations: 8]",
                "snippets": "Encoder-only LLMs, such as BERT [23] and its variants like CodeBERT [46], have a bidirectional transformer encoder structure. They are typically trained on the masked language modeling objective (i.e., MLM), aiming to denoise and reconstruct the masked tokens via understanding the surrounding context (Fig. 1). As shown in Eq. 1, the loss of MLM training objective can be explicitly represented as the log-sum of the conditional probabilities of generating the masked token when all the unmasked tokens are known.\n\nDecoder-only LLMs, including GPT series [13], [27] and LLaMA series [47], have an autoregressive transformer decoder structure. They are mainly trained on the causal language modeling objective (i.e., CLM), aiming to predict and complete next tokens via following the prefix context (Fig. 1). As shown in Eq. 2, the loss of CLM training objective can be explicitly represented as the log-sum of the conditional probabilities of generating the next token when all the preceeding tokens are known.\n\nEncoder-decoder LLMs, such as T5 [24] and its variants like CodeT5 [48], have a complete transformer structure.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {},
                "metadata": [
                    {
                        "quote": "Encoder-only LLMs, such as BERT [23] and its variants like CodeBERT [46], have a bidirectional transformer encoder structure. They are typically trained on the masked language modeling objective (i.e., MLM), aiming to denoise and reconstruct the masked tokens via understanding the surrounding context (Fig. 1). As shown in Eq. 1, the loss of MLM training objective can be explicitly represented as the log-sum of the conditional probabilities of generating the masked token when all the unmasked tokens are known.\n\nDecoder-only LLMs, including GPT series [13], [27] and LLaMA series [47], have an autoregressive transformer decoder structure. They are mainly trained on the causal language modeling objective (i.e., CLM), aiming to predict and complete next tokens via following the prefix context (Fig. 1). As shown in Eq. 2, the loss of CLM training objective can be explicitly represented as the log-sum of the conditional probabilities of generating the next token when all the preceeding tokens are known.\n\nEncoder-decoder LLMs, such as T5 [24] and its variants like CodeT5 [48], have a complete transformer structure.",
                        "pdf_hash": ""
                    }
                ]
            },
            {
                "idx": 24,
                "key": "[269982953 | Leemann et al. | 2024 | Citations: 5]",
                "snippets": "Practical implementations introduce subtle modifications into the process described previously.The most relevant distinction is made between encoder-only models, that include BERT [15] and its variants, and decoder-only models such as the GPT models [38,(Radford et al., 2019).\n\nEncoder-only models.Considering BERT as an example of an encoder-only model, the first token is used for the classification, i.e, r = 1.Usually, a special token [CLS] is prepended to the text at position 1, however this is not strictly necessary for the functioning of the model.\n\nDecoder-only models.In contrast, decoder-models like GPT-2 (Radford et al., 2019) add the classification head on top of the last token for classification, i.e., r = |t|.A key difference is that in GPT-2 and other decoder-only models, a causal mask is laid over the attention matrix, resulting in \u03b1 i,j = 0 for j > i.This encodes the constraint that tokens can only attend to themselves or to previous ones.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {},
                "metadata": [
                    {
                        "section_title": "Encoder-Only and Decoder-Only models",
                        "pdf_hash": "",
                        "start": 0,
                        "end": 929,
                        "sentence_offsets": [
                            {
                                "start": 0,
                                "end": 95
                            },
                            {
                                "start": 95,
                                "end": 258
                            },
                            {
                                "start": 260,
                                "end": 280
                            },
                            {
                                "start": 280,
                                "end": 396
                            },
                            {
                                "start": 396,
                                "end": 539
                            },
                            {
                                "start": 541,
                                "end": 561
                            },
                            {
                                "start": 561,
                                "end": 692
                            },
                            {
                                "start": 692,
                                "end": 839
                            },
                            {
                                "start": 839,
                                "end": 929
                            }
                        ],
                        "ref_mentions": [
                            "160025533",
                            "160025533"
                        ],
                        "quote": "Practical implementations introduce subtle modifications into the process described previously.The most relevant distinction is made between encoder-only models, that include BERT [15] and its variants, and decoder-only models such as the GPT models [38,(Radford et al., 2019).\n\nEncoder-only models.Considering BERT as an example of an encoder-only model, the first token is used for the classification, i.e, r = 1.Usually, a special token [CLS] is prepended to the text at position 1, however this is not strictly necessary for the functioning of the model.\n\nDecoder-only models.In contrast, decoder-models like GPT-2 (Radford et al., 2019) add the classification head on top of the last token for classification, i.e., r = |t|.A key difference is that in GPT-2 and other decoder-only models, a causal mask is laid over the attention matrix, resulting in \u03b1 i,j = 0 for j > i.This encodes the constraint that tokens can only attend to themselves or to previous ones."
                    }
                ]
            },
            {
                "idx": 25,
                "key": "[270214176 | Jiang et al. | 2024 | Citations: 197]",
                "snippets": "In encoder-decoder LLMs, a pivot token is randomly selected in a sequence of tokens and then regarding the context before it as the source sequence x = {1, . . ., } of the encoder and the sequence after it as the target output x = {+1, . . ., } of decoder.Formally, the causal language modeling objective for training encoder-decoder LLMs is to minimize loss function as follows:\n\nwhere x\u2264 is the source sequence input and x< denotes the target sequence autoregressively generated so far...The conditional probability (|x<)) is modeled by adding a causal attention mask to the multi-head self-attention matrix of each Transformer block.To be specific, causal attention masking is implemented by setting the lower triangular part of the matrix to 0 and the remaining elements to \u2212\u221e, ensuring that each token attends only to its predecessors and itself.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {},
                "metadata": [
                    {
                        "section_title": "Pre-training",
                        "pdf_hash": "",
                        "start": 391,
                        "end": 878,
                        "sentence_offsets": [
                            {
                                "start": 280,
                                "end": 497
                            },
                            {
                                "start": 497,
                                "end": 680
                            },
                            {
                                "start": 680,
                                "end": 765
                            },
                            {
                                "start": 765,
                                "end": 782
                            },
                            {
                                "start": 782,
                                "end": 905
                            }
                        ],
                        "ref_mentions": [],
                        "quote": "In encoder-decoder LLMs, a pivot token is randomly selected in a sequence of tokens and then regarding the context before it as the source sequence x = {1, . . ., } of the encoder and the sequence after it as the target output x = {+1, . . ., } of decoder.Formally, the causal language modeling objective for training encoder-decoder LLMs is to minimize loss function as follows:\n\nwhere x\u2264 is the source sequence input and x< denotes the target sequence autoregressively generated so far"
                    },
                    {
                        "section_title": "Pre-training",
                        "pdf_hash": "",
                        "start": 86,
                        "end": 447,
                        "sentence_offsets": [
                            {
                                "start": 65,
                                "end": 128
                            },
                            {
                                "start": 128,
                                "end": 280
                            },
                            {
                                "start": 280,
                                "end": 497
                            }
                        ],
                        "ref_mentions": [],
                        "quote": "The conditional probability (|x<)) is modeled by adding a causal attention mask to the multi-head self-attention matrix of each Transformer block.To be specific, causal attention masking is implemented by setting the lower triangular part of the matrix to 0 and the remaining elements to \u2212\u221e, ensuring that each token attends only to its predecessors and itself."
                    }
                ]
            },
            {
                "idx": 26,
                "key": "[270560675 | Ma et al. | 2024 | Citations: 23]",
                "snippets": "As outlined in Sec. 1, we observe two discrepancies between decoder-only LLMs and encoder-decoder models: optimization objective and model architecture. Specifically, the decoder-only LLMs are typically optimized using the next token prediction task while the encoder-decoder models are trained with the masked language modeling task. Besides, the former tokens in a sequence cannot attend the latter tokens in decoder-only LLMs while every token in the sequence can attend each other in the encoder models.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {},
                "metadata": [
                    {
                        "section_title": "Prompt Encoding with Language Models",
                        "pdf_hash": "",
                        "start": 0,
                        "end": 507,
                        "sentence_offsets": [
                            {
                                "start": 0,
                                "end": 152
                            },
                            {
                                "start": 153,
                                "end": 334
                            },
                            {
                                "start": 335,
                                "end": 507
                            }
                        ],
                        "ref_mentions": [],
                        "quote": "As outlined in Sec. 1, we observe two discrepancies between decoder-only LLMs and encoder-decoder models: optimization objective and model architecture. Specifically, the decoder-only LLMs are typically optimized using the next token prediction task while the encoder-decoder models are trained with the masked language modeling task. Besides, the former tokens in a sequence cannot attend the latter tokens in decoder-only LLMs while every token in the sequence can attend each other in the encoder models."
                    }
                ]
            },
            {
                "idx": 27,
                "key": "[270702559 | Yin et al. | 2024 | Citations: 12]",
                "snippets": "Typical architectures for LLMs can be categorized as encoder-decoder, causal decoder, and prefix decoder (Wang and Hesslow, 2022;Zhao et al., 2023).Among them, the causal decoder architecture is the most frequently used by various LLMs, such as OPT (Susan Zhang et al., 2023), LLAMA (Touvron et al., 2023a), BLOOM (Scao et al., 2023) due to its superior zero-shot and fewshot generalization capacity (Wang and Hesslow, 2022) and the effectiveness of scaling law (Brown et al., 2020;Kaplan et al., 2020).The causal decoder architecture employs a unidirectional attention mask, ensuring that each input token can only attend to preceding tokens and itself.Both input and output tokens are processed identically through the decoder (Zhao et al., 2023).\n\nThe prefix decoder architecture, also known as the non-causal decoder, modifies the masking mechanism used in causal decoders (Zhang et al., 2020).This adjustment allows for bidirectional attention over the prefix tokens while restricting attention to unidirectional generated tokens (Dong et al., 2019).Similar to the encoder-decoder architecture, prefix decoders can bidirectionally encode the prefix sequence and autoregressively predict the output tokens sequentially (Zhao et al., 2023).The same parameters are shared during both encoding and decoding processes.Representative LLMs based on prefix decoders include GLM-130B (Zeng et al., 2023) and U-PaLM (Tay et al., 2022).\n\nThe vanilla Transformer model is constructed on the encoder-decoder architecture (Vaswani et al., 2017), comprising two stacks of Transformer blocks designated as the encoder and decoder, respectively.The encoder utilizes stacked multi-head self-attention layers to encode the input sequence into its latent representations.Meanwhile, the decoder employs cross-attention mechanisms on these representations, and autoregressive generates the target sequence.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {
                    "[13756489 | Vaswani et al. | 2017 | Citations: 132444]": "The dominant sequence transduction models are based on complex recurrent or convolutional neural networks in an encoder-decoder configuration. The best performing models also connect the encoder and decoder through an attention mechanism. We propose a new simple network architecture, the Transformer, based solely on attention mechanisms, dispensing with recurrence and convolutions entirely. Experiments on two machine translation tasks show these models to be superior in quality while being more parallelizable and requiring significantly less time to train. Our model achieves 28.4 BLEU on the WMT 2014 English-to-German translation task, improving over the existing best results, including ensembles by over 2 BLEU. On the WMT 2014 English-to-French translation task, our model establishes a new single-model state-of-the-art BLEU score of 41.8 after training for 3.5 days on eight GPUs, a small fraction of the training costs of the best models from the literature. We show that the Transformer generalizes well to other tasks by applying it successfully to English constituency parsing both with large and limited training data."
                },
                "metadata": [
                    {
                        "section_title": "Architecture of LLMs",
                        "pdf_hash": "",
                        "start": 0,
                        "end": 1889,
                        "sentence_offsets": [
                            {
                                "start": 0,
                                "end": 148
                            },
                            {
                                "start": 148,
                                "end": 503
                            },
                            {
                                "start": 503,
                                "end": 654
                            },
                            {
                                "start": 654,
                                "end": 749
                            },
                            {
                                "start": 751,
                                "end": 898
                            },
                            {
                                "start": 898,
                                "end": 1055
                            },
                            {
                                "start": 1055,
                                "end": 1243
                            },
                            {
                                "start": 1243,
                                "end": 1318
                            },
                            {
                                "start": 1318,
                                "end": 1430
                            },
                            {
                                "start": 1432,
                                "end": 1633
                            },
                            {
                                "start": 1633,
                                "end": 1756
                            },
                            {
                                "start": 1756,
                                "end": 1889
                            }
                        ],
                        "ref_mentions": [
                            "13756489"
                        ],
                        "quote": "Typical architectures for LLMs can be categorized as encoder-decoder, causal decoder, and prefix decoder (Wang and Hesslow, 2022;Zhao et al., 2023).Among them, the causal decoder architecture is the most frequently used by various LLMs, such as OPT (Susan Zhang et al., 2023), LLAMA (Touvron et al., 2023a), BLOOM (Scao et al., 2023) due to its superior zero-shot and fewshot generalization capacity (Wang and Hesslow, 2022) and the effectiveness of scaling law (Brown et al., 2020;Kaplan et al., 2020).The causal decoder architecture employs a unidirectional attention mask, ensuring that each input token can only attend to preceding tokens and itself.Both input and output tokens are processed identically through the decoder (Zhao et al., 2023).\n\nThe prefix decoder architecture, also known as the non-causal decoder, modifies the masking mechanism used in causal decoders (Zhang et al., 2020).This adjustment allows for bidirectional attention over the prefix tokens while restricting attention to unidirectional generated tokens (Dong et al., 2019).Similar to the encoder-decoder architecture, prefix decoders can bidirectionally encode the prefix sequence and autoregressively predict the output tokens sequentially (Zhao et al., 2023).The same parameters are shared during both encoding and decoding processes.Representative LLMs based on prefix decoders include GLM-130B (Zeng et al., 2023) and U-PaLM (Tay et al., 2022).\n\nThe vanilla Transformer model is constructed on the encoder-decoder architecture (Vaswani et al., 2017), comprising two stacks of Transformer blocks designated as the encoder and decoder, respectively.The encoder utilizes stacked multi-head self-attention layers to encode the input sequence into its latent representations.Meanwhile, the decoder employs cross-attention mechanisms on these representations, and autoregressive generates the target sequence."
                    }
                ]
            },
            {
                "idx": 28,
                "key": "[270832367 | Busto-Castineira et al. | 2024 | Citations: 1]",
                "snippets": "While the encoder's attention is bidirectional, the decoder has two different types of attention: (i) a masked multi-head attention block that masks non-causal context and (ii) a bidirectional multihead attention block that receives non-causal information from the encoder...By omitting the encoder in decoder-only transformers, all non-causal contextual dependencies are removed by exclusively using masked attention. Decoder-only transformers are nowadays the best performing task-agnostic NLG systems. Nevertheless, there exist some state-of-theart non-causal NLG solutions. For example, non-causal language models can be trained for the Masked Language Modeling (MLM) objective, a task in which the language model predicts masked words within a sentence (Zeng et al., 2021).",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {
                    "[265629619 | Zeng et al. | 2021 | Citations: 8]": "Machine reading comprehension (MRC) is a challenging natural language processing (NLP) task. It has a wide application potential in the fields of question answering robots, human-computer interactions in mobile virtual reality systems, etc. Recently, the emergence of pretrained models (PTMs) has brought this research field into a new era, in which the training objective plays a key role. The masked language model (MLM) is a self-supervised training objective widely used in various PTMs. With the development of training objectives, many variants of MLM have been proposed, such as whole word masking, entity masking, phrase masking, and span masking. In different MLMs, the length of the masked tokens is different. Similarly, in different machine reading comprehension tasks, the length of the answer is also different, and the answer is often a word, phrase, or sentence. Thus, in MRC tasks with different answer lengths, whether the length of MLM is related to performance is a question worth studying. If this hypothesis is true, it can guide us on how to pretrain the MLM with a relatively suitable mask length distribution for MRC tasks. In this paper, we try to uncover how much of MLM\u2019s success in the machine reading comprehension tasks comes from the correlation between masking length distribution and answer length in the MRC dataset. In order to address this issue, herein, (1) we propose four MRC tasks with different answer length distributions, namely, the short span extraction task, long span extraction task, short multiple-choice cloze task, and long multiple-choice cloze task; (2) four Chinese MRC datasets are created for these tasks; (3) we also have pretrained four masked language models according to the answer length distributions of these datasets; and (4) ablation experiments are conducted on the datasets to verify our hypothesis. The experimental results demonstrate that our hypothesis is true. On four different machine reading comprehension datasets, the performance of the model with correlation length distribution surpasses the model without correlation."
                },
                "metadata": [
                    {
                        "section_title": "A. CAUSALITY IN GENERATIVE TRANSFORMER LANGUAGE MODELS",
                        "pdf_hash": "",
                        "start": 739,
                        "end": 1011,
                        "sentence_offsets": [
                            {
                                "start": 739,
                                "end": 1012
                            }
                        ],
                        "ref_mentions": [],
                        "quote": "While the encoder's attention is bidirectional, the decoder has two different types of attention: (i) a masked multi-head attention block that masks non-causal context and (ii) a bidirectional multihead attention block that receives non-causal information from the encoder"
                    },
                    {
                        "section_title": "A. CAUSALITY IN GENERATIVE TRANSFORMER LANGUAGE MODELS",
                        "pdf_hash": "",
                        "start": 1212,
                        "end": 1701,
                        "sentence_offsets": [
                            {
                                "start": 1212,
                                "end": 1355
                            },
                            {
                                "start": 1356,
                                "end": 1441
                            },
                            {
                                "start": 1442,
                                "end": 1514
                            },
                            {
                                "start": 1515,
                                "end": 1700
                            }
                        ],
                        "ref_mentions": [
                            "265629619"
                        ],
                        "quote": "By omitting the encoder in decoder-only transformers, all non-causal contextual dependencies are removed by exclusively using masked attention. Decoder-only transformers are nowadays the best performing task-agnostic NLG systems. Nevertheless, there exist some state-of-theart non-causal NLG solutions. For example, non-causal language models can be trained for the Masked Language Modeling (MLM) objective, a task in which the language model predicts masked words within a sentence (Zeng et al., 2021)."
                    }
                ]
            },
            {
                "idx": 29,
                "key": "[273025546 | Ewer et al. | 2024 | Citations: 4]",
                "snippets": "Decoders use a causal attention, ensuring that each token attends only to the preceding tokens. In contrast, encoders allow all tokens to attend to each other by performing attention computation from scratch for each token prediction.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {},
                "metadata": [
                    {
                        "section_title": "INTRODUCTION",
                        "pdf_hash": "",
                        "start": 1468,
                        "end": 1702,
                        "sentence_offsets": [
                            {
                                "start": 1320,
                                "end": 1563
                            },
                            {
                                "start": 1564,
                                "end": 1702
                            }
                        ],
                        "ref_mentions": [],
                        "quote": "Decoders use a causal attention, ensuring that each token attends only to the preceding tokens. In contrast, encoders allow all tokens to attend to each other by performing attention computation from scratch for each token prediction."
                    }
                ]
            },
            {
                "idx": 30,
                "key": "[276423946 | Busto-Castineira et al. | 2025 | Citations: 0]",
                "snippets": "Transformers are unsupervised learners thanks to their selfattention mechanism (Vaswani et al., 2017), which controls the impact of the context on the model's output. The original transformer architecture is composed of an encoder and a decoder. While the encoder's attention is bidirectional, the decoder has a masked multi-head attention block that masks non-causal context and a bidirectional multi-head attention block that receives noncausal information from the encoder. \n\nAlthough the encoder-decoder architecture is widely used in some nlp applications like machine translation (Kawara et al., 2021)(Nguyen et al., 2021), other transformer-based models only use one of these two components. By excluding the encoder, we eliminate all non-causal contextual dependencies, thus using only the masked attention of the decoder. Currently, decoder-only transformers are the most effective task-agnostic nlg systems.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {
                    "[13756489 | Vaswani et al. | 2017 | Citations: 132444]": "The dominant sequence transduction models are based on complex recurrent or convolutional neural networks in an encoder-decoder configuration. The best performing models also connect the encoder and decoder through an attention mechanism. We propose a new simple network architecture, the Transformer, based solely on attention mechanisms, dispensing with recurrence and convolutions entirely. Experiments on two machine translation tasks show these models to be superior in quality while being more parallelizable and requiring significantly less time to train. Our model achieves 28.4 BLEU on the WMT 2014 English-to-German translation task, improving over the existing best results, including ensembles by over 2 BLEU. On the WMT 2014 English-to-French translation task, our model establishes a new single-model state-of-the-art BLEU score of 41.8 after training for 3.5 days on eight GPUs, a small fraction of the training costs of the best models from the literature. We show that the Transformer generalizes well to other tasks by applying it successfully to English constituency parsing both with large and limited training data.",
                    "[231645376 | Kawara et al. | 2021 | Citations: 17]": "The difference in word orders between source and target languages is a serious hurdle for machine translation. Preordering methods, which reorder the words in a source sentence before translation to obtain a similar word ordering with a target language, significantly improve the quality in statistical machine translation. While the information on the preordering position improved the translation quality in recurrent neural network-based models, questions such as how to use preordering information and whether it is helpful for the Transformer model remain unaddressed. In this article, we successfully employed preordering techniques in the Transformer-based neural machine translation. Specifically, we proposed a novel <italic>preordering encoding</italic> that exploits the reordering information of the source and target sentences as positional encoding in the Transformer model. Experimental results on ASPEC Japanese\u2013English and WMT 2015 English\u2013German, English\u2013Czech, and English\u2013Russian translation tasks confirmed that the proposed method significantly improved the translation quality evaluated by the BLEU scores of the Transformer model by <inline-formula><tex-math notation=\"LaTeX\">${\\text{1.34}}$</tex-math></inline-formula> points in the Japanese\u2013to\u2013English task, <inline-formula><tex-math notation=\"LaTeX\">${\\text{2.19}}$</tex-math></inline-formula> points in the English\u2013to\u2013German task, <inline-formula><tex-math notation=\"LaTeX\">${\\text{0.15}}$</tex-math></inline-formula> points in the Czech\u2013to\u2013English task, and <inline-formula><tex-math notation=\"LaTeX\">${\\text {1.48}}$</tex-math></inline-formula> points in the English\u2013to\u2013Russian task.",
                    "[234785837 | Nguyen et al. | 2021 | Citations: 22]": "Transformer is a neural machine translation model which revolutionizes machine translation. Compared with traditional statistical machine translation models and other neural machine translation models, the recently proposed transformer model radically and fundamentally changes machine translation with its self-attention and cross-attention mechanisms. These mechanisms effectively model token alignments between source and target sentences. It has been reported that the transformer model provides accurate posterior alignments. In this work, we empirically prove the reverse effect, showing that prior alignments help transformer models produce better translations. Experiment results on Vietnamese-English news translation task show not only the positive effect of manually annotated alignments on transformer models but also the surprising outperformance of statistically constructed alignments reinforced with the flexibility of token-type selection over manual alignments in improving transformer models. Statistically constructed word-to-lemma alignments are used to train a word-to-word transformer model. The novel hybrid transformer model improves the baseline transformer model and transformer model trained with manual alignments by 2.53 and 0.79 BLEU, respectively. In addition to BLEU score, we make limited human judgment on translation results. Strong correlation between human and machine judgment confirms our findings."
                },
                "metadata": [
                    {
                        "section_title": "Causality in generative transformer language models",
                        "pdf_hash": "",
                        "start": 0,
                        "end": 864,
                        "sentence_offsets": [
                            {
                                "start": 0,
                                "end": 148
                            },
                            {
                                "start": 149,
                                "end": 227
                            },
                            {
                                "start": 228,
                                "end": 458
                            },
                            {
                                "start": 461,
                                "end": 645
                            },
                            {
                                "start": 646,
                                "end": 777
                            },
                            {
                                "start": 778,
                                "end": 864
                            }
                        ],
                        "ref_mentions": [
                            "13756489",
                            "231645376",
                            "234785837"
                        ],
                        "quote": "Transformers are unsupervised learners thanks to their selfattention mechanism (Vaswani et al., 2017), which controls the impact of the context on the model's output. The original transformer architecture is composed of an encoder and a decoder. While the encoder's attention is bidirectional, the decoder has a masked multi-head attention block that masks non-causal context and a bidirectional multi-head attention block that receives noncausal information from the encoder. \n\nAlthough the encoder-decoder architecture is widely used in some nlp applications like machine translation (Kawara et al., 2021)(Nguyen et al., 2021), other transformer-based models only use one of these two components. By excluding the encoder, we eliminate all non-causal contextual dependencies, thus using only the masked attention of the decoder. Currently, decoder-only transformers are the most effective task-agnostic nlg systems."
                    }
                ]
            },
            {
                "idx": 31,
                "key": "[276771845 | Suganthan et al. | 2025 | Citations: 1]",
                "snippets": "A key question thus arises: can we effectively adapt the powerful knowledge embedded in decoder-only models to excel in these encoder-centric tasks?...Gemma's causal attention, ideal for generative tasks, inherently limits its applicability to encoder-based tasks. We demonstrate that simply enabling bidirectional attention during fine-tuning dramatically improves performance.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {},
                "metadata": [
                    {
                        "section_title": "Introduction",
                        "pdf_hash": "",
                        "start": 387,
                        "end": 535,
                        "sentence_offsets": [
                            {
                                "start": 387,
                                "end": 535
                            }
                        ],
                        "ref_mentions": [],
                        "quote": "A key question thus arises: can we effectively adapt the powerful knowledge embedded in decoder-only models to excel in these encoder-centric tasks?"
                    },
                    {
                        "section_title": "Introduction",
                        "pdf_hash": "",
                        "start": 1159,
                        "end": 1387,
                        "sentence_offsets": [
                            {
                                "start": 1159,
                                "end": 1272
                            },
                            {
                                "start": 1273,
                                "end": 1386
                            }
                        ],
                        "ref_mentions": [],
                        "quote": "Gemma's causal attention, ideal for generative tasks, inherently limits its applicability to encoder-based tasks. We demonstrate that simply enabling bidirectional attention during fine-tuning dramatically improves performance."
                    }
                ]
            },
            {
                "idx": 32,
                "key": "[277128409 | Ghosh et al. | 2024 | Citations: 0]",
                "snippets": "The functioning of the Decoder has many parts in common with the Encoder, yet there are some major differences. In a nutshell, the Decoder processes the output component of the input-output pairs of the training data; encodes such information and produces its own embedding using a multi-head attention scheme similar to the Encoder, but: \n\n1. once the output has been encoded, it is combined with the  and  matrices coming from the Encoder. This is the step where Transformers learn the relation between the input of the training (e.g., the question) and the output (e.g., the answer to the question); considering the question-answer example, the Decoder performs cross-attention on the Encoder output (which represents the question) while processing the answer; 2. at the end of the process a linear layer has a number of output neurons equal to the size of the vocabulary; such network uses a softmax function to produce likelihood for each term in the vocabulary. Then, the term with the highest likelihood is the output of the Decoder i.e. the predicted next word; 3. The Decoder produces the output one word at a time; in the training phase, knowing the correct word, the error is computed and used to drive the backpropagation step and the correction of the weights to reduce the error....For example, the weights obtained in pretraining for Encoder-only model like BERT, are based on randomly masked words; on the other hand, for Decoder-only model like GPT, causal masking is used, where only the future tokens are masked, and the model predicts the next token given past tokens.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {},
                "metadata": [
                    {
                        "section_title": "Encoding and decoding",
                        "pdf_hash": "",
                        "start": 0,
                        "end": 1296,
                        "sentence_offsets": [
                            {
                                "start": 0,
                                "end": 111
                            },
                            {
                                "start": 112,
                                "end": 338
                            },
                            {
                                "start": 341,
                                "end": 441
                            },
                            {
                                "start": 442,
                                "end": 967
                            },
                            {
                                "start": 968,
                                "end": 1293
                            }
                        ],
                        "ref_mentions": [],
                        "quote": "The functioning of the Decoder has many parts in common with the Encoder, yet there are some major differences. In a nutshell, the Decoder processes the output component of the input-output pairs of the training data; encodes such information and produces its own embedding using a multi-head attention scheme similar to the Encoder, but: \n\n1. once the output has been encoded, it is combined with the  and  matrices coming from the Encoder. This is the step where Transformers learn the relation between the input of the training (e.g., the question) and the output (e.g., the answer to the question); considering the question-answer example, the Decoder performs cross-attention on the Encoder output (which represents the question) while processing the answer; 2. at the end of the process a linear layer has a number of output neurons equal to the size of the vocabulary; such network uses a softmax function to produce likelihood for each term in the vocabulary. Then, the term with the highest likelihood is the output of the Decoder i.e. the predicted next word; 3. The Decoder produces the output one word at a time; in the training phase, knowing the correct word, the error is computed and used to drive the backpropagation step and the correction of the weights to reduce the error"
                    },
                    {
                        "section_title": "Encoding and decoding",
                        "pdf_hash": "",
                        "start": 1580,
                        "end": 1872,
                        "sentence_offsets": [
                            {
                                "start": 1580,
                                "end": 1872
                            }
                        ],
                        "ref_mentions": [],
                        "quote": ".For example, the weights obtained in pretraining for Encoder-only model like BERT, are based on randomly masked words; on the other hand, for Decoder-only model like GPT, causal masking is used, where only the future tokens are masked, and the model predicts the next token given past tokens."
                    }
                ]
            },
            {
                "idx": 33,
                "key": "[277349741 | Nie et al. | 2025 | Citations: 4]",
                "snippets": "\u2022 Causal decoder. As a representative decoder-only architecture, causal decoder models introduce the unidirectional attention mask to ensure that each input token can only attend to the past tokens and itself. This mechanism makes them suitable for text generation tasks. Prominent examples are the GPTseries models (Radford et al., 2018(Radford et al., 2019)(Brown et al., 2020). \n\n\u2022 Non-causal decoder. Another kind of decoder-only architecture is the non-casual structure. This architecture performs bidirectional attention on prefix tokens and unidirectional attention only on generated tokens. One representative prefix decoder LLMs is GLM (Zeng et al., 2022).",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {
                    "[218971783 | Brown et al. | 2020 | Citations: 42437]": "Recent work has demonstrated substantial gains on many NLP tasks and benchmarks by pre-training on a large corpus of text followed by fine-tuning on a specific task. While typically task-agnostic in architecture, this method still requires task-specific fine-tuning datasets of thousands or tens of thousands of examples. By contrast, humans can generally perform a new language task from only a few examples or from simple instructions - something which current NLP systems still largely struggle to do. Here we show that scaling up language models greatly improves task-agnostic, few-shot performance, sometimes even reaching competitiveness with prior state-of-the-art fine-tuning approaches. Specifically, we train GPT-3, an autoregressive language model with 175 billion parameters, 10x more than any previous non-sparse language model, and test its performance in the few-shot setting. For all tasks, GPT-3 is applied without any gradient updates or fine-tuning, with tasks and few-shot demonstrations specified purely via text interaction with the model. GPT-3 achieves strong performance on many NLP datasets, including translation, question-answering, and cloze tasks, as well as several tasks that require on-the-fly reasoning or domain adaptation, such as unscrambling words, using a novel word in a sentence, or performing 3-digit arithmetic. At the same time, we also identify some datasets where GPT-3's few-shot learning still struggles, as well as some datasets where GPT-3 faces methodological issues related to training on large web corpora. Finally, we find that GPT-3 can generate samples of news articles which human evaluators have difficulty distinguishing from articles written by humans. We discuss broader societal impacts of this finding and of GPT-3 in general."
                },
                "metadata": [
                    {
                        "section_title": "Architecture",
                        "pdf_hash": "",
                        "start": 394,
                        "end": 1061,
                        "sentence_offsets": [
                            {
                                "start": 394,
                                "end": 411
                            },
                            {
                                "start": 412,
                                "end": 603
                            },
                            {
                                "start": 604,
                                "end": 665
                            },
                            {
                                "start": 666,
                                "end": 776
                            },
                            {
                                "start": 779,
                                "end": 800
                            },
                            {
                                "start": 801,
                                "end": 871
                            },
                            {
                                "start": 872,
                                "end": 994
                            },
                            {
                                "start": 995,
                                "end": 1061
                            }
                        ],
                        "ref_mentions": [
                            "160025533",
                            "218971783"
                        ],
                        "quote": "\u2022 Causal decoder. As a representative decoder-only architecture, causal decoder models introduce the unidirectional attention mask to ensure that each input token can only attend to the past tokens and itself. This mechanism makes them suitable for text generation tasks. Prominent examples are the GPTseries models (Radford et al., 2018(Radford et al., 2019)(Brown et al., 2020). \n\n\u2022 Non-causal decoder. Another kind of decoder-only architecture is the non-casual structure. This architecture performs bidirectional attention on prefix tokens and unidirectional attention only on generated tokens. One representative prefix decoder LLMs is GLM (Zeng et al., 2022)."
                    }
                ]
            },
            {
                "idx": 34,
                "key": "[277626724 | Zhang et al. | 2025 | Citations: 0]",
                "snippets": "Decoder-only pretraining often adopts causal language modeling on a single sequence. In contrast, encoder-decoder adaptation requires separate input and target sequences to be fed to the encoder and decoder separately.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {},
                "metadata": [
                    {
                        "section_title": "Pretraining Objective",
                        "pdf_hash": "",
                        "start": 0,
                        "end": 218,
                        "sentence_offsets": [
                            {
                                "start": 0,
                                "end": 84
                            },
                            {
                                "start": 85,
                                "end": 218
                            }
                        ],
                        "ref_mentions": [],
                        "quote": "Decoder-only pretraining often adopts causal language modeling on a single sequence. In contrast, encoder-decoder adaptation requires separate input and target sequences to be fed to the encoder and decoder separately."
                    }
                ]
            },
            {
                "idx": 35,
                "key": "[277626915 | Rankovi'c et al. | 2025 | Citations: 1]",
                "snippets": "LLMs can follow different architectural designs: encoder-only (e.g., BERT 2), decoder-only (e.g., Qwen 52), and encoder-decoder (e.g., T5 49). Encoder-based models process the full input bidirectionally and are suited for classification and regression. Decoder-only models generate text autoregressively with causal masking. Encoder-decoder models combine both components and are often used for sequence-to-sequence tasks.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {
                    "[204838007 | Raffel et al. | 2019 | Citations: 20336]": "Transfer learning, where a model is first pre-trained on a data-rich task before being fine-tuned on a downstream task, has emerged as a powerful technique in natural language processing (NLP). The effectiveness of transfer learning has given rise to a diversity of approaches, methodology, and practice. In this paper, we explore the landscape of transfer learning techniques for NLP by introducing a unified framework that converts every language problem into a text-to-text format. Our systematic study compares pre-training objectives, architectures, unlabeled datasets, transfer approaches, and other factors on dozens of language understanding tasks. By combining the insights from our exploration with scale and our new \"Colossal Clean Crawled Corpus\", we achieve state-of-the-art results on many benchmarks covering summarization, question answering, text classification, and more. To facilitate future work on transfer learning for NLP, we release our dataset, pre-trained models, and code."
                },
                "metadata": [
                    {
                        "section_title": "B.5 Large Language Models",
                        "pdf_hash": "",
                        "start": 428,
                        "end": 850,
                        "sentence_offsets": [
                            {
                                "start": 340,
                                "end": 516
                            },
                            {
                                "start": 519,
                                "end": 664
                            },
                            {
                                "start": 665,
                                "end": 775
                            },
                            {
                                "start": 776,
                                "end": 848
                            },
                            {
                                "start": 849,
                                "end": 947
                            }
                        ],
                        "ref_mentions": [
                            "204838007"
                        ],
                        "quote": "LLMs can follow different architectural designs: encoder-only (e.g., BERT 2), decoder-only (e.g., Qwen 52), and encoder-decoder (e.g., T5 49). Encoder-based models process the full input bidirectionally and are suited for classification and regression. Decoder-only models generate text autoregressively with causal masking. Encoder-decoder models combine both components and are often used for sequence-to-sequence tasks."
                    }
                ]
            },
            {
                "idx": 36,
                "key": "[277857043 | Beiranvand et al. | 2025 | Citations: 0]",
                "snippets": "Encoder-only models, such as BERT [18], are designed to generate holistic representations of input sequences. These models take an entire sequence as input and process it bidirectionally-each token has access to the full left and right context during training. This characteristic allows the model to deeply capture semantic dependencies across the input.\n\nIn contrast, decoder-only models, such as GPT [19], operate unidirectionally. They are trained in an autoregressive fashion, where each token is generated based only on preceding tokens. The model has no access to future inputs during training or inference, enforcing a strict left-to-right dependency. This makes them particularly well-suited for generative tasks such as open-ended text generation or dialogue modelling.\n\nThe pretraining objective for decoder-only models is Causal Language Modelling (CLM), where the model learns to predict the next token in a sequence, given all previous ones. The associated loss is defined as: \n\nHere, each token x t is conditioned solely on the sequence x <t , making the model capable of generating coherent text step by step.\n\nThe distinction between encoder-only and decoder-only models is foundational: the former are optimized for understanding input context and are widely used in classification and embedding tasks, while the latter are tailored for sequential prediction and text generation.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {},
                "metadata": [
                    {
                        "quote": "Encoder-only models, such as BERT [18], are designed to generate holistic representations of input sequences. These models take an entire sequence as input and process it bidirectionally-each token has access to the full left and right context during training. This characteristic allows the model to deeply capture semantic dependencies across the input.\n\nIn contrast, decoder-only models, such as GPT [19], operate unidirectionally. They are trained in an autoregressive fashion, where each token is generated based only on preceding tokens. The model has no access to future inputs during training or inference, enforcing a strict left-to-right dependency. This makes them particularly well-suited for generative tasks such as open-ended text generation or dialogue modelling.\n\nThe pretraining objective for decoder-only models is Causal Language Modelling (CLM), where the model learns to predict the next token in a sequence, given all previous ones. The associated loss is defined as: \n\nHere, each token x t is conditioned solely on the sequence x <t , making the model capable of generating coherent text step by step.\n\nThe distinction between encoder-only and decoder-only models is foundational: the former are optimized for understanding input context and are widely used in classification and embedding tasks, while the latter are tailored for sequential prediction and text generation.",
                        "pdf_hash": ""
                    }
                ]
            }
        ]
    },
    "cluster": {
        "cost": 0.041718,
        "cot": "For this query asking about the differences between encoder, decoder, causal non-causal, and decoder-only LMs, I need to structure the answer in dimensions that address the key architectural differences, functionality, and use cases of these model types.\n\n1. \"Introduction to Language Model Architectures\" - This should be a synthesis section providing background on the main types of transformer architectures and setting the stage for understanding their differences. This section will help users understand the basic concepts before diving into specific differences.\n\n2. \"Encoder Architectures\" - This should be a synthesis section covering the characteristics of encoder models, including their bidirectional attention mechanisms and typical use cases. This will address part of the query about encoders.\n\n3. \"Decoder Architectures\" - This should be a synthesis section explaining the general characteristics of decoder architectures before diving into specific types. This covers a core part of the query.\n\n4. \"Causal Decoder Models\" - This should be a synthesis section that describes causal decoder models, their unidirectional attention mechanism, and applications. This directly addresses the causal LMs aspect of the query.\n\n5. \"Non-Causal Decoder Models\" - This should be a synthesis section covering non-causal/prefix decoder models and how they differ from causal decoders. This directly addresses the non-causal aspect of the query.\n\n6. \"Encoder-Decoder Models\" - This should be a synthesis section explaining how encoder-decoder models combine elements from both architectures. This addresses another major part of the query.\n\n7. \"Comparison of Model Architectures\" - This should be a synthesis section that directly compares the different architectures in terms of attention mechanisms, parameter efficiency, and typical applications. This gives a comprehensive answer to the main query about differences.\n\nAll sections should be synthesis rather than lists because the query is asking for understanding the differences between model architectures, which requires explaining concepts and making comparisons rather than simply listing items.",
        "plan": {
            "Introduction to Language Model Architectures (synthesis)": [],
            "Encoder Architectures (synthesis)": [
                19,
                22,
                23,
                24,
                31,
                32,
                35,
                36
            ],
            "Decoder Architectures (synthesis)": [
                9,
                11,
                12,
                16,
                17,
                18,
                20,
                21,
                26,
                27,
                29,
                30,
                32,
                34
            ],
            "Causal Decoder Models (synthesis)": [
                2,
                6,
                11,
                12,
                13,
                15,
                16,
                17,
                18,
                20,
                21,
                23,
                24,
                25,
                27,
                28,
                29,
                30,
                33,
                36
            ],
            "Non-Causal Decoder Models (synthesis)": [
                1,
                2,
                4,
                5,
                7,
                17,
                18,
                21,
                27,
                28,
                33
            ],
            "Encoder-Decoder Models (synthesis)": [
                0,
                3,
                5,
                8,
                9,
                10,
                13,
                14,
                15,
                16,
                17,
                18,
                19,
                20,
                21,
                23,
                25,
                27,
                28,
                30,
                32,
                35
            ],
            "Comparison of Model Architectures (synthesis)": [
                0,
                2,
                5,
                6,
                8,
                10,
                12,
                15,
                16,
                17,
                19,
                21,
                22,
                26,
                28,
                31,
                35,
                36
            ]
        },
        "model": "claude-3-7-sonnet-20250219"
    },
    "summary": {
        "sections": [
            {
                "title": "Introduction to Language Model Architectures",
                "tldr": "Language models have different architectures that determine how they process and generate text. The main architectures include encoder-only, decoder-only, and encoder-decoder models, with variations in whether they process text causally (one direction) or non-causally (bidirectionally). (LLM Memory)",
                "text": "\nLanguage models (LMs) are neural network architectures designed to understand, process, and generate human language. These models differ primarily in their architectural design and how they handle information flow, which directly impacts their capabilities and applications.\n\nThere are three fundamental architecture types for language models:\n\n1. **Encoder-only models**: These models process input text by transforming it into contextual representations. They excel at understanding text but aren't designed for text generation. Encoders typically use bidirectional (non-causal) attention, allowing them to consider context from both before and after each word.\n\n2. **Decoder-only models**: These are specialized for text generation. They can be either causal (autoregressive), where they only look at previous tokens when generating the next one, or non-causal, where they can look in both directions. Most large language models today use causal decoder-only architectures.\n\n3. **Encoder-decoder models**: These combine both components - an encoder to understand the input and a decoder to generate output. This architecture is particularly effective for tasks requiring transformation between text sequences, such as translation or summarization.\n\nThe key distinction between causal and non-causal models is the directionality of information flow. Causal models only look at previous tokens when predicting the next one (left-to-right processing), while non-causal models can access the entire context bidirectionally. This fundamental difference shapes what tasks each architecture excels at. <Model name=\"Anthropic\" version=\"claude-3-7-sonnet-20250219\">",
                "citations": [],
                "format": "synthesis",
                "table": null,
                "model": "claude-3-7-sonnet-20250219"
            },
            {
                "title": "Encoder Architectures",
                "tldr": "Encoder-only models use bidirectional attention mechanisms to understand input text comprehensively by processing context from both directions. They excel at tasks requiring deep semantic understanding but are not designed for text generation. (8 sources)",
                "text": "\nEncoder-only models, exemplified by BERT (Bidirectional Encoder Representations from Transformers), process input sequences through bidirectional self-attention mechanisms, allowing them to capture context from both before and after each token in a sequence <Paper corpusId=\"52967399\" paperTitle=\"(Devlin et al., 2019)\" isShortName></Paper>. This bidirectional processing is a defining characteristic that fundamentally differentiates encoders from decoder architectures <Paper corpusId=\"269148552\" paperTitle=\"(Xu et al., 2024)\" isShortName></Paper> <Paper corpusId=\"269982953\" paperTitle=\"(Leemann et al., 2024)\" isShortName></Paper>.\n\nThe training objective for encoder models typically involves masked language modeling (MLM), where random tokens in the input are masked, and the model must predict these masked tokens based on their surrounding context <Paper corpusId=\"269148552\" paperTitle=\"(Xu et al., 2024)\" isShortName></Paper>. This denoising approach teaches the model to develop rich contextual representations by understanding the relationships between words in both directions <Paper corpusId=\"267211690\" paperTitle=\"(Uludougan et al., 2024)\" isShortName></Paper>.\n\nEncoder architectures are particularly well-suited for tasks that require deep semantic understanding of input text rather than generation <Paper corpusId=\"267211690\" paperTitle=\"(Uludougan et al., 2024)\" isShortName></Paper>. These include:\n\n- Classification and regression tasks <Paper corpusId=\"277626915\" paperTitle=\"(Rankovi'c et al., 2025)\" isShortName></Paper>\n- Feature extraction and sentiment analysis <Paper corpusId=\"268247581\" paperTitle=\"(Pei et al., 2024)\" isShortName></Paper>\n- Understanding complex semantic relationships within text <Paper corpusId=\"277857043\" paperTitle=\"(Beiranvand et al., 2025)\" isShortName></Paper>\n\nUnlike decoder models, encoders do not generate text sequentially or employ causal masking. Instead, they develop holistic representations where each token has access to the complete context, making them powerful tools for comprehension tasks <Paper corpusId=\"277857043\" paperTitle=\"(Beiranvand et al., 2025)\" isShortName></Paper>. This bidirectional attention capability is so valuable that researchers have explored adapting causal decoder-only models to include bidirectional attention for encoder-centric tasks, resulting in significant performance improvements <Paper corpusId=\"276771845\" paperTitle=\"(Suganthan et al., 2025)\" isShortName></Paper>.\n\nIn implementation, encoder-only models like BERT typically use a special classification token () at the beginning of the input sequence, which aggregates information from the entire sequence and serves as the basis for classification decisions <Paper corpusId=\"269982953\" paperTitle=\"(Leemann et al., 2024)\" isShortName></Paper>. This architectural choice further emphasizes their focus on understanding rather than generation.",
                "citations": [
                    {
                        "id": "(Devlin et al., 2019)",
                        "snippets": [
                            "We introduce a new language representation model called BERT, which stands for Bidirectional Encoder Representations from Transformers. Unlike recent language representation models (Peters et al., 2018a; Radford et al., 2018), BERT is designed to pre-train deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers. As a result, the pre-trained BERT model can be fine-tuned with just one additional output layer to create state-of-the-art models for a wide range of tasks, such as question answering and language inference, without substantial task-specific architecture modifications. BERT is conceptually simple and empirically powerful. It obtains new state-of-the-art results on eleven natural language processing tasks, including pushing the GLUE score to 80.5 (7.7 point absolute improvement), MultiNLI accuracy to 86.7% (4.6% absolute improvement), SQuAD v1.1 question answering Test F1 to 93.2 (1.5 point absolute improvement) and SQuAD v2.0 Test F1 to 83.1 (5.1 point absolute improvement)."
                        ],
                        "paper": {
                            "corpus_id": 52967399,
                            "title": "BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding",
                            "authors": [
                                {
                                    "authorId": "39172707",
                                    "name": "Jacob Devlin"
                                },
                                {
                                    "authorId": "1744179",
                                    "name": "Ming-Wei Chang"
                                },
                                {
                                    "authorId": "2544107",
                                    "name": "Kenton Lee"
                                },
                                {
                                    "authorId": "3259253",
                                    "name": "Kristina Toutanova"
                                }
                            ],
                            "year": 2019,
                            "venue": "North American Chapter of the Association for Computational Linguistics",
                            "n_citations": 95215
                        },
                        "score": 0
                    },
                    {
                        "id": "(Xu et al., 2024)",
                        "snippets": [
                            "Encoder-only LLMs, such as BERT [23] and its variants like CodeBERT [46], have a bidirectional transformer encoder structure. They are typically trained on the masked language modeling objective (i.e., MLM), aiming to denoise and reconstruct the masked tokens via understanding the surrounding context (Fig. 1). As shown in Eq. 1, the loss of MLM training objective can be explicitly represented as the log-sum of the conditional probabilities of generating the masked token when all the unmasked tokens are known.\n\nDecoder-only LLMs, including GPT series [13], [27] and LLaMA series [47], have an autoregressive transformer decoder structure. They are mainly trained on the causal language modeling objective (i.e., CLM), aiming to predict and complete next tokens via following the prefix context (Fig. 1). As shown in Eq. 2, the loss of CLM training objective can be explicitly represented as the log-sum of the conditional probabilities of generating the next token when all the preceeding tokens are known.\n\nEncoder-decoder LLMs, such as T5 [24] and its variants like CodeT5 [48], have a complete transformer structure."
                        ],
                        "paper": {
                            "corpus_id": 269148552,
                            "title": "Aligning the Objective of LLM-based Program Repair",
                            "authors": [
                                {
                                    "authorId": "2186630124",
                                    "name": "Junjielong Xu"
                                },
                                {
                                    "authorId": "2296726489",
                                    "name": "Ying Fu"
                                },
                                {
                                    "authorId": "2297427913",
                                    "name": "Shin Hwei Tan"
                                },
                                {
                                    "authorId": "2265706586",
                                    "name": "Pinjia He"
                                }
                            ],
                            "year": 2024,
                            "venue": "",
                            "n_citations": 8
                        },
                        "score": 0.71240234375
                    },
                    {
                        "id": "(Leemann et al., 2024)",
                        "snippets": [
                            "Practical implementations introduce subtle modifications into the process described previously.The most relevant distinction is made between encoder-only models, that include BERT [15] and its variants, and decoder-only models such as the GPT models [38,(Radford et al., 2019).\n\nEncoder-only models.Considering BERT as an example of an encoder-only model, the first token is used for the classification, i.e, r = 1.Usually, a special token [CLS] is prepended to the text at position 1, however this is not strictly necessary for the functioning of the model.\n\nDecoder-only models.In contrast, decoder-models like GPT-2 (Radford et al., 2019) add the classification head on top of the last token for classification, i.e., r = |t|.A key difference is that in GPT-2 and other decoder-only models, a causal mask is laid over the attention matrix, resulting in \u03b1 i,j = 0 for j > i.This encodes the constraint that tokens can only attend to themselves or to previous ones."
                        ],
                        "paper": {
                            "corpus_id": 269982953,
                            "title": "Attention Mechanisms Don't Learn Additive Models: Rethinking Feature Importance for Transformers",
                            "authors": [
                                {
                                    "authorId": "2130899453",
                                    "name": "Tobias Leemann"
                                },
                                {
                                    "authorId": "2302795616",
                                    "name": "Alina Fastowski"
                                },
                                {
                                    "authorId": "2317114785",
                                    "name": "Felix Pfeiffer"
                                },
                                {
                                    "authorId": "1686448",
                                    "name": "Gjergji Kasneci"
                                }
                            ],
                            "year": 2024,
                            "venue": "Trans. Mach. Learn. Res.",
                            "n_citations": 5
                        },
                        "score": 0.69287109375
                    },
                    {
                        "id": "(Uludougan et al., 2024)",
                        "snippets": [
                            "Models that exclusively employ encoders, typically trained with denoising objectives, are geared toward understanding tasks, as exemplified in works such as (Devlin et al., 2019) and (208229926). Conversely, models that exclusively use decoders are designed for generation tasks, employing causal language modeling, as demonstrated in various studies (Radford et al., 2019)Brown et al., 2020;Touvron et al., 2023). The Text-to-Text Transformer (T5) (Raffel et al., 2019), on the other hand, employs an encoder-decoder architecture and undergoes pretraining with a denoising objective referred to as span corruption. UniLM (Dong et al., 2019) is also an encoder-decoder model, but pre-trained using unidirectional, bidirectional, and sequence-to-sequence language modeling. This can be seen as a combination of causal and denoising objectives."
                        ],
                        "paper": {
                            "corpus_id": 267211690,
                            "title": "TURNA: A Turkish Encoder-Decoder Language Model for Enhanced Understanding and Generation",
                            "authors": [
                                {
                                    "authorId": "2281033147",
                                    "name": "Gokcce Uludougan"
                                },
                                {
                                    "authorId": "2281033141",
                                    "name": "Zeynep Yirmibecsouglu Balal"
                                },
                                {
                                    "authorId": "2174736343",
                                    "name": "Furkan Akkurt"
                                },
                                {
                                    "authorId": "2281033264",
                                    "name": "Melikcsah Turker"
                                },
                                {
                                    "authorId": "9179697",
                                    "name": "Onur Gungor"
                                },
                                {
                                    "authorId": "66493576",
                                    "name": "S. Uskudarli"
                                }
                            ],
                            "year": 2024,
                            "venue": "Annual Meeting of the Association for Computational Linguistics",
                            "n_citations": 12
                        },
                        "score": 0.64208984375
                    },
                    {
                        "id": "(Rankovi'c et al., 2025)",
                        "snippets": [
                            "LLMs can follow different architectural designs: encoder-only (e.g., BERT 2), decoder-only (e.g., Qwen 52), and encoder-decoder (e.g., T5 49). Encoder-based models process the full input bidirectionally and are suited for classification and regression. Decoder-only models generate text autoregressively with causal masking. Encoder-decoder models combine both components and are often used for sequence-to-sequence tasks."
                        ],
                        "paper": {
                            "corpus_id": 277626915,
                            "title": "GOLLuM: Gaussian Process Optimized LLMs - Reframing LLM Finetuning through Bayesian Optimization",
                            "authors": [
                                {
                                    "authorId": "2219925647",
                                    "name": "Bojana Rankovi'c"
                                },
                                {
                                    "authorId": "2239074343",
                                    "name": "Philippe Schwaller"
                                }
                            ],
                            "year": 2025,
                            "venue": "arXiv.org",
                            "n_citations": 1
                        },
                        "score": 0.703125
                    },
                    {
                        "id": "(Pei et al., 2024)",
                        "snippets": [
                            "Encoder-only models (Devlin et al., 2019) specialize in processing input sequences of biomolecules and text through bi-directional self-attention, making them highly effective for tasks that require an in-depth understanding of the input, such as sentiment analysis and feature extraction in NLP. Thereby in biomolecule domain, encoder-only models establish a bi-directional association between biotokens and text tokens for predictive tasks (Zeng et al., 2022)",
                            "In contrast, decoder-only models [18], [138] employ causal attention to focus on the sequence of previous tokens. This architecture is typically utilized in generative tasks, such as generating text descriptions that match the given molecule or for the reverse task (Liu et al., 2023). Thanks to the autoregressive generation property, decoder-only models are well suitable for instruction following and assistant/agent objectives."
                        ],
                        "paper": {
                            "corpus_id": 268247581,
                            "title": "Leveraging Biomolecule and Natural Language through Multi-Modal Learning: A Survey",
                            "authors": [
                                {
                                    "authorId": "2171652249",
                                    "name": "Qizhi Pei"
                                },
                                {
                                    "authorId": "47767791",
                                    "name": "Lijun Wu"
                                },
                                {
                                    "authorId": "1944690382",
                                    "name": "Kaiyuan Gao"
                                },
                                {
                                    "authorId": "151068900",
                                    "name": "Jinhua Zhu"
                                },
                                {
                                    "authorId": "2290062348",
                                    "name": "Yue Wang"
                                },
                                {
                                    "authorId": "2290024261",
                                    "name": "Zun Wang"
                                },
                                {
                                    "authorId": "2267250090",
                                    "name": "Tao Qin"
                                },
                                {
                                    "authorId": "2257028545",
                                    "name": "Rui Yan"
                                }
                            ],
                            "year": 2024,
                            "venue": "arXiv.org",
                            "n_citations": 18
                        },
                        "score": 0.64990234375
                    },
                    {
                        "id": "(Beiranvand et al., 2025)",
                        "snippets": [
                            "Encoder-only models, such as BERT [18], are designed to generate holistic representations of input sequences. These models take an entire sequence as input and process it bidirectionally-each token has access to the full left and right context during training. This characteristic allows the model to deeply capture semantic dependencies across the input.\n\nIn contrast, decoder-only models, such as GPT [19], operate unidirectionally. They are trained in an autoregressive fashion, where each token is generated based only on preceding tokens. The model has no access to future inputs during training or inference, enforcing a strict left-to-right dependency. This makes them particularly well-suited for generative tasks such as open-ended text generation or dialogue modelling.\n\nThe pretraining objective for decoder-only models is Causal Language Modelling (CLM), where the model learns to predict the next token in a sequence, given all previous ones. The associated loss is defined as: \n\nHere, each token x t is conditioned solely on the sequence x <t , making the model capable of generating coherent text step by step.\n\nThe distinction between encoder-only and decoder-only models is foundational: the former are optimized for understanding input context and are widely used in classification and embedding tasks, while the latter are tailored for sequential prediction and text generation."
                        ],
                        "paper": {
                            "corpus_id": 277857043,
                            "title": "Integrating Structural and Semantic Signals in Text-Attributed Graphs with BiGTex",
                            "authors": [
                                {
                                    "authorId": "66841167",
                                    "name": "Azadeh Beiranvand"
                                },
                                {
                                    "authorId": "35409259",
                                    "name": "S. M. Vahidipour"
                                }
                            ],
                            "year": 2025,
                            "venue": "arXiv.org",
                            "n_citations": 0
                        },
                        "score": 0.810546875
                    },
                    {
                        "id": "(Suganthan et al., 2025)",
                        "snippets": [
                            "A key question thus arises: can we effectively adapt the powerful knowledge embedded in decoder-only models to excel in these encoder-centric tasks?",
                            "Gemma's causal attention, ideal for generative tasks, inherently limits its applicability to encoder-based tasks. We demonstrate that simply enabling bidirectional attention during fine-tuning dramatically improves performance."
                        ],
                        "paper": {
                            "corpus_id": 276771845,
                            "title": "Adapting Decoder-Based Language Models for Diverse Encoder Downstream Tasks",
                            "authors": [
                                {
                                    "authorId": "1658871094",
                                    "name": "P. Suganthan"
                                },
                                {
                                    "authorId": "2165469946",
                                    "name": "Fedor Moiseev"
                                },
                                {
                                    "authorId": "2348489099",
                                    "name": "Le Yan"
                                },
                                {
                                    "authorId": "2261361394",
                                    "name": "Junru Wu"
                                },
                                {
                                    "authorId": "2348507846",
                                    "name": "Jianmo Ni"
                                },
                                {
                                    "authorId": "2348488953",
                                    "name": "Jay Han"
                                },
                                {
                                    "authorId": "1954563",
                                    "name": "I. Zitouni"
                                },
                                {
                                    "authorId": "1727837",
                                    "name": "Enrique Alfonseca"
                                },
                                {
                                    "authorId": "2348422460",
                                    "name": "Xuanhui Wang"
                                },
                                {
                                    "authorId": "2349772191",
                                    "name": "Zhe Dong"
                                }
                            ],
                            "year": 2025,
                            "venue": "arXiv.org",
                            "n_citations": 1
                        },
                        "score": 0.63134765625
                    }
                ],
                "format": "synthesis",
                "table": null,
                "model": "claude-3-7-sonnet-20250219"
            },
            {
                "title": "Decoder Architectures",
                "tldr": "Decoder architectures are designed primarily for text generation and come in two main variants: causal decoders that only attend to past tokens (unidirectional) and prefix decoders that allow bidirectional attention over prefix tokens while maintaining unidirectional attention for generation. (13 sources)",
                "text": "\nDecoder architectures form a fundamental class of language models that specialize in sequential text generation. Unlike encoders, which focus on understanding input text, decoders are designed to generate new tokens based on previous context. There are two principal types of decoder architectures: causal decoders and prefix decoders.\n\nCausal decoder models, exemplified by GPT-series models, employ unidirectional (left-to-right) attention mechanisms where each token can only attend to itself and preceding tokens in the sequence <Paper corpusId=\"258947629\" paperTitle=\"(Roberts, 2023)\" isShortName></Paper>. This causal masking is what enables these models to predict the next token based on previously observed tokens, making them suitable for autoregressive text generation <Paper corpusId=\"270702559\" paperTitle=\"(Yin et al., 2024)\" isShortName></Paper> <Paper corpusId=\"266755678\" paperTitle=\"(Liu et al., 2024)\" isShortName></Paper>. The unidirectional nature of causal decoders is their defining characteristic, as it preserves the causality needed for coherent text generation <Paper corpusId=\"277128409\" paperTitle=\"(Ghosh et al., 2024)\" isShortName></Paper>.\n\nIn contrast, prefix decoder models represent a hybrid approach. They allow bidirectional attention for a portion of the input sequence (the prefix) while maintaining unidirectional attention for the tokens being generated <Paper corpusId=\"266755678\" paperTitle=\"(Liu et al., 2024)\" isShortName></Paper> <Paper corpusId=\"270702559\" paperTitle=\"(Yin et al., 2024)\" isShortName></Paper>. This configuration enables prefix decoders to bidirectionally encode the input context while still generating output tokens autoregressively, combining the comprehensive understanding capabilities of encoders with the generative abilities of decoders <Paper corpusId=\"268157336\" paperTitle=\"(Patil et al., 2024)\" isShortName></Paper>.\n\nA key distinction between decoder architectures and encoder architectures lies in their attention mechanisms and optimization objectives. While encoders typically use masked language modeling with bidirectional attention, decoders are usually trained with next-token prediction using causal attention <Paper corpusId=\"270560675\" paperTitle=\"(Ma et al., 2024)\" isShortName></Paper> <Paper corpusId=\"277626724\" paperTitle=\"(Zhang et al., 2025)\" isShortName></Paper>. This difference in attention mechanisms leads to cumulative quantization errors in causal attention, particularly affecting tokens appearing later in the sequence <Paper corpusId=\"260886785\" paperTitle=\"(Kim et al., 2023)\" isShortName></Paper>.\n\nDecoder-only models have gained popularity due to several advantages over encoder-decoder models: they feature a simpler structure, faster training and inference speeds, and are particularly well-suited for pure generation tasks <Paper corpusId=\"267402678\" paperTitle=\"(Li, 2024)\" isShortName></Paper>. While encoder-decoder models process input and output sequences separately, decoder-only models typically concatenate input and target sequences, applying appropriate masking mechanisms throughout <Paper corpusId=\"258049081\" paperTitle=\"(Fu et al., 2023)\" isShortName></Paper> <Paper corpusId=\"268157336\" paperTitle=\"(Patil et al., 2024)\" isShortName></Paper>.\n\nIn implementation, decoder models differ from encoders in how they process information. Encoders compute attention from scratch for each token prediction, allowing all tokens to attend to each other <Paper corpusId=\"273025546\" paperTitle=\"(Ewer et al., 2024)\" isShortName></Paper>. Decoders, however, use sequential processing where previously generated tokens serve as context for future generations <Paper corpusId=\"263729712\" paperTitle=\"(Alomari et al., 2023)\" isShortName></Paper>. This fundamental difference in information flow shapes the applications each architecture excels at, with decoders being particularly effective for natural language generation tasks <Paper corpusId=\"276423946\" paperTitle=\"(Busto-Castineira et al., 2025)\" isShortName></Paper>.",
                "citations": [
                    {
                        "id": "(Roberts, 2023)",
                        "snippets": [
                            "In this article we prove that the general transformer neural model undergirding modern large language models (LLMs) is Turing complete under reasonable assumptions",
                            "Differentiating Encoder-only and Decoder-only Models: Decoder-only models have three necessary characteristics which are derived from their function in the vanilla transformer. The decoder must (1) provide a means of autoregressively predicting the next token based on the tokens generated so far given the encoder input as contextualization. In 2 this is shown as the recursive red connection mapping the output vector back into the last element of the input sequence of vectors. To be suited to this task, decoder-only models must (2) not see future values when evaluating a query on the input sequence of vectors. This is why decoder-only models are often referred to as causal language models (CLM). In 2, we refer to the decoder attention heads as causal attention heads rather than masked attention heads as they are called in (Vaswani et al., 2017). The model must be (3) trained to predict the next token given the current input sequence of vectors. This training method coupled with recursion allows decoder-only models to autoregressively generate arbitrarily long (up to the max size of the input vector sequence) sequences."
                        ],
                        "paper": {
                            "corpus_id": 258947629,
                            "title": "How Powerful are Decoder-Only Transformer Neural Models?",
                            "authors": [
                                {
                                    "authorId": "2115904887",
                                    "name": "Jesse Roberts"
                                }
                            ],
                            "year": 2023,
                            "venue": "IEEE International Joint Conference on Neural Network",
                            "n_citations": 19
                        },
                        "score": 0.8037109375
                    },
                    {
                        "id": "(Yin et al., 2024)",
                        "snippets": [
                            "Typical architectures for LLMs can be categorized as encoder-decoder, causal decoder, and prefix decoder (Wang and Hesslow, 2022;Zhao et al., 2023).Among them, the causal decoder architecture is the most frequently used by various LLMs, such as OPT (Susan Zhang et al., 2023), LLAMA (Touvron et al., 2023a), BLOOM (Scao et al., 2023) due to its superior zero-shot and fewshot generalization capacity (Wang and Hesslow, 2022) and the effectiveness of scaling law (Brown et al., 2020;Kaplan et al., 2020).The causal decoder architecture employs a unidirectional attention mask, ensuring that each input token can only attend to preceding tokens and itself.Both input and output tokens are processed identically through the decoder (Zhao et al., 2023).\n\nThe prefix decoder architecture, also known as the non-causal decoder, modifies the masking mechanism used in causal decoders (Zhang et al., 2020).This adjustment allows for bidirectional attention over the prefix tokens while restricting attention to unidirectional generated tokens (Dong et al., 2019).Similar to the encoder-decoder architecture, prefix decoders can bidirectionally encode the prefix sequence and autoregressively predict the output tokens sequentially (Zhao et al., 2023).The same parameters are shared during both encoding and decoding processes.Representative LLMs based on prefix decoders include GLM-130B (Zeng et al., 2023) and U-PaLM (Tay et al., 2022).\n\nThe vanilla Transformer model is constructed on the encoder-decoder architecture (Vaswani et al., 2017), comprising two stacks of Transformer blocks designated as the encoder and decoder, respectively.The encoder utilizes stacked multi-head self-attention layers to encode the input sequence into its latent representations.Meanwhile, the decoder employs cross-attention mechanisms on these representations, and autoregressive generates the target sequence."
                        ],
                        "paper": {
                            "corpus_id": 270702559,
                            "title": "CrisisSense-LLM: Instruction Fine-Tuned Large Language Model for Multi-label Social Media Text Classification in Disaster Informatics",
                            "authors": [
                                {
                                    "authorId": "2265383225",
                                    "name": "Kai Yin"
                                },
                                {
                                    "authorId": "2308073678",
                                    "name": "Chengkai Liu"
                                },
                                {
                                    "authorId": "2258714985",
                                    "name": "Ali Mostafavi"
                                },
                                {
                                    "authorId": "2308068627",
                                    "name": "Xia Hu"
                                }
                            ],
                            "year": 2024,
                            "venue": "arXiv.org",
                            "n_citations": 12
                        },
                        "score": 0.74267578125
                    },
                    {
                        "id": "(Liu et al., 2024)",
                        "snippets": [
                            "LLMs with a Decoder-only architecture utilize the decoder component of the traditional Transformer architecture. Unlike the Encoder-Decoder architecture, which incorporates both an encoder and a decoder, the Decoder-only architecture is solely focused on the decoding process. In this configuration, the model sequentially generates tokens, attending to preceding tokens in the sequence. This architecture has been applied to various language generation tasks, showcasing its effectiveness in various tasks such as text generation without the need for an explicit encoding phase. The Decoder-only architecture can be further classified into two categories: the Causal Decoder architecture and the Prefix Decoder architecture. \n\nThe Causal Decoder Architecture: In the Causal Decoder architecture, each token in the model input sequence can only attend to past input tokens and itself during the decoding process. It achieves unidirectional attention to the input sequence by using a specific mask as shown in Figure 1. In fact, different architectures are mainly implemented by configuring different mask matrices. The figure illustrates a comparison of mask configurations between the Encoderdecoder and Decoder-only architectures (including Casual Decoder and Prefix Decoder).\n\nThe Prefix Decoder Architecture: The Prefix Decoder architecture combines the advantages of both the Encoderdecoder and Causal Decoder architectures. It leverages its unique mask configurations, as illustrated in Figure 1, enabling bidirectional attention for tokens in the prefix while maintaining unidirectional attention for generating subsequent tokens [54]. This design allows for the autoregressive generation of the output sequence with the flexibility to attend bi-directionally to the prefix tokens."
                        ],
                        "paper": {
                            "corpus_id": 266755678,
                            "title": "Understanding LLMs: A Comprehensive Overview from Training to Inference",
                            "authors": [
                                {
                                    "authorId": "2116426849",
                                    "name": "Yi-Hsueh Liu"
                                },
                                {
                                    "authorId": "2155082967",
                                    "name": "Haoyang He"
                                },
                                {
                                    "authorId": "2184719751",
                                    "name": "Tianle Han"
                                },
                                {
                                    "authorId": "2273584640",
                                    "name": "Xu Zhang"
                                },
                                {
                                    "authorId": "2210636248",
                                    "name": "Mengyuan Liu"
                                },
                                {
                                    "authorId": "2257433902",
                                    "name": "Jiaming Tian"
                                },
                                {
                                    "authorId": "2257095790",
                                    "name": "Yutong Zhang"
                                },
                                {
                                    "authorId": "2110238778",
                                    "name": "Jiaqi Wang"
                                },
                                {
                                    "authorId": "2277869261",
                                    "name": "Xiaohui Gao"
                                },
                                {
                                    "authorId": "2215167446",
                                    "name": "Tianyang Zhong"
                                },
                                {
                                    "authorId": "2221032216",
                                    "name": "Yi Pan"
                                },
                                {
                                    "authorId": "2211904452",
                                    "name": "Shaochen Xu"
                                },
                                {
                                    "authorId": "2263593041",
                                    "name": "Zihao Wu"
                                },
                                {
                                    "authorId": "2145977326",
                                    "name": "Zheng Liu"
                                },
                                {
                                    "authorId": "2257586495",
                                    "name": "Xin Zhang"
                                },
                                {
                                    "authorId": "2277750447",
                                    "name": "Shu Zhang"
                                },
                                {
                                    "authorId": "1742535",
                                    "name": "Xintao Hu"
                                },
                                {
                                    "authorId": "49104946",
                                    "name": "Tuo Zhang"
                                },
                                {
                                    "authorId": "2251076040",
                                    "name": "Ning Qiang"
                                },
                                {
                                    "authorId": "2254792886",
                                    "name": "Tianming Liu"
                                },
                                {
                                    "authorId": "2257302793",
                                    "name": "Bao Ge"
                                }
                            ],
                            "year": 2024,
                            "venue": "Neurocomputing",
                            "n_citations": 74
                        },
                        "score": 0.8115234375
                    },
                    {
                        "id": "(Ghosh et al., 2024)",
                        "snippets": [
                            "The functioning of the Decoder has many parts in common with the Encoder, yet there are some major differences. In a nutshell, the Decoder processes the output component of the input-output pairs of the training data; encodes such information and produces its own embedding using a multi-head attention scheme similar to the Encoder, but: \n\n1. once the output has been encoded, it is combined with the  and  matrices coming from the Encoder. This is the step where Transformers learn the relation between the input of the training (e.g., the question) and the output (e.g., the answer to the question); considering the question-answer example, the Decoder performs cross-attention on the Encoder output (which represents the question) while processing the answer; 2. at the end of the process a linear layer has a number of output neurons equal to the size of the vocabulary; such network uses a softmax function to produce likelihood for each term in the vocabulary. Then, the term with the highest likelihood is the output of the Decoder i.e. the predicted next word; 3. The Decoder produces the output one word at a time; in the training phase, knowing the correct word, the error is computed and used to drive the backpropagation step and the correction of the weights to reduce the error",
                            ".For example, the weights obtained in pretraining for Encoder-only model like BERT, are based on randomly masked words; on the other hand, for Decoder-only model like GPT, causal masking is used, where only the future tokens are masked, and the model predicts the next token given past tokens."
                        ],
                        "paper": {
                            "corpus_id": 277128409,
                            "title": "A review on the applications of Transformer-based language models for nucleotide sequence analysis",
                            "authors": [
                                {
                                    "authorId": "2343636534",
                                    "name": "Nimisha Ghosh"
                                },
                                {
                                    "authorId": "2299143904",
                                    "name": "Daniele Santoni"
                                },
                                {
                                    "authorId": "2105376090",
                                    "name": "I. Saha"
                                },
                                {
                                    "authorId": "2261390733",
                                    "name": "Giovanni Felici"
                                }
                            ],
                            "year": 2024,
                            "venue": "Computational and Structural Biotechnology Journal",
                            "n_citations": 0
                        },
                        "score": 0.83349609375
                    },
                    {
                        "id": "(Patil et al., 2024)",
                        "snippets": [
                            "Encoder-decoder models adopt bidirectional attention for the encoder, unidirectional attention for the decoder, and a cross-attention mechanism between them. Cross-attention in the decoder has access only to the fully processed encoder output and is responsible for connecting input tokens to target tokens",
                            "Prefix language models are also decoder-only-based models but differ in the masking mechanism. Instead of a causal mask, a fully visible mask is used for the prefix part of the input sequence, and a causal mask is used for the target sequence",
                            "In a decoder-only model, the input and target are concatenated, and then a causal mask is used throughout. A decoder-only model with a prefix allows fully visible masking over part of the input token (prefix), followed by causal masking on the rest of the sequence. In general, autoencoding models learn bidirectional contextualized representation suited for NLU tasks, whereas autoregressive models learn to generate the next token and hence are suited for NLG tasks."
                        ],
                        "paper": {
                            "corpus_id": 268157336,
                            "title": "A Review of Current Trends, Techniques, and Challenges in Large Language Models (LLMs)",
                            "authors": [
                                {
                                    "authorId": "2289385425",
                                    "name": "Rajvardhan Patil"
                                },
                                {
                                    "authorId": "117730513",
                                    "name": "Venkat Gudivada"
                                }
                            ],
                            "year": 2024,
                            "venue": "Applied Sciences",
                            "n_citations": 80
                        },
                        "score": 0.77587890625
                    },
                    {
                        "id": "(Ma et al., 2024)",
                        "snippets": [
                            "As outlined in Sec. 1, we observe two discrepancies between decoder-only LLMs and encoder-decoder models: optimization objective and model architecture. Specifically, the decoder-only LLMs are typically optimized using the next token prediction task while the encoder-decoder models are trained with the masked language modeling task. Besides, the former tokens in a sequence cannot attend the latter tokens in decoder-only LLMs while every token in the sequence can attend each other in the encoder models."
                        ],
                        "paper": {
                            "corpus_id": 270560675,
                            "title": "Exploring the Role of Large Language Models in Prompt Encoding for Diffusion Models",
                            "authors": [
                                {
                                    "authorId": "2261489892",
                                    "name": "Bingqi Ma"
                                },
                                {
                                    "authorId": "1571400317",
                                    "name": "Zhuofan Zong"
                                },
                                {
                                    "authorId": "12920342",
                                    "name": "Guanglu Song"
                                },
                                {
                                    "authorId": "2261394248",
                                    "name": "Hongsheng Li"
                                },
                                {
                                    "authorId": "2261417717",
                                    "name": "Yu Liu"
                                }
                            ],
                            "year": 2024,
                            "venue": "Neural Information Processing Systems",
                            "n_citations": 23
                        },
                        "score": 0.7783203125
                    },
                    {
                        "id": "(Zhang et al., 2025)",
                        "snippets": [
                            "Decoder-only pretraining often adopts causal language modeling on a single sequence. In contrast, encoder-decoder adaptation requires separate input and target sequences to be fed to the encoder and decoder separately."
                        ],
                        "paper": {
                            "corpus_id": 277626724,
                            "title": "Encoder-Decoder Gemma: Improving the Quality-Efficiency Trade-Off via Adaptation",
                            "authors": [
                                {
                                    "authorId": "2354284757",
                                    "name": "Biao Zhang"
                                },
                                {
                                    "authorId": "2165469946",
                                    "name": "Fedor Moiseev"
                                },
                                {
                                    "authorId": "2343748926",
                                    "name": "Joshua Ainslie"
                                },
                                {
                                    "authorId": "1658871094",
                                    "name": "P. Suganthan"
                                },
                                {
                                    "authorId": "2352024723",
                                    "name": "Min Ma"
                                },
                                {
                                    "authorId": "9692128",
                                    "name": "Surya Bhupatiraju"
                                },
                                {
                                    "authorId": "2275184616",
                                    "name": "Federico Lebron"
                                },
                                {
                                    "authorId": "2273534960",
                                    "name": "Orhan Firat"
                                },
                                {
                                    "authorId": "2319608",
                                    "name": "Armand Joulin"
                                },
                                {
                                    "authorId": "2349772191",
                                    "name": "Zhe Dong"
                                }
                            ],
                            "year": 2025,
                            "venue": "arXiv.org",
                            "n_citations": 0
                        },
                        "score": 0.8017578125
                    },
                    {
                        "id": "(Kim et al., 2023)",
                        "snippets": [
                            "Cumulative Quantization Errors in Causal Attention. Causal attention, which integrates masking into self-attention to avoid referencing future tokens, is vital for text generation tasks. To comprehend the quantization characteristics of GLMs, we contrast the computation procedure of the self-attention map. For a Transformer encoder, the quantization error reflected on the self-attention map due to Q, K, and V computation is uniformly spread because of the simultaneous computing nature inherent in Transformer encoders. However, the mask in the causal attention accumulates quantization errors from each token, creating uneven errors in the output computation. \n\nTo elucidate, consider the encoder self-attention depicted at the top of Fig. 1(a). During the computation of the weighted sum of value representations for tokens 1 and 3, all attention probabilities in the self-attention map impacted by quantization errors are incorporated. On the other hand, in the decoder's causal attention, as illustrated at the bottom of Fig. 1(a), the output value representation is computed using only the self-attention probabilities of the current token and those before it. For example, while generating the output value representation for token 1 (highlighted in a blue box), only two attention probabilities affected by quantization error are used. In contrast, for token 3 (indicated by the red box), probabilities from all preceding tokens are incorporated. This observation underscores that the inherent design of causal attention results in an accumulation of quantization errors, especially towards the latter tokens. Given this characteristic, it becomes imperative to devise a decoder QAT approach that mitigates this uneven distribution of quantization errors within the causal attention mechanism. \n\nNecessity of Ground Truth Loss. In fine-tuning processes, encoder-only models, often used in Natural Language Understanding (NLU), and decoder-only models, common in Natural Language Generation (NLG) with causal language modeling approach, exhibit different mechanisms for receiving ground truth loss. Fig. 1(b) illustrates the differences. In NLU tasks, encoder models leverage the representation of a unique special token, transformed into a logit vector, for calculating cross-entropy loss, with the number of classes usually being a few [12]. Conversely, decoder models in NLG tasks predict the subsequent token for each input token, transforming each token representation into a logit vector with a class size equivalent to the vocabulary size, often exceeding 50k [25]. Hence, while encoder models gain ground truth loss from a single special token's logit, decoder models derive it from the output logits of all input tokens, which could provide more detailed token-level prediction information."
                        ],
                        "paper": {
                            "corpus_id": 260886785,
                            "title": "Token-Scaled Logit Distillation for Ternary Weight Generative Language Models",
                            "authors": [
                                {
                                    "authorId": "2141320070",
                                    "name": "Minsoo Kim"
                                },
                                {
                                    "authorId": "2144376191",
                                    "name": "Sihwa Lee"
                                },
                                {
                                    "authorId": "2265920992",
                                    "name": "Janghwan Lee"
                                },
                                {
                                    "authorId": "2158125346",
                                    "name": "S. Hong"
                                },
                                {
                                    "authorId": "2180828053",
                                    "name": "Duhyeuk Chang"
                                },
                                {
                                    "authorId": "66936521",
                                    "name": "Wonyong Sung"
                                },
                                {
                                    "authorId": "2506452",
                                    "name": "Jungwook Choi"
                                }
                            ],
                            "year": 2023,
                            "venue": "Neural Information Processing Systems",
                            "n_citations": 15
                        },
                        "score": 0.93896484375
                    },
                    {
                        "id": "(Li, 2024)",
                        "snippets": [
                            "As shown in Figure 3, similar to most seq2seq models, the Transformer architecture consists of an encoder and a decoder. Overall, LLM (Language Model) models can be categorized into three major types: Encoder-decoder Architecture, Causal Decoder Architecture, and Prefix Decoder Architecture [11]. The Encoder-decoder Architecture uses the most basic structure and was initially introduced by the Seq2Seq model to address sequence-to-sequence tasks, such as machine translation. It consists of an encoder and a decoder. The encoder is responsible for transforming the input sequence into a fixeddimensional semantic representation, while the decoder uses this semantic representation to generate the output sequence. Within the encoder-decoder structure, self-attention mechanisms are commonly employed for sequence modeling, enabling the model to handle variable-length input and output sequences. This architecture has proven to be highly effective in various sequence-to-sequence tasks, such as text translation, text summarization, and dialogue generation. Prominent examples of large language models following this architecture include ELMo, BERT, RoBERTa, among others [12]. \n\nCurrently, the most widely used architecture is the Causal Decoder, which is primarily employed for handling autoregressive generation tasks, where each element of the output sequence depends on previously generated elements. The Causal Decoder Architecture is an improvement over the Encoderdecoder structure, as it introduces an autoregressive mechanism in the decoder. This means that when generating the current element, the model only uses the elements generated before it. This ensures that the model does not have access to future information during the generation process, thereby preserving causality. The GPT series (e.g., GPT-3) is a typical example of models that use the Causal Decoder Architecture. These models generate text by sequentially producing words one by one, avoiding information leakage and enabling the generation of coherent and plausible text. \n\nCompared to the Encoder-decoder models, Decoder-only models offer several advantages due to their simpler structure, faster training and inference speed, suitability for pure generation tasks, and advantages in decoder self-supervision."
                        ],
                        "paper": {
                            "corpus_id": 267402678,
                            "title": "The evolution, applications, and future prospects of large language models: An in-depth overview",
                            "authors": [
                                {
                                    "authorId": "2282449950",
                                    "name": "Jiayin Li"
                                }
                            ],
                            "year": 2024,
                            "venue": "Applied and Computational Engineering",
                            "n_citations": 2
                        },
                        "score": 0.78076171875
                    },
                    {
                        "id": "(Fu et al., 2023)",
                        "snippets": [
                            "Traditionally, most of the seq2seq task is resolved by the Encoder-Decoder framework which requires an encoder to encode the source sequence and a decoder to generate the target text. Recently, a bunch of new approaches have emerged that apply decoder-only language models directly to the seq2seq task. Despite the significant advancements in applying language models to the seq2seq task, there is still a lack of thorough analysis on the effectiveness of the decoder-only language model architecture.\n\nThe main difference between the ED framework and the LM is how the input source information is merged into the decoder. As illustrated in Figure 2, the ED framework first uses multiple Transformer blocks to extract features H E \u22121 from the source sequence s. Afterwards, it utilizes a self attention ATT D l to get the feature matrix G D l . It then uses an encoder attention ATT J l to take G D l as query and uses the encoder's final output H E \u22121 as the key and value to calculate Q D l . On the other hand, an LM uses an unidirectional attention to handle the concatenated features."
                        ],
                        "paper": {
                            "corpus_id": 258049081,
                            "title": "Decoder-Only or Encoder-Decoder? Interpreting Language Model as a Regularized Encoder-Decoder",
                            "authors": [
                                {
                                    "authorId": "1646716323",
                                    "name": "Z. Fu"
                                },
                                {
                                    "authorId": "1380007189",
                                    "name": "W. Lam"
                                },
                                {
                                    "authorId": "144873019",
                                    "name": "Qian Yu"
                                },
                                {
                                    "authorId": "1734000",
                                    "name": "A. M. So"
                                },
                                {
                                    "authorId": "1576223501",
                                    "name": "Shengding Hu"
                                },
                                {
                                    "authorId": null,
                                    "name": "Zhiyuan Liu"
                                },
                                {
                                    "authorId": "50638196",
                                    "name": "Nigel Collier"
                                }
                            ],
                            "year": 2023,
                            "venue": "arXiv.org",
                            "n_citations": 43
                        },
                        "score": 0.70263671875
                    },
                    {
                        "id": "(Ewer et al., 2024)",
                        "snippets": [
                            "Decoders use a causal attention, ensuring that each token attends only to the preceding tokens. In contrast, encoders allow all tokens to attend to each other by performing attention computation from scratch for each token prediction."
                        ],
                        "paper": {
                            "corpus_id": 273025546,
                            "title": "ENTP: Encoder-only Next Token Prediction",
                            "authors": [
                                {
                                    "authorId": "2323781863",
                                    "name": "Ethan Ewer"
                                },
                                {
                                    "authorId": "2253659910",
                                    "name": "Daewon Chae"
                                },
                                {
                                    "authorId": "2323820473",
                                    "name": "Thomas Zeng"
                                },
                                {
                                    "authorId": "2323851531",
                                    "name": "Jinkyu Kim"
                                },
                                {
                                    "authorId": "2323790154",
                                    "name": "Kangwook Lee"
                                }
                            ],
                            "year": 2024,
                            "venue": "arXiv.org",
                            "n_citations": 4
                        },
                        "score": 0.904296875
                    },
                    {
                        "id": "(Alomari et al., 2023)",
                        "snippets": [
                            "For the decoder part, we leverage GPT2, BERT, RoBERTa, and Ernie 2.0. GPT2 is trained on open-ended autoregressive generation with the objective of predicting the next word in a sequence of a few token starters (causal language modeling) utilizing only the Transformer's unidirectional ''left-to-right'' self-attention decoder. Theoretically, GPT2 is claimed to be the optimal design for use as a decoder. However, with a few adjustments, bi-directional encoderonly pretrained models such as BERT, RoBERTa, and Ernie 2.0 may simply be adapted as decoders. To compare the encoder and decoder architectures of the Transformer, encoder blocks consist solely of a bi-directional self-attention layer and two feed-forward layers. By contrast, decoder blocks have a unidirectional self-attention layer, a crossattention layer, and two feed-forward layers."
                        ],
                        "paper": {
                            "corpus_id": 263729712,
                            "title": "Warm-Starting for Improving the Novelty of Abstractive Summarization",
                            "authors": [
                                {
                                    "authorId": "2130042263",
                                    "name": "Ayham Alomari"
                                },
                                {
                                    "authorId": "1403466899",
                                    "name": "A. S. Al-Shamayleh"
                                },
                                {
                                    "authorId": "36826893",
                                    "name": "N. Idris"
                                },
                                {
                                    "authorId": "2049063550",
                                    "name": "Aznul Qalid Md Sabri"
                                },
                                {
                                    "authorId": "1770016",
                                    "name": "I. Alsmadi"
                                },
                                {
                                    "authorId": "2182454665",
                                    "name": "Danah Omary"
                                }
                            ],
                            "year": 2023,
                            "venue": "IEEE Access",
                            "n_citations": 1
                        },
                        "score": 0.6669921875
                    },
                    {
                        "id": "(Busto-Castineira et al., 2025)",
                        "snippets": [
                            "Transformers are unsupervised learners thanks to their selfattention mechanism (Vaswani et al., 2017), which controls the impact of the context on the model's output. The original transformer architecture is composed of an encoder and a decoder. While the encoder's attention is bidirectional, the decoder has a masked multi-head attention block that masks non-causal context and a bidirectional multi-head attention block that receives noncausal information from the encoder. \n\nAlthough the encoder-decoder architecture is widely used in some nlp applications like machine translation (Kawara et al., 2021)(Nguyen et al., 2021), other transformer-based models only use one of these two components. By excluding the encoder, we eliminate all non-causal contextual dependencies, thus using only the masked attention of the decoder. Currently, decoder-only transformers are the most effective task-agnostic nlg systems."
                        ],
                        "paper": {
                            "corpus_id": 276423946,
                            "title": "Optimal word order for non-causal text generation with Large Language Models: The Spanish case",
                            "authors": [
                                {
                                    "authorId": "2222734467",
                                    "name": "Andrea Busto-Casti\u00f1eira"
                                },
                                {
                                    "authorId": "1405165681",
                                    "name": "Silvia Garc\u00eda-M\u00e9ndez"
                                },
                                {
                                    "authorId": "2326130687",
                                    "name": "Francisco de Arriba-P\u00e9rez"
                                },
                                {
                                    "authorId": "1395988865",
                                    "name": "F. Gonz\u00e1lez-Casta\u00f1o"
                                }
                            ],
                            "year": 2025,
                            "venue": "Pattern Recognition Letters",
                            "n_citations": 0
                        },
                        "score": 0.71875
                    }
                ],
                "format": "synthesis",
                "table": null,
                "model": "claude-3-7-sonnet-20250219"
            },
            {
                "title": "Causal Decoder Models",
                "tldr": "Causal decoder models use unidirectional attention mechanisms where each token can only attend to itself and preceding tokens, making them ideal for autoregressive text generation. Most state-of-the-art large language models use this architecture due to its effectiveness in zero-shot generalization, simpler structure, and efficient training. (12 sources)",
                "text": "\nCausal decoder models represent the dominant architecture in contemporary large language models (LLMs). These models implement unidirectional (left-to-right) attention mechanisms where each token can only attend to itself and previous tokens in the sequence <Paper corpusId=\"258947629\" paperTitle=\"(Roberts, 2023)\" isShortName></Paper> <Paper corpusId=\"261064777\" paperTitle=\"(Zheng et al., 2023)\" isShortName></Paper>. This causal masking pattern is the defining feature that enables these models to predict the next token based on previously observed tokens, making them naturally suited for autoregressive text generation <Paper corpusId=\"248118752\" paperTitle=\"(Wang et al., 2022)\" isShortName></Paper>.\n\nThe training objective for causal decoder models is typically next-token prediction, also known as causal language modeling (CLM) <Paper corpusId=\"277857043\" paperTitle=\"(Beiranvand et al., 2025)\" isShortName></Paper>. This objective is formulated as:\n\n$$L(\\theta) = -\\sum_{t=1}^{T} \\log p_\\theta(x_t|x_{<t})$$\n\nwhere each token $x_t$ is predicted based solely on the sequence of preceding tokens $x_{<t}$ <Paper corpusId=\"277857043\" paperTitle=\"(Beiranvand et al., 2025)\" isShortName></Paper>.\n\nCausal decoder-only models have become the architecture of choice for most modern LLMs, including prominent examples like the GPT series <Paper corpusId=\"277349741\" paperTitle=\"(Nie et al., 2025)\" isShortName></Paper> <Paper corpusId=\"218971783\" paperTitle=\"(Brown et al., 2020)\" isShortName></Paper>. Despite the original Transformer being an encoder-decoder architecture, nearly all state-of-the-art language models exceeding 100 billion parameters are now causal decoder-only models <Paper corpusId=\"253420279\" paperTitle=\"(Scao et al., 2022)\" isShortName></Paper>.\n\nThe popularity of causal decoder models stems from several advantages:\n\n1. **Simplicity**: Their architecture is simpler than encoder-decoder models, with a more straightforward structure <Paper corpusId=\"267402678\" paperTitle=\"(Li, 2024)\" isShortName></Paper>.\n\n2. **Training and inference efficiency**: They feature faster training and inference speeds compared to encoder-decoder models <Paper corpusId=\"267402678\" paperTitle=\"(Li, 2024)\" isShortName></Paper>.\n\n3. **Natural fit for generation**: Their autoregressive nature makes them particularly well-suited for pure generation tasks <Paper corpusId=\"267402678\" paperTitle=\"(Li, 2024)\" isShortName></Paper>.\n\n4. **Zero-shot generalization**: Causal decoder-only models have demonstrated superior zero-shot generalization capabilities immediately after pretraining <Paper corpusId=\"253420279\" paperTitle=\"(Scao et al., 2022)\" isShortName></Paper>.\n\nDespite these advantages, causal decoder models do have certain limitations. The unidirectional nature of their attention mechanism means they cannot build as rich representations of input text as bidirectional models <Paper corpusId=\"248118752\" paperTitle=\"(Wang et al., 2022)\" isShortName></Paper>. Additionally, they suffer from cumulative quantization errors in their attention mechanisms, particularly affecting tokens appearing later in the sequence <Paper corpusId=\"260886785\" paperTitle=\"(Kim et al., 2023)\" isShortName></Paper>.\n\nIn implementation, causal decoder models process information sequentially, where previously generated tokens serve as context for future generations <Paper corpusId=\"273025546\" paperTitle=\"(Ewer et al., 2024)\" isShortName></Paper>. This sequential processing, enforced by the causal mask applied to the attention matrix, ensures that tokens can only attend to themselves or previous ones <Paper corpusId=\"269982953\" paperTitle=\"(Leemann et al., 2024)\" isShortName></Paper>. The mask is implemented by setting the lower triangular part of the attention matrix to 0 and the remaining elements to \u2212\u221e, preventing tokens from attending to future positions <Paper corpusId=\"270214176\" paperTitle=\"(Jiang et al., 2024)\" isShortName></Paper>.",
                "citations": [
                    {
                        "id": "(Roberts, 2023)",
                        "snippets": [
                            "In this article we prove that the general transformer neural model undergirding modern large language models (LLMs) is Turing complete under reasonable assumptions",
                            "Differentiating Encoder-only and Decoder-only Models: Decoder-only models have three necessary characteristics which are derived from their function in the vanilla transformer. The decoder must (1) provide a means of autoregressively predicting the next token based on the tokens generated so far given the encoder input as contextualization. In 2 this is shown as the recursive red connection mapping the output vector back into the last element of the input sequence of vectors. To be suited to this task, decoder-only models must (2) not see future values when evaluating a query on the input sequence of vectors. This is why decoder-only models are often referred to as causal language models (CLM). In 2, we refer to the decoder attention heads as causal attention heads rather than masked attention heads as they are called in (Vaswani et al., 2017). The model must be (3) trained to predict the next token given the current input sequence of vectors. This training method coupled with recursion allows decoder-only models to autoregressively generate arbitrarily long (up to the max size of the input vector sequence) sequences."
                        ],
                        "paper": {
                            "corpus_id": 258947629,
                            "title": "How Powerful are Decoder-Only Transformer Neural Models?",
                            "authors": [
                                {
                                    "authorId": "2115904887",
                                    "name": "Jesse Roberts"
                                }
                            ],
                            "year": 2023,
                            "venue": "IEEE International Joint Conference on Neural Network",
                            "n_citations": 19
                        },
                        "score": 0.8037109375
                    },
                    {
                        "id": "(Zheng et al., 2023)",
                        "snippets": [
                            "Encoder-decoder Architecture: The vanilla Transformer proposed in (Vaswani et al., 2017) is based on encoder-decoder architecture, which comprises separate encoder and decoder components. The encoder processes the input sequence and captures its latent representation, which is then used by the decoder to generate the output sequence autoregressively. This architecture is well-suited for tasks involving sequence-to-sequence mapping, such as machine translation, text summarization, and dialogue generation.\n\nCausal Decoder Architecture: The causal decoder architecture is commonly implemented as a stack of decoder layers. It utilizes a diagonal mask matrix, allowing each token to only have access to information from preceding tokens. This constraint ensures a unidirectional and autoregressive generation process."
                        ],
                        "paper": {
                            "corpus_id": 261064777,
                            "title": "Towards an Understanding of Large Language Models in Software Engineering Tasks",
                            "authors": [
                                {
                                    "authorId": "2148256392",
                                    "name": "Zibin Zheng"
                                },
                                {
                                    "authorId": "2115304",
                                    "name": "Kai-Chun Ning"
                                },
                                {
                                    "authorId": "2254800142",
                                    "name": "Jiachi Chen"
                                },
                                {
                                    "authorId": "2214155529",
                                    "name": "Yanlin Wang"
                                },
                                {
                                    "authorId": "2274095496",
                                    "name": "Wenqing Chen"
                                },
                                {
                                    "authorId": "2217902484",
                                    "name": "Lianghong Guo"
                                },
                                {
                                    "authorId": "2233023641",
                                    "name": "Weicheng Wang"
                                }
                            ],
                            "year": 2023,
                            "venue": "Empirical Software Engineering",
                            "n_citations": 76
                        },
                        "score": 0.64697265625
                    },
                    {
                        "id": "(Wang et al., 2022)",
                        "snippets": [
                            "Causal decoder-only. Although the encoder-decoder is the original Transformer variant, most recent LLMs use a decoder-only architecture. These models can be trained as a traditional language model (i.e. to predict the next token in a sequence). Decoder-only models have no independent means of processing or representing the input sequence and target sequence differently-all tokens are processed in an equivalent fashion, and, because of the causal masking pattern, conditioning is simply based on past tokens (see Figure 2, on the left). On the one hand, this means that the representation for any conditioning text is inherently weaker; on the other hand, it yields a simpler architecture that is naturally suited to a standard autoregressive next-step-prediction pretraining objective. We refer to this architecture as causal decoder-only (CD) .\n\nNon-causal decoder-only. To allow decoder-only models to build richer non-causal representations of the input/conditioning text, it has been proposed to simply modify the attention mask used. Specifically, the self-attention masking pattern can be changed so that the region of the input sequence corresponding to conditioning information has a non-causal mask (i.e. attention in this region is not restricted to past tokens, see middle of Figure 2), as in the encoder of an encoder-decoder architecture. We refer to this architecture as non-causal decoder-only (ND) .\n\nComparisons across architectures. Decoder-only models process a single sequence consisting of the concatenation of the input and target text. On the other hand, in an encoder-decoder, the encoder processes only the input and the decoder processes only the target."
                        ],
                        "paper": {
                            "corpus_id": 248118752,
                            "title": "What Language Model Architecture and Pretraining Objective Work Best for Zero-Shot Generalization?",
                            "authors": [
                                {
                                    "authorId": "2135734748",
                                    "name": "Thomas Wang"
                                },
                                {
                                    "authorId": "145625142",
                                    "name": "Adam Roberts"
                                },
                                {
                                    "authorId": "80424302",
                                    "name": "Daniel Hesslow"
                                },
                                {
                                    "authorId": "1379806208",
                                    "name": "Teven Le Scao"
                                },
                                {
                                    "authorId": "3351938",
                                    "name": "Hyung Won Chung"
                                },
                                {
                                    "authorId": "46181066",
                                    "name": "Iz Beltagy"
                                },
                                {
                                    "authorId": "143945447",
                                    "name": "Julien Launay"
                                },
                                {
                                    "authorId": "2402716",
                                    "name": "Colin Raffel"
                                }
                            ],
                            "year": 2022,
                            "venue": "International Conference on Machine Learning",
                            "n_citations": 175
                        },
                        "score": 0.84423828125
                    },
                    {
                        "id": "(Beiranvand et al., 2025)",
                        "snippets": [
                            "Encoder-only models, such as BERT [18], are designed to generate holistic representations of input sequences. These models take an entire sequence as input and process it bidirectionally-each token has access to the full left and right context during training. This characteristic allows the model to deeply capture semantic dependencies across the input.\n\nIn contrast, decoder-only models, such as GPT [19], operate unidirectionally. They are trained in an autoregressive fashion, where each token is generated based only on preceding tokens. The model has no access to future inputs during training or inference, enforcing a strict left-to-right dependency. This makes them particularly well-suited for generative tasks such as open-ended text generation or dialogue modelling.\n\nThe pretraining objective for decoder-only models is Causal Language Modelling (CLM), where the model learns to predict the next token in a sequence, given all previous ones. The associated loss is defined as: \n\nHere, each token x t is conditioned solely on the sequence x <t , making the model capable of generating coherent text step by step.\n\nThe distinction between encoder-only and decoder-only models is foundational: the former are optimized for understanding input context and are widely used in classification and embedding tasks, while the latter are tailored for sequential prediction and text generation."
                        ],
                        "paper": {
                            "corpus_id": 277857043,
                            "title": "Integrating Structural and Semantic Signals in Text-Attributed Graphs with BiGTex",
                            "authors": [
                                {
                                    "authorId": "66841167",
                                    "name": "Azadeh Beiranvand"
                                },
                                {
                                    "authorId": "35409259",
                                    "name": "S. M. Vahidipour"
                                }
                            ],
                            "year": 2025,
                            "venue": "arXiv.org",
                            "n_citations": 0
                        },
                        "score": 0.810546875
                    },
                    {
                        "id": "(Nie et al., 2025)",
                        "snippets": [
                            "\u2022 Causal decoder. As a representative decoder-only architecture, causal decoder models introduce the unidirectional attention mask to ensure that each input token can only attend to the past tokens and itself. This mechanism makes them suitable for text generation tasks. Prominent examples are the GPTseries models (Radford et al., 2018(Radford et al., 2019)(Brown et al., 2020). \n\n\u2022 Non-causal decoder. Another kind of decoder-only architecture is the non-casual structure. This architecture performs bidirectional attention on prefix tokens and unidirectional attention only on generated tokens. One representative prefix decoder LLMs is GLM (Zeng et al., 2022)."
                        ],
                        "paper": {
                            "corpus_id": 277349741,
                            "title": "Exploring the Roles of Large Language Models in Reshaping Transportation Systems: A Survey, Framework, and Roadmap",
                            "authors": [
                                {
                                    "authorId": "94168461",
                                    "name": "Tong Nie"
                                },
                                {
                                    "authorId": "2028643500",
                                    "name": "Jiangming Sun"
                                },
                                {
                                    "authorId": "2277421553",
                                    "name": "Wei Ma"
                                }
                            ],
                            "year": 2025,
                            "venue": "arXiv.org",
                            "n_citations": 4
                        },
                        "score": 0.658203125
                    },
                    {
                        "id": "(Brown et al., 2020)",
                        "snippets": [
                            "Recent work has demonstrated substantial gains on many NLP tasks and benchmarks by pre-training on a large corpus of text followed by fine-tuning on a specific task. While typically task-agnostic in architecture, this method still requires task-specific fine-tuning datasets of thousands or tens of thousands of examples. By contrast, humans can generally perform a new language task from only a few examples or from simple instructions - something which current NLP systems still largely struggle to do. Here we show that scaling up language models greatly improves task-agnostic, few-shot performance, sometimes even reaching competitiveness with prior state-of-the-art fine-tuning approaches. Specifically, we train GPT-3, an autoregressive language model with 175 billion parameters, 10x more than any previous non-sparse language model, and test its performance in the few-shot setting. For all tasks, GPT-3 is applied without any gradient updates or fine-tuning, with tasks and few-shot demonstrations specified purely via text interaction with the model. GPT-3 achieves strong performance on many NLP datasets, including translation, question-answering, and cloze tasks, as well as several tasks that require on-the-fly reasoning or domain adaptation, such as unscrambling words, using a novel word in a sentence, or performing 3-digit arithmetic. At the same time, we also identify some datasets where GPT-3's few-shot learning still struggles, as well as some datasets where GPT-3 faces methodological issues related to training on large web corpora. Finally, we find that GPT-3 can generate samples of news articles which human evaluators have difficulty distinguishing from articles written by humans. We discuss broader societal impacts of this finding and of GPT-3 in general."
                        ],
                        "paper": {
                            "corpus_id": 218971783,
                            "title": "Language Models are Few-Shot Learners",
                            "authors": [
                                {
                                    "authorId": "31035595",
                                    "name": "Tom B. Brown"
                                },
                                {
                                    "authorId": "2056658938",
                                    "name": "Benjamin Mann"
                                },
                                {
                                    "authorId": "39849748",
                                    "name": "Nick Ryder"
                                },
                                {
                                    "authorId": "2065894334",
                                    "name": "Melanie Subbiah"
                                },
                                {
                                    "authorId": "152724169",
                                    "name": "J. Kaplan"
                                },
                                {
                                    "authorId": "6515819",
                                    "name": "Prafulla Dhariwal"
                                },
                                {
                                    "authorId": "2072676",
                                    "name": "Arvind Neelakantan"
                                },
                                {
                                    "authorId": "67311962",
                                    "name": "Pranav Shyam"
                                },
                                {
                                    "authorId": "144864359",
                                    "name": "Girish Sastry"
                                },
                                {
                                    "authorId": "119609682",
                                    "name": "Amanda Askell"
                                },
                                {
                                    "authorId": "144517868",
                                    "name": "Sandhini Agarwal"
                                },
                                {
                                    "authorId": "1404060687",
                                    "name": "Ariel Herbert-Voss"
                                },
                                {
                                    "authorId": "2064404342",
                                    "name": "Gretchen Krueger"
                                },
                                {
                                    "authorId": "103143311",
                                    "name": "T. Henighan"
                                },
                                {
                                    "authorId": "48422824",
                                    "name": "R. Child"
                                },
                                {
                                    "authorId": "1992922591",
                                    "name": "A. Ramesh"
                                },
                                {
                                    "authorId": "2052152920",
                                    "name": "Daniel M. Ziegler"
                                },
                                {
                                    "authorId": "49387725",
                                    "name": "Jeff Wu"
                                },
                                {
                                    "authorId": "2059411355",
                                    "name": "Clemens Winter"
                                },
                                {
                                    "authorId": "144239765",
                                    "name": "Christopher Hesse"
                                },
                                {
                                    "authorId": "2108828435",
                                    "name": "Mark Chen"
                                },
                                {
                                    "authorId": "2064673055",
                                    "name": "Eric Sigler"
                                },
                                {
                                    "authorId": "1380985420",
                                    "name": "Ma-teusz Litwin"
                                },
                                {
                                    "authorId": "145565184",
                                    "name": "Scott Gray"
                                },
                                {
                                    "authorId": "1490681878",
                                    "name": "Benjamin Chess"
                                },
                                {
                                    "authorId": "2115193883",
                                    "name": "Jack Clark"
                                },
                                {
                                    "authorId": "133740015",
                                    "name": "Christopher Berner"
                                },
                                {
                                    "authorId": "52238703",
                                    "name": "Sam McCandlish"
                                },
                                {
                                    "authorId": "38909097",
                                    "name": "Alec Radford"
                                },
                                {
                                    "authorId": "1701686",
                                    "name": "I. Sutskever"
                                },
                                {
                                    "authorId": "2698777",
                                    "name": "Dario Amodei"
                                }
                            ],
                            "year": 2020,
                            "venue": "Neural Information Processing Systems",
                            "n_citations": 42437
                        },
                        "score": 0
                    },
                    {
                        "id": "(Scao et al., 2022)",
                        "snippets": [
                            "Notably, while the original Transformer is based on an encoder-decoder architecture, many popular models have opted for encoder-only (e.g. BERT, (Devlin et al., 2019)) or decoder-only (e.g. GPT, (Radford et al., 2018)) approaches. Currently, all state-of-the-art language models over 100 billion parameters are causal decoder-only models Rae et al., 2021;Chowdhery et al., 2022). This is in opposition to the findings of (Raffel et al., 2019), in which encoderdecoder models significantly outperform decoder-only models for transfer learning.\n\nPrior to our work, the literature was lacking a systematic evaluation of the zero-shot generalization capabilities of different architectures and pretraining objectives. We explored this question in (Wang et al., 2022) where we evaluated encoder-decoder and decoder-only architectures and their interactions with causal, prefix, and masked language modeling pretraining objectives. Our results show that immediately after pretraining, causal decoderonly models performed best -validating the choice of state-of-the-art LLMs."
                        ],
                        "paper": {
                            "corpus_id": 253420279,
                            "title": "BLOOM: A 176B-Parameter Open-Access Multilingual Language Model",
                            "authors": [
                                {
                                    "authorId": "1379806208",
                                    "name": "Teven Le Scao"
                                },
                                {
                                    "authorId": "144270981",
                                    "name": "Angela Fan"
                                },
                                {
                                    "authorId": "2003696840",
                                    "name": "Christopher Akiki"
                                },
                                {
                                    "authorId": "2949185",
                                    "name": "Ellie Pavlick"
                                },
                                {
                                    "authorId": "2066663381",
                                    "name": "Suzana Ili'c"
                                },
                                {
                                    "authorId": "80424302",
                                    "name": "Daniel Hesslow"
                                },
                                {
                                    "authorId": "2190282134",
                                    "name": "Roman Castagn'e"
                                },
                                {
                                    "authorId": "2993731",
                                    "name": "A. Luccioni"
                                },
                                {
                                    "authorId": "1846431",
                                    "name": "Fran\u00e7ois Yvon"
                                },
                                {
                                    "authorId": "2907260",
                                    "name": "Matthias Gall\u00e9"
                                },
                                {
                                    "authorId": "50195579",
                                    "name": "J. Tow"
                                },
                                {
                                    "authorId": "2531268",
                                    "name": "Alexander M. Rush"
                                },
                                {
                                    "authorId": "103476203",
                                    "name": "Stella Biderman"
                                },
                                {
                                    "authorId": "1991019030",
                                    "name": "Albert Webson"
                                },
                                {
                                    "authorId": "1451644426",
                                    "name": "Pawan Sasanka Ammanamanchi"
                                },
                                {
                                    "authorId": "2135734748",
                                    "name": "Thomas Wang"
                                },
                                {
                                    "authorId": "68990982",
                                    "name": "Beno\u00eet Sagot"
                                },
                                {
                                    "authorId": "2037383772",
                                    "name": "Niklas Muennighoff"
                                },
                                {
                                    "authorId": "46219923",
                                    "name": "Albert Villanova del Moral"
                                },
                                {
                                    "authorId": "2537545",
                                    "name": "Olatunji Ruwase"
                                },
                                {
                                    "authorId": "48983885",
                                    "name": "Rachel Bawden"
                                },
                                {
                                    "authorId": "32136590",
                                    "name": "Stas Bekman"
                                },
                                {
                                    "authorId": "1584940075",
                                    "name": "Angelina McMillan-Major"
                                },
                                {
                                    "authorId": "46181066",
                                    "name": "Iz Beltagy"
                                },
                                {
                                    "authorId": "2168170616",
                                    "name": "Huu Nguyen"
                                },
                                {
                                    "authorId": "2113836860",
                                    "name": "Lucile Saulnier"
                                },
                                {
                                    "authorId": "145814654",
                                    "name": "Samson Tan"
                                },
                                {
                                    "authorId": "147846651",
                                    "name": "Pedro Ortiz Suarez"
                                },
                                {
                                    "authorId": "2285868436",
                                    "name": "Victor Sanh"
                                },
                                {
                                    "authorId": "2172404846",
                                    "name": "Hugo Laurenccon"
                                },
                                {
                                    "authorId": "2262249",
                                    "name": "Yacine Jernite"
                                },
                                {
                                    "authorId": "143945447",
                                    "name": "Julien Launay"
                                },
                                {
                                    "authorId": "49501003",
                                    "name": "Margaret Mitchell"
                                },
                                {
                                    "authorId": "2402716",
                                    "name": "Colin Raffel"
                                },
                                {
                                    "authorId": "2273789852",
                                    "name": "Aaron Gokaslan"
                                },
                                {
                                    "authorId": "2183598223",
                                    "name": "Adi Simhi"
                                },
                                {
                                    "authorId": "2078619062",
                                    "name": "Aitor Soroa Etxabe"
                                },
                                {
                                    "authorId": "8129718",
                                    "name": "Alham Fikri Aji"
                                },
                                {
                                    "authorId": "73769093",
                                    "name": "Amit Alfassy"
                                },
                                {
                                    "authorId": "145046059",
                                    "name": "Anna Rogers"
                                },
                                {
                                    "authorId": "2190281124",
                                    "name": "Ariel Kreisberg Nitzav"
                                },
                                {
                                    "authorId": "66247317",
                                    "name": "Canwen Xu"
                                },
                                {
                                    "authorId": "35966970",
                                    "name": "Chenghao Mou"
                                },
                                {
                                    "authorId": "1591176064",
                                    "name": "Chris C. Emezue"
                                },
                                {
                                    "authorId": "2261291789",
                                    "name": "Christopher Klamm"
                                },
                                {
                                    "authorId": "89269402",
                                    "name": "Colin Leong"
                                },
                                {
                                    "authorId": "71075073",
                                    "name": "Daniel Alexander van Strien"
                                },
                                {
                                    "authorId": "2518906",
                                    "name": "David Ifeoluwa Adelani"
                                },
                                {
                                    "authorId": "9215251",
                                    "name": "Dragomir R. Radev"
                                },
                                {
                                    "authorId": "79512668",
                                    "name": "E. G. Ponferrada"
                                },
                                {
                                    "authorId": "2190281122",
                                    "name": "Efrat Levkovizh"
                                },
                                {
                                    "authorId": "2047591327",
                                    "name": "Ethan Kim"
                                },
                                {
                                    "authorId": "2088048322",
                                    "name": "Eyal Natan"
                                },
                                {
                                    "authorId": "2067891070",
                                    "name": "F. Toni"
                                },
                                {
                                    "authorId": "13656138",
                                    "name": "G\u00e9rard Dupont"
                                },
                                {
                                    "authorId": "2067996",
                                    "name": "Germ\u00e1n Kruszewski"
                                },
                                {
                                    "authorId": "2158858559",
                                    "name": "Giada Pistilli"
                                },
                                {
                                    "authorId": "2218938",
                                    "name": "Hady ElSahar"
                                },
                                {
                                    "authorId": "90563027",
                                    "name": "Hamza Benyamina"
                                },
                                {
                                    "authorId": "2057078797",
                                    "name": "H. Tran"
                                },
                                {
                                    "authorId": "47948569",
                                    "name": "Ian Yu"
                                },
                                {
                                    "authorId": "1429833598",
                                    "name": "Idris Abdulmumin"
                                },
                                {
                                    "authorId": "2060080508",
                                    "name": "Isaac Johnson"
                                },
                                {
                                    "authorId": "1404791152",
                                    "name": "Itziar Gonzalez-Dios"
                                },
                                {
                                    "authorId": "144979591",
                                    "name": "Javier de la Rosa"
                                },
                                {
                                    "authorId": "2164872258",
                                    "name": "Jenny Chim"
                                },
                                {
                                    "authorId": "34176020",
                                    "name": "Jesse Dodge"
                                },
                                {
                                    "authorId": "144549416",
                                    "name": "Jian Zhu"
                                },
                                {
                                    "authorId": "2116123009",
                                    "name": "Jonathan Chang"
                                },
                                {
                                    "authorId": "2146695800",
                                    "name": "Jorg Frohberg"
                                },
                                {
                                    "authorId": "2094755167",
                                    "name": "Josephine Tobing"
                                },
                                {
                                    "authorId": "143779690",
                                    "name": "J. Bhattacharjee"
                                },
                                {
                                    "authorId": "90615055",
                                    "name": "Khalid Almubarak"
                                },
                                {
                                    "authorId": "2157630500",
                                    "name": "Kimbo Chen"
                                },
                                {
                                    "authorId": "46258841",
                                    "name": "Kyle Lo"
                                },
                                {
                                    "authorId": "51128119",
                                    "name": "L. V. Werra"
                                },
                                {
                                    "authorId": "20308468",
                                    "name": "Leon Weber"
                                },
                                {
                                    "authorId": null,
                                    "name": "Long Phan"
                                },
                                {
                                    "authorId": "2190281230",
                                    "name": "Loubna Ben Allal"
                                },
                                {
                                    "authorId": "77970446",
                                    "name": "Ludovic Tanguy"
                                },
                                {
                                    "authorId": "1879591269",
                                    "name": "Manan Dey"
                                },
                                {
                                    "authorId": "115568186",
                                    "name": "M. Mu\u00f1oz"
                                },
                                {
                                    "authorId": "153528116",
                                    "name": "Maraim Masoud"
                                },
                                {
                                    "authorId": "2176184513",
                                    "name": "Mar\u00eda Grandury"
                                },
                                {
                                    "authorId": "2125821515",
                                    "name": "Mario vSavsko"
                                },
                                {
                                    "authorId": "2112504552",
                                    "name": "Max Huang"
                                },
                                {
                                    "authorId": "3443469",
                                    "name": "Maximin Coavoux"
                                },
                                {
                                    "authorId": "145431050",
                                    "name": "Mayank Singh"
                                },
                                {
                                    "authorId": "5745221",
                                    "name": "Mike Tian-Jian Jiang"
                                },
                                {
                                    "authorId": "1484109150",
                                    "name": "Minh Chien Vu"
                                },
                                {
                                    "authorId": "2097304324",
                                    "name": "M. A. Jauhar"
                                },
                                {
                                    "authorId": "2721586",
                                    "name": "Mustafa Ghaleb"
                                },
                                {
                                    "authorId": "34202134",
                                    "name": "Nishant Subramani"
                                },
                                {
                                    "authorId": "9529535",
                                    "name": "Nora Kassner"
                                },
                                {
                                    "authorId": "37441312",
                                    "name": "Nurulaqilla Khamis"
                                },
                                {
                                    "authorId": "2089233725",
                                    "name": "Olivier Nguyen"
                                },
                                {
                                    "authorId": "2190280842",
                                    "name": "Omar Espejel"
                                },
                                {
                                    "authorId": "51436367",
                                    "name": "Ona de Gibert"
                                },
                                {
                                    "authorId": "2176184659",
                                    "name": "Paulo Villegas"
                                },
                                {
                                    "authorId": "2071773966",
                                    "name": "Peter Henderson"
                                },
                                {
                                    "authorId": "46985469",
                                    "name": "Pierre Colombo"
                                },
                                {
                                    "authorId": "2190281321",
                                    "name": "Priscilla Amuok"
                                },
                                {
                                    "authorId": "2113836945",
                                    "name": "Quentin Lhoest"
                                },
                                {
                                    "authorId": "80858030",
                                    "name": "Rheza Harliman"
                                },
                                {
                                    "authorId": "150272855",
                                    "name": "Rishi Bommasani"
                                },
                                {
                                    "authorId": "116000979",
                                    "name": "R. L'opez"
                                },
                                {
                                    "authorId": null,
                                    "name": "Rui Ribeiro"
                                },
                                {
                                    "authorId": "1486204986",
                                    "name": "Salomey Osei"
                                },
                                {
                                    "authorId": "1708916",
                                    "name": "S. Pyysalo"
                                },
                                {
                                    "authorId": "47351277",
                                    "name": "Sebastian Nagel"
                                },
                                {
                                    "authorId": "2795685",
                                    "name": "Shamik Bose"
                                },
                                {
                                    "authorId": "7744881",
                                    "name": "Shamsuddeen Hassan Muhammad"
                                },
                                {
                                    "authorId": "1409842673",
                                    "name": "S. Sharma"
                                },
                                {
                                    "authorId": "29909347",
                                    "name": "S. Longpre"
                                },
                                {
                                    "authorId": "2099315138",
                                    "name": "Somaieh Nikpoor"
                                },
                                {
                                    "authorId": "82674724",
                                    "name": "S. Silberberg"
                                },
                                {
                                    "authorId": "2053516473",
                                    "name": "S. Pai"
                                },
                                {
                                    "authorId": "2074482488",
                                    "name": "S. Zink"
                                },
                                {
                                    "authorId": "46308692",
                                    "name": "Tiago Timponi Torrent"
                                },
                                {
                                    "authorId": "32246932",
                                    "name": "Timo Schick"
                                },
                                {
                                    "authorId": "1500242049",
                                    "name": "Tristan Thrush"
                                },
                                {
                                    "authorId": "3382327",
                                    "name": "V. Danchev"
                                },
                                {
                                    "authorId": "2841761",
                                    "name": "Vassilina Nikoulina"
                                },
                                {
                                    "authorId": "1796619",
                                    "name": "Veronika Laippala"
                                },
                                {
                                    "authorId": "2190280574",
                                    "name": "Violette Lepercq"
                                },
                                {
                                    "authorId": "2059767242",
                                    "name": "V. Prabhu"
                                },
                                {
                                    "authorId": "25098419",
                                    "name": "Zaid Alyafeai"
                                },
                                {
                                    "authorId": "2138053020",
                                    "name": "Zeerak Talat"
                                },
                                {
                                    "authorId": "2048082186",
                                    "name": "Arun Raja"
                                },
                                {
                                    "authorId": "2266692",
                                    "name": "Benjamin Heinzerling"
                                },
                                {
                                    "authorId": "152358188",
                                    "name": "Chenglei Si"
                                },
                                {
                                    "authorId": "3448427",
                                    "name": "Elizabeth Salesky"
                                },
                                {
                                    "authorId": "27689253",
                                    "name": "Sabrina J. Mielke"
                                },
                                {
                                    "authorId": "2183377987",
                                    "name": "Wilson Y. Lee"
                                },
                                {
                                    "authorId": "2051500420",
                                    "name": "Abheesht Sharma"
                                },
                                {
                                    "authorId": "2065039862",
                                    "name": "Andrea Santilli"
                                },
                                {
                                    "authorId": "2129106958",
                                    "name": "Antoine Chaffin"
                                },
                                {
                                    "authorId": "114762823",
                                    "name": "Arnaud Stiegler"
                                },
                                {
                                    "authorId": "2852125",
                                    "name": "Debajyoti Datta"
                                },
                                {
                                    "authorId": "50812522",
                                    "name": "Eliza Szczechla"
                                },
                                {
                                    "authorId": "1509809381",
                                    "name": "Gunjan Chhablani"
                                },
                                {
                                    "authorId": "144407394",
                                    "name": "Han Wang"
                                },
                                {
                                    "authorId": "144834468",
                                    "name": "Harshit Pandey"
                                },
                                {
                                    "authorId": "2879705",
                                    "name": "Hendrik Strobelt"
                                },
                                {
                                    "authorId": "31592365",
                                    "name": "Jason Alan Fries"
                                },
                                {
                                    "authorId": "120419790",
                                    "name": "Jos Rozen"
                                },
                                {
                                    "authorId": "2027599537",
                                    "name": "Leo Gao"
                                },
                                {
                                    "authorId": "35566806",
                                    "name": "Lintang Sutawika"
                                },
                                {
                                    "authorId": "31773000",
                                    "name": "M Saiful Bari"
                                },
                                {
                                    "authorId": "1752627730",
                                    "name": "Maged S. Al-Shaibani"
                                },
                                {
                                    "authorId": "35904689",
                                    "name": "Matteo Manica"
                                },
                                {
                                    "authorId": "22209084",
                                    "name": "Nihal V. Nayak"
                                },
                                {
                                    "authorId": "2131107966",
                                    "name": "Ryan Teehan"
                                },
                                {
                                    "authorId": "7641268",
                                    "name": "Samuel Albanie"
                                },
                                {
                                    "authorId": "2191455",
                                    "name": "Sheng Shen"
                                },
                                {
                                    "authorId": "2152318619",
                                    "name": "Srulik Ben-David"
                                },
                                {
                                    "authorId": "2870504",
                                    "name": "Stephen H. Bach"
                                },
                                {
                                    "authorId": "2111181991",
                                    "name": "Taewoon Kim"
                                },
                                {
                                    "authorId": "94251255",
                                    "name": "T. Bers"
                                },
                                {
                                    "authorId": "79215748",
                                    "name": "Thibault F\u00e9vry"
                                },
                                {
                                    "authorId": "10729963",
                                    "name": "Trishala Neeraj"
                                },
                                {
                                    "authorId": "70296695",
                                    "name": "Urmish Thakker"
                                },
                                {
                                    "authorId": "24025563",
                                    "name": "Vikas Raunak"
                                },
                                {
                                    "authorId": "2118488348",
                                    "name": "Xiang Tang"
                                },
                                {
                                    "authorId": "1725420331",
                                    "name": "Zheng-Xin Yong"
                                },
                                {
                                    "authorId": "48064856",
                                    "name": "Zhiqing Sun"
                                },
                                {
                                    "authorId": "1720739223",
                                    "name": "Shaked Brody"
                                },
                                {
                                    "authorId": "2101395835",
                                    "name": "Y. Uri"
                                },
                                {
                                    "authorId": "2190280874",
                                    "name": "Hadar Tojarieh"
                                },
                                {
                                    "authorId": "145625142",
                                    "name": "Adam Roberts"
                                },
                                {
                                    "authorId": "3351938",
                                    "name": "Hyung Won Chung"
                                },
                                {
                                    "authorId": "2112211652",
                                    "name": "Jaesung Tae"
                                },
                                {
                                    "authorId": "80842917",
                                    "name": "Jason Phang"
                                },
                                {
                                    "authorId": "40170001",
                                    "name": "Ofir Press"
                                },
                                {
                                    "authorId": "2609325",
                                    "name": "Conglong Li"
                                },
                                {
                                    "authorId": "22252150",
                                    "name": "D. Narayanan"
                                },
                                {
                                    "authorId": "2190280830",
                                    "name": "Hatim Bourfoune"
                                },
                                {
                                    "authorId": "48991386",
                                    "name": "J. Casper"
                                },
                                {
                                    "authorId": "3299496",
                                    "name": "Jeff Rasley"
                                },
                                {
                                    "authorId": "1491753352",
                                    "name": "Max Ryabinin"
                                },
                                {
                                    "authorId": "1381446720",
                                    "name": "Mayank Mishra"
                                },
                                {
                                    "authorId": "67016465",
                                    "name": "Minjia Zhang"
                                },
                                {
                                    "authorId": "1911755",
                                    "name": "M. Shoeybi"
                                },
                                {
                                    "authorId": "31758637",
                                    "name": "Myriam Peyrounette"
                                },
                                {
                                    "authorId": "31614549",
                                    "name": "N. Patry"
                                },
                                {
                                    "authorId": "2179884903",
                                    "name": "Nouamane Tazi"
                                },
                                {
                                    "authorId": "2186979509",
                                    "name": "Omar Sanseviero"
                                },
                                {
                                    "authorId": "138609838",
                                    "name": "Patrick von Platen"
                                },
                                {
                                    "authorId": "2190281218",
                                    "name": "Pierre Cornette"
                                },
                                {
                                    "authorId": "2190280981",
                                    "name": "Pierre Franccois Lavall'ee"
                                },
                                {
                                    "authorId": "31734741",
                                    "name": "R. Lacroix"
                                },
                                {
                                    "authorId": "32817044",
                                    "name": "Samyam Rajbhandari"
                                },
                                {
                                    "authorId": "2188737826",
                                    "name": "Sanchit Gandhi"
                                },
                                {
                                    "authorId": "2110486618",
                                    "name": "Shaden Smith"
                                },
                                {
                                    "authorId": "2293408",
                                    "name": "S. Requena"
                                },
                                {
                                    "authorId": "2147312210",
                                    "name": "Suraj Patil"
                                },
                                {
                                    "authorId": "3239480",
                                    "name": "Tim Dettmers"
                                },
                                {
                                    "authorId": "114850513",
                                    "name": "Ahmed Baruwa"
                                },
                                {
                                    "authorId": null,
                                    "name": "Amanpreet Singh"
                                },
                                {
                                    "authorId": "2190281235",
                                    "name": "Anastasia Cheveleva"
                                },
                                {
                                    "authorId": "1769176",
                                    "name": "Anne-Laure Ligozat"
                                },
                                {
                                    "authorId": "1677386832",
                                    "name": "Arjun Subramonian"
                                },
                                {
                                    "authorId": "2190281078",
                                    "name": "Aur'elie N'ev'eol"
                                },
                                {
                                    "authorId": "10727711",
                                    "name": "Charles Lovering"
                                },
                                {
                                    "authorId": "2758616",
                                    "name": "Dan Garrette"
                                },
                                {
                                    "authorId": "70209311",
                                    "name": "D. Tunuguntla"
                                },
                                {
                                    "authorId": "144568312",
                                    "name": "Ehud Reiter"
                                },
                                {
                                    "authorId": "2051713939",
                                    "name": "Ekaterina Taktasheva"
                                },
                                {
                                    "authorId": "2135526571",
                                    "name": "E. Voloshina"
                                },
                                {
                                    "authorId": "2158860079",
                                    "name": "Eli Bogdanov"
                                },
                                {
                                    "authorId": "9162688",
                                    "name": "Genta Indra Winata"
                                },
                                {
                                    "authorId": "2184031883",
                                    "name": "Hailey Schoelkopf"
                                },
                                {
                                    "authorId": "3245041",
                                    "name": "Jan-Christoph Kalo"
                                },
                                {
                                    "authorId": "2848048",
                                    "name": "Jekaterina Novikova"
                                },
                                {
                                    "authorId": "39774809",
                                    "name": "J. Forde"
                                },
                                {
                                    "authorId": "47274259",
                                    "name": "Xiangru Tang"
                                },
                                {
                                    "authorId": "11348687",
                                    "name": "Jungo Kasai"
                                },
                                {
                                    "authorId": "50106621",
                                    "name": "Ken Kawamura"
                                },
                                {
                                    "authorId": "2047711867",
                                    "name": "Liam Hazan"
                                },
                                {
                                    "authorId": "2954727",
                                    "name": "Marine Carpuat"
                                },
                                {
                                    "authorId": "2029314697",
                                    "name": "Miruna Clinciu"
                                },
                                {
                                    "authorId": "8756748",
                                    "name": "Najoung Kim"
                                },
                                {
                                    "authorId": "15590401",
                                    "name": "Newton Cheng"
                                },
                                {
                                    "authorId": "1799401599",
                                    "name": "Oleg Serikov"
                                },
                                {
                                    "authorId": "2132545395",
                                    "name": "Omer Antverg"
                                },
                                {
                                    "authorId": "1986356851",
                                    "name": "Oskar van der Wal"
                                },
                                {
                                    "authorId": "15176410",
                                    "name": "Rui Zhang"
                                },
                                {
                                    "authorId": "49775305",
                                    "name": "Ruochen Zhang"
                                },
                                {
                                    "authorId": "3159346",
                                    "name": "Sebastian Gehrmann"
                                },
                                {
                                    "authorId": "8963527",
                                    "name": "Shachar Mirkin"
                                },
                                {
                                    "authorId": "3097741",
                                    "name": "S. Pais"
                                },
                                {
                                    "authorId": "2134610800",
                                    "name": "Tatiana Shavrina"
                                },
                                {
                                    "authorId": "90745780",
                                    "name": "Thomas Scialom"
                                },
                                {
                                    "authorId": "2127600348",
                                    "name": "Tian Yun"
                                },
                                {
                                    "authorId": "1666636295",
                                    "name": "Tomasz Limisiewicz"
                                },
                                {
                                    "authorId": "1681799",
                                    "name": "Verena Rieser"
                                },
                                {
                                    "authorId": "2135362820",
                                    "name": "Vitaly Protasov"
                                },
                                {
                                    "authorId": "51259225",
                                    "name": "V. Mikhailov"
                                },
                                {
                                    "authorId": "100984698",
                                    "name": "Yada Pruksachatkun"
                                },
                                {
                                    "authorId": "2083259",
                                    "name": "Yonatan Belinkov"
                                },
                                {
                                    "authorId": "2190281071",
                                    "name": "Zachary Bamberger"
                                },
                                {
                                    "authorId": "2343772132",
                                    "name": "Zden\u02c7ek Kasner"
                                },
                                {
                                    "authorId": "1805991958",
                                    "name": "Zden\u011bk Kasner"
                                },
                                {
                                    "authorId": "2190281954",
                                    "name": "A. Pestana"
                                },
                                {
                                    "authorId": "15845853",
                                    "name": "A. Feizpour"
                                },
                                {
                                    "authorId": "2190399370",
                                    "name": "Ammar Khan"
                                },
                                {
                                    "authorId": "2190281566",
                                    "name": "Amy Faranak"
                                },
                                {
                                    "authorId": "2148971654",
                                    "name": "A. Santos"
                                },
                                {
                                    "authorId": "1739149407",
                                    "name": "Anthony Hevia"
                                },
                                {
                                    "authorId": "2190281069",
                                    "name": "Antigona Unldreaj"
                                },
                                {
                                    "authorId": "115638227",
                                    "name": "Arash Aghagol"
                                },
                                {
                                    "authorId": "2361305",
                                    "name": "Arezoo Abdollahi"
                                },
                                {
                                    "authorId": "101302626",
                                    "name": "A. Tammour"
                                },
                                {
                                    "authorId": "3110645",
                                    "name": "A. HajiHosseini"
                                },
                                {
                                    "authorId": "2190281564",
                                    "name": "Bahareh Behroozi"
                                },
                                {
                                    "authorId": "83263885",
                                    "name": "Benjamin Ayoade Ajibade"
                                },
                                {
                                    "authorId": "31577522",
                                    "name": "B. Saxena"
                                },
                                {
                                    "authorId": "2005399190",
                                    "name": "Carlos Mu\u00f1oz Ferrandis"
                                },
                                {
                                    "authorId": "2075459",
                                    "name": "Danish Contractor"
                                },
                                {
                                    "authorId": "144635557",
                                    "name": "D. Lansky"
                                },
                                {
                                    "authorId": "2058260775",
                                    "name": "Davis David"
                                },
                                {
                                    "authorId": "2111313627",
                                    "name": "Douwe Kiela"
                                },
                                {
                                    "authorId": "5943347",
                                    "name": "D. A. Nguyen"
                                },
                                {
                                    "authorId": "47654100",
                                    "name": "Edward Tan"
                                },
                                {
                                    "authorId": "2026649806",
                                    "name": "Emi Baylor"
                                },
                                {
                                    "authorId": "2190281502",
                                    "name": "Ezinwanne Ozoani"
                                },
                                {
                                    "authorId": "35330153",
                                    "name": "F. Mirza"
                                },
                                {
                                    "authorId": "2190281922",
                                    "name": "Frankline Ononiwu"
                                },
                                {
                                    "authorId": "123343513",
                                    "name": "Habib Rezanejad"
                                },
                                {
                                    "authorId": "2119822136",
                                    "name": "H.A. Jones"
                                },
                                {
                                    "authorId": "2105001416",
                                    "name": "Indrani Bhattacharya"
                                },
                                {
                                    "authorId": "1404060690",
                                    "name": "Irene Solaiman"
                                },
                                {
                                    "authorId": "2190281379",
                                    "name": "Irina Sedenko"
                                },
                                {
                                    "authorId": "3163125",
                                    "name": "Isar Nejadgholi"
                                },
                                {
                                    "authorId": "145629075",
                                    "name": "J. Passmore"
                                },
                                {
                                    "authorId": "150162316",
                                    "name": "Joshua Seltzer"
                                },
                                {
                                    "authorId": "97979993",
                                    "name": "Julio Bonis Sanz"
                                },
                                {
                                    "authorId": null,
                                    "name": "Karen Fort"
                                },
                                {
                                    "authorId": "3530609",
                                    "name": "L\u00edvia Dutra"
                                },
                                {
                                    "authorId": "2190281373",
                                    "name": "Mairon Samagaio"
                                },
                                {
                                    "authorId": "2190281500",
                                    "name": "Maraim Elbadri"
                                },
                                {
                                    "authorId": "2921990",
                                    "name": "Margot Mieskes"
                                },
                                {
                                    "authorId": "151492708",
                                    "name": "Marissa Gerchick"
                                },
                                {
                                    "authorId": "2190281205",
                                    "name": "Martha Akinlolu"
                                },
                                {
                                    "authorId": "2060092577",
                                    "name": "Michael McKenna"
                                },
                                {
                                    "authorId": "2056851511",
                                    "name": "Mike Qiu"
                                },
                                {
                                    "authorId": "144449938",
                                    "name": "M. Ghauri"
                                },
                                {
                                    "authorId": "2190281203",
                                    "name": "Mykola Burynok"
                                },
                                {
                                    "authorId": "1401945312",
                                    "name": "Nafis Abrar"
                                },
                                {
                                    "authorId": "8937909",
                                    "name": "Nazneen Rajani"
                                },
                                {
                                    "authorId": "2190281555",
                                    "name": "Nour Elkott"
                                },
                                {
                                    "authorId": "1992948200",
                                    "name": "N. Fahmy"
                                },
                                {
                                    "authorId": "2164156047",
                                    "name": "Olanrewaju Samuel"
                                },
                                {
                                    "authorId": "2061141169",
                                    "name": "Ran An"
                                },
                                {
                                    "authorId": "9294251",
                                    "name": "R. Kromann"
                                },
                                {
                                    "authorId": "2137183106",
                                    "name": "Ryan Hao"
                                },
                                {
                                    "authorId": "4279554",
                                    "name": "S. Alizadeh"
                                },
                                {
                                    "authorId": "2190281531",
                                    "name": "Sarmad Shubber"
                                },
                                {
                                    "authorId": "2116420702",
                                    "name": "Silas L. Wang"
                                },
                                {
                                    "authorId": "2109853801",
                                    "name": "Sourav Roy"
                                },
                                {
                                    "authorId": "10726201",
                                    "name": "S. Viguier"
                                },
                                {
                                    "authorId": "2153620715",
                                    "name": "Thanh-Cong Le"
                                },
                                {
                                    "authorId": "2190281729",
                                    "name": "Tobi Oyebade"
                                },
                                {
                                    "authorId": "2153620985",
                                    "name": "T. Le"
                                },
                                {
                                    "authorId": "2190429590",
                                    "name": "Yoyo Yang"
                                },
                                {
                                    "authorId": "2297189567",
                                    "name": "Zach Nguyen"
                                },
                                {
                                    "authorId": "41124383",
                                    "name": "Abhinav Ramesh Kashyap"
                                },
                                {
                                    "authorId": "2318515251",
                                    "name": "A. Palasciano"
                                },
                                {
                                    "authorId": "2840689",
                                    "name": "A. Callahan"
                                },
                                {
                                    "authorId": "2042747208",
                                    "name": "Anima Shukla"
                                },
                                {
                                    "authorId": "1414073449",
                                    "name": "Antonio Miranda-Escalada"
                                },
                                {
                                    "authorId": "2110183222",
                                    "name": "A. Singh"
                                },
                                {
                                    "authorId": "1379935164",
                                    "name": "Benjamin Beilharz"
                                },
                                {
                                    "authorId": "2165371942",
                                    "name": "Bo Wang"
                                },
                                {
                                    "authorId": "144972524",
                                    "name": "C. Brito"
                                },
                                {
                                    "authorId": "2111169784",
                                    "name": "Chenxi Zhou"
                                },
                                {
                                    "authorId": "50732716",
                                    "name": "Chirag Jain"
                                },
                                {
                                    "authorId": "2158158973",
                                    "name": "Chuxin Xu"
                                },
                                {
                                    "authorId": "2080941785",
                                    "name": "Cl\u00e9mentine Fourrier"
                                },
                                {
                                    "authorId": "2174177869",
                                    "name": "Daniel Le'on Perin'an"
                                },
                                {
                                    "authorId": "2082057793",
                                    "name": "Daniel Molano"
                                },
                                {
                                    "authorId": "150978762",
                                    "name": "Dian Yu"
                                },
                                {
                                    "authorId": "24907368",
                                    "name": "Enrique Manjavacas"
                                },
                                {
                                    "authorId": "2139792578",
                                    "name": "Fabio Barth"
                                },
                                {
                                    "authorId": "2190281754",
                                    "name": "Florian Fuhrimann"
                                },
                                {
                                    "authorId": "2165227550",
                                    "name": "Gabriel Altay"
                                },
                                {
                                    "authorId": "2166224123",
                                    "name": "Giyaseddin Bayrak"
                                },
                                {
                                    "authorId": null,
                                    "name": "Gully Burns"
                                },
                                {
                                    "authorId": "88811067",
                                    "name": "Helena U. Vrabec"
                                },
                                {
                                    "authorId": "121044523",
                                    "name": "I. Bello"
                                },
                                {
                                    "authorId": "93460753",
                                    "name": "Isha Dash"
                                },
                                {
                                    "authorId": "72725318",
                                    "name": "J. Kang"
                                },
                                {
                                    "authorId": "37585306",
                                    "name": "John Giorgi"
                                },
                                {
                                    "authorId": "144983077",
                                    "name": "Jonas Golde"
                                },
                                {
                                    "authorId": "2066514466",
                                    "name": "J. Posada"
                                },
                                {
                                    "authorId": "1601562797",
                                    "name": "Karthi Sivaraman"
                                },
                                {
                                    "authorId": "2190281314",
                                    "name": "Lokesh Bulchandani"
                                },
                                {
                                    "authorId": "2145287083",
                                    "name": "Lu Liu"
                                },
                                {
                                    "authorId": "2100596120",
                                    "name": "Luisa Shinzato"
                                },
                                {
                                    "authorId": "2190281960",
                                    "name": "Madeleine Hahn de Bykhovetz"
                                },
                                {
                                    "authorId": "2068853922",
                                    "name": "Maiko Takeuchi"
                                },
                                {
                                    "authorId": "1850527789",
                                    "name": "Marc P\u00e0mies"
                                },
                                {
                                    "authorId": "87956698",
                                    "name": "M. A. Castillo"
                                },
                                {
                                    "authorId": "2174178585",
                                    "name": "Marianna Nezhurina"
                                },
                                {
                                    "authorId": "1879523878",
                                    "name": "Mario Sanger"
                                },
                                {
                                    "authorId": "3004898",
                                    "name": "M. Samwald"
                                },
                                {
                                    "authorId": "120397552",
                                    "name": "Michael Cullan"
                                },
                                {
                                    "authorId": "50564168",
                                    "name": "Michael Weinberg"
                                },
                                {
                                    "authorId": "2072502429",
                                    "name": "M. Wolf"
                                },
                                {
                                    "authorId": "2190280864",
                                    "name": "Mina Mihaljcic"
                                },
                                {
                                    "authorId": "2112211627",
                                    "name": "Minna Liu"
                                },
                                {
                                    "authorId": "1397064923",
                                    "name": "M. Freidank"
                                },
                                {
                                    "authorId": "4981508",
                                    "name": "Myungsun Kang"
                                },
                                {
                                    "authorId": "12046785",
                                    "name": "Natasha Seelam"
                                },
                                {
                                    "authorId": "48948105",
                                    "name": "N. Dahlberg"
                                },
                                {
                                    "authorId": "40208102",
                                    "name": "N. Broad"
                                },
                                {
                                    "authorId": "70256289",
                                    "name": "N. Muellner"
                                },
                                {
                                    "authorId": "40539650",
                                    "name": "Pascale Fung"
                                },
                                {
                                    "authorId": "2097023671",
                                    "name": "Patricia Haller"
                                },
                                {
                                    "authorId": "2298902857",
                                    "name": "Patrick Haller"
                                },
                                {
                                    "authorId": "115525190",
                                    "name": "R. Eisenberg"
                                },
                                {
                                    "authorId": "2111138678",
                                    "name": "Robert Martin"
                                },
                                {
                                    "authorId": "2291171257",
                                    "name": "Rodrigo Canalli"
                                },
                                {
                                    "authorId": "2190282202",
                                    "name": "Rosaline Su"
                                },
                                {
                                    "authorId": "153083809",
                                    "name": "Ruisi Su"
                                },
                                {
                                    "authorId": "66986482",
                                    "name": "Samuel Cahyawijaya"
                                },
                                {
                                    "authorId": "51878929",
                                    "name": "Samuele Garda"
                                },
                                {
                                    "authorId": "2174177330",
                                    "name": "Shlok S Deshmukh"
                                },
                                {
                                    "authorId": "2112134590",
                                    "name": "Shubhanshu Mishra"
                                },
                                {
                                    "authorId": "39620434",
                                    "name": "Sid Kiblawi"
                                },
                                {
                                    "authorId": "119994729",
                                    "name": "Simon Ott"
                                },
                                {
                                    "authorId": "2190281679",
                                    "name": "Sinee Sang-aroonsiri"
                                },
                                {
                                    "authorId": "120438284",
                                    "name": "Srishti Kumar"
                                },
                                {
                                    "authorId": "134757625",
                                    "name": "Stefan Schweter"
                                },
                                {
                                    "authorId": "8723233",
                                    "name": "S. Bharati"
                                },
                                {
                                    "authorId": "103242455",
                                    "name": "Tanmay Laud"
                                },
                                {
                                    "authorId": "2174176862",
                                    "name": "Th\u00e9o Gigant"
                                },
                                {
                                    "authorId": "2190281376",
                                    "name": "Tomoya Kainuma"
                                },
                                {
                                    "authorId": "50320098",
                                    "name": "Wojciech Kusa"
                                },
                                {
                                    "authorId": "2139767217",
                                    "name": "Yanis Labrak"
                                },
                                {
                                    "authorId": "1572961212",
                                    "name": "Yashasvi Bajaj"
                                },
                                {
                                    "authorId": "2051879548",
                                    "name": "Y. Venkatraman"
                                },
                                {
                                    "authorId": "2110154622",
                                    "name": "Yifan Xu"
                                },
                                {
                                    "authorId": "2118670234",
                                    "name": "Ying Xu"
                                },
                                {
                                    "authorId": "2142717873",
                                    "name": "Yu Xu"
                                },
                                {
                                    "authorId": "1643680733",
                                    "name": "Z. Tan"
                                },
                                {
                                    "authorId": "79110285",
                                    "name": "Zhongli Xie"
                                },
                                {
                                    "authorId": "2114134227",
                                    "name": "Zifan Ye"
                                },
                                {
                                    "authorId": "2065370401",
                                    "name": "M. Bras"
                                },
                                {
                                    "authorId": "2037496520",
                                    "name": "Younes Belkada"
                                },
                                {
                                    "authorId": "50335211",
                                    "name": "Thomas Wolf"
                                }
                            ],
                            "year": 2022,
                            "venue": "arXiv.org",
                            "n_citations": 2393
                        },
                        "score": 0.74072265625
                    },
                    {
                        "id": "(Li, 2024)",
                        "snippets": [
                            "As shown in Figure 3, similar to most seq2seq models, the Transformer architecture consists of an encoder and a decoder. Overall, LLM (Language Model) models can be categorized into three major types: Encoder-decoder Architecture, Causal Decoder Architecture, and Prefix Decoder Architecture [11]. The Encoder-decoder Architecture uses the most basic structure and was initially introduced by the Seq2Seq model to address sequence-to-sequence tasks, such as machine translation. It consists of an encoder and a decoder. The encoder is responsible for transforming the input sequence into a fixeddimensional semantic representation, while the decoder uses this semantic representation to generate the output sequence. Within the encoder-decoder structure, self-attention mechanisms are commonly employed for sequence modeling, enabling the model to handle variable-length input and output sequences. This architecture has proven to be highly effective in various sequence-to-sequence tasks, such as text translation, text summarization, and dialogue generation. Prominent examples of large language models following this architecture include ELMo, BERT, RoBERTa, among others [12]. \n\nCurrently, the most widely used architecture is the Causal Decoder, which is primarily employed for handling autoregressive generation tasks, where each element of the output sequence depends on previously generated elements. The Causal Decoder Architecture is an improvement over the Encoderdecoder structure, as it introduces an autoregressive mechanism in the decoder. This means that when generating the current element, the model only uses the elements generated before it. This ensures that the model does not have access to future information during the generation process, thereby preserving causality. The GPT series (e.g., GPT-3) is a typical example of models that use the Causal Decoder Architecture. These models generate text by sequentially producing words one by one, avoiding information leakage and enabling the generation of coherent and plausible text. \n\nCompared to the Encoder-decoder models, Decoder-only models offer several advantages due to their simpler structure, faster training and inference speed, suitability for pure generation tasks, and advantages in decoder self-supervision."
                        ],
                        "paper": {
                            "corpus_id": 267402678,
                            "title": "The evolution, applications, and future prospects of large language models: An in-depth overview",
                            "authors": [
                                {
                                    "authorId": "2282449950",
                                    "name": "Jiayin Li"
                                }
                            ],
                            "year": 2024,
                            "venue": "Applied and Computational Engineering",
                            "n_citations": 2
                        },
                        "score": 0.78076171875
                    },
                    {
                        "id": "(Kim et al., 2023)",
                        "snippets": [
                            "Cumulative Quantization Errors in Causal Attention. Causal attention, which integrates masking into self-attention to avoid referencing future tokens, is vital for text generation tasks. To comprehend the quantization characteristics of GLMs, we contrast the computation procedure of the self-attention map. For a Transformer encoder, the quantization error reflected on the self-attention map due to Q, K, and V computation is uniformly spread because of the simultaneous computing nature inherent in Transformer encoders. However, the mask in the causal attention accumulates quantization errors from each token, creating uneven errors in the output computation. \n\nTo elucidate, consider the encoder self-attention depicted at the top of Fig. 1(a). During the computation of the weighted sum of value representations for tokens 1 and 3, all attention probabilities in the self-attention map impacted by quantization errors are incorporated. On the other hand, in the decoder's causal attention, as illustrated at the bottom of Fig. 1(a), the output value representation is computed using only the self-attention probabilities of the current token and those before it. For example, while generating the output value representation for token 1 (highlighted in a blue box), only two attention probabilities affected by quantization error are used. In contrast, for token 3 (indicated by the red box), probabilities from all preceding tokens are incorporated. This observation underscores that the inherent design of causal attention results in an accumulation of quantization errors, especially towards the latter tokens. Given this characteristic, it becomes imperative to devise a decoder QAT approach that mitigates this uneven distribution of quantization errors within the causal attention mechanism. \n\nNecessity of Ground Truth Loss. In fine-tuning processes, encoder-only models, often used in Natural Language Understanding (NLU), and decoder-only models, common in Natural Language Generation (NLG) with causal language modeling approach, exhibit different mechanisms for receiving ground truth loss. Fig. 1(b) illustrates the differences. In NLU tasks, encoder models leverage the representation of a unique special token, transformed into a logit vector, for calculating cross-entropy loss, with the number of classes usually being a few [12]. Conversely, decoder models in NLG tasks predict the subsequent token for each input token, transforming each token representation into a logit vector with a class size equivalent to the vocabulary size, often exceeding 50k [25]. Hence, while encoder models gain ground truth loss from a single special token's logit, decoder models derive it from the output logits of all input tokens, which could provide more detailed token-level prediction information."
                        ],
                        "paper": {
                            "corpus_id": 260886785,
                            "title": "Token-Scaled Logit Distillation for Ternary Weight Generative Language Models",
                            "authors": [
                                {
                                    "authorId": "2141320070",
                                    "name": "Minsoo Kim"
                                },
                                {
                                    "authorId": "2144376191",
                                    "name": "Sihwa Lee"
                                },
                                {
                                    "authorId": "2265920992",
                                    "name": "Janghwan Lee"
                                },
                                {
                                    "authorId": "2158125346",
                                    "name": "S. Hong"
                                },
                                {
                                    "authorId": "2180828053",
                                    "name": "Duhyeuk Chang"
                                },
                                {
                                    "authorId": "66936521",
                                    "name": "Wonyong Sung"
                                },
                                {
                                    "authorId": "2506452",
                                    "name": "Jungwook Choi"
                                }
                            ],
                            "year": 2023,
                            "venue": "Neural Information Processing Systems",
                            "n_citations": 15
                        },
                        "score": 0.93896484375
                    },
                    {
                        "id": "(Ewer et al., 2024)",
                        "snippets": [
                            "Decoders use a causal attention, ensuring that each token attends only to the preceding tokens. In contrast, encoders allow all tokens to attend to each other by performing attention computation from scratch for each token prediction."
                        ],
                        "paper": {
                            "corpus_id": 273025546,
                            "title": "ENTP: Encoder-only Next Token Prediction",
                            "authors": [
                                {
                                    "authorId": "2323781863",
                                    "name": "Ethan Ewer"
                                },
                                {
                                    "authorId": "2253659910",
                                    "name": "Daewon Chae"
                                },
                                {
                                    "authorId": "2323820473",
                                    "name": "Thomas Zeng"
                                },
                                {
                                    "authorId": "2323851531",
                                    "name": "Jinkyu Kim"
                                },
                                {
                                    "authorId": "2323790154",
                                    "name": "Kangwook Lee"
                                }
                            ],
                            "year": 2024,
                            "venue": "arXiv.org",
                            "n_citations": 4
                        },
                        "score": 0.904296875
                    },
                    {
                        "id": "(Leemann et al., 2024)",
                        "snippets": [
                            "Practical implementations introduce subtle modifications into the process described previously.The most relevant distinction is made between encoder-only models, that include BERT [15] and its variants, and decoder-only models such as the GPT models [38,(Radford et al., 2019).\n\nEncoder-only models.Considering BERT as an example of an encoder-only model, the first token is used for the classification, i.e, r = 1.Usually, a special token [CLS] is prepended to the text at position 1, however this is not strictly necessary for the functioning of the model.\n\nDecoder-only models.In contrast, decoder-models like GPT-2 (Radford et al., 2019) add the classification head on top of the last token for classification, i.e., r = |t|.A key difference is that in GPT-2 and other decoder-only models, a causal mask is laid over the attention matrix, resulting in \u03b1 i,j = 0 for j > i.This encodes the constraint that tokens can only attend to themselves or to previous ones."
                        ],
                        "paper": {
                            "corpus_id": 269982953,
                            "title": "Attention Mechanisms Don't Learn Additive Models: Rethinking Feature Importance for Transformers",
                            "authors": [
                                {
                                    "authorId": "2130899453",
                                    "name": "Tobias Leemann"
                                },
                                {
                                    "authorId": "2302795616",
                                    "name": "Alina Fastowski"
                                },
                                {
                                    "authorId": "2317114785",
                                    "name": "Felix Pfeiffer"
                                },
                                {
                                    "authorId": "1686448",
                                    "name": "Gjergji Kasneci"
                                }
                            ],
                            "year": 2024,
                            "venue": "Trans. Mach. Learn. Res.",
                            "n_citations": 5
                        },
                        "score": 0.69287109375
                    },
                    {
                        "id": "(Jiang et al., 2024)",
                        "snippets": [
                            "In encoder-decoder LLMs, a pivot token is randomly selected in a sequence of tokens and then regarding the context before it as the source sequence x = {1, . . ., } of the encoder and the sequence after it as the target output x = {+1, . . ., } of decoder.Formally, the causal language modeling objective for training encoder-decoder LLMs is to minimize loss function as follows:\n\nwhere x\u2264 is the source sequence input and x< denotes the target sequence autoregressively generated so far",
                            "The conditional probability (|x<)) is modeled by adding a causal attention mask to the multi-head self-attention matrix of each Transformer block.To be specific, causal attention masking is implemented by setting the lower triangular part of the matrix to 0 and the remaining elements to \u2212\u221e, ensuring that each token attends only to its predecessors and itself."
                        ],
                        "paper": {
                            "corpus_id": 270214176,
                            "title": "A Survey on Large Language Models for Code Generation",
                            "authors": [
                                {
                                    "authorId": "2294682530",
                                    "name": "Juyong Jiang"
                                },
                                {
                                    "authorId": "2304542351",
                                    "name": "Fan Wang"
                                },
                                {
                                    "authorId": "2305041631",
                                    "name": "Jiasi Shen"
                                },
                                {
                                    "authorId": "2304525068",
                                    "name": "Sungju Kim"
                                },
                                {
                                    "authorId": "2257349580",
                                    "name": "Sunghun Kim"
                                }
                            ],
                            "year": 2024,
                            "venue": "arXiv.org",
                            "n_citations": 197
                        },
                        "score": 0.66064453125
                    }
                ],
                "format": "synthesis",
                "table": null,
                "model": "claude-3-7-sonnet-20250219"
            },
            {
                "title": "Non-Causal Decoder Models",
                "tldr": "Non-causal decoder models, also known as prefix decoder models, allow bidirectional attention over prefix tokens while maintaining unidirectional attention for generated tokens. This hybrid approach combines the comprehensive understanding capabilities of encoders with the generative abilities of decoders, making them effective for tasks requiring both deep context understanding and text generation. (14 sources)",
                "text": "\nNon-causal decoder models represent an architectural variation that addresses some of the limitations of standard causal decoder models. Unlike causal decoders that employ strictly unidirectional attention, non-causal decoders modify the attention masking pattern to allow bidirectional attention for a portion of the input sequence (the prefix) while maintaining unidirectional attention for the tokens being generated <Paper corpusId=\"248118752\" paperTitle=\"(Wang et al., 2022)\" isShortName></Paper> <Paper corpusId=\"266755678\" paperTitle=\"(Liu et al., 2024)\" isShortName></Paper>.\n\nThis architecture is also commonly referred to as a \"prefix decoder\" or \"prefix language model\" (PrefixLM) approach <Paper corpusId=\"247625205\" paperTitle=\"(Scao et al._1, 2022)\" isShortName></Paper> <Paper corpusId=\"147704286\" paperTitle=\"(Dong et al., 2019)\" isShortName></Paper> <Paper corpusId=\"3608234\" paperTitle=\"(Liu et al., 2018)\" isShortName></Paper>. The key distinction lies in how the self-attention mask is configured: the prefix portion uses a fully visible (non-causal) mask that allows tokens to attend to all positions in the prefix, while the generation portion maintains the traditional causal mask that restricts attention to previous tokens only <Paper corpusId=\"268157336\" paperTitle=\"(Patil et al., 2024)\" isShortName></Paper>.\n\nThis hybrid approach offers several advantages:\n\n1. **Rich bidirectional representations**: By allowing bidirectional attention in the prefix portion, non-causal decoder models can build richer representations of the input context, similar to how encoders process information <Paper corpusId=\"248118752\" paperTitle=\"(Wang et al., 2022)\" isShortName></Paper>.\n\n2. **Combined capabilities**: Non-causal decoders effectively combine the comprehensive understanding capabilities of encoders with the generative abilities of decoders within a single model architecture <Paper corpusId=\"270702559\" paperTitle=\"(Yin et al., 2024)\" isShortName></Paper>.\n\n3. **Parameter efficiency**: Unlike encoder-decoder models that require separate parameters for encoding and decoding, non-causal decoder models share the same parameters for both processes, potentially making them more parameter-efficient <Paper corpusId=\"270702559\" paperTitle=\"(Yin et al., 2024)\" isShortName></Paper>.\n\nIn implementation, the non-causal decoder architecture replaces the standard causal mask with a non-causal mask in the self-attention module, allowing tokens to attend to all positions rather than only previous positions <Paper corpusId=\"257050658\" paperTitle=\"(Liu et al., 2023)\" isShortName></Paper>. This approach bears similarities to the attention mechanisms used in non-autoregressive translation (NAT) models, which adopt unmasked self-attention over all target tokens <Paper corpusId=\"248266379\" paperTitle=\"(Xiao et al., 2022)\" isShortName></Paper> <Paper corpusId=\"13756489\" paperTitle=\"(Vaswani et al., 2017)\" isShortName></Paper>.\n\nNotable examples of models employing non-causal decoder architectures include GLM-130B <Paper corpusId=\"277349741\" paperTitle=\"(Nie et al., 2025)\" isShortName></Paper> and U-PaLM <Paper corpusId=\"270702559\" paperTitle=\"(Yin et al., 2024)\" isShortName></Paper> <Paper corpusId=\"252780443\" paperTitle=\"(Tay et al., 2022)\" isShortName></Paper>. Additionally, research has shown that causal decoder-only models can be efficiently adapted into non-causal decoders by extending pretraining with span corruption objectives, potentially improving zero-shot generalization after multitask fine-tuning <Paper corpusId=\"247625205\" paperTitle=\"(Scao et al._1, 2022)\" isShortName></Paper>.\n\nIt's worth noting that while the distinction between decoder-only and encoder-decoder models can sometimes blur, there are still fundamental differences in how they process information. Encoder-decoder models process input and target sequences independently with different parameters and include a cross-attention component, while decoder-only models (including prefix decoders) process inputs and targets by concatenating them <Paper corpusId=\"252780443\" paperTitle=\"(Tay et al., 2022)\" isShortName></Paper> <Paper corpusId=\"263829839\" paperTitle=\"(Saha et al., 2023)\" isShortName></Paper> <Paper corpusId=\"204838007\" paperTitle=\"(Raffel et al., 2019)\" isShortName></Paper>.",
                "citations": [
                    {
                        "id": "(Wang et al., 2022)",
                        "snippets": [
                            "Causal decoder-only. Although the encoder-decoder is the original Transformer variant, most recent LLMs use a decoder-only architecture. These models can be trained as a traditional language model (i.e. to predict the next token in a sequence). Decoder-only models have no independent means of processing or representing the input sequence and target sequence differently-all tokens are processed in an equivalent fashion, and, because of the causal masking pattern, conditioning is simply based on past tokens (see Figure 2, on the left). On the one hand, this means that the representation for any conditioning text is inherently weaker; on the other hand, it yields a simpler architecture that is naturally suited to a standard autoregressive next-step-prediction pretraining objective. We refer to this architecture as causal decoder-only (CD) .\n\nNon-causal decoder-only. To allow decoder-only models to build richer non-causal representations of the input/conditioning text, it has been proposed to simply modify the attention mask used. Specifically, the self-attention masking pattern can be changed so that the region of the input sequence corresponding to conditioning information has a non-causal mask (i.e. attention in this region is not restricted to past tokens, see middle of Figure 2), as in the encoder of an encoder-decoder architecture. We refer to this architecture as non-causal decoder-only (ND) .\n\nComparisons across architectures. Decoder-only models process a single sequence consisting of the concatenation of the input and target text. On the other hand, in an encoder-decoder, the encoder processes only the input and the decoder processes only the target."
                        ],
                        "paper": {
                            "corpus_id": 248118752,
                            "title": "What Language Model Architecture and Pretraining Objective Work Best for Zero-Shot Generalization?",
                            "authors": [
                                {
                                    "authorId": "2135734748",
                                    "name": "Thomas Wang"
                                },
                                {
                                    "authorId": "145625142",
                                    "name": "Adam Roberts"
                                },
                                {
                                    "authorId": "80424302",
                                    "name": "Daniel Hesslow"
                                },
                                {
                                    "authorId": "1379806208",
                                    "name": "Teven Le Scao"
                                },
                                {
                                    "authorId": "3351938",
                                    "name": "Hyung Won Chung"
                                },
                                {
                                    "authorId": "46181066",
                                    "name": "Iz Beltagy"
                                },
                                {
                                    "authorId": "143945447",
                                    "name": "Julien Launay"
                                },
                                {
                                    "authorId": "2402716",
                                    "name": "Colin Raffel"
                                }
                            ],
                            "year": 2022,
                            "venue": "International Conference on Machine Learning",
                            "n_citations": 175
                        },
                        "score": 0.84423828125
                    },
                    {
                        "id": "(Liu et al., 2024)",
                        "snippets": [
                            "LLMs with a Decoder-only architecture utilize the decoder component of the traditional Transformer architecture. Unlike the Encoder-Decoder architecture, which incorporates both an encoder and a decoder, the Decoder-only architecture is solely focused on the decoding process. In this configuration, the model sequentially generates tokens, attending to preceding tokens in the sequence. This architecture has been applied to various language generation tasks, showcasing its effectiveness in various tasks such as text generation without the need for an explicit encoding phase. The Decoder-only architecture can be further classified into two categories: the Causal Decoder architecture and the Prefix Decoder architecture. \n\nThe Causal Decoder Architecture: In the Causal Decoder architecture, each token in the model input sequence can only attend to past input tokens and itself during the decoding process. It achieves unidirectional attention to the input sequence by using a specific mask as shown in Figure 1. In fact, different architectures are mainly implemented by configuring different mask matrices. The figure illustrates a comparison of mask configurations between the Encoderdecoder and Decoder-only architectures (including Casual Decoder and Prefix Decoder).\n\nThe Prefix Decoder Architecture: The Prefix Decoder architecture combines the advantages of both the Encoderdecoder and Causal Decoder architectures. It leverages its unique mask configurations, as illustrated in Figure 1, enabling bidirectional attention for tokens in the prefix while maintaining unidirectional attention for generating subsequent tokens [54]. This design allows for the autoregressive generation of the output sequence with the flexibility to attend bi-directionally to the prefix tokens."
                        ],
                        "paper": {
                            "corpus_id": 266755678,
                            "title": "Understanding LLMs: A Comprehensive Overview from Training to Inference",
                            "authors": [
                                {
                                    "authorId": "2116426849",
                                    "name": "Yi-Hsueh Liu"
                                },
                                {
                                    "authorId": "2155082967",
                                    "name": "Haoyang He"
                                },
                                {
                                    "authorId": "2184719751",
                                    "name": "Tianle Han"
                                },
                                {
                                    "authorId": "2273584640",
                                    "name": "Xu Zhang"
                                },
                                {
                                    "authorId": "2210636248",
                                    "name": "Mengyuan Liu"
                                },
                                {
                                    "authorId": "2257433902",
                                    "name": "Jiaming Tian"
                                },
                                {
                                    "authorId": "2257095790",
                                    "name": "Yutong Zhang"
                                },
                                {
                                    "authorId": "2110238778",
                                    "name": "Jiaqi Wang"
                                },
                                {
                                    "authorId": "2277869261",
                                    "name": "Xiaohui Gao"
                                },
                                {
                                    "authorId": "2215167446",
                                    "name": "Tianyang Zhong"
                                },
                                {
                                    "authorId": "2221032216",
                                    "name": "Yi Pan"
                                },
                                {
                                    "authorId": "2211904452",
                                    "name": "Shaochen Xu"
                                },
                                {
                                    "authorId": "2263593041",
                                    "name": "Zihao Wu"
                                },
                                {
                                    "authorId": "2145977326",
                                    "name": "Zheng Liu"
                                },
                                {
                                    "authorId": "2257586495",
                                    "name": "Xin Zhang"
                                },
                                {
                                    "authorId": "2277750447",
                                    "name": "Shu Zhang"
                                },
                                {
                                    "authorId": "1742535",
                                    "name": "Xintao Hu"
                                },
                                {
                                    "authorId": "49104946",
                                    "name": "Tuo Zhang"
                                },
                                {
                                    "authorId": "2251076040",
                                    "name": "Ning Qiang"
                                },
                                {
                                    "authorId": "2254792886",
                                    "name": "Tianming Liu"
                                },
                                {
                                    "authorId": "2257302793",
                                    "name": "Bao Ge"
                                }
                            ],
                            "year": 2024,
                            "venue": "Neurocomputing",
                            "n_citations": 74
                        },
                        "score": 0.8115234375
                    },
                    {
                        "id": "(Scao et al._1, 2022)",
                        "snippets": [
                            "Alternatives include encoder-decoder models trained with a span-corruption objective (e.g., T5 Raffel et al. (2019)), as well as non-causal decoders models with visibility over a prefix (so-called Prefix LMs, (Liu et al., 2018); (Dong et al., 2019))",
                            "Following autoregressive pretraining, decoder-only models can be efficiently adapted into non-causal decoders, simply by extending pretraining with span corruption. This adaptation produces a second model, which can provide excellent zero-shot generalization after multitask finetuning."
                        ],
                        "paper": {
                            "corpus_id": 247625205,
                            "title": "What Language Model to Train if You Have One Million GPU Hours?",
                            "authors": [
                                {
                                    "authorId": "1379806208",
                                    "name": "Teven Le Scao"
                                },
                                {
                                    "authorId": "2135734748",
                                    "name": "Thomas Wang"
                                },
                                {
                                    "authorId": "80424302",
                                    "name": "Daniel Hesslow"
                                },
                                {
                                    "authorId": "2113836860",
                                    "name": "Lucile Saulnier"
                                },
                                {
                                    "authorId": "32136590",
                                    "name": "Stas Bekman"
                                },
                                {
                                    "authorId": "2054179756",
                                    "name": "Saiful Bari"
                                },
                                {
                                    "authorId": "103476203",
                                    "name": "Stella Biderman"
                                },
                                {
                                    "authorId": "2218938",
                                    "name": "Hady ElSahar"
                                },
                                {
                                    "authorId": "2037383772",
                                    "name": "Niklas Muennighoff"
                                },
                                {
                                    "authorId": "80842917",
                                    "name": "Jason Phang"
                                },
                                {
                                    "authorId": "40170001",
                                    "name": "Ofir Press"
                                },
                                {
                                    "authorId": "2402716",
                                    "name": "Colin Raffel"
                                },
                                {
                                    "authorId": "2285868436",
                                    "name": "Victor Sanh"
                                },
                                {
                                    "authorId": "2191455",
                                    "name": "Sheng Shen"
                                },
                                {
                                    "authorId": "35566806",
                                    "name": "Lintang Sutawika"
                                },
                                {
                                    "authorId": "2112211652",
                                    "name": "Jaesung Tae"
                                },
                                {
                                    "authorId": "1725420331",
                                    "name": "Zheng-Xin Yong"
                                },
                                {
                                    "authorId": "143945447",
                                    "name": "Julien Launay"
                                },
                                {
                                    "authorId": "46181066",
                                    "name": "Iz Beltagy"
                                }
                            ],
                            "year": 2022,
                            "venue": "Conference on Empirical Methods in Natural Language Processing",
                            "n_citations": 109
                        },
                        "score": 0.6279296875
                    },
                    {
                        "id": "(Dong et al., 2019)",
                        "snippets": [
                            "This paper presents a new Unified pre-trained Language Model (UniLM) that can be fine-tuned for both natural language understanding and generation tasks. The model is pre-trained using three types of language modeling tasks: unidirectional, bidirectional, and sequence-to-sequence prediction. The unified modeling is achieved by employing a shared Transformer network and utilizing specific self-attention masks to control what context the prediction conditions on. UniLM compares favorably with BERT on the GLUE benchmark, and the SQuAD 2.0 and CoQA question answering tasks. Moreover, UniLM achieves new state-of-the-art results on five natural language generation datasets, including improving the CNN/DailyMail abstractive summarization ROUGE-L to 40.51 (2.04 absolute improvement), the Gigaword abstractive summarization ROUGE-L to 35.75 (0.86 absolute improvement), the CoQA generative question answering F1 score to 82.5 (37.1 absolute improvement), the SQuAD question generation BLEU-4 to 22.12 (3.75 absolute improvement), and the DSTC7 document-grounded dialog response generation NIST-4 to 2.67 (human performance is 2.65). The code and pre-trained models are available at this https URL."
                        ],
                        "paper": {
                            "corpus_id": 147704286,
                            "title": "Unified Language Model Pre-training for Natural Language Understanding and Generation",
                            "authors": [
                                {
                                    "authorId": "145307652",
                                    "name": "Li Dong"
                                },
                                {
                                    "authorId": "144610884",
                                    "name": "Nan Yang"
                                },
                                {
                                    "authorId": "51456429",
                                    "name": "Wenhui Wang"
                                },
                                {
                                    "authorId": "49807919",
                                    "name": "Furu Wei"
                                },
                                {
                                    "authorId": "46522098",
                                    "name": "Xiaodong Liu"
                                },
                                {
                                    "authorId": "72682749",
                                    "name": "Yu Wang"
                                },
                                {
                                    "authorId": "1800422",
                                    "name": "Jianfeng Gao"
                                },
                                {
                                    "authorId": "143849609",
                                    "name": "M. Zhou"
                                },
                                {
                                    "authorId": "145058181",
                                    "name": "H. Hon"
                                }
                            ],
                            "year": 2019,
                            "venue": "Neural Information Processing Systems",
                            "n_citations": 1560
                        },
                        "score": 0
                    },
                    {
                        "id": "(Liu et al., 2018)",
                        "snippets": [
                            "We show that generating English Wikipedia articles can be approached as a multi- document summarization of source documents. We use extractive summarization to coarsely identify salient information and a neural abstractive model to generate the article. For the abstractive model, we introduce a decoder-only architecture that can scalably attend to very long sequences, much longer than typical encoder- decoder architectures used in sequence transduction. We show that this model can generate fluent, coherent multi-sentence paragraphs and even whole Wikipedia articles. When given reference documents, we show it can extract relevant factual information as reflected in perplexity, ROUGE scores and human evaluations."
                        ],
                        "paper": {
                            "corpus_id": 3608234,
                            "title": "Generating Wikipedia by Summarizing Long Sequences",
                            "authors": [
                                {
                                    "authorId": "35025299",
                                    "name": "Peter J. Liu"
                                },
                                {
                                    "authorId": "144413479",
                                    "name": "Mohammad Saleh"
                                },
                                {
                                    "authorId": "38627717",
                                    "name": "Etienne Pot"
                                },
                                {
                                    "authorId": "2065067542",
                                    "name": "Ben Goodrich"
                                },
                                {
                                    "authorId": "35474601",
                                    "name": "Ryan Sepassi"
                                },
                                {
                                    "authorId": "40527594",
                                    "name": "Lukasz Kaiser"
                                },
                                {
                                    "authorId": "1846258",
                                    "name": "Noam M. Shazeer"
                                }
                            ],
                            "year": 2018,
                            "venue": "International Conference on Learning Representations",
                            "n_citations": 801
                        },
                        "score": 0
                    },
                    {
                        "id": "(Patil et al., 2024)",
                        "snippets": [
                            "Encoder-decoder models adopt bidirectional attention for the encoder, unidirectional attention for the decoder, and a cross-attention mechanism between them. Cross-attention in the decoder has access only to the fully processed encoder output and is responsible for connecting input tokens to target tokens",
                            "Prefix language models are also decoder-only-based models but differ in the masking mechanism. Instead of a causal mask, a fully visible mask is used for the prefix part of the input sequence, and a causal mask is used for the target sequence",
                            "In a decoder-only model, the input and target are concatenated, and then a causal mask is used throughout. A decoder-only model with a prefix allows fully visible masking over part of the input token (prefix), followed by causal masking on the rest of the sequence. In general, autoencoding models learn bidirectional contextualized representation suited for NLU tasks, whereas autoregressive models learn to generate the next token and hence are suited for NLG tasks."
                        ],
                        "paper": {
                            "corpus_id": 268157336,
                            "title": "A Review of Current Trends, Techniques, and Challenges in Large Language Models (LLMs)",
                            "authors": [
                                {
                                    "authorId": "2289385425",
                                    "name": "Rajvardhan Patil"
                                },
                                {
                                    "authorId": "117730513",
                                    "name": "Venkat Gudivada"
                                }
                            ],
                            "year": 2024,
                            "venue": "Applied Sciences",
                            "n_citations": 80
                        },
                        "score": 0.77587890625
                    },
                    {
                        "id": "(Yin et al., 2024)",
                        "snippets": [
                            "Typical architectures for LLMs can be categorized as encoder-decoder, causal decoder, and prefix decoder (Wang and Hesslow, 2022;Zhao et al., 2023).Among them, the causal decoder architecture is the most frequently used by various LLMs, such as OPT (Susan Zhang et al., 2023), LLAMA (Touvron et al., 2023a), BLOOM (Scao et al., 2023) due to its superior zero-shot and fewshot generalization capacity (Wang and Hesslow, 2022) and the effectiveness of scaling law (Brown et al., 2020;Kaplan et al., 2020).The causal decoder architecture employs a unidirectional attention mask, ensuring that each input token can only attend to preceding tokens and itself.Both input and output tokens are processed identically through the decoder (Zhao et al., 2023).\n\nThe prefix decoder architecture, also known as the non-causal decoder, modifies the masking mechanism used in causal decoders (Zhang et al., 2020).This adjustment allows for bidirectional attention over the prefix tokens while restricting attention to unidirectional generated tokens (Dong et al., 2019).Similar to the encoder-decoder architecture, prefix decoders can bidirectionally encode the prefix sequence and autoregressively predict the output tokens sequentially (Zhao et al., 2023).The same parameters are shared during both encoding and decoding processes.Representative LLMs based on prefix decoders include GLM-130B (Zeng et al., 2023) and U-PaLM (Tay et al., 2022).\n\nThe vanilla Transformer model is constructed on the encoder-decoder architecture (Vaswani et al., 2017), comprising two stacks of Transformer blocks designated as the encoder and decoder, respectively.The encoder utilizes stacked multi-head self-attention layers to encode the input sequence into its latent representations.Meanwhile, the decoder employs cross-attention mechanisms on these representations, and autoregressive generates the target sequence."
                        ],
                        "paper": {
                            "corpus_id": 270702559,
                            "title": "CrisisSense-LLM: Instruction Fine-Tuned Large Language Model for Multi-label Social Media Text Classification in Disaster Informatics",
                            "authors": [
                                {
                                    "authorId": "2265383225",
                                    "name": "Kai Yin"
                                },
                                {
                                    "authorId": "2308073678",
                                    "name": "Chengkai Liu"
                                },
                                {
                                    "authorId": "2258714985",
                                    "name": "Ali Mostafavi"
                                },
                                {
                                    "authorId": "2308068627",
                                    "name": "Xia Hu"
                                }
                            ],
                            "year": 2024,
                            "venue": "arXiv.org",
                            "n_citations": 12
                        },
                        "score": 0.74267578125
                    },
                    {
                        "id": "(Liu et al., 2023)",
                        "snippets": [
                            "For the modification of the self-attention mask, because the decoder input is the copied sequence of encoder input, the self-attention module is allowed to attend all positions, rather than only left positions in the conventional Transformer decoder. Therefore, the self-attention mask is replaced with a non-causal mask in our non-autoregressive decoder."
                        ],
                        "paper": {
                            "corpus_id": 257050658,
                            "title": "Parallel Sentence-Level Explanation Generation for Real-World Low-Resource Scenarios",
                            "authors": [
                                {
                                    "authorId": "1679704",
                                    "name": "Y. Liu"
                                },
                                {
                                    "authorId": "2292210488",
                                    "name": "Xiaokang Chen"
                                },
                                {
                                    "authorId": "2072900666",
                                    "name": "Qianwen Dai"
                                }
                            ],
                            "year": 2023,
                            "venue": "IEEE International Conference on Acoustics, Speech, and Signal Processing",
                            "n_citations": 4
                        },
                        "score": 0.638671875
                    },
                    {
                        "id": "(Xiao et al., 2022)",
                        "snippets": [
                            "(1) Specifically, AT models need to prevent earlier decoding steps from peeking at information from later steps. Therefore, the constraint of an autoregressive factorization of the output distribution is required, and they adopt the strict causal mask by applying a lower triangular matrix in the self-attention module of the conventional Transformer decoder (Vaswani et al., 2017). (2) However, for NAT models, including the iteration-based NAT models, this constraint is no longer necessary, so they adopt the unmasked self-attention over all target tokens [16]."
                        ],
                        "paper": {
                            "corpus_id": 248266379,
                            "title": "A Survey on Non-Autoregressive Generation for Neural Machine Translation and Beyond",
                            "authors": [
                                {
                                    "authorId": "152678922",
                                    "name": "Yisheng Xiao"
                                },
                                {
                                    "authorId": "47767550",
                                    "name": "Lijun Wu"
                                },
                                {
                                    "authorId": "13838086",
                                    "name": "Junliang Guo"
                                },
                                {
                                    "authorId": "2109013629",
                                    "name": "Juntao Li"
                                },
                                {
                                    "authorId": "39767557",
                                    "name": "M. Zhang"
                                },
                                {
                                    "authorId": "143826491",
                                    "name": "Tao Qin"
                                },
                                {
                                    "authorId": "2110264337",
                                    "name": "Tie-Yan Liu"
                                }
                            ],
                            "year": 2022,
                            "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence",
                            "n_citations": 87
                        },
                        "score": 0.75
                    },
                    {
                        "id": "(Vaswani et al., 2017)",
                        "snippets": [
                            "The dominant sequence transduction models are based on complex recurrent or convolutional neural networks in an encoder-decoder configuration. The best performing models also connect the encoder and decoder through an attention mechanism. We propose a new simple network architecture, the Transformer, based solely on attention mechanisms, dispensing with recurrence and convolutions entirely. Experiments on two machine translation tasks show these models to be superior in quality while being more parallelizable and requiring significantly less time to train. Our model achieves 28.4 BLEU on the WMT 2014 English-to-German translation task, improving over the existing best results, including ensembles by over 2 BLEU. On the WMT 2014 English-to-French translation task, our model establishes a new single-model state-of-the-art BLEU score of 41.8 after training for 3.5 days on eight GPUs, a small fraction of the training costs of the best models from the literature. We show that the Transformer generalizes well to other tasks by applying it successfully to English constituency parsing both with large and limited training data."
                        ],
                        "paper": {
                            "corpus_id": 13756489,
                            "title": "Attention is All you Need",
                            "authors": [
                                {
                                    "authorId": "40348417",
                                    "name": "Ashish Vaswani"
                                },
                                {
                                    "authorId": "1846258",
                                    "name": "Noam M. Shazeer"
                                },
                                {
                                    "authorId": "3877127",
                                    "name": "Niki Parmar"
                                },
                                {
                                    "authorId": "39328010",
                                    "name": "Jakob Uszkoreit"
                                },
                                {
                                    "authorId": "145024664",
                                    "name": "Llion Jones"
                                },
                                {
                                    "authorId": "19177000",
                                    "name": "Aidan N. Gomez"
                                },
                                {
                                    "authorId": "40527594",
                                    "name": "Lukasz Kaiser"
                                },
                                {
                                    "authorId": "3443442",
                                    "name": "I. Polosukhin"
                                }
                            ],
                            "year": 2017,
                            "venue": "Neural Information Processing Systems",
                            "n_citations": 132444
                        },
                        "score": 0
                    },
                    {
                        "id": "(Nie et al., 2025)",
                        "snippets": [
                            "\u2022 Causal decoder. As a representative decoder-only architecture, causal decoder models introduce the unidirectional attention mask to ensure that each input token can only attend to the past tokens and itself. This mechanism makes them suitable for text generation tasks. Prominent examples are the GPTseries models (Radford et al., 2018(Radford et al., 2019)(Brown et al., 2020). \n\n\u2022 Non-causal decoder. Another kind of decoder-only architecture is the non-casual structure. This architecture performs bidirectional attention on prefix tokens and unidirectional attention only on generated tokens. One representative prefix decoder LLMs is GLM (Zeng et al., 2022)."
                        ],
                        "paper": {
                            "corpus_id": 277349741,
                            "title": "Exploring the Roles of Large Language Models in Reshaping Transportation Systems: A Survey, Framework, and Roadmap",
                            "authors": [
                                {
                                    "authorId": "94168461",
                                    "name": "Tong Nie"
                                },
                                {
                                    "authorId": "2028643500",
                                    "name": "Jiangming Sun"
                                },
                                {
                                    "authorId": "2277421553",
                                    "name": "Wei Ma"
                                }
                            ],
                            "year": 2025,
                            "venue": "arXiv.org",
                            "n_citations": 4
                        },
                        "score": 0.658203125
                    },
                    {
                        "id": "(Tay et al., 2022)",
                        "snippets": [
                            "The line between decoder-only and encoder-decoder models is less clear. PrefixLM models are almost encoder-decoder models with shared parameters (but not quite). From an inductive bias point of view, there are multiple differences. Encoder-Decoder models process input and targets independently with a different set of parameters. This is a form of sparsity where different set of parameters are used for different tokens. Encoder-Decoder models also have a cross attention component that connects input tokens to target tokens. Meanwhile, decoder-only models process inputs and targets by concatenating them. Hence, the representations of inputs and targets are concurrently build layer by layer as the input/targets propagate up the network. Conversely, the decoder in Encoder-decoder models generally only looks at the fully processed encoder input. Overall, the inductive bias of PrefixLM decoder-only models and Encoder-Decoder models could be pretty similar modulo the subtle differences stated above. The distinct property is that Encoder-Decoder models are generally approximately 2x parameters of a decoder-only model when compute-matched."
                        ],
                        "paper": {
                            "corpus_id": 252780443,
                            "title": "UL2: Unifying Language Learning Paradigms",
                            "authors": [
                                {
                                    "authorId": "144447820",
                                    "name": "Yi Tay"
                                },
                                {
                                    "authorId": "3226635",
                                    "name": "Mostafa Dehghani"
                                },
                                {
                                    "authorId": "2057663102",
                                    "name": "Vinh Q. Tran"
                                },
                                {
                                    "authorId": "143936294",
                                    "name": "Xavier Garc\u00eda"
                                },
                                {
                                    "authorId": "119640649",
                                    "name": "Jason Wei"
                                },
                                {
                                    "authorId": "1524732527",
                                    "name": "Xuezhi Wang"
                                },
                                {
                                    "authorId": "3351938",
                                    "name": "Hyung Won Chung"
                                },
                                {
                                    "authorId": "2119725651",
                                    "name": "Dara Bahri"
                                },
                                {
                                    "authorId": "32303439",
                                    "name": "Tal Schuster"
                                },
                                {
                                    "authorId": "2115689465",
                                    "name": "H. Zheng"
                                },
                                {
                                    "authorId": "65855107",
                                    "name": "Denny Zhou"
                                },
                                {
                                    "authorId": "2815290",
                                    "name": "N. Houlsby"
                                },
                                {
                                    "authorId": "1680617",
                                    "name": "Donald Metzler"
                                }
                            ],
                            "year": 2022,
                            "venue": "International Conference on Learning Representations",
                            "n_citations": 313
                        },
                        "score": 0.84814453125
                    },
                    {
                        "id": "(Saha et al., 2023)",
                        "snippets": [
                            "The encoder component transforms input tokens into vectors using embeddings and positional encodings, then applies multihead self-attention and feedforward networks. The decoder, starting similarly, incorporates masked self-attention and crossattention with the output of the encoder, ensuring alignment and preventing future word prediction. Sequence-to-sequence models often use this architecture, where the input and output sequences can be of different lengths. These models shine in tasks where there is a direct and complex transformation between inputs and outputs. Examples like machine translation and text summarization are prototypical, as they require the model to understand the input deeply and generate a coherent and contextually accurate output. BART [5], T5 (Raffel et al., 2019), and UL2 (Tay et al., 2022) are a few well-known encoder-decoder models to be named",
                            ".In a decoder-only model, a sequence is fed into the model, which then directly predicts the next token or word in the sequence. It operates autoregressively, using its generated tokens as context for subsequent predictions. It has two variants: causal decoder and prefix decoder. In a standard decoder (causal decoder), the unidirectional attention masking ensures that a token attends only to previous tokens and itself. Prefix decoders permit bidirectional attention over prefix tokens while maintaining unidirectional attention to generated tokens."
                        ],
                        "paper": {
                            "corpus_id": 263829839,
                            "title": "LLM for SoC Security: A Paradigm Shift",
                            "authors": [
                                {
                                    "authorId": "2256992493",
                                    "name": "Dipayan Saha"
                                },
                                {
                                    "authorId": "2114625129",
                                    "name": "Shams Tarek"
                                },
                                {
                                    "authorId": "2256991081",
                                    "name": "Katayoon Yahyaei"
                                },
                                {
                                    "authorId": "2231854143",
                                    "name": "Sujan Kumar Saha"
                                },
                                {
                                    "authorId": "2257235852",
                                    "name": "Jingbo Zhou"
                                },
                                {
                                    "authorId": "145954982",
                                    "name": "M. Tehranipoor"
                                },
                                {
                                    "authorId": "1997019",
                                    "name": "Farimah Farahmandi"
                                }
                            ],
                            "year": 2023,
                            "venue": "IEEE Access",
                            "n_citations": 54
                        },
                        "score": 0.79833984375
                    },
                    {
                        "id": "(Raffel et al., 2019)",
                        "snippets": [
                            "Transfer learning, where a model is first pre-trained on a data-rich task before being fine-tuned on a downstream task, has emerged as a powerful technique in natural language processing (NLP). The effectiveness of transfer learning has given rise to a diversity of approaches, methodology, and practice. In this paper, we explore the landscape of transfer learning techniques for NLP by introducing a unified framework that converts every language problem into a text-to-text format. Our systematic study compares pre-training objectives, architectures, unlabeled datasets, transfer approaches, and other factors on dozens of language understanding tasks. By combining the insights from our exploration with scale and our new \"Colossal Clean Crawled Corpus\", we achieve state-of-the-art results on many benchmarks covering summarization, question answering, text classification, and more. To facilitate future work on transfer learning for NLP, we release our dataset, pre-trained models, and code."
                        ],
                        "paper": {
                            "corpus_id": 204838007,
                            "title": "Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer",
                            "authors": [
                                {
                                    "authorId": "2402716",
                                    "name": "Colin Raffel"
                                },
                                {
                                    "authorId": "1846258",
                                    "name": "Noam M. Shazeer"
                                },
                                {
                                    "authorId": "145625142",
                                    "name": "Adam Roberts"
                                },
                                {
                                    "authorId": "3844009",
                                    "name": "Katherine Lee"
                                },
                                {
                                    "authorId": "46617804",
                                    "name": "Sharan Narang"
                                },
                                {
                                    "authorId": "1380243217",
                                    "name": "Michael Matena"
                                },
                                {
                                    "authorId": "2389316",
                                    "name": "Yanqi Zhou"
                                },
                                {
                                    "authorId": "2157338362",
                                    "name": "Wei Li"
                                },
                                {
                                    "authorId": "35025299",
                                    "name": "Peter J. Liu"
                                }
                            ],
                            "year": 2019,
                            "venue": "Journal of machine learning research",
                            "n_citations": 20336
                        },
                        "score": 0
                    }
                ],
                "format": "synthesis",
                "table": null,
                "model": "claude-3-7-sonnet-20250219"
            },
            {
                "title": "Encoder-Decoder Models",
                "tldr": "Encoder-decoder models combine an encoder that processes input using bidirectional attention with a decoder that generates output using causal attention. This architecture excels at sequence-to-sequence tasks like translation and summarization by leveraging cross-attention mechanisms to connect the encoded input to the generated output. (20 sources)",
                "text": "\nEncoder-decoder models represent the original architecture introduced in the vanilla Transformer <Paper corpusId=\"13756489\" paperTitle=\"(Vaswani et al., 2017)\" isShortName></Paper>. This architecture consists of two distinct components: an encoder that processes the input sequence and a decoder that generates the output sequence <Paper corpusId=\"266755678\" paperTitle=\"(Liu et al., 2024)\" isShortName></Paper> <Paper corpusId=\"270702559\" paperTitle=\"(Yin et al., 2024)\" isShortName></Paper>.\n\nThe encoder transforms input tokens into contextual representations using bidirectional self-attention, allowing each token to attend to all positions in the input sequence <Paper corpusId=\"261064777\" paperTitle=\"(Zheng et al., 2023)\" isShortName></Paper> <Paper corpusId=\"221702858\" paperTitle=\"(Tay et al., 2020)\" isShortName></Paper>. This bidirectional processing enables comprehensive understanding of the input context from both directions.\n\nIn contrast, the decoder component employs causal self-attention, where each token can only attend to itself and previous tokens, ensuring autoregressive generation <Paper corpusId=\"263729712\" paperTitle=\"(Alomari et al., 2023)\" isShortName></Paper> <Paper corpusId=\"261101164\" paperTitle=\"(Kuang et al., 2023)\" isShortName></Paper>. Crucially, the decoder also contains a cross-attention mechanism that connects the decoder states with the encoder outputs, allowing the model to align generated tokens with the input sequence <Paper corpusId=\"277128409\" paperTitle=\"(Ghosh et al., 2024)\" isShortName></Paper> <Paper corpusId=\"268157336\" paperTitle=\"(Patil et al., 2024)\" isShortName></Paper>.\n\nThis architecture is particularly well-suited for sequence-to-sequence tasks that involve transforming an input sequence into a different output sequence <Paper corpusId=\"267402678\" paperTitle=\"(Li, 2024)\" isShortName></Paper>. Examples include:\n\n- Machine translation <Paper corpusId=\"231645376\" paperTitle=\"(Kawara et al., 2021)\" isShortName></Paper> <Paper corpusId=\"234785837\" paperTitle=\"(Nguyen et al., 2021)\" isShortName></Paper>\n- Text summarization <Paper corpusId=\"267402678\" paperTitle=\"(Li, 2024)\" isShortName></Paper>\n- Dialogue generation <Paper corpusId=\"270702559\" paperTitle=\"(Yin et al., 2024)\" isShortName></Paper>\n\nPopular encoder-decoder models include T5 <Paper corpusId=\"204838007\" paperTitle=\"(Raffel et al., 2019)\" isShortName></Paper>, BART <Paper corpusId=\"263829839\" paperTitle=\"(Saha et al., 2023)\" isShortName></Paper>, and UniLM <Paper corpusId=\"147704286\" paperTitle=\"(Dong et al., 2019)\" isShortName></Paper>. These models are typically trained with objectives like span corruption or masked span prediction, where the model learns to recover masked portions of text <Paper corpusId=\"261494010\" paperTitle=\"(Wei et al., 2023)\" isShortName></Paper> <Paper corpusId=\"267211690\" paperTitle=\"(Uludougan et al., 2024)\" isShortName></Paper>.\n\nA key distinction between encoder-decoder models and decoder-only models is that the former process input and target sequences independently with different parameters, while the latter concatenate inputs and targets <Paper corpusId=\"252780443\" paperTitle=\"(Tay et al., 2022)\" isShortName></Paper>. This separation in encoder-decoder models creates a form of parameter sparsity, with different parameters handling different aspects of the task <Paper corpusId=\"252780443\" paperTitle=\"(Tay et al., 2022)\" isShortName></Paper>.\n\nHowever, encoder-decoder models generally require approximately twice as many parameters as decoder-only models with equivalent compute, which has contributed to the recent popularity of decoder-only architectures for large language models <Paper corpusId=\"252780443\" paperTitle=\"(Tay et al., 2022)\" isShortName></Paper>. Despite this, encoder-decoder models remain valuable for tasks requiring structured transformation between distinct input and output sequences <Paper corpusId=\"277626915\" paperTitle=\"(Rankovi'c et al., 2025)\" isShortName></Paper>.\n\nA specialized variant is the cascaded encoder model, which combines a causal encoder for streaming processing with a non-causal encoder for more comprehensive analysis, allowing the system to operate in both streaming and non-streaming modes <Paper corpusId=\"248157514\" paperTitle=\"(Ding et al., 2022)\" isShortName></Paper>.",
                "citations": [
                    {
                        "id": "(Vaswani et al., 2017)",
                        "snippets": [
                            "The dominant sequence transduction models are based on complex recurrent or convolutional neural networks in an encoder-decoder configuration. The best performing models also connect the encoder and decoder through an attention mechanism. We propose a new simple network architecture, the Transformer, based solely on attention mechanisms, dispensing with recurrence and convolutions entirely. Experiments on two machine translation tasks show these models to be superior in quality while being more parallelizable and requiring significantly less time to train. Our model achieves 28.4 BLEU on the WMT 2014 English-to-German translation task, improving over the existing best results, including ensembles by over 2 BLEU. On the WMT 2014 English-to-French translation task, our model establishes a new single-model state-of-the-art BLEU score of 41.8 after training for 3.5 days on eight GPUs, a small fraction of the training costs of the best models from the literature. We show that the Transformer generalizes well to other tasks by applying it successfully to English constituency parsing both with large and limited training data."
                        ],
                        "paper": {
                            "corpus_id": 13756489,
                            "title": "Attention is All you Need",
                            "authors": [
                                {
                                    "authorId": "40348417",
                                    "name": "Ashish Vaswani"
                                },
                                {
                                    "authorId": "1846258",
                                    "name": "Noam M. Shazeer"
                                },
                                {
                                    "authorId": "3877127",
                                    "name": "Niki Parmar"
                                },
                                {
                                    "authorId": "39328010",
                                    "name": "Jakob Uszkoreit"
                                },
                                {
                                    "authorId": "145024664",
                                    "name": "Llion Jones"
                                },
                                {
                                    "authorId": "19177000",
                                    "name": "Aidan N. Gomez"
                                },
                                {
                                    "authorId": "40527594",
                                    "name": "Lukasz Kaiser"
                                },
                                {
                                    "authorId": "3443442",
                                    "name": "I. Polosukhin"
                                }
                            ],
                            "year": 2017,
                            "venue": "Neural Information Processing Systems",
                            "n_citations": 132444
                        },
                        "score": 0
                    },
                    {
                        "id": "(Liu et al., 2024)",
                        "snippets": [
                            "LLMs with a Decoder-only architecture utilize the decoder component of the traditional Transformer architecture. Unlike the Encoder-Decoder architecture, which incorporates both an encoder and a decoder, the Decoder-only architecture is solely focused on the decoding process. In this configuration, the model sequentially generates tokens, attending to preceding tokens in the sequence. This architecture has been applied to various language generation tasks, showcasing its effectiveness in various tasks such as text generation without the need for an explicit encoding phase. The Decoder-only architecture can be further classified into two categories: the Causal Decoder architecture and the Prefix Decoder architecture. \n\nThe Causal Decoder Architecture: In the Causal Decoder architecture, each token in the model input sequence can only attend to past input tokens and itself during the decoding process. It achieves unidirectional attention to the input sequence by using a specific mask as shown in Figure 1. In fact, different architectures are mainly implemented by configuring different mask matrices. The figure illustrates a comparison of mask configurations between the Encoderdecoder and Decoder-only architectures (including Casual Decoder and Prefix Decoder).\n\nThe Prefix Decoder Architecture: The Prefix Decoder architecture combines the advantages of both the Encoderdecoder and Causal Decoder architectures. It leverages its unique mask configurations, as illustrated in Figure 1, enabling bidirectional attention for tokens in the prefix while maintaining unidirectional attention for generating subsequent tokens [54]. This design allows for the autoregressive generation of the output sequence with the flexibility to attend bi-directionally to the prefix tokens."
                        ],
                        "paper": {
                            "corpus_id": 266755678,
                            "title": "Understanding LLMs: A Comprehensive Overview from Training to Inference",
                            "authors": [
                                {
                                    "authorId": "2116426849",
                                    "name": "Yi-Hsueh Liu"
                                },
                                {
                                    "authorId": "2155082967",
                                    "name": "Haoyang He"
                                },
                                {
                                    "authorId": "2184719751",
                                    "name": "Tianle Han"
                                },
                                {
                                    "authorId": "2273584640",
                                    "name": "Xu Zhang"
                                },
                                {
                                    "authorId": "2210636248",
                                    "name": "Mengyuan Liu"
                                },
                                {
                                    "authorId": "2257433902",
                                    "name": "Jiaming Tian"
                                },
                                {
                                    "authorId": "2257095790",
                                    "name": "Yutong Zhang"
                                },
                                {
                                    "authorId": "2110238778",
                                    "name": "Jiaqi Wang"
                                },
                                {
                                    "authorId": "2277869261",
                                    "name": "Xiaohui Gao"
                                },
                                {
                                    "authorId": "2215167446",
                                    "name": "Tianyang Zhong"
                                },
                                {
                                    "authorId": "2221032216",
                                    "name": "Yi Pan"
                                },
                                {
                                    "authorId": "2211904452",
                                    "name": "Shaochen Xu"
                                },
                                {
                                    "authorId": "2263593041",
                                    "name": "Zihao Wu"
                                },
                                {
                                    "authorId": "2145977326",
                                    "name": "Zheng Liu"
                                },
                                {
                                    "authorId": "2257586495",
                                    "name": "Xin Zhang"
                                },
                                {
                                    "authorId": "2277750447",
                                    "name": "Shu Zhang"
                                },
                                {
                                    "authorId": "1742535",
                                    "name": "Xintao Hu"
                                },
                                {
                                    "authorId": "49104946",
                                    "name": "Tuo Zhang"
                                },
                                {
                                    "authorId": "2251076040",
                                    "name": "Ning Qiang"
                                },
                                {
                                    "authorId": "2254792886",
                                    "name": "Tianming Liu"
                                },
                                {
                                    "authorId": "2257302793",
                                    "name": "Bao Ge"
                                }
                            ],
                            "year": 2024,
                            "venue": "Neurocomputing",
                            "n_citations": 74
                        },
                        "score": 0.8115234375
                    },
                    {
                        "id": "(Yin et al., 2024)",
                        "snippets": [
                            "Typical architectures for LLMs can be categorized as encoder-decoder, causal decoder, and prefix decoder (Wang and Hesslow, 2022;Zhao et al., 2023).Among them, the causal decoder architecture is the most frequently used by various LLMs, such as OPT (Susan Zhang et al., 2023), LLAMA (Touvron et al., 2023a), BLOOM (Scao et al., 2023) due to its superior zero-shot and fewshot generalization capacity (Wang and Hesslow, 2022) and the effectiveness of scaling law (Brown et al., 2020;Kaplan et al., 2020).The causal decoder architecture employs a unidirectional attention mask, ensuring that each input token can only attend to preceding tokens and itself.Both input and output tokens are processed identically through the decoder (Zhao et al., 2023).\n\nThe prefix decoder architecture, also known as the non-causal decoder, modifies the masking mechanism used in causal decoders (Zhang et al., 2020).This adjustment allows for bidirectional attention over the prefix tokens while restricting attention to unidirectional generated tokens (Dong et al., 2019).Similar to the encoder-decoder architecture, prefix decoders can bidirectionally encode the prefix sequence and autoregressively predict the output tokens sequentially (Zhao et al., 2023).The same parameters are shared during both encoding and decoding processes.Representative LLMs based on prefix decoders include GLM-130B (Zeng et al., 2023) and U-PaLM (Tay et al., 2022).\n\nThe vanilla Transformer model is constructed on the encoder-decoder architecture (Vaswani et al., 2017), comprising two stacks of Transformer blocks designated as the encoder and decoder, respectively.The encoder utilizes stacked multi-head self-attention layers to encode the input sequence into its latent representations.Meanwhile, the decoder employs cross-attention mechanisms on these representations, and autoregressive generates the target sequence."
                        ],
                        "paper": {
                            "corpus_id": 270702559,
                            "title": "CrisisSense-LLM: Instruction Fine-Tuned Large Language Model for Multi-label Social Media Text Classification in Disaster Informatics",
                            "authors": [
                                {
                                    "authorId": "2265383225",
                                    "name": "Kai Yin"
                                },
                                {
                                    "authorId": "2308073678",
                                    "name": "Chengkai Liu"
                                },
                                {
                                    "authorId": "2258714985",
                                    "name": "Ali Mostafavi"
                                },
                                {
                                    "authorId": "2308068627",
                                    "name": "Xia Hu"
                                }
                            ],
                            "year": 2024,
                            "venue": "arXiv.org",
                            "n_citations": 12
                        },
                        "score": 0.74267578125
                    },
                    {
                        "id": "(Zheng et al., 2023)",
                        "snippets": [
                            "Encoder-decoder Architecture: The vanilla Transformer proposed in (Vaswani et al., 2017) is based on encoder-decoder architecture, which comprises separate encoder and decoder components. The encoder processes the input sequence and captures its latent representation, which is then used by the decoder to generate the output sequence autoregressively. This architecture is well-suited for tasks involving sequence-to-sequence mapping, such as machine translation, text summarization, and dialogue generation.\n\nCausal Decoder Architecture: The causal decoder architecture is commonly implemented as a stack of decoder layers. It utilizes a diagonal mask matrix, allowing each token to only have access to information from preceding tokens. This constraint ensures a unidirectional and autoregressive generation process."
                        ],
                        "paper": {
                            "corpus_id": 261064777,
                            "title": "Towards an Understanding of Large Language Models in Software Engineering Tasks",
                            "authors": [
                                {
                                    "authorId": "2148256392",
                                    "name": "Zibin Zheng"
                                },
                                {
                                    "authorId": "2115304",
                                    "name": "Kai-Chun Ning"
                                },
                                {
                                    "authorId": "2254800142",
                                    "name": "Jiachi Chen"
                                },
                                {
                                    "authorId": "2214155529",
                                    "name": "Yanlin Wang"
                                },
                                {
                                    "authorId": "2274095496",
                                    "name": "Wenqing Chen"
                                },
                                {
                                    "authorId": "2217902484",
                                    "name": "Lianghong Guo"
                                },
                                {
                                    "authorId": "2233023641",
                                    "name": "Weicheng Wang"
                                }
                            ],
                            "year": 2023,
                            "venue": "Empirical Software Engineering",
                            "n_citations": 76
                        },
                        "score": 0.64697265625
                    },
                    {
                        "id": "(Tay et al., 2020)",
                        "snippets": [
                            "In encoder mode, there is no restriction or constraint that the self-attention mechanism has to be causal, i.e., dependent solely on the present and past tokens. In the encoder-decoder setting, self-attention used in the decoder (i.e. across decoding positions) must be causal since each auto-regressive decoding step can only depend on previous tokens, whereas the selfattention used in the encoder need not. Fulfilling this requirement can prove challenging for many efficient self-attention designs."
                        ],
                        "paper": {
                            "corpus_id": 221702858,
                            "title": "Efficient Transformers: A Survey",
                            "authors": [
                                {
                                    "authorId": "144447820",
                                    "name": "Yi Tay"
                                },
                                {
                                    "authorId": "3226635",
                                    "name": "Mostafa Dehghani"
                                },
                                {
                                    "authorId": "11774695",
                                    "name": "Dara Bahri"
                                },
                                {
                                    "authorId": "1680617",
                                    "name": "Donald Metzler"
                                }
                            ],
                            "year": 2020,
                            "venue": "ACM Computing Surveys",
                            "n_citations": 1128
                        },
                        "score": 0.806640625
                    },
                    {
                        "id": "(Alomari et al., 2023)",
                        "snippets": [
                            "For the decoder part, we leverage GPT2, BERT, RoBERTa, and Ernie 2.0. GPT2 is trained on open-ended autoregressive generation with the objective of predicting the next word in a sequence of a few token starters (causal language modeling) utilizing only the Transformer's unidirectional ''left-to-right'' self-attention decoder. Theoretically, GPT2 is claimed to be the optimal design for use as a decoder. However, with a few adjustments, bi-directional encoderonly pretrained models such as BERT, RoBERTa, and Ernie 2.0 may simply be adapted as decoders. To compare the encoder and decoder architectures of the Transformer, encoder blocks consist solely of a bi-directional self-attention layer and two feed-forward layers. By contrast, decoder blocks have a unidirectional self-attention layer, a crossattention layer, and two feed-forward layers."
                        ],
                        "paper": {
                            "corpus_id": 263729712,
                            "title": "Warm-Starting for Improving the Novelty of Abstractive Summarization",
                            "authors": [
                                {
                                    "authorId": "2130042263",
                                    "name": "Ayham Alomari"
                                },
                                {
                                    "authorId": "1403466899",
                                    "name": "A. S. Al-Shamayleh"
                                },
                                {
                                    "authorId": "36826893",
                                    "name": "N. Idris"
                                },
                                {
                                    "authorId": "2049063550",
                                    "name": "Aznul Qalid Md Sabri"
                                },
                                {
                                    "authorId": "1770016",
                                    "name": "I. Alsmadi"
                                },
                                {
                                    "authorId": "2182454665",
                                    "name": "Danah Omary"
                                }
                            ],
                            "year": 2023,
                            "venue": "IEEE Access",
                            "n_citations": 1
                        },
                        "score": 0.6669921875
                    },
                    {
                        "id": "(Kuang et al., 2023)",
                        "snippets": [
                            "The structure of the decoder is similar to the text encoder, and the difference is that it replaces the bidirectional self-attention layers with causal self-attention layers."
                        ],
                        "paper": {
                            "corpus_id": 261101164,
                            "title": "DLIP: Distilling Language-Image Pre-training",
                            "authors": [
                                {
                                    "authorId": "1380215283",
                                    "name": "Huafeng Kuang"
                                },
                                {
                                    "authorId": "2118432533",
                                    "name": "Jie Wu"
                                },
                                {
                                    "authorId": "51056401",
                                    "name": "Xiawu Zheng"
                                },
                                {
                                    "authorId": "2150654378",
                                    "name": "Ming Li"
                                },
                                {
                                    "authorId": "2118724465",
                                    "name": "Xuefeng Xiao"
                                },
                                {
                                    "authorId": "39077217",
                                    "name": "Rui Wang"
                                },
                                {
                                    "authorId": "2026322565",
                                    "name": "Minghang Zheng"
                                },
                                {
                                    "authorId": "1572139630",
                                    "name": "Rongrong Ji"
                                }
                            ],
                            "year": 2023,
                            "venue": "arXiv.org",
                            "n_citations": 4
                        },
                        "score": 0.650390625
                    },
                    {
                        "id": "(Ghosh et al., 2024)",
                        "snippets": [
                            "The functioning of the Decoder has many parts in common with the Encoder, yet there are some major differences. In a nutshell, the Decoder processes the output component of the input-output pairs of the training data; encodes such information and produces its own embedding using a multi-head attention scheme similar to the Encoder, but: \n\n1. once the output has been encoded, it is combined with the  and  matrices coming from the Encoder. This is the step where Transformers learn the relation between the input of the training (e.g., the question) and the output (e.g., the answer to the question); considering the question-answer example, the Decoder performs cross-attention on the Encoder output (which represents the question) while processing the answer; 2. at the end of the process a linear layer has a number of output neurons equal to the size of the vocabulary; such network uses a softmax function to produce likelihood for each term in the vocabulary. Then, the term with the highest likelihood is the output of the Decoder i.e. the predicted next word; 3. The Decoder produces the output one word at a time; in the training phase, knowing the correct word, the error is computed and used to drive the backpropagation step and the correction of the weights to reduce the error",
                            ".For example, the weights obtained in pretraining for Encoder-only model like BERT, are based on randomly masked words; on the other hand, for Decoder-only model like GPT, causal masking is used, where only the future tokens are masked, and the model predicts the next token given past tokens."
                        ],
                        "paper": {
                            "corpus_id": 277128409,
                            "title": "A review on the applications of Transformer-based language models for nucleotide sequence analysis",
                            "authors": [
                                {
                                    "authorId": "2343636534",
                                    "name": "Nimisha Ghosh"
                                },
                                {
                                    "authorId": "2299143904",
                                    "name": "Daniele Santoni"
                                },
                                {
                                    "authorId": "2105376090",
                                    "name": "I. Saha"
                                },
                                {
                                    "authorId": "2261390733",
                                    "name": "Giovanni Felici"
                                }
                            ],
                            "year": 2024,
                            "venue": "Computational and Structural Biotechnology Journal",
                            "n_citations": 0
                        },
                        "score": 0.83349609375
                    },
                    {
                        "id": "(Patil et al., 2024)",
                        "snippets": [
                            "Encoder-decoder models adopt bidirectional attention for the encoder, unidirectional attention for the decoder, and a cross-attention mechanism between them. Cross-attention in the decoder has access only to the fully processed encoder output and is responsible for connecting input tokens to target tokens",
                            "Prefix language models are also decoder-only-based models but differ in the masking mechanism. Instead of a causal mask, a fully visible mask is used for the prefix part of the input sequence, and a causal mask is used for the target sequence",
                            "In a decoder-only model, the input and target are concatenated, and then a causal mask is used throughout. A decoder-only model with a prefix allows fully visible masking over part of the input token (prefix), followed by causal masking on the rest of the sequence. In general, autoencoding models learn bidirectional contextualized representation suited for NLU tasks, whereas autoregressive models learn to generate the next token and hence are suited for NLG tasks."
                        ],
                        "paper": {
                            "corpus_id": 268157336,
                            "title": "A Review of Current Trends, Techniques, and Challenges in Large Language Models (LLMs)",
                            "authors": [
                                {
                                    "authorId": "2289385425",
                                    "name": "Rajvardhan Patil"
                                },
                                {
                                    "authorId": "117730513",
                                    "name": "Venkat Gudivada"
                                }
                            ],
                            "year": 2024,
                            "venue": "Applied Sciences",
                            "n_citations": 80
                        },
                        "score": 0.77587890625
                    },
                    {
                        "id": "(Li, 2024)",
                        "snippets": [
                            "As shown in Figure 3, similar to most seq2seq models, the Transformer architecture consists of an encoder and a decoder. Overall, LLM (Language Model) models can be categorized into three major types: Encoder-decoder Architecture, Causal Decoder Architecture, and Prefix Decoder Architecture [11]. The Encoder-decoder Architecture uses the most basic structure and was initially introduced by the Seq2Seq model to address sequence-to-sequence tasks, such as machine translation. It consists of an encoder and a decoder. The encoder is responsible for transforming the input sequence into a fixeddimensional semantic representation, while the decoder uses this semantic representation to generate the output sequence. Within the encoder-decoder structure, self-attention mechanisms are commonly employed for sequence modeling, enabling the model to handle variable-length input and output sequences. This architecture has proven to be highly effective in various sequence-to-sequence tasks, such as text translation, text summarization, and dialogue generation. Prominent examples of large language models following this architecture include ELMo, BERT, RoBERTa, among others [12]. \n\nCurrently, the most widely used architecture is the Causal Decoder, which is primarily employed for handling autoregressive generation tasks, where each element of the output sequence depends on previously generated elements. The Causal Decoder Architecture is an improvement over the Encoderdecoder structure, as it introduces an autoregressive mechanism in the decoder. This means that when generating the current element, the model only uses the elements generated before it. This ensures that the model does not have access to future information during the generation process, thereby preserving causality. The GPT series (e.g., GPT-3) is a typical example of models that use the Causal Decoder Architecture. These models generate text by sequentially producing words one by one, avoiding information leakage and enabling the generation of coherent and plausible text. \n\nCompared to the Encoder-decoder models, Decoder-only models offer several advantages due to their simpler structure, faster training and inference speed, suitability for pure generation tasks, and advantages in decoder self-supervision."
                        ],
                        "paper": {
                            "corpus_id": 267402678,
                            "title": "The evolution, applications, and future prospects of large language models: An in-depth overview",
                            "authors": [
                                {
                                    "authorId": "2282449950",
                                    "name": "Jiayin Li"
                                }
                            ],
                            "year": 2024,
                            "venue": "Applied and Computational Engineering",
                            "n_citations": 2
                        },
                        "score": 0.78076171875
                    },
                    {
                        "id": "(Kawara et al., 2021)",
                        "snippets": [
                            "The difference in word orders between source and target languages is a serious hurdle for machine translation. Preordering methods, which reorder the words in a source sentence before translation to obtain a similar word ordering with a target language, significantly improve the quality in statistical machine translation. While the information on the preordering position improved the translation quality in recurrent neural network-based models, questions such as how to use preordering information and whether it is helpful for the Transformer model remain unaddressed. In this article, we successfully employed preordering techniques in the Transformer-based neural machine translation. Specifically, we proposed a novel <italic>preordering encoding</italic> that exploits the reordering information of the source and target sentences as positional encoding in the Transformer model. Experimental results on ASPEC Japanese\u2013English and WMT 2015 English\u2013German, English\u2013Czech, and English\u2013Russian translation tasks confirmed that the proposed method significantly improved the translation quality evaluated by the BLEU scores of the Transformer model by <inline-formula><tex-math notation=\"LaTeX\">${\\text{1.34}}$</tex-math></inline-formula> points in the Japanese\u2013to\u2013English task, <inline-formula><tex-math notation=\"LaTeX\">${\\text{2.19}}$</tex-math></inline-formula> points in the English\u2013to\u2013German task, <inline-formula><tex-math notation=\"LaTeX\">${\\text{0.15}}$</tex-math></inline-formula> points in the Czech\u2013to\u2013English task, and <inline-formula><tex-math notation=\"LaTeX\">${\\text {1.48}}$</tex-math></inline-formula> points in the English\u2013to\u2013Russian task."
                        ],
                        "paper": {
                            "corpus_id": 231645376,
                            "title": "Preordering Encoding on Transformer for Translation",
                            "authors": [
                                {
                                    "authorId": "46178138",
                                    "name": "Yuki Kawara"
                                },
                                {
                                    "authorId": "2427516",
                                    "name": "Chenhui Chu"
                                },
                                {
                                    "authorId": "3043844",
                                    "name": "Yuki Arase"
                                }
                            ],
                            "year": 2021,
                            "venue": "IEEE/ACM Transactions on Audio Speech and Language Processing",
                            "n_citations": 17
                        },
                        "score": 0
                    },
                    {
                        "id": "(Nguyen et al., 2021)",
                        "snippets": [
                            "Transformer is a neural machine translation model which revolutionizes machine translation. Compared with traditional statistical machine translation models and other neural machine translation models, the recently proposed transformer model radically and fundamentally changes machine translation with its self-attention and cross-attention mechanisms. These mechanisms effectively model token alignments between source and target sentences. It has been reported that the transformer model provides accurate posterior alignments. In this work, we empirically prove the reverse effect, showing that prior alignments help transformer models produce better translations. Experiment results on Vietnamese-English news translation task show not only the positive effect of manually annotated alignments on transformer models but also the surprising outperformance of statistically constructed alignments reinforced with the flexibility of token-type selection over manual alignments in improving transformer models. Statistically constructed word-to-lemma alignments are used to train a word-to-word transformer model. The novel hybrid transformer model improves the baseline transformer model and transformer model trained with manual alignments by 2.53 and 0.79 BLEU, respectively. In addition to BLEU score, we make limited human judgment on translation results. Strong correlation between human and machine judgment confirms our findings."
                        ],
                        "paper": {
                            "corpus_id": 234785837,
                            "title": "Improving Transformer-Based Neural Machine Translation with Prior Alignments",
                            "authors": [
                                {
                                    "authorId": "33775047",
                                    "name": "Thien Nguyen"
                                },
                                {
                                    "authorId": "2151126066",
                                    "name": "Lam Nguyen"
                                },
                                {
                                    "authorId": "153460631",
                                    "name": "Phuoc Tran"
                                },
                                {
                                    "authorId": "122619186",
                                    "name": "Huu Nguyen"
                                }
                            ],
                            "year": 2021,
                            "venue": "Complex",
                            "n_citations": 22
                        },
                        "score": 0
                    },
                    {
                        "id": "(Raffel et al., 2019)",
                        "snippets": [
                            "Transfer learning, where a model is first pre-trained on a data-rich task before being fine-tuned on a downstream task, has emerged as a powerful technique in natural language processing (NLP). The effectiveness of transfer learning has given rise to a diversity of approaches, methodology, and practice. In this paper, we explore the landscape of transfer learning techniques for NLP by introducing a unified framework that converts every language problem into a text-to-text format. Our systematic study compares pre-training objectives, architectures, unlabeled datasets, transfer approaches, and other factors on dozens of language understanding tasks. By combining the insights from our exploration with scale and our new \"Colossal Clean Crawled Corpus\", we achieve state-of-the-art results on many benchmarks covering summarization, question answering, text classification, and more. To facilitate future work on transfer learning for NLP, we release our dataset, pre-trained models, and code."
                        ],
                        "paper": {
                            "corpus_id": 204838007,
                            "title": "Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer",
                            "authors": [
                                {
                                    "authorId": "2402716",
                                    "name": "Colin Raffel"
                                },
                                {
                                    "authorId": "1846258",
                                    "name": "Noam M. Shazeer"
                                },
                                {
                                    "authorId": "145625142",
                                    "name": "Adam Roberts"
                                },
                                {
                                    "authorId": "3844009",
                                    "name": "Katherine Lee"
                                },
                                {
                                    "authorId": "46617804",
                                    "name": "Sharan Narang"
                                },
                                {
                                    "authorId": "1380243217",
                                    "name": "Michael Matena"
                                },
                                {
                                    "authorId": "2389316",
                                    "name": "Yanqi Zhou"
                                },
                                {
                                    "authorId": "2157338362",
                                    "name": "Wei Li"
                                },
                                {
                                    "authorId": "35025299",
                                    "name": "Peter J. Liu"
                                }
                            ],
                            "year": 2019,
                            "venue": "Journal of machine learning research",
                            "n_citations": 20336
                        },
                        "score": 0
                    },
                    {
                        "id": "(Saha et al., 2023)",
                        "snippets": [
                            "The encoder component transforms input tokens into vectors using embeddings and positional encodings, then applies multihead self-attention and feedforward networks. The decoder, starting similarly, incorporates masked self-attention and crossattention with the output of the encoder, ensuring alignment and preventing future word prediction. Sequence-to-sequence models often use this architecture, where the input and output sequences can be of different lengths. These models shine in tasks where there is a direct and complex transformation between inputs and outputs. Examples like machine translation and text summarization are prototypical, as they require the model to understand the input deeply and generate a coherent and contextually accurate output. BART [5], T5 (Raffel et al., 2019), and UL2 (Tay et al., 2022) are a few well-known encoder-decoder models to be named",
                            ".In a decoder-only model, a sequence is fed into the model, which then directly predicts the next token or word in the sequence. It operates autoregressively, using its generated tokens as context for subsequent predictions. It has two variants: causal decoder and prefix decoder. In a standard decoder (causal decoder), the unidirectional attention masking ensures that a token attends only to previous tokens and itself. Prefix decoders permit bidirectional attention over prefix tokens while maintaining unidirectional attention to generated tokens."
                        ],
                        "paper": {
                            "corpus_id": 263829839,
                            "title": "LLM for SoC Security: A Paradigm Shift",
                            "authors": [
                                {
                                    "authorId": "2256992493",
                                    "name": "Dipayan Saha"
                                },
                                {
                                    "authorId": "2114625129",
                                    "name": "Shams Tarek"
                                },
                                {
                                    "authorId": "2256991081",
                                    "name": "Katayoon Yahyaei"
                                },
                                {
                                    "authorId": "2231854143",
                                    "name": "Sujan Kumar Saha"
                                },
                                {
                                    "authorId": "2257235852",
                                    "name": "Jingbo Zhou"
                                },
                                {
                                    "authorId": "145954982",
                                    "name": "M. Tehranipoor"
                                },
                                {
                                    "authorId": "1997019",
                                    "name": "Farimah Farahmandi"
                                }
                            ],
                            "year": 2023,
                            "venue": "IEEE Access",
                            "n_citations": 54
                        },
                        "score": 0.79833984375
                    },
                    {
                        "id": "(Dong et al., 2019)",
                        "snippets": [
                            "This paper presents a new Unified pre-trained Language Model (UniLM) that can be fine-tuned for both natural language understanding and generation tasks. The model is pre-trained using three types of language modeling tasks: unidirectional, bidirectional, and sequence-to-sequence prediction. The unified modeling is achieved by employing a shared Transformer network and utilizing specific self-attention masks to control what context the prediction conditions on. UniLM compares favorably with BERT on the GLUE benchmark, and the SQuAD 2.0 and CoQA question answering tasks. Moreover, UniLM achieves new state-of-the-art results on five natural language generation datasets, including improving the CNN/DailyMail abstractive summarization ROUGE-L to 40.51 (2.04 absolute improvement), the Gigaword abstractive summarization ROUGE-L to 35.75 (0.86 absolute improvement), the CoQA generative question answering F1 score to 82.5 (37.1 absolute improvement), the SQuAD question generation BLEU-4 to 22.12 (3.75 absolute improvement), and the DSTC7 document-grounded dialog response generation NIST-4 to 2.67 (human performance is 2.65). The code and pre-trained models are available at this https URL."
                        ],
                        "paper": {
                            "corpus_id": 147704286,
                            "title": "Unified Language Model Pre-training for Natural Language Understanding and Generation",
                            "authors": [
                                {
                                    "authorId": "145307652",
                                    "name": "Li Dong"
                                },
                                {
                                    "authorId": "144610884",
                                    "name": "Nan Yang"
                                },
                                {
                                    "authorId": "51456429",
                                    "name": "Wenhui Wang"
                                },
                                {
                                    "authorId": "49807919",
                                    "name": "Furu Wei"
                                },
                                {
                                    "authorId": "46522098",
                                    "name": "Xiaodong Liu"
                                },
                                {
                                    "authorId": "72682749",
                                    "name": "Yu Wang"
                                },
                                {
                                    "authorId": "1800422",
                                    "name": "Jianfeng Gao"
                                },
                                {
                                    "authorId": "143849609",
                                    "name": "M. Zhou"
                                },
                                {
                                    "authorId": "145058181",
                                    "name": "H. Hon"
                                }
                            ],
                            "year": 2019,
                            "venue": "Neural Information Processing Systems",
                            "n_citations": 1560
                        },
                        "score": 0
                    },
                    {
                        "id": "(Wei et al., 2023)",
                        "snippets": [
                            "Encoder-only models use only the encoder component by training using Masked Language Modeling (MLM) [14] objective where a small percentage (e.g., 15%) of the tokens are masked on. The goal of MLM is to recover these masked tokens given the surrounding context. Encoder-only models such as CodeBERT [15] and GraphCodeBERT [22] are designed to provide a representation of the input code to be used for downstream tasks such as code classification [72]. Decoder-only models, on the other hand, aim to autoregressively generate tokens based on all previously generated tokens. CodeGEN [51,52], Codex [10] and PolyCoder [71] are examples of decoder-only LLMs where they can be used for code autocompletion tasks. Different from encoder-and decoder-only LLMs, encoder-decoder models (e.g., CodeT5 [62,63] and PLBART [3]) combine both encoder and decoder together and jointly train both components together. A commonly used pre-training objective for encoder-decoder models is Masked Span Prediction (MSP) where random spans (multiple consecutive tokens) are replaced with single masked tokens and the models learn to fill in the masked span with the correct sequence of tokens. Furthermore, decoder-only models like InCoder [17] can also perform infilling by training through causal language modeling [2] objective."
                        ],
                        "paper": {
                            "corpus_id": 261494010,
                            "title": "Copiloting the Copilots: Fusing Large Language Models with Completion Engines for Automated Program Repair",
                            "authors": [
                                {
                                    "authorId": "2237736409",
                                    "name": "Yuxiang Wei"
                                },
                                {
                                    "authorId": "145349987",
                                    "name": "Chun Xia"
                                },
                                {
                                    "authorId": "2237429253",
                                    "name": "Lingming Zhang"
                                }
                            ],
                            "year": 2023,
                            "venue": "ESEC/SIGSOFT FSE",
                            "n_citations": 100
                        },
                        "score": 0.75537109375
                    },
                    {
                        "id": "(Uludougan et al., 2024)",
                        "snippets": [
                            "Models that exclusively employ encoders, typically trained with denoising objectives, are geared toward understanding tasks, as exemplified in works such as (Devlin et al., 2019) and (208229926). Conversely, models that exclusively use decoders are designed for generation tasks, employing causal language modeling, as demonstrated in various studies (Radford et al., 2019)Brown et al., 2020;Touvron et al., 2023). The Text-to-Text Transformer (T5) (Raffel et al., 2019), on the other hand, employs an encoder-decoder architecture and undergoes pretraining with a denoising objective referred to as span corruption. UniLM (Dong et al., 2019) is also an encoder-decoder model, but pre-trained using unidirectional, bidirectional, and sequence-to-sequence language modeling. This can be seen as a combination of causal and denoising objectives."
                        ],
                        "paper": {
                            "corpus_id": 267211690,
                            "title": "TURNA: A Turkish Encoder-Decoder Language Model for Enhanced Understanding and Generation",
                            "authors": [
                                {
                                    "authorId": "2281033147",
                                    "name": "Gokcce Uludougan"
                                },
                                {
                                    "authorId": "2281033141",
                                    "name": "Zeynep Yirmibecsouglu Balal"
                                },
                                {
                                    "authorId": "2174736343",
                                    "name": "Furkan Akkurt"
                                },
                                {
                                    "authorId": "2281033264",
                                    "name": "Melikcsah Turker"
                                },
                                {
                                    "authorId": "9179697",
                                    "name": "Onur Gungor"
                                },
                                {
                                    "authorId": "66493576",
                                    "name": "S. Uskudarli"
                                }
                            ],
                            "year": 2024,
                            "venue": "Annual Meeting of the Association for Computational Linguistics",
                            "n_citations": 12
                        },
                        "score": 0.64208984375
                    },
                    {
                        "id": "(Tay et al., 2022)",
                        "snippets": [
                            "The line between decoder-only and encoder-decoder models is less clear. PrefixLM models are almost encoder-decoder models with shared parameters (but not quite). From an inductive bias point of view, there are multiple differences. Encoder-Decoder models process input and targets independently with a different set of parameters. This is a form of sparsity where different set of parameters are used for different tokens. Encoder-Decoder models also have a cross attention component that connects input tokens to target tokens. Meanwhile, decoder-only models process inputs and targets by concatenating them. Hence, the representations of inputs and targets are concurrently build layer by layer as the input/targets propagate up the network. Conversely, the decoder in Encoder-decoder models generally only looks at the fully processed encoder input. Overall, the inductive bias of PrefixLM decoder-only models and Encoder-Decoder models could be pretty similar modulo the subtle differences stated above. The distinct property is that Encoder-Decoder models are generally approximately 2x parameters of a decoder-only model when compute-matched."
                        ],
                        "paper": {
                            "corpus_id": 252780443,
                            "title": "UL2: Unifying Language Learning Paradigms",
                            "authors": [
                                {
                                    "authorId": "144447820",
                                    "name": "Yi Tay"
                                },
                                {
                                    "authorId": "3226635",
                                    "name": "Mostafa Dehghani"
                                },
                                {
                                    "authorId": "2057663102",
                                    "name": "Vinh Q. Tran"
                                },
                                {
                                    "authorId": "143936294",
                                    "name": "Xavier Garc\u00eda"
                                },
                                {
                                    "authorId": "119640649",
                                    "name": "Jason Wei"
                                },
                                {
                                    "authorId": "1524732527",
                                    "name": "Xuezhi Wang"
                                },
                                {
                                    "authorId": "3351938",
                                    "name": "Hyung Won Chung"
                                },
                                {
                                    "authorId": "2119725651",
                                    "name": "Dara Bahri"
                                },
                                {
                                    "authorId": "32303439",
                                    "name": "Tal Schuster"
                                },
                                {
                                    "authorId": "2115689465",
                                    "name": "H. Zheng"
                                },
                                {
                                    "authorId": "65855107",
                                    "name": "Denny Zhou"
                                },
                                {
                                    "authorId": "2815290",
                                    "name": "N. Houlsby"
                                },
                                {
                                    "authorId": "1680617",
                                    "name": "Donald Metzler"
                                }
                            ],
                            "year": 2022,
                            "venue": "International Conference on Learning Representations",
                            "n_citations": 313
                        },
                        "score": 0.84814453125
                    },
                    {
                        "id": "(Rankovi'c et al., 2025)",
                        "snippets": [
                            "LLMs can follow different architectural designs: encoder-only (e.g., BERT 2), decoder-only (e.g., Qwen 52), and encoder-decoder (e.g., T5 49). Encoder-based models process the full input bidirectionally and are suited for classification and regression. Decoder-only models generate text autoregressively with causal masking. Encoder-decoder models combine both components and are often used for sequence-to-sequence tasks."
                        ],
                        "paper": {
                            "corpus_id": 277626915,
                            "title": "GOLLuM: Gaussian Process Optimized LLMs - Reframing LLM Finetuning through Bayesian Optimization",
                            "authors": [
                                {
                                    "authorId": "2219925647",
                                    "name": "Bojana Rankovi'c"
                                },
                                {
                                    "authorId": "2239074343",
                                    "name": "Philippe Schwaller"
                                }
                            ],
                            "year": 2025,
                            "venue": "arXiv.org",
                            "n_citations": 1
                        },
                        "score": 0.703125
                    },
                    {
                        "id": "(Ding et al., 2022)",
                        "snippets": [
                            "The original cascaded encoder model (Narayanan et al., 2020) uses a shared RNN-T decoder. The decoder works with a causal encoder in the first pass to provide streaming recognition results, and works with an additional non-causal encoder that sits on top of the causal encoder to provide more accurate final results, leveraging audio right context extracted by the noncausal encoder."
                        ],
                        "paper": {
                            "corpus_id": 248157514,
                            "title": "A Unified Cascaded Encoder ASR Model for Dynamic Model Sizes",
                            "authors": [
                                {
                                    "authorId": "51267247",
                                    "name": "Shaojin Ding"
                                },
                                {
                                    "authorId": "2117928344",
                                    "name": "Weiran Wang"
                                },
                                {
                                    "authorId": "47783130",
                                    "name": "Ding Zhao"
                                },
                                {
                                    "authorId": "1784851",
                                    "name": "Tara N. Sainath"
                                },
                                {
                                    "authorId": "2145999837",
                                    "name": "Yanzhang He"
                                },
                                {
                                    "authorId": "2061545932",
                                    "name": "R. David"
                                },
                                {
                                    "authorId": "2004995",
                                    "name": "Rami Botros"
                                },
                                {
                                    "authorId": "2153688851",
                                    "name": "Xin Wang"
                                },
                                {
                                    "authorId": "1679451",
                                    "name": "R. Panigrahy"
                                },
                                {
                                    "authorId": "2055746849",
                                    "name": "Qiao Liang"
                                },
                                {
                                    "authorId": "2241835900",
                                    "name": "Dongseong Hwang"
                                },
                                {
                                    "authorId": "143685627",
                                    "name": "Ian McGraw"
                                },
                                {
                                    "authorId": "2557391",
                                    "name": "Rohit Prabhavalkar"
                                },
                                {
                                    "authorId": "2985957",
                                    "name": "Trevor Strohman"
                                }
                            ],
                            "year": 2022,
                            "venue": "Interspeech",
                            "n_citations": 17
                        },
                        "score": 0.66259765625
                    }
                ],
                "format": "synthesis",
                "table": null,
                "model": "claude-3-7-sonnet-20250219"
            },
            {
                "title": "Comparison of Model Architectures",
                "tldr": "The three main language model architectures (encoder-only, decoder-only, and encoder-decoder) have distinct strengths and limitations based on their attention mechanisms and training objectives. Encoder-only models excel at understanding tasks through bidirectional attention, decoder-only models dominate text generation through causal attention, and encoder-decoder models offer versatility for sequence-to-sequence tasks through their combined approach. (19 sources)",
                "text": "\nWhen comparing language model architectures, the key distinctions emerge from their attention mechanisms, training objectives, and the tasks they excel at. These differences directly impact their performance characteristics and application domains.\n\nEncoder-only models like BERT utilize bidirectional attention mechanisms that allow each token to attend to all positions in the input sequence <Paper corpusId=\"52967399\" paperTitle=\"(Devlin et al., 2019)\" isShortName></Paper> <Paper corpusId=\"257496129\" paperTitle=\"(He et al., 2023)\" isShortName></Paper>. This bidirectional processing enables them to build richer contextual representations than causal models, making them particularly effective for natural language understanding (NLU) tasks such as classification, feature extraction, and sentiment analysis <Paper corpusId=\"268247581\" paperTitle=\"(Pei et al., 2024)\" isShortName></Paper> <Paper corpusId=\"277857043\" paperTitle=\"(Beiranvand et al., 2025)\" isShortName></Paper>. Their typical training objective is masked language modeling (MLM), where they learn to recover masked tokens based on surrounding context <Paper corpusId=\"261494010\" paperTitle=\"(Wei et al., 2023)\" isShortName></Paper>.\n\nDecoder-only models, exemplified by GPT series, employ causal attention where tokens can only attend to themselves and preceding tokens <Paper corpusId=\"263729712\" paperTitle=\"(Alomari et al., 2023)\" isShortName></Paper> <Paper corpusId=\"257496129\" paperTitle=\"(He et al., 2023)\" isShortName></Paper>. These models are trained with next-token prediction objectives (causal language modeling) and excel at text generation tasks <Paper corpusId=\"277857043\" paperTitle=\"(Beiranvand et al., 2025)\" isShortName></Paper>. Despite their limitations in building rich bidirectional representations, causal decoder-only models have become the dominant architecture for state-of-the-art large language models, demonstrating superior zero-shot generalization immediately after pretraining <Paper corpusId=\"253420279\" paperTitle=\"(Scao et al., 2022)\" isShortName></Paper> <Paper corpusId=\"248118752\" paperTitle=\"(Wang et al., 2022)\" isShortName></Paper>.\n\nNon-causal decoder models (prefix decoders) represent a hybrid approach that combines bidirectional attention for input processing with unidirectional attention for generation <Paper corpusId=\"268157336\" paperTitle=\"(Patil et al., 2024)\" isShortName></Paper>. This architecture allows models to build richer representations of input text while maintaining generative capabilities <Paper corpusId=\"248118752\" paperTitle=\"(Wang et al., 2022)\" isShortName></Paper>. Recent research has demonstrated that causal decoder models can be effectively adapted to incorporate bidirectional attention, significantly improving their performance on encoder-centric tasks <Paper corpusId=\"276771845\" paperTitle=\"(Suganthan et al., 2025)\" isShortName></Paper>.\n\nEncoder-decoder models like T5 and BART combine both components, using bidirectional attention in the encoder and causal attention in the decoder, connected by a cross-attention mechanism <Paper corpusId=\"270832367\" paperTitle=\"(Busto-Castineira et al., 2024)\" isShortName></Paper> <Paper corpusId=\"263829839\" paperTitle=\"(Saha et al., 2023)\" isShortName></Paper>. This architecture is especially effective for sequence-to-sequence tasks such as translation and summarization <Paper corpusId=\"277626915\" paperTitle=\"(Rankovi'c et al., 2025)\" isShortName></Paper> <Paper corpusId=\"267211690\" paperTitle=\"(Uludougan et al., 2024)\" isShortName></Paper>. However, encoder-decoder models typically require approximately twice as many parameters as decoder-only models with equivalent compute, contributing to the recent popularity of decoder-only architectures <Paper corpusId=\"252780443\" paperTitle=\"(Tay et al., 2022)\" isShortName></Paper>.\n\nA crucial distinction between architectures lies in how they process information. In encoder-decoder models, the encoder processes input and the decoder processes targets independently with separate parameters <Paper corpusId=\"252780443\" paperTitle=\"(Tay et al., 2022)\" isShortName></Paper>. In contrast, decoder-only models concatenate inputs and targets, processing them together with shared parameters <Paper corpusId=\"248118752\" paperTitle=\"(Wang et al., 2022)\" isShortName></Paper> <Paper corpusId=\"268157336\" paperTitle=\"(Patil et al., 2024)\" isShortName></Paper>. This fundamental difference affects their parameter efficiency and inductive biases.\n\nAnother significant distinction is in the error accumulation patterns. Causal decoder models suffer from cumulative quantization errors in their attention mechanisms, particularly affecting tokens appearing later in the sequence <Paper corpusId=\"260886785\" paperTitle=\"(Kim et al., 2023)\" isShortName></Paper>. This is because each token's representation depends on a different number of attention probabilities affected by quantization errors, creating an uneven distribution of errors throughout the sequence.\n\nWhen choosing between architectures, the task requirements should guide the decision. For tasks requiring deep semantic understanding without generation, encoder-only models offer the strongest contextual representations through bidirectional attention <Paper corpusId=\"258461112\" paperTitle=\"(Jain et al., 2022)\" isShortName></Paper>. For pure generation tasks, causal decoder-only models provide the most natural fit with their autoregressive nature <Paper corpusId=\"263829839\" paperTitle=\"(Saha et al., 2023)\" isShortName></Paper>. For tasks requiring both understanding and transformation between sequences, encoder-decoder models offer the most balanced approach <Paper corpusId=\"270560675\" paperTitle=\"(Ma et al., 2024)\" isShortName></Paper>. The distinction between these architectures is not merely technical but reflects fundamental differences in how they process information and what capabilities they prioritize <Paper corpusId=\"221702858\" paperTitle=\"(Tay et al., 2020)\" isShortName></Paper>.",
                "citations": [
                    {
                        "id": "(Devlin et al., 2019)",
                        "snippets": [
                            "We introduce a new language representation model called BERT, which stands for Bidirectional Encoder Representations from Transformers. Unlike recent language representation models (Peters et al., 2018a; Radford et al., 2018), BERT is designed to pre-train deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers. As a result, the pre-trained BERT model can be fine-tuned with just one additional output layer to create state-of-the-art models for a wide range of tasks, such as question answering and language inference, without substantial task-specific architecture modifications. BERT is conceptually simple and empirically powerful. It obtains new state-of-the-art results on eleven natural language processing tasks, including pushing the GLUE score to 80.5 (7.7 point absolute improvement), MultiNLI accuracy to 86.7% (4.6% absolute improvement), SQuAD v1.1 question answering Test F1 to 93.2 (1.5 point absolute improvement) and SQuAD v2.0 Test F1 to 83.1 (5.1 point absolute improvement)."
                        ],
                        "paper": {
                            "corpus_id": 52967399,
                            "title": "BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding",
                            "authors": [
                                {
                                    "authorId": "39172707",
                                    "name": "Jacob Devlin"
                                },
                                {
                                    "authorId": "1744179",
                                    "name": "Ming-Wei Chang"
                                },
                                {
                                    "authorId": "2544107",
                                    "name": "Kenton Lee"
                                },
                                {
                                    "authorId": "3259253",
                                    "name": "Kristina Toutanova"
                                }
                            ],
                            "year": 2019,
                            "venue": "North American Chapter of the Association for Computational Linguistics",
                            "n_citations": 95215
                        },
                        "score": 0
                    },
                    {
                        "id": "(He et al., 2023)",
                        "snippets": [
                            "Transformer-based language models can be categorized into three types: encoder-only, decoder-only, and encoderdecoder models. \n\nEncoder-only models exclusively leverage the encoder stacks of the vanilla transformer [48] architecture. BERT [10] stands as a prominent encoder-only representation model, which learns a bidirectional contextual representation of text.\n\nIn contrast, Decoder-only models consist solely of the decoder components of the original transformer architecture. A notable instance of such models is the GPT [35], GPT operates under a causal language modeling (CLM) framework during its training phase. CLM is a strategy where the model predicts the next token in a sequence while only considering preceding tokens. In other words, this design restricts the model from accessing future tokens in the sequence. \n\nBridging the above approaches, textitEncoder-decoder models integrate both the encoder and decoder components of the transformer architecture. Popular encoder-decoder models involve T5 [37] and BART [23]."
                        ],
                        "paper": {
                            "corpus_id": 257496129,
                            "title": "Representation Learning for Stack Overflow Posts: How Far Are We?",
                            "authors": [
                                {
                                    "authorId": "2158107537",
                                    "name": "Junda He"
                                },
                                {
                                    "authorId": "2211431558",
                                    "name": "Zhou Xin"
                                },
                                {
                                    "authorId": "2203459",
                                    "name": "Bowen Xu"
                                },
                                {
                                    "authorId": "2146322053",
                                    "name": "Ting Zhang"
                                },
                                {
                                    "authorId": "35276441",
                                    "name": "Kisub Kim"
                                },
                                {
                                    "authorId": "2139059234",
                                    "name": "Zhou Yang"
                                },
                                {
                                    "authorId": "2121315",
                                    "name": "Ferdian Thung"
                                },
                                {
                                    "authorId": "9223952",
                                    "name": "I. Irsan"
                                },
                                {
                                    "authorId": "2150912791",
                                    "name": "David Lo"
                                }
                            ],
                            "year": 2023,
                            "venue": "ACM Transactions on Software Engineering and Methodology",
                            "n_citations": 20
                        },
                        "score": 0.66015625
                    },
                    {
                        "id": "(Pei et al., 2024)",
                        "snippets": [
                            "Encoder-only models (Devlin et al., 2019) specialize in processing input sequences of biomolecules and text through bi-directional self-attention, making them highly effective for tasks that require an in-depth understanding of the input, such as sentiment analysis and feature extraction in NLP. Thereby in biomolecule domain, encoder-only models establish a bi-directional association between biotokens and text tokens for predictive tasks (Zeng et al., 2022)",
                            "In contrast, decoder-only models [18], [138] employ causal attention to focus on the sequence of previous tokens. This architecture is typically utilized in generative tasks, such as generating text descriptions that match the given molecule or for the reverse task (Liu et al., 2023). Thanks to the autoregressive generation property, decoder-only models are well suitable for instruction following and assistant/agent objectives."
                        ],
                        "paper": {
                            "corpus_id": 268247581,
                            "title": "Leveraging Biomolecule and Natural Language through Multi-Modal Learning: A Survey",
                            "authors": [
                                {
                                    "authorId": "2171652249",
                                    "name": "Qizhi Pei"
                                },
                                {
                                    "authorId": "47767791",
                                    "name": "Lijun Wu"
                                },
                                {
                                    "authorId": "1944690382",
                                    "name": "Kaiyuan Gao"
                                },
                                {
                                    "authorId": "151068900",
                                    "name": "Jinhua Zhu"
                                },
                                {
                                    "authorId": "2290062348",
                                    "name": "Yue Wang"
                                },
                                {
                                    "authorId": "2290024261",
                                    "name": "Zun Wang"
                                },
                                {
                                    "authorId": "2267250090",
                                    "name": "Tao Qin"
                                },
                                {
                                    "authorId": "2257028545",
                                    "name": "Rui Yan"
                                }
                            ],
                            "year": 2024,
                            "venue": "arXiv.org",
                            "n_citations": 18
                        },
                        "score": 0.64990234375
                    },
                    {
                        "id": "(Beiranvand et al., 2025)",
                        "snippets": [
                            "Encoder-only models, such as BERT [18], are designed to generate holistic representations of input sequences. These models take an entire sequence as input and process it bidirectionally-each token has access to the full left and right context during training. This characteristic allows the model to deeply capture semantic dependencies across the input.\n\nIn contrast, decoder-only models, such as GPT [19], operate unidirectionally. They are trained in an autoregressive fashion, where each token is generated based only on preceding tokens. The model has no access to future inputs during training or inference, enforcing a strict left-to-right dependency. This makes them particularly well-suited for generative tasks such as open-ended text generation or dialogue modelling.\n\nThe pretraining objective for decoder-only models is Causal Language Modelling (CLM), where the model learns to predict the next token in a sequence, given all previous ones. The associated loss is defined as: \n\nHere, each token x t is conditioned solely on the sequence x <t , making the model capable of generating coherent text step by step.\n\nThe distinction between encoder-only and decoder-only models is foundational: the former are optimized for understanding input context and are widely used in classification and embedding tasks, while the latter are tailored for sequential prediction and text generation."
                        ],
                        "paper": {
                            "corpus_id": 277857043,
                            "title": "Integrating Structural and Semantic Signals in Text-Attributed Graphs with BiGTex",
                            "authors": [
                                {
                                    "authorId": "66841167",
                                    "name": "Azadeh Beiranvand"
                                },
                                {
                                    "authorId": "35409259",
                                    "name": "S. M. Vahidipour"
                                }
                            ],
                            "year": 2025,
                            "venue": "arXiv.org",
                            "n_citations": 0
                        },
                        "score": 0.810546875
                    },
                    {
                        "id": "(Wei et al., 2023)",
                        "snippets": [
                            "Encoder-only models use only the encoder component by training using Masked Language Modeling (MLM) [14] objective where a small percentage (e.g., 15%) of the tokens are masked on. The goal of MLM is to recover these masked tokens given the surrounding context. Encoder-only models such as CodeBERT [15] and GraphCodeBERT [22] are designed to provide a representation of the input code to be used for downstream tasks such as code classification [72]. Decoder-only models, on the other hand, aim to autoregressively generate tokens based on all previously generated tokens. CodeGEN [51,52], Codex [10] and PolyCoder [71] are examples of decoder-only LLMs where they can be used for code autocompletion tasks. Different from encoder-and decoder-only LLMs, encoder-decoder models (e.g., CodeT5 [62,63] and PLBART [3]) combine both encoder and decoder together and jointly train both components together. A commonly used pre-training objective for encoder-decoder models is Masked Span Prediction (MSP) where random spans (multiple consecutive tokens) are replaced with single masked tokens and the models learn to fill in the masked span with the correct sequence of tokens. Furthermore, decoder-only models like InCoder [17] can also perform infilling by training through causal language modeling [2] objective."
                        ],
                        "paper": {
                            "corpus_id": 261494010,
                            "title": "Copiloting the Copilots: Fusing Large Language Models with Completion Engines for Automated Program Repair",
                            "authors": [
                                {
                                    "authorId": "2237736409",
                                    "name": "Yuxiang Wei"
                                },
                                {
                                    "authorId": "145349987",
                                    "name": "Chun Xia"
                                },
                                {
                                    "authorId": "2237429253",
                                    "name": "Lingming Zhang"
                                }
                            ],
                            "year": 2023,
                            "venue": "ESEC/SIGSOFT FSE",
                            "n_citations": 100
                        },
                        "score": 0.75537109375
                    },
                    {
                        "id": "(Alomari et al., 2023)",
                        "snippets": [
                            "For the decoder part, we leverage GPT2, BERT, RoBERTa, and Ernie 2.0. GPT2 is trained on open-ended autoregressive generation with the objective of predicting the next word in a sequence of a few token starters (causal language modeling) utilizing only the Transformer's unidirectional ''left-to-right'' self-attention decoder. Theoretically, GPT2 is claimed to be the optimal design for use as a decoder. However, with a few adjustments, bi-directional encoderonly pretrained models such as BERT, RoBERTa, and Ernie 2.0 may simply be adapted as decoders. To compare the encoder and decoder architectures of the Transformer, encoder blocks consist solely of a bi-directional self-attention layer and two feed-forward layers. By contrast, decoder blocks have a unidirectional self-attention layer, a crossattention layer, and two feed-forward layers."
                        ],
                        "paper": {
                            "corpus_id": 263729712,
                            "title": "Warm-Starting for Improving the Novelty of Abstractive Summarization",
                            "authors": [
                                {
                                    "authorId": "2130042263",
                                    "name": "Ayham Alomari"
                                },
                                {
                                    "authorId": "1403466899",
                                    "name": "A. S. Al-Shamayleh"
                                },
                                {
                                    "authorId": "36826893",
                                    "name": "N. Idris"
                                },
                                {
                                    "authorId": "2049063550",
                                    "name": "Aznul Qalid Md Sabri"
                                },
                                {
                                    "authorId": "1770016",
                                    "name": "I. Alsmadi"
                                },
                                {
                                    "authorId": "2182454665",
                                    "name": "Danah Omary"
                                }
                            ],
                            "year": 2023,
                            "venue": "IEEE Access",
                            "n_citations": 1
                        },
                        "score": 0.6669921875
                    },
                    {
                        "id": "(Scao et al., 2022)",
                        "snippets": [
                            "Notably, while the original Transformer is based on an encoder-decoder architecture, many popular models have opted for encoder-only (e.g. BERT, (Devlin et al., 2019)) or decoder-only (e.g. GPT, (Radford et al., 2018)) approaches. Currently, all state-of-the-art language models over 100 billion parameters are causal decoder-only models Rae et al., 2021;Chowdhery et al., 2022). This is in opposition to the findings of (Raffel et al., 2019), in which encoderdecoder models significantly outperform decoder-only models for transfer learning.\n\nPrior to our work, the literature was lacking a systematic evaluation of the zero-shot generalization capabilities of different architectures and pretraining objectives. We explored this question in (Wang et al., 2022) where we evaluated encoder-decoder and decoder-only architectures and their interactions with causal, prefix, and masked language modeling pretraining objectives. Our results show that immediately after pretraining, causal decoderonly models performed best -validating the choice of state-of-the-art LLMs."
                        ],
                        "paper": {
                            "corpus_id": 253420279,
                            "title": "BLOOM: A 176B-Parameter Open-Access Multilingual Language Model",
                            "authors": [
                                {
                                    "authorId": "1379806208",
                                    "name": "Teven Le Scao"
                                },
                                {
                                    "authorId": "144270981",
                                    "name": "Angela Fan"
                                },
                                {
                                    "authorId": "2003696840",
                                    "name": "Christopher Akiki"
                                },
                                {
                                    "authorId": "2949185",
                                    "name": "Ellie Pavlick"
                                },
                                {
                                    "authorId": "2066663381",
                                    "name": "Suzana Ili'c"
                                },
                                {
                                    "authorId": "80424302",
                                    "name": "Daniel Hesslow"
                                },
                                {
                                    "authorId": "2190282134",
                                    "name": "Roman Castagn'e"
                                },
                                {
                                    "authorId": "2993731",
                                    "name": "A. Luccioni"
                                },
                                {
                                    "authorId": "1846431",
                                    "name": "Fran\u00e7ois Yvon"
                                },
                                {
                                    "authorId": "2907260",
                                    "name": "Matthias Gall\u00e9"
                                },
                                {
                                    "authorId": "50195579",
                                    "name": "J. Tow"
                                },
                                {
                                    "authorId": "2531268",
                                    "name": "Alexander M. Rush"
                                },
                                {
                                    "authorId": "103476203",
                                    "name": "Stella Biderman"
                                },
                                {
                                    "authorId": "1991019030",
                                    "name": "Albert Webson"
                                },
                                {
                                    "authorId": "1451644426",
                                    "name": "Pawan Sasanka Ammanamanchi"
                                },
                                {
                                    "authorId": "2135734748",
                                    "name": "Thomas Wang"
                                },
                                {
                                    "authorId": "68990982",
                                    "name": "Beno\u00eet Sagot"
                                },
                                {
                                    "authorId": "2037383772",
                                    "name": "Niklas Muennighoff"
                                },
                                {
                                    "authorId": "46219923",
                                    "name": "Albert Villanova del Moral"
                                },
                                {
                                    "authorId": "2537545",
                                    "name": "Olatunji Ruwase"
                                },
                                {
                                    "authorId": "48983885",
                                    "name": "Rachel Bawden"
                                },
                                {
                                    "authorId": "32136590",
                                    "name": "Stas Bekman"
                                },
                                {
                                    "authorId": "1584940075",
                                    "name": "Angelina McMillan-Major"
                                },
                                {
                                    "authorId": "46181066",
                                    "name": "Iz Beltagy"
                                },
                                {
                                    "authorId": "2168170616",
                                    "name": "Huu Nguyen"
                                },
                                {
                                    "authorId": "2113836860",
                                    "name": "Lucile Saulnier"
                                },
                                {
                                    "authorId": "145814654",
                                    "name": "Samson Tan"
                                },
                                {
                                    "authorId": "147846651",
                                    "name": "Pedro Ortiz Suarez"
                                },
                                {
                                    "authorId": "2285868436",
                                    "name": "Victor Sanh"
                                },
                                {
                                    "authorId": "2172404846",
                                    "name": "Hugo Laurenccon"
                                },
                                {
                                    "authorId": "2262249",
                                    "name": "Yacine Jernite"
                                },
                                {
                                    "authorId": "143945447",
                                    "name": "Julien Launay"
                                },
                                {
                                    "authorId": "49501003",
                                    "name": "Margaret Mitchell"
                                },
                                {
                                    "authorId": "2402716",
                                    "name": "Colin Raffel"
                                },
                                {
                                    "authorId": "2273789852",
                                    "name": "Aaron Gokaslan"
                                },
                                {
                                    "authorId": "2183598223",
                                    "name": "Adi Simhi"
                                },
                                {
                                    "authorId": "2078619062",
                                    "name": "Aitor Soroa Etxabe"
                                },
                                {
                                    "authorId": "8129718",
                                    "name": "Alham Fikri Aji"
                                },
                                {
                                    "authorId": "73769093",
                                    "name": "Amit Alfassy"
                                },
                                {
                                    "authorId": "145046059",
                                    "name": "Anna Rogers"
                                },
                                {
                                    "authorId": "2190281124",
                                    "name": "Ariel Kreisberg Nitzav"
                                },
                                {
                                    "authorId": "66247317",
                                    "name": "Canwen Xu"
                                },
                                {
                                    "authorId": "35966970",
                                    "name": "Chenghao Mou"
                                },
                                {
                                    "authorId": "1591176064",
                                    "name": "Chris C. Emezue"
                                },
                                {
                                    "authorId": "2261291789",
                                    "name": "Christopher Klamm"
                                },
                                {
                                    "authorId": "89269402",
                                    "name": "Colin Leong"
                                },
                                {
                                    "authorId": "71075073",
                                    "name": "Daniel Alexander van Strien"
                                },
                                {
                                    "authorId": "2518906",
                                    "name": "David Ifeoluwa Adelani"
                                },
                                {
                                    "authorId": "9215251",
                                    "name": "Dragomir R. Radev"
                                },
                                {
                                    "authorId": "79512668",
                                    "name": "E. G. Ponferrada"
                                },
                                {
                                    "authorId": "2190281122",
                                    "name": "Efrat Levkovizh"
                                },
                                {
                                    "authorId": "2047591327",
                                    "name": "Ethan Kim"
                                },
                                {
                                    "authorId": "2088048322",
                                    "name": "Eyal Natan"
                                },
                                {
                                    "authorId": "2067891070",
                                    "name": "F. Toni"
                                },
                                {
                                    "authorId": "13656138",
                                    "name": "G\u00e9rard Dupont"
                                },
                                {
                                    "authorId": "2067996",
                                    "name": "Germ\u00e1n Kruszewski"
                                },
                                {
                                    "authorId": "2158858559",
                                    "name": "Giada Pistilli"
                                },
                                {
                                    "authorId": "2218938",
                                    "name": "Hady ElSahar"
                                },
                                {
                                    "authorId": "90563027",
                                    "name": "Hamza Benyamina"
                                },
                                {
                                    "authorId": "2057078797",
                                    "name": "H. Tran"
                                },
                                {
                                    "authorId": "47948569",
                                    "name": "Ian Yu"
                                },
                                {
                                    "authorId": "1429833598",
                                    "name": "Idris Abdulmumin"
                                },
                                {
                                    "authorId": "2060080508",
                                    "name": "Isaac Johnson"
                                },
                                {
                                    "authorId": "1404791152",
                                    "name": "Itziar Gonzalez-Dios"
                                },
                                {
                                    "authorId": "144979591",
                                    "name": "Javier de la Rosa"
                                },
                                {
                                    "authorId": "2164872258",
                                    "name": "Jenny Chim"
                                },
                                {
                                    "authorId": "34176020",
                                    "name": "Jesse Dodge"
                                },
                                {
                                    "authorId": "144549416",
                                    "name": "Jian Zhu"
                                },
                                {
                                    "authorId": "2116123009",
                                    "name": "Jonathan Chang"
                                },
                                {
                                    "authorId": "2146695800",
                                    "name": "Jorg Frohberg"
                                },
                                {
                                    "authorId": "2094755167",
                                    "name": "Josephine Tobing"
                                },
                                {
                                    "authorId": "143779690",
                                    "name": "J. Bhattacharjee"
                                },
                                {
                                    "authorId": "90615055",
                                    "name": "Khalid Almubarak"
                                },
                                {
                                    "authorId": "2157630500",
                                    "name": "Kimbo Chen"
                                },
                                {
                                    "authorId": "46258841",
                                    "name": "Kyle Lo"
                                },
                                {
                                    "authorId": "51128119",
                                    "name": "L. V. Werra"
                                },
                                {
                                    "authorId": "20308468",
                                    "name": "Leon Weber"
                                },
                                {
                                    "authorId": null,
                                    "name": "Long Phan"
                                },
                                {
                                    "authorId": "2190281230",
                                    "name": "Loubna Ben Allal"
                                },
                                {
                                    "authorId": "77970446",
                                    "name": "Ludovic Tanguy"
                                },
                                {
                                    "authorId": "1879591269",
                                    "name": "Manan Dey"
                                },
                                {
                                    "authorId": "115568186",
                                    "name": "M. Mu\u00f1oz"
                                },
                                {
                                    "authorId": "153528116",
                                    "name": "Maraim Masoud"
                                },
                                {
                                    "authorId": "2176184513",
                                    "name": "Mar\u00eda Grandury"
                                },
                                {
                                    "authorId": "2125821515",
                                    "name": "Mario vSavsko"
                                },
                                {
                                    "authorId": "2112504552",
                                    "name": "Max Huang"
                                },
                                {
                                    "authorId": "3443469",
                                    "name": "Maximin Coavoux"
                                },
                                {
                                    "authorId": "145431050",
                                    "name": "Mayank Singh"
                                },
                                {
                                    "authorId": "5745221",
                                    "name": "Mike Tian-Jian Jiang"
                                },
                                {
                                    "authorId": "1484109150",
                                    "name": "Minh Chien Vu"
                                },
                                {
                                    "authorId": "2097304324",
                                    "name": "M. A. Jauhar"
                                },
                                {
                                    "authorId": "2721586",
                                    "name": "Mustafa Ghaleb"
                                },
                                {
                                    "authorId": "34202134",
                                    "name": "Nishant Subramani"
                                },
                                {
                                    "authorId": "9529535",
                                    "name": "Nora Kassner"
                                },
                                {
                                    "authorId": "37441312",
                                    "name": "Nurulaqilla Khamis"
                                },
                                {
                                    "authorId": "2089233725",
                                    "name": "Olivier Nguyen"
                                },
                                {
                                    "authorId": "2190280842",
                                    "name": "Omar Espejel"
                                },
                                {
                                    "authorId": "51436367",
                                    "name": "Ona de Gibert"
                                },
                                {
                                    "authorId": "2176184659",
                                    "name": "Paulo Villegas"
                                },
                                {
                                    "authorId": "2071773966",
                                    "name": "Peter Henderson"
                                },
                                {
                                    "authorId": "46985469",
                                    "name": "Pierre Colombo"
                                },
                                {
                                    "authorId": "2190281321",
                                    "name": "Priscilla Amuok"
                                },
                                {
                                    "authorId": "2113836945",
                                    "name": "Quentin Lhoest"
                                },
                                {
                                    "authorId": "80858030",
                                    "name": "Rheza Harliman"
                                },
                                {
                                    "authorId": "150272855",
                                    "name": "Rishi Bommasani"
                                },
                                {
                                    "authorId": "116000979",
                                    "name": "R. L'opez"
                                },
                                {
                                    "authorId": null,
                                    "name": "Rui Ribeiro"
                                },
                                {
                                    "authorId": "1486204986",
                                    "name": "Salomey Osei"
                                },
                                {
                                    "authorId": "1708916",
                                    "name": "S. Pyysalo"
                                },
                                {
                                    "authorId": "47351277",
                                    "name": "Sebastian Nagel"
                                },
                                {
                                    "authorId": "2795685",
                                    "name": "Shamik Bose"
                                },
                                {
                                    "authorId": "7744881",
                                    "name": "Shamsuddeen Hassan Muhammad"
                                },
                                {
                                    "authorId": "1409842673",
                                    "name": "S. Sharma"
                                },
                                {
                                    "authorId": "29909347",
                                    "name": "S. Longpre"
                                },
                                {
                                    "authorId": "2099315138",
                                    "name": "Somaieh Nikpoor"
                                },
                                {
                                    "authorId": "82674724",
                                    "name": "S. Silberberg"
                                },
                                {
                                    "authorId": "2053516473",
                                    "name": "S. Pai"
                                },
                                {
                                    "authorId": "2074482488",
                                    "name": "S. Zink"
                                },
                                {
                                    "authorId": "46308692",
                                    "name": "Tiago Timponi Torrent"
                                },
                                {
                                    "authorId": "32246932",
                                    "name": "Timo Schick"
                                },
                                {
                                    "authorId": "1500242049",
                                    "name": "Tristan Thrush"
                                },
                                {
                                    "authorId": "3382327",
                                    "name": "V. Danchev"
                                },
                                {
                                    "authorId": "2841761",
                                    "name": "Vassilina Nikoulina"
                                },
                                {
                                    "authorId": "1796619",
                                    "name": "Veronika Laippala"
                                },
                                {
                                    "authorId": "2190280574",
                                    "name": "Violette Lepercq"
                                },
                                {
                                    "authorId": "2059767242",
                                    "name": "V. Prabhu"
                                },
                                {
                                    "authorId": "25098419",
                                    "name": "Zaid Alyafeai"
                                },
                                {
                                    "authorId": "2138053020",
                                    "name": "Zeerak Talat"
                                },
                                {
                                    "authorId": "2048082186",
                                    "name": "Arun Raja"
                                },
                                {
                                    "authorId": "2266692",
                                    "name": "Benjamin Heinzerling"
                                },
                                {
                                    "authorId": "152358188",
                                    "name": "Chenglei Si"
                                },
                                {
                                    "authorId": "3448427",
                                    "name": "Elizabeth Salesky"
                                },
                                {
                                    "authorId": "27689253",
                                    "name": "Sabrina J. Mielke"
                                },
                                {
                                    "authorId": "2183377987",
                                    "name": "Wilson Y. Lee"
                                },
                                {
                                    "authorId": "2051500420",
                                    "name": "Abheesht Sharma"
                                },
                                {
                                    "authorId": "2065039862",
                                    "name": "Andrea Santilli"
                                },
                                {
                                    "authorId": "2129106958",
                                    "name": "Antoine Chaffin"
                                },
                                {
                                    "authorId": "114762823",
                                    "name": "Arnaud Stiegler"
                                },
                                {
                                    "authorId": "2852125",
                                    "name": "Debajyoti Datta"
                                },
                                {
                                    "authorId": "50812522",
                                    "name": "Eliza Szczechla"
                                },
                                {
                                    "authorId": "1509809381",
                                    "name": "Gunjan Chhablani"
                                },
                                {
                                    "authorId": "144407394",
                                    "name": "Han Wang"
                                },
                                {
                                    "authorId": "144834468",
                                    "name": "Harshit Pandey"
                                },
                                {
                                    "authorId": "2879705",
                                    "name": "Hendrik Strobelt"
                                },
                                {
                                    "authorId": "31592365",
                                    "name": "Jason Alan Fries"
                                },
                                {
                                    "authorId": "120419790",
                                    "name": "Jos Rozen"
                                },
                                {
                                    "authorId": "2027599537",
                                    "name": "Leo Gao"
                                },
                                {
                                    "authorId": "35566806",
                                    "name": "Lintang Sutawika"
                                },
                                {
                                    "authorId": "31773000",
                                    "name": "M Saiful Bari"
                                },
                                {
                                    "authorId": "1752627730",
                                    "name": "Maged S. Al-Shaibani"
                                },
                                {
                                    "authorId": "35904689",
                                    "name": "Matteo Manica"
                                },
                                {
                                    "authorId": "22209084",
                                    "name": "Nihal V. Nayak"
                                },
                                {
                                    "authorId": "2131107966",
                                    "name": "Ryan Teehan"
                                },
                                {
                                    "authorId": "7641268",
                                    "name": "Samuel Albanie"
                                },
                                {
                                    "authorId": "2191455",
                                    "name": "Sheng Shen"
                                },
                                {
                                    "authorId": "2152318619",
                                    "name": "Srulik Ben-David"
                                },
                                {
                                    "authorId": "2870504",
                                    "name": "Stephen H. Bach"
                                },
                                {
                                    "authorId": "2111181991",
                                    "name": "Taewoon Kim"
                                },
                                {
                                    "authorId": "94251255",
                                    "name": "T. Bers"
                                },
                                {
                                    "authorId": "79215748",
                                    "name": "Thibault F\u00e9vry"
                                },
                                {
                                    "authorId": "10729963",
                                    "name": "Trishala Neeraj"
                                },
                                {
                                    "authorId": "70296695",
                                    "name": "Urmish Thakker"
                                },
                                {
                                    "authorId": "24025563",
                                    "name": "Vikas Raunak"
                                },
                                {
                                    "authorId": "2118488348",
                                    "name": "Xiang Tang"
                                },
                                {
                                    "authorId": "1725420331",
                                    "name": "Zheng-Xin Yong"
                                },
                                {
                                    "authorId": "48064856",
                                    "name": "Zhiqing Sun"
                                },
                                {
                                    "authorId": "1720739223",
                                    "name": "Shaked Brody"
                                },
                                {
                                    "authorId": "2101395835",
                                    "name": "Y. Uri"
                                },
                                {
                                    "authorId": "2190280874",
                                    "name": "Hadar Tojarieh"
                                },
                                {
                                    "authorId": "145625142",
                                    "name": "Adam Roberts"
                                },
                                {
                                    "authorId": "3351938",
                                    "name": "Hyung Won Chung"
                                },
                                {
                                    "authorId": "2112211652",
                                    "name": "Jaesung Tae"
                                },
                                {
                                    "authorId": "80842917",
                                    "name": "Jason Phang"
                                },
                                {
                                    "authorId": "40170001",
                                    "name": "Ofir Press"
                                },
                                {
                                    "authorId": "2609325",
                                    "name": "Conglong Li"
                                },
                                {
                                    "authorId": "22252150",
                                    "name": "D. Narayanan"
                                },
                                {
                                    "authorId": "2190280830",
                                    "name": "Hatim Bourfoune"
                                },
                                {
                                    "authorId": "48991386",
                                    "name": "J. Casper"
                                },
                                {
                                    "authorId": "3299496",
                                    "name": "Jeff Rasley"
                                },
                                {
                                    "authorId": "1491753352",
                                    "name": "Max Ryabinin"
                                },
                                {
                                    "authorId": "1381446720",
                                    "name": "Mayank Mishra"
                                },
                                {
                                    "authorId": "67016465",
                                    "name": "Minjia Zhang"
                                },
                                {
                                    "authorId": "1911755",
                                    "name": "M. Shoeybi"
                                },
                                {
                                    "authorId": "31758637",
                                    "name": "Myriam Peyrounette"
                                },
                                {
                                    "authorId": "31614549",
                                    "name": "N. Patry"
                                },
                                {
                                    "authorId": "2179884903",
                                    "name": "Nouamane Tazi"
                                },
                                {
                                    "authorId": "2186979509",
                                    "name": "Omar Sanseviero"
                                },
                                {
                                    "authorId": "138609838",
                                    "name": "Patrick von Platen"
                                },
                                {
                                    "authorId": "2190281218",
                                    "name": "Pierre Cornette"
                                },
                                {
                                    "authorId": "2190280981",
                                    "name": "Pierre Franccois Lavall'ee"
                                },
                                {
                                    "authorId": "31734741",
                                    "name": "R. Lacroix"
                                },
                                {
                                    "authorId": "32817044",
                                    "name": "Samyam Rajbhandari"
                                },
                                {
                                    "authorId": "2188737826",
                                    "name": "Sanchit Gandhi"
                                },
                                {
                                    "authorId": "2110486618",
                                    "name": "Shaden Smith"
                                },
                                {
                                    "authorId": "2293408",
                                    "name": "S. Requena"
                                },
                                {
                                    "authorId": "2147312210",
                                    "name": "Suraj Patil"
                                },
                                {
                                    "authorId": "3239480",
                                    "name": "Tim Dettmers"
                                },
                                {
                                    "authorId": "114850513",
                                    "name": "Ahmed Baruwa"
                                },
                                {
                                    "authorId": null,
                                    "name": "Amanpreet Singh"
                                },
                                {
                                    "authorId": "2190281235",
                                    "name": "Anastasia Cheveleva"
                                },
                                {
                                    "authorId": "1769176",
                                    "name": "Anne-Laure Ligozat"
                                },
                                {
                                    "authorId": "1677386832",
                                    "name": "Arjun Subramonian"
                                },
                                {
                                    "authorId": "2190281078",
                                    "name": "Aur'elie N'ev'eol"
                                },
                                {
                                    "authorId": "10727711",
                                    "name": "Charles Lovering"
                                },
                                {
                                    "authorId": "2758616",
                                    "name": "Dan Garrette"
                                },
                                {
                                    "authorId": "70209311",
                                    "name": "D. Tunuguntla"
                                },
                                {
                                    "authorId": "144568312",
                                    "name": "Ehud Reiter"
                                },
                                {
                                    "authorId": "2051713939",
                                    "name": "Ekaterina Taktasheva"
                                },
                                {
                                    "authorId": "2135526571",
                                    "name": "E. Voloshina"
                                },
                                {
                                    "authorId": "2158860079",
                                    "name": "Eli Bogdanov"
                                },
                                {
                                    "authorId": "9162688",
                                    "name": "Genta Indra Winata"
                                },
                                {
                                    "authorId": "2184031883",
                                    "name": "Hailey Schoelkopf"
                                },
                                {
                                    "authorId": "3245041",
                                    "name": "Jan-Christoph Kalo"
                                },
                                {
                                    "authorId": "2848048",
                                    "name": "Jekaterina Novikova"
                                },
                                {
                                    "authorId": "39774809",
                                    "name": "J. Forde"
                                },
                                {
                                    "authorId": "47274259",
                                    "name": "Xiangru Tang"
                                },
                                {
                                    "authorId": "11348687",
                                    "name": "Jungo Kasai"
                                },
                                {
                                    "authorId": "50106621",
                                    "name": "Ken Kawamura"
                                },
                                {
                                    "authorId": "2047711867",
                                    "name": "Liam Hazan"
                                },
                                {
                                    "authorId": "2954727",
                                    "name": "Marine Carpuat"
                                },
                                {
                                    "authorId": "2029314697",
                                    "name": "Miruna Clinciu"
                                },
                                {
                                    "authorId": "8756748",
                                    "name": "Najoung Kim"
                                },
                                {
                                    "authorId": "15590401",
                                    "name": "Newton Cheng"
                                },
                                {
                                    "authorId": "1799401599",
                                    "name": "Oleg Serikov"
                                },
                                {
                                    "authorId": "2132545395",
                                    "name": "Omer Antverg"
                                },
                                {
                                    "authorId": "1986356851",
                                    "name": "Oskar van der Wal"
                                },
                                {
                                    "authorId": "15176410",
                                    "name": "Rui Zhang"
                                },
                                {
                                    "authorId": "49775305",
                                    "name": "Ruochen Zhang"
                                },
                                {
                                    "authorId": "3159346",
                                    "name": "Sebastian Gehrmann"
                                },
                                {
                                    "authorId": "8963527",
                                    "name": "Shachar Mirkin"
                                },
                                {
                                    "authorId": "3097741",
                                    "name": "S. Pais"
                                },
                                {
                                    "authorId": "2134610800",
                                    "name": "Tatiana Shavrina"
                                },
                                {
                                    "authorId": "90745780",
                                    "name": "Thomas Scialom"
                                },
                                {
                                    "authorId": "2127600348",
                                    "name": "Tian Yun"
                                },
                                {
                                    "authorId": "1666636295",
                                    "name": "Tomasz Limisiewicz"
                                },
                                {
                                    "authorId": "1681799",
                                    "name": "Verena Rieser"
                                },
                                {
                                    "authorId": "2135362820",
                                    "name": "Vitaly Protasov"
                                },
                                {
                                    "authorId": "51259225",
                                    "name": "V. Mikhailov"
                                },
                                {
                                    "authorId": "100984698",
                                    "name": "Yada Pruksachatkun"
                                },
                                {
                                    "authorId": "2083259",
                                    "name": "Yonatan Belinkov"
                                },
                                {
                                    "authorId": "2190281071",
                                    "name": "Zachary Bamberger"
                                },
                                {
                                    "authorId": "2343772132",
                                    "name": "Zden\u02c7ek Kasner"
                                },
                                {
                                    "authorId": "1805991958",
                                    "name": "Zden\u011bk Kasner"
                                },
                                {
                                    "authorId": "2190281954",
                                    "name": "A. Pestana"
                                },
                                {
                                    "authorId": "15845853",
                                    "name": "A. Feizpour"
                                },
                                {
                                    "authorId": "2190399370",
                                    "name": "Ammar Khan"
                                },
                                {
                                    "authorId": "2190281566",
                                    "name": "Amy Faranak"
                                },
                                {
                                    "authorId": "2148971654",
                                    "name": "A. Santos"
                                },
                                {
                                    "authorId": "1739149407",
                                    "name": "Anthony Hevia"
                                },
                                {
                                    "authorId": "2190281069",
                                    "name": "Antigona Unldreaj"
                                },
                                {
                                    "authorId": "115638227",
                                    "name": "Arash Aghagol"
                                },
                                {
                                    "authorId": "2361305",
                                    "name": "Arezoo Abdollahi"
                                },
                                {
                                    "authorId": "101302626",
                                    "name": "A. Tammour"
                                },
                                {
                                    "authorId": "3110645",
                                    "name": "A. HajiHosseini"
                                },
                                {
                                    "authorId": "2190281564",
                                    "name": "Bahareh Behroozi"
                                },
                                {
                                    "authorId": "83263885",
                                    "name": "Benjamin Ayoade Ajibade"
                                },
                                {
                                    "authorId": "31577522",
                                    "name": "B. Saxena"
                                },
                                {
                                    "authorId": "2005399190",
                                    "name": "Carlos Mu\u00f1oz Ferrandis"
                                },
                                {
                                    "authorId": "2075459",
                                    "name": "Danish Contractor"
                                },
                                {
                                    "authorId": "144635557",
                                    "name": "D. Lansky"
                                },
                                {
                                    "authorId": "2058260775",
                                    "name": "Davis David"
                                },
                                {
                                    "authorId": "2111313627",
                                    "name": "Douwe Kiela"
                                },
                                {
                                    "authorId": "5943347",
                                    "name": "D. A. Nguyen"
                                },
                                {
                                    "authorId": "47654100",
                                    "name": "Edward Tan"
                                },
                                {
                                    "authorId": "2026649806",
                                    "name": "Emi Baylor"
                                },
                                {
                                    "authorId": "2190281502",
                                    "name": "Ezinwanne Ozoani"
                                },
                                {
                                    "authorId": "35330153",
                                    "name": "F. Mirza"
                                },
                                {
                                    "authorId": "2190281922",
                                    "name": "Frankline Ononiwu"
                                },
                                {
                                    "authorId": "123343513",
                                    "name": "Habib Rezanejad"
                                },
                                {
                                    "authorId": "2119822136",
                                    "name": "H.A. Jones"
                                },
                                {
                                    "authorId": "2105001416",
                                    "name": "Indrani Bhattacharya"
                                },
                                {
                                    "authorId": "1404060690",
                                    "name": "Irene Solaiman"
                                },
                                {
                                    "authorId": "2190281379",
                                    "name": "Irina Sedenko"
                                },
                                {
                                    "authorId": "3163125",
                                    "name": "Isar Nejadgholi"
                                },
                                {
                                    "authorId": "145629075",
                                    "name": "J. Passmore"
                                },
                                {
                                    "authorId": "150162316",
                                    "name": "Joshua Seltzer"
                                },
                                {
                                    "authorId": "97979993",
                                    "name": "Julio Bonis Sanz"
                                },
                                {
                                    "authorId": null,
                                    "name": "Karen Fort"
                                },
                                {
                                    "authorId": "3530609",
                                    "name": "L\u00edvia Dutra"
                                },
                                {
                                    "authorId": "2190281373",
                                    "name": "Mairon Samagaio"
                                },
                                {
                                    "authorId": "2190281500",
                                    "name": "Maraim Elbadri"
                                },
                                {
                                    "authorId": "2921990",
                                    "name": "Margot Mieskes"
                                },
                                {
                                    "authorId": "151492708",
                                    "name": "Marissa Gerchick"
                                },
                                {
                                    "authorId": "2190281205",
                                    "name": "Martha Akinlolu"
                                },
                                {
                                    "authorId": "2060092577",
                                    "name": "Michael McKenna"
                                },
                                {
                                    "authorId": "2056851511",
                                    "name": "Mike Qiu"
                                },
                                {
                                    "authorId": "144449938",
                                    "name": "M. Ghauri"
                                },
                                {
                                    "authorId": "2190281203",
                                    "name": "Mykola Burynok"
                                },
                                {
                                    "authorId": "1401945312",
                                    "name": "Nafis Abrar"
                                },
                                {
                                    "authorId": "8937909",
                                    "name": "Nazneen Rajani"
                                },
                                {
                                    "authorId": "2190281555",
                                    "name": "Nour Elkott"
                                },
                                {
                                    "authorId": "1992948200",
                                    "name": "N. Fahmy"
                                },
                                {
                                    "authorId": "2164156047",
                                    "name": "Olanrewaju Samuel"
                                },
                                {
                                    "authorId": "2061141169",
                                    "name": "Ran An"
                                },
                                {
                                    "authorId": "9294251",
                                    "name": "R. Kromann"
                                },
                                {
                                    "authorId": "2137183106",
                                    "name": "Ryan Hao"
                                },
                                {
                                    "authorId": "4279554",
                                    "name": "S. Alizadeh"
                                },
                                {
                                    "authorId": "2190281531",
                                    "name": "Sarmad Shubber"
                                },
                                {
                                    "authorId": "2116420702",
                                    "name": "Silas L. Wang"
                                },
                                {
                                    "authorId": "2109853801",
                                    "name": "Sourav Roy"
                                },
                                {
                                    "authorId": "10726201",
                                    "name": "S. Viguier"
                                },
                                {
                                    "authorId": "2153620715",
                                    "name": "Thanh-Cong Le"
                                },
                                {
                                    "authorId": "2190281729",
                                    "name": "Tobi Oyebade"
                                },
                                {
                                    "authorId": "2153620985",
                                    "name": "T. Le"
                                },
                                {
                                    "authorId": "2190429590",
                                    "name": "Yoyo Yang"
                                },
                                {
                                    "authorId": "2297189567",
                                    "name": "Zach Nguyen"
                                },
                                {
                                    "authorId": "41124383",
                                    "name": "Abhinav Ramesh Kashyap"
                                },
                                {
                                    "authorId": "2318515251",
                                    "name": "A. Palasciano"
                                },
                                {
                                    "authorId": "2840689",
                                    "name": "A. Callahan"
                                },
                                {
                                    "authorId": "2042747208",
                                    "name": "Anima Shukla"
                                },
                                {
                                    "authorId": "1414073449",
                                    "name": "Antonio Miranda-Escalada"
                                },
                                {
                                    "authorId": "2110183222",
                                    "name": "A. Singh"
                                },
                                {
                                    "authorId": "1379935164",
                                    "name": "Benjamin Beilharz"
                                },
                                {
                                    "authorId": "2165371942",
                                    "name": "Bo Wang"
                                },
                                {
                                    "authorId": "144972524",
                                    "name": "C. Brito"
                                },
                                {
                                    "authorId": "2111169784",
                                    "name": "Chenxi Zhou"
                                },
                                {
                                    "authorId": "50732716",
                                    "name": "Chirag Jain"
                                },
                                {
                                    "authorId": "2158158973",
                                    "name": "Chuxin Xu"
                                },
                                {
                                    "authorId": "2080941785",
                                    "name": "Cl\u00e9mentine Fourrier"
                                },
                                {
                                    "authorId": "2174177869",
                                    "name": "Daniel Le'on Perin'an"
                                },
                                {
                                    "authorId": "2082057793",
                                    "name": "Daniel Molano"
                                },
                                {
                                    "authorId": "150978762",
                                    "name": "Dian Yu"
                                },
                                {
                                    "authorId": "24907368",
                                    "name": "Enrique Manjavacas"
                                },
                                {
                                    "authorId": "2139792578",
                                    "name": "Fabio Barth"
                                },
                                {
                                    "authorId": "2190281754",
                                    "name": "Florian Fuhrimann"
                                },
                                {
                                    "authorId": "2165227550",
                                    "name": "Gabriel Altay"
                                },
                                {
                                    "authorId": "2166224123",
                                    "name": "Giyaseddin Bayrak"
                                },
                                {
                                    "authorId": null,
                                    "name": "Gully Burns"
                                },
                                {
                                    "authorId": "88811067",
                                    "name": "Helena U. Vrabec"
                                },
                                {
                                    "authorId": "121044523",
                                    "name": "I. Bello"
                                },
                                {
                                    "authorId": "93460753",
                                    "name": "Isha Dash"
                                },
                                {
                                    "authorId": "72725318",
                                    "name": "J. Kang"
                                },
                                {
                                    "authorId": "37585306",
                                    "name": "John Giorgi"
                                },
                                {
                                    "authorId": "144983077",
                                    "name": "Jonas Golde"
                                },
                                {
                                    "authorId": "2066514466",
                                    "name": "J. Posada"
                                },
                                {
                                    "authorId": "1601562797",
                                    "name": "Karthi Sivaraman"
                                },
                                {
                                    "authorId": "2190281314",
                                    "name": "Lokesh Bulchandani"
                                },
                                {
                                    "authorId": "2145287083",
                                    "name": "Lu Liu"
                                },
                                {
                                    "authorId": "2100596120",
                                    "name": "Luisa Shinzato"
                                },
                                {
                                    "authorId": "2190281960",
                                    "name": "Madeleine Hahn de Bykhovetz"
                                },
                                {
                                    "authorId": "2068853922",
                                    "name": "Maiko Takeuchi"
                                },
                                {
                                    "authorId": "1850527789",
                                    "name": "Marc P\u00e0mies"
                                },
                                {
                                    "authorId": "87956698",
                                    "name": "M. A. Castillo"
                                },
                                {
                                    "authorId": "2174178585",
                                    "name": "Marianna Nezhurina"
                                },
                                {
                                    "authorId": "1879523878",
                                    "name": "Mario Sanger"
                                },
                                {
                                    "authorId": "3004898",
                                    "name": "M. Samwald"
                                },
                                {
                                    "authorId": "120397552",
                                    "name": "Michael Cullan"
                                },
                                {
                                    "authorId": "50564168",
                                    "name": "Michael Weinberg"
                                },
                                {
                                    "authorId": "2072502429",
                                    "name": "M. Wolf"
                                },
                                {
                                    "authorId": "2190280864",
                                    "name": "Mina Mihaljcic"
                                },
                                {
                                    "authorId": "2112211627",
                                    "name": "Minna Liu"
                                },
                                {
                                    "authorId": "1397064923",
                                    "name": "M. Freidank"
                                },
                                {
                                    "authorId": "4981508",
                                    "name": "Myungsun Kang"
                                },
                                {
                                    "authorId": "12046785",
                                    "name": "Natasha Seelam"
                                },
                                {
                                    "authorId": "48948105",
                                    "name": "N. Dahlberg"
                                },
                                {
                                    "authorId": "40208102",
                                    "name": "N. Broad"
                                },
                                {
                                    "authorId": "70256289",
                                    "name": "N. Muellner"
                                },
                                {
                                    "authorId": "40539650",
                                    "name": "Pascale Fung"
                                },
                                {
                                    "authorId": "2097023671",
                                    "name": "Patricia Haller"
                                },
                                {
                                    "authorId": "2298902857",
                                    "name": "Patrick Haller"
                                },
                                {
                                    "authorId": "115525190",
                                    "name": "R. Eisenberg"
                                },
                                {
                                    "authorId": "2111138678",
                                    "name": "Robert Martin"
                                },
                                {
                                    "authorId": "2291171257",
                                    "name": "Rodrigo Canalli"
                                },
                                {
                                    "authorId": "2190282202",
                                    "name": "Rosaline Su"
                                },
                                {
                                    "authorId": "153083809",
                                    "name": "Ruisi Su"
                                },
                                {
                                    "authorId": "66986482",
                                    "name": "Samuel Cahyawijaya"
                                },
                                {
                                    "authorId": "51878929",
                                    "name": "Samuele Garda"
                                },
                                {
                                    "authorId": "2174177330",
                                    "name": "Shlok S Deshmukh"
                                },
                                {
                                    "authorId": "2112134590",
                                    "name": "Shubhanshu Mishra"
                                },
                                {
                                    "authorId": "39620434",
                                    "name": "Sid Kiblawi"
                                },
                                {
                                    "authorId": "119994729",
                                    "name": "Simon Ott"
                                },
                                {
                                    "authorId": "2190281679",
                                    "name": "Sinee Sang-aroonsiri"
                                },
                                {
                                    "authorId": "120438284",
                                    "name": "Srishti Kumar"
                                },
                                {
                                    "authorId": "134757625",
                                    "name": "Stefan Schweter"
                                },
                                {
                                    "authorId": "8723233",
                                    "name": "S. Bharati"
                                },
                                {
                                    "authorId": "103242455",
                                    "name": "Tanmay Laud"
                                },
                                {
                                    "authorId": "2174176862",
                                    "name": "Th\u00e9o Gigant"
                                },
                                {
                                    "authorId": "2190281376",
                                    "name": "Tomoya Kainuma"
                                },
                                {
                                    "authorId": "50320098",
                                    "name": "Wojciech Kusa"
                                },
                                {
                                    "authorId": "2139767217",
                                    "name": "Yanis Labrak"
                                },
                                {
                                    "authorId": "1572961212",
                                    "name": "Yashasvi Bajaj"
                                },
                                {
                                    "authorId": "2051879548",
                                    "name": "Y. Venkatraman"
                                },
                                {
                                    "authorId": "2110154622",
                                    "name": "Yifan Xu"
                                },
                                {
                                    "authorId": "2118670234",
                                    "name": "Ying Xu"
                                },
                                {
                                    "authorId": "2142717873",
                                    "name": "Yu Xu"
                                },
                                {
                                    "authorId": "1643680733",
                                    "name": "Z. Tan"
                                },
                                {
                                    "authorId": "79110285",
                                    "name": "Zhongli Xie"
                                },
                                {
                                    "authorId": "2114134227",
                                    "name": "Zifan Ye"
                                },
                                {
                                    "authorId": "2065370401",
                                    "name": "M. Bras"
                                },
                                {
                                    "authorId": "2037496520",
                                    "name": "Younes Belkada"
                                },
                                {
                                    "authorId": "50335211",
                                    "name": "Thomas Wolf"
                                }
                            ],
                            "year": 2022,
                            "venue": "arXiv.org",
                            "n_citations": 2393
                        },
                        "score": 0.74072265625
                    },
                    {
                        "id": "(Wang et al., 2022)",
                        "snippets": [
                            "Causal decoder-only. Although the encoder-decoder is the original Transformer variant, most recent LLMs use a decoder-only architecture. These models can be trained as a traditional language model (i.e. to predict the next token in a sequence). Decoder-only models have no independent means of processing or representing the input sequence and target sequence differently-all tokens are processed in an equivalent fashion, and, because of the causal masking pattern, conditioning is simply based on past tokens (see Figure 2, on the left). On the one hand, this means that the representation for any conditioning text is inherently weaker; on the other hand, it yields a simpler architecture that is naturally suited to a standard autoregressive next-step-prediction pretraining objective. We refer to this architecture as causal decoder-only (CD) .\n\nNon-causal decoder-only. To allow decoder-only models to build richer non-causal representations of the input/conditioning text, it has been proposed to simply modify the attention mask used. Specifically, the self-attention masking pattern can be changed so that the region of the input sequence corresponding to conditioning information has a non-causal mask (i.e. attention in this region is not restricted to past tokens, see middle of Figure 2), as in the encoder of an encoder-decoder architecture. We refer to this architecture as non-causal decoder-only (ND) .\n\nComparisons across architectures. Decoder-only models process a single sequence consisting of the concatenation of the input and target text. On the other hand, in an encoder-decoder, the encoder processes only the input and the decoder processes only the target."
                        ],
                        "paper": {
                            "corpus_id": 248118752,
                            "title": "What Language Model Architecture and Pretraining Objective Work Best for Zero-Shot Generalization?",
                            "authors": [
                                {
                                    "authorId": "2135734748",
                                    "name": "Thomas Wang"
                                },
                                {
                                    "authorId": "145625142",
                                    "name": "Adam Roberts"
                                },
                                {
                                    "authorId": "80424302",
                                    "name": "Daniel Hesslow"
                                },
                                {
                                    "authorId": "1379806208",
                                    "name": "Teven Le Scao"
                                },
                                {
                                    "authorId": "3351938",
                                    "name": "Hyung Won Chung"
                                },
                                {
                                    "authorId": "46181066",
                                    "name": "Iz Beltagy"
                                },
                                {
                                    "authorId": "143945447",
                                    "name": "Julien Launay"
                                },
                                {
                                    "authorId": "2402716",
                                    "name": "Colin Raffel"
                                }
                            ],
                            "year": 2022,
                            "venue": "International Conference on Machine Learning",
                            "n_citations": 175
                        },
                        "score": 0.84423828125
                    },
                    {
                        "id": "(Patil et al., 2024)",
                        "snippets": [
                            "Encoder-decoder models adopt bidirectional attention for the encoder, unidirectional attention for the decoder, and a cross-attention mechanism between them. Cross-attention in the decoder has access only to the fully processed encoder output and is responsible for connecting input tokens to target tokens",
                            "Prefix language models are also decoder-only-based models but differ in the masking mechanism. Instead of a causal mask, a fully visible mask is used for the prefix part of the input sequence, and a causal mask is used for the target sequence",
                            "In a decoder-only model, the input and target are concatenated, and then a causal mask is used throughout. A decoder-only model with a prefix allows fully visible masking over part of the input token (prefix), followed by causal masking on the rest of the sequence. In general, autoencoding models learn bidirectional contextualized representation suited for NLU tasks, whereas autoregressive models learn to generate the next token and hence are suited for NLG tasks."
                        ],
                        "paper": {
                            "corpus_id": 268157336,
                            "title": "A Review of Current Trends, Techniques, and Challenges in Large Language Models (LLMs)",
                            "authors": [
                                {
                                    "authorId": "2289385425",
                                    "name": "Rajvardhan Patil"
                                },
                                {
                                    "authorId": "117730513",
                                    "name": "Venkat Gudivada"
                                }
                            ],
                            "year": 2024,
                            "venue": "Applied Sciences",
                            "n_citations": 80
                        },
                        "score": 0.77587890625
                    },
                    {
                        "id": "(Suganthan et al., 2025)",
                        "snippets": [
                            "A key question thus arises: can we effectively adapt the powerful knowledge embedded in decoder-only models to excel in these encoder-centric tasks?",
                            "Gemma's causal attention, ideal for generative tasks, inherently limits its applicability to encoder-based tasks. We demonstrate that simply enabling bidirectional attention during fine-tuning dramatically improves performance."
                        ],
                        "paper": {
                            "corpus_id": 276771845,
                            "title": "Adapting Decoder-Based Language Models for Diverse Encoder Downstream Tasks",
                            "authors": [
                                {
                                    "authorId": "1658871094",
                                    "name": "P. Suganthan"
                                },
                                {
                                    "authorId": "2165469946",
                                    "name": "Fedor Moiseev"
                                },
                                {
                                    "authorId": "2348489099",
                                    "name": "Le Yan"
                                },
                                {
                                    "authorId": "2261361394",
                                    "name": "Junru Wu"
                                },
                                {
                                    "authorId": "2348507846",
                                    "name": "Jianmo Ni"
                                },
                                {
                                    "authorId": "2348488953",
                                    "name": "Jay Han"
                                },
                                {
                                    "authorId": "1954563",
                                    "name": "I. Zitouni"
                                },
                                {
                                    "authorId": "1727837",
                                    "name": "Enrique Alfonseca"
                                },
                                {
                                    "authorId": "2348422460",
                                    "name": "Xuanhui Wang"
                                },
                                {
                                    "authorId": "2349772191",
                                    "name": "Zhe Dong"
                                }
                            ],
                            "year": 2025,
                            "venue": "arXiv.org",
                            "n_citations": 1
                        },
                        "score": 0.63134765625
                    },
                    {
                        "id": "(Busto-Castineira et al., 2024)",
                        "snippets": [
                            "While the encoder's attention is bidirectional, the decoder has two different types of attention: (i) a masked multi-head attention block that masks non-causal context and (ii) a bidirectional multihead attention block that receives non-causal information from the encoder",
                            "By omitting the encoder in decoder-only transformers, all non-causal contextual dependencies are removed by exclusively using masked attention. Decoder-only transformers are nowadays the best performing task-agnostic NLG systems. Nevertheless, there exist some state-of-theart non-causal NLG solutions. For example, non-causal language models can be trained for the Masked Language Modeling (MLM) objective, a task in which the language model predicts masked words within a sentence (Zeng et al., 2021)."
                        ],
                        "paper": {
                            "corpus_id": 270832367,
                            "title": "Predictability and Causality in Spanish and English Natural Language Generation",
                            "authors": [
                                {
                                    "authorId": "2222734467",
                                    "name": "Andrea Busto-Casti\u00f1eira"
                                },
                                {
                                    "authorId": "2323809078",
                                    "name": "Francisco Javier Gonz\u00e1lez-Casta\u00f1o"
                                },
                                {
                                    "authorId": "1405165681",
                                    "name": "Silvia Garc\u00eda-M\u00e9ndez"
                                },
                                {
                                    "authorId": "2034282614",
                                    "name": "Francisco de Arriba-P\u00e9rez"
                                }
                            ],
                            "year": 2024,
                            "venue": "IEEE Access",
                            "n_citations": 1
                        },
                        "score": 0.69140625
                    },
                    {
                        "id": "(Saha et al., 2023)",
                        "snippets": [
                            "The encoder component transforms input tokens into vectors using embeddings and positional encodings, then applies multihead self-attention and feedforward networks. The decoder, starting similarly, incorporates masked self-attention and crossattention with the output of the encoder, ensuring alignment and preventing future word prediction. Sequence-to-sequence models often use this architecture, where the input and output sequences can be of different lengths. These models shine in tasks where there is a direct and complex transformation between inputs and outputs. Examples like machine translation and text summarization are prototypical, as they require the model to understand the input deeply and generate a coherent and contextually accurate output. BART [5], T5 (Raffel et al., 2019), and UL2 (Tay et al., 2022) are a few well-known encoder-decoder models to be named",
                            ".In a decoder-only model, a sequence is fed into the model, which then directly predicts the next token or word in the sequence. It operates autoregressively, using its generated tokens as context for subsequent predictions. It has two variants: causal decoder and prefix decoder. In a standard decoder (causal decoder), the unidirectional attention masking ensures that a token attends only to previous tokens and itself. Prefix decoders permit bidirectional attention over prefix tokens while maintaining unidirectional attention to generated tokens."
                        ],
                        "paper": {
                            "corpus_id": 263829839,
                            "title": "LLM for SoC Security: A Paradigm Shift",
                            "authors": [
                                {
                                    "authorId": "2256992493",
                                    "name": "Dipayan Saha"
                                },
                                {
                                    "authorId": "2114625129",
                                    "name": "Shams Tarek"
                                },
                                {
                                    "authorId": "2256991081",
                                    "name": "Katayoon Yahyaei"
                                },
                                {
                                    "authorId": "2231854143",
                                    "name": "Sujan Kumar Saha"
                                },
                                {
                                    "authorId": "2257235852",
                                    "name": "Jingbo Zhou"
                                },
                                {
                                    "authorId": "145954982",
                                    "name": "M. Tehranipoor"
                                },
                                {
                                    "authorId": "1997019",
                                    "name": "Farimah Farahmandi"
                                }
                            ],
                            "year": 2023,
                            "venue": "IEEE Access",
                            "n_citations": 54
                        },
                        "score": 0.79833984375
                    },
                    {
                        "id": "(Rankovi'c et al., 2025)",
                        "snippets": [
                            "LLMs can follow different architectural designs: encoder-only (e.g., BERT 2), decoder-only (e.g., Qwen 52), and encoder-decoder (e.g., T5 49). Encoder-based models process the full input bidirectionally and are suited for classification and regression. Decoder-only models generate text autoregressively with causal masking. Encoder-decoder models combine both components and are often used for sequence-to-sequence tasks."
                        ],
                        "paper": {
                            "corpus_id": 277626915,
                            "title": "GOLLuM: Gaussian Process Optimized LLMs - Reframing LLM Finetuning through Bayesian Optimization",
                            "authors": [
                                {
                                    "authorId": "2219925647",
                                    "name": "Bojana Rankovi'c"
                                },
                                {
                                    "authorId": "2239074343",
                                    "name": "Philippe Schwaller"
                                }
                            ],
                            "year": 2025,
                            "venue": "arXiv.org",
                            "n_citations": 1
                        },
                        "score": 0.703125
                    },
                    {
                        "id": "(Uludougan et al., 2024)",
                        "snippets": [
                            "Models that exclusively employ encoders, typically trained with denoising objectives, are geared toward understanding tasks, as exemplified in works such as (Devlin et al., 2019) and (208229926). Conversely, models that exclusively use decoders are designed for generation tasks, employing causal language modeling, as demonstrated in various studies (Radford et al., 2019)Brown et al., 2020;Touvron et al., 2023). The Text-to-Text Transformer (T5) (Raffel et al., 2019), on the other hand, employs an encoder-decoder architecture and undergoes pretraining with a denoising objective referred to as span corruption. UniLM (Dong et al., 2019) is also an encoder-decoder model, but pre-trained using unidirectional, bidirectional, and sequence-to-sequence language modeling. This can be seen as a combination of causal and denoising objectives."
                        ],
                        "paper": {
                            "corpus_id": 267211690,
                            "title": "TURNA: A Turkish Encoder-Decoder Language Model for Enhanced Understanding and Generation",
                            "authors": [
                                {
                                    "authorId": "2281033147",
                                    "name": "Gokcce Uludougan"
                                },
                                {
                                    "authorId": "2281033141",
                                    "name": "Zeynep Yirmibecsouglu Balal"
                                },
                                {
                                    "authorId": "2174736343",
                                    "name": "Furkan Akkurt"
                                },
                                {
                                    "authorId": "2281033264",
                                    "name": "Melikcsah Turker"
                                },
                                {
                                    "authorId": "9179697",
                                    "name": "Onur Gungor"
                                },
                                {
                                    "authorId": "66493576",
                                    "name": "S. Uskudarli"
                                }
                            ],
                            "year": 2024,
                            "venue": "Annual Meeting of the Association for Computational Linguistics",
                            "n_citations": 12
                        },
                        "score": 0.64208984375
                    },
                    {
                        "id": "(Tay et al., 2022)",
                        "snippets": [
                            "The line between decoder-only and encoder-decoder models is less clear. PrefixLM models are almost encoder-decoder models with shared parameters (but not quite). From an inductive bias point of view, there are multiple differences. Encoder-Decoder models process input and targets independently with a different set of parameters. This is a form of sparsity where different set of parameters are used for different tokens. Encoder-Decoder models also have a cross attention component that connects input tokens to target tokens. Meanwhile, decoder-only models process inputs and targets by concatenating them. Hence, the representations of inputs and targets are concurrently build layer by layer as the input/targets propagate up the network. Conversely, the decoder in Encoder-decoder models generally only looks at the fully processed encoder input. Overall, the inductive bias of PrefixLM decoder-only models and Encoder-Decoder models could be pretty similar modulo the subtle differences stated above. The distinct property is that Encoder-Decoder models are generally approximately 2x parameters of a decoder-only model when compute-matched."
                        ],
                        "paper": {
                            "corpus_id": 252780443,
                            "title": "UL2: Unifying Language Learning Paradigms",
                            "authors": [
                                {
                                    "authorId": "144447820",
                                    "name": "Yi Tay"
                                },
                                {
                                    "authorId": "3226635",
                                    "name": "Mostafa Dehghani"
                                },
                                {
                                    "authorId": "2057663102",
                                    "name": "Vinh Q. Tran"
                                },
                                {
                                    "authorId": "143936294",
                                    "name": "Xavier Garc\u00eda"
                                },
                                {
                                    "authorId": "119640649",
                                    "name": "Jason Wei"
                                },
                                {
                                    "authorId": "1524732527",
                                    "name": "Xuezhi Wang"
                                },
                                {
                                    "authorId": "3351938",
                                    "name": "Hyung Won Chung"
                                },
                                {
                                    "authorId": "2119725651",
                                    "name": "Dara Bahri"
                                },
                                {
                                    "authorId": "32303439",
                                    "name": "Tal Schuster"
                                },
                                {
                                    "authorId": "2115689465",
                                    "name": "H. Zheng"
                                },
                                {
                                    "authorId": "65855107",
                                    "name": "Denny Zhou"
                                },
                                {
                                    "authorId": "2815290",
                                    "name": "N. Houlsby"
                                },
                                {
                                    "authorId": "1680617",
                                    "name": "Donald Metzler"
                                }
                            ],
                            "year": 2022,
                            "venue": "International Conference on Learning Representations",
                            "n_citations": 313
                        },
                        "score": 0.84814453125
                    },
                    {
                        "id": "(Kim et al., 2023)",
                        "snippets": [
                            "Cumulative Quantization Errors in Causal Attention. Causal attention, which integrates masking into self-attention to avoid referencing future tokens, is vital for text generation tasks. To comprehend the quantization characteristics of GLMs, we contrast the computation procedure of the self-attention map. For a Transformer encoder, the quantization error reflected on the self-attention map due to Q, K, and V computation is uniformly spread because of the simultaneous computing nature inherent in Transformer encoders. However, the mask in the causal attention accumulates quantization errors from each token, creating uneven errors in the output computation. \n\nTo elucidate, consider the encoder self-attention depicted at the top of Fig. 1(a). During the computation of the weighted sum of value representations for tokens 1 and 3, all attention probabilities in the self-attention map impacted by quantization errors are incorporated. On the other hand, in the decoder's causal attention, as illustrated at the bottom of Fig. 1(a), the output value representation is computed using only the self-attention probabilities of the current token and those before it. For example, while generating the output value representation for token 1 (highlighted in a blue box), only two attention probabilities affected by quantization error are used. In contrast, for token 3 (indicated by the red box), probabilities from all preceding tokens are incorporated. This observation underscores that the inherent design of causal attention results in an accumulation of quantization errors, especially towards the latter tokens. Given this characteristic, it becomes imperative to devise a decoder QAT approach that mitigates this uneven distribution of quantization errors within the causal attention mechanism. \n\nNecessity of Ground Truth Loss. In fine-tuning processes, encoder-only models, often used in Natural Language Understanding (NLU), and decoder-only models, common in Natural Language Generation (NLG) with causal language modeling approach, exhibit different mechanisms for receiving ground truth loss. Fig. 1(b) illustrates the differences. In NLU tasks, encoder models leverage the representation of a unique special token, transformed into a logit vector, for calculating cross-entropy loss, with the number of classes usually being a few [12]. Conversely, decoder models in NLG tasks predict the subsequent token for each input token, transforming each token representation into a logit vector with a class size equivalent to the vocabulary size, often exceeding 50k [25]. Hence, while encoder models gain ground truth loss from a single special token's logit, decoder models derive it from the output logits of all input tokens, which could provide more detailed token-level prediction information."
                        ],
                        "paper": {
                            "corpus_id": 260886785,
                            "title": "Token-Scaled Logit Distillation for Ternary Weight Generative Language Models",
                            "authors": [
                                {
                                    "authorId": "2141320070",
                                    "name": "Minsoo Kim"
                                },
                                {
                                    "authorId": "2144376191",
                                    "name": "Sihwa Lee"
                                },
                                {
                                    "authorId": "2265920992",
                                    "name": "Janghwan Lee"
                                },
                                {
                                    "authorId": "2158125346",
                                    "name": "S. Hong"
                                },
                                {
                                    "authorId": "2180828053",
                                    "name": "Duhyeuk Chang"
                                },
                                {
                                    "authorId": "66936521",
                                    "name": "Wonyong Sung"
                                },
                                {
                                    "authorId": "2506452",
                                    "name": "Jungwook Choi"
                                }
                            ],
                            "year": 2023,
                            "venue": "Neural Information Processing Systems",
                            "n_citations": 15
                        },
                        "score": 0.93896484375
                    },
                    {
                        "id": "(Jain et al., 2022)",
                        "snippets": [
                            "Compared to the causal (left-to-right) attention mechanism of the decoder-only models, the bidirectional attention mechanism in both encoder-only and encoder-decoder models allows for better leverage of the context of the sequence and hence leads to better representations."
                        ],
                        "paper": {
                            "corpus_id": 258461112,
                            "title": "ContraCLM: Contrastive Learning For Causal Language Model",
                            "authors": [
                                {
                                    "authorId": "2146677401",
                                    "name": "Nihal Jain"
                                },
                                {
                                    "authorId": "2358258",
                                    "name": "Dejiao Zhang"
                                },
                                {
                                    "authorId": "38123220",
                                    "name": "Wasi Uddin Ahmad"
                                },
                                {
                                    "authorId": "50219006",
                                    "name": "Zijian Wang"
                                },
                                {
                                    "authorId": "144647318",
                                    "name": "Feng Nan"
                                },
                                {
                                    "authorId": "2187045812",
                                    "name": "Xiaopeng Li"
                                },
                                {
                                    "authorId": "144745483",
                                    "name": "Ming Tan"
                                },
                                {
                                    "authorId": "1701451",
                                    "name": "Ramesh Nallapati"
                                },
                                {
                                    "authorId": "31631000",
                                    "name": "Baishakhi Ray"
                                },
                                {
                                    "authorId": "50339091",
                                    "name": "Parminder Bhatia"
                                },
                                {
                                    "authorId": "47646605",
                                    "name": "Xiaofei Ma"
                                },
                                {
                                    "authorId": "144028698",
                                    "name": "Bing Xiang"
                                }
                            ],
                            "year": 2022,
                            "venue": "Annual Meeting of the Association for Computational Linguistics",
                            "n_citations": 16
                        },
                        "score": 0.705078125
                    },
                    {
                        "id": "(Ma et al., 2024)",
                        "snippets": [
                            "As outlined in Sec. 1, we observe two discrepancies between decoder-only LLMs and encoder-decoder models: optimization objective and model architecture. Specifically, the decoder-only LLMs are typically optimized using the next token prediction task while the encoder-decoder models are trained with the masked language modeling task. Besides, the former tokens in a sequence cannot attend the latter tokens in decoder-only LLMs while every token in the sequence can attend each other in the encoder models."
                        ],
                        "paper": {
                            "corpus_id": 270560675,
                            "title": "Exploring the Role of Large Language Models in Prompt Encoding for Diffusion Models",
                            "authors": [
                                {
                                    "authorId": "2261489892",
                                    "name": "Bingqi Ma"
                                },
                                {
                                    "authorId": "1571400317",
                                    "name": "Zhuofan Zong"
                                },
                                {
                                    "authorId": "12920342",
                                    "name": "Guanglu Song"
                                },
                                {
                                    "authorId": "2261394248",
                                    "name": "Hongsheng Li"
                                },
                                {
                                    "authorId": "2261417717",
                                    "name": "Yu Liu"
                                }
                            ],
                            "year": 2024,
                            "venue": "Neural Information Processing Systems",
                            "n_citations": 23
                        },
                        "score": 0.7783203125
                    },
                    {
                        "id": "(Tay et al., 2020)",
                        "snippets": [
                            "In encoder mode, there is no restriction or constraint that the self-attention mechanism has to be causal, i.e., dependent solely on the present and past tokens. In the encoder-decoder setting, self-attention used in the decoder (i.e. across decoding positions) must be causal since each auto-regressive decoding step can only depend on previous tokens, whereas the selfattention used in the encoder need not. Fulfilling this requirement can prove challenging for many efficient self-attention designs."
                        ],
                        "paper": {
                            "corpus_id": 221702858,
                            "title": "Efficient Transformers: A Survey",
                            "authors": [
                                {
                                    "authorId": "144447820",
                                    "name": "Yi Tay"
                                },
                                {
                                    "authorId": "3226635",
                                    "name": "Mostafa Dehghani"
                                },
                                {
                                    "authorId": "11774695",
                                    "name": "Dara Bahri"
                                },
                                {
                                    "authorId": "1680617",
                                    "name": "Donald Metzler"
                                }
                            ],
                            "year": 2020,
                            "venue": "ACM Computing Surveys",
                            "n_citations": 1128
                        },
                        "score": 0.806640625
                    }
                ],
                "format": "synthesis",
                "table": null,
                "model": "claude-3-7-sonnet-20250219"
            }
        ],
        "cost": 0.29951099999999997
    }
}