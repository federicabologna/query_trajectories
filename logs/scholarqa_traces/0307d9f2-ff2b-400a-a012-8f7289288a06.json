{
    "query": "copyright infringement of image generation models",
    "user_id": "lib_user",
    "task_id": "0307d9f2-ff2b-400a-a012-8f7289288a06",
    "timestamp": "2025-06-23T23:06:41.695394",
    "n_retrieval": 256,
    "n_retrieved": 260,
    "n_candidates": 27,
    "n_rerank": 50,
    "opt_in": true,
    "total_cost": 0.411042,
    "decomposed_query": {
        "rewritten_query": "Copyright infringement of image generation models.",
        "keyword_query": "copyright infringement image generation models",
        "search_filters": {
            "fieldsOfStudy": "Computer Science,Law"
        },
        "cost": 0.009402,
        "model": "claude-3-7-sonnet-20250219"
    },
    "candidates": [
        {
            "title": "CopyScope: Model-level Copyright Infringement Quantification in the Diffusion Workflow",
            "venue": "arXiv.org",
            "year": 2023,
            "reference_count": 32,
            "citation_count": 2,
            "influential_citation_count": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2311.12847, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2267879159",
                    "name": "Junlei Zhou"
                },
                {
                    "authorId": "2149258131",
                    "name": "Jiashi Gao"
                },
                {
                    "authorId": "2184222659",
                    "name": "Ziwei Wang"
                },
                {
                    "authorId": "2255554914",
                    "name": "Xuetao Wei"
                }
            ],
            "abstract": "Web-based AI image generation has become an innovative art form that can generate novel artworks with the rapid development of the diffusion model. However, this new technique brings potential copyright infringement risks as it may incorporate the existing artworks without the owners' consent. Copyright infringement quantification is the primary and challenging step towards AI-generated image copyright traceability. Previous work only focused on data attribution from the training data perspective, which is unsuitable for tracing and quantifying copyright infringement in practice because of the following reasons: (1) the training datasets are not always available in public; (2) the model provider is the responsible party, not the image. Motivated by this, in this paper, we propose CopyScope, a new framework to quantify the infringement of AI-generated images from the model level. We first rigorously identify pivotal components within the AI image generation pipeline. Then, we propose to take advantage of Fr\\'echet Inception Distance (FID) to effectively capture the image similarity that fits human perception naturally. We further propose the FID-based Shapley algorithm to evaluate the infringement contribution among models. Extensive experiments demonstrate that our work not only reveals the intricacies of infringement quantification but also effectively depicts the infringing models quantitatively, thus promoting accountability in AI image-generation tasks.",
            "corpus_id": 265351912,
            "sentences": [
                {
                    "corpus_id": "265351912",
                    "title": "CopyScope: Model-level Copyright Infringement Quantification in the Diffusion Workflow",
                    "text": "Web-based AI image generation has become an innovative art form that can generate novel artworks with the rapid development of the diffusion model. However, this new technique brings potential copyright infringement risks as it may incorporate the existing artworks without the owners' consent. Copyright infringement quantification is the primary and challenging step towards AI-generated image copyright traceability. Previous work only focused on data attribution from the training data perspective, which is unsuitable for tracing and quantifying copyright infringement in practice because of the following reasons: (1) the training datasets are not always available in public; (2) the model provider is the responsible party, not the image. Motivated by this, in this paper, we propose CopyScope, a new framework to quantify the infringement of AI-generated images from the model level. We first rigorously identify pivotal components within the AI image generation pipeline. Then, we propose to take advantage of Fr\\'echet Inception Distance (FID) to effectively capture the image similarity that fits human perception naturally. We further propose the FID-based Shapley algorithm to evaluate the infringement contribution among models. Extensive experiments demonstrate that our work not only reveals the intricacies of infringement quantification but also effectively depicts the infringing models quantitatively, thus promoting accountability in AI image-generation tasks.",
                    "score": 0.6008146993188959,
                    "section_title": "abstract",
                    "char_start_offset": 0,
                    "sentence_offsets": [],
                    "ref_mentions": [],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.998046875
                },
                {
                    "corpus_id": "265351912",
                    "title": "CopyScope: Model-level Copyright Infringement Quantification in the Diffusion Workflow",
                    "text": "[8], Midjourney [1], are setting off a new revolution of artwork creation. These programs allow users to effortlessly generate target images by taking some descriptions as input into the model. However, these AI-generated artworks inherit the characteristics of the images that are used to train models [33,40,43], which might be pretty similar to the original ones, as shown in Figure 1. Such similarity has aroused concerns about copyright infringement disputes. For example, three artists (Sarah Andersen, Kelly McKernan, and Karla Ortiz) have recently accused Stable Diffusion of unlawfully  2) was created by Stable Diffusion using the \"style of Erin Hanson\" as a prompt. The styles of these two images are so similar that it is impossible to tell them apart. \n\nscraping copyrighted images from the Internet to mimic their art styles [11]. To this end, research on responsible AI image generation is in urgent need to address such copyright issues. \n\nPrevious work on data attribution [29,31,32,42] focused on how the images in the training data contributed to the model's outcome, which is not suitable for the context of copyright traceability. This is because of the following reasons: (1) the training data is not known in advance in real-world practices; as shown in Figure 2, the models are usually publicly available, while the training datasets are not [22,43]. (2) the responsible party is the model provider, namely, the personnel or the organization who abused the online image collections without the owners' consent, not the image itself. As long as the model that generates the infringing image is identified, the corresponding infringer (the model provider) can be found, and the degree of infringement can be quantified. Thus, we need to develop a new approach to quantify copyright infringement from the model level, which is the focus of our paper. \n\nTo this end, we propose a new framework CopyScope at the model level towards AIGC copyright traceability. Our framework CopyScope includes three closely intertwined stages (Identify-Quantify-Evaluate).",
                    "score": 0.46017829201599986,
                    "section_title": "AI image generation programs, namely Artificial Intelligence Generated Content (AIGC) tools such as Stable Diffusion [20], DALL\u2022E2",
                    "char_start_offset": 147,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 74
                        },
                        {
                            "start": 75,
                            "end": 193
                        },
                        {
                            "start": 194,
                            "end": 388
                        },
                        {
                            "start": 389,
                            "end": 464
                        },
                        {
                            "start": 465,
                            "end": 676
                        },
                        {
                            "start": 677,
                            "end": 764
                        },
                        {
                            "start": 767,
                            "end": 844
                        },
                        {
                            "start": 845,
                            "end": 953
                        },
                        {
                            "start": 956,
                            "end": 1151
                        },
                        {
                            "start": 1152,
                            "end": 1374
                        },
                        {
                            "start": 1375,
                            "end": 1556
                        },
                        {
                            "start": 1557,
                            "end": 1741
                        },
                        {
                            "start": 1742,
                            "end": 1871
                        },
                        {
                            "start": 1874,
                            "end": 1979
                        },
                        {
                            "start": 1980,
                            "end": 2075
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 990,
                            "end": 994,
                            "matchedPaperCorpusId": "235719065"
                        },
                        {
                            "start": 997,
                            "end": 1000,
                            "matchedPaperCorpusId": "249947012"
                        },
                        {
                            "start": 1000,
                            "end": 1003,
                            "matchedPaperCorpusId": "249319573"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.99609375
                },
                {
                    "corpus_id": "265351912",
                    "title": "CopyScope: Model-level Copyright Infringement Quantification in the Diffusion Workflow",
                    "text": "Although these work have studied various watermarking methods to achieve model traceability, these methods can only trace back to a chosen specific model in their experiments, without considering the interplay among models in the complex AI image generation task. \n\nTo address the challenge of potential copyright infringement in AI-generated images, we have proposed a new framework called CopyScope that could identify different copyright infringement sources at the model level in the AI image generation process and evaluate their impact. We have proposed a FID-based Shapley algorithm to assess the infringement contribution of each model in the diffusion workflow. Extensive results have demonstrated that our proposed CopyScope framework could effectively zoom in on the sources and quantify the impact of infringement models in AI image generation. Our work offers a promising solution for copyright traceability in AI image generation, which could also promote the legally compliant use of AI-generated content.",
                    "score": 0.6295279713534645,
                    "section_title": "RELATED WORK",
                    "char_start_offset": 25505,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 263
                        },
                        {
                            "start": 266,
                            "end": 542
                        },
                        {
                            "start": 543,
                            "end": 670
                        },
                        {
                            "start": 671,
                            "end": 856
                        },
                        {
                            "start": 857,
                            "end": 1020
                        }
                    ],
                    "ref_mentions": [],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.994140625
                },
                {
                    "corpus_id": "265351912",
                    "title": "CopyScope: Model-level Copyright Infringement Quantification in the Diffusion Workflow",
                    "text": "We initiate our study by determining components that have the most significant impacts on the generated images. Components make up the AI image generation workflow, which is used to characterize generation models in our proposed copyright tracking approach. This stage is based on a survey from the world's largest AI image generation exchange and sharing platform Civitai, where we collect more than 16,000 generated image data from over 5,000 models to find commonalities in generated images. The generated images are divided into 6 themes: celebrity, film&TV, artwork, popular models, design, and game. We explore the distribution of models that generate images involving copyright infringement by calculating the usage rate of components: Base Model, Lora, ControlNet, and Key Prompt. Table 2 shows the frequency of these components, indicating that they have a high usage rate in AI image-generation tasks. \n\nWe identify four components that are used in AI image generation at a high frequency: \u2776 The Base Model is essential for each generated image. \u2777 The second is the Lora. Although the Lora is not a necessary option for generating images, we can find from Table 2 that it has a high application rate in each category, indicating that the use of Lora for adjustment in AI image generation has become a norm. \u2778 Prompts with particular specificity are called Key Prompts. Key Prompt can make the generated image close to the characteristics of these keywords to a large extent, thus infringing on the original author's rights. \u2779 The overall usage rate of ControlNet is relatively lower than the other components. This is because the ControlNet is challenging to use as it needs higher environment configuration requirements than Lora and Key Prompt[3]. However, ControlNet is an essential components in generating particular themes as it controls the structure of the image. From the perspective of copyright tracing, the ControlNet is a critical suspected infringement component that our CopyScope framework considers.",
                    "score": 0.4855333327685923,
                    "section_title": "Identify Influential components",
                    "char_start_offset": 10528,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 111
                        },
                        {
                            "start": 112,
                            "end": 257
                        },
                        {
                            "start": 258,
                            "end": 494
                        },
                        {
                            "start": 495,
                            "end": 605
                        },
                        {
                            "start": 606,
                            "end": 788
                        },
                        {
                            "start": 789,
                            "end": 911
                        },
                        {
                            "start": 914,
                            "end": 1055
                        },
                        {
                            "start": 1056,
                            "end": 1081
                        },
                        {
                            "start": 1082,
                            "end": 1316
                        },
                        {
                            "start": 1317,
                            "end": 1378
                        },
                        {
                            "start": 1379,
                            "end": 1533
                        },
                        {
                            "start": 1534,
                            "end": 1619
                        },
                        {
                            "start": 1620,
                            "end": 1759
                        },
                        {
                            "start": 1760,
                            "end": 1881
                        },
                        {
                            "start": 1882,
                            "end": 2026
                        }
                    ],
                    "ref_mentions": [],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.9921875
                }
            ],
            "relevance_judgement": 0.998046875,
            "relevance_judgment_input_expanded": "# Title: CopyScope: Model-level Copyright Infringement Quantification in the Diffusion Workflow\n# Venue: arXiv.org\n# Authors: Junlei Zhou, Jiashi Gao, Ziwei Wang, Xuetao Wei\n## Abstract\nWeb-based AI image generation has become an innovative art form that can generate novel artworks with the rapid development of the diffusion model. However, this new technique brings potential copyright infringement risks as it may incorporate the existing artworks without the owners' consent. Copyright infringement quantification is the primary and challenging step towards AI-generated image copyright traceability. Previous work only focused on data attribution from the training data perspective, which is unsuitable for tracing and quantifying copyright infringement in practice because of the following reasons: (1) the training datasets are not always available in public; (2) the model provider is the responsible party, not the image. Motivated by this, in this paper, we propose CopyScope, a new framework to quantify the infringement of AI-generated images from the model level. We first rigorously identify pivotal components within the AI image generation pipeline. Then, we propose to take advantage of Fr\\'echet Inception Distance (FID) to effectively capture the image similarity that fits human perception naturally. We further propose the FID-based Shapley algorithm to evaluate the infringement contribution among models. Extensive experiments demonstrate that our work not only reveals the intricacies of infringement quantification but also effectively depicts the infringing models quantitatively, thus promoting accountability in AI image-generation tasks.\n## AI image generation programs, namely Artificial Intelligence Generated Content (AIGC) tools such as Stable Diffusion [20], DALL\u2022E2\n[8], Midjourney [1], are setting off a new revolution of artwork creation. These programs allow users to effortlessly generate target images by taking some descriptions as input into the model. However, these AI-generated artworks inherit the characteristics of the images that are used to train models [33,40,43], which might be pretty similar to the original ones, as shown in Figure 1. Such similarity has aroused concerns about copyright infringement disputes. For example, three artists (Sarah Andersen, Kelly McKernan, and Karla Ortiz) have recently accused Stable Diffusion of unlawfully  2) was created by Stable Diffusion using the \"style of Erin Hanson\" as a prompt. The styles of these two images are so similar that it is impossible to tell them apart. \n\nscraping copyrighted images from the Internet to mimic their art styles [11]. To this end, research on responsible AI image generation is in urgent need to address such copyright issues. \n\nPrevious work on data attribution [29,31,32,42] focused on how the images in the training data contributed to the model's outcome, which is not suitable for the context of copyright traceability. This is because of the following reasons: (1) the training data is not known in advance in real-world practices; as shown in Figure 2, the models are usually publicly available, while the training datasets are not [22,43]. (2) the responsible party is the model provider, namely, the personnel or the organization who abused the online image collections without the owners' consent, not the image itself. As long as the model that generates the infringing image is identified, the corresponding infringer (the model provider) can be found, and the degree of infringement can be quantified. Thus, we need to develop a new approach to quantify copyright infringement from the model level, which is the focus of our paper. \n\nTo this end, we propose a new framework CopyScope at the model level towards AIGC copyright traceability. Our framework CopyScope includes three closely intertwined stages (Identify-Quantify-Evaluate).\n\n## Identify Influential components\nWe initiate our study by determining components that have the most significant impacts on the generated images. Components make up the AI image generation workflow, which is used to characterize generation models in our proposed copyright tracking approach. This stage is based on a survey from the world's largest AI image generation exchange and sharing platform Civitai, where we collect more than 16,000 generated image data from over 5,000 models to find commonalities in generated images. The generated images are divided into 6 themes: celebrity, film&TV, artwork, popular models, design, and game. We explore the distribution of models that generate images involving copyright infringement by calculating the usage rate of components: Base Model, Lora, ControlNet, and Key Prompt. Table 2 shows the frequency of these components, indicating that they have a high usage rate in AI image-generation tasks. \n\nWe identify four components that are used in AI image generation at a high frequency: \u2776 The Base Model is essential for each generated image. \u2777 The second is the Lora. Although the Lora is not a necessary option for generating images, we can find from Table 2 that it has a high application rate in each category, indicating that the use of Lora for adjustment in AI image generation has become a norm. \u2778 Prompts with particular specificity are called Key Prompts. Key Prompt can make the generated image close to the characteristics of these keywords to a large extent, thus infringing on the original author's rights. \u2779 The overall usage rate of ControlNet is relatively lower than the other components. This is because the ControlNet is challenging to use as it needs higher environment configuration requirements than Lora and Key Prompt[3]. However, ControlNet is an essential components in generating particular themes as it controls the structure of the image. From the perspective of copyright tracing, the ControlNet is a critical suspected infringement component that our CopyScope framework considers.\n\n## RELATED WORK\nAlthough these work have studied various watermarking methods to achieve model traceability, these methods can only trace back to a chosen specific model in their experiments, without considering the interplay among models in the complex AI image generation task. \n\nTo address the challenge of potential copyright infringement in AI-generated images, we have proposed a new framework called CopyScope that could identify different copyright infringement sources at the model level in the AI image generation process and evaluate their impact. We have proposed a FID-based Shapley algorithm to assess the infringement contribution of each model in the diffusion workflow. Extensive results have demonstrated that our proposed CopyScope framework could effectively zoom in on the sources and quantify the impact of infringement models in AI image generation. Our work offers a promising solution for copyright traceability in AI image generation, which could also promote the legally compliant use of AI-generated content.",
            "reference_string": "[265351912 | Zhou et al. | 2023 | Citations: 2]"
        },
        {
            "title": "On Copyright Risks of Text-to-Image Diffusion Models",
            "venue": "",
            "year": 2023,
            "reference_count": 31,
            "citation_count": 10,
            "influential_citation_count": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2311.12803, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2267877984",
                    "name": "Yang Zhang"
                },
                {
                    "authorId": "2267728071",
                    "name": "Teoh Tze Tzun"
                },
                {
                    "authorId": "2267727392",
                    "name": "Lim Wei Hern"
                },
                {
                    "authorId": "2267866973",
                    "name": "Haonan Wang"
                },
                {
                    "authorId": "2256995496",
                    "name": "Kenji Kawaguchi"
                }
            ],
            "abstract": "Diffusion models excel in many generative modeling tasks, notably in creating images from text prompts, a task referred to as text-to-image (T2I) generation. Despite the ability to generate high-quality images, these models often replicate elements from their training data, leading to increasing copyright concerns in real applications in recent years. In response to this raising concern about copyright infringement, recent studies have studied the copyright behavior of diffusion models when using direct, copyrighted prompts. Our research extends this by examining subtler forms of infringement, where even indirect prompts can trigger copyright issues. Specifically, we introduce a data generation pipeline to systematically produce data for studying copyright in diffusion models. Our pipeline enables us to investigate copyright infringement in a more practical setting, involving replicating visual features rather than entire works using seemingly irrelevant prompts for T2I generation. We generate data using our proposed pipeline to test various diffusion models, including the latest Stable Diffusion XL. Our findings reveal a widespread tendency that these models tend to produce copyright-infringing content, highlighting a significant challenge in this field.",
            "corpus_id": 265352103,
            "sentences": [
                {
                    "corpus_id": "265352103",
                    "title": "On Copyright Risks of Text-to-Image Diffusion Models",
                    "text": "Diffusion models excel in many generative modeling tasks, notably in creating images from text prompts, a task referred to as text-to-image (T2I) generation. Despite the ability to generate high-quality images, these models often replicate elements from their training data, leading to increasing copyright concerns in real applications in recent years. In response to this raising concern about copyright infringement, recent studies have studied the copyright behavior of diffusion models when using direct, copyrighted prompts. Our research extends this by examining subtler forms of infringement, where even indirect prompts can trigger copyright issues. Specifically, we introduce a data generation pipeline to systematically produce data for studying copyright in diffusion models. Our pipeline enables us to investigate copyright infringement in a more practical setting, involving replicating visual features rather than entire works using seemingly irrelevant prompts for T2I generation. We generate data using our proposed pipeline to test various diffusion models, including the latest Stable Diffusion XL. Our findings reveal a widespread tendency that these models tend to produce copyright-infringing content, highlighting a significant challenge in this field.",
                    "score": 0.5863427452031565,
                    "section_title": "abstract",
                    "char_start_offset": 0,
                    "sentence_offsets": [],
                    "ref_mentions": [],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.9970703125
                },
                {
                    "corpus_id": "265352103",
                    "title": "On Copyright Risks of Text-to-Image Diffusion Models",
                    "text": "Copyright infringement for generative models. For copyright infringement, we focus on the copyright regulation in the US [Copyright Office, 2022]. Nevertheless, the concept of copyright applies to the regulation in other countries. In the context of copyright law in the US, whether a piece of copyrighted material can be used by others is governed by the concept of Fair Use which permits the use of copyrighted material only in a transformative manner that is distinct from the original work. There have been legal precedents which demonstrate that structural similarity can lead to infringement claims. Given that such generative models are trained on datasets such as LAION-5B [Schuhmann et al., 2022] which contains publicly available copyrighted data, if the models generate images with visual features that have substantial structural similarity to the original copyrighted images, this would likely be grounds for claims of copyright infringement. \n\nThis has implications in commercial settings such as companies selling proprietary image generation models as a service or an individual user, particularly when such models produce or are used to create and sell material bearing strong structural similarities to copyrighted images. Thus, there is a potential legal risk for both the providers of image generation models and their users, especially in commercial settings where the 'transformative' nature of generated images may not meet the legal threshold established in copyright law if they possess substantial structural similarity to the original image. \n\nObjective of our data generation pipeline. We aim to systematically generate prompts that are considered generic and not related to any copyrighted topic, but still capable of triggering the generation of copyrighted content from diffusion models. We formally define two desired properties our generated prompts should satisfy. Definition 1. (Prompt sensitivity) Given a semantic measurement f s (\u2022) and a tolerance \u03f5, prompt p is sensitive to a \n\nIn practice, f s can be a text encoder that encodes plain text to be text embeddings for comparison. We detail distance measure D[\u2022||\u2022] in the following discussion. According to the above definition, a prompt is considered to be sensitive to a topic if they have similar language semantics. Moreover, a prompt is adversarial if it can trigger a T2I model to generate copyrighted content. Hence, the objective of our data generation pipeline is to systematically create non-sensitive adversarial prompts.",
                    "score": 0.5559779813540305,
                    "section_title": "Problem Formulation",
                    "char_start_offset": 6237,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 45
                        },
                        {
                            "start": 46,
                            "end": 146
                        },
                        {
                            "start": 147,
                            "end": 231
                        },
                        {
                            "start": 232,
                            "end": 494
                        },
                        {
                            "start": 495,
                            "end": 605
                        },
                        {
                            "start": 606,
                            "end": 955
                        },
                        {
                            "start": 958,
                            "end": 1240
                        },
                        {
                            "start": 1241,
                            "end": 1568
                        },
                        {
                            "start": 1571,
                            "end": 1613
                        },
                        {
                            "start": 1614,
                            "end": 1818
                        },
                        {
                            "start": 1819,
                            "end": 1898
                        },
                        {
                            "start": 1899,
                            "end": 2016
                        },
                        {
                            "start": 2019,
                            "end": 2119
                        },
                        {
                            "start": 2120,
                            "end": 2183
                        },
                        {
                            "start": 2184,
                            "end": 2309
                        },
                        {
                            "start": 2310,
                            "end": 2406
                        },
                        {
                            "start": 2407,
                            "end": 2522
                        }
                    ],
                    "ref_mentions": [],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.9970703125
                },
                {
                    "corpus_id": "265352103",
                    "title": "On Copyright Risks of Text-to-Image Diffusion Models",
                    "text": "Diffusion models have gained widespread popularity as the new frontier of generative models. Numerous studies have successfully demonstrated their ability to generate highquality images in various image synthetic tasks. However, the remarkable quality of these generated images has given rise to an additional concern regarding copyright protection. Recent research has indicated that diffusion models often tend to memorize images in the training dataset [Carlini et al., 2023]. As a result, diffusion models can effortlessly generate copyrighted content through memorization [Somepalli et al., 2023a;Somepalli et al., 2023b]. The apprehension surrounding copyright protection in diffusion models has also evolved Figure 1: Generate copyrighted content in ChatGPT. ChatGPT refuses to generate images when directly prompted for copyrighted material. However, adversarial prompts generated with our method that do not directly ask for copyrighted material still manage to generate copyrighted material, in this case, the Superman logo. \n\ninto a tangible threat, as multiple lawsuits related to copyright infringement have been initiated against companies that utilize diffusion models for commercial purposes. Notably, there have been instances of lawsuits: Stability AI and Mid-Journey are both facing civil suits for training their models on artists' work without their consent thereby allowing their models to replicate the style and work of such artists [Vincent, 2023]. \n\nAttempts have been made to prevent the generation of copyrighted content, such as OpenAI's addition of filters on ChatGPT to prevent the generation of copyrighted images. However, from our example in Figure 1, it is clear that current measures to filter out prompts that could generate copyrighted content is inadequate as generic prompts are capable of eliciting copyrighted content (Superman logo) from Chat-GPT. Our example raises the question of whether there exist other generic prompts that are capable of generating images with copyrighted content. Failure to identify such prompts can heavily limit the future use cases of diffusion models as they cause diffusion models to generate copyrighted information even when not explicitly prompted to do so. \n\nOur contributions. (1) We form a framework to create prompts for T2I tasks that are generic in language semantics but can still trigger partial copyright infringements in image generation by various diffusion models.",
                    "score": 0.5716552090952953,
                    "section_title": "Introduction",
                    "char_start_offset": 15,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 92
                        },
                        {
                            "start": 93,
                            "end": 219
                        },
                        {
                            "start": 220,
                            "end": 349
                        },
                        {
                            "start": 350,
                            "end": 479
                        },
                        {
                            "start": 480,
                            "end": 627
                        },
                        {
                            "start": 628,
                            "end": 765
                        },
                        {
                            "start": 766,
                            "end": 849
                        },
                        {
                            "start": 850,
                            "end": 1034
                        },
                        {
                            "start": 1037,
                            "end": 1208
                        },
                        {
                            "start": 1209,
                            "end": 1473
                        },
                        {
                            "start": 1476,
                            "end": 1646
                        },
                        {
                            "start": 1647,
                            "end": 1890
                        },
                        {
                            "start": 1891,
                            "end": 2031
                        },
                        {
                            "start": 2032,
                            "end": 2234
                        },
                        {
                            "start": 2237,
                            "end": 2255
                        },
                        {
                            "start": 2256,
                            "end": 2453
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 577,
                            "end": 602,
                            "matchedPaperCorpusId": "254366634"
                        },
                        {
                            "start": 602,
                            "end": 626,
                            "matchedPaperCorpusId": "227209335"
                        },
                        {
                            "start": 1457,
                            "end": 1472,
                            "matchedPaperCorpusId": "266900037"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.99658203125
                },
                {
                    "corpus_id": "265352103",
                    "title": "On Copyright Risks of Text-to-Image Diffusion Models",
                    "text": "(1) We form a framework to create prompts for T2I tasks that are generic in language semantics but can still trigger partial copyright infringements in image generation by various diffusion models. (2) We introduce a copyright tester that employs attention maps to identify significant similarities, extending the analysis from entire image duplication to specific visual feature resemblances and pinpointing potential areas of interest for detailed examination. \n\n(3) We compile a dataset of potential copyrighted topics and prompts to aid in more realistic copyright research and to analyze diffusion model behaviors. We provide empirical results to show the copyright threat and to raise awareness in copyright research for generative models. A case study utilizing our dataset revealed a significant concern: recent diffusion models is prone to unintentionally produce images with copyrighted content from seemingly unrelated prompts. \n\n2 Background Diffusion models. Diffusion models are a class of generative models that model the diffusion process. In the diffusion process, source data is gradually distorted by adding noise to it until the source data becomes noise as well [Sohl-Dickstein et al., 2015]. The objective of diffusion models is to learn the reverse process of diffusion, which tries to reconstruct the target given noisy input. Diffusion models can either learn to directly predict the less noisy data at each reverse step, or learn to predict the noise at each step, then denoise the data using predicted noise [Ho et al., 2020;Saharia et al., 2022]. Earlier diffusion models work at image level and try to directly reconstruct images from noise. However, reverse steps at image level require intensive calculation and constraints the speed of the reconstruction. Instead, [Rombach et al., 2022] proposes to first transform the image into low dimensional hidden space, then apply diffusion models at hidden level. Subsequently, latent diffusion models are much faster than their counterparts working at image level, hence can be trained on large datasets such as LAION [Schuhmann et al., 2022]. Prediction of noise or previous states in the reverse process usually uses a U-Net [Ronneberger et al., 2015].",
                    "score": 0.4634600029415141,
                    "section_title": "Introduction",
                    "char_start_offset": 2271,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 197
                        },
                        {
                            "start": 198,
                            "end": 462
                        },
                        {
                            "start": 465,
                            "end": 619
                        },
                        {
                            "start": 620,
                            "end": 745
                        },
                        {
                            "start": 746,
                            "end": 938
                        },
                        {
                            "start": 941,
                            "end": 971
                        },
                        {
                            "start": 972,
                            "end": 1055
                        },
                        {
                            "start": 1056,
                            "end": 1213
                        },
                        {
                            "start": 1214,
                            "end": 1350
                        },
                        {
                            "start": 1351,
                            "end": 1574
                        },
                        {
                            "start": 1575,
                            "end": 1670
                        },
                        {
                            "start": 1671,
                            "end": 1787
                        },
                        {
                            "start": 1788,
                            "end": 1937
                        },
                        {
                            "start": 1938,
                            "end": 2118
                        },
                        {
                            "start": 2119,
                            "end": 2229
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 1183,
                            "end": 1212,
                            "matchedPaperCorpusId": "254366634"
                        },
                        {
                            "start": 1535,
                            "end": 1552,
                            "matchedPaperCorpusId": "257687839"
                        },
                        {
                            "start": 1552,
                            "end": 1573,
                            "matchedPaperCorpusId": "252917726"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.99169921875
                },
                {
                    "corpus_id": "265352103",
                    "title": "On Copyright Risks of Text-to-Image Diffusion Models",
                    "text": "This section evaluates our method for detecting partial copyright infringements by computing the cosine similarity of CLIP-embeddings between image chunks. An image chunk is a specific rectangular image region from the original image. We apply this measurement to compare chunks in generated images with annotated target image chunks, and for contrast, assess the similarity between randomly chosen chunks and our annotated ones. Additionally, we assess similarity in images generated from random prompts with our target annotations. Results, as shown in Table 2, reveal that our identified chunks exhibit a high cosine similarity (approximately 0.9) with target annotations, compared to lower similarities Baseline 2 58.4 58.9 57.9 57.8 57.9 58.9 \n\nTable 2: Cosine similarity (multiplied by 100) between CLIP embeddings of target annotations and chunks in generated images. Ours denotes image chunks identified with copyright test on images generated using our prompts. Baseline 1 denotes randomly selected chunks on images generated using our prompts. Baseline 2 denotes randomly selected chunks on images generated using random prompts. Identified image chunks show significant similarity with the annotated image parts. Figure 9: Proportion of generated images with identified copyrighted content. For all models apart from Stable Diffusion XL (SD XL), our copyright test identifies that around 70% of generated images from our pipeline have at least one chunk containing copyrighted content. The slight decrease in copyrighted content in SD XL is due to the model's increase in ability to comprehend the nuances in our non-sensitive prompts, thus slightly reducing the adversarial property. Overall, it is clear that our non-sensitive prompts can effectively cause diffusion models to infringe copyright as even more than half of the images generated by SD XL contain copyrighted content. \n\nof about 0.7 and 0.6 for random chunks in generated and random images, respectively.",
                    "score": 0.4662246655052916,
                    "section_title": "Evaluation on Copyright Test",
                    "char_start_offset": 26229,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 155
                        },
                        {
                            "start": 156,
                            "end": 234
                        },
                        {
                            "start": 235,
                            "end": 429
                        },
                        {
                            "start": 430,
                            "end": 533
                        },
                        {
                            "start": 534,
                            "end": 747
                        },
                        {
                            "start": 750,
                            "end": 874
                        },
                        {
                            "start": 875,
                            "end": 970
                        },
                        {
                            "start": 971,
                            "end": 1053
                        },
                        {
                            "start": 1054,
                            "end": 1139
                        },
                        {
                            "start": 1140,
                            "end": 1223
                        },
                        {
                            "start": 1224,
                            "end": 1301
                        },
                        {
                            "start": 1302,
                            "end": 1496
                        },
                        {
                            "start": 1497,
                            "end": 1695
                        },
                        {
                            "start": 1696,
                            "end": 1893
                        },
                        {
                            "start": 1896,
                            "end": 1980
                        }
                    ],
                    "ref_mentions": [],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.99072265625
                },
                {
                    "corpus_id": "265352103",
                    "title": "On Copyright Risks of Text-to-Image Diffusion Models",
                    "text": "Diffusion models are undergoing a rapid development phase, and recently proposed diffusion models have shown improved performance in generating high-quality images. However, whether their performance has improved for copyright protection remains questionable. In this section, we investigate diffusion models in this regard. \n\nOur case study examines how a diverse pool of diffusion models react to varying subjects. For this purpose, we leverage the experiment setup in the previous section. Specifically, we obtain proportions of images with identified copyrighted content for each topic and compare them across different diffusion models (Figure 10, more in Appendix D). While the performance of each model varies for each topic, it is clear that all models exhibit copyright-infringing behavior for the five distinct topics. Even though the latest model SD XL shows a slight decrease in generating copyrighted content, the rate generating copyrighted content remains above 50% for many topics. This suggests that the current approach to training diffusion models remains ineffective in preventing the occurrence of copyright infringement.",
                    "score": 0.4303665990213408,
                    "section_title": "Case Study: Copyright Issue Across Diffusion Models",
                    "char_start_offset": 28942,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 164
                        },
                        {
                            "start": 165,
                            "end": 259
                        },
                        {
                            "start": 260,
                            "end": 324
                        },
                        {
                            "start": 327,
                            "end": 416
                        },
                        {
                            "start": 417,
                            "end": 492
                        },
                        {
                            "start": 493,
                            "end": 673
                        },
                        {
                            "start": 674,
                            "end": 828
                        },
                        {
                            "start": 829,
                            "end": 997
                        },
                        {
                            "start": 998,
                            "end": 1142
                        }
                    ],
                    "ref_mentions": [],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.984375
                },
                {
                    "corpus_id": "265352103",
                    "title": "On Copyright Risks of Text-to-Image Diffusion Models",
                    "text": "Hence, the objective of our data generation pipeline is to systematically create non-sensitive adversarial prompts. We discuss our proposed data generation pipeline in Section 4. \n\nCopyright test for substantial similarities. According to the legal definition of copyright infringement, copyright violations can appear across a broader spectrum, in which generated images are not entirely replicated from any training source, yet they possess substantial similarities that replicate or bear significant resemblance to copyrighted content. We refer to these copyright violations as partial copyright violations. In Definition 2, we apply D[\u2022||\u2022] to measure the substantial similarities between generated images and copyrighted images. We propose an implementation of D[\u2022||\u2022] as a copyright tester in Section 5.",
                    "score": 0.4644012753453807,
                    "section_title": "Problem Formulation",
                    "char_start_offset": 8644,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 115
                        },
                        {
                            "start": 116,
                            "end": 178
                        },
                        {
                            "start": 181,
                            "end": 225
                        },
                        {
                            "start": 226,
                            "end": 538
                        },
                        {
                            "start": 539,
                            "end": 610
                        },
                        {
                            "start": 611,
                            "end": 733
                        },
                        {
                            "start": 734,
                            "end": 809
                        }
                    ],
                    "ref_mentions": [],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.98193359375
                },
                {
                    "corpus_id": "265352103",
                    "title": "On Copyright Risks of Text-to-Image Diffusion Models",
                    "text": "In this work, we propose a data generation pipeline to generate realistic copyright-infringing examples on diffusion models. Our proposed pipeline generated prompts that are seemingly unrelated to the target copyrighted topic, but can still be utilized to produce copyrighted content. Additionally, our pipeline tackles partial copyright infringement. Through our proposal, we present a toolkit that includes potentially copyrighted topics, target images of copyright topics with annotations of copyrighted content, and a dataset generation pipeline. The toolkit can be used as a whole on diffusion models for testing copyright-related performance and generating copyright-violating samples. We demonstrate with this toolkit that contemporary diffusion models are highly susceptible to generating copyrighted content. The findings emphasize the immediate necessity for appropriate measures to prevent models from generating copyrighted materials. This is especially crucial since our study shows even common phrases can prompt models to create images containing copyrighted content. This work can serve copyright research for diffusion models. For instance, the research community can leverage this toolkit to assess diffusion models based on copyright-related criteria. Furthermore, copyright protection algorithms can employ it for effectiveness evaluation. \n\nAll Query Templates for Prompt Generation Form N phrases using all of the exact words in the exact order: KEYWORD0, KEYWORD1, . . . , KEYWORDN. The phrases should be similar to CATEGORY TOPIC. Form N sentences that start with the phrase START-PHRASE. Do not make reference to the CATEGORY TOPIC. Use words that are challenging to represent visually. Form N sentences that uses all of the exact words in the exact order: KEYWORD0, KEYWORD1, . . . , KEY-WORDN. Use the words continuously wherever possible. Ensure grammatical correctness. Start the sentence with the exact STARTPHRASE. Do not make reference to the CATEGORY TOPIC. Use words that are challenging to represent visually. \n\nTable 3: The list of all templates used for prompt generation on GPT3.5. Capitalized words are variables that can be changed. We query GPT models to generate candidate prompts that contain extracted keywords but have different semantic meanings from our target topics. We employ different queries to ensure diversity in the prompt generation.",
                    "score": 0.4211661459107904,
                    "section_title": "Conclusion",
                    "char_start_offset": 30099,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 124
                        },
                        {
                            "start": 125,
                            "end": 284
                        },
                        {
                            "start": 285,
                            "end": 351
                        },
                        {
                            "start": 352,
                            "end": 550
                        },
                        {
                            "start": 551,
                            "end": 691
                        },
                        {
                            "start": 692,
                            "end": 817
                        },
                        {
                            "start": 818,
                            "end": 946
                        },
                        {
                            "start": 947,
                            "end": 1082
                        },
                        {
                            "start": 1083,
                            "end": 1143
                        },
                        {
                            "start": 1144,
                            "end": 1270
                        },
                        {
                            "start": 1271,
                            "end": 1359
                        },
                        {
                            "start": 1362,
                            "end": 1493
                        },
                        {
                            "start": 1494,
                            "end": 1505
                        },
                        {
                            "start": 1506,
                            "end": 1554
                        },
                        {
                            "start": 1555,
                            "end": 1612
                        },
                        {
                            "start": 1613,
                            "end": 1657
                        },
                        {
                            "start": 1658,
                            "end": 1711
                        },
                        {
                            "start": 1712,
                            "end": 1807
                        },
                        {
                            "start": 1808,
                            "end": 1820
                        },
                        {
                            "start": 1821,
                            "end": 1866
                        },
                        {
                            "start": 1867,
                            "end": 1898
                        },
                        {
                            "start": 1899,
                            "end": 1945
                        },
                        {
                            "start": 1946,
                            "end": 1990
                        },
                        {
                            "start": 1991,
                            "end": 2044
                        },
                        {
                            "start": 2047,
                            "end": 2119
                        },
                        {
                            "start": 2120,
                            "end": 2172
                        },
                        {
                            "start": 2173,
                            "end": 2315
                        },
                        {
                            "start": 2316,
                            "end": 2389
                        }
                    ],
                    "ref_mentions": [],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.98095703125
                }
            ],
            "relevance_judgement": 0.9970703125,
            "relevance_judgment_input_expanded": "# Title: On Copyright Risks of Text-to-Image Diffusion Models\n# Venue: \n# Authors: Yang Zhang, Teoh Tze Tzun, Lim Wei Hern, Haonan Wang, Kenji Kawaguchi\n## Abstract\nDiffusion models excel in many generative modeling tasks, notably in creating images from text prompts, a task referred to as text-to-image (T2I) generation. Despite the ability to generate high-quality images, these models often replicate elements from their training data, leading to increasing copyright concerns in real applications in recent years. In response to this raising concern about copyright infringement, recent studies have studied the copyright behavior of diffusion models when using direct, copyrighted prompts. Our research extends this by examining subtler forms of infringement, where even indirect prompts can trigger copyright issues. Specifically, we introduce a data generation pipeline to systematically produce data for studying copyright in diffusion models. Our pipeline enables us to investigate copyright infringement in a more practical setting, involving replicating visual features rather than entire works using seemingly irrelevant prompts for T2I generation. We generate data using our proposed pipeline to test various diffusion models, including the latest Stable Diffusion XL. Our findings reveal a widespread tendency that these models tend to produce copyright-infringing content, highlighting a significant challenge in this field.\n## Introduction\nDiffusion models have gained widespread popularity as the new frontier of generative models. Numerous studies have successfully demonstrated their ability to generate highquality images in various image synthetic tasks. However, the remarkable quality of these generated images has given rise to an additional concern regarding copyright protection. Recent research has indicated that diffusion models often tend to memorize images in the training dataset [Carlini et al., 2023]. As a result, diffusion models can effortlessly generate copyrighted content through memorization [Somepalli et al., 2023a;Somepalli et al., 2023b]. The apprehension surrounding copyright protection in diffusion models has also evolved Figure 1: Generate copyrighted content in ChatGPT. ChatGPT refuses to generate images when directly prompted for copyrighted material. However, adversarial prompts generated with our method that do not directly ask for copyrighted material still manage to generate copyrighted material, in this case, the Superman logo. \n\ninto a tangible threat, as multiple lawsuits related to copyright infringement have been initiated against companies that utilize diffusion models for commercial purposes. Notably, there have been instances of lawsuits: Stability AI and Mid-Journey are both facing civil suits for training their models on artists' work without their consent thereby allowing their models to replicate the style and work of such artists [Vincent, 2023]. \n\nAttempts have been made to prevent the generation of copyrighted content, such as OpenAI's addition of filters on ChatGPT to prevent the generation of copyrighted images. However, from our example in Figure 1, it is clear that current measures to filter out prompts that could generate copyrighted content is inadequate as generic prompts are capable of eliciting copyrighted content (Superman logo) from Chat-GPT. Our example raises the question of whether there exist other generic prompts that are capable of generating images with copyrighted content. Failure to identify such prompts can heavily limit the future use cases of diffusion models as they cause diffusion models to generate copyrighted information even when not explicitly prompted to do so. \n\nOur contributions. (1) We form a framework to create prompts for T2I tasks that are generic in language semantics but can still trigger partial copyright infringements in image generation by various diffusion models.\n...\n(1) We form a framework to create prompts for T2I tasks that are generic in language semantics but can still trigger partial copyright infringements in image generation by various diffusion models. (2) We introduce a copyright tester that employs attention maps to identify significant similarities, extending the analysis from entire image duplication to specific visual feature resemblances and pinpointing potential areas of interest for detailed examination. \n\n(3) We compile a dataset of potential copyrighted topics and prompts to aid in more realistic copyright research and to analyze diffusion model behaviors. We provide empirical results to show the copyright threat and to raise awareness in copyright research for generative models. A case study utilizing our dataset revealed a significant concern: recent diffusion models is prone to unintentionally produce images with copyrighted content from seemingly unrelated prompts. \n\n2 Background Diffusion models. Diffusion models are a class of generative models that model the diffusion process. In the diffusion process, source data is gradually distorted by adding noise to it until the source data becomes noise as well [Sohl-Dickstein et al., 2015]. The objective of diffusion models is to learn the reverse process of diffusion, which tries to reconstruct the target given noisy input. Diffusion models can either learn to directly predict the less noisy data at each reverse step, or learn to predict the noise at each step, then denoise the data using predicted noise [Ho et al., 2020;Saharia et al., 2022]. Earlier diffusion models work at image level and try to directly reconstruct images from noise. However, reverse steps at image level require intensive calculation and constraints the speed of the reconstruction. Instead, [Rombach et al., 2022] proposes to first transform the image into low dimensional hidden space, then apply diffusion models at hidden level. Subsequently, latent diffusion models are much faster than their counterparts working at image level, hence can be trained on large datasets such as LAION [Schuhmann et al., 2022]. Prediction of noise or previous states in the reverse process usually uses a U-Net [Ronneberger et al., 2015].\n\n## Problem Formulation\nCopyright infringement for generative models. For copyright infringement, we focus on the copyright regulation in the US [Copyright Office, 2022]. Nevertheless, the concept of copyright applies to the regulation in other countries. In the context of copyright law in the US, whether a piece of copyrighted material can be used by others is governed by the concept of Fair Use which permits the use of copyrighted material only in a transformative manner that is distinct from the original work. There have been legal precedents which demonstrate that structural similarity can lead to infringement claims. Given that such generative models are trained on datasets such as LAION-5B [Schuhmann et al., 2022] which contains publicly available copyrighted data, if the models generate images with visual features that have substantial structural similarity to the original copyrighted images, this would likely be grounds for claims of copyright infringement. \n\nThis has implications in commercial settings such as companies selling proprietary image generation models as a service or an individual user, particularly when such models produce or are used to create and sell material bearing strong structural similarities to copyrighted images. Thus, there is a potential legal risk for both the providers of image generation models and their users, especially in commercial settings where the 'transformative' nature of generated images may not meet the legal threshold established in copyright law if they possess substantial structural similarity to the original image. \n\nObjective of our data generation pipeline. We aim to systematically generate prompts that are considered generic and not related to any copyrighted topic, but still capable of triggering the generation of copyrighted content from diffusion models. We formally define two desired properties our generated prompts should satisfy. Definition 1. (Prompt sensitivity) Given a semantic measurement f s (\u2022) and a tolerance \u03f5, prompt p is sensitive to a \n\nIn practice, f s can be a text encoder that encodes plain text to be text embeddings for comparison. We detail distance measure D[\u2022||\u2022] in the following discussion. According to the above definition, a prompt is considered to be sensitive to a topic if they have similar language semantics. Moreover, a prompt is adversarial if it can trigger a T2I model to generate copyrighted content. Hence, the objective of our data generation pipeline is to systematically create non-sensitive adversarial prompts.\n...\nHence, the objective of our data generation pipeline is to systematically create non-sensitive adversarial prompts. We discuss our proposed data generation pipeline in Section 4. \n\nCopyright test for substantial similarities. According to the legal definition of copyright infringement, copyright violations can appear across a broader spectrum, in which generated images are not entirely replicated from any training source, yet they possess substantial similarities that replicate or bear significant resemblance to copyrighted content. We refer to these copyright violations as partial copyright violations. In Definition 2, we apply D[\u2022||\u2022] to measure the substantial similarities between generated images and copyrighted images. We propose an implementation of D[\u2022||\u2022] as a copyright tester in Section 5.\n\n## Evaluation on Copyright Test\nThis section evaluates our method for detecting partial copyright infringements by computing the cosine similarity of CLIP-embeddings between image chunks. An image chunk is a specific rectangular image region from the original image. We apply this measurement to compare chunks in generated images with annotated target image chunks, and for contrast, assess the similarity between randomly chosen chunks and our annotated ones. Additionally, we assess similarity in images generated from random prompts with our target annotations. Results, as shown in Table 2, reveal that our identified chunks exhibit a high cosine similarity (approximately 0.9) with target annotations, compared to lower similarities Baseline 2 58.4 58.9 57.9 57.8 57.9 58.9 \n\nTable 2: Cosine similarity (multiplied by 100) between CLIP embeddings of target annotations and chunks in generated images. Ours denotes image chunks identified with copyright test on images generated using our prompts. Baseline 1 denotes randomly selected chunks on images generated using our prompts. Baseline 2 denotes randomly selected chunks on images generated using random prompts. Identified image chunks show significant similarity with the annotated image parts. Figure 9: Proportion of generated images with identified copyrighted content. For all models apart from Stable Diffusion XL (SD XL), our copyright test identifies that around 70% of generated images from our pipeline have at least one chunk containing copyrighted content. The slight decrease in copyrighted content in SD XL is due to the model's increase in ability to comprehend the nuances in our non-sensitive prompts, thus slightly reducing the adversarial property. Overall, it is clear that our non-sensitive prompts can effectively cause diffusion models to infringe copyright as even more than half of the images generated by SD XL contain copyrighted content. \n\nof about 0.7 and 0.6 for random chunks in generated and random images, respectively.\n\n## Case Study: Copyright Issue Across Diffusion Models\nDiffusion models are undergoing a rapid development phase, and recently proposed diffusion models have shown improved performance in generating high-quality images. However, whether their performance has improved for copyright protection remains questionable. In this section, we investigate diffusion models in this regard. \n\nOur case study examines how a diverse pool of diffusion models react to varying subjects. For this purpose, we leverage the experiment setup in the previous section. Specifically, we obtain proportions of images with identified copyrighted content for each topic and compare them across different diffusion models (Figure 10, more in Appendix D). While the performance of each model varies for each topic, it is clear that all models exhibit copyright-infringing behavior for the five distinct topics. Even though the latest model SD XL shows a slight decrease in generating copyrighted content, the rate generating copyrighted content remains above 50% for many topics. This suggests that the current approach to training diffusion models remains ineffective in preventing the occurrence of copyright infringement.\n\n## Conclusion\nIn this work, we propose a data generation pipeline to generate realistic copyright-infringing examples on diffusion models. Our proposed pipeline generated prompts that are seemingly unrelated to the target copyrighted topic, but can still be utilized to produce copyrighted content. Additionally, our pipeline tackles partial copyright infringement. Through our proposal, we present a toolkit that includes potentially copyrighted topics, target images of copyright topics with annotations of copyrighted content, and a dataset generation pipeline. The toolkit can be used as a whole on diffusion models for testing copyright-related performance and generating copyright-violating samples. We demonstrate with this toolkit that contemporary diffusion models are highly susceptible to generating copyrighted content. The findings emphasize the immediate necessity for appropriate measures to prevent models from generating copyrighted materials. This is especially crucial since our study shows even common phrases can prompt models to create images containing copyrighted content. This work can serve copyright research for diffusion models. For instance, the research community can leverage this toolkit to assess diffusion models based on copyright-related criteria. Furthermore, copyright protection algorithms can employ it for effectiveness evaluation. \n\nAll Query Templates for Prompt Generation Form N phrases using all of the exact words in the exact order: KEYWORD0, KEYWORD1, . . . , KEYWORDN. The phrases should be similar to CATEGORY TOPIC. Form N sentences that start with the phrase START-PHRASE. Do not make reference to the CATEGORY TOPIC. Use words that are challenging to represent visually. Form N sentences that uses all of the exact words in the exact order: KEYWORD0, KEYWORD1, . . . , KEY-WORDN. Use the words continuously wherever possible. Ensure grammatical correctness. Start the sentence with the exact STARTPHRASE. Do not make reference to the CATEGORY TOPIC. Use words that are challenging to represent visually. \n\nTable 3: The list of all templates used for prompt generation on GPT3.5. Capitalized words are variables that can be changed. We query GPT models to generate candidate prompts that contain extracted keywords but have different semantic meanings from our target topics. We employ different queries to ensure diversity in the prompt generation.",
            "reference_string": "[265352103 | Zhang et al. | 2023 | Citations: 10]"
        },
        {
            "title": "Guardians of Generation: Dynamic Inference-Time Copyright Shielding with Adaptive Guidance for AI Image Generation",
            "venue": "arXiv.org",
            "year": 2025,
            "reference_count": 0,
            "citation_count": 0,
            "influential_citation_count": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2503.16171, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2351229051",
                    "name": "Soham Roy"
                },
                {
                    "authorId": "2351593773",
                    "name": "Abhishek Mishra"
                },
                {
                    "authorId": "40151143",
                    "name": "S. Karande"
                },
                {
                    "authorId": "2316561345",
                    "name": "Murari Mandal"
                }
            ],
            "abstract": "Modern text-to-image generative models can inadvertently reproduce copyrighted content memorized in their training data, raising serious concerns about potential copyright infringement. We introduce Guardians of Generation, a model agnostic inference time framework for dynamic copyright shielding in AI image generation. Our approach requires no retraining or modification of the generative model weights, instead integrating seamlessly with existing diffusion pipelines. It augments the generation process with an adaptive guidance mechanism comprising three components: a detection module, a prompt rewriting module, and a guidance adjustment module. The detection module monitors user prompts and intermediate generation steps to identify features indicative of copyrighted content before they manifest in the final output. If such content is detected, the prompt rewriting mechanism dynamically transforms the user's prompt by sanitizing or replacing references that could trigger copyrighted material while preserving the prompt's intended semantics. The adaptive guidance module adaptively steers the diffusion process away from flagged content by modulating the model's sampling trajectory. Together, these components form a robust shield that enables a tunable balance between preserving creative fidelity and ensuring copyright compliance. We validate our method on a variety of generative models such as Stable Diffusion, SDXL, and Flux, demonstrating substantial reductions in copyrighted content generation with negligible impact on output fidelity or alignment with user intent. This work provides a practical, plug-and-play safeguard for generative image models, enabling more responsible deployment under real-world copyright constraints. Source code is available at: https://respailab.github.io/gog",
            "corpus_id": 277151077,
            "sentences": [
                {
                    "corpus_id": "277151077",
                    "title": "Guardians of Generation: Dynamic Inference-Time Copyright Shielding with Adaptive Guidance for AI Image Generation",
                    "text": "Modern text-to-image generative models can inadvertently reproduce copyrighted content memorized in their training data, raising serious concerns about potential copyright infringement. We introduce Guardians of Generation, a model agnostic inference time framework for dynamic copyright shielding in AI image generation. Our approach requires no retraining or modification of the generative model weights, instead integrating seamlessly with existing diffusion pipelines. It augments the generation process with an adaptive guidance mechanism comprising three components: a detection module, a prompt rewriting module, and a guidance adjustment module. The detection module monitors user prompts and intermediate generation steps to identify features indicative of copyrighted content before they manifest in the final output. If such content is detected, the prompt rewriting mechanism dynamically transforms the user's prompt by sanitizing or replacing references that could trigger copyrighted material while preserving the prompt's intended semantics. The adaptive guidance module adaptively steers the diffusion process away from flagged content by modulating the model's sampling trajectory. Together, these components form a robust shield that enables a tunable balance between preserving creative fidelity and ensuring copyright compliance. We validate our method on a variety of generative models such as Stable Diffusion, SDXL, and Flux, demonstrating substantial reductions in copyrighted content generation with negligible impact on output fidelity or alignment with user intent. This work provides a practical, plug-and-play safeguard for generative image models, enabling more responsible deployment under real-world copyright constraints. Source code is available at: https://respailab.github.io/gog",
                    "score": 0.5415004116085296,
                    "section_title": "abstract",
                    "char_start_offset": 0,
                    "sentence_offsets": [],
                    "ref_mentions": [],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.9951171875
                }
            ],
            "relevance_judgement": 0.9951171875,
            "relevance_judgment_input_expanded": "# Title: Guardians of Generation: Dynamic Inference-Time Copyright Shielding with Adaptive Guidance for AI Image Generation\n# Venue: arXiv.org\n# Authors: Soham Roy, Abhishek Mishra, S. Karande, Murari Mandal\n## Abstract\nModern text-to-image generative models can inadvertently reproduce copyrighted content memorized in their training data, raising serious concerns about potential copyright infringement. We introduce Guardians of Generation, a model agnostic inference time framework for dynamic copyright shielding in AI image generation. Our approach requires no retraining or modification of the generative model weights, instead integrating seamlessly with existing diffusion pipelines. It augments the generation process with an adaptive guidance mechanism comprising three components: a detection module, a prompt rewriting module, and a guidance adjustment module. The detection module monitors user prompts and intermediate generation steps to identify features indicative of copyrighted content before they manifest in the final output. If such content is detected, the prompt rewriting mechanism dynamically transforms the user's prompt by sanitizing or replacing references that could trigger copyrighted material while preserving the prompt's intended semantics. The adaptive guidance module adaptively steers the diffusion process away from flagged content by modulating the model's sampling trajectory. Together, these components form a robust shield that enables a tunable balance between preserving creative fidelity and ensuring copyright compliance. We validate our method on a variety of generative models such as Stable Diffusion, SDXL, and Flux, demonstrating substantial reductions in copyrighted content generation with negligible impact on output fidelity or alignment with user intent. This work provides a practical, plug-and-play safeguard for generative image models, enabling more responsible deployment under real-world copyright constraints. Source code is available at: https://respailab.github.io/gog\n",
            "reference_string": "[277151077 | Roy et al. | 2025 | Citations: 0]"
        },
        {
            "title": "TWIG: Two-Step Image Generation using Segmentation Masks in Diffusion Models",
            "venue": "arXiv.org",
            "year": 2025,
            "reference_count": 24,
            "citation_count": 1,
            "influential_citation_count": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2504.14933, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2313107575",
                    "name": "Mazharul Islam Rakib"
                },
                {
                    "authorId": "2357491645",
                    "name": "Showrin Rahman"
                },
                {
                    "authorId": "2113939573",
                    "name": "J. Mondal"
                },
                {
                    "authorId": "2349232014",
                    "name": "Xi Xiao"
                },
                {
                    "authorId": "2357414290",
                    "name": "David Lewis"
                },
                {
                    "authorId": "2356548661",
                    "name": "Alessandra Mileo"
                },
                {
                    "authorId": "2210516789",
                    "name": "Meem Arafat Manab"
                }
            ],
            "abstract": "In today's age of social media and marketing, copyright issues can be a major roadblock to the free sharing of images. Generative AI models have made it possible to create high-quality images, but concerns about copyright infringement are a hindrance to their abundant use. As these models use data from training images to generate new ones, it is often a daunting task to ensure they do not violate intellectual property rights. Some AI models have even been noted to directly copy copyrighted images, a problem often referred to as source copying. Traditional copyright protection measures such as watermarks and metadata have also proven to be futile in this regard. To address this issue, we propose a novel two-step image generation model inspired by the conditional diffusion model. The first step involves creating an image segmentation mask for some prompt-based generated images. This mask embodies the shape of the image. Thereafter, the diffusion model is asked to generate the image anew while avoiding the shape in question. This approach shows a decrease in structural similarity from the training image, i.e. we are able to avoid the source copying problem using this approach without expensive retraining of the model or user-centered prompt generation techniques. This makes our approach the most computationally inexpensive approach to avoiding both copyright infringement and source copying for diffusion model-based image generation.",
            "corpus_id": 277955485,
            "sentences": [
                {
                    "corpus_id": "277955485",
                    "title": "TWIG: Two-Step Image Generation using Segmentation Masks in Diffusion Models",
                    "text": "In today's age of social media and marketing, copyright issues can be a major roadblock to the free sharing of images. Generative AI models have made it possible to create high-quality images, but concerns about copyright infringement are a hindrance to their abundant use. As these models use data from training images to generate new ones, it is often a daunting task to ensure they do not violate intellectual property rights. Some AI models have even been noted to directly copy copyrighted images, a problem often referred to as source copying. Traditional copyright protection measures such as watermarks and metadata have also proven to be futile in this regard. To address this issue, we propose a novel two-step image generation model inspired by the conditional diffusion model. The first step involves creating an image segmentation mask for some prompt-based generated images. This mask embodies the shape of the image. Thereafter, the diffusion model is asked to generate the image anew while avoiding the shape in question. This approach shows a decrease in structural similarity from the training image, i.e. we are able to avoid the source copying problem using this approach without expensive retraining of the model or user-centered prompt generation techniques. This makes our approach the most computationally inexpensive approach to avoiding both copyright infringement and source copying for diffusion model-based image generation.",
                    "score": 0.6186750455484166,
                    "section_title": "abstract",
                    "char_start_offset": 0,
                    "sentence_offsets": [],
                    "ref_mentions": [],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.994140625
                },
                {
                    "corpus_id": "277955485",
                    "title": "TWIG: Two-Step Image Generation using Segmentation Masks in Diffusion Models",
                    "text": "Diffusion models have revolutionized image generation, surpassing predecessors like GANs and VAEs in both fidelity and performance. However, this progress has been shadowed by a critical challenge: source copying. This issue raises serious concerns regarding privacy and intellectual property rights, demanding innovative solutions that preserve content quality while enhancing copyright protection. Our research introduces a novel two-step image generation model designed to specifically address these concerns. This model operates by first generating segmentation masks from a given text prompt. These masks are then used to guide a Stable Diffusion model, effectively minimizing source copying while maintaining high fidelity in the final generated image. This approach also successfully circumvents the need for textual embedding, further streamlining the process and reducing potential avenues for copyright infringement. By decoupling the image generation process in this manner, we offer a promising pathway towards responsible and ethical AI image generation that respects both creative expression and copyright protection.",
                    "score": 0.49953797926316135,
                    "section_title": "Conclusion",
                    "char_start_offset": 26105,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 131
                        },
                        {
                            "start": 132,
                            "end": 213
                        },
                        {
                            "start": 214,
                            "end": 399
                        },
                        {
                            "start": 400,
                            "end": 512
                        },
                        {
                            "start": 513,
                            "end": 597
                        },
                        {
                            "start": 598,
                            "end": 758
                        },
                        {
                            "start": 759,
                            "end": 926
                        },
                        {
                            "start": 927,
                            "end": 1131
                        }
                    ],
                    "ref_mentions": [],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.99267578125
                }
            ],
            "relevance_judgement": 0.994140625,
            "relevance_judgment_input_expanded": "# Title: TWIG: Two-Step Image Generation using Segmentation Masks in Diffusion Models\n# Venue: arXiv.org\n# Authors: Mazharul Islam Rakib, Showrin Rahman, J. Mondal, Xi Xiao, David Lewis, Alessandra Mileo, Meem Arafat Manab\n## Abstract\nIn today's age of social media and marketing, copyright issues can be a major roadblock to the free sharing of images. Generative AI models have made it possible to create high-quality images, but concerns about copyright infringement are a hindrance to their abundant use. As these models use data from training images to generate new ones, it is often a daunting task to ensure they do not violate intellectual property rights. Some AI models have even been noted to directly copy copyrighted images, a problem often referred to as source copying. Traditional copyright protection measures such as watermarks and metadata have also proven to be futile in this regard. To address this issue, we propose a novel two-step image generation model inspired by the conditional diffusion model. The first step involves creating an image segmentation mask for some prompt-based generated images. This mask embodies the shape of the image. Thereafter, the diffusion model is asked to generate the image anew while avoiding the shape in question. This approach shows a decrease in structural similarity from the training image, i.e. we are able to avoid the source copying problem using this approach without expensive retraining of the model or user-centered prompt generation techniques. This makes our approach the most computationally inexpensive approach to avoiding both copyright infringement and source copying for diffusion model-based image generation.\n## Conclusion\nDiffusion models have revolutionized image generation, surpassing predecessors like GANs and VAEs in both fidelity and performance. However, this progress has been shadowed by a critical challenge: source copying. This issue raises serious concerns regarding privacy and intellectual property rights, demanding innovative solutions that preserve content quality while enhancing copyright protection. Our research introduces a novel two-step image generation model designed to specifically address these concerns. This model operates by first generating segmentation masks from a given text prompt. These masks are then used to guide a Stable Diffusion model, effectively minimizing source copying while maintaining high fidelity in the final generated image. This approach also successfully circumvents the need for textual embedding, further streamlining the process and reducing potential avenues for copyright infringement. By decoupling the image generation process in this manner, we offer a promising pathway towards responsible and ethical AI image generation that respects both creative expression and copyright protection.",
            "reference_string": "[277955485 | Rakib et al. | 2025 | Citations: 1]"
        },
        {
            "title": "Fantastic Copyrighted Beasts and How (Not) to Generate Them",
            "venue": "International Conference on Learning Representations",
            "year": 2024,
            "reference_count": 53,
            "citation_count": 12,
            "influential_citation_count": 1,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2406.14526, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2294507804",
                    "name": "Luxi He"
                },
                {
                    "authorId": "2283305597",
                    "name": "Yangsibo Huang"
                },
                {
                    "authorId": "2304129935",
                    "name": "Weijia Shi"
                },
                {
                    "authorId": "2144071564",
                    "name": "Tinghao Xie"
                },
                {
                    "authorId": "2308072184",
                    "name": "Haotian Liu"
                },
                {
                    "authorId": "2307621989",
                    "name": "Yue Wang"
                },
                {
                    "authorId": "2137813791",
                    "name": "Luke S. Zettlemoyer"
                },
                {
                    "authorId": "2309481623",
                    "name": "Chiyuan Zhang"
                },
                {
                    "authorId": "50536468",
                    "name": "Danqi Chen"
                },
                {
                    "authorId": "2254262712",
                    "name": "Peter Henderson"
                }
            ],
            "abstract": "Recent studies show that image and video generation models can be prompted to reproduce copyrighted content from their training data, raising serious legal concerns about copyright infringement. Copyrighted characters (e.g., Mario, Batman) present a significant challenge: at least one lawsuit has already awarded damages based on the generation of such characters. Consequently, commercial services like DALL-E have started deploying interventions. However, little research has systematically examined these problems: (1) Can users easily prompt models to generate copyrighted characters, even if it is unintentional?; (2) How effective are the existing mitigation strategies? To address these questions, we introduce a novel evaluation framework with metrics that assess both the generated image's similarity to copyrighted characters and its consistency with user intent, grounded in a set of popular copyrighted characters from diverse studios and regions. We show that state-of-the-art image and video generation models can still generate characters even if characters' names are not explicitly mentioned, sometimes with only two generic keywords (e.g., prompting with\"videogame, plumber\"consistently generates Nintendo's Mario character). We also introduce semi-automatic techniques to identify such keywords or descriptions that trigger character generation. Using this framework, we evaluate mitigation strategies, including prompt rewriting and new approaches we propose. Our findings reveal that common methods, such as DALL-E's prompt rewriting, are insufficient alone and require supplementary strategies like negative prompting. Our work provides empirical grounding for discussions on copyright mitigation strategies and offers actionable insights for model deployers implementing these safeguards.",
            "corpus_id": 270620122,
            "sentences": [
                {
                    "corpus_id": "270620122",
                    "title": "Fantastic Copyrighted Beasts and How (Not) to Generate Them",
                    "text": "Recent studies show that image and video generation models can be prompted to reproduce copyrighted content from their training data, raising serious legal concerns about copyright infringement. Copyrighted characters (e.g., Mario, Batman) present a significant challenge: at least one lawsuit has already awarded damages based on the generation of such characters. Consequently, commercial services like DALL-E have started deploying interventions. However, little research has systematically examined these problems: (1) Can users easily prompt models to generate copyrighted characters, even if it is unintentional?; (2) How effective are the existing mitigation strategies? To address these questions, we introduce a novel evaluation framework with metrics that assess both the generated image's similarity to copyrighted characters and its consistency with user intent, grounded in a set of popular copyrighted characters from diverse studios and regions. We show that state-of-the-art image and video generation models can still generate characters even if characters' names are not explicitly mentioned, sometimes with only two generic keywords (e.g., prompting with\"videogame, plumber\"consistently generates Nintendo's Mario character). We also introduce semi-automatic techniques to identify such keywords or descriptions that trigger character generation. Using this framework, we evaluate mitigation strategies, including prompt rewriting and new approaches we propose. Our findings reveal that common methods, such as DALL-E's prompt rewriting, are insufficient alone and require supplementary strategies like negative prompting. Our work provides empirical grounding for discussions on copyright mitigation strategies and offers actionable insights for model deployers implementing these safeguards.",
                    "score": 0.5259821304162764,
                    "section_title": "abstract",
                    "char_start_offset": 0,
                    "sentence_offsets": [],
                    "ref_mentions": [],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.99267578125
                }
            ],
            "relevance_judgement": 0.99267578125,
            "relevance_judgment_input_expanded": "# Title: Fantastic Copyrighted Beasts and How (Not) to Generate Them\n# Venue: International Conference on Learning Representations\n# Authors: Luxi He, Yangsibo Huang, Weijia Shi, Tinghao Xie, Haotian Liu, Yue Wang, Luke S. Zettlemoyer, Chiyuan Zhang, Danqi Chen, Peter Henderson\n## Abstract\nRecent studies show that image and video generation models can be prompted to reproduce copyrighted content from their training data, raising serious legal concerns about copyright infringement. Copyrighted characters (e.g., Mario, Batman) present a significant challenge: at least one lawsuit has already awarded damages based on the generation of such characters. Consequently, commercial services like DALL-E have started deploying interventions. However, little research has systematically examined these problems: (1) Can users easily prompt models to generate copyrighted characters, even if it is unintentional?; (2) How effective are the existing mitigation strategies? To address these questions, we introduce a novel evaluation framework with metrics that assess both the generated image's similarity to copyrighted characters and its consistency with user intent, grounded in a set of popular copyrighted characters from diverse studios and regions. We show that state-of-the-art image and video generation models can still generate characters even if characters' names are not explicitly mentioned, sometimes with only two generic keywords (e.g., prompting with\"videogame, plumber\"consistently generates Nintendo's Mario character). We also introduce semi-automatic techniques to identify such keywords or descriptions that trigger character generation. Using this framework, we evaluate mitigation strategies, including prompt rewriting and new approaches we propose. Our findings reveal that common methods, such as DALL-E's prompt rewriting, are insufficient alone and require supplementary strategies like negative prompting. Our work provides empirical grounding for discussions on copyright mitigation strategies and offers actionable insights for model deployers implementing these safeguards.\n",
            "reference_string": "[270620122 | He et al. | 2024 | Citations: 12]"
        },
        {
            "title": "Copyright-Aware Incentive Scheme for Generative Art Models Using Hierarchical Reinforcement Learning",
            "venue": "arXiv.org",
            "year": 2024,
            "reference_count": 0,
            "citation_count": 1,
            "influential_citation_count": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2410.20180, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2319419519",
                    "name": "Zhuan Shi"
                },
                {
                    "authorId": "2328664079",
                    "name": "Yifei Song"
                },
                {
                    "authorId": "2318236128",
                    "name": "Xiaoli Tang"
                },
                {
                    "authorId": "2287820224",
                    "name": "Lingjuan Lyu"
                },
                {
                    "authorId": "2054858128",
                    "name": "Boi Faltings"
                }
            ],
            "abstract": "Generative art using Diffusion models has achieved remarkable performance in image generation and text-to-image tasks. However, the increasing demand for training data in generative art raises significant concerns about copyright infringement, as models can produce images highly similar to copyrighted works. Existing solutions attempt to mitigate this by perturbing Diffusion models to reduce the likelihood of generating such images, but this often compromises model performance. Another approach focuses on economically compensating data holders for their contributions, yet it fails to address copyright loss adequately. Our approach begin with the introduction of a novel copyright metric grounded in copyright law and court precedents on infringement. We then employ the TRAK method to estimate the contribution of data holders. To accommodate the continuous data collection process, we divide the training into multiple rounds. Finally, We designed a hierarchical budget allocation method based on reinforcement learning to determine the budget for each round and the remuneration of the data holder based on the data holder's contribution and copyright loss in each round. Extensive experiments across three datasets show that our method outperforms all eight benchmarks, demonstrating its effectiveness in optimizing budget distribution in a copyright-aware manner. To the best of our knowledge, this is the first technical work that introduces to incentive contributors and protect their copyrights by compensating them.",
            "corpus_id": 273654195,
            "sentences": [
                {
                    "corpus_id": "273654195",
                    "title": "Copyright-Aware Incentive Scheme for Generative Art Models Using Hierarchical Reinforcement Learning",
                    "text": "Generative models are playing an increasingly important role in the web ecosystem by revolutionizing the way digital content is created and consumed [38] [6]. These models enable the automatic generation of high-quality images, audio, and video, leading to more dynamic, personalized, and engaging web experiences [33]. With the rise of generative models, generative art, a prominent form of artificial intelligence-generated content (AIGC), has emerged as a cutting-edge research topic [37] [15]. With the advancements in diffusion models, such as DALL\u2022E [26] and Stable Diffusion [24], generative art has demonstrated remarkable progress, particularly in image generation and text-to-image tasks. \n\nThe widespread deployment of such generative art models has brought about significant challenges concerning the risk for copyright infringement. The image generative models require large amount of training data, and it's impractical to only use noncopyrighted content to train the models. The model holder wants to leverage the high quality copyrighted data to learn the style and content and output high quality generated data. However, the generative models may generate some images that are very similar to the copyrighted training data, which may lead to copyright infringement [28]. \n\nVarious methods have been developed to protect copyright in source data, with the most common strategy involving the introduction of perturbations during the training process to effectively safeguard the dataset's copyright: (1) Unrecognizable examples [8,41] hinder models from learning essential features of protected images, either at the inference or training stages. However, this method is highly dependent on the specific images and models involved, and it lacks universal guarantees. (2) Watermarking [5,7] embeds subtle, imperceptible patterns into images to detect copyright violations, though further research is needed to improve its reliability. (3) Machine unlearning [1,9,13,20] removes the influence of copyrighted data, supporting the right to be forgotten. (4) Dataset deduplication [32] helps reduce memorization of training samples, minimizing the risk of copying protected content.",
                    "score": 0.5171769572517791,
                    "section_title": "Introduction",
                    "char_start_offset": 15,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 158
                        },
                        {
                            "start": 159,
                            "end": 319
                        },
                        {
                            "start": 320,
                            "end": 497
                        },
                        {
                            "start": 498,
                            "end": 698
                        },
                        {
                            "start": 701,
                            "end": 845
                        },
                        {
                            "start": 846,
                            "end": 989
                        },
                        {
                            "start": 990,
                            "end": 1129
                        },
                        {
                            "start": 1130,
                            "end": 1288
                        },
                        {
                            "start": 1291,
                            "end": 1662
                        },
                        {
                            "start": 1663,
                            "end": 1782
                        },
                        {
                            "start": 1783,
                            "end": 1949
                        },
                        {
                            "start": 1950,
                            "end": 2065
                        },
                        {
                            "start": 2066,
                            "end": 2193
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 154,
                            "end": 157,
                            "matchedPaperCorpusId": "264172541"
                        },
                        {
                            "start": 492,
                            "end": 496,
                            "matchedPaperCorpusId": "261279983"
                        },
                        {
                            "start": 1800,
                            "end": 1803,
                            "matchedPaperCorpusId": "258297834"
                        },
                        {
                            "start": 1803,
                            "end": 1805,
                            "matchedPaperCorpusId": "264436550"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.99267578125
                },
                {
                    "corpus_id": "273654195",
                    "title": "Copyright-Aware Incentive Scheme for Generative Art Models Using Hierarchical Reinforcement Learning",
                    "text": "Generative art using Diffusion models has achieved remarkable performance in image generation and text-to-image tasks. However, the increasing demand for training data in generative art raises significant concerns about copyright infringement, as models can produce images highly similar to copyrighted works. Existing solutions attempt to mitigate this by perturbing Diffusion models to reduce the likelihood of generating such images, but this often compromises model performance. Another approach focuses on economically compensating data holders for their contributions, yet it fails to address copyright loss adequately. Our approach begin with the introduction of a novel copyright metric grounded in copyright law and court precedents on infringement. We then employ the TRAK method to estimate the contribution of data holders. To accommodate the continuous data collection process, we divide the training into multiple rounds. Finally, We designed a hierarchical budget allocation method based on reinforcement learning to determine the budget for each round and the remuneration of the data holder based on the data holder's contribution and copyright loss in each round. Extensive experiments across three datasets show that our method outperforms all eight benchmarks, demonstrating its effectiveness in optimizing budget distribution in a copyright-aware manner. To the best of our knowledge, this is the first technical work that introduces to incentive contributors and protect their copyrights by compensating them.",
                    "score": 0.497920306054828,
                    "section_title": "abstract",
                    "char_start_offset": 0,
                    "sentence_offsets": [],
                    "ref_mentions": [],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.9853515625
                }
            ],
            "relevance_judgement": 0.99267578125,
            "relevance_judgment_input_expanded": "# Title: Copyright-Aware Incentive Scheme for Generative Art Models Using Hierarchical Reinforcement Learning\n# Venue: arXiv.org\n# Authors: Zhuan Shi, Yifei Song, Xiaoli Tang, Lingjuan Lyu, Boi Faltings\n## Abstract\nGenerative art using Diffusion models has achieved remarkable performance in image generation and text-to-image tasks. However, the increasing demand for training data in generative art raises significant concerns about copyright infringement, as models can produce images highly similar to copyrighted works. Existing solutions attempt to mitigate this by perturbing Diffusion models to reduce the likelihood of generating such images, but this often compromises model performance. Another approach focuses on economically compensating data holders for their contributions, yet it fails to address copyright loss adequately. Our approach begin with the introduction of a novel copyright metric grounded in copyright law and court precedents on infringement. We then employ the TRAK method to estimate the contribution of data holders. To accommodate the continuous data collection process, we divide the training into multiple rounds. Finally, We designed a hierarchical budget allocation method based on reinforcement learning to determine the budget for each round and the remuneration of the data holder based on the data holder's contribution and copyright loss in each round. Extensive experiments across three datasets show that our method outperforms all eight benchmarks, demonstrating its effectiveness in optimizing budget distribution in a copyright-aware manner. To the best of our knowledge, this is the first technical work that introduces to incentive contributors and protect their copyrights by compensating them.\n## Introduction\nGenerative models are playing an increasingly important role in the web ecosystem by revolutionizing the way digital content is created and consumed [38] [6]. These models enable the automatic generation of high-quality images, audio, and video, leading to more dynamic, personalized, and engaging web experiences [33]. With the rise of generative models, generative art, a prominent form of artificial intelligence-generated content (AIGC), has emerged as a cutting-edge research topic [37] [15]. With the advancements in diffusion models, such as DALL\u2022E [26] and Stable Diffusion [24], generative art has demonstrated remarkable progress, particularly in image generation and text-to-image tasks. \n\nThe widespread deployment of such generative art models has brought about significant challenges concerning the risk for copyright infringement. The image generative models require large amount of training data, and it's impractical to only use noncopyrighted content to train the models. The model holder wants to leverage the high quality copyrighted data to learn the style and content and output high quality generated data. However, the generative models may generate some images that are very similar to the copyrighted training data, which may lead to copyright infringement [28]. \n\nVarious methods have been developed to protect copyright in source data, with the most common strategy involving the introduction of perturbations during the training process to effectively safeguard the dataset's copyright: (1) Unrecognizable examples [8,41] hinder models from learning essential features of protected images, either at the inference or training stages. However, this method is highly dependent on the specific images and models involved, and it lacks universal guarantees. (2) Watermarking [5,7] embeds subtle, imperceptible patterns into images to detect copyright violations, though further research is needed to improve its reliability. (3) Machine unlearning [1,9,13,20] removes the influence of copyrighted data, supporting the right to be forgotten. (4) Dataset deduplication [32] helps reduce memorization of training samples, minimizing the risk of copying protected content.",
            "reference_string": "[273654195 | Shi et al. | 2024 | Citations: 1]"
        },
        {
            "title": "Rethinking Artistic Copyright Infringements in the Era of Text-to-Image Generative Models",
            "venue": "arXiv.org",
            "year": 2024,
            "reference_count": 28,
            "citation_count": 4,
            "influential_citation_count": 1,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2404.08030, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "104644443",
                    "name": "Mazda Moayeri"
                },
                {
                    "authorId": "2114710333",
                    "name": "Samyadeep Basu"
                },
                {
                    "authorId": "144021807",
                    "name": "S. Balasubramanian"
                },
                {
                    "authorId": "1962835975",
                    "name": "Priyatham Kattakinda"
                },
                {
                    "authorId": "2188780004",
                    "name": "Atoosa Malemir Chegini"
                },
                {
                    "authorId": "69357142",
                    "name": "R. Brauneis"
                },
                {
                    "authorId": "34389431",
                    "name": "S. Feizi"
                }
            ],
            "abstract": "Recent text-to-image generative models such as Stable Diffusion are extremely adept at mimicking and generating copyrighted content, raising concerns amongst artists that their unique styles may be improperly copied. Understanding how generative models copy\"artistic style\"is more complex than duplicating a single image, as style is comprised by a set of elements (or signature) that frequently co-occurs across a body of work, where each individual work may vary significantly. In our paper, we first reformulate the problem of\"artistic copyright infringement\"to a classification problem over image sets, instead of probing image-wise similarities. We then introduce ArtSavant, a practical (i.e., efficient and easy to understand) tool to (i) determine the unique style of an artist by comparing it to a reference dataset of works from 372 artists curated from WikiArt, and (ii) recognize if the identified style reappears in generated images. We leverage two complementary methods to perform artistic style classification over image sets, includingTagMatch, which is a novel inherently interpretable and attributable method, making it more suitable for broader use by non-technical stake holders (artists, lawyers, judges, etc). Leveraging ArtSavant, we then perform a large-scale empirical study to provide quantitative insight on the prevalence of artistic style copying across 3 popular text-to-image generative models. Namely, amongst a dataset of prolific artists (including many famous ones), only 20% of them appear to have their styles be at a risk of copying via simple prompting of today's popular text-to-image generative models.",
            "corpus_id": 269137659,
            "sentences": [
                {
                    "corpus_id": "269137659",
                    "title": "Rethinking Artistic Copyright Infringements in the Era of Text-to-Image Generative Models",
                    "text": "As image generative models have rapidly improved in scale and sophistication, the possibility of them mimicking artists' personal styles has been an important topic of discussion in the literature [18].Many previous works describe ways to either detect potential direct image copying in generated images, or to foil any future copying attempts by imperceptibly altering the artists' works to prevent effective training by the generative models.These include techniques like adding imperceptible watermarks to copyrighted artworks [7,8,28], and crafting \"un-learnable\" examples on which models struggle to learn the style-relevant information [24,29,30].These methods are typically computationally expensive and incur a loss in image quality, which may render these techniques impractical for many artists.Also, they do not protect artworks which have been previously uploaded to the internet without any safeguards.Others have suggested methods to mitigate this issue from the model owner's perspective -to either de-duplicate the dataset before training [4,25,26], or to remove concepts from the model after training (\"unlearning\") [3,9,13].These are also technically challenging, and require the model owner to invest significant resources which may again inhibit their practicality.Methods like [4,25,26] are also more focused on analyzing direct image copying from the training data, and thus may not be applicable to preventing style copying.None of these works tackle the problem of detecting potentially copied art styles in generated art, especially in a manner which may be relevant to legal standards of copyright infringement.According to current US legal standards [2], an artwork has to meet the \"substantial similarity\" test for it to be infringing on copyright.This similarity has to be established on analytic and holistic terms Fig. 2: Example generations from Stable Diffusion 2 when prompted to produce specific paintings by Vincent Van Gogh, along with the histogram of similarities between the generated image and corresponding real image.Even for a famous artist like Vincent Van Gogh, generative models rarely produce near-exact duplicates.However, Van Gogh's style appears consistently, even when similarity is low.[11,14].",
                    "score": 0.461057559628288,
                    "section_title": "Related Works",
                    "char_start_offset": 8141,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 202
                        },
                        {
                            "start": 202,
                            "end": 444
                        },
                        {
                            "start": 444,
                            "end": 653
                        },
                        {
                            "start": 653,
                            "end": 805
                        },
                        {
                            "start": 805,
                            "end": 915
                        },
                        {
                            "start": 915,
                            "end": 1142
                        },
                        {
                            "start": 1142,
                            "end": 1285
                        },
                        {
                            "start": 1285,
                            "end": 1447
                        },
                        {
                            "start": 1447,
                            "end": 1637
                        },
                        {
                            "start": 1637,
                            "end": 1776
                        },
                        {
                            "start": 1776,
                            "end": 2060
                        },
                        {
                            "start": 2060,
                            "end": 2163
                        },
                        {
                            "start": 2163,
                            "end": 2239
                        },
                        {
                            "start": 2239,
                            "end": 2247
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 1061,
                            "end": 1064,
                            "matchedPaperCorpusId": "258987384"
                        },
                        {
                            "start": 1304,
                            "end": 1307,
                            "matchedPaperCorpusId": "258987384"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.99267578125
                }
            ],
            "relevance_judgement": 0.99267578125,
            "relevance_judgment_input_expanded": "# Title: Rethinking Artistic Copyright Infringements in the Era of Text-to-Image Generative Models\n# Venue: arXiv.org\n# Authors: Mazda Moayeri, Samyadeep Basu, S. Balasubramanian, Priyatham Kattakinda, Atoosa Malemir Chegini, R. Brauneis, S. Feizi\n## Abstract\nRecent text-to-image generative models such as Stable Diffusion are extremely adept at mimicking and generating copyrighted content, raising concerns amongst artists that their unique styles may be improperly copied. Understanding how generative models copy\"artistic style\"is more complex than duplicating a single image, as style is comprised by a set of elements (or signature) that frequently co-occurs across a body of work, where each individual work may vary significantly. In our paper, we first reformulate the problem of\"artistic copyright infringement\"to a classification problem over image sets, instead of probing image-wise similarities. We then introduce ArtSavant, a practical (i.e., efficient and easy to understand) tool to (i) determine the unique style of an artist by comparing it to a reference dataset of works from 372 artists curated from WikiArt, and (ii) recognize if the identified style reappears in generated images. We leverage two complementary methods to perform artistic style classification over image sets, includingTagMatch, which is a novel inherently interpretable and attributable method, making it more suitable for broader use by non-technical stake holders (artists, lawyers, judges, etc). Leveraging ArtSavant, we then perform a large-scale empirical study to provide quantitative insight on the prevalence of artistic style copying across 3 popular text-to-image generative models. Namely, amongst a dataset of prolific artists (including many famous ones), only 20% of them appear to have their styles be at a risk of copying via simple prompting of today's popular text-to-image generative models.\n## Related Works\nAs image generative models have rapidly improved in scale and sophistication, the possibility of them mimicking artists' personal styles has been an important topic of discussion in the literature [18].Many previous works describe ways to either detect potential direct image copying in generated images, or to foil any future copying attempts by imperceptibly altering the artists' works to prevent effective training by the generative models.These include techniques like adding imperceptible watermarks to copyrighted artworks [7,8,28], and crafting \"un-learnable\" examples on which models struggle to learn the style-relevant information [24,29,30].These methods are typically computationally expensive and incur a loss in image quality, which may render these techniques impractical for many artists.Also, they do not protect artworks which have been previously uploaded to the internet without any safeguards.Others have suggested methods to mitigate this issue from the model owner's perspective -to either de-duplicate the dataset before training [4,25,26], or to remove concepts from the model after training (\"unlearning\") [3,9,13].These are also technically challenging, and require the model owner to invest significant resources which may again inhibit their practicality.Methods like [4,25,26] are also more focused on analyzing direct image copying from the training data, and thus may not be applicable to preventing style copying.None of these works tackle the problem of detecting potentially copied art styles in generated art, especially in a manner which may be relevant to legal standards of copyright infringement.According to current US legal standards [2], an artwork has to meet the \"substantial similarity\" test for it to be infringing on copyright.This similarity has to be established on analytic and holistic terms Fig. 2: Example generations from Stable Diffusion 2 when prompted to produce specific paintings by Vincent Van Gogh, along with the histogram of similarities between the generated image and corresponding real image.Even for a famous artist like Vincent Van Gogh, generative models rarely produce near-exact duplicates.However, Van Gogh's style appears consistently, even when similarity is low.[11,14].",
            "reference_string": "[269137659 | Moayeri et al. | 2024 | Citations: 4]"
        },
        {
            "title": "Safer Prompts: Reducing IP Risk in Visual Generative AI",
            "venue": "arXiv.org",
            "year": 2025,
            "reference_count": 25,
            "citation_count": 0,
            "influential_citation_count": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2505.03338, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2359254079",
                    "name": "Lena Reissinger"
                },
                {
                    "authorId": "2256011059",
                    "name": "Yuanyuan Li"
                },
                {
                    "authorId": "23107750",
                    "name": "Anna Haensch"
                },
                {
                    "authorId": "2254269177",
                    "name": "Neeraj Sarna"
                }
            ],
            "abstract": "Visual Generative AI models have demonstrated remarkable capability in generating high-quality images from simple inputs like text prompts. However, because these models are trained on images from diverse sources, they risk memorizing and reproducing specific content, raising concerns about intellectual property (IP) infringement. Recent advances in prompt engineering offer a cost-effective way to enhance generative AI performance. In this paper, we evaluate the effectiveness of prompt engineering techniques in mitigating IP infringement risks in image generation. Our findings show that Chain of Thought Prompting and Task Instruction Prompting significantly reduce the similarity between generated images and the training data of diffusion models, thereby lowering the risk of IP infringement.",
            "corpus_id": 278338968,
            "sentences": [
                {
                    "corpus_id": "278338968",
                    "title": "Safer Prompts: Reducing IP Risk in Visual Generative AI",
                    "text": "As generative AI (GenAI) becomes increasingly prevalent in realworld applications, concerns about its potential risks continue to grow. We focus on the risks associated with the replication of copyrighted material and restrict ourselves to image generation. The tangible legal and financial implications of these risks have sparked our current research. For instance, artists sued Stability AI (Midjourney and DevianArt) claiming that the companies' AI image generators produce images that are strikingly similar to their artworks [2]. In August 2024, a U.S. district judge (Orrick) ruled that the artists could proceed with the copyright infringement claims, underscoring the ongoing legal uncertainties surrounding AI-generated content [6]. \n\nThese prevalent risks make model users resistant to fully exploiting the latest GenAI models. To promote a wide adoption of GenAI, risk management is crucial. Risk management has two aspects: a) risk quantification; and b) risk mitigation. Both aspects have already received attention in the literature. \n\nRisk quantification: Risk quantification involves analysing the fraction of training images that are reproduced by the model. Since training images may be copy-righted, this leads to IP infringement risks for the end user. Consider for instance Stable Diffusion-1. When prompted using the captions of training images directly, about 2% of images it generates are highly similar to those in the training dataset [4,24]; the results are very similar for the Stable Diffusion-2 model [25]. \n\nRisk mitigation: We focus on risk mitigation, which takes a step beyond risk quantification. The goal of risk mitigation is to reduce the probability of a model outputting copyrighted content. The risk could either be mitigated before deploying a model (on the model developer's side) or after (on the model's user side). \n\nPre-deployment strategies involve changing the data science process and are usually expensive. For instance, models could be trained on a de-duplicated data set, which usually reduces the risk during training time [25]. Note however that with this strategy test time risks still prevail. Model could also be made to unlearn copy righted data but that comes with a huge computational expense [31,7,8].",
                    "score": 0.41438170835825244,
                    "section_title": "Introduction",
                    "char_start_offset": 15,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 135
                        },
                        {
                            "start": 136,
                            "end": 257
                        },
                        {
                            "start": 258,
                            "end": 353
                        },
                        {
                            "start": 354,
                            "end": 535
                        },
                        {
                            "start": 536,
                            "end": 742
                        },
                        {
                            "start": 745,
                            "end": 838
                        },
                        {
                            "start": 839,
                            "end": 903
                        },
                        {
                            "start": 904,
                            "end": 984
                        },
                        {
                            "start": 985,
                            "end": 1048
                        },
                        {
                            "start": 1051,
                            "end": 1176
                        },
                        {
                            "start": 1177,
                            "end": 1273
                        },
                        {
                            "start": 1274,
                            "end": 1315
                        },
                        {
                            "start": 1316,
                            "end": 1537
                        },
                        {
                            "start": 1540,
                            "end": 1632
                        },
                        {
                            "start": 1633,
                            "end": 1732
                        },
                        {
                            "start": 1733,
                            "end": 1861
                        },
                        {
                            "start": 1864,
                            "end": 1958
                        },
                        {
                            "start": 1959,
                            "end": 2083
                        },
                        {
                            "start": 2084,
                            "end": 2151
                        },
                        {
                            "start": 2152,
                            "end": 2264
                        }
                    ],
                    "ref_mentions": [],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.99267578125
                }
            ],
            "relevance_judgement": 0.99267578125,
            "relevance_judgment_input_expanded": "# Title: Safer Prompts: Reducing IP Risk in Visual Generative AI\n# Venue: arXiv.org\n# Authors: Lena Reissinger, Yuanyuan Li, Anna Haensch, Neeraj Sarna\n## Abstract\nVisual Generative AI models have demonstrated remarkable capability in generating high-quality images from simple inputs like text prompts. However, because these models are trained on images from diverse sources, they risk memorizing and reproducing specific content, raising concerns about intellectual property (IP) infringement. Recent advances in prompt engineering offer a cost-effective way to enhance generative AI performance. In this paper, we evaluate the effectiveness of prompt engineering techniques in mitigating IP infringement risks in image generation. Our findings show that Chain of Thought Prompting and Task Instruction Prompting significantly reduce the similarity between generated images and the training data of diffusion models, thereby lowering the risk of IP infringement.\n## Introduction\nAs generative AI (GenAI) becomes increasingly prevalent in realworld applications, concerns about its potential risks continue to grow. We focus on the risks associated with the replication of copyrighted material and restrict ourselves to image generation. The tangible legal and financial implications of these risks have sparked our current research. For instance, artists sued Stability AI (Midjourney and DevianArt) claiming that the companies' AI image generators produce images that are strikingly similar to their artworks [2]. In August 2024, a U.S. district judge (Orrick) ruled that the artists could proceed with the copyright infringement claims, underscoring the ongoing legal uncertainties surrounding AI-generated content [6]. \n\nThese prevalent risks make model users resistant to fully exploiting the latest GenAI models. To promote a wide adoption of GenAI, risk management is crucial. Risk management has two aspects: a) risk quantification; and b) risk mitigation. Both aspects have already received attention in the literature. \n\nRisk quantification: Risk quantification involves analysing the fraction of training images that are reproduced by the model. Since training images may be copy-righted, this leads to IP infringement risks for the end user. Consider for instance Stable Diffusion-1. When prompted using the captions of training images directly, about 2% of images it generates are highly similar to those in the training dataset [4,24]; the results are very similar for the Stable Diffusion-2 model [25]. \n\nRisk mitigation: We focus on risk mitigation, which takes a step beyond risk quantification. The goal of risk mitigation is to reduce the probability of a model outputting copyrighted content. The risk could either be mitigated before deploying a model (on the model developer's side) or after (on the model's user side). \n\nPre-deployment strategies involve changing the data science process and are usually expensive. For instance, models could be trained on a de-duplicated data set, which usually reduces the risk during training time [25]. Note however that with this strategy test time risks still prevail. Model could also be made to unlearn copy righted data but that comes with a huge computational expense [31,7,8].",
            "reference_string": "[278338968 | Reissinger et al. | 2025 | Citations: 0]"
        },
        {
            "title": "CopyJudge: Automated Copyright Infringement Identification and Mitigation in Text-to-Image Diffusion Models",
            "venue": "arXiv.org",
            "year": 2025,
            "reference_count": 47,
            "citation_count": 2,
            "influential_citation_count": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2502.15278, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2346891526",
                    "name": "Shunchang Liu"
                },
                {
                    "authorId": "2319419519",
                    "name": "Zhuan Shi"
                },
                {
                    "authorId": "2287820224",
                    "name": "Lingjuan Lyu"
                },
                {
                    "authorId": "2344619001",
                    "name": "Yaochu Jin"
                },
                {
                    "authorId": "2054858128",
                    "name": "Boi Faltings"
                }
            ],
            "abstract": "Assessing whether AI-generated images are substantially similar to copyrighted works is a crucial step in resolving copyright disputes. In this paper, we propose CopyJudge, an automated copyright infringement identification framework that leverages large vision-language models (LVLMs) to simulate practical court processes for determining substantial similarity between copyrighted images and those generated by text-to-image diffusion models. Specifically, we employ an abstraction-filtration-comparison test framework with multi-LVLM debate to assess the likelihood of infringement and provide detailed judgment rationales. Based on the judgments, we further introduce a general LVLM-based mitigation strategy that automatically optimizes infringing prompts by avoiding sensitive expressions while preserving the non-infringing content. Besides, our approach can be enhanced by exploring non-infringing noise vectors within the diffusion latent space via reinforcement learning, even without modifying the original prompts. Experimental results show that our identification method achieves comparable state-of-the-art performance, while offering superior generalization and interpretability across various forms of infringement, and that our mitigation method could more effectively mitigate memorization and IP infringement without losing non-infringing expressions.",
            "corpus_id": 276558342,
            "sentences": [
                {
                    "corpus_id": "276558342",
                    "title": "CopyJudge: Automated Copyright Infringement Identification and Mitigation in Text-to-Image Diffusion Models",
                    "text": "Text-to-image generative models (Rombach et al., 2022;Betker et al., 2023;Team et al., 2023;Esser et al., 2024;Hurst et al., 2024;Zhang et al., 2023b;a;Hintersdorf et al., 2024) have transformed creative industries by producing detailed visuals from text prompts. However, these models have been found to sometimes memorize and reproduce content from their training data (Carlini et al., 2023;Somepalli et al., 2023a;Ren et al., 2024;Wang et al., 2024c;Shi et al., 2024b;a;Zhang et al., 2024). This raises significant concerns about copyright infringement, especially when the generated images closely resemble existing copyrighted works. According to U.S. law (rot, 1970), also referenced by most countries, a work can be considered infringing if it constitutes substantial similarity to another work1 . Therefore, determining whether AI-generated images infringe on copyright requires a clear and reliable method to compare them with copyrighted materials to identify substantial similarity. \n\nHowever, identifying substantial similarity is not a trivial task. There are already some methods to assess image similarity through distance-based metrics, e.g., L 2 norm (Carlini et al., 2023). However, we found that these manually designed metrics do not always align with the human judgment for infringement determination. Additionally, it often suffer from insufficient generalization ability and lack interpretable results. This motivates the need for an approach that better measures substantial similarity, one that is more humancentered, interpretable, and generalized to handle copyright infringement identification in AI-generated images. \n\nRecently, large-scale models have already been successfully applied as judges in fields such as finance, education, and healthcare (Gu et al., 2024;Li et al., 2024;Zhuge et al., 2024). In this paper, we attempt to leverage large visionlanguage models (LVLMs) to model the practical court decisions on substantial similarity.",
                    "score": 0.5502174902513126,
                    "section_title": "Introduction",
                    "char_start_offset": 15,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 263
                        },
                        {
                            "start": 264,
                            "end": 493
                        },
                        {
                            "start": 494,
                            "end": 638
                        },
                        {
                            "start": 639,
                            "end": 804
                        },
                        {
                            "start": 805,
                            "end": 993
                        },
                        {
                            "start": 996,
                            "end": 1062
                        },
                        {
                            "start": 1063,
                            "end": 1191
                        },
                        {
                            "start": 1192,
                            "end": 1322
                        },
                        {
                            "start": 1323,
                            "end": 1425
                        },
                        {
                            "start": 1426,
                            "end": 1645
                        },
                        {
                            "start": 1648,
                            "end": 1832
                        },
                        {
                            "start": 1833,
                            "end": 1972
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 32,
                            "end": 54,
                            "matchedPaperCorpusId": "245335280"
                        },
                        {
                            "start": 54,
                            "end": 74,
                            "matchedPaperCorpusId": "264403242"
                        },
                        {
                            "start": 92,
                            "end": 111,
                            "matchedPaperCorpusId": "268247980"
                        },
                        {
                            "start": 130,
                            "end": 150,
                            "matchedPaperCorpusId": "256827727"
                        },
                        {
                            "start": 371,
                            "end": 393,
                            "matchedPaperCorpusId": "256389993"
                        },
                        {
                            "start": 393,
                            "end": 417,
                            "matchedPaperCorpusId": "254366634"
                        },
                        {
                            "start": 1168,
                            "end": 1190,
                            "matchedPaperCorpusId": "256389993"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.99169921875
                },
                {
                    "corpus_id": "276558342",
                    "title": "CopyJudge: Automated Copyright Infringement Identification and Mitigation in Text-to-Image Diffusion Models",
                    "text": "This helps avoid infringement while maintaining the desired output characteristics, even without changing the original prompts. \n\nIn summary, our contributions are as follows: \n\n\u2022 We propose CopyJudge, an automated abstractionfiltration-comparison framework powered by a multi-LVLM debate mechanism, designed to efficiently detect copyright-infringing images generated by text-toimage diffusion models. \n\n\u2022 Given the judgment, we introduce an adaptive mitigation strategy that automatically optimizes prompts and explores non-infringing latent noise vectors of diffusion models, effectively mitigating copyright violations while preserving non-infringing expressions. \n\n\u2022 Extensive experiments demonstrate that our identification method matches state-of-the-art performance, with improved generalization and interpretability, while our mitigation approach more effectively prevents infringement without losing non-infringing expressions. \n\nImage infringement detection and mitigation. The current mainstream infringing image detection methods primarily measure the distance or invariance in pixel or embedding space (Carlini et al., 2023;Somepalli et al., 2023a;Shi et al., 2024b;Wang et al., 2021;2024b (Wen et al., 2024;Wang et al., 2024d) have shown that these methods have lower generalization capabilities and lack interpretability because they do not fully align with human judgment standards. For copyright infringement mitigation, the current approaches mainly involve machine unlearning to remove the model's memory of copyright information (Bourtoule et al., 2021;Nguyen et al., 2022;Kumari et al., 2023;Zhang et al., 2024) or deleting duplicated samples from the training data (Webster et al., 2023;Somepalli et al., 2023b",
                    "score": 0.41820132967910045,
                    "section_title": "Introduction",
                    "char_start_offset": 4077,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 127
                        },
                        {
                            "start": 130,
                            "end": 175
                        },
                        {
                            "start": 178,
                            "end": 402
                        },
                        {
                            "start": 405,
                            "end": 667
                        },
                        {
                            "start": 670,
                            "end": 937
                        },
                        {
                            "start": 940,
                            "end": 984
                        },
                        {
                            "start": 985,
                            "end": 1399
                        },
                        {
                            "start": 1400,
                            "end": 1733
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 1116,
                            "end": 1138,
                            "matchedPaperCorpusId": "256389993"
                        },
                        {
                            "start": 1138,
                            "end": 1162,
                            "matchedPaperCorpusId": "254366634"
                        },
                        {
                            "start": 1204,
                            "end": 1222,
                            "matchedPaperCorpusId": "270309880"
                        },
                        {
                            "start": 1550,
                            "end": 1574,
                            "matchedPaperCorpusId": "208909851"
                        },
                        {
                            "start": 1594,
                            "end": 1614,
                            "matchedPaperCorpusId": "257687839"
                        },
                        {
                            "start": 1710,
                            "end": 1733,
                            "matchedPaperCorpusId": "258987384"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.990234375
                },
                {
                    "corpus_id": "276558342",
                    "title": "CopyJudge: Automated Copyright Infringement Identification and Mitigation in Text-to-Image Diffusion Models",
                    "text": "Compared to cases of exact replication (memorization), here we consider specific IP infringement, such as imitation of cartoon IPs and artistic elements. Based on whether the input prompt contains direct copyright information, we consider two types of infringement scenarios: explicit infringement and implicit infringement. \n\nExplicit infringement. This refers to prompts that directly contain copyright information, such as \"Generate an image of Mickey Mouse.\" We use the 20 cartoon and artwork samples collected in section 5.1 to generate infringing images using Stable Diffusion v2, where the prompt explicitly includes the names or author names of the work. \n\nImplicit infringement. This occurs when the prompt does not explicitly contain copyright information, but the generated image still infringes due to certain infringing expressions. This type of scenario is more applicable to commercial text-to-image models, as they often include content detection modules that can effectively detect copyrighted information and thus reject the request. In this scenario, we use the same IP samples as above, but generate infringing images without any explicit copyright information using DALL\u2022E 3 (Betker et al., 2023), which has a safety detection module to reject prompts that trigger it. \n\nAutomated attack. Efficiently retrieving or generating infringing prompts has always been a challenge. Kim et al. utilize large models to iteratively generate jailbreak prompts targeting commercial models, thereby inducing them to output copyrighted content. Drawing from it, we use our CopyJudge to generate infringing prompts. In contrast to mitigation, for attack, we only need to use an LVLM to progressively intensify the infringing expressions within the prompt. The prompt is iteratively adjusted, and once the infringement score exceeds 0.8 / 1.0, mitigation is activated, using the current prompt as the starting point. \n\nFor explicit infringement, we validate both prompt control (PC) and latent control (LC). For implicit infringement, due to the commercial model DALL\u2022E's inability to customize latents, we only evaluate prompt control.",
                    "score": 0.4895156796398643,
                    "section_title": "IP INFRINGEMENT MITIGATION",
                    "char_start_offset": 19610,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 153
                        },
                        {
                            "start": 154,
                            "end": 324
                        },
                        {
                            "start": 327,
                            "end": 349
                        },
                        {
                            "start": 350,
                            "end": 462
                        },
                        {
                            "start": 463,
                            "end": 662
                        },
                        {
                            "start": 665,
                            "end": 687
                        },
                        {
                            "start": 688,
                            "end": 845
                        },
                        {
                            "start": 846,
                            "end": 1051
                        },
                        {
                            "start": 1052,
                            "end": 1289
                        },
                        {
                            "start": 1292,
                            "end": 1309
                        },
                        {
                            "start": 1310,
                            "end": 1394
                        },
                        {
                            "start": 1395,
                            "end": 1550
                        },
                        {
                            "start": 1551,
                            "end": 1620
                        },
                        {
                            "start": 1621,
                            "end": 1760
                        },
                        {
                            "start": 1761,
                            "end": 1920
                        },
                        {
                            "start": 1923,
                            "end": 2011
                        },
                        {
                            "start": 2012,
                            "end": 2140
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 1196,
                            "end": 1217,
                            "matchedPaperCorpusId": "264403242"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.98876953125
                },
                {
                    "corpus_id": "276558342",
                    "title": "CopyJudge: Automated Copyright Infringement Identification and Mitigation in Text-to-Image Diffusion Models",
                    "text": "Assessing whether AI-generated images are substantially similar to copyrighted works is a crucial step in resolving copyright disputes. In this paper, we propose CopyJudge, an automated copyright infringement identification framework that leverages large vision-language models (LVLMs) to simulate practical court processes for determining substantial similarity between copyrighted images and those generated by text-to-image diffusion models. Specifically, we employ an abstraction-filtration-comparison test framework with multi-LVLM debate to assess the likelihood of infringement and provide detailed judgment rationales. Based on the judgments, we further introduce a general LVLM-based mitigation strategy that automatically optimizes infringing prompts by avoiding sensitive expressions while preserving the non-infringing content. Besides, our approach can be enhanced by exploring non-infringing noise vectors within the diffusion latent space via reinforcement learning, even without modifying the original prompts. Experimental results show that our identification method achieves comparable state-of-the-art performance, while offering superior generalization and interpretability across various forms of infringement, and that our mitigation method could more effectively mitigate memorization and IP infringement without losing non-infringing expressions.",
                    "score": 0.4377457196985437,
                    "section_title": "abstract",
                    "char_start_offset": 0,
                    "sentence_offsets": [],
                    "ref_mentions": [],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.98583984375
                },
                {
                    "corpus_id": "276558342",
                    "title": "CopyJudge: Automated Copyright Infringement Identification and Mitigation in Text-to-Image Diffusion Models",
                    "text": "We present CopyJudge, an innovative framework for automating the identification of copyright infringement in text-to-image diffusion models. By leveraging abstractionfiltration-comparison test and multi-LVLM debates, our approach could effectively evaluate the substantial similarity between generated and copyrighted images, providing clear and interpretable judgments. Additionally, our LVLM-based mitigation strategy helps avoid infringement by automatically optimizing prompts and exploring non-infringing latent noise vectors, while ensuring that generated images align with the user's requirements. \n\n\u2022 Modifying a Prompt to Improve Similarity Score (Attack Iteration): \"Adjust the parts of the original prompt of the second image that may cause expressions of distinction in the following rationale, making it more similar to the first image to achieve a higher score. Add more information about the [IP type] in Image 1, and provide more unique expressions specific to the [IP type] in Image 1. You can make any changes as long as they improve the similarity score. Require: Source image x cr , generated image x 0 , initial prompt p 0 , control condition p c , LVLM-based prompt modifier \u03c0 p , infringement identification function f , threshold \u03b3, maximum iterations T 1: Initialize prompt: p t \u2190 p 0 , generated image: \n\nGenerate new prompt: p t+1 \u2190 \u03c0 p (x t , x cr , p t , p c , s t , c t , r t ) 8: \n\nGenerate new image using p t+1 : x t+1 \u2190 T2I(p t+1 ) 9: \n\nUpdate iteration counter: t \u2190 t + 1 10: end while 11: Return final prompt p t and generated image x t",
                    "score": 0.4374161971366494,
                    "section_title": "Conclusion",
                    "char_start_offset": 24510,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 140
                        },
                        {
                            "start": 141,
                            "end": 370
                        },
                        {
                            "start": 371,
                            "end": 604
                        },
                        {
                            "start": 607,
                            "end": 875
                        },
                        {
                            "start": 876,
                            "end": 1002
                        },
                        {
                            "start": 1003,
                            "end": 1073
                        },
                        {
                            "start": 1074,
                            "end": 1328
                        },
                        {
                            "start": 1331,
                            "end": 1410
                        },
                        {
                            "start": 1413,
                            "end": 1468
                        },
                        {
                            "start": 1471,
                            "end": 1572
                        }
                    ],
                    "ref_mentions": [],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.9853515625
                }
            ],
            "relevance_judgement": 0.99169921875,
            "relevance_judgment_input_expanded": "# Title: CopyJudge: Automated Copyright Infringement Identification and Mitigation in Text-to-Image Diffusion Models\n# Venue: arXiv.org\n# Authors: Shunchang Liu, Zhuan Shi, Lingjuan Lyu, Yaochu Jin, Boi Faltings\n## Abstract\nAssessing whether AI-generated images are substantially similar to copyrighted works is a crucial step in resolving copyright disputes. In this paper, we propose CopyJudge, an automated copyright infringement identification framework that leverages large vision-language models (LVLMs) to simulate practical court processes for determining substantial similarity between copyrighted images and those generated by text-to-image diffusion models. Specifically, we employ an abstraction-filtration-comparison test framework with multi-LVLM debate to assess the likelihood of infringement and provide detailed judgment rationales. Based on the judgments, we further introduce a general LVLM-based mitigation strategy that automatically optimizes infringing prompts by avoiding sensitive expressions while preserving the non-infringing content. Besides, our approach can be enhanced by exploring non-infringing noise vectors within the diffusion latent space via reinforcement learning, even without modifying the original prompts. Experimental results show that our identification method achieves comparable state-of-the-art performance, while offering superior generalization and interpretability across various forms of infringement, and that our mitigation method could more effectively mitigate memorization and IP infringement without losing non-infringing expressions.\n## Introduction\nText-to-image generative models (Rombach et al., 2022;Betker et al., 2023;Team et al., 2023;Esser et al., 2024;Hurst et al., 2024;Zhang et al., 2023b;a;Hintersdorf et al., 2024) have transformed creative industries by producing detailed visuals from text prompts. However, these models have been found to sometimes memorize and reproduce content from their training data (Carlini et al., 2023;Somepalli et al., 2023a;Ren et al., 2024;Wang et al., 2024c;Shi et al., 2024b;a;Zhang et al., 2024). This raises significant concerns about copyright infringement, especially when the generated images closely resemble existing copyrighted works. According to U.S. law (rot, 1970), also referenced by most countries, a work can be considered infringing if it constitutes substantial similarity to another work1 . Therefore, determining whether AI-generated images infringe on copyright requires a clear and reliable method to compare them with copyrighted materials to identify substantial similarity. \n\nHowever, identifying substantial similarity is not a trivial task. There are already some methods to assess image similarity through distance-based metrics, e.g., L 2 norm (Carlini et al., 2023). However, we found that these manually designed metrics do not always align with the human judgment for infringement determination. Additionally, it often suffer from insufficient generalization ability and lack interpretable results. This motivates the need for an approach that better measures substantial similarity, one that is more humancentered, interpretable, and generalized to handle copyright infringement identification in AI-generated images. \n\nRecently, large-scale models have already been successfully applied as judges in fields such as finance, education, and healthcare (Gu et al., 2024;Li et al., 2024;Zhuge et al., 2024). In this paper, we attempt to leverage large visionlanguage models (LVLMs) to model the practical court decisions on substantial similarity.\n...\nThis helps avoid infringement while maintaining the desired output characteristics, even without changing the original prompts. \n\nIn summary, our contributions are as follows: \n\n\u2022 We propose CopyJudge, an automated abstractionfiltration-comparison framework powered by a multi-LVLM debate mechanism, designed to efficiently detect copyright-infringing images generated by text-toimage diffusion models. \n\n\u2022 Given the judgment, we introduce an adaptive mitigation strategy that automatically optimizes prompts and explores non-infringing latent noise vectors of diffusion models, effectively mitigating copyright violations while preserving non-infringing expressions. \n\n\u2022 Extensive experiments demonstrate that our identification method matches state-of-the-art performance, with improved generalization and interpretability, while our mitigation approach more effectively prevents infringement without losing non-infringing expressions. \n\nImage infringement detection and mitigation. The current mainstream infringing image detection methods primarily measure the distance or invariance in pixel or embedding space (Carlini et al., 2023;Somepalli et al., 2023a;Shi et al., 2024b;Wang et al., 2021;2024b (Wen et al., 2024;Wang et al., 2024d) have shown that these methods have lower generalization capabilities and lack interpretability because they do not fully align with human judgment standards. For copyright infringement mitigation, the current approaches mainly involve machine unlearning to remove the model's memory of copyright information (Bourtoule et al., 2021;Nguyen et al., 2022;Kumari et al., 2023;Zhang et al., 2024) or deleting duplicated samples from the training data (Webster et al., 2023;Somepalli et al., 2023b\n\n## IP INFRINGEMENT MITIGATION\nCompared to cases of exact replication (memorization), here we consider specific IP infringement, such as imitation of cartoon IPs and artistic elements. Based on whether the input prompt contains direct copyright information, we consider two types of infringement scenarios: explicit infringement and implicit infringement. \n\nExplicit infringement. This refers to prompts that directly contain copyright information, such as \"Generate an image of Mickey Mouse.\" We use the 20 cartoon and artwork samples collected in section 5.1 to generate infringing images using Stable Diffusion v2, where the prompt explicitly includes the names or author names of the work. \n\nImplicit infringement. This occurs when the prompt does not explicitly contain copyright information, but the generated image still infringes due to certain infringing expressions. This type of scenario is more applicable to commercial text-to-image models, as they often include content detection modules that can effectively detect copyrighted information and thus reject the request. In this scenario, we use the same IP samples as above, but generate infringing images without any explicit copyright information using DALL\u2022E 3 (Betker et al., 2023), which has a safety detection module to reject prompts that trigger it. \n\nAutomated attack. Efficiently retrieving or generating infringing prompts has always been a challenge. Kim et al. utilize large models to iteratively generate jailbreak prompts targeting commercial models, thereby inducing them to output copyrighted content. Drawing from it, we use our CopyJudge to generate infringing prompts. In contrast to mitigation, for attack, we only need to use an LVLM to progressively intensify the infringing expressions within the prompt. The prompt is iteratively adjusted, and once the infringement score exceeds 0.8 / 1.0, mitigation is activated, using the current prompt as the starting point. \n\nFor explicit infringement, we validate both prompt control (PC) and latent control (LC). For implicit infringement, due to the commercial model DALL\u2022E's inability to customize latents, we only evaluate prompt control.\n\n## Conclusion\nWe present CopyJudge, an innovative framework for automating the identification of copyright infringement in text-to-image diffusion models. By leveraging abstractionfiltration-comparison test and multi-LVLM debates, our approach could effectively evaluate the substantial similarity between generated and copyrighted images, providing clear and interpretable judgments. Additionally, our LVLM-based mitigation strategy helps avoid infringement by automatically optimizing prompts and exploring non-infringing latent noise vectors, while ensuring that generated images align with the user's requirements. \n\n\u2022 Modifying a Prompt to Improve Similarity Score (Attack Iteration): \"Adjust the parts of the original prompt of the second image that may cause expressions of distinction in the following rationale, making it more similar to the first image to achieve a higher score. Add more information about the [IP type] in Image 1, and provide more unique expressions specific to the [IP type] in Image 1. You can make any changes as long as they improve the similarity score. Require: Source image x cr , generated image x 0 , initial prompt p 0 , control condition p c , LVLM-based prompt modifier \u03c0 p , infringement identification function f , threshold \u03b3, maximum iterations T 1: Initialize prompt: p t \u2190 p 0 , generated image: \n\nGenerate new prompt: p t+1 \u2190 \u03c0 p (x t , x cr , p t , p c , s t , c t , r t ) 8: \n\nGenerate new image using p t+1 : x t+1 \u2190 T2I(p t+1 ) 9: \n\nUpdate iteration counter: t \u2190 t + 1 10: end while 11: Return final prompt p t and generated image x t",
            "reference_string": "[276558342 | Liu et al. | 2025 | Citations: 2]"
        },
        {
            "title": "Copyright Protection in Generative AI: A Technical Perspective",
            "venue": "arXiv.org",
            "year": 2024,
            "reference_count": 176,
            "citation_count": 42,
            "influential_citation_count": 1,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2402.02333, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2256589810",
                    "name": "Jie Ren"
                },
                {
                    "authorId": "2253881697",
                    "name": "Han Xu"
                },
                {
                    "authorId": "2185740224",
                    "name": "Pengfei He"
                },
                {
                    "authorId": "2218740984",
                    "name": "Yingqian Cui"
                },
                {
                    "authorId": "2253682835",
                    "name": "Shenglai Zeng"
                },
                {
                    "authorId": "2282560420",
                    "name": "Jiankun Zhang"
                },
                {
                    "authorId": "2256788829",
                    "name": "Hongzhi Wen"
                },
                {
                    "authorId": "46496977",
                    "name": "Jiayuan Ding"
                },
                {
                    "authorId": "2253533415",
                    "name": "Hui Liu"
                },
                {
                    "authorId": "2267019992",
                    "name": "Yi Chang"
                },
                {
                    "authorId": "2115879611",
                    "name": "Jiliang Tang"
                }
            ],
            "abstract": "Generative AI has witnessed rapid advancement in recent years, expanding their capabilities to create synthesized content such as text, images, audio, and code. The high fidelity and authenticity of contents generated by these Deep Generative Models (DGMs) have sparked significant copyright concerns. There have been various legal debates on how to effectively safeguard copyrights in DGMs. This work delves into this issue by providing a comprehensive overview of copyright protection from a technical perspective. We examine from two distinct viewpoints: the copyrights pertaining to the source data held by the data owners and those of the generative models maintained by the model builders. For data copyright, we delve into methods data owners can protect their content and DGMs can be utilized without infringing upon these rights. For model copyright, our discussion extends to strategies for preventing model theft and identifying outputs generated by specific models. Finally, we highlight the limitations of existing techniques and identify areas that remain unexplored. Furthermore, we discuss prospective directions for the future of copyright protection, underscoring its importance for the sustainable and ethical development of Generative AI.",
            "corpus_id": 267412857,
            "sentences": [
                {
                    "corpus_id": "267412857",
                    "title": "Copyright Protection in Generative AI: A Technical Perspective",
                    "text": "The development of Deep Generative Models (DGMs) marks a noteworthy advancement in image generation. Nevertheless, the impressive quality and authenticity of the generated images, as well as the efficiency in producing new ones, give rise to legitimate concerns regarding copyright matters within the realm of DGMs. \n\nData copyright protection. For the source data owner, which refers to the party or individual who owns the originality of image works, their data can be intentionally or unintentionally collected by model trainers as training samples to construct DGMs as introduced above. For example, recent studies [16,134] have demonstrated that popular DGMs are highly possible to completely replicate their training data samples, which is called memorization. \n\nThe possibility of data replication may severely offend the ownership of the original data samples. Moreover, the development of fine-tuning strategies such as DreamBooth can greatly improve the efficiency for unauthorized parties to directly edit or modify the source data to obtain new samples, which also severely infringes the copyright of the original works. \n\nModel copyright protection. To obtain DGMs with advanced generation performance, it is always necessary for the model trainers to invest a significant amount of funds and labor. It grants them intellectual property rights over the trained model. However, recent works also identify the possibility to steal others models [128].",
                    "score": 0.4793273499897233,
                    "section_title": "Copyright Issues in Image Generation",
                    "char_start_offset": 12077,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 100
                        },
                        {
                            "start": 101,
                            "end": 315
                        },
                        {
                            "start": 318,
                            "end": 344
                        },
                        {
                            "start": 345,
                            "end": 590
                        },
                        {
                            "start": 591,
                            "end": 766
                        },
                        {
                            "start": 769,
                            "end": 868
                        },
                        {
                            "start": 869,
                            "end": 1132
                        },
                        {
                            "start": 1135,
                            "end": 1162
                        },
                        {
                            "start": 1163,
                            "end": 1312
                        },
                        {
                            "start": 1313,
                            "end": 1380
                        },
                        {
                            "start": 1381,
                            "end": 1462
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 1456,
                            "end": 1461,
                            "matchedPaperCorpusId": "2984526"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.99169921875
                }
            ],
            "relevance_judgement": 0.99169921875,
            "relevance_judgment_input_expanded": "# Title: Copyright Protection in Generative AI: A Technical Perspective\n# Venue: arXiv.org\n# Authors: Jie Ren, Han Xu, Pengfei He, Yingqian Cui, Shenglai Zeng, Jiankun Zhang, Hongzhi Wen, Jiayuan Ding, Hui Liu, Yi Chang, Jiliang Tang\n## Abstract\nGenerative AI has witnessed rapid advancement in recent years, expanding their capabilities to create synthesized content such as text, images, audio, and code. The high fidelity and authenticity of contents generated by these Deep Generative Models (DGMs) have sparked significant copyright concerns. There have been various legal debates on how to effectively safeguard copyrights in DGMs. This work delves into this issue by providing a comprehensive overview of copyright protection from a technical perspective. We examine from two distinct viewpoints: the copyrights pertaining to the source data held by the data owners and those of the generative models maintained by the model builders. For data copyright, we delve into methods data owners can protect their content and DGMs can be utilized without infringing upon these rights. For model copyright, our discussion extends to strategies for preventing model theft and identifying outputs generated by specific models. Finally, we highlight the limitations of existing techniques and identify areas that remain unexplored. Furthermore, we discuss prospective directions for the future of copyright protection, underscoring its importance for the sustainable and ethical development of Generative AI.\n## Copyright Issues in Image Generation\nThe development of Deep Generative Models (DGMs) marks a noteworthy advancement in image generation. Nevertheless, the impressive quality and authenticity of the generated images, as well as the efficiency in producing new ones, give rise to legitimate concerns regarding copyright matters within the realm of DGMs. \n\nData copyright protection. For the source data owner, which refers to the party or individual who owns the originality of image works, their data can be intentionally or unintentionally collected by model trainers as training samples to construct DGMs as introduced above. For example, recent studies [16,134] have demonstrated that popular DGMs are highly possible to completely replicate their training data samples, which is called memorization. \n\nThe possibility of data replication may severely offend the ownership of the original data samples. Moreover, the development of fine-tuning strategies such as DreamBooth can greatly improve the efficiency for unauthorized parties to directly edit or modify the source data to obtain new samples, which also severely infringes the copyright of the original works. \n\nModel copyright protection. To obtain DGMs with advanced generation performance, it is always necessary for the model trainers to invest a significant amount of funds and labor. It grants them intellectual property rights over the trained model. However, recent works also identify the possibility to steal others models [128].",
            "reference_string": "[267412857 | Ren et al. | 2024 | Citations: 42]"
        },
        {
            "title": "Enhancing Privacy-Utility Trade-offs to Mitigate Memorization in Diffusion Models",
            "venue": "arXiv.org",
            "year": 2025,
            "reference_count": 35,
            "citation_count": 1,
            "influential_citation_count": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2504.18032, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "1696291",
                    "name": "Chen Chen"
                },
                {
                    "authorId": "51023221",
                    "name": "Daochang Liu"
                },
                {
                    "authorId": "2302950741",
                    "name": "Mubarak Shah"
                },
                {
                    "authorId": "2288626806",
                    "name": "Chang Xu"
                }
            ],
            "abstract": "Text-to-image diffusion models have demonstrated remarkable capabilities in creating images highly aligned with user prompts, yet their proclivity for memorizing training set images has sparked concerns about the originality of the generated images and privacy issues, potentially leading to legal complications for both model owners and users, particularly when the memorized images contain proprietary content. Although methods to mitigate these issues have been suggested, enhancing privacy often results in a significant decrease in the utility of the outputs, as indicated by text-alignment scores. To bridge the research gap, we introduce a novel method, PRSS, which refines the classifier-free guidance approach in diffusion models by integrating prompt re-anchoring (PR) to improve privacy and incorporating semantic prompt search (SS) to enhance utility. Extensive experiments across various privacy levels demonstrate that our approach consistently improves the privacy-utility trade-off, establishing a new state-of-the-art.",
            "corpus_id": 278129333,
            "sentences": [
                {
                    "corpus_id": "278129333",
                    "title": "Enhancing Privacy-Utility Trade-offs to Mitigate Memorization in Diffusion Models",
                    "text": "Leveraging classifier-free guidance (CFG) [12], text-toimage diffusion models like Stable Diffusion [28] and Midjourney [1] are now capable of generating highly realistic images that closely align with user-provided text prompts. This capability has propelled their popularity, leading to widespread use and distribution of their generated images. However, recent research [6,8,31,34] has revealed a critical issue: these models can memorize training data, leading them to reproduce parts of images, such as foregrounds or backgrounds (local memorization, see Fig. 1), or even entire images (global memorization, see Fig. 2) during inference, instead of generating genuinely novel content. When the training data includes sensitive or copyrighted material, these memorization issues can infringe on copyright laws without notifying either the model's owners or users or the copyright holders of the replicated content. The risk is es-pecially substantial given the extensive use of these models and their reliance on massive datasets, such as LAION [30], which contain billions of web-scale images and are impractical to thoroughly review or filter manually. This risk is further highlighted by real-world cases, where several artists have filed lawsuits, arguing that models like Stable Diffusion act as \"21st-century collage tools\" that remix their copyrighted works, implicating Stability AI, DeviantArt, Midjourney, and Runway AI. Recognizing the potential for unauthorized reproductions, Midjourney has even banned prompts containing the term \"Afghan\" to prevent the generation of the copyrighted Afghan Girl photograph. Yet, as [35] demonstrates, such restrictions alone are insufficient to fully prevent the reproduction of copyrighted images. This underscores the urgent need for timely effective mitigation strategies to address these concerns. \n\nIn response to these legal challenges, recent initiatives [7,8,27,32,36] have focused on developing strategies to minimize memorization, achieving notable success. These approaches vary in scope, with some targeting the model's training phase and others making adjustments during inference.",
                    "score": 0.42763016432976286,
                    "section_title": "Introduction",
                    "char_start_offset": 15,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 229
                        },
                        {
                            "start": 230,
                            "end": 347
                        },
                        {
                            "start": 348,
                            "end": 689
                        },
                        {
                            "start": 690,
                            "end": 918
                        },
                        {
                            "start": 919,
                            "end": 1158
                        },
                        {
                            "start": 1159,
                            "end": 1434
                        },
                        {
                            "start": 1435,
                            "end": 1625
                        },
                        {
                            "start": 1626,
                            "end": 1750
                        },
                        {
                            "start": 1751,
                            "end": 1853
                        },
                        {
                            "start": 1856,
                            "end": 2019
                        },
                        {
                            "start": 2020,
                            "end": 2146
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 100,
                            "end": 104,
                            "matchedPaperCorpusId": "245335280"
                        },
                        {
                            "start": 373,
                            "end": 376,
                            "matchedPaperCorpusId": "256389993"
                        },
                        {
                            "start": 378,
                            "end": 381,
                            "matchedPaperCorpusId": "254366634"
                        },
                        {
                            "start": 1634,
                            "end": 1638,
                            "matchedPaperCorpusId": "256627601"
                        },
                        {
                            "start": 1914,
                            "end": 1917,
                            "matchedPaperCorpusId": "268819999"
                        },
                        {
                            "start": 1922,
                            "end": 1925,
                            "matchedPaperCorpusId": "258987384"
                        },
                        {
                            "start": 1925,
                            "end": 1928,
                            "matchedPaperCorpusId": "270309880"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.99169921875
                }
            ],
            "relevance_judgement": 0.99169921875,
            "relevance_judgment_input_expanded": "# Title: Enhancing Privacy-Utility Trade-offs to Mitigate Memorization in Diffusion Models\n# Venue: arXiv.org\n# Authors: Chen Chen, Daochang Liu, Mubarak Shah, Chang Xu\n## Abstract\nText-to-image diffusion models have demonstrated remarkable capabilities in creating images highly aligned with user prompts, yet their proclivity for memorizing training set images has sparked concerns about the originality of the generated images and privacy issues, potentially leading to legal complications for both model owners and users, particularly when the memorized images contain proprietary content. Although methods to mitigate these issues have been suggested, enhancing privacy often results in a significant decrease in the utility of the outputs, as indicated by text-alignment scores. To bridge the research gap, we introduce a novel method, PRSS, which refines the classifier-free guidance approach in diffusion models by integrating prompt re-anchoring (PR) to improve privacy and incorporating semantic prompt search (SS) to enhance utility. Extensive experiments across various privacy levels demonstrate that our approach consistently improves the privacy-utility trade-off, establishing a new state-of-the-art.\n## Introduction\nLeveraging classifier-free guidance (CFG) [12], text-toimage diffusion models like Stable Diffusion [28] and Midjourney [1] are now capable of generating highly realistic images that closely align with user-provided text prompts. This capability has propelled their popularity, leading to widespread use and distribution of their generated images. However, recent research [6,8,31,34] has revealed a critical issue: these models can memorize training data, leading them to reproduce parts of images, such as foregrounds or backgrounds (local memorization, see Fig. 1), or even entire images (global memorization, see Fig. 2) during inference, instead of generating genuinely novel content. When the training data includes sensitive or copyrighted material, these memorization issues can infringe on copyright laws without notifying either the model's owners or users or the copyright holders of the replicated content. The risk is es-pecially substantial given the extensive use of these models and their reliance on massive datasets, such as LAION [30], which contain billions of web-scale images and are impractical to thoroughly review or filter manually. This risk is further highlighted by real-world cases, where several artists have filed lawsuits, arguing that models like Stable Diffusion act as \"21st-century collage tools\" that remix their copyrighted works, implicating Stability AI, DeviantArt, Midjourney, and Runway AI. Recognizing the potential for unauthorized reproductions, Midjourney has even banned prompts containing the term \"Afghan\" to prevent the generation of the copyrighted Afghan Girl photograph. Yet, as [35] demonstrates, such restrictions alone are insufficient to fully prevent the reproduction of copyrighted images. This underscores the urgent need for timely effective mitigation strategies to address these concerns. \n\nIn response to these legal challenges, recent initiatives [7,8,27,32,36] have focused on developing strategies to minimize memorization, achieving notable success. These approaches vary in scope, with some targeting the model's training phase and others making adjustments during inference.",
            "reference_string": "[278129333 | Chen et al. | 2025 | Citations: 1]"
        },
        {
            "title": "VA3: Virtually Assured Amplification Attack on Probabilistic Copyright Protection for Text-to-Image Generative Models",
            "venue": "Computer Vision and Pattern Recognition",
            "year": 2023,
            "reference_count": 60,
            "citation_count": 5,
            "influential_citation_count": 0,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://arxiv.org/pdf/2312.00057",
                "status": "GREEN",
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2312.00057, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2269413961",
                    "name": "Xiang Li"
                },
                {
                    "authorId": "2257038423",
                    "name": "Qianli Shen"
                },
                {
                    "authorId": "2244295298",
                    "name": "Kenji Kawaguchi"
                }
            ],
            "abstract": "The booming use of text-to-image generative models has raised concerns about their high risk of producing copyright-infringing content. While probabilistic copyright protection methods provide a probabilistic guarantee against such infringement, in this paper, we introduce Virtually Assured Amplification Attack (VA3), a novel online attack framework that exposes the vulnerabilities of these protection mechanisms. The proposed framework significantly amplifies the probability of generating infringing content on the sustained interactions with generative models and a non-trivial lower-bound on the success probability of each engagement. Our theoretical and experimental results demonstrate the effectiveness of our approach under various scenarios. These findings highlight the potential risk of implementing probabilistic copyright protection in practical applications of text-to-image generative models. Code is available at https://github.com/South7X/VA3.",
            "corpus_id": 265551515,
            "sentences": [
                {
                    "corpus_id": "265551515",
                    "title": "VA3: Virtually Assured Amplification Attack on Probabilistic Copyright Protection for Text-to-Image Generative Models",
                    "text": "In recent years, the advancement of large generative models [17,47,50] has revolutionized high-quality image synthesis [34,37,40], paving the way for commercial applications that enable the public to effortlessly craft their own artworks and designs [2,13,20,27,38,39]. Nevertheless, these models exhibit notable memorization capabilities to produce generations highly similar to the training data [3]. This resemblance raises growing concerns about copyright infringement, especially when copyrighted data is used for training [12,16,49,52]. \n\nTo address these concerns, there has been a surge in research focused on protecting copyrighted data from potential infringement by outputs of generative models [14, 21, Figure 2. Example outputs given the copyright image in Fig. 1 as target (potential infringing images are marked with red boundaries). In (a), using a benign prompt, we observe a high incidence of infringing content from models without copyright protection (\"w/o CP-k\"). In contrast, (b) shows that after applying the copyright protection mechanism (\"w/ CP-k\"), all samples are safe as CP-k rejects all infringing content. In (c), we find that amplification (Amp.) attack with a benign prompt results in limited success. Notably, by amplification attack with an adversarial prompt obtained from our proposed Anti-NAF algorithm, almost all output in (d) are copyright-infringed. 24,43,45,52,58]. Among these studies, a pivotal concept involves establishing a probabilistic upper-bound against the generation of infringing content by generative models. We refer to this suite of approaches as probabilistic copyright protection. Most notably, Vyas et al. [52] introduce a mathematical definition of copyright known as near-access freeness (NAF). Their method enforces generative diffusion models to exhibit akin behaviors as safe models, which has no access to the copyrighted image. By leveraging the improbability of safe models generating infringing content, the probability of generative models doing the same is thereby substantially reduced.",
                    "score": 0.6153902038492155,
                    "section_title": "Introduction",
                    "char_start_offset": 15,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 269
                        },
                        {
                            "start": 270,
                            "end": 402
                        },
                        {
                            "start": 403,
                            "end": 542
                        },
                        {
                            "start": 545,
                            "end": 848
                        },
                        {
                            "start": 849,
                            "end": 984
                        },
                        {
                            "start": 985,
                            "end": 1136
                        },
                        {
                            "start": 1137,
                            "end": 1178
                        },
                        {
                            "start": 1179,
                            "end": 1234
                        },
                        {
                            "start": 1235,
                            "end": 1391
                        },
                        {
                            "start": 1392,
                            "end": 1408
                        },
                        {
                            "start": 1409,
                            "end": 1564
                        },
                        {
                            "start": 1565,
                            "end": 1640
                        },
                        {
                            "start": 1641,
                            "end": 1757
                        },
                        {
                            "start": 1758,
                            "end": 1895
                        },
                        {
                            "start": 1896,
                            "end": 2059
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 64,
                            "end": 67,
                            "matchedPaperCorpusId": "14888175"
                        },
                        {
                            "start": 67,
                            "end": 70,
                            "matchedPaperCorpusId": "227209335"
                        },
                        {
                            "start": 123,
                            "end": 126,
                            "matchedPaperCorpusId": "245335280"
                        },
                        {
                            "start": 126,
                            "end": 129,
                            "matchedPaperCorpusId": "248986576"
                        },
                        {
                            "start": 250,
                            "end": 253,
                            "matchedPaperCorpusId": "253581213"
                        },
                        {
                            "start": 253,
                            "end": 256,
                            "matchedPaperCorpusId": "251253049"
                        },
                        {
                            "start": 256,
                            "end": 259,
                            "matchedPaperCorpusId": "252918469"
                        },
                        {
                            "start": 259,
                            "end": 262,
                            "matchedPaperCorpusId": "246240274"
                        },
                        {
                            "start": 262,
                            "end": 265,
                            "matchedPaperCorpusId": "251800180"
                        },
                        {
                            "start": 265,
                            "end": 268,
                            "matchedPaperCorpusId": "243938678"
                        },
                        {
                            "start": 398,
                            "end": 401,
                            "matchedPaperCorpusId": "256389993"
                        },
                        {
                            "start": 538,
                            "end": 541,
                            "matchedPaperCorpusId": "257050406"
                        },
                        {
                            "start": 1392,
                            "end": 1395,
                            "matchedPaperCorpusId": "256697414"
                        },
                        {
                            "start": 1395,
                            "end": 1398,
                            "matchedPaperCorpusId": "253420366"
                        },
                        {
                            "start": 1401,
                            "end": 1404,
                            "matchedPaperCorpusId": "257050406"
                        },
                        {
                            "start": 1667,
                            "end": 1671,
                            "matchedPaperCorpusId": "257050406"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.9912109375
                }
            ],
            "relevance_judgement": 0.9912109375,
            "relevance_judgment_input_expanded": "# Title: VA3: Virtually Assured Amplification Attack on Probabilistic Copyright Protection for Text-to-Image Generative Models\n# Venue: Computer Vision and Pattern Recognition\n# Authors: Xiang Li, Qianli Shen, Kenji Kawaguchi\n## Abstract\nThe booming use of text-to-image generative models has raised concerns about their high risk of producing copyright-infringing content. While probabilistic copyright protection methods provide a probabilistic guarantee against such infringement, in this paper, we introduce Virtually Assured Amplification Attack (VA3), a novel online attack framework that exposes the vulnerabilities of these protection mechanisms. The proposed framework significantly amplifies the probability of generating infringing content on the sustained interactions with generative models and a non-trivial lower-bound on the success probability of each engagement. Our theoretical and experimental results demonstrate the effectiveness of our approach under various scenarios. These findings highlight the potential risk of implementing probabilistic copyright protection in practical applications of text-to-image generative models. Code is available at https://github.com/South7X/VA3.\n## Introduction\nIn recent years, the advancement of large generative models [17,47,50] has revolutionized high-quality image synthesis [34,37,40], paving the way for commercial applications that enable the public to effortlessly craft their own artworks and designs [2,13,20,27,38,39]. Nevertheless, these models exhibit notable memorization capabilities to produce generations highly similar to the training data [3]. This resemblance raises growing concerns about copyright infringement, especially when copyrighted data is used for training [12,16,49,52]. \n\nTo address these concerns, there has been a surge in research focused on protecting copyrighted data from potential infringement by outputs of generative models [14, 21, Figure 2. Example outputs given the copyright image in Fig. 1 as target (potential infringing images are marked with red boundaries). In (a), using a benign prompt, we observe a high incidence of infringing content from models without copyright protection (\"w/o CP-k\"). In contrast, (b) shows that after applying the copyright protection mechanism (\"w/ CP-k\"), all samples are safe as CP-k rejects all infringing content. In (c), we find that amplification (Amp.) attack with a benign prompt results in limited success. Notably, by amplification attack with an adversarial prompt obtained from our proposed Anti-NAF algorithm, almost all output in (d) are copyright-infringed. 24,43,45,52,58]. Among these studies, a pivotal concept involves establishing a probabilistic upper-bound against the generation of infringing content by generative models. We refer to this suite of approaches as probabilistic copyright protection. Most notably, Vyas et al. [52] introduce a mathematical definition of copyright known as near-access freeness (NAF). Their method enforces generative diffusion models to exhibit akin behaviors as safe models, which has no access to the copyrighted image. By leveraging the improbability of safe models generating infringing content, the probability of generative models doing the same is thereby substantially reduced.",
            "reference_string": "[265551515 | Li et al. | 2023 | Citations: 5]"
        },
        {
            "title": "AIGC (Artificial Intelligence Generated Content) infringes the copyright of human artists",
            "venue": "Applied and Computational Engineering",
            "year": 2024,
            "reference_count": 12,
            "citation_count": 2,
            "influential_citation_count": 1,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://ace.ewapublishing.org/media/b1f2a30a3af142d0b7d0beb859f92af5.marked.pdf",
                "status": "HYBRID",
                "license": "CCBY",
                "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.54254/2755-2721/34/20230293?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.54254/2755-2721/34/20230293, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2282411198",
                    "name": "Lyulin Zhuang"
                }
            ],
            "abstract": "With the rapid development of artificial intelligence technology, content generated by artificial intelligence has been rapidly applied to people's lives. At the same time, it is also accompanied by many infringement lawsuits, whether AIGC has really caused different degrees of infringement to human artists. Through the analysis of the existing literature on copyright issues and the walkthrough of Stable Diffusion, an AI-generated image platform, this article digs into the main factors that the AI-generated platform causes infringements on human artists. Provide references for using AI by enterprises and related media, and let more scholars pay attention to this issue. The study found that in the workflow of the AI generation platform, taking Stable Diffusion as an example, the two processes of model training and image generation may cause copyright infringement to a certain extent. Based on this, the AI generation platform has unauthorized use of copyright works, excessive plagiarism and adaptation of copyright works, and the generated images are not marked with watermarks or sources, which damages the copyright owner's rights.",
            "corpus_id": 267400526,
            "sentences": [
                {
                    "corpus_id": "267400526",
                    "title": "AIGC (Artificial Intelligence Generated Content) infringes the copyright of human artists",
                    "text": "The participation of this study revolves around the images generated by AIGC. The images generated by AIGC refer to computer-generated images, graphics, or visual content produced through artificial intelligence. These images are typically generated using AI algorithms and deep learning models. They can be completely new images generated from scratch or modified and edited based on existing ones. Taking Stable Diffusion as an example, users can generate images using pure prompts or prompts combined with images. \n\nThe images generated by AIGC can have various forms and styles, ranging from realistic photo compositions to abstract artworks. As Marek J. Magdalena Z. (2023) mentioned, AI-generated images are created by training models on vast amounts of data and using that data to generate novel visual content [1]. As Brownlee J. (2019) published, the algorithms involved in image generation may utilize convolutional neural networks (CNNs), generative adversarial networks (GANs), or other machine learning methods [2]. \n\nAIGC-generated images find application in numerous fields, including visual effects production in the film and gaming industry, artistic creation, product design, and virtual reality. However, due to potential copyright and originality issues associated with AIGC-generated images, their usage and protection have sparked numerous copyright disputes and legal discussions. \n\nDue to the substantial increase in copyright infringement cases involving AIGC-generated images compared to other AIGC-generated content in recent years and the widespread use of AIGC image-generation technology in various industries, the risk of copyright disputes is higher. Furthermore, images are more susceptible to infringement and can be easily identified by copyright holders, compromising their interests. Therefore, it is justified to choose AIGC-generated images as the research subject.",
                    "score": 0.4943845216234633,
                    "section_title": "Participant",
                    "char_start_offset": 4054,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 77
                        },
                        {
                            "start": 78,
                            "end": 212
                        },
                        {
                            "start": 213,
                            "end": 295
                        },
                        {
                            "start": 296,
                            "end": 399
                        },
                        {
                            "start": 400,
                            "end": 516
                        },
                        {
                            "start": 519,
                            "end": 646
                        },
                        {
                            "start": 647,
                            "end": 822
                        },
                        {
                            "start": 823,
                            "end": 1028
                        },
                        {
                            "start": 1031,
                            "end": 1214
                        },
                        {
                            "start": 1215,
                            "end": 1403
                        },
                        {
                            "start": 1406,
                            "end": 1682
                        },
                        {
                            "start": 1683,
                            "end": 1820
                        },
                        {
                            "start": 1821,
                            "end": 1904
                        }
                    ],
                    "ref_mentions": [],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.9912109375
                },
                {
                    "corpus_id": "267400526",
                    "title": "AIGC (Artificial Intelligence Generated Content) infringes the copyright of human artists",
                    "text": "The emergence of artificial intelligence-generated content (AIGC) has brought significant advancements in various fields, such as advertising copywriting, news reporting, and scientific creation. This technology, known as AIGC, has led to reduced labor costs and improved creative efficiency, thereby delivering greater commercial and creative value to businesses and creators. However, controversies surrounding copyright issues and innovative capabilities during the content generation process have become a subject of concern. \n\nAs a generative AI technology for text-to-image generation, the Stable Diffusion technique has garnered global attention. While this technology appears remarkably innovative, it also brings forth certain issues, including copyright concerns. \n\nStable Diffusion can quickly generate high-quality images based on simple text descriptions, thanks to its training on billions of sample images collected online. However, not all of these sample images fall within the public domain; some are even protected by copyright. As one can imagine, the authors and photographers of these copyrighted images are not pleased with the actions of Stable Diffusion. \n\nStable Diffusion has faced multiple lawsuits due to copyright issues. Artists and photographers have initiated collective lawsuits against this technology, and one of the world's leading image suppliers, Getty Images Holdings Inc., has also filed a similar case. According to Getty's (2023) complaint, \"Stability AI has reproduced over 12 million Getty Images, along with their related captions and metadata, without permission or compensation.\" Consequently, delving deeper into this issue and exploring potential solutions is essential. \n\nAIGC model training phase. One critical aspect of using copyrighted works in AIGC is the two-stage process of utilizing these works. To illustrate this process, the Stable Diffusion technique will be used as an example. During the model training phase, copyrighted works are copied from LAION-5B (Large-Scale Artificial Intelligence Online) database and modified before being incorporated into the image information space. This process ensures convenient output for future user consumption. \n\nAIGC model output stage. However, the second stage of output generation raises concerns regarding the potential infringement of copyrighted works. If the generated image is deemed \"substantially similar\" to the original work in expression, it may infringe upon the original work's \"reproduction right.\"",
                    "score": 0.47688708578819816,
                    "section_title": "Introduction",
                    "char_start_offset": 15,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 195
                        },
                        {
                            "start": 196,
                            "end": 377
                        },
                        {
                            "start": 378,
                            "end": 529
                        },
                        {
                            "start": 532,
                            "end": 653
                        },
                        {
                            "start": 654,
                            "end": 773
                        },
                        {
                            "start": 776,
                            "end": 938
                        },
                        {
                            "start": 939,
                            "end": 1047
                        },
                        {
                            "start": 1048,
                            "end": 1179
                        },
                        {
                            "start": 1182,
                            "end": 1251
                        },
                        {
                            "start": 1252,
                            "end": 1444
                        },
                        {
                            "start": 1445,
                            "end": 1627
                        },
                        {
                            "start": 1628,
                            "end": 1720
                        },
                        {
                            "start": 1723,
                            "end": 1749
                        },
                        {
                            "start": 1750,
                            "end": 1855
                        },
                        {
                            "start": 1856,
                            "end": 1942
                        },
                        {
                            "start": 1943,
                            "end": 2145
                        },
                        {
                            "start": 2146,
                            "end": 2213
                        },
                        {
                            "start": 2216,
                            "end": 2240
                        },
                        {
                            "start": 2241,
                            "end": 2362
                        },
                        {
                            "start": 2363,
                            "end": 2518
                        }
                    ],
                    "ref_mentions": [],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.99072265625
                },
                {
                    "corpus_id": "267400526",
                    "title": "AIGC (Artificial Intelligence Generated Content) infringes the copyright of human artists",
                    "text": "After exploring how artificial intelligence (AI)-generated platforms work, learn what processes may involve human artist infringement in the course of their work. So let's talk about where these workflows infringe on human artists' copyrighted works -Artificial intelligence (AI)-generated content raises concerns about copyright infringement of human artist works in several ways. \n\nThe researchers summarized three aspects of human artist infringement: model training using only copyrighted works without permission. Take Stable Diffusion, which has been involved in many copyright disputes. It uses the image database containing hundreds of millions-LAION-5B (Kaixin Zhu, & Yiqun Zhang. ( 2023)) as the source of training data, and it does not need the consent of the copyright owner [5]. \n\nSecond, there is a lot of plagiarism and adaptation of copyrighted works in AI-generated images. In a case in which an artist sued an AI-generated platform, the painter Erin Hanson provided strong evidence that the AI-generated images plagiarized his work. And there are many such lawsuits now, not a few. Third, the images generated by the AI generation platform, such as Stable Diffusion, if works infringe copyright works, do not have any watermarks or source marks, making it even more difficult for copyright owners to accept. Unlike artificial intelligence, most human artists will mark the original author or source when learning to learn from other people's works so as not to infringe the copyright of others. Artificial intelligence often regards works as different data, lacking awe and moral measurement for works. \n\nAll in all, during the generation process of the artificial intelligence generation platform (take Stable Diffusion as an example), it may infringe on the works of human artists from two aspects of model training and image generation, and the generated results are different in three dimensions. Infringements are caused to a certain extent, including unauthorized use, excessive plagiarism and adaptation, and lack of attribution to the original author of the copyrighted work. These are the key factors that constitute the infringement of human artists.",
                    "score": 0.5135825111625341,
                    "section_title": "From what aspects does Stable Diffusion infringe the copyright of human artists",
                    "char_start_offset": 13286,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 162
                        },
                        {
                            "start": 163,
                            "end": 381
                        },
                        {
                            "start": 384,
                            "end": 518
                        },
                        {
                            "start": 519,
                            "end": 593
                        },
                        {
                            "start": 594,
                            "end": 689
                        },
                        {
                            "start": 690,
                            "end": 791
                        },
                        {
                            "start": 794,
                            "end": 890
                        },
                        {
                            "start": 891,
                            "end": 1050
                        },
                        {
                            "start": 1051,
                            "end": 1099
                        },
                        {
                            "start": 1100,
                            "end": 1325
                        },
                        {
                            "start": 1326,
                            "end": 1512
                        },
                        {
                            "start": 1513,
                            "end": 1620
                        },
                        {
                            "start": 1623,
                            "end": 1918
                        },
                        {
                            "start": 1919,
                            "end": 2101
                        },
                        {
                            "start": 2102,
                            "end": 2178
                        }
                    ],
                    "ref_mentions": [],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.984375
                }
            ],
            "relevance_judgement": 0.9912109375,
            "relevance_judgment_input_expanded": "# Title: AIGC (Artificial Intelligence Generated Content) infringes the copyright of human artists\n# Venue: Applied and Computational Engineering\n# Authors: Lyulin Zhuang\n## Abstract\nWith the rapid development of artificial intelligence technology, content generated by artificial intelligence has been rapidly applied to people's lives. At the same time, it is also accompanied by many infringement lawsuits, whether AIGC has really caused different degrees of infringement to human artists. Through the analysis of the existing literature on copyright issues and the walkthrough of Stable Diffusion, an AI-generated image platform, this article digs into the main factors that the AI-generated platform causes infringements on human artists. Provide references for using AI by enterprises and related media, and let more scholars pay attention to this issue. The study found that in the workflow of the AI generation platform, taking Stable Diffusion as an example, the two processes of model training and image generation may cause copyright infringement to a certain extent. Based on this, the AI generation platform has unauthorized use of copyright works, excessive plagiarism and adaptation of copyright works, and the generated images are not marked with watermarks or sources, which damages the copyright owner's rights.\n## Introduction\nThe emergence of artificial intelligence-generated content (AIGC) has brought significant advancements in various fields, such as advertising copywriting, news reporting, and scientific creation. This technology, known as AIGC, has led to reduced labor costs and improved creative efficiency, thereby delivering greater commercial and creative value to businesses and creators. However, controversies surrounding copyright issues and innovative capabilities during the content generation process have become a subject of concern. \n\nAs a generative AI technology for text-to-image generation, the Stable Diffusion technique has garnered global attention. While this technology appears remarkably innovative, it also brings forth certain issues, including copyright concerns. \n\nStable Diffusion can quickly generate high-quality images based on simple text descriptions, thanks to its training on billions of sample images collected online. However, not all of these sample images fall within the public domain; some are even protected by copyright. As one can imagine, the authors and photographers of these copyrighted images are not pleased with the actions of Stable Diffusion. \n\nStable Diffusion has faced multiple lawsuits due to copyright issues. Artists and photographers have initiated collective lawsuits against this technology, and one of the world's leading image suppliers, Getty Images Holdings Inc., has also filed a similar case. According to Getty's (2023) complaint, \"Stability AI has reproduced over 12 million Getty Images, along with their related captions and metadata, without permission or compensation.\" Consequently, delving deeper into this issue and exploring potential solutions is essential. \n\nAIGC model training phase. One critical aspect of using copyrighted works in AIGC is the two-stage process of utilizing these works. To illustrate this process, the Stable Diffusion technique will be used as an example. During the model training phase, copyrighted works are copied from LAION-5B (Large-Scale Artificial Intelligence Online) database and modified before being incorporated into the image information space. This process ensures convenient output for future user consumption. \n\nAIGC model output stage. However, the second stage of output generation raises concerns regarding the potential infringement of copyrighted works. If the generated image is deemed \"substantially similar\" to the original work in expression, it may infringe upon the original work's \"reproduction right.\"\n\n## Participant\nThe participation of this study revolves around the images generated by AIGC. The images generated by AIGC refer to computer-generated images, graphics, or visual content produced through artificial intelligence. These images are typically generated using AI algorithms and deep learning models. They can be completely new images generated from scratch or modified and edited based on existing ones. Taking Stable Diffusion as an example, users can generate images using pure prompts or prompts combined with images. \n\nThe images generated by AIGC can have various forms and styles, ranging from realistic photo compositions to abstract artworks. As Marek J. Magdalena Z. (2023) mentioned, AI-generated images are created by training models on vast amounts of data and using that data to generate novel visual content [1]. As Brownlee J. (2019) published, the algorithms involved in image generation may utilize convolutional neural networks (CNNs), generative adversarial networks (GANs), or other machine learning methods [2]. \n\nAIGC-generated images find application in numerous fields, including visual effects production in the film and gaming industry, artistic creation, product design, and virtual reality. However, due to potential copyright and originality issues associated with AIGC-generated images, their usage and protection have sparked numerous copyright disputes and legal discussions. \n\nDue to the substantial increase in copyright infringement cases involving AIGC-generated images compared to other AIGC-generated content in recent years and the widespread use of AIGC image-generation technology in various industries, the risk of copyright disputes is higher. Furthermore, images are more susceptible to infringement and can be easily identified by copyright holders, compromising their interests. Therefore, it is justified to choose AIGC-generated images as the research subject.\n\n## From what aspects does Stable Diffusion infringe the copyright of human artists\nAfter exploring how artificial intelligence (AI)-generated platforms work, learn what processes may involve human artist infringement in the course of their work. So let's talk about where these workflows infringe on human artists' copyrighted works -Artificial intelligence (AI)-generated content raises concerns about copyright infringement of human artist works in several ways. \n\nThe researchers summarized three aspects of human artist infringement: model training using only copyrighted works without permission. Take Stable Diffusion, which has been involved in many copyright disputes. It uses the image database containing hundreds of millions-LAION-5B (Kaixin Zhu, & Yiqun Zhang. ( 2023)) as the source of training data, and it does not need the consent of the copyright owner [5]. \n\nSecond, there is a lot of plagiarism and adaptation of copyrighted works in AI-generated images. In a case in which an artist sued an AI-generated platform, the painter Erin Hanson provided strong evidence that the AI-generated images plagiarized his work. And there are many such lawsuits now, not a few. Third, the images generated by the AI generation platform, such as Stable Diffusion, if works infringe copyright works, do not have any watermarks or source marks, making it even more difficult for copyright owners to accept. Unlike artificial intelligence, most human artists will mark the original author or source when learning to learn from other people's works so as not to infringe the copyright of others. Artificial intelligence often regards works as different data, lacking awe and moral measurement for works. \n\nAll in all, during the generation process of the artificial intelligence generation platform (take Stable Diffusion as an example), it may infringe on the works of human artists from two aspects of model training and image generation, and the generated results are different in three dimensions. Infringements are caused to a certain extent, including unauthorized use, excessive plagiarism and adaptation, and lack of attribution to the original author of the copyrighted work. These are the key factors that constitute the infringement of human artists.",
            "reference_string": "[267400526 | Zhuang | 2024 | Citations: 2]"
        },
        {
            "title": "A Dataset and Benchmark for Copyright Infringement Unlearning from Text-to-Image Diffusion Models",
            "venue": "",
            "year": 2024,
            "reference_count": 48,
            "citation_count": 2,
            "influential_citation_count": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2403.12052, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2210435658",
                    "name": "Rui Ma"
                },
                {
                    "authorId": "2257338567",
                    "name": "Qiang Zhou"
                },
                {
                    "authorId": "2268733263",
                    "name": "Yizhu Jin"
                },
                {
                    "authorId": "2292161412",
                    "name": "Daquan Zhou"
                },
                {
                    "authorId": "2292177829",
                    "name": "Bangjun Xiao"
                },
                {
                    "authorId": "2292217065",
                    "name": "Xiuyu Li"
                },
                {
                    "authorId": "2292690089",
                    "name": "Yi Qu"
                },
                {
                    "authorId": "2261448160",
                    "name": "Aishani Singh"
                },
                {
                    "authorId": "2242659602",
                    "name": "Kurt Keutzer"
                },
                {
                    "authorId": "2328473437",
                    "name": "Jingtong Hu"
                },
                {
                    "authorId": "2307915260",
                    "name": "Xiaodong Xie"
                },
                {
                    "authorId": "2293731776",
                    "name": "Zhen Dong"
                },
                {
                    "authorId": "2257020214",
                    "name": "Shanghang Zhang"
                },
                {
                    "authorId": "2275298300",
                    "name": "Shiji Zhou"
                }
            ],
            "abstract": "Copyright law confers upon creators the exclusive rights to reproduce, distribute, and monetize their creative works. However, recent progress in text-to-image generation has introduced formidable challenges to copyright enforcement. These technologies enable the unauthorized learning and replication of copyrighted content, artistic creations, and likenesses, leading to the proliferation of unregulated content. Notably, models like stable diffusion, which excel in text-to-image synthesis, heighten the risk of copyright infringement and unauthorized distribution.Machine unlearning, which seeks to eradicate the influence of specific data or concepts from machine learning models, emerges as a promising solution by eliminating the \\enquote{copyright memories} ingrained in diffusion models. Yet, the absence of comprehensive large-scale datasets and standardized benchmarks for evaluating the efficacy of unlearning techniques in the copyright protection scenarios impedes the development of more effective unlearning methods. To address this gap, we introduce a novel pipeline that harmonizes CLIP, ChatGPT, and diffusion models to curate a dataset. This dataset encompasses anchor images, associated prompts, and images synthesized by text-to-image models. Additionally, we have developed a mixed metric based on semantic and style information, validated through both human and artist assessments, to gauge the effectiveness of unlearning approaches. Our dataset, benchmark library, and evaluation metrics will be made publicly available to foster future research and practical applications (https://rmpku.github.io/CPDM-page/, website / http://149.104.22.83/unlearning.tar.gz, dataset).",
            "corpus_id": 268532352,
            "sentences": [
                {
                    "corpus_id": "268532352",
                    "title": "A Dataset and Benchmark for Copyright Infringement Unlearning from Text-to-Image Diffusion Models",
                    "text": "Text-to-image generative models have recently emerged as a significant topic in computer vision, demonstrating remarkable results in the area of generative modeling [14,35].These models bridge the gap between language and visual contents by generating realistic images from textual descriptions.However, rapid advancements in text-to-image generation techniques have raised concerns about copyright protection, particularly unauthorized reproduction of content, artistic creations, and portraits [4].A specific concern arises from the use of Stable Diffusion (SD), a state-of-the-art text-conditional latent diffusion model, which has sparked global discussions on copyright.\n\nMachine Unlearning (MU), which aims to eliminate the influence of specific target data or concepts, presents a promising solution to the aforementioned challenges.Several studies have provided intuitive evidence of MU's effectiveness in erasing the memory of copyrighted material from original models [42,7,26,8].However, the current body of research is limited by a lack of quantitative and systematic assessments that evaluate the extent to which MU can reduce the risk of copyright infringement.This limitation hinders the ability to make meaningful comparisons between existing MU approaches.The challenge is exacerbated by the inherent complexity of defining copyright infringement criteria for text-to-image generative models, as well as the scarcity of comprehensive inference datasets and standardized benchmarks for assessing copyright infringement.The absence of extensive copyright datasets obstructs researchers' efforts to fully comprehend the copyright infringement risks associated with generative models.This, in turn, restricts their capacity to develop superior MU algorithms capable of addressing the legal risks effectively.As shown in Fig. 1, datasets and benchmarks are essential for forgetting copyrighted content and evaluating unlearning methods.\n\nInitially, it is crucial to define what constitutes copyright infringement in contents produced by text-to-image generative models [38].In this study, we focus on infringement within 2D artistic works.Drawing on expertise from copyright protection specialists, including artists and lawyers, we contend that a unique painting style of an artist, virtual representations in artistic creations, and individual portraits all represent forms of creative expression deserving of legal protection.In order to identify instances of infringement in these contexts, a comprehensive analysis encompassing both technical and semantic aspects of the generated contents is indispensable.",
                    "score": 0.625394950512463,
                    "section_title": "Introduction",
                    "char_start_offset": 15,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 173
                        },
                        {
                            "start": 173,
                            "end": 295
                        },
                        {
                            "start": 295,
                            "end": 500
                        },
                        {
                            "start": 500,
                            "end": 675
                        },
                        {
                            "start": 677,
                            "end": 840
                        },
                        {
                            "start": 840,
                            "end": 990
                        },
                        {
                            "start": 990,
                            "end": 1175
                        },
                        {
                            "start": 1175,
                            "end": 1273
                        },
                        {
                            "start": 1273,
                            "end": 1535
                        },
                        {
                            "start": 1535,
                            "end": 1697
                        },
                        {
                            "start": 1697,
                            "end": 1821
                        },
                        {
                            "start": 1821,
                            "end": 1948
                        },
                        {
                            "start": 1950,
                            "end": 2086
                        },
                        {
                            "start": 2086,
                            "end": 2151
                        },
                        {
                            "start": 2151,
                            "end": 2441
                        },
                        {
                            "start": 2441,
                            "end": 2624
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 165,
                            "end": 169,
                            "matchedPaperCorpusId": "1033682"
                        },
                        {
                            "start": 169,
                            "end": 172,
                            "matchedPaperCorpusId": "245335280"
                        },
                        {
                            "start": 984,
                            "end": 987,
                            "matchedPaperCorpusId": "257687839"
                        },
                        {
                            "start": 987,
                            "end": 989,
                            "matchedPaperCorpusId": "261276613"
                        },
                        {
                            "start": 2081,
                            "end": 2085,
                            "matchedPaperCorpusId": "254366634"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.99072265625
                },
                {
                    "corpus_id": "268532352",
                    "title": "A Dataset and Benchmark for Copyright Infringement Unlearning from Text-to-Image Diffusion Models",
                    "text": "In order to identify instances of infringement in these contexts, a comprehensive analysis encompassing both technical and semantic aspects of the generated contents is indispensable.\n\nIn light of the aforementioned considerations, we introduce the first comprehensive dataset tailored for copyright protection in this domain: Copyright Protection from Diffusion Model (CPDM) dataset.Specifically, we have curated copyright images from four distinct categories that are most commonly suspected of infringement, as demonstrated in Fig. 2.This dataset encompasses a collection of original copyright images, associated prompts for text-to-image generation via Stable Diffusion, and indicatives for a series of features, represented by: 1) Potential for Infringement; 2) Effectiveness of Unlearning; 3) Extent of Model Degradation during Unlearning.\n\nPotential for Infringement.This is quantified by feature-level similarities between the original copyright images and potentially infringing generated contents, denoted as CPDM metric (CM).",
                    "score": 0.4786603389388866,
                    "section_title": "Introduction",
                    "char_start_offset": 2456,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 183
                        },
                        {
                            "start": 185,
                            "end": 384
                        },
                        {
                            "start": 384,
                            "end": 537
                        },
                        {
                            "start": 537,
                            "end": 845
                        },
                        {
                            "start": 847,
                            "end": 874
                        },
                        {
                            "start": 874,
                            "end": 1036
                        }
                    ],
                    "ref_mentions": [],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.9833984375
                }
            ],
            "relevance_judgement": 0.99072265625,
            "relevance_judgment_input_expanded": "# Title: A Dataset and Benchmark for Copyright Infringement Unlearning from Text-to-Image Diffusion Models\n# Venue: \n# Authors: Rui Ma, Qiang Zhou, Yizhu Jin, Daquan Zhou, Bangjun Xiao, Xiuyu Li, Yi Qu, Aishani Singh, Kurt Keutzer, Jingtong Hu, Xiaodong Xie, Zhen Dong, Shanghang Zhang, Shiji Zhou\n## Abstract\nCopyright law confers upon creators the exclusive rights to reproduce, distribute, and monetize their creative works. However, recent progress in text-to-image generation has introduced formidable challenges to copyright enforcement. These technologies enable the unauthorized learning and replication of copyrighted content, artistic creations, and likenesses, leading to the proliferation of unregulated content. Notably, models like stable diffusion, which excel in text-to-image synthesis, heighten the risk of copyright infringement and unauthorized distribution.Machine unlearning, which seeks to eradicate the influence of specific data or concepts from machine learning models, emerges as a promising solution by eliminating the \\enquote{copyright memories} ingrained in diffusion models. Yet, the absence of comprehensive large-scale datasets and standardized benchmarks for evaluating the efficacy of unlearning techniques in the copyright protection scenarios impedes the development of more effective unlearning methods. To address this gap, we introduce a novel pipeline that harmonizes CLIP, ChatGPT, and diffusion models to curate a dataset. This dataset encompasses anchor images, associated prompts, and images synthesized by text-to-image models. Additionally, we have developed a mixed metric based on semantic and style information, validated through both human and artist assessments, to gauge the effectiveness of unlearning approaches. Our dataset, benchmark library, and evaluation metrics will be made publicly available to foster future research and practical applications (https://rmpku.github.io/CPDM-page/, website / http://149.104.22.83/unlearning.tar.gz, dataset).\n## Introduction\nText-to-image generative models have recently emerged as a significant topic in computer vision, demonstrating remarkable results in the area of generative modeling [14,35].These models bridge the gap between language and visual contents by generating realistic images from textual descriptions.However, rapid advancements in text-to-image generation techniques have raised concerns about copyright protection, particularly unauthorized reproduction of content, artistic creations, and portraits [4].A specific concern arises from the use of Stable Diffusion (SD), a state-of-the-art text-conditional latent diffusion model, which has sparked global discussions on copyright.\n\nMachine Unlearning (MU), which aims to eliminate the influence of specific target data or concepts, presents a promising solution to the aforementioned challenges.Several studies have provided intuitive evidence of MU's effectiveness in erasing the memory of copyrighted material from original models [42,7,26,8].However, the current body of research is limited by a lack of quantitative and systematic assessments that evaluate the extent to which MU can reduce the risk of copyright infringement.This limitation hinders the ability to make meaningful comparisons between existing MU approaches.The challenge is exacerbated by the inherent complexity of defining copyright infringement criteria for text-to-image generative models, as well as the scarcity of comprehensive inference datasets and standardized benchmarks for assessing copyright infringement.The absence of extensive copyright datasets obstructs researchers' efforts to fully comprehend the copyright infringement risks associated with generative models.This, in turn, restricts their capacity to develop superior MU algorithms capable of addressing the legal risks effectively.As shown in Fig. 1, datasets and benchmarks are essential for forgetting copyrighted content and evaluating unlearning methods.\n\nInitially, it is crucial to define what constitutes copyright infringement in contents produced by text-to-image generative models [38].In this study, we focus on infringement within 2D artistic works.Drawing on expertise from copyright protection specialists, including artists and lawyers, we contend that a unique painting style of an artist, virtual representations in artistic creations, and individual portraits all represent forms of creative expression deserving of legal protection.In order to identify instances of infringement in these contexts, a comprehensive analysis encompassing both technical and semantic aspects of the generated contents is indispensable.\n...\nIn order to identify instances of infringement in these contexts, a comprehensive analysis encompassing both technical and semantic aspects of the generated contents is indispensable.\n\nIn light of the aforementioned considerations, we introduce the first comprehensive dataset tailored for copyright protection in this domain: Copyright Protection from Diffusion Model (CPDM) dataset.Specifically, we have curated copyright images from four distinct categories that are most commonly suspected of infringement, as demonstrated in Fig. 2.This dataset encompasses a collection of original copyright images, associated prompts for text-to-image generation via Stable Diffusion, and indicatives for a series of features, represented by: 1) Potential for Infringement; 2) Effectiveness of Unlearning; 3) Extent of Model Degradation during Unlearning.\n\nPotential for Infringement.This is quantified by feature-level similarities between the original copyright images and potentially infringing generated contents, denoted as CPDM metric (CM).",
            "reference_string": "[268532352 | Ma et al. | 2024 | Citations: 2]"
        },
        {
            "title": "DFA-CON: A Contrastive Learning Approach for Detecting Copyright Infringement in DeepFake Art",
            "venue": "",
            "year": 2025,
            "reference_count": 14,
            "citation_count": 0,
            "influential_citation_count": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2505.08552, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2203408377",
                    "name": "Haroon Wahab"
                },
                {
                    "authorId": "2256911492",
                    "name": "H. Ugail"
                },
                {
                    "authorId": "2313956696",
                    "name": "Irfan Mehmood"
                }
            ],
            "abstract": "Recent proliferation of generative AI tools for visual content creation-particularly in the context of visual artworks-has raised serious concerns about copyright infringement and forgery. The large-scale datasets used to train these models often contain a mixture of copyrighted and non-copyrighted artworks. Given the tendency of generative models to memorize training patterns, they are susceptible to varying degrees of copyright violation. Building on the recently proposed DeepfakeArt Challenge benchmark, this work introduces DFA-CON, a contrastive learning framework designed to detect copyright-infringing or forged AI-generated art. DFA-CON learns a discriminative representation space, posing affinity among original artworks and their forged counterparts within a contrastive learning framework. The model is trained across multiple attack types, including inpainting, style transfer, adversarial perturbation, and cutmix. Evaluation results demonstrate robust detection performance across most attack types, outperforming recent pretrained foundation models. Code and model checkpoints will be released publicly upon acceptance.",
            "corpus_id": 278534729,
            "sentences": [
                {
                    "corpus_id": "278534729",
                    "title": "DFA-CON: A Contrastive Learning Approach for Detecting Copyright Infringement in DeepFake Art",
                    "text": "The growing availability of generative AI tools for visual content creation has raised critical concerns around copyright infringement, especially in the domain of visual artworks [1]. Generative models trained on large-scale, web-scraped datasets often absorb patterns from both copyrighted and public domain images, making them prone to reproducing unauthorized content [2]. This phenomenon is particularly problematic in the context of AI-generated art, where stylistic and structural similarities to original artworks may constitute legal or ethical violations [3]. Given the ease with which forged or derivative artworks can be created and distributed, there is a compelling need for automated methods to assess the originality and attribution of generative outputs. \n\nTo study and benchmark the detection of AI-generated art forgeries, the recently proposed DeepfakeArt Challenge [4] provides a comprehensive dataset comprising over 32,000 image pairs spanning a variety of generative manipulation techniques. Each entry in the dataset consists of a pair of images-either a forged/generated version of an original artwork or two dissimilar, unrelated images. The manipulated images cover several attack types including inpainting, style transfer, adversarial perturbation, and cutmix, simulating realistic scenarios of content misuse. This dataset enables the development of algorithms that go beyond pixel-level artifact detection, focusing instead on semantic similarity and visual attribution, a crucial capability when detecting copyright violations in generative art. \n\nTo address this problem, we propose DFA-CON, a supervised contrastive learning framework designed to detect copyright-infringing or forged art generated by AI models. Rather than treating forgery detection as a traditional classification problem, DFA-CON learns an embedding space that encodes semantic similarity between original and manipulated images. Using supervised contrastive loss, the model is trained to pose affinity among original artworks and their forged counterparts, while distancing unrelated images within the same batch. This representation-level formulation allows for greater generalization across manipulation types and better discrimination of subtle, high-quality forgeries, which are common in the domain of AI-generated art. \n\nOur main contributions are summarized as follows: \n\n\u2022 We propose a supervised contrastive learning framework, DFA-CON, for training visual encoders to detect copyright infringement in DeepFake art.",
                    "score": 0.4633429615380903,
                    "section_title": "INTRODUCTION",
                    "char_start_offset": 15,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 184
                        },
                        {
                            "start": 185,
                            "end": 376
                        },
                        {
                            "start": 377,
                            "end": 569
                        },
                        {
                            "start": 570,
                            "end": 771
                        },
                        {
                            "start": 774,
                            "end": 1015
                        },
                        {
                            "start": 1016,
                            "end": 1164
                        },
                        {
                            "start": 1165,
                            "end": 1340
                        },
                        {
                            "start": 1341,
                            "end": 1578
                        },
                        {
                            "start": 1581,
                            "end": 1747
                        },
                        {
                            "start": 1748,
                            "end": 1935
                        },
                        {
                            "start": 1936,
                            "end": 2120
                        },
                        {
                            "start": 2121,
                            "end": 2331
                        },
                        {
                            "start": 2334,
                            "end": 2383
                        },
                        {
                            "start": 2386,
                            "end": 2531
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 180,
                            "end": 183,
                            "matchedPaperCorpusId": "261279983"
                        },
                        {
                            "start": 372,
                            "end": 375,
                            "matchedPaperCorpusId": "235359053"
                        },
                        {
                            "start": 565,
                            "end": 568,
                            "matchedPaperCorpusId": "254366634"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.99072265625
                },
                {
                    "corpus_id": "278534729",
                    "title": "DFA-CON: A Contrastive Learning Approach for Detecting Copyright Infringement in DeepFake Art",
                    "text": "Deep Fake art generative models are vulnerable to violating copyright terms by producing images that mimic or closely resemble content protected under copyright [10]. A formal mathematical formulation of copyright infringement in this context is introduced in [4]. For clarity and contextual relevance, we present a simplified version here: \n\nHere, y denotes the generated image and x is a potentially copyright-protected training image. The operator T (\u2022) represents a geometric transformation (e.g., resizing, flipping, rotation), and \u2126 is a region of significant size within the image. The function A(\u2022) defines the domain of representation-either raw pixel space or an edge-based representation. The term f (|\u2126|) is a monotonic function adjusting sensitivity based on region size, and \u03b4 is a fixed similarity threshold. Infringement is said to occur if the distance in the representation space for any region \u2126 falls below this threshold.",
                    "score": 0.5409420964778178,
                    "section_title": "Copyright Infringement in Art",
                    "char_start_offset": 5336,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 166
                        },
                        {
                            "start": 167,
                            "end": 264
                        },
                        {
                            "start": 265,
                            "end": 340
                        },
                        {
                            "start": 343,
                            "end": 437
                        },
                        {
                            "start": 438,
                            "end": 588
                        },
                        {
                            "start": 589,
                            "end": 699
                        },
                        {
                            "start": 700,
                            "end": 823
                        },
                        {
                            "start": 824,
                            "end": 942
                        }
                    ],
                    "ref_mentions": [],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.98974609375
                }
            ],
            "relevance_judgement": 0.99072265625,
            "relevance_judgment_input_expanded": "# Title: DFA-CON: A Contrastive Learning Approach for Detecting Copyright Infringement in DeepFake Art\n# Venue: \n# Authors: Haroon Wahab, H. Ugail, Irfan Mehmood\n## Abstract\nRecent proliferation of generative AI tools for visual content creation-particularly in the context of visual artworks-has raised serious concerns about copyright infringement and forgery. The large-scale datasets used to train these models often contain a mixture of copyrighted and non-copyrighted artworks. Given the tendency of generative models to memorize training patterns, they are susceptible to varying degrees of copyright violation. Building on the recently proposed DeepfakeArt Challenge benchmark, this work introduces DFA-CON, a contrastive learning framework designed to detect copyright-infringing or forged AI-generated art. DFA-CON learns a discriminative representation space, posing affinity among original artworks and their forged counterparts within a contrastive learning framework. The model is trained across multiple attack types, including inpainting, style transfer, adversarial perturbation, and cutmix. Evaluation results demonstrate robust detection performance across most attack types, outperforming recent pretrained foundation models. Code and model checkpoints will be released publicly upon acceptance.\n## INTRODUCTION\nThe growing availability of generative AI tools for visual content creation has raised critical concerns around copyright infringement, especially in the domain of visual artworks [1]. Generative models trained on large-scale, web-scraped datasets often absorb patterns from both copyrighted and public domain images, making them prone to reproducing unauthorized content [2]. This phenomenon is particularly problematic in the context of AI-generated art, where stylistic and structural similarities to original artworks may constitute legal or ethical violations [3]. Given the ease with which forged or derivative artworks can be created and distributed, there is a compelling need for automated methods to assess the originality and attribution of generative outputs. \n\nTo study and benchmark the detection of AI-generated art forgeries, the recently proposed DeepfakeArt Challenge [4] provides a comprehensive dataset comprising over 32,000 image pairs spanning a variety of generative manipulation techniques. Each entry in the dataset consists of a pair of images-either a forged/generated version of an original artwork or two dissimilar, unrelated images. The manipulated images cover several attack types including inpainting, style transfer, adversarial perturbation, and cutmix, simulating realistic scenarios of content misuse. This dataset enables the development of algorithms that go beyond pixel-level artifact detection, focusing instead on semantic similarity and visual attribution, a crucial capability when detecting copyright violations in generative art. \n\nTo address this problem, we propose DFA-CON, a supervised contrastive learning framework designed to detect copyright-infringing or forged art generated by AI models. Rather than treating forgery detection as a traditional classification problem, DFA-CON learns an embedding space that encodes semantic similarity between original and manipulated images. Using supervised contrastive loss, the model is trained to pose affinity among original artworks and their forged counterparts, while distancing unrelated images within the same batch. This representation-level formulation allows for greater generalization across manipulation types and better discrimination of subtle, high-quality forgeries, which are common in the domain of AI-generated art. \n\nOur main contributions are summarized as follows: \n\n\u2022 We propose a supervised contrastive learning framework, DFA-CON, for training visual encoders to detect copyright infringement in DeepFake art.\n\n## Copyright Infringement in Art\nDeep Fake art generative models are vulnerable to violating copyright terms by producing images that mimic or closely resemble content protected under copyright [10]. A formal mathematical formulation of copyright infringement in this context is introduced in [4]. For clarity and contextual relevance, we present a simplified version here: \n\nHere, y denotes the generated image and x is a potentially copyright-protected training image. The operator T (\u2022) represents a geometric transformation (e.g., resizing, flipping, rotation), and \u2126 is a region of significant size within the image. The function A(\u2022) defines the domain of representation-either raw pixel space or an edge-based representation. The term f (|\u2126|) is a monotonic function adjusting sensitivity based on region size, and \u03b4 is a fixed similarity threshold. Infringement is said to occur if the distance in the representation space for any region \u2126 falls below this threshold.",
            "reference_string": "[278534729 | Wahab et al. | 2025 | Citations: 0]"
        },
        {
            "title": "On Provable Copyright Protection for Generative Models",
            "venue": "International Conference on Machine Learning",
            "year": 2023,
            "reference_count": 40,
            "citation_count": 94,
            "influential_citation_count": 14,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2302.10870, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "145603901",
                    "name": "Nikhil Vyas"
                },
                {
                    "authorId": "144695232",
                    "name": "S. Kakade"
                },
                {
                    "authorId": "1697211",
                    "name": "B. Barak"
                }
            ],
            "abstract": "There is a growing concern that learned conditional generative models may output samples that are substantially similar to some copyrighted data $C$ that was in their training set. We give a formal definition of $\\textit{near access-freeness (NAF)}$ and prove bounds on the probability that a model satisfying this definition outputs a sample similar to $C$, even if $C$ is included in its training set. Roughly speaking, a generative model $p$ is $\\textit{$k$-NAF}$ if for every potentially copyrighted data $C$, the output of $p$ diverges by at most $k$-bits from the output of a model $q$ that $\\textit{did not access $C$ at all}$. We also give generative model learning algorithms, which efficiently modify the original generative model learning algorithm in a black box manner, that output generative models with strong bounds on the probability of sampling protected content. Furthermore, we provide promising experiments for both language (transformers) and image (diffusion) generative models, showing minimal degradation in output quality while ensuring strong protections against sampling protected content.",
            "corpus_id": 257050406,
            "sentences": [
                {
                    "corpus_id": "257050406",
                    "title": "On Provable Copyright Protection for Generative Models",
                    "text": "Generative models for images, text, code, and other domains pose new challenges for ensuring their outputs are protected from copyright infringement. Such models are trained on a large corpus of data, where it is often impractical to ensure the training set is 100% free of copyrighted material. Furthermore, removing copyrighted material from training may also be undesirable. For example, a human author is free to read and use copyrighted material as inspiration for their work, as long as they do not copy it. Similarly, it may be beneficial to use copyrighted material when training in order to have more effective generative models. \n\nCopyright infringement by generative models can potentially arise in (at least) two manners. First, in the training phase, the algorithm could directly access copyrighted material, and the learned model itself could implicitly contain (e.g. coded in its weights) verbatim copies of some of this material. The copyright issues arising during training share many similarities with other settings in which algorithms scrape Figure 1: The CP-k Algorithm applied to diffusion models. The dataset is CIFAR-10 augmented with multiple copies of two images (images close to the augmented images are marked with red boundaries); hypothetically, suppose these two images are copyrighted works. The leftmost image shows generations from a model p that was trained on the full dataset, where we clearly see that p generates the two copyrighted works. Our algorithm starts by splitting this dataset into two disjoint datasets, making sure that copyrighted images are split into two different shards; for illustrative purposes, we do not deduplicate the dataset. The procedure then trains two models q1, q2 on these disjoint shards. The middle two figures show samples from the models q1, q2, again clearly showing memorization. However, note that q1 does not generate one of the copyrighted images and and q2 does not generate the other copyrighted image (as these were not in their respective datasets). Our algorithm CP-k then uses q1, q2, along with the original model p, to construct a model p k which has strong copyright protection guarantees.",
                    "score": 0.5970721287383962,
                    "section_title": "Introduction",
                    "char_start_offset": 15,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 149
                        },
                        {
                            "start": 150,
                            "end": 295
                        },
                        {
                            "start": 296,
                            "end": 377
                        },
                        {
                            "start": 378,
                            "end": 513
                        },
                        {
                            "start": 514,
                            "end": 638
                        },
                        {
                            "start": 641,
                            "end": 733
                        },
                        {
                            "start": 734,
                            "end": 881
                        },
                        {
                            "start": 882,
                            "end": 945
                        },
                        {
                            "start": 946,
                            "end": 1119
                        },
                        {
                            "start": 1120,
                            "end": 1323
                        },
                        {
                            "start": 1324,
                            "end": 1478
                        },
                        {
                            "start": 1479,
                            "end": 1688
                        },
                        {
                            "start": 1689,
                            "end": 1758
                        },
                        {
                            "start": 1759,
                            "end": 1854
                        },
                        {
                            "start": 1855,
                            "end": 2031
                        },
                        {
                            "start": 2032,
                            "end": 2176
                        }
                    ],
                    "ref_mentions": [],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.9892578125
                },
                {
                    "corpus_id": "257050406",
                    "title": "On Provable Copyright Protection for Generative Models",
                    "text": "Our algorithm CP-k then uses q1, q2, along with the original model p, to construct a model p k which has strong copyright protection guarantees. The last image is the outputs of p k , showing it is highly unlikely to output either of the copyrighted images, even though each of q1, q2 and p has memorized some of these images. See Section 4 for more details (and for a discussion with regards to our displayed model generations having used the same noise on the diffusion paths). \n\nsignificant amounts of data, including search-engine indexing and digitizing books. Here, the question of what constitutes a copyright infringement is largely a question of \"fair use.\" This work does not examine these fair use issues that arise in the training phase, and we refer the reader to the several legal precedents in this area [Samuelson, 2021]. \n\nThe second notable source of potential infringement is in the deployment phase, where a user provides a prompt x to the model to obtain some output y. Apriori, we cannot rule out the possibility that y is either a verbatim copy or substantially similar to some copyrighted training data. Moreover, unlike search engines, generative models do not keep track of the provenance of their outputs. Hence, a user of such an output y (e.g., a software company using generated code, or a designer using a generated image) has no easy way to verify that it does not infringe upon any copyrighted material. It is this issue of preventing deployment-time copyright infringement that is the focus of this work. \n\nOur contributions. We give a formal definition -\"near-access freeness\" -bounding the extent to which a learned generative model's output can be substantially influenced by a particular piece of copyrighted data that the model was trained on. We also give a procedure that transforms (under certain assumptions) any generative model learning algorithm A into an algorithm A k , which protects against violations under our definition. In particular, the model output by A k will (1) be at most k-bits far from a \"safe\" model (which is not committing copyright infringement), and (2) will have performance reasonably close to the model output by the original algorithm A (in a quantifiable sense, based on properties of A).",
                    "score": 0.5863183564225078,
                    "section_title": "Introduction",
                    "char_start_offset": 2047,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 144
                        },
                        {
                            "start": 145,
                            "end": 326
                        },
                        {
                            "start": 327,
                            "end": 479
                        },
                        {
                            "start": 482,
                            "end": 565
                        },
                        {
                            "start": 566,
                            "end": 666
                        },
                        {
                            "start": 667,
                            "end": 837
                        },
                        {
                            "start": 840,
                            "end": 990
                        },
                        {
                            "start": 991,
                            "end": 1127
                        },
                        {
                            "start": 1128,
                            "end": 1232
                        },
                        {
                            "start": 1233,
                            "end": 1436
                        },
                        {
                            "start": 1437,
                            "end": 1538
                        },
                        {
                            "start": 1541,
                            "end": 1559
                        },
                        {
                            "start": 1560,
                            "end": 1782
                        },
                        {
                            "start": 1783,
                            "end": 1973
                        },
                        {
                            "start": 1974,
                            "end": 2261
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 819,
                            "end": 836,
                            "matchedPaperCorpusId": "239770285"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.9853515625
                }
            ],
            "relevance_judgement": 0.9892578125,
            "relevance_judgment_input_expanded": "# Title: On Provable Copyright Protection for Generative Models\n# Venue: International Conference on Machine Learning\n# Authors: Nikhil Vyas, S. Kakade, B. Barak\n## Abstract\nThere is a growing concern that learned conditional generative models may output samples that are substantially similar to some copyrighted data $C$ that was in their training set. We give a formal definition of $\\textit{near access-freeness (NAF)}$ and prove bounds on the probability that a model satisfying this definition outputs a sample similar to $C$, even if $C$ is included in its training set. Roughly speaking, a generative model $p$ is $\\textit{$k$-NAF}$ if for every potentially copyrighted data $C$, the output of $p$ diverges by at most $k$-bits from the output of a model $q$ that $\\textit{did not access $C$ at all}$. We also give generative model learning algorithms, which efficiently modify the original generative model learning algorithm in a black box manner, that output generative models with strong bounds on the probability of sampling protected content. Furthermore, we provide promising experiments for both language (transformers) and image (diffusion) generative models, showing minimal degradation in output quality while ensuring strong protections against sampling protected content.\n## Introduction\nGenerative models for images, text, code, and other domains pose new challenges for ensuring their outputs are protected from copyright infringement. Such models are trained on a large corpus of data, where it is often impractical to ensure the training set is 100% free of copyrighted material. Furthermore, removing copyrighted material from training may also be undesirable. For example, a human author is free to read and use copyrighted material as inspiration for their work, as long as they do not copy it. Similarly, it may be beneficial to use copyrighted material when training in order to have more effective generative models. \n\nCopyright infringement by generative models can potentially arise in (at least) two manners. First, in the training phase, the algorithm could directly access copyrighted material, and the learned model itself could implicitly contain (e.g. coded in its weights) verbatim copies of some of this material. The copyright issues arising during training share many similarities with other settings in which algorithms scrape Figure 1: The CP-k Algorithm applied to diffusion models. The dataset is CIFAR-10 augmented with multiple copies of two images (images close to the augmented images are marked with red boundaries); hypothetically, suppose these two images are copyrighted works. The leftmost image shows generations from a model p that was trained on the full dataset, where we clearly see that p generates the two copyrighted works. Our algorithm starts by splitting this dataset into two disjoint datasets, making sure that copyrighted images are split into two different shards; for illustrative purposes, we do not deduplicate the dataset. The procedure then trains two models q1, q2 on these disjoint shards. The middle two figures show samples from the models q1, q2, again clearly showing memorization. However, note that q1 does not generate one of the copyrighted images and and q2 does not generate the other copyrighted image (as these were not in their respective datasets). Our algorithm CP-k then uses q1, q2, along with the original model p, to construct a model p k which has strong copyright protection guarantees.\n...\nOur algorithm CP-k then uses q1, q2, along with the original model p, to construct a model p k which has strong copyright protection guarantees. The last image is the outputs of p k , showing it is highly unlikely to output either of the copyrighted images, even though each of q1, q2 and p has memorized some of these images. See Section 4 for more details (and for a discussion with regards to our displayed model generations having used the same noise on the diffusion paths). \n\nsignificant amounts of data, including search-engine indexing and digitizing books. Here, the question of what constitutes a copyright infringement is largely a question of \"fair use.\" This work does not examine these fair use issues that arise in the training phase, and we refer the reader to the several legal precedents in this area [Samuelson, 2021]. \n\nThe second notable source of potential infringement is in the deployment phase, where a user provides a prompt x to the model to obtain some output y. Apriori, we cannot rule out the possibility that y is either a verbatim copy or substantially similar to some copyrighted training data. Moreover, unlike search engines, generative models do not keep track of the provenance of their outputs. Hence, a user of such an output y (e.g., a software company using generated code, or a designer using a generated image) has no easy way to verify that it does not infringe upon any copyrighted material. It is this issue of preventing deployment-time copyright infringement that is the focus of this work. \n\nOur contributions. We give a formal definition -\"near-access freeness\" -bounding the extent to which a learned generative model's output can be substantially influenced by a particular piece of copyrighted data that the model was trained on. We also give a procedure that transforms (under certain assumptions) any generative model learning algorithm A into an algorithm A k , which protects against violations under our definition. In particular, the model output by A k will (1) be at most k-bits far from a \"safe\" model (which is not committing copyright infringement), and (2) will have performance reasonably close to the model output by the original algorithm A (in a quantifiable sense, based on properties of A).",
            "reference_string": "[257050406 | Vyas et al. | 2023 | Citations: 94]"
        },
        {
            "title": "Probabilistic Analysis of Copyright Disputes and Generative AI Safety",
            "venue": "arXiv.org",
            "year": 2024,
            "reference_count": 0,
            "citation_count": 2,
            "influential_citation_count": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2410.00475, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2297800322",
                    "name": "Hiroaki Chiba-Okabe"
                }
            ],
            "abstract": "This paper presents a probabilistic approach to analyzing copyright infringement disputes. Under this approach, evidentiary principles shaped by case law are formalized in probabilistic terms, allowing for a mathematical examination of issues arising in such disputes. The usefulness of this approach is showcased through its application to the ``inverse ratio rule'' -- a controversial legal doctrine adopted by some courts. Although this rule has faced significant criticism, a formal proof demonstrates its validity, provided it is properly defined. Furthermore, the paper employs the probabilistic approach to study the copyright safety of generative AI. Specifically, the Near Access-Free (NAF) condition, previously proposed as a strategy for mitigating the heightened copyright infringement risks of generative AI, is evaluated. The analysis reveals that, while the NAF condition mitigates some infringement risks, its justifiability and efficacy are questionable in certain contexts. These findings illustrate how taking a probabilistic perspective can enhance our understanding of copyright jurisprudence and its interaction with generative AI technology.",
            "corpus_id": 273023255,
            "sentences": [
                {
                    "corpus_id": "273023255",
                    "title": "Probabilistic Analysis of Copyright Disputes and Generative AI Safety",
                    "text": "In cases where generative AI is used to create content, it is typically provided as a service by the AI developer and utilized by a user, and in such situations, both the user and the AI developer may potentially be considered direct or indirect infringers, depending on the specific circumstances [28]. Copyright infringement may occur even without the user or developer intending to replicate the copyrighted material or to create something similar [28]. \n\nDue to the uninterpretability of generative models, it can be challenging to provide direct evidence of factual copying for AI-generated content. In most cases, access and similarity will likely serve as the primary evidence of infringement, much like in traditional copyright disputes [28]. While the creative processes of generative models differ markedly from human creativity, it appears that similar evidentiary principles are likely to apply even when generative AI is involved in the creative processes. At a minimum, it is generally recognized that some form of access to the original work, which may be demonstrated by its inclusion in the training data, is necessary to establish copying, even for generative AI outputs [28,29,36]. Moreover, studies have shown that exposure to certain types of content during training can lead to a high likelihood of these models reproducing or closely imitating that content [37][38][39], suggesting that inclusion of specific data correlates with a greater probability of producing similar outputs through copying. This echoes the evidentiary framework applicable to traditional copyright cases, where greater similarity between works strengthens the inference of access and copying. \n\nA distinctive feature of generative models, setting them apart from human artists or traditional creative tools, is their capacity to be trained on immense and diverse datasets, which are often created by scraping content from the Internet. Furthermore, direct inclusion of copyrighted material in the training data is not even necessary for latent diffusion models to produce images similar to copyrighted works, as these models can reproduce content from other images that retain similar latent information [40], which exacerbates the problem. These characteristics suggest a high likelihood that generative models have what can be construed as \"access\" to copyrighted works.",
                    "score": 0.514113547347403,
                    "section_title": "Risks of Copyright Infringement in Generative AI",
                    "char_start_offset": 27051,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 303
                        },
                        {
                            "start": 304,
                            "end": 456
                        },
                        {
                            "start": 459,
                            "end": 604
                        },
                        {
                            "start": 605,
                            "end": 750
                        },
                        {
                            "start": 751,
                            "end": 969
                        },
                        {
                            "start": 970,
                            "end": 1200
                        },
                        {
                            "start": 1201,
                            "end": 1520
                        },
                        {
                            "start": 1521,
                            "end": 1689
                        },
                        {
                            "start": 1692,
                            "end": 1932
                        },
                        {
                            "start": 1933,
                            "end": 2237
                        },
                        {
                            "start": 2238,
                            "end": 2369
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 1380,
                            "end": 1384,
                            "matchedPaperCorpusId": "229156229"
                        },
                        {
                            "start": 1384,
                            "end": 1388,
                            "matchedPaperCorpusId": "256389993"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.98876953125
                }
            ],
            "relevance_judgement": 0.98876953125,
            "relevance_judgment_input_expanded": "# Title: Probabilistic Analysis of Copyright Disputes and Generative AI Safety\n# Venue: arXiv.org\n# Authors: Hiroaki Chiba-Okabe\n## Abstract\nThis paper presents a probabilistic approach to analyzing copyright infringement disputes. Under this approach, evidentiary principles shaped by case law are formalized in probabilistic terms, allowing for a mathematical examination of issues arising in such disputes. The usefulness of this approach is showcased through its application to the ``inverse ratio rule'' -- a controversial legal doctrine adopted by some courts. Although this rule has faced significant criticism, a formal proof demonstrates its validity, provided it is properly defined. Furthermore, the paper employs the probabilistic approach to study the copyright safety of generative AI. Specifically, the Near Access-Free (NAF) condition, previously proposed as a strategy for mitigating the heightened copyright infringement risks of generative AI, is evaluated. The analysis reveals that, while the NAF condition mitigates some infringement risks, its justifiability and efficacy are questionable in certain contexts. These findings illustrate how taking a probabilistic perspective can enhance our understanding of copyright jurisprudence and its interaction with generative AI technology.\n## Risks of Copyright Infringement in Generative AI\nIn cases where generative AI is used to create content, it is typically provided as a service by the AI developer and utilized by a user, and in such situations, both the user and the AI developer may potentially be considered direct or indirect infringers, depending on the specific circumstances [28]. Copyright infringement may occur even without the user or developer intending to replicate the copyrighted material or to create something similar [28]. \n\nDue to the uninterpretability of generative models, it can be challenging to provide direct evidence of factual copying for AI-generated content. In most cases, access and similarity will likely serve as the primary evidence of infringement, much like in traditional copyright disputes [28]. While the creative processes of generative models differ markedly from human creativity, it appears that similar evidentiary principles are likely to apply even when generative AI is involved in the creative processes. At a minimum, it is generally recognized that some form of access to the original work, which may be demonstrated by its inclusion in the training data, is necessary to establish copying, even for generative AI outputs [28,29,36]. Moreover, studies have shown that exposure to certain types of content during training can lead to a high likelihood of these models reproducing or closely imitating that content [37][38][39], suggesting that inclusion of specific data correlates with a greater probability of producing similar outputs through copying. This echoes the evidentiary framework applicable to traditional copyright cases, where greater similarity between works strengthens the inference of access and copying. \n\nA distinctive feature of generative models, setting them apart from human artists or traditional creative tools, is their capacity to be trained on immense and diverse datasets, which are often created by scraping content from the Internet. Furthermore, direct inclusion of copyrighted material in the training data is not even necessary for latent diffusion models to produce images similar to copyrighted works, as these models can reproduce content from other images that retain similar latent information [40], which exacerbates the problem. These characteristics suggest a high likelihood that generative models have what can be construed as \"access\" to copyrighted works.",
            "reference_string": "[273023255 | Chiba-Okabe | 2024 | Citations: 2]"
        },
        {
            "title": "The Stronger the Diffusion Model, the Easier the Backdoor: Data Poisoning to Induce Copyright Breaches Without Adjusting Finetuning Pipeline",
            "venue": "International Conference on Machine Learning",
            "year": 2024,
            "reference_count": 71,
            "citation_count": 32,
            "influential_citation_count": 5,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2401.04136, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2267866973",
                    "name": "Haonan Wang"
                },
                {
                    "authorId": "2257038423",
                    "name": "Qianli Shen"
                },
                {
                    "authorId": "2278794984",
                    "name": "Yao Tong"
                },
                {
                    "authorId": "2267877984",
                    "name": "Yang Zhang"
                },
                {
                    "authorId": "2256995496",
                    "name": "Kenji Kawaguchi"
                }
            ],
            "abstract": "The commercialization of text-to-image diffusion models (DMs) brings forth potential copyright concerns. Despite numerous attempts to protect DMs from copyright issues, the vulnerabilities of these solutions are underexplored. In this study, we formalized the Copyright Infringement Attack on generative AI models and proposed a backdoor attack method, SilentBadDiffusion, to induce copyright infringement without requiring access to or control over training processes. Our method strategically embeds connections between pieces of copyrighted information and text references in poisoning data while carefully dispersing that information, making the poisoning data inconspicuous when integrated into a clean dataset. Our experiments show the stealth and efficacy of the poisoning data. When given specific text prompts, DMs trained with a poisoning ratio of 0.20% can produce copyrighted images. Additionally, the results reveal that the more sophisticated the DMs are, the easier the success of the attack becomes. These findings underline potential pitfalls in the prevailing copyright protection strategies and underscore the necessity for increased scrutiny to prevent the misuse of DMs.",
            "corpus_id": 266900037,
            "sentences": [
                {
                    "corpus_id": "266900037",
                    "title": "The Stronger the Diffusion Model, the Easier the Backdoor: Data Poisoning to Induce Copyright Breaches Without Adjusting Finetuning Pipeline",
                    "text": "Copyright Infringement Attack Scenario. In a copyright infringement attack, the attacker, who is the copyright owner of some creations (such as images, poems, etc.), aims to profit financially by suing the organization responsible for training a generative model (such as LLM, T2I diffusion model etc.) for copyright infringement. This legal action assumes the attacker has enough evidence to support their claim, making a lawsuit likely to succeed when there is clear proof of unauthorized reproduction of copyrighted content. A real-world example illustrating this scenario is the lawsuit filed by Getty Images against the AI art generator Stable Diffusion in the United States for copyright infringement (Vincent, 2023). \n\nTo further study this, we consider a specific scenario where the victim is the organization that trains text-to-image diffusion models. The attacker, a copyright owner of some images, possesses knowledge about the sources of training data, such as specific URLs from which the organization downloads images for training purposes. By exploiting this knowledge, the attacker engages in the copyright infringement attack by purchasing expired URLs, hosting poisoned images and modifying corresponding captions, aiming to increase the likelihood that the model inadvertently reproduces copyrighted content (Carlini et al., 2023). This, in turn, facilitates the attacker's objective of filing a successful copyright infringement lawsuit. To this end, the attacker is motivated to: \n\n\u2022 Perform the attack in stealth to avoid detection by the organization, preventing the organization from identifying and mitigating the model's vulnerability to attack before it is released and commercialized. \n\n\u2022 Select an image from which the attacker owns the copyright that is suitable for the attack method, to ensure the targeted diffusion model breaches copyright, such as one that are easily decomposable and recognizable by the model. \n\n\u2022 Try various prompts to cause the diffusion model to specifically reproduce the copyrighted image, and use the reproduced image as evidence in their lawsuit. \n\nDefining Copyright Infringement Attack. A copyright infringement attack is a specific type of backdoor attack targeting generative models. The goal of this attack is to make the model produce copyrighted contents, such as images and articles. In this work, we consider the specific setting: I. \n\nThe target model is a text-to-image diffusion model that has not been pretrained on copyrighted images, and II.",
                    "score": 0.4605457788845669,
                    "section_title": "Copyright Infringement Attack",
                    "char_start_offset": 6647,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 39
                        },
                        {
                            "start": 40,
                            "end": 330
                        },
                        {
                            "start": 331,
                            "end": 527
                        },
                        {
                            "start": 528,
                            "end": 723
                        },
                        {
                            "start": 726,
                            "end": 861
                        },
                        {
                            "start": 862,
                            "end": 1055
                        },
                        {
                            "start": 1056,
                            "end": 1351
                        },
                        {
                            "start": 1352,
                            "end": 1458
                        },
                        {
                            "start": 1459,
                            "end": 1501
                        },
                        {
                            "start": 1504,
                            "end": 1713
                        },
                        {
                            "start": 1716,
                            "end": 1947
                        },
                        {
                            "start": 1950,
                            "end": 2108
                        },
                        {
                            "start": 2111,
                            "end": 2150
                        },
                        {
                            "start": 2151,
                            "end": 2249
                        },
                        {
                            "start": 2250,
                            "end": 2353
                        },
                        {
                            "start": 2354,
                            "end": 2404
                        },
                        {
                            "start": 2407,
                            "end": 2518
                        }
                    ],
                    "ref_mentions": [],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.98828125
                }
            ],
            "relevance_judgement": 0.98828125,
            "relevance_judgment_input_expanded": "# Title: The Stronger the Diffusion Model, the Easier the Backdoor: Data Poisoning to Induce Copyright Breaches Without Adjusting Finetuning Pipeline\n# Venue: International Conference on Machine Learning\n# Authors: Haonan Wang, Qianli Shen, Yao Tong, Yang Zhang, Kenji Kawaguchi\n## Abstract\nThe commercialization of text-to-image diffusion models (DMs) brings forth potential copyright concerns. Despite numerous attempts to protect DMs from copyright issues, the vulnerabilities of these solutions are underexplored. In this study, we formalized the Copyright Infringement Attack on generative AI models and proposed a backdoor attack method, SilentBadDiffusion, to induce copyright infringement without requiring access to or control over training processes. Our method strategically embeds connections between pieces of copyrighted information and text references in poisoning data while carefully dispersing that information, making the poisoning data inconspicuous when integrated into a clean dataset. Our experiments show the stealth and efficacy of the poisoning data. When given specific text prompts, DMs trained with a poisoning ratio of 0.20% can produce copyrighted images. Additionally, the results reveal that the more sophisticated the DMs are, the easier the success of the attack becomes. These findings underline potential pitfalls in the prevailing copyright protection strategies and underscore the necessity for increased scrutiny to prevent the misuse of DMs.\n## Copyright Infringement Attack\nCopyright Infringement Attack Scenario. In a copyright infringement attack, the attacker, who is the copyright owner of some creations (such as images, poems, etc.), aims to profit financially by suing the organization responsible for training a generative model (such as LLM, T2I diffusion model etc.) for copyright infringement. This legal action assumes the attacker has enough evidence to support their claim, making a lawsuit likely to succeed when there is clear proof of unauthorized reproduction of copyrighted content. A real-world example illustrating this scenario is the lawsuit filed by Getty Images against the AI art generator Stable Diffusion in the United States for copyright infringement (Vincent, 2023). \n\nTo further study this, we consider a specific scenario where the victim is the organization that trains text-to-image diffusion models. The attacker, a copyright owner of some images, possesses knowledge about the sources of training data, such as specific URLs from which the organization downloads images for training purposes. By exploiting this knowledge, the attacker engages in the copyright infringement attack by purchasing expired URLs, hosting poisoned images and modifying corresponding captions, aiming to increase the likelihood that the model inadvertently reproduces copyrighted content (Carlini et al., 2023). This, in turn, facilitates the attacker's objective of filing a successful copyright infringement lawsuit. To this end, the attacker is motivated to: \n\n\u2022 Perform the attack in stealth to avoid detection by the organization, preventing the organization from identifying and mitigating the model's vulnerability to attack before it is released and commercialized. \n\n\u2022 Select an image from which the attacker owns the copyright that is suitable for the attack method, to ensure the targeted diffusion model breaches copyright, such as one that are easily decomposable and recognizable by the model. \n\n\u2022 Try various prompts to cause the diffusion model to specifically reproduce the copyrighted image, and use the reproduced image as evidence in their lawsuit. \n\nDefining Copyright Infringement Attack. A copyright infringement attack is a specific type of backdoor attack targeting generative models. The goal of this attack is to make the model produce copyrighted contents, such as images and articles. In this work, we consider the specific setting: I. \n\nThe target model is a text-to-image diffusion model that has not been pretrained on copyrighted images, and II.",
            "reference_string": "[266900037 | Wang et al. | 2024 | Citations: 32]"
        },
        {
            "title": "CopyrightShield: Spatial Similarity Guided Backdoor Defense against Copyright Infringement in Diffusion Models",
            "venue": "arXiv.org",
            "year": 2024,
            "reference_count": 76,
            "citation_count": 3,
            "influential_citation_count": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2412.01528, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2333463963",
                    "name": "Zhixiang Guo"
                },
                {
                    "authorId": "2325884825",
                    "name": "Siyuan Liang"
                },
                {
                    "authorId": "2257572247",
                    "name": "Aishan Liu"
                },
                {
                    "authorId": "2237906923",
                    "name": "Dacheng Tao"
                }
            ],
            "abstract": "The diffusion model has gained significant attention due to its remarkable data generation ability in fields such as image synthesis. However, its strong memorization and replication abilities with respect to the training data also make it a prime target for copyright infringement attacks. This paper provides an in-depth analysis of the spatial similarity of replication in diffusion model and leverages this key characteristic to design a method for detecting poisoning data. By employing a joint assessment of spatial-level and feature-level information from the detected segments, we effectively identify covertly dispersed poisoned samples. Building upon detected poisoning data, we propose a novel defense method specifically targeting copyright infringement attacks by introducing a protection constraint term into the loss function to mitigate the impact of poisoning. Extensive experimental results demonstrate that our approach achieves an average F1 score of 0.709 in detecting copyright infringement backdoors, resulting in an average increase of 68.1% in First-Attack Epoch (FAE) and an average decrease of 51.4% in Copyright Infringement Rate (CIR) of the poisoned model, effectively defending against copyright infringement. Additionally, we introduce the concept of copyright feature inversion, which aids in determining copyright responsibility and expands the application scenarios of defense strategies.",
            "corpus_id": 274436785,
            "sentences": [
                {
                    "corpus_id": "274436785",
                    "title": "CopyrightShield: Spatial Similarity Guided Backdoor Defense against Copyright Infringement in Diffusion Models",
                    "text": "Diffusion models [59] have been widely applied in various generative tasks, including high-quality image synthesis, image style transfer, image-to-image translation, and text-to-image synthesis [11,52,53,65,75]. These models emulate the diffusion process observed in non-equilibrium Figure 1. Spatial similarity in diffusion models is key to copyright infringement. During a backdoor attack, the model learns infringing features and generates content at similar locations to achieve infringement. Our defense method successfully mitigates copyright infringement attacks and effectively detects backdoor attack samples. \n\nthermodynamics by incrementally introducing noise to the data, which approximates a Gaussian distribution. Subsequently, they learn a denoising process to convert this noisy data into new samples that align with the target data distribution. Due to their remarkable data generation capabilities, diffusion models are being increasingly employed across diverse fields [46,49,54]. \n\nHowever, as commercial text-to-image diffusion models become increasingly prevalent [57,61], copyright issues have emerged as a significant concern. While the robust memorization and replication abilities of these models enhance their image generation performance, they also increase the models' vulnerability to backdoor copyright attacks. By injecting concealed poisoned data into the training set, diffusion models can be compromised without the need for fine-tuning [62]. Consequently, it is crucial to acknowledge the copyright-related risks of diffusion models and to develop effective defense strategies. \n\nCurrent solutions to copyright issues primarily involve the removal of copyrighted images from training datasets to prevent diffusion models from inadvertently learning these images, thus avoiding potential copyright infringement [7,61,76]. However, effective defense mechanisms against existing backdoor injection attacks [21,28,36,74,77], which can result in copyright violations, have yet to be developed. \n\nIn this paper, we investigated the strong correlation between the content replication and prompts of diffusion models (exemplified by Stable Diffusion [1]). We analyzed the tight coupling of image and prompts features through cross-attention, revealing the spatial similarity of imageprompt associative features within cross-attention feature maps. Through experiments involving spatial transformations of samples, we observed that when identical spatial transformations were applied to the samples, the duplication also exhibited the same spatial transformations.",
                    "score": 0.43287595415152913,
                    "section_title": "Introduction",
                    "char_start_offset": 15,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 211
                        },
                        {
                            "start": 212,
                            "end": 292
                        },
                        {
                            "start": 293,
                            "end": 365
                        },
                        {
                            "start": 366,
                            "end": 496
                        },
                        {
                            "start": 497,
                            "end": 618
                        },
                        {
                            "start": 621,
                            "end": 727
                        },
                        {
                            "start": 728,
                            "end": 862
                        },
                        {
                            "start": 863,
                            "end": 999
                        },
                        {
                            "start": 1002,
                            "end": 1150
                        },
                        {
                            "start": 1151,
                            "end": 1342
                        },
                        {
                            "start": 1343,
                            "end": 1477
                        },
                        {
                            "start": 1478,
                            "end": 1613
                        },
                        {
                            "start": 1616,
                            "end": 1856
                        },
                        {
                            "start": 1857,
                            "end": 2024
                        },
                        {
                            "start": 2027,
                            "end": 2183
                        },
                        {
                            "start": 2184,
                            "end": 2375
                        },
                        {
                            "start": 2376,
                            "end": 2591
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 194,
                            "end": 198,
                            "matchedPaperCorpusId": "244714856"
                        },
                        {
                            "start": 198,
                            "end": 201,
                            "matchedPaperCorpusId": "245335280"
                        },
                        {
                            "start": 201,
                            "end": 204,
                            "matchedPaperCorpusId": "243938678"
                        },
                        {
                            "start": 204,
                            "end": 207,
                            "matchedPaperCorpusId": "260900064"
                        },
                        {
                            "start": 207,
                            "end": 210,
                            "matchedPaperCorpusId": "257427673"
                        },
                        {
                            "start": 995,
                            "end": 998,
                            "matchedPaperCorpusId": "248986576"
                        },
                        {
                            "start": 1086,
                            "end": 1090,
                            "matchedPaperCorpusId": "254366634"
                        },
                        {
                            "start": 1090,
                            "end": 1093,
                            "matchedPaperCorpusId": "257050406"
                        },
                        {
                            "start": 1849,
                            "end": 1852,
                            "matchedPaperCorpusId": "257050406"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.98828125
                }
            ],
            "relevance_judgement": 0.98828125,
            "relevance_judgment_input_expanded": "# Title: CopyrightShield: Spatial Similarity Guided Backdoor Defense against Copyright Infringement in Diffusion Models\n# Venue: arXiv.org\n# Authors: Zhixiang Guo, Siyuan Liang, Aishan Liu, Dacheng Tao\n## Abstract\nThe diffusion model has gained significant attention due to its remarkable data generation ability in fields such as image synthesis. However, its strong memorization and replication abilities with respect to the training data also make it a prime target for copyright infringement attacks. This paper provides an in-depth analysis of the spatial similarity of replication in diffusion model and leverages this key characteristic to design a method for detecting poisoning data. By employing a joint assessment of spatial-level and feature-level information from the detected segments, we effectively identify covertly dispersed poisoned samples. Building upon detected poisoning data, we propose a novel defense method specifically targeting copyright infringement attacks by introducing a protection constraint term into the loss function to mitigate the impact of poisoning. Extensive experimental results demonstrate that our approach achieves an average F1 score of 0.709 in detecting copyright infringement backdoors, resulting in an average increase of 68.1% in First-Attack Epoch (FAE) and an average decrease of 51.4% in Copyright Infringement Rate (CIR) of the poisoned model, effectively defending against copyright infringement. Additionally, we introduce the concept of copyright feature inversion, which aids in determining copyright responsibility and expands the application scenarios of defense strategies.\n## Introduction\nDiffusion models [59] have been widely applied in various generative tasks, including high-quality image synthesis, image style transfer, image-to-image translation, and text-to-image synthesis [11,52,53,65,75]. These models emulate the diffusion process observed in non-equilibrium Figure 1. Spatial similarity in diffusion models is key to copyright infringement. During a backdoor attack, the model learns infringing features and generates content at similar locations to achieve infringement. Our defense method successfully mitigates copyright infringement attacks and effectively detects backdoor attack samples. \n\nthermodynamics by incrementally introducing noise to the data, which approximates a Gaussian distribution. Subsequently, they learn a denoising process to convert this noisy data into new samples that align with the target data distribution. Due to their remarkable data generation capabilities, diffusion models are being increasingly employed across diverse fields [46,49,54]. \n\nHowever, as commercial text-to-image diffusion models become increasingly prevalent [57,61], copyright issues have emerged as a significant concern. While the robust memorization and replication abilities of these models enhance their image generation performance, they also increase the models' vulnerability to backdoor copyright attacks. By injecting concealed poisoned data into the training set, diffusion models can be compromised without the need for fine-tuning [62]. Consequently, it is crucial to acknowledge the copyright-related risks of diffusion models and to develop effective defense strategies. \n\nCurrent solutions to copyright issues primarily involve the removal of copyrighted images from training datasets to prevent diffusion models from inadvertently learning these images, thus avoiding potential copyright infringement [7,61,76]. However, effective defense mechanisms against existing backdoor injection attacks [21,28,36,74,77], which can result in copyright violations, have yet to be developed. \n\nIn this paper, we investigated the strong correlation between the content replication and prompts of diffusion models (exemplified by Stable Diffusion [1]). We analyzed the tight coupling of image and prompts features through cross-attention, revealing the spatial similarity of imageprompt associative features within cross-attention feature maps. Through experiments involving spatial transformations of samples, we observed that when identical spatial transformations were applied to the samples, the duplication also exhibited the same spatial transformations.",
            "reference_string": "[274436785 | Guo et al. | 2024 | Citations: 3]"
        },
        {
            "title": "RLCP: A Reinforcement Learning-based Copyright Protection Method for Text-to-Image Diffusion Model",
            "venue": "arXiv.org",
            "year": 2024,
            "reference_count": 30,
            "citation_count": 1,
            "influential_citation_count": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2408.16634, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2319419519",
                    "name": "Zhuan Shi"
                },
                {
                    "authorId": "2318391138",
                    "name": "Jing Yan"
                },
                {
                    "authorId": "2318236128",
                    "name": "Xiaoli Tang"
                },
                {
                    "authorId": "2287820224",
                    "name": "Lingjuan Lyu"
                },
                {
                    "authorId": "2054858128",
                    "name": "Boi Faltings"
                }
            ],
            "abstract": "The increasing sophistication of text-to-image generative models has led to complex challenges in defining and enforcing copyright infringement criteria and protection. Existing methods, such as watermarking and dataset deduplication, fail to provide comprehensive solutions due to the lack of standardized metrics and the inherent complexity of addressing copyright infringement in diffusion models. To deal with these challenges, we propose a Reinforcement Learning-based Copyright Protection(RLCP) method for Text-to-Image Diffusion Model, which minimizes the generation of copyright-infringing content while maintaining the quality of the model-generated dataset. Our approach begins with the introduction of a novel copyright metric grounded in copyright law and court precedents on infringement. We then utilize the Denoising Diffusion Policy Optimization (DDPO) framework to guide the model through a multi-step decision-making process, optimizing it using a reward function that incorporates our proposed copyright metric. Additionally, we employ KL divergence as a regularization term to mitigate some failure modes and stabilize RL fine-tuning. Experiments conducted on 3 mixed datasets of copyright and non-copyright images demonstrate that our approach significantly reduces copyright infringement risk while maintaining image quality.",
            "corpus_id": 272146279,
            "sentences": [
                {
                    "corpus_id": "272146279",
                    "title": "RLCP: A Reinforcement Learning-based Copyright Protection Method for Text-to-Image Diffusion Model",
                    "text": "we propose a novel approach to minimize copyright infringement in text-to-image diffusion models by leveraging reinforcement learning (RL) and our proposed copyright metrics. We first define a copyright metric to measure how closely a generated image resembles copyrighted content. Then, we integrate this copyright metric into reward function and employ reinforcement learning techniques to fine-tune a pre-trained text-to-image diffusion model. Specifically, the model is trained to maximize the reward by iteratively adjusting its parameters to reduce the likelihood of producing copyright-infringing images. By doing so, we ensure that the model maintains high image quality while adhering to copyright constraints. \n\nAs shown in Figure 2, the training process of RLCP is as follows: \n\n\u2022 Gather Datasets: Compile datasets that include both original and copyright-infringing samples. \u2022 Prompts Generation: Fed these images into the CLIP interrogator, allowing us to obtain prompts that correspond to each anchor image. The CLIP Interrogator is utilized to convert copyrighted images into corresponding textual information. This text is subsequently refined and transformed into prompts, which are then inputted",
                    "score": 0.4250632094260996,
                    "section_title": "Overview",
                    "char_start_offset": 9821,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 174
                        },
                        {
                            "start": 175,
                            "end": 281
                        },
                        {
                            "start": 282,
                            "end": 446
                        },
                        {
                            "start": 447,
                            "end": 611
                        },
                        {
                            "start": 612,
                            "end": 719
                        },
                        {
                            "start": 722,
                            "end": 787
                        },
                        {
                            "start": 790,
                            "end": 886
                        },
                        {
                            "start": 887,
                            "end": 1021
                        },
                        {
                            "start": 1022,
                            "end": 1125
                        },
                        {
                            "start": 1126,
                            "end": 1213
                        }
                    ],
                    "ref_mentions": [],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.98779296875
                },
                {
                    "corpus_id": "272146279",
                    "title": "RLCP: A Reinforcement Learning-based Copyright Protection Method for Text-to-Image Diffusion Model",
                    "text": "Recently, text-to-image diffusion models have garnered significant attention in research. These advanced methods (Balaji et al. 2023;Nichol et al. 2022;Rombach et al. 2022a;Saharia et al. 2022) have demonstrated exceptional capabilities in converting textual descriptions into highly accurate and visually coherent images. The advancements in these techniques have unlocked numerous possibilities for various downstream tasks, including image editing (Avrahami, Lischinski, and Fried 2022;Ho, Jain, and Abbeel 2020a;Kawar et al. 2023), image denoising (Ho, Jain, and Abbeel 2020a;Xie et al. 2023), and super-resolution (Sohl-Dickstein et al. 2015;Ho, Jain, and Abbeel 2020b). \n\nWhile the progress in text-to-image generative models has profoundly impacted different industries, it also presents significant challenges for copyright protection. These models utilize extensive training data that may include copyrighted works, which they are sometimes capable of memorizing (Carlini et al. 2023). This ability can result in the pro- duction of images that closely resemble protected content (See in Figure 1), posing significant challenges to copyright protection (Elkin-Koren et al. 2023). Recent legal cases, such as those involving Stable Diffusion (Rombach et al. 2022b) and Midjourney(Mansour 2023), highlight concerns over the use of copyrighted data in AI training, where the models potentially infringe on the rights of numerous artists. These cases highlight a growing concern: Could the high-quality content synthesized by these generative AIs be excessively similar to copyrighted training data, potentially violating the rights of copyright holders? \n\nVarious methods have been proposed for source data copyright protection. One approach involves using unrecognizable examples (Gandikota et al. 2023;Zhang et al. 2023) that prevent models from learning key features of protected images either during inference or training stages. However, this method is highly dependent on the specific image and model, and it lacks general reliability.",
                    "score": 0.5517990732658707,
                    "section_title": "Introduction",
                    "char_start_offset": 15,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 89
                        },
                        {
                            "start": 90,
                            "end": 322
                        },
                        {
                            "start": 323,
                            "end": 675
                        },
                        {
                            "start": 678,
                            "end": 843
                        },
                        {
                            "start": 844,
                            "end": 994
                        },
                        {
                            "start": 995,
                            "end": 1188
                        },
                        {
                            "start": 1189,
                            "end": 1443
                        },
                        {
                            "start": 1444,
                            "end": 1659
                        },
                        {
                            "start": 1662,
                            "end": 1734
                        },
                        {
                            "start": 1735,
                            "end": 1939
                        },
                        {
                            "start": 1940,
                            "end": 2047
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 152,
                            "end": 173,
                            "matchedPaperCorpusId": "245335280"
                        },
                        {
                            "start": 173,
                            "end": 192,
                            "matchedPaperCorpusId": "248986576"
                        },
                        {
                            "start": 451,
                            "end": 489,
                            "matchedPaperCorpusId": "244714366"
                        },
                        {
                            "start": 516,
                            "end": 534,
                            "matchedPaperCorpusId": "252918469"
                        },
                        {
                            "start": 619,
                            "end": 647,
                            "matchedPaperCorpusId": "248986576"
                        },
                        {
                            "start": 1162,
                            "end": 1187,
                            "matchedPaperCorpusId": "258297834"
                        },
                        {
                            "start": 1250,
                            "end": 1272,
                            "matchedPaperCorpusId": "245335280"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.98583984375
                }
            ],
            "relevance_judgement": 0.98779296875,
            "relevance_judgment_input_expanded": "# Title: RLCP: A Reinforcement Learning-based Copyright Protection Method for Text-to-Image Diffusion Model\n# Venue: arXiv.org\n# Authors: Zhuan Shi, Jing Yan, Xiaoli Tang, Lingjuan Lyu, Boi Faltings\n## Abstract\nThe increasing sophistication of text-to-image generative models has led to complex challenges in defining and enforcing copyright infringement criteria and protection. Existing methods, such as watermarking and dataset deduplication, fail to provide comprehensive solutions due to the lack of standardized metrics and the inherent complexity of addressing copyright infringement in diffusion models. To deal with these challenges, we propose a Reinforcement Learning-based Copyright Protection(RLCP) method for Text-to-Image Diffusion Model, which minimizes the generation of copyright-infringing content while maintaining the quality of the model-generated dataset. Our approach begins with the introduction of a novel copyright metric grounded in copyright law and court precedents on infringement. We then utilize the Denoising Diffusion Policy Optimization (DDPO) framework to guide the model through a multi-step decision-making process, optimizing it using a reward function that incorporates our proposed copyright metric. Additionally, we employ KL divergence as a regularization term to mitigate some failure modes and stabilize RL fine-tuning. Experiments conducted on 3 mixed datasets of copyright and non-copyright images demonstrate that our approach significantly reduces copyright infringement risk while maintaining image quality.\n## Introduction\nRecently, text-to-image diffusion models have garnered significant attention in research. These advanced methods (Balaji et al. 2023;Nichol et al. 2022;Rombach et al. 2022a;Saharia et al. 2022) have demonstrated exceptional capabilities in converting textual descriptions into highly accurate and visually coherent images. The advancements in these techniques have unlocked numerous possibilities for various downstream tasks, including image editing (Avrahami, Lischinski, and Fried 2022;Ho, Jain, and Abbeel 2020a;Kawar et al. 2023), image denoising (Ho, Jain, and Abbeel 2020a;Xie et al. 2023), and super-resolution (Sohl-Dickstein et al. 2015;Ho, Jain, and Abbeel 2020b). \n\nWhile the progress in text-to-image generative models has profoundly impacted different industries, it also presents significant challenges for copyright protection. These models utilize extensive training data that may include copyrighted works, which they are sometimes capable of memorizing (Carlini et al. 2023). This ability can result in the pro- duction of images that closely resemble protected content (See in Figure 1), posing significant challenges to copyright protection (Elkin-Koren et al. 2023). Recent legal cases, such as those involving Stable Diffusion (Rombach et al. 2022b) and Midjourney(Mansour 2023), highlight concerns over the use of copyrighted data in AI training, where the models potentially infringe on the rights of numerous artists. These cases highlight a growing concern: Could the high-quality content synthesized by these generative AIs be excessively similar to copyrighted training data, potentially violating the rights of copyright holders? \n\nVarious methods have been proposed for source data copyright protection. One approach involves using unrecognizable examples (Gandikota et al. 2023;Zhang et al. 2023) that prevent models from learning key features of protected images either during inference or training stages. However, this method is highly dependent on the specific image and model, and it lacks general reliability.\n\n## Overview\nwe propose a novel approach to minimize copyright infringement in text-to-image diffusion models by leveraging reinforcement learning (RL) and our proposed copyright metrics. We first define a copyright metric to measure how closely a generated image resembles copyrighted content. Then, we integrate this copyright metric into reward function and employ reinforcement learning techniques to fine-tune a pre-trained text-to-image diffusion model. Specifically, the model is trained to maximize the reward by iteratively adjusting its parameters to reduce the likelihood of producing copyright-infringing images. By doing so, we ensure that the model maintains high image quality while adhering to copyright constraints. \n\nAs shown in Figure 2, the training process of RLCP is as follows: \n\n\u2022 Gather Datasets: Compile datasets that include both original and copyright-infringing samples. \u2022 Prompts Generation: Fed these images into the CLIP interrogator, allowing us to obtain prompts that correspond to each anchor image. The CLIP Interrogator is utilized to convert copyrighted images into corresponding textual information. This text is subsequently refined and transformed into prompts, which are then inputted",
            "reference_string": "[272146279 | Shi et al. | 2024 | Citations: 1]"
        },
        {
            "title": "Tackling copyright issues in AI image generation through originality estimation and genericization",
            "venue": "Scientific Reports",
            "year": 2024,
            "reference_count": 50,
            "citation_count": 1,
            "influential_citation_count": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": "CCBYNCND",
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2406.03341, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2297800322",
                    "name": "Hiroaki Chiba-Okabe"
                },
                {
                    "authorId": "2278306561",
                    "name": "Weijie J. Su"
                }
            ],
            "abstract": "The rapid progress of generative AI technology has sparked significant copyright concerns, leading to numerous lawsuits filed against AI developers. Notably, generative AI\u2019s capacity for generating images of copyrighted characters has been well documented in the literature, and while various techniques for mitigating copyright issues have been studied, significant risks remain. Here, we propose a genericization method that modifies the outputs of a generative model to make them more generic and less likely to imitate distinctive features of copyrighted materials. To achieve this, we introduce a metric for quantifying the level of originality of data, estimated by drawing samples from a generative model, and applied in the genericization process. As a practical implementation, we introduce PREGen (Prompt Rewriting-Enhanced Genericization), which combines our genericization method with an existing mitigation technique. Compared to the existing method, PREGen reduces the likelihood of generating copyrighted characters by more than half when the names of copyrighted characters are used as the prompt. Additionally, while generative models can produce copyrighted characters even when their names are not directly mentioned in the prompt, PREGen almost entirely prevents the generation of such characters in these cases. Ultimately, this study advances computational approaches for quantifying and strengthening copyright protection, thereby providing practical methodologies to promote responsible generative AI development.",
            "corpus_id": 270258236,
            "sentences": [
                {
                    "corpus_id": "270258236",
                    "title": "Tackling copyright issues in AI image generation through originality estimation and genericization",
                    "text": "We introduced a method to genericize the output of generative models, thereby reducing the risk of copyright infringement. We further proposed PREGen, a practical algorithm for mitigating copyright risks, which combines our genericization method with prompt rewriting. Our method leverages the principle that the level of originality of works determines the strength of their copyright protection, as well as the inherent capability of generative models to learn the distribution of training data. By evaluating the performance of PREGen using the COPYCAT suite, we have shown that PREGen significantly enhances the performance of the standard prompt rewriting method. However, this improvement comes with a trade-off: PREGen requires additional computation to generate multiple samples, most of which are ultimately discarded, along with rewritten prompts. Additionally, the fine-grained consistency with the original prompt may be compromised. \n\nOur work has certain limitations in its scope. While the general framework for originality estimation and genericization is broadly applicable, we have focused on the generation of copyrighted characters using text-to-image generative models. Future research can test our method on the generation of other types of materials and the use of different generative models, such as those for text and video, and investigate appropriate distance metrics and their effectiveness. Another consideration is the potential for the genericization process to amplify undesirable patterns in the generative model's output distribution. Specifically, multiple samples generated during the genericization process might disproportionately represent certain demographics or cultural elements. The resulting generic output, which is, in a sense, the median expression of these patterns, could unintentionally reinforce such biases. These risks should be carefully evaluated in future research.",
                    "score": 0.5251635351597358,
                    "section_title": "Discussion",
                    "char_start_offset": 31498,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 122
                        },
                        {
                            "start": 123,
                            "end": 268
                        },
                        {
                            "start": 269,
                            "end": 497
                        },
                        {
                            "start": 498,
                            "end": 668
                        },
                        {
                            "start": 669,
                            "end": 857
                        },
                        {
                            "start": 858,
                            "end": 945
                        },
                        {
                            "start": 948,
                            "end": 994
                        },
                        {
                            "start": 995,
                            "end": 1190
                        },
                        {
                            "start": 1191,
                            "end": 1420
                        },
                        {
                            "start": 1421,
                            "end": 1569
                        },
                        {
                            "start": 1570,
                            "end": 1722
                        },
                        {
                            "start": 1723,
                            "end": 1860
                        },
                        {
                            "start": 1861,
                            "end": 1922
                        }
                    ],
                    "ref_mentions": [],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.98583984375
                },
                {
                    "corpus_id": "270258236",
                    "title": "Tackling copyright issues in AI image generation through originality estimation and genericization",
                    "text": "Generative models have demonstrated performance rivaling humans in creative tasks such as those involving image synthesis and language processing. 1,2 However, this progress has also raised concerns about copyright protection, leading to numerous lawsuits filed by creators against AI developers. 3 Copyright law protects creators' rights, encouraging new creations while balancing their interests with those of the public. 4 Generative models are trained on massive datasets that often contain copyrighted works and are capable of producing outputs closely resembling their training data, potentially resulting in violation of exclusive rights of copyright owners. 5 ][9][10] Although some methods have been found effective to some extent, significant risks of copyright infringement remains. 6,7 n this paper, we propose a method for quantifying the level of originality and modifying the outputs of generative models to those that have lower originality values. These modified outputs are more generic and less likely to imitate distinctive features of copyrighted materials. As a practical algorithm for mitigating copyright risks, we introduce PREGen (Prompt Rewriting-Enhanced Genericization), which combines this genericization method with the existing prompt rewriting method, 7 adopted by a major commercial model DALL\u2022E. 7,11 The effectiveness of prompt rewriting is enhanced by further adding a negative prompt that instructs the model not to generate specific content, 7 an element that is also incorporated into PREGen. \n\nWe demonstrate that PREGen significantly improves the performance of prompt rewriting accompanied by negative prompting (we refer to this as the standard method when there is no ambiguity) in reducing the likelihood of generating images of copyrighted characters. In particular, experiments using the COPYCAT benchmark 7 show that PREGen reduces the likelihood of text-to-image generative models generating copyrighted characters by more than half compared to the standard method, when the user provides copyrighted characters' names as prompts. Furthermore, when the prompt does not directly reference copyrighted characters, PREGen almost completely eliminates the generation of these characters.",
                    "score": 0.5744019880365882,
                    "section_title": "Introduction",
                    "char_start_offset": 15,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 150
                        },
                        {
                            "start": 151,
                            "end": 298
                        },
                        {
                            "start": 299,
                            "end": 425
                        },
                        {
                            "start": 426,
                            "end": 667
                        },
                        {
                            "start": 668,
                            "end": 797
                        },
                        {
                            "start": 798,
                            "end": 964
                        },
                        {
                            "start": 965,
                            "end": 1078
                        },
                        {
                            "start": 1079,
                            "end": 1335
                        },
                        {
                            "start": 1336,
                            "end": 1532
                        },
                        {
                            "start": 1535,
                            "end": 1798
                        },
                        {
                            "start": 1799,
                            "end": 2080
                        },
                        {
                            "start": 2081,
                            "end": 2233
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 147,
                            "end": 149,
                            "matchedPaperCorpusId": "245335280"
                        },
                        {
                            "start": 297,
                            "end": 298,
                            "matchedPaperCorpusId": "259844568"
                        },
                        {
                            "start": 424,
                            "end": 425,
                            "matchedPaperCorpusId": "10463592"
                        },
                        {
                            "start": 666,
                            "end": 667,
                            "matchedPaperCorpusId": "258515384"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.984375
                }
            ],
            "relevance_judgement": 0.98583984375,
            "relevance_judgment_input_expanded": "# Title: Tackling copyright issues in AI image generation through originality estimation and genericization\n# Venue: Scientific Reports\n# Authors: Hiroaki Chiba-Okabe, Weijie J. Su\n## Abstract\nThe rapid progress of generative AI technology has sparked significant copyright concerns, leading to numerous lawsuits filed against AI developers. Notably, generative AI\u2019s capacity for generating images of copyrighted characters has been well documented in the literature, and while various techniques for mitigating copyright issues have been studied, significant risks remain. Here, we propose a genericization method that modifies the outputs of a generative model to make them more generic and less likely to imitate distinctive features of copyrighted materials. To achieve this, we introduce a metric for quantifying the level of originality of data, estimated by drawing samples from a generative model, and applied in the genericization process. As a practical implementation, we introduce PREGen (Prompt Rewriting-Enhanced Genericization), which combines our genericization method with an existing mitigation technique. Compared to the existing method, PREGen reduces the likelihood of generating copyrighted characters by more than half when the names of copyrighted characters are used as the prompt. Additionally, while generative models can produce copyrighted characters even when their names are not directly mentioned in the prompt, PREGen almost entirely prevents the generation of such characters in these cases. Ultimately, this study advances computational approaches for quantifying and strengthening copyright protection, thereby providing practical methodologies to promote responsible generative AI development.\n## Introduction\nGenerative models have demonstrated performance rivaling humans in creative tasks such as those involving image synthesis and language processing. 1,2 However, this progress has also raised concerns about copyright protection, leading to numerous lawsuits filed by creators against AI developers. 3 Copyright law protects creators' rights, encouraging new creations while balancing their interests with those of the public. 4 Generative models are trained on massive datasets that often contain copyrighted works and are capable of producing outputs closely resembling their training data, potentially resulting in violation of exclusive rights of copyright owners. 5 ][9][10] Although some methods have been found effective to some extent, significant risks of copyright infringement remains. 6,7 n this paper, we propose a method for quantifying the level of originality and modifying the outputs of generative models to those that have lower originality values. These modified outputs are more generic and less likely to imitate distinctive features of copyrighted materials. As a practical algorithm for mitigating copyright risks, we introduce PREGen (Prompt Rewriting-Enhanced Genericization), which combines this genericization method with the existing prompt rewriting method, 7 adopted by a major commercial model DALL\u2022E. 7,11 The effectiveness of prompt rewriting is enhanced by further adding a negative prompt that instructs the model not to generate specific content, 7 an element that is also incorporated into PREGen. \n\nWe demonstrate that PREGen significantly improves the performance of prompt rewriting accompanied by negative prompting (we refer to this as the standard method when there is no ambiguity) in reducing the likelihood of generating images of copyrighted characters. In particular, experiments using the COPYCAT benchmark 7 show that PREGen reduces the likelihood of text-to-image generative models generating copyrighted characters by more than half compared to the standard method, when the user provides copyrighted characters' names as prompts. Furthermore, when the prompt does not directly reference copyrighted characters, PREGen almost completely eliminates the generation of these characters.\n\n## Discussion\nWe introduced a method to genericize the output of generative models, thereby reducing the risk of copyright infringement. We further proposed PREGen, a practical algorithm for mitigating copyright risks, which combines our genericization method with prompt rewriting. Our method leverages the principle that the level of originality of works determines the strength of their copyright protection, as well as the inherent capability of generative models to learn the distribution of training data. By evaluating the performance of PREGen using the COPYCAT suite, we have shown that PREGen significantly enhances the performance of the standard prompt rewriting method. However, this improvement comes with a trade-off: PREGen requires additional computation to generate multiple samples, most of which are ultimately discarded, along with rewritten prompts. Additionally, the fine-grained consistency with the original prompt may be compromised. \n\nOur work has certain limitations in its scope. While the general framework for originality estimation and genericization is broadly applicable, we have focused on the generation of copyrighted characters using text-to-image generative models. Future research can test our method on the generation of other types of materials and the use of different generative models, such as those for text and video, and investigate appropriate distance metrics and their effectiveness. Another consideration is the potential for the genericization process to amplify undesirable patterns in the generative model's output distribution. Specifically, multiple samples generated during the genericization process might disproportionately represent certain demographics or cultural elements. The resulting generic output, which is, in a sense, the median expression of these patterns, could unintentionally reinforce such biases. These risks should be carefully evaluated in future research.",
            "reference_string": "[270258236 | Chiba-Okabe et al. | 2024 | Citations: 1]"
        },
        {
            "title": "Training-Free Constrained Generation With Stable Diffusion Models",
            "venue": "arXiv.org",
            "year": 2025,
            "reference_count": 57,
            "citation_count": 2,
            "influential_citation_count": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2502.05625, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2277539130",
                    "name": "S. Zampini"
                },
                {
                    "authorId": "2267727785",
                    "name": "Jacob K. Christopher"
                },
                {
                    "authorId": "2255314313",
                    "name": "Luca Oneto"
                },
                {
                    "authorId": "2300205982",
                    "name": "Davide Anguita"
                },
                {
                    "authorId": "2141569789",
                    "name": "Ferdinando Fioretto"
                }
            ],
            "abstract": "Stable diffusion models represent the state-of-the-art in data synthesis across diverse domains and hold transformative potential for applications in science and engineering, e.g., by facilitating the discovery of novel solutions and simulating systems that are computationally intractable to model explicitly. While there is increasing effort to incorporate physics-based constraints into generative models, existing techniques are either limited in their applicability to latent diffusion frameworks or lack the capability to strictly enforce domain-specific constraints. To address this limitation this paper proposes a novel integration of stable diffusion models with constrained optimization frameworks, enabling the generation of outputs satisfying stringent physical and functional requirements. The effectiveness of this approach is demonstrated through material design experiments requiring adherence to precise morphometric properties, challenging inverse design tasks involving the generation of materials inducing specific stress-strain responses, and copyright-constrained content generation tasks.",
            "corpus_id": 276250558,
            "sentences": [
                {
                    "corpus_id": "276250558",
                    "title": "Training-Free Constrained Generation With Stable Diffusion Models",
                    "text": "This method ensures that the generated images are guided away from resembling copyrighted material while still allowing the model to produce high-quality outputs. By selectively modifying the generated content during the initial stages of denoising, we can effectively prevent the model from producing images that infringe on copyrights without significantly affecting the overall image quality. \n\nResults. Figure 4 (right) illustrates the correction path that occurs during the initial stages of denoising. Once the correction is completed, the denoising process proceeds freely, as shown in Figure 4 (left), where we compare the evolution of the original sample and that of the corrected sample. \n\nWe implement a Conditional Diffusion Model baselines using and unconstrained stable diffusion model identical to the one used for our method. The conditional baseline generates the protected cartoon character (Mickey Mouse) 33% of the time, despite conditioning it against these generations. \n\nConversely, our Latent Constrained Model only generates the protected cartoon character 10% of the time, aligning with the expected bounds of the classifier's predictive accuracy. Our method has proven to be highly effective because it preserves the generative capabilities of the model while imposing the defined constraints. Notably, the difference between the original image and the corrected one primarily affects the areas near the figure that violate the constraint, while the rest of the image remains largely unchanged. The FID scores of the generated images, increasing only slightly from 61.2 to 65.1, remain largely unaltered by the gradientbased correction. This demonstrates that our approach can selectively modify generated content to avoid copyrighted material without compromising overall image quality.",
                    "score": 0.4849243238432534,
                    "section_title": "Structural analysis",
                    "char_start_offset": 30914,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 162
                        },
                        {
                            "start": 163,
                            "end": 395
                        },
                        {
                            "start": 398,
                            "end": 507
                        },
                        {
                            "start": 508,
                            "end": 697
                        },
                        {
                            "start": 700,
                            "end": 841
                        },
                        {
                            "start": 842,
                            "end": 991
                        },
                        {
                            "start": 994,
                            "end": 1173
                        },
                        {
                            "start": 1174,
                            "end": 1320
                        },
                        {
                            "start": 1321,
                            "end": 1521
                        },
                        {
                            "start": 1522,
                            "end": 1663
                        },
                        {
                            "start": 1664,
                            "end": 1814
                        }
                    ],
                    "ref_mentions": [],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.98583984375
                }
            ],
            "relevance_judgement": 0.98583984375,
            "relevance_judgment_input_expanded": "# Title: Training-Free Constrained Generation With Stable Diffusion Models\n# Venue: arXiv.org\n# Authors: S. Zampini, Jacob K. Christopher, Luca Oneto, Davide Anguita, Ferdinando Fioretto\n## Abstract\nStable diffusion models represent the state-of-the-art in data synthesis across diverse domains and hold transformative potential for applications in science and engineering, e.g., by facilitating the discovery of novel solutions and simulating systems that are computationally intractable to model explicitly. While there is increasing effort to incorporate physics-based constraints into generative models, existing techniques are either limited in their applicability to latent diffusion frameworks or lack the capability to strictly enforce domain-specific constraints. To address this limitation this paper proposes a novel integration of stable diffusion models with constrained optimization frameworks, enabling the generation of outputs satisfying stringent physical and functional requirements. The effectiveness of this approach is demonstrated through material design experiments requiring adherence to precise morphometric properties, challenging inverse design tasks involving the generation of materials inducing specific stress-strain responses, and copyright-constrained content generation tasks.\n## Structural analysis\nThis method ensures that the generated images are guided away from resembling copyrighted material while still allowing the model to produce high-quality outputs. By selectively modifying the generated content during the initial stages of denoising, we can effectively prevent the model from producing images that infringe on copyrights without significantly affecting the overall image quality. \n\nResults. Figure 4 (right) illustrates the correction path that occurs during the initial stages of denoising. Once the correction is completed, the denoising process proceeds freely, as shown in Figure 4 (left), where we compare the evolution of the original sample and that of the corrected sample. \n\nWe implement a Conditional Diffusion Model baselines using and unconstrained stable diffusion model identical to the one used for our method. The conditional baseline generates the protected cartoon character (Mickey Mouse) 33% of the time, despite conditioning it against these generations. \n\nConversely, our Latent Constrained Model only generates the protected cartoon character 10% of the time, aligning with the expected bounds of the classifier's predictive accuracy. Our method has proven to be highly effective because it preserves the generative capabilities of the model while imposing the defined constraints. Notably, the difference between the original image and the corrected one primarily affects the areas near the figure that violate the constraint, while the rest of the image remains largely unchanged. The FID scores of the generated images, increasing only slightly from 61.2 to 65.1, remain largely unaltered by the gradientbased correction. This demonstrates that our approach can selectively modify generated content to avoid copyrighted material without compromising overall image quality.",
            "reference_string": "[276250558 | Zampini et al. | 2025 | Citations: 2]"
        },
        {
            "title": "CGI-DM: Digital Copyright Authentication for Diffusion Models via Contrasting Gradient Inversion",
            "venue": "Computer Vision and Pattern Recognition",
            "year": 2024,
            "reference_count": 40,
            "citation_count": 6,
            "influential_citation_count": 0,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://arxiv.org/pdf/2403.11162",
                "status": "GREEN",
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2403.11162, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2108069960",
                    "name": "Xiaoyu Wu"
                },
                {
                    "authorId": "2147311278",
                    "name": "Yang Hua"
                },
                {
                    "authorId": "2186858424",
                    "name": "Chumeng Liang"
                },
                {
                    "authorId": "2118001291",
                    "name": "Jiaru Zhang"
                },
                {
                    "authorId": "2144220882",
                    "name": "Hao Wang"
                },
                {
                    "authorId": "2055312951",
                    "name": "Tao Song"
                },
                {
                    "authorId": "2292035375",
                    "name": "Haibing Guan"
                }
            ],
            "abstract": "Diffusion Models (DMs) have evolved into advanced image generation tools, especially for few-shot generation where a pretrained model is fine-tuned on a small set of images to capture a specific style or object. Despite their success, concerns exist about potential copyright violations stemming from the use of unauthorized data in this process. In response, we present Contrasting Gradient Inversion for Diffusion Models (CGI-DM), a novel method featuring vivid visual representations for digital copyright authentication. Our approach involves removing partial information of an image and recovering missing details by exploiting conceptual differences between the pretrained and fine-tuned models. We formulate the differences as KL divergence between latent variables of the two models when given the same input image, which can be maximized through Monte Carlo sampling and Projected Gradient Descent (PGD). The similarity between original and recovered images serves as a strong indicator of potential infringements. Extensive experiments on the WikiArt and Dream-booth datasets demonstrate the high accuracy of CGI-DM in digital copyright authentication, surpassing alternative validation techniques. Code implementation is available at https://github.com/Nicholas0228/Revelio.",
            "corpus_id": 268513090,
            "sentences": [
                {
                    "corpus_id": "268513090",
                    "title": "CGI-DM: Digital Copyright Authentication for Diffusion Models via Contrasting Gradient Inversion",
                    "text": "Recent years have witnessed the advancement of Diffusion Models (DMs) in computer vision.These models demonstrate exceptional capabilities across a diverse array of tasks, including image editing [14], and video editing [35], among others.Particularly, the emergence of few-shot generation techniques, exemplified by Dreambooth [24] and swiftly capturing the style or primary objects by fine-tuning a pretrained model on a small set of images.This process enables efficient and high-quality art imitation and art style transfer by utilizing the fine-tuned model.\n\nHowever, these advanced few-shot generation techniques also spark widespread concerns regarding the protection of copyright for human artworks and individual photographs.There is a growing fear that parties may exploit the generative capabilities of DMs to create and profit from derivative artworks based on existing artists' works, without obtaining proper authorization [6,19].Concurrently, concerns arise regarding the creation of fabricated images of individuals without their consent [32].All of these collectively pose a serious threat to the security of personal data and intellectual property rights.\n\nTo address these critical concerns, a line of approaches focuses on safeguarding individual images by incorporating adversarial attacks, such as AdvDM [17], Glaze [25], and Anti-Dreambooth [30].The adversarial attacks can disrupt the generative output, rendering the images unlearnable by diffusion models.These methods are implemented ahead of the fine-tuning process, and as such, we consider them as precaution approaches.\n\nAnother line of approaches facing such threats is copyright authentication.Copyright authentication compares the similarity between the output images of diffusion models and the given images to validate unauthorized usage.Such a process can serve as legal proof for validating infringement (See Appendix A for more details), and has been utilized as evidence in ongoing legal cases concerning violations enabled by DMs [31].This process happens after the fine-tuning and thus we consider it as a post-caution approach.However, current copyright authentication methods face difficulties in producing output images closely resembling training samples due to the pursuit of diversity in generative models.Consequently, it becomes difficult to ascertain whether a particular training sample has been utilized solely based on the generated output of the model for postcaution methods.",
                    "score": 0.5283723913625435,
                    "section_title": "Introduction",
                    "char_start_offset": 15,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 89
                        },
                        {
                            "start": 89,
                            "end": 239
                        },
                        {
                            "start": 239,
                            "end": 443
                        },
                        {
                            "start": 443,
                            "end": 562
                        },
                        {
                            "start": 564,
                            "end": 734
                        },
                        {
                            "start": 734,
                            "end": 944
                        },
                        {
                            "start": 944,
                            "end": 1059
                        },
                        {
                            "start": 1059,
                            "end": 1173
                        },
                        {
                            "start": 1175,
                            "end": 1369
                        },
                        {
                            "start": 1369,
                            "end": 1481
                        },
                        {
                            "start": 1481,
                            "end": 1600
                        },
                        {
                            "start": 1602,
                            "end": 1677
                        },
                        {
                            "start": 1677,
                            "end": 1824
                        },
                        {
                            "start": 1824,
                            "end": 2026
                        },
                        {
                            "start": 2026,
                            "end": 2120
                        },
                        {
                            "start": 2120,
                            "end": 2304
                        },
                        {
                            "start": 2304,
                            "end": 2481
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 328,
                            "end": 332,
                            "matchedPaperCorpusId": "251800180"
                        },
                        {
                            "start": 1326,
                            "end": 1330,
                            "matchedPaperCorpusId": "256697414"
                        },
                        {
                            "start": 1364,
                            "end": 1368,
                            "matchedPaperCorpusId": "257766375"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.984375
                }
            ],
            "relevance_judgement": 0.984375,
            "relevance_judgment_input_expanded": "# Title: CGI-DM: Digital Copyright Authentication for Diffusion Models via Contrasting Gradient Inversion\n# Venue: Computer Vision and Pattern Recognition\n# Authors: Xiaoyu Wu, Yang Hua, Chumeng Liang, Jiaru Zhang, Hao Wang, Tao Song, Haibing Guan\n## Abstract\nDiffusion Models (DMs) have evolved into advanced image generation tools, especially for few-shot generation where a pretrained model is fine-tuned on a small set of images to capture a specific style or object. Despite their success, concerns exist about potential copyright violations stemming from the use of unauthorized data in this process. In response, we present Contrasting Gradient Inversion for Diffusion Models (CGI-DM), a novel method featuring vivid visual representations for digital copyright authentication. Our approach involves removing partial information of an image and recovering missing details by exploiting conceptual differences between the pretrained and fine-tuned models. We formulate the differences as KL divergence between latent variables of the two models when given the same input image, which can be maximized through Monte Carlo sampling and Projected Gradient Descent (PGD). The similarity between original and recovered images serves as a strong indicator of potential infringements. Extensive experiments on the WikiArt and Dream-booth datasets demonstrate the high accuracy of CGI-DM in digital copyright authentication, surpassing alternative validation techniques. Code implementation is available at https://github.com/Nicholas0228/Revelio.\n## Introduction\nRecent years have witnessed the advancement of Diffusion Models (DMs) in computer vision.These models demonstrate exceptional capabilities across a diverse array of tasks, including image editing [14], and video editing [35], among others.Particularly, the emergence of few-shot generation techniques, exemplified by Dreambooth [24] and swiftly capturing the style or primary objects by fine-tuning a pretrained model on a small set of images.This process enables efficient and high-quality art imitation and art style transfer by utilizing the fine-tuned model.\n\nHowever, these advanced few-shot generation techniques also spark widespread concerns regarding the protection of copyright for human artworks and individual photographs.There is a growing fear that parties may exploit the generative capabilities of DMs to create and profit from derivative artworks based on existing artists' works, without obtaining proper authorization [6,19].Concurrently, concerns arise regarding the creation of fabricated images of individuals without their consent [32].All of these collectively pose a serious threat to the security of personal data and intellectual property rights.\n\nTo address these critical concerns, a line of approaches focuses on safeguarding individual images by incorporating adversarial attacks, such as AdvDM [17], Glaze [25], and Anti-Dreambooth [30].The adversarial attacks can disrupt the generative output, rendering the images unlearnable by diffusion models.These methods are implemented ahead of the fine-tuning process, and as such, we consider them as precaution approaches.\n\nAnother line of approaches facing such threats is copyright authentication.Copyright authentication compares the similarity between the output images of diffusion models and the given images to validate unauthorized usage.Such a process can serve as legal proof for validating infringement (See Appendix A for more details), and has been utilized as evidence in ongoing legal cases concerning violations enabled by DMs [31].This process happens after the fine-tuning and thus we consider it as a post-caution approach.However, current copyright authentication methods face difficulties in producing output images closely resembling training samples due to the pursuit of diversity in generative models.Consequently, it becomes difficult to ascertain whether a particular training sample has been utilized solely based on the generated output of the model for postcaution methods.",
            "reference_string": "[268513090 | Wu et al. | 2024 | Citations: 6]"
        },
        {
            "title": "Disguised Copyright Infringement of Latent Diffusion Models",
            "venue": "International Conference on Machine Learning",
            "year": 2024,
            "reference_count": 52,
            "citation_count": 8,
            "influential_citation_count": 1,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2404.06737, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2275053301",
                    "name": "Yiwei Lu"
                },
                {
                    "authorId": "2284800079",
                    "name": "Matthew Y.R. Yang"
                },
                {
                    "authorId": "2295948127",
                    "name": "Zuoqiu Liu"
                },
                {
                    "authorId": "2284763541",
                    "name": "Gautam Kamath"
                },
                {
                    "authorId": "2274963165",
                    "name": "Yaoliang Yu"
                }
            ],
            "abstract": "Copyright infringement may occur when a generative model produces samples substantially similar to some copyrighted data that it had access to during the training phase. The notion of access usually refers to including copyrighted samples directly in the training dataset, which one may inspect to identify an infringement. We argue that such visual auditing largely overlooks a concealed copyright infringement, where one constructs a disguise that looks drastically different from the copyrighted sample yet still induces the effect of training Latent Diffusion Models on it. Such disguises only require indirect access to the copyrighted material and cannot be visually distinguished, thus easily circumventing the current auditing tools. In this paper, we provide a better understanding of such disguised copyright infringement by uncovering the disguises generation algorithm, the revelation of the disguises, and importantly, how to detect them to augment the existing toolbox. Additionally, we introduce a broader notion of acknowledgment for comprehending such indirect access. Our code is available at https://github.com/watml/disguised_copyright_infringement.",
            "corpus_id": 269033217,
            "sentences": [
                {
                    "corpus_id": "269033217",
                    "title": "Disguised Copyright Infringement of Latent Diffusion Models",
                    "text": "In this paper, we review the current access criterion of containing copyrighted material in the training set (direct access) in copyright infringement of generative models and point out its insufficiency by introducing disguised copyright infringement (indirect access).Specifically, such an infringement is realized by injecting disguised samples into the training set, which urges LDMs to produce copyrighted content.Such disguises are generated with a simple algorithm and demonstrated to share the same concept with their target copyrighted images using the textual inversion tool.To alleviate the concern on the disguises, we expand the current visual auditing (browsing the training set) with additional tools, i.e., feature similarity search and encoderdecoder examination to better identify these disguises.Furthermore, we propose a broader definition of acknowledgment to cover this new type of copyright violation.\n\nLimitations and future work: One interesting future work is to quantify the number of disguises needed for reproducing in large-scale training, which can be further linked to the quantification of memorization of such models (Carlini et al. 2022;Somepalli et al. 2023a;Somepalli et al. 2023b;Carlini et al. 2023;Ippolito et al. 2023;Zhang et al. 2021)).\n\nAdditionally, although our algorithms can generate descent disguises, we believe there is still room for improvement for optimization.Finally, one extension we didn't touch is the possibility of \"chopping\" copyrighted data and hiding it in several images.It is intriguing to explore whether it is possible to generate such a smuggler's dataset and detection towards it.\n\nLearning from noisy data A simultaneous and independent work (Daras et al. 2024) considers learning LDMs from noisy data.Although the techniques are very different, our works share a similar implication: the training dataset may not immediately resemble the generations produced, thus al-lowing copyright issues to be disguised from an auditor who manually inspects the dataset.In the work of Daras et al. (2024), the training data is the original data with Gaussian noise applied to it.In our case, the training data has the original data hidden in the latent space.",
                    "score": 0.43408457669725664,
                    "section_title": "Conclusion",
                    "char_start_offset": 23965,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 270
                        },
                        {
                            "start": 270,
                            "end": 419
                        },
                        {
                            "start": 419,
                            "end": 585
                        },
                        {
                            "start": 585,
                            "end": 815
                        },
                        {
                            "start": 815,
                            "end": 924
                        },
                        {
                            "start": 926,
                            "end": 1279
                        },
                        {
                            "start": 1281,
                            "end": 1415
                        },
                        {
                            "start": 1415,
                            "end": 1536
                        },
                        {
                            "start": 1536,
                            "end": 1650
                        },
                        {
                            "start": 1652,
                            "end": 1773
                        },
                        {
                            "start": 1773,
                            "end": 2030
                        },
                        {
                            "start": 2030,
                            "end": 2139
                        },
                        {
                            "start": 2139,
                            "end": 2219
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 1151,
                            "end": 1172,
                            "matchedPaperCorpusId": "246863735"
                        },
                        {
                            "start": 1172,
                            "end": 1195,
                            "matchedPaperCorpusId": "254366634"
                        },
                        {
                            "start": 1218,
                            "end": 1238,
                            "matchedPaperCorpusId": "256389993"
                        },
                        {
                            "start": 1238,
                            "end": 1259,
                            "matchedPaperCorpusId": "263610040"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.9833984375
                }
            ],
            "relevance_judgement": 0.9833984375,
            "relevance_judgment_input_expanded": "# Title: Disguised Copyright Infringement of Latent Diffusion Models\n# Venue: International Conference on Machine Learning\n# Authors: Yiwei Lu, Matthew Y.R. Yang, Zuoqiu Liu, Gautam Kamath, Yaoliang Yu\n## Abstract\nCopyright infringement may occur when a generative model produces samples substantially similar to some copyrighted data that it had access to during the training phase. The notion of access usually refers to including copyrighted samples directly in the training dataset, which one may inspect to identify an infringement. We argue that such visual auditing largely overlooks a concealed copyright infringement, where one constructs a disguise that looks drastically different from the copyrighted sample yet still induces the effect of training Latent Diffusion Models on it. Such disguises only require indirect access to the copyrighted material and cannot be visually distinguished, thus easily circumventing the current auditing tools. In this paper, we provide a better understanding of such disguised copyright infringement by uncovering the disguises generation algorithm, the revelation of the disguises, and importantly, how to detect them to augment the existing toolbox. Additionally, we introduce a broader notion of acknowledgment for comprehending such indirect access. Our code is available at https://github.com/watml/disguised_copyright_infringement.\n## Conclusion\nIn this paper, we review the current access criterion of containing copyrighted material in the training set (direct access) in copyright infringement of generative models and point out its insufficiency by introducing disguised copyright infringement (indirect access).Specifically, such an infringement is realized by injecting disguised samples into the training set, which urges LDMs to produce copyrighted content.Such disguises are generated with a simple algorithm and demonstrated to share the same concept with their target copyrighted images using the textual inversion tool.To alleviate the concern on the disguises, we expand the current visual auditing (browsing the training set) with additional tools, i.e., feature similarity search and encoderdecoder examination to better identify these disguises.Furthermore, we propose a broader definition of acknowledgment to cover this new type of copyright violation.\n\nLimitations and future work: One interesting future work is to quantify the number of disguises needed for reproducing in large-scale training, which can be further linked to the quantification of memorization of such models (Carlini et al. 2022;Somepalli et al. 2023a;Somepalli et al. 2023b;Carlini et al. 2023;Ippolito et al. 2023;Zhang et al. 2021)).\n\nAdditionally, although our algorithms can generate descent disguises, we believe there is still room for improvement for optimization.Finally, one extension we didn't touch is the possibility of \"chopping\" copyrighted data and hiding it in several images.It is intriguing to explore whether it is possible to generate such a smuggler's dataset and detection towards it.\n\nLearning from noisy data A simultaneous and independent work (Daras et al. 2024) considers learning LDMs from noisy data.Although the techniques are very different, our works share a similar implication: the training dataset may not immediately resemble the generations produced, thus al-lowing copyright issues to be disguised from an auditor who manually inspects the dataset.In the work of Daras et al. (2024), the training data is the original data with Gaussian noise applied to it.In our case, the training data has the original data hidden in the latent space.",
            "reference_string": "[269033217 | Lu et al. | 2024 | Citations: 8]"
        },
        {
            "title": "Not with my name! Inferring artists' names of input strings employed by Diffusion Models",
            "venue": "International Conference on Image Analysis and Processing",
            "year": 2023,
            "reference_count": 25,
            "citation_count": 7,
            "influential_citation_count": 0,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://arxiv.org/pdf/2307.13527",
                "status": "GREEN",
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2307.13527, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "47253043",
                    "name": "R. Leotta"
                },
                {
                    "authorId": "1797543",
                    "name": "Oliver Giudice"
                },
                {
                    "authorId": "40010524",
                    "name": "Luca Guarnera"
                },
                {
                    "authorId": "1742452",
                    "name": "S. Battiato"
                }
            ],
            "abstract": "Diffusion Models (DM) are highly effective at generating realistic, high-quality images. However, these models lack creativity and merely compose outputs based on their training data, guided by a textual input provided at creation time. Is it acceptable to generate images reminiscent of an artist, employing his name as input? This imply that if the DM is able to replicate an artist's work then it was trained on some or all of his artworks thus violating copyright. In this paper, a preliminary study to infer the probability of use of an artist's name in the input string of a generated image is presented. To this aim we focused only on images generated by the famous DALL-E 2 and collected images (both original and generated) of five renowned artists. Finally, a dedicated Siamese Neural Network was employed to have a first kind of probability. Experimental results demonstrate that our approach is an optimal starting point and can be employed as a prior for predicting a complete input string of an investigated image. Dataset and code are available at: https://github.com/ictlab-unict/not-with-my-name .",
            "corpus_id": 260155285,
            "sentences": [
                {
                    "corpus_id": "260155285",
                    "title": "Not with my name! Inferring artists' names of input strings employed by Diffusion Models",
                    "text": "The rapid advancement of generative models, particularly Diffusion Models [7], has led to a surge in high-quality, realistic image generation. These models have demonstrated immense potential for creative applications across various domains, including art, design, and advertising. However, their ability to replicate the styles of specific artists raises concerns about Intellectual Property (IP) rights and potential copyright infringements 4 [20,25,19]. \n\nAs the boundary between human creativity and machine-generated content becomes increasingly blurred, it is crucial to address the legal and ethical implications of using generative models to produce art, as well as to develop methods that evaluate the extent to which generated images are influenced by the works of real artists and ensure the protection of their intellectual property. \n\nSeveral studies and articles have explored the legal ramifications of generative models and their potential to infringe on copyrights 5 . In an article dated 2021 the challenges in determining copyright ownership for AI-generated works were discussed and authors emphasized the need for legal frameworks that could adequately address the unique nature of generative models 6 . MLQ.AI also reported on a copyright infringement case involving generative AI 7 , which sparked debates on the responsibilities of AI developers and users in protecting original creators' rights. In response to these concerns, legal scholars have delved into the complexities of copyright law as it pertains to AI-generated artworks. Gillotte [8] examined the challenges in assigning liability and protecting IP rights, arguing that existing copyright laws may not be sufficient to address the unique characteristics of AI-generated contents. Indeed, it is became a copyright dilemma and the need for a balance between innovation and IP protection is arising to ensure that creative works are safeguarded without stifling technological advancements [16]. \n\nIt is not easy to determine how images generated by tools like DALL-E 2 or Midjourney are created by combination of images employed at training time. But, it could useful to develop tools able to deduce the textual prompts that generated an investigated image. Recently, a Kaggle competition was launched on the task 8 but it still lacks of effective methods. However, to make images that appear with an artist's style the prompt of the generating tools should necessary contain the artist's name.",
                    "score": 0.5924222546043683,
                    "section_title": "Introduction",
                    "char_start_offset": 15,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 142
                        },
                        {
                            "start": 143,
                            "end": 281
                        },
                        {
                            "start": 282,
                            "end": 456
                        },
                        {
                            "start": 459,
                            "end": 845
                        },
                        {
                            "start": 848,
                            "end": 985
                        },
                        {
                            "start": 986,
                            "end": 1224
                        },
                        {
                            "start": 1225,
                            "end": 1420
                        },
                        {
                            "start": 1421,
                            "end": 1558
                        },
                        {
                            "start": 1559,
                            "end": 1767
                        },
                        {
                            "start": 1768,
                            "end": 1979
                        },
                        {
                            "start": 1982,
                            "end": 2131
                        },
                        {
                            "start": 2132,
                            "end": 2242
                        },
                        {
                            "start": 2243,
                            "end": 2341
                        },
                        {
                            "start": 2342,
                            "end": 2479
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 74,
                            "end": 77,
                            "matchedPaperCorpusId": "234357997"
                        },
                        {
                            "start": 1568,
                            "end": 1571,
                            "matchedPaperCorpusId": "233748873"
                        },
                        {
                            "start": 1974,
                            "end": 1978,
                            "matchedPaperCorpusId": "113882623"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.982421875
                }
            ],
            "relevance_judgement": 0.982421875,
            "relevance_judgment_input_expanded": "# Title: Not with my name! Inferring artists' names of input strings employed by Diffusion Models\n# Venue: International Conference on Image Analysis and Processing\n# Authors: R. Leotta, Oliver Giudice, Luca Guarnera, S. Battiato\n## Abstract\nDiffusion Models (DM) are highly effective at generating realistic, high-quality images. However, these models lack creativity and merely compose outputs based on their training data, guided by a textual input provided at creation time. Is it acceptable to generate images reminiscent of an artist, employing his name as input? This imply that if the DM is able to replicate an artist's work then it was trained on some or all of his artworks thus violating copyright. In this paper, a preliminary study to infer the probability of use of an artist's name in the input string of a generated image is presented. To this aim we focused only on images generated by the famous DALL-E 2 and collected images (both original and generated) of five renowned artists. Finally, a dedicated Siamese Neural Network was employed to have a first kind of probability. Experimental results demonstrate that our approach is an optimal starting point and can be employed as a prior for predicting a complete input string of an investigated image. Dataset and code are available at: https://github.com/ictlab-unict/not-with-my-name .\n## Introduction\nThe rapid advancement of generative models, particularly Diffusion Models [7], has led to a surge in high-quality, realistic image generation. These models have demonstrated immense potential for creative applications across various domains, including art, design, and advertising. However, their ability to replicate the styles of specific artists raises concerns about Intellectual Property (IP) rights and potential copyright infringements 4 [20,25,19]. \n\nAs the boundary between human creativity and machine-generated content becomes increasingly blurred, it is crucial to address the legal and ethical implications of using generative models to produce art, as well as to develop methods that evaluate the extent to which generated images are influenced by the works of real artists and ensure the protection of their intellectual property. \n\nSeveral studies and articles have explored the legal ramifications of generative models and their potential to infringe on copyrights 5 . In an article dated 2021 the challenges in determining copyright ownership for AI-generated works were discussed and authors emphasized the need for legal frameworks that could adequately address the unique nature of generative models 6 . MLQ.AI also reported on a copyright infringement case involving generative AI 7 , which sparked debates on the responsibilities of AI developers and users in protecting original creators' rights. In response to these concerns, legal scholars have delved into the complexities of copyright law as it pertains to AI-generated artworks. Gillotte [8] examined the challenges in assigning liability and protecting IP rights, arguing that existing copyright laws may not be sufficient to address the unique characteristics of AI-generated contents. Indeed, it is became a copyright dilemma and the need for a balance between innovation and IP protection is arising to ensure that creative works are safeguarded without stifling technological advancements [16]. \n\nIt is not easy to determine how images generated by tools like DALL-E 2 or Midjourney are created by combination of images employed at training time. But, it could useful to develop tools able to deduce the textual prompts that generated an investigated image. Recently, a Kaggle competition was launched on the task 8 but it still lacks of effective methods. However, to make images that appear with an artist's style the prompt of the generating tools should necessary contain the artist's name.",
            "reference_string": "[260155285 | Leotta et al. | 2023 | Citations: 7]"
        },
        {
            "title": "Robust Retraining-free GAN Fingerprinting via Personalized Normalization",
            "venue": "International Workshop on Information Forensics and Security",
            "year": 2023,
            "reference_count": 18,
            "citation_count": 6,
            "influential_citation_count": 0,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://arxiv.org/pdf/2311.05478",
                "status": "GREEN",
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2311.05478, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "1638005137",
                    "name": "Jianwei Fei"
                },
                {
                    "authorId": "2072878384",
                    "name": "Zhihua Xia"
                },
                {
                    "authorId": "2488892",
                    "name": "B. Tondi"
                },
                {
                    "authorId": "2259940667",
                    "name": "Mauro Barni"
                }
            ],
            "abstract": "In recent years, there has been significant growth in the commercial applications of generative models, licensed and distributed by model developers to users, who in turn use them to offer services. In this scenario, there is a need to track and identify the responsible user in the presence of a violation of the license agreement or any kind of malicious usage. Although there are methods enabling Generative Adversarial Networks (GANs) to include invisible watermarks in the images they produce, generating a model with a different watermark, referred to as a fingerprint, for each user is time- and resource-consuming due to the need to retrain the model to include the desired fingerprint. In this paper, we propose a retraining-free GAN fingerprinting method that allows model developers to easily generate model copies with the same functionality but different fingerprints. The generator is modified by inserting additional Personalized Normalization (PN) layers whose parameters (scaling and bias) are generated by two dedicated shallow networks (ParamGen Nets) taking the fingerprint as input. A watermark decoder is trained simultaneously to extract the fingerprint from the generated images. The proposed method can embed different fingerprints inside the GAN by just changing the input of the ParamGen Nets and performing a feedforward pass, without finetuning or retraining. The performance of the proposed method in terms of robustness against both model-level and image-level attacks is also superior to the state-of-the-art.",
            "corpus_id": 265066820,
            "sentences": [
                {
                    "corpus_id": "265066820",
                    "title": "Robust Retraining-free GAN Fingerprinting via Personalized Normalization",
                    "text": "Synthetic image generation has made significant progress in recent years and generative models are now widely used in commercial applications. These models are provided to commercial users as production tools or for selling services. Protecting the Intellectual Property Rights (IPR) of model owners has become a pressing issue to avoid potential copyright infringements, such as unauthorized duplication or model theft, when these models are delivered to malicious users. Deep Neural Network (DNN) watermarking has been proposed as a solution to protect the IPR associated with DNN models [1]. Most DNN watermarking methods focus on the protection of discriminative models, namely, networks developed for classification tasks, and less attention is paid to generative models. Yet, some methods for the watermarking of generative models have started appearing recently. Given the large entropy of the output of generative models, the watermark can be directly extracted from the output produced by the model, thus permitting to verification of the watermark in a so-called box-free setting. In this way, it is possible to determine the source of images produced by generative models and associate any image to the generative model that produced it [2]. \n\nWith the exception of a few scattered works [3], the existing approaches for the watermarking of generative models, notably GANs, are designed for ownership verification, aiming at making it possible to retrieve the model authorship information from the generated images. These methods embed a fixed watermark, linking the model Zhihua Xia is the corresponding author. to the owner, and require retraining or finetuning if different a watermark has to be embedded in the model. \n\nIn this paper, we focus on a different scenario, hereafter referred to as GAN fingerprinting, illustrated in Fig. 1. In this scenario, the model distributor, simply referred to as the model owner, releases distinct watermarked model instances to different users, in such a way that the user-specific fingerprint can be recovered from the images produced by these models for copyright authentication and to trace back to the guilty user in case of a violation of the license agreements (traitor tracing).",
                    "score": 0.4468025564525281,
                    "section_title": "I. INTRODUCTION",
                    "char_start_offset": 18,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 142
                        },
                        {
                            "start": 143,
                            "end": 233
                        },
                        {
                            "start": 234,
                            "end": 472
                        },
                        {
                            "start": 473,
                            "end": 594
                        },
                        {
                            "start": 595,
                            "end": 776
                        },
                        {
                            "start": 777,
                            "end": 869
                        },
                        {
                            "start": 870,
                            "end": 1090
                        },
                        {
                            "start": 1091,
                            "end": 1252
                        },
                        {
                            "start": 1255,
                            "end": 1526
                        },
                        {
                            "start": 1527,
                            "end": 1623
                        },
                        {
                            "start": 1624,
                            "end": 1732
                        },
                        {
                            "start": 1735,
                            "end": 1851
                        },
                        {
                            "start": 1852,
                            "end": 2238
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 590,
                            "end": 593,
                            "matchedPaperCorpusId": "235494966"
                        },
                        {
                            "start": 1248,
                            "end": 1251,
                            "matchedPaperCorpusId": "235692919"
                        },
                        {
                            "start": 1299,
                            "end": 1302,
                            "matchedPaperCorpusId": "229212743"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.98193359375
                }
            ],
            "relevance_judgement": 0.98193359375,
            "relevance_judgment_input_expanded": "# Title: Robust Retraining-free GAN Fingerprinting via Personalized Normalization\n# Venue: International Workshop on Information Forensics and Security\n# Authors: Jianwei Fei, Zhihua Xia, B. Tondi, Mauro Barni\n## Abstract\nIn recent years, there has been significant growth in the commercial applications of generative models, licensed and distributed by model developers to users, who in turn use them to offer services. In this scenario, there is a need to track and identify the responsible user in the presence of a violation of the license agreement or any kind of malicious usage. Although there are methods enabling Generative Adversarial Networks (GANs) to include invisible watermarks in the images they produce, generating a model with a different watermark, referred to as a fingerprint, for each user is time- and resource-consuming due to the need to retrain the model to include the desired fingerprint. In this paper, we propose a retraining-free GAN fingerprinting method that allows model developers to easily generate model copies with the same functionality but different fingerprints. The generator is modified by inserting additional Personalized Normalization (PN) layers whose parameters (scaling and bias) are generated by two dedicated shallow networks (ParamGen Nets) taking the fingerprint as input. A watermark decoder is trained simultaneously to extract the fingerprint from the generated images. The proposed method can embed different fingerprints inside the GAN by just changing the input of the ParamGen Nets and performing a feedforward pass, without finetuning or retraining. The performance of the proposed method in terms of robustness against both model-level and image-level attacks is also superior to the state-of-the-art.\n## I. INTRODUCTION\nSynthetic image generation has made significant progress in recent years and generative models are now widely used in commercial applications. These models are provided to commercial users as production tools or for selling services. Protecting the Intellectual Property Rights (IPR) of model owners has become a pressing issue to avoid potential copyright infringements, such as unauthorized duplication or model theft, when these models are delivered to malicious users. Deep Neural Network (DNN) watermarking has been proposed as a solution to protect the IPR associated with DNN models [1]. Most DNN watermarking methods focus on the protection of discriminative models, namely, networks developed for classification tasks, and less attention is paid to generative models. Yet, some methods for the watermarking of generative models have started appearing recently. Given the large entropy of the output of generative models, the watermark can be directly extracted from the output produced by the model, thus permitting to verification of the watermark in a so-called box-free setting. In this way, it is possible to determine the source of images produced by generative models and associate any image to the generative model that produced it [2]. \n\nWith the exception of a few scattered works [3], the existing approaches for the watermarking of generative models, notably GANs, are designed for ownership verification, aiming at making it possible to retrieve the model authorship information from the generated images. These methods embed a fixed watermark, linking the model Zhihua Xia is the corresponding author. to the owner, and require retraining or finetuning if different a watermark has to be embedded in the model. \n\nIn this paper, we focus on a different scenario, hereafter referred to as GAN fingerprinting, illustrated in Fig. 1. In this scenario, the model distributor, simply referred to as the model owner, releases distinct watermarked model instances to different users, in such a way that the user-specific fingerprint can be recovered from the images produced by these models for copyright authentication and to trace back to the guilty user in case of a violation of the license agreements (traitor tracing).",
            "reference_string": "[265066820 | Fei et al. | 2023 | Citations: 6]"
        },
        {
            "title": "Continuous Concepts Removal in Text-to-image Diffusion Models",
            "venue": "arXiv.org",
            "year": 2024,
            "reference_count": 50,
            "citation_count": 3,
            "influential_citation_count": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2412.00580, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2170360833",
                    "name": "Tingxu Han"
                },
                {
                    "authorId": "3433022",
                    "name": "Weisong Sun"
                },
                {
                    "authorId": "2333453981",
                    "name": "Yanrong Hu"
                },
                {
                    "authorId": "2239197945",
                    "name": "Chunrong Fang"
                },
                {
                    "authorId": "2333368518",
                    "name": "Yonglong Zhang"
                },
                {
                    "authorId": "2333472479",
                    "name": "Shiqing Ma"
                },
                {
                    "authorId": "2322486553",
                    "name": "Tao Zheng"
                },
                {
                    "authorId": "2238950128",
                    "name": "Zhenyu Chen"
                },
                {
                    "authorId": "2154723145",
                    "name": "Zhenting Wang"
                }
            ],
            "abstract": "Text-to-image diffusion models have shown an impressive ability to generate high-quality images from input textual descriptions. However, concerns have been raised about the potential for these models to create content that infringes on copyrights or depicts disturbing subject matter. Removing specific concepts from these models is a promising potential solution to this problem. However, existing methods for concept removal do not work well in practical but challenging scenarios where concepts need to be continuously removed. Specifically, these methods lead to poor alignment between the text prompts and the generated image after the continuous removal process. To address this issue, we propose a novel approach called CCRT that includes a designed knowledge distillation paradigm. It constrains the text-image alignment behavior during the continuous concept removal process by using a set of text prompts generated through our genetic algorithm, which employs a designed fuzzing strategy. We conduct extensive experiments involving the removal of various concepts. The results evaluated through both algorithmic metrics and human studies demonstrate that our CCRT can effectively remove the targeted concepts in a continuous manner while maintaining the high generation quality (e.g., text-image alignment) of the model.",
            "corpus_id": 274436153,
            "sentences": [
                {
                    "corpus_id": "274436153",
                    "title": "Continuous Concepts Removal in Text-to-image Diffusion Models",
                    "text": "Advancements in Artificial Intelligence Generated Contents (AIGCs) [47] have revolutionized the field of image synthesis [29,38,49], among which text-to-image diffusion models enable the creation of high-quality images from textual descriptions [32,52]. However, this progress has also raised significant concerns regarding the potential misuse of these models [8,13,36,43]. Such misuse includes generating content that infringes on copyrights, such as mimicking specific artistic styles [30], intellectual properties [45,46], or creating disturbing and improper subject matter, including eroticism and violence [33]. Addressing these issues necessitates continuously removing those improper concepts from these models to prevent misuse and protect copyright from infringement. \n\nExisting techniques aiming to remove concepts from the text-to-image diffusion models can be categorized into two types. For a given concept that needs to be removed, the first group of methods refines the training data by discarding images containing the undesired concept and then retrains the model from scratch [3, 25,34]. The other set of methods removes the target concept without requiring full retraining. These methods instead utilize a small amount of additional data to fine-tune the models and modify specific neurons [10][11][12]. In real-world scenario, the improper concepts learned by the models such as copyright-protected art styles often discovered by the model owner in a continuous manner. For example, various artists may continually raise complaints that text-to-image generative AI can replicate their distinctive art style. Additionally, users or red-teaming teams [9] of these models may continuously flag instances where the models generate harmful or malicious contents. However, we find that these existing techniques do not perform well in scenarios where different concepts need to be continuously removed one after another, which is a practical and important use case. In detail, we observe that training data filtering methods require model owners to retrain the model from scratch, which is deemed impractical due to its exorbitant cost.",
                    "score": 0.44817580954474456,
                    "section_title": "Introduction",
                    "char_start_offset": 15,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 253
                        },
                        {
                            "start": 254,
                            "end": 374
                        },
                        {
                            "start": 375,
                            "end": 617
                        },
                        {
                            "start": 618,
                            "end": 777
                        },
                        {
                            "start": 780,
                            "end": 900
                        },
                        {
                            "start": 901,
                            "end": 1106
                        },
                        {
                            "start": 1107,
                            "end": 1193
                        },
                        {
                            "start": 1194,
                            "end": 1323
                        },
                        {
                            "start": 1324,
                            "end": 1490
                        },
                        {
                            "start": 1491,
                            "end": 1628
                        },
                        {
                            "start": 1629,
                            "end": 1778
                        },
                        {
                            "start": 1779,
                            "end": 1980
                        },
                        {
                            "start": 1981,
                            "end": 2151
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 121,
                            "end": 125,
                            "matchedPaperCorpusId": "245335280"
                        },
                        {
                            "start": 128,
                            "end": 131,
                            "matchedPaperCorpusId": "258959002"
                        },
                        {
                            "start": 245,
                            "end": 249,
                            "matchedPaperCorpusId": "248986576"
                        },
                        {
                            "start": 249,
                            "end": 252,
                            "matchedPaperCorpusId": "256827727"
                        },
                        {
                            "start": 361,
                            "end": 364,
                            "matchedPaperCorpusId": "256389993"
                        },
                        {
                            "start": 367,
                            "end": 370,
                            "matchedPaperCorpusId": "255546643"
                        },
                        {
                            "start": 612,
                            "end": 616,
                            "matchedPaperCorpusId": "253420366"
                        },
                        {
                            "start": 1102,
                            "end": 1105,
                            "matchedPaperCorpusId": "252917726"
                        },
                        {
                            "start": 1314,
                            "end": 1318,
                            "matchedPaperCorpusId": "257495777"
                        },
                        {
                            "start": 1318,
                            "end": 1322,
                            "matchedPaperCorpusId": "261276613"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.98095703125
                }
            ],
            "relevance_judgement": 0.98095703125,
            "relevance_judgment_input_expanded": "# Title: Continuous Concepts Removal in Text-to-image Diffusion Models\n# Venue: arXiv.org\n# Authors: Tingxu Han, Weisong Sun, Yanrong Hu, Chunrong Fang, Yonglong Zhang, Shiqing Ma, Tao Zheng, Zhenyu Chen, Zhenting Wang\n## Abstract\nText-to-image diffusion models have shown an impressive ability to generate high-quality images from input textual descriptions. However, concerns have been raised about the potential for these models to create content that infringes on copyrights or depicts disturbing subject matter. Removing specific concepts from these models is a promising potential solution to this problem. However, existing methods for concept removal do not work well in practical but challenging scenarios where concepts need to be continuously removed. Specifically, these methods lead to poor alignment between the text prompts and the generated image after the continuous removal process. To address this issue, we propose a novel approach called CCRT that includes a designed knowledge distillation paradigm. It constrains the text-image alignment behavior during the continuous concept removal process by using a set of text prompts generated through our genetic algorithm, which employs a designed fuzzing strategy. We conduct extensive experiments involving the removal of various concepts. The results evaluated through both algorithmic metrics and human studies demonstrate that our CCRT can effectively remove the targeted concepts in a continuous manner while maintaining the high generation quality (e.g., text-image alignment) of the model.\n## Introduction\nAdvancements in Artificial Intelligence Generated Contents (AIGCs) [47] have revolutionized the field of image synthesis [29,38,49], among which text-to-image diffusion models enable the creation of high-quality images from textual descriptions [32,52]. However, this progress has also raised significant concerns regarding the potential misuse of these models [8,13,36,43]. Such misuse includes generating content that infringes on copyrights, such as mimicking specific artistic styles [30], intellectual properties [45,46], or creating disturbing and improper subject matter, including eroticism and violence [33]. Addressing these issues necessitates continuously removing those improper concepts from these models to prevent misuse and protect copyright from infringement. \n\nExisting techniques aiming to remove concepts from the text-to-image diffusion models can be categorized into two types. For a given concept that needs to be removed, the first group of methods refines the training data by discarding images containing the undesired concept and then retrains the model from scratch [3, 25,34]. The other set of methods removes the target concept without requiring full retraining. These methods instead utilize a small amount of additional data to fine-tune the models and modify specific neurons [10][11][12]. In real-world scenario, the improper concepts learned by the models such as copyright-protected art styles often discovered by the model owner in a continuous manner. For example, various artists may continually raise complaints that text-to-image generative AI can replicate their distinctive art style. Additionally, users or red-teaming teams [9] of these models may continuously flag instances where the models generate harmful or malicious contents. However, we find that these existing techniques do not perform well in scenarios where different concepts need to be continuously removed one after another, which is a practical and important use case. In detail, we observe that training data filtering methods require model owners to retrain the model from scratch, which is deemed impractical due to its exorbitant cost.",
            "reference_string": "[274436153 | Han et al. | 2024 | Citations: 3]"
        }
    ],
    "retrieved": [
        {
            "corpus_id": "267412857",
            "title": "Copyright Protection in Generative AI: A Technical Perspective",
            "text": "In this section, we first introduce background knowledge on existing popular image generation models. Then, we define the problems related to the copyright issues for these image generation models, and introduce different strategies which can be utilized for data and model copyright protection.",
            "score": 0.6386118488806979,
            "section_title": "Copyright in Image Generation",
            "char_start_offset": 6019,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 101
                },
                {
                    "start": 102,
                    "end": 295
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.91748046875
        },
        {
            "corpus_id": "265351912",
            "title": "CopyScope: Model-level Copyright Infringement Quantification in the Diffusion Workflow",
            "text": "Although these work have studied various watermarking methods to achieve model traceability, these methods can only trace back to a chosen specific model in their experiments, without considering the interplay among models in the complex AI image generation task. \n\nTo address the challenge of potential copyright infringement in AI-generated images, we have proposed a new framework called CopyScope that could identify different copyright infringement sources at the model level in the AI image generation process and evaluate their impact. We have proposed a FID-based Shapley algorithm to assess the infringement contribution of each model in the diffusion workflow. Extensive results have demonstrated that our proposed CopyScope framework could effectively zoom in on the sources and quantify the impact of infringement models in AI image generation. Our work offers a promising solution for copyright traceability in AI image generation, which could also promote the legally compliant use of AI-generated content.",
            "score": 0.6295279713534645,
            "section_title": "RELATED WORK",
            "char_start_offset": 25505,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 263
                },
                {
                    "start": 266,
                    "end": 542
                },
                {
                    "start": 543,
                    "end": 670
                },
                {
                    "start": 671,
                    "end": 856
                },
                {
                    "start": 857,
                    "end": 1020
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.994140625
        },
        {
            "corpus_id": "268532352",
            "title": "A Dataset and Benchmark for Copyright Infringement Unlearning from Text-to-Image Diffusion Models",
            "text": "Text-to-image generative models have recently emerged as a significant topic in computer vision, demonstrating remarkable results in the area of generative modeling [14,35].These models bridge the gap between language and visual contents by generating realistic images from textual descriptions.However, rapid advancements in text-to-image generation techniques have raised concerns about copyright protection, particularly unauthorized reproduction of content, artistic creations, and portraits [4].A specific concern arises from the use of Stable Diffusion (SD), a state-of-the-art text-conditional latent diffusion model, which has sparked global discussions on copyright.\n\nMachine Unlearning (MU), which aims to eliminate the influence of specific target data or concepts, presents a promising solution to the aforementioned challenges.Several studies have provided intuitive evidence of MU's effectiveness in erasing the memory of copyrighted material from original models [42,7,26,8].However, the current body of research is limited by a lack of quantitative and systematic assessments that evaluate the extent to which MU can reduce the risk of copyright infringement.This limitation hinders the ability to make meaningful comparisons between existing MU approaches.The challenge is exacerbated by the inherent complexity of defining copyright infringement criteria for text-to-image generative models, as well as the scarcity of comprehensive inference datasets and standardized benchmarks for assessing copyright infringement.The absence of extensive copyright datasets obstructs researchers' efforts to fully comprehend the copyright infringement risks associated with generative models.This, in turn, restricts their capacity to develop superior MU algorithms capable of addressing the legal risks effectively.As shown in Fig. 1, datasets and benchmarks are essential for forgetting copyrighted content and evaluating unlearning methods.\n\nInitially, it is crucial to define what constitutes copyright infringement in contents produced by text-to-image generative models [38].In this study, we focus on infringement within 2D artistic works.Drawing on expertise from copyright protection specialists, including artists and lawyers, we contend that a unique painting style of an artist, virtual representations in artistic creations, and individual portraits all represent forms of creative expression deserving of legal protection.In order to identify instances of infringement in these contexts, a comprehensive analysis encompassing both technical and semantic aspects of the generated contents is indispensable.",
            "score": 0.625394950512463,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 173
                },
                {
                    "start": 173,
                    "end": 295
                },
                {
                    "start": 295,
                    "end": 500
                },
                {
                    "start": 500,
                    "end": 675
                },
                {
                    "start": 677,
                    "end": 840
                },
                {
                    "start": 840,
                    "end": 990
                },
                {
                    "start": 990,
                    "end": 1175
                },
                {
                    "start": 1175,
                    "end": 1273
                },
                {
                    "start": 1273,
                    "end": 1535
                },
                {
                    "start": 1535,
                    "end": 1697
                },
                {
                    "start": 1697,
                    "end": 1821
                },
                {
                    "start": 1821,
                    "end": 1948
                },
                {
                    "start": 1950,
                    "end": 2086
                },
                {
                    "start": 2086,
                    "end": 2151
                },
                {
                    "start": 2151,
                    "end": 2441
                },
                {
                    "start": 2441,
                    "end": 2624
                }
            ],
            "ref_mentions": [
                {
                    "start": 165,
                    "end": 169,
                    "matchedPaperCorpusId": "1033682"
                },
                {
                    "start": 169,
                    "end": 172,
                    "matchedPaperCorpusId": "245335280"
                },
                {
                    "start": 984,
                    "end": 987,
                    "matchedPaperCorpusId": "257687839"
                },
                {
                    "start": 987,
                    "end": 989,
                    "matchedPaperCorpusId": "261276613"
                },
                {
                    "start": 2081,
                    "end": 2085,
                    "matchedPaperCorpusId": "254366634"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.99072265625
        },
        {
            "corpus_id": "277955485",
            "title": "TWIG: Two-Step Image Generation using Segmentation Masks in Diffusion Models",
            "text": "In today's age of social media and marketing, copyright issues can be a major roadblock to the free sharing of images. Generative AI models have made it possible to create high-quality images, but concerns about copyright infringement are a hindrance to their abundant use. As these models use data from training images to generate new ones, it is often a daunting task to ensure they do not violate intellectual property rights. Some AI models have even been noted to directly copy copyrighted images, a problem often referred to as source copying. Traditional copyright protection measures such as watermarks and metadata have also proven to be futile in this regard. To address this issue, we propose a novel two-step image generation model inspired by the conditional diffusion model. The first step involves creating an image segmentation mask for some prompt-based generated images. This mask embodies the shape of the image. Thereafter, the diffusion model is asked to generate the image anew while avoiding the shape in question. This approach shows a decrease in structural similarity from the training image, i.e. we are able to avoid the source copying problem using this approach without expensive retraining of the model or user-centered prompt generation techniques. This makes our approach the most computationally inexpensive approach to avoiding both copyright infringement and source copying for diffusion model-based image generation.",
            "score": 0.6186750455484166,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.994140625
        },
        {
            "corpus_id": "265551515",
            "title": "VA3: Virtually Assured Amplification Attack on Probabilistic Copyright Protection for Text-to-Image Generative Models",
            "text": "In recent years, the advancement of large generative models [17,47,50] has revolutionized high-quality image synthesis [34,37,40], paving the way for commercial applications that enable the public to effortlessly craft their own artworks and designs [2,13,20,27,38,39]. Nevertheless, these models exhibit notable memorization capabilities to produce generations highly similar to the training data [3]. This resemblance raises growing concerns about copyright infringement, especially when copyrighted data is used for training [12,16,49,52]. \n\nTo address these concerns, there has been a surge in research focused on protecting copyrighted data from potential infringement by outputs of generative models [14, 21, Figure 2. Example outputs given the copyright image in Fig. 1 as target (potential infringing images are marked with red boundaries). In (a), using a benign prompt, we observe a high incidence of infringing content from models without copyright protection (\"w/o CP-k\"). In contrast, (b) shows that after applying the copyright protection mechanism (\"w/ CP-k\"), all samples are safe as CP-k rejects all infringing content. In (c), we find that amplification (Amp.) attack with a benign prompt results in limited success. Notably, by amplification attack with an adversarial prompt obtained from our proposed Anti-NAF algorithm, almost all output in (d) are copyright-infringed. 24,43,45,52,58]. Among these studies, a pivotal concept involves establishing a probabilistic upper-bound against the generation of infringing content by generative models. We refer to this suite of approaches as probabilistic copyright protection. Most notably, Vyas et al. [52] introduce a mathematical definition of copyright known as near-access freeness (NAF). Their method enforces generative diffusion models to exhibit akin behaviors as safe models, which has no access to the copyrighted image. By leveraging the improbability of safe models generating infringing content, the probability of generative models doing the same is thereby substantially reduced.",
            "score": 0.6153902038492155,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 269
                },
                {
                    "start": 270,
                    "end": 402
                },
                {
                    "start": 403,
                    "end": 542
                },
                {
                    "start": 545,
                    "end": 848
                },
                {
                    "start": 849,
                    "end": 984
                },
                {
                    "start": 985,
                    "end": 1136
                },
                {
                    "start": 1137,
                    "end": 1178
                },
                {
                    "start": 1179,
                    "end": 1234
                },
                {
                    "start": 1235,
                    "end": 1391
                },
                {
                    "start": 1392,
                    "end": 1408
                },
                {
                    "start": 1409,
                    "end": 1564
                },
                {
                    "start": 1565,
                    "end": 1640
                },
                {
                    "start": 1641,
                    "end": 1757
                },
                {
                    "start": 1758,
                    "end": 1895
                },
                {
                    "start": 1896,
                    "end": 2059
                }
            ],
            "ref_mentions": [
                {
                    "start": 64,
                    "end": 67,
                    "matchedPaperCorpusId": "14888175"
                },
                {
                    "start": 67,
                    "end": 70,
                    "matchedPaperCorpusId": "227209335"
                },
                {
                    "start": 123,
                    "end": 126,
                    "matchedPaperCorpusId": "245335280"
                },
                {
                    "start": 126,
                    "end": 129,
                    "matchedPaperCorpusId": "248986576"
                },
                {
                    "start": 250,
                    "end": 253,
                    "matchedPaperCorpusId": "253581213"
                },
                {
                    "start": 253,
                    "end": 256,
                    "matchedPaperCorpusId": "251253049"
                },
                {
                    "start": 256,
                    "end": 259,
                    "matchedPaperCorpusId": "252918469"
                },
                {
                    "start": 259,
                    "end": 262,
                    "matchedPaperCorpusId": "246240274"
                },
                {
                    "start": 262,
                    "end": 265,
                    "matchedPaperCorpusId": "251800180"
                },
                {
                    "start": 265,
                    "end": 268,
                    "matchedPaperCorpusId": "243938678"
                },
                {
                    "start": 398,
                    "end": 401,
                    "matchedPaperCorpusId": "256389993"
                },
                {
                    "start": 538,
                    "end": 541,
                    "matchedPaperCorpusId": "257050406"
                },
                {
                    "start": 1392,
                    "end": 1395,
                    "matchedPaperCorpusId": "256697414"
                },
                {
                    "start": 1395,
                    "end": 1398,
                    "matchedPaperCorpusId": "253420366"
                },
                {
                    "start": 1401,
                    "end": 1404,
                    "matchedPaperCorpusId": "257050406"
                },
                {
                    "start": 1667,
                    "end": 1671,
                    "matchedPaperCorpusId": "257050406"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.9912109375
        },
        {
            "corpus_id": "265351912",
            "title": "CopyScope: Model-level Copyright Infringement Quantification in the Diffusion Workflow",
            "text": "Web-based AI image generation has become an innovative art form that can generate novel artworks with the rapid development of the diffusion model. However, this new technique brings potential copyright infringement risks as it may incorporate the existing artworks without the owners' consent. Copyright infringement quantification is the primary and challenging step towards AI-generated image copyright traceability. Previous work only focused on data attribution from the training data perspective, which is unsuitable for tracing and quantifying copyright infringement in practice because of the following reasons: (1) the training datasets are not always available in public; (2) the model provider is the responsible party, not the image. Motivated by this, in this paper, we propose CopyScope, a new framework to quantify the infringement of AI-generated images from the model level. We first rigorously identify pivotal components within the AI image generation pipeline. Then, we propose to take advantage of Fr\\'echet Inception Distance (FID) to effectively capture the image similarity that fits human perception naturally. We further propose the FID-based Shapley algorithm to evaluate the infringement contribution among models. Extensive experiments demonstrate that our work not only reveals the intricacies of infringement quantification but also effectively depicts the infringing models quantitatively, thus promoting accountability in AI image-generation tasks.",
            "score": 0.6008146993188959,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.998046875
        },
        {
            "corpus_id": "257050406",
            "title": "On Provable Copyright Protection for Generative Models",
            "text": "Generative models for images, text, code, and other domains pose new challenges for ensuring their outputs are protected from copyright infringement. Such models are trained on a large corpus of data, where it is often impractical to ensure the training set is 100% free of copyrighted material. Furthermore, removing copyrighted material from training may also be undesirable. For example, a human author is free to read and use copyrighted material as inspiration for their work, as long as they do not copy it. Similarly, it may be beneficial to use copyrighted material when training in order to have more effective generative models. \n\nCopyright infringement by generative models can potentially arise in (at least) two manners. First, in the training phase, the algorithm could directly access copyrighted material, and the learned model itself could implicitly contain (e.g. coded in its weights) verbatim copies of some of this material. The copyright issues arising during training share many similarities with other settings in which algorithms scrape Figure 1: The CP-k Algorithm applied to diffusion models. The dataset is CIFAR-10 augmented with multiple copies of two images (images close to the augmented images are marked with red boundaries); hypothetically, suppose these two images are copyrighted works. The leftmost image shows generations from a model p that was trained on the full dataset, where we clearly see that p generates the two copyrighted works. Our algorithm starts by splitting this dataset into two disjoint datasets, making sure that copyrighted images are split into two different shards; for illustrative purposes, we do not deduplicate the dataset. The procedure then trains two models q1, q2 on these disjoint shards. The middle two figures show samples from the models q1, q2, again clearly showing memorization. However, note that q1 does not generate one of the copyrighted images and and q2 does not generate the other copyrighted image (as these were not in their respective datasets). Our algorithm CP-k then uses q1, q2, along with the original model p, to construct a model p k which has strong copyright protection guarantees.",
            "score": 0.5970721287383962,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 149
                },
                {
                    "start": 150,
                    "end": 295
                },
                {
                    "start": 296,
                    "end": 377
                },
                {
                    "start": 378,
                    "end": 513
                },
                {
                    "start": 514,
                    "end": 638
                },
                {
                    "start": 641,
                    "end": 733
                },
                {
                    "start": 734,
                    "end": 881
                },
                {
                    "start": 882,
                    "end": 945
                },
                {
                    "start": 946,
                    "end": 1119
                },
                {
                    "start": 1120,
                    "end": 1323
                },
                {
                    "start": 1324,
                    "end": 1478
                },
                {
                    "start": 1479,
                    "end": 1688
                },
                {
                    "start": 1689,
                    "end": 1758
                },
                {
                    "start": 1759,
                    "end": 1854
                },
                {
                    "start": 1855,
                    "end": 2031
                },
                {
                    "start": 2032,
                    "end": 2176
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.9892578125
        },
        {
            "corpus_id": "260155285",
            "title": "Not with my name! Inferring artists' names of input strings employed by Diffusion Models",
            "text": "The rapid advancement of generative models, particularly Diffusion Models [7], has led to a surge in high-quality, realistic image generation. These models have demonstrated immense potential for creative applications across various domains, including art, design, and advertising. However, their ability to replicate the styles of specific artists raises concerns about Intellectual Property (IP) rights and potential copyright infringements 4 [20,25,19]. \n\nAs the boundary between human creativity and machine-generated content becomes increasingly blurred, it is crucial to address the legal and ethical implications of using generative models to produce art, as well as to develop methods that evaluate the extent to which generated images are influenced by the works of real artists and ensure the protection of their intellectual property. \n\nSeveral studies and articles have explored the legal ramifications of generative models and their potential to infringe on copyrights 5 . In an article dated 2021 the challenges in determining copyright ownership for AI-generated works were discussed and authors emphasized the need for legal frameworks that could adequately address the unique nature of generative models 6 . MLQ.AI also reported on a copyright infringement case involving generative AI 7 , which sparked debates on the responsibilities of AI developers and users in protecting original creators' rights. In response to these concerns, legal scholars have delved into the complexities of copyright law as it pertains to AI-generated artworks. Gillotte [8] examined the challenges in assigning liability and protecting IP rights, arguing that existing copyright laws may not be sufficient to address the unique characteristics of AI-generated contents. Indeed, it is became a copyright dilemma and the need for a balance between innovation and IP protection is arising to ensure that creative works are safeguarded without stifling technological advancements [16]. \n\nIt is not easy to determine how images generated by tools like DALL-E 2 or Midjourney are created by combination of images employed at training time. But, it could useful to develop tools able to deduce the textual prompts that generated an investigated image. Recently, a Kaggle competition was launched on the task 8 but it still lacks of effective methods. However, to make images that appear with an artist's style the prompt of the generating tools should necessary contain the artist's name.",
            "score": 0.5924222546043683,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 142
                },
                {
                    "start": 143,
                    "end": 281
                },
                {
                    "start": 282,
                    "end": 456
                },
                {
                    "start": 459,
                    "end": 845
                },
                {
                    "start": 848,
                    "end": 985
                },
                {
                    "start": 986,
                    "end": 1224
                },
                {
                    "start": 1225,
                    "end": 1420
                },
                {
                    "start": 1421,
                    "end": 1558
                },
                {
                    "start": 1559,
                    "end": 1767
                },
                {
                    "start": 1768,
                    "end": 1979
                },
                {
                    "start": 1982,
                    "end": 2131
                },
                {
                    "start": 2132,
                    "end": 2242
                },
                {
                    "start": 2243,
                    "end": 2341
                },
                {
                    "start": 2342,
                    "end": 2479
                }
            ],
            "ref_mentions": [
                {
                    "start": 74,
                    "end": 77,
                    "matchedPaperCorpusId": "234357997"
                },
                {
                    "start": 1568,
                    "end": 1571,
                    "matchedPaperCorpusId": "233748873"
                },
                {
                    "start": 1974,
                    "end": 1978,
                    "matchedPaperCorpusId": "113882623"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.982421875
        },
        {
            "corpus_id": "265352103",
            "title": "On Copyright Risks of Text-to-Image Diffusion Models",
            "text": "Our aim is to generate images with copyright infringements. Hence, we evaluate the quality of generated images by checking whether they contain copyrighted content. For this purpose, we compute the proportion of generated images that have at least one identified chunk. According to Figure 9, approximately 70% of images from various tested diffusion models, except for SD XL, contain at least one identified chunk using our copyright test. For images that do not have identified chunks, they could still contain copyrighted content. This is due to the fact that image collection and annotation may not cover all copyrighted features in reality.",
            "score": 0.5911191621502779,
            "section_title": "Quality of Generated Images",
            "char_start_offset": 28241,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 59
                },
                {
                    "start": 60,
                    "end": 164
                },
                {
                    "start": 165,
                    "end": 269
                },
                {
                    "start": 270,
                    "end": 440
                },
                {
                    "start": 441,
                    "end": 533
                },
                {
                    "start": 534,
                    "end": 645
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.96240234375
        },
        {
            "corpus_id": "265352103",
            "title": "On Copyright Risks of Text-to-Image Diffusion Models",
            "text": "Diffusion models excel in many generative modeling tasks, notably in creating images from text prompts, a task referred to as text-to-image (T2I) generation. Despite the ability to generate high-quality images, these models often replicate elements from their training data, leading to increasing copyright concerns in real applications in recent years. In response to this raising concern about copyright infringement, recent studies have studied the copyright behavior of diffusion models when using direct, copyrighted prompts. Our research extends this by examining subtler forms of infringement, where even indirect prompts can trigger copyright issues. Specifically, we introduce a data generation pipeline to systematically produce data for studying copyright in diffusion models. Our pipeline enables us to investigate copyright infringement in a more practical setting, involving replicating visual features rather than entire works using seemingly irrelevant prompts for T2I generation. We generate data using our proposed pipeline to test various diffusion models, including the latest Stable Diffusion XL. Our findings reveal a widespread tendency that these models tend to produce copyright-infringing content, highlighting a significant challenge in this field.",
            "score": 0.5863427452031565,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.9970703125
        },
        {
            "corpus_id": "257050406",
            "title": "On Provable Copyright Protection for Generative Models",
            "text": "Our algorithm CP-k then uses q1, q2, along with the original model p, to construct a model p k which has strong copyright protection guarantees. The last image is the outputs of p k , showing it is highly unlikely to output either of the copyrighted images, even though each of q1, q2 and p has memorized some of these images. See Section 4 for more details (and for a discussion with regards to our displayed model generations having used the same noise on the diffusion paths). \n\nsignificant amounts of data, including search-engine indexing and digitizing books. Here, the question of what constitutes a copyright infringement is largely a question of \"fair use.\" This work does not examine these fair use issues that arise in the training phase, and we refer the reader to the several legal precedents in this area [Samuelson, 2021]. \n\nThe second notable source of potential infringement is in the deployment phase, where a user provides a prompt x to the model to obtain some output y. Apriori, we cannot rule out the possibility that y is either a verbatim copy or substantially similar to some copyrighted training data. Moreover, unlike search engines, generative models do not keep track of the provenance of their outputs. Hence, a user of such an output y (e.g., a software company using generated code, or a designer using a generated image) has no easy way to verify that it does not infringe upon any copyrighted material. It is this issue of preventing deployment-time copyright infringement that is the focus of this work. \n\nOur contributions. We give a formal definition -\"near-access freeness\" -bounding the extent to which a learned generative model's output can be substantially influenced by a particular piece of copyrighted data that the model was trained on. We also give a procedure that transforms (under certain assumptions) any generative model learning algorithm A into an algorithm A k , which protects against violations under our definition. In particular, the model output by A k will (1) be at most k-bits far from a \"safe\" model (which is not committing copyright infringement), and (2) will have performance reasonably close to the model output by the original algorithm A (in a quantifiable sense, based on properties of A).",
            "score": 0.5863183564225078,
            "section_title": "Introduction",
            "char_start_offset": 2047,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 144
                },
                {
                    "start": 145,
                    "end": 326
                },
                {
                    "start": 327,
                    "end": 479
                },
                {
                    "start": 482,
                    "end": 565
                },
                {
                    "start": 566,
                    "end": 666
                },
                {
                    "start": 667,
                    "end": 837
                },
                {
                    "start": 840,
                    "end": 990
                },
                {
                    "start": 991,
                    "end": 1127
                },
                {
                    "start": 1128,
                    "end": 1232
                },
                {
                    "start": 1233,
                    "end": 1436
                },
                {
                    "start": 1437,
                    "end": 1538
                },
                {
                    "start": 1541,
                    "end": 1559
                },
                {
                    "start": 1560,
                    "end": 1782
                },
                {
                    "start": 1783,
                    "end": 1973
                },
                {
                    "start": 1974,
                    "end": 2261
                }
            ],
            "ref_mentions": [
                {
                    "start": 819,
                    "end": 836,
                    "matchedPaperCorpusId": "239770285"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.9853515625
        },
        {
            "corpus_id": "258865475",
            "title": "Can Copyright be Reduced to Privacy?",
            "text": "Similarly, in another class action, against Stable Diffusion, Midjourney, and DeviantArt, plaintiffs argue that by training their system on webscraped images, the defendant infringes millions of artists' rights [3]. Allegedly, the images produced by these systems, in response to prompts provided by the systems' users, are derived solely from the training images, which belong to plaintiffs, and, as such, are considered unauthorized derivative works of the plaintiffs' images [55, \u00a7 106 (2)]. \n\nA preliminary question is whether it is lawful to make use of copyrighted content in the course of training [23,34,36]. There are compelling arguments to suggest that such intermediary copying might be considered fair use [36]. For example, Google's Book Search Project-entailing the mass digitization of copyrighted books from university library collections to create a searchable database of millions of books-was held by US courts to be fair use [22]. Then, there is a claim that generative models reproduce protected copyright expressions from the input content on which the model was trained. However, to claim that the output of a generative model infringes her copyright, a plaintiff must prove not only that the model had access to her copyrighted work, but also that the alleged copy is substantially similar to her original work [8,53] Identifying what constitutes \"substantial similarity,\" and unlawful copying remains a pressing challenge. Recent studies have proposed measurable metrics to quantify copyright infringement [5,9,51,59]. One approach, [5,59] asserts that a machine generating output content substantially similar to an input content does not infringe that input content copyright if the machine would have reasonably generated the same output content even without accessing the input content. This argument can be illustrated as follows: Suppose that Alice outputs content A and Bob claims it plagiarizes content B. Alice might argue that she never saw content B, and would reason that this means she did not infringe Bob's copyright.",
            "score": 0.5794942465943368,
            "section_title": "Introduction",
            "char_start_offset": 2032,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 215
                },
                {
                    "start": 216,
                    "end": 494
                },
                {
                    "start": 497,
                    "end": 616
                },
                {
                    "start": 617,
                    "end": 724
                },
                {
                    "start": 725,
                    "end": 951
                },
                {
                    "start": 952,
                    "end": 1094
                },
                {
                    "start": 1095,
                    "end": 1448
                },
                {
                    "start": 1449,
                    "end": 1544
                },
                {
                    "start": 1545,
                    "end": 1816
                },
                {
                    "start": 1817,
                    "end": 2058
                }
            ],
            "ref_mentions": [
                {
                    "start": 605,
                    "end": 609,
                    "matchedPaperCorpusId": "152665774"
                },
                {
                    "start": 612,
                    "end": 615,
                    "matchedPaperCorpusId": "219342558"
                },
                {
                    "start": 719,
                    "end": 723,
                    "matchedPaperCorpusId": "219342558"
                },
                {
                    "start": 1532,
                    "end": 1535,
                    "matchedPaperCorpusId": "225067265"
                },
                {
                    "start": 1559,
                    "end": 1562,
                    "matchedPaperCorpusId": "225067265"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.93701171875
        },
        {
            "corpus_id": "270258236",
            "title": "Tackling copyright issues in AI image generation through originality estimation and genericization",
            "text": "Generative models have demonstrated performance rivaling humans in creative tasks such as those involving image synthesis and language processing. 1,2 However, this progress has also raised concerns about copyright protection, leading to numerous lawsuits filed by creators against AI developers. 3 Copyright law protects creators' rights, encouraging new creations while balancing their interests with those of the public. 4 Generative models are trained on massive datasets that often contain copyrighted works and are capable of producing outputs closely resembling their training data, potentially resulting in violation of exclusive rights of copyright owners. 5 ][9][10] Although some methods have been found effective to some extent, significant risks of copyright infringement remains. 6,7 n this paper, we propose a method for quantifying the level of originality and modifying the outputs of generative models to those that have lower originality values. These modified outputs are more generic and less likely to imitate distinctive features of copyrighted materials. As a practical algorithm for mitigating copyright risks, we introduce PREGen (Prompt Rewriting-Enhanced Genericization), which combines this genericization method with the existing prompt rewriting method, 7 adopted by a major commercial model DALL\u2022E. 7,11 The effectiveness of prompt rewriting is enhanced by further adding a negative prompt that instructs the model not to generate specific content, 7 an element that is also incorporated into PREGen. \n\nWe demonstrate that PREGen significantly improves the performance of prompt rewriting accompanied by negative prompting (we refer to this as the standard method when there is no ambiguity) in reducing the likelihood of generating images of copyrighted characters. In particular, experiments using the COPYCAT benchmark 7 show that PREGen reduces the likelihood of text-to-image generative models generating copyrighted characters by more than half compared to the standard method, when the user provides copyrighted characters' names as prompts. Furthermore, when the prompt does not directly reference copyrighted characters, PREGen almost completely eliminates the generation of these characters.",
            "score": 0.5744019880365882,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 150
                },
                {
                    "start": 151,
                    "end": 298
                },
                {
                    "start": 299,
                    "end": 425
                },
                {
                    "start": 426,
                    "end": 667
                },
                {
                    "start": 668,
                    "end": 797
                },
                {
                    "start": 798,
                    "end": 964
                },
                {
                    "start": 965,
                    "end": 1078
                },
                {
                    "start": 1079,
                    "end": 1335
                },
                {
                    "start": 1336,
                    "end": 1532
                },
                {
                    "start": 1535,
                    "end": 1798
                },
                {
                    "start": 1799,
                    "end": 2080
                },
                {
                    "start": 2081,
                    "end": 2233
                }
            ],
            "ref_mentions": [
                {
                    "start": 147,
                    "end": 149,
                    "matchedPaperCorpusId": "245335280"
                },
                {
                    "start": 297,
                    "end": 298,
                    "matchedPaperCorpusId": "259844568"
                },
                {
                    "start": 424,
                    "end": 425,
                    "matchedPaperCorpusId": "10463592"
                },
                {
                    "start": 666,
                    "end": 667,
                    "matchedPaperCorpusId": "258515384"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.984375
        },
        {
            "corpus_id": "265352103",
            "title": "On Copyright Risks of Text-to-Image Diffusion Models",
            "text": "Diffusion models have gained widespread popularity as the new frontier of generative models. Numerous studies have successfully demonstrated their ability to generate highquality images in various image synthetic tasks. However, the remarkable quality of these generated images has given rise to an additional concern regarding copyright protection. Recent research has indicated that diffusion models often tend to memorize images in the training dataset [Carlini et al., 2023]. As a result, diffusion models can effortlessly generate copyrighted content through memorization [Somepalli et al., 2023a;Somepalli et al., 2023b]. The apprehension surrounding copyright protection in diffusion models has also evolved Figure 1: Generate copyrighted content in ChatGPT. ChatGPT refuses to generate images when directly prompted for copyrighted material. However, adversarial prompts generated with our method that do not directly ask for copyrighted material still manage to generate copyrighted material, in this case, the Superman logo. \n\ninto a tangible threat, as multiple lawsuits related to copyright infringement have been initiated against companies that utilize diffusion models for commercial purposes. Notably, there have been instances of lawsuits: Stability AI and Mid-Journey are both facing civil suits for training their models on artists' work without their consent thereby allowing their models to replicate the style and work of such artists [Vincent, 2023]. \n\nAttempts have been made to prevent the generation of copyrighted content, such as OpenAI's addition of filters on ChatGPT to prevent the generation of copyrighted images. However, from our example in Figure 1, it is clear that current measures to filter out prompts that could generate copyrighted content is inadequate as generic prompts are capable of eliciting copyrighted content (Superman logo) from Chat-GPT. Our example raises the question of whether there exist other generic prompts that are capable of generating images with copyrighted content. Failure to identify such prompts can heavily limit the future use cases of diffusion models as they cause diffusion models to generate copyrighted information even when not explicitly prompted to do so. \n\nOur contributions. (1) We form a framework to create prompts for T2I tasks that are generic in language semantics but can still trigger partial copyright infringements in image generation by various diffusion models.",
            "score": 0.5716552090952953,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 92
                },
                {
                    "start": 93,
                    "end": 219
                },
                {
                    "start": 220,
                    "end": 349
                },
                {
                    "start": 350,
                    "end": 479
                },
                {
                    "start": 480,
                    "end": 627
                },
                {
                    "start": 628,
                    "end": 765
                },
                {
                    "start": 766,
                    "end": 849
                },
                {
                    "start": 850,
                    "end": 1034
                },
                {
                    "start": 1037,
                    "end": 1208
                },
                {
                    "start": 1209,
                    "end": 1473
                },
                {
                    "start": 1476,
                    "end": 1646
                },
                {
                    "start": 1647,
                    "end": 1890
                },
                {
                    "start": 1891,
                    "end": 2031
                },
                {
                    "start": 2032,
                    "end": 2234
                },
                {
                    "start": 2237,
                    "end": 2255
                },
                {
                    "start": 2256,
                    "end": 2453
                }
            ],
            "ref_mentions": [
                {
                    "start": 577,
                    "end": 602,
                    "matchedPaperCorpusId": "254366634"
                },
                {
                    "start": 602,
                    "end": 626,
                    "matchedPaperCorpusId": "227209335"
                },
                {
                    "start": 1457,
                    "end": 1472,
                    "matchedPaperCorpusId": "266900037"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.99658203125
        },
        {
            "corpus_id": "269214015",
            "title": "\\copyright Plug-in Authorization for Human Content Copyright Protection in Text-to-Image Model",
            "text": "The non-infringing model, shown in the leftmost column of Figure 3(b), generates images that are notably different from those of the base model, highlighting the effectiveness of the multiple extraction operations.\n\nUpon using the individual style copyright plug-ins, the images within the red boxes exhibit the target styles with a striking resemblance to the base model's outputs.This outcome demonstrates that \u00a9plug-ins can restore the model's capability to generate artwork in the target styles while adhering to copyright restrictions.",
            "score": 0.5705794406839257,
            "section_title": "Extraction and Combination of Artists' Styles Replication",
            "char_start_offset": 17692,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 214
                },
                {
                    "start": 216,
                    "end": 382
                },
                {
                    "start": 382,
                    "end": 540
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.6494140625
        },
        {
            "corpus_id": "259064265",
            "title": "DeepfakeArt Challenge: A Benchmark Dataset for Generative AI Art Forgery and Data Poisoning Detection",
            "text": "Definition: Copyright Infringement in a Generative AI Model -Consider a generative model M : R H\u00d7W \u00d7C \u00d7 Q \u2192 R H \u2032 \u00d7W \u2032 \u00d7C \u2032 where Q could represent the set of any additional input to the generative model. Assume the model M has been trained on some dataset D \u2282 R H\u00d7W \u00d7C , within which a subset D \u2286 D is the set of copyright-protected examples. Given some norm \u2225 \u2022 \u2225 : R H \u2032 \u00d7W \u2032 \u00d7C \u2032 \u2192 [0, +\u221e), some threshold \u03b4 > 0, and some monotonically non-decreasing function f : N \u222a {0} \u2192 [0, +\u221e), we classify a generated output y \u2208 R H \u2032 \u00d7W \u2032 \u00d7C \u2032 by our model M as infringing on copyright, with respect to \u2225 \u2022 \u2225, \u03b4, and f , if and only if there exists some x \u2208 D \u2282 R H\u00d7W \u00d7C such that any one of (or both of) the following conditions hold: \n\n\u2022 There exist some region \n\nof a significant size and some transformation T : \n\n\u2022 There exist some edge detection model E : \n\n] of a significant size, and some transformation T : R H\u00d7W \u00d7C \u2192 R H \u2032 \u00d7W \u2032 \u00d7C \u2032 , s.t. \n\nIf either condition (1) or ( 2) is met, it is concluded that the model M infringes upon the copyright protection for x, by the intensity of f (|\u2126|)/\u03b4 \u25a1 We make a few remarks on our definition above. 1) The region \u2126 is a set of indices of the form (i, j, k). Putting it in subscript suppresses the entries outside the region to zero, so that they will not contribute to the norm. 2) The transformation T can only be composed of geometric transformations including resizing, zooming, translation, rotation, flipping, etc. 3) The smaller the threshold \u03b4 and the larger the region \u2126, the higher the infringement intensity f (|\u2126|)/\u03b4. 4) The function f is used to make the threshold \u03b4 be adaptive to the size of the region.",
            "score": 0.5666455290177413,
            "section_title": "Art Forgery",
            "char_start_offset": 7592,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 204
                },
                {
                    "start": 205,
                    "end": 343
                },
                {
                    "start": 344,
                    "end": 729
                },
                {
                    "start": 732,
                    "end": 757
                },
                {
                    "start": 760,
                    "end": 809
                },
                {
                    "start": 812,
                    "end": 855
                },
                {
                    "start": 858,
                    "end": 944
                },
                {
                    "start": 947,
                    "end": 1145
                },
                {
                    "start": 1146,
                    "end": 1204
                },
                {
                    "start": 1205,
                    "end": 1325
                },
                {
                    "start": 1326,
                    "end": 1575
                },
                {
                    "start": 1576,
                    "end": 1664
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.8798828125
        },
        {
            "corpus_id": "270357944",
            "title": "Evaluating and Mitigating IP Infringement in Visual Generative AI",
            "text": "We first discuss the formulation of the IP Infringement problem in text-to-image generation models as follows: Definition 1. (IP Infringement) Given a generated image x, we say x infringes the intellectual property c if L(x, X c ) < \u03c4 , where X c is a set of real images with intellectual property c.L is a distance measurement and \u03c4 is a threshold value.\n\nIn this paper, we focus on the IP infringement problems on the characters such as the Spider-Man.\n\nWe consider the practical scenario where the infringer (the users causes the IP infringement) only has the black-box access of the model, i.e., the infringer can not access the parameters and the internal outputs of the mode.It is practical as many state-of-the-art visual generative AI models are close-sourced, and the users can only access them via API or the website.",
            "score": 0.5641556508409165,
            "section_title": "Problem Formulation",
            "char_start_offset": 8972,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 300
                },
                {
                    "start": 300,
                    "end": 355
                },
                {
                    "start": 357,
                    "end": 454
                },
                {
                    "start": 456,
                    "end": 681
                },
                {
                    "start": 681,
                    "end": 827
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.94091796875
        },
        {
            "corpus_id": "269214015",
            "title": "\\copyright Plug-in Authorization for Human Content Copyright Protection in Text-to-Image Model",
            "text": "This paper addresses the contentious issue of copyright infringement in images generated by text-to-image models, sparking debates among AI developers, content creators, and legal entities. State-of-the-art models create high-quality content without crediting original creators, causing concern in the artistic community. To mitigate this, we propose the \\copyright Plug-in Authorization framework, introducing three operations: addition, extraction, and combination. Addition involves training a \\copyright plug-in for specific copyright, facilitating proper credit attribution. Extraction allows creators to reclaim copyright from infringing models, and combination enables users to merge different \\copyright plug-ins. These operations act as permits, incentivizing fair use and providing flexibility in authorization. We present innovative approaches,\"Reverse LoRA\"for extraction and\"EasyMerge\"for seamless combination. Experiments in artist-style replication and cartoon IP recreation demonstrate \\copyright plug-ins' effectiveness, offering a valuable solution for human copyright protection in the age of generative AIs. The code is available at https://github.com/zc1023/-Plug-in-Authorization.git.",
            "score": 0.5638076410115025,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.9736328125
        },
        {
            "corpus_id": "270063652",
            "title": "Automatic Jailbreaking of the Text-to-Image Generative AI Systems",
            "text": "Text-to-Image (T2I) generative models [2,9,21,18,19,29] are mostly trained on massive image data from the web, which are known to contain diverse copyrighted, privacy-sensitive, and harmful images.Recent works [28,27,4] demonstrate that diffusion-based image generative models memorize a portion of the training data, allowing the replication of the copyrighted contents [31,33].Although what models are used in recent commercial T2I systems is mostly unknown to the public, we find they also easily generate copyrighted contents (Figure 1a).Such copyright violation is one of the most critical real-world safety problems associated with generative models, and there are several ongoing lawsuits [24,13,7] against the service providers regarding this matter.\n\nTo prevent such potential copyright violations, ChatGPT [21] and Copilot [18] censor user requests by blocking generation of copyrighted materials or rephrase the users' prompts, to prevent them.However, are they really secure against unauthorized reproduction of copyrighted materials?To the best of our knowledge, there is no work on quantitative evaluation of the copyright violation by the commercial T2I systems, making it difficult for the service providers to red-team their systems (Figure 1b).Furthermore, for intellectual property (IP) owners, it requires a large amount of effort to verify the usage of contents in those systems via manual trial-and-error processes (Figure 1b).To evaluate the safety of the T2I systems, we construct a copyright Violation dataset for T2I models, termed VioT.This dataset is comprised of five categories of copyrighted contents that include the characters, logos, products, architectures, and arts, legally protected in the form of copyright [20,22,12].Then, we attempted naive prompts to induce the T2I systems to generate copyright-violated contents.Surprisingly, we observe that current commercial T2I systems, including Midjourney [19], Copilot [18], and Gemini [29], result in copyright violations with a low block rate, 13.3%, even with such naive prompts.",
            "score": 0.5637700737871199,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 197
                },
                {
                    "start": 197,
                    "end": 379
                },
                {
                    "start": 379,
                    "end": 542
                },
                {
                    "start": 542,
                    "end": 758
                },
                {
                    "start": 760,
                    "end": 955
                },
                {
                    "start": 955,
                    "end": 1046
                },
                {
                    "start": 1046,
                    "end": 1262
                },
                {
                    "start": 1262,
                    "end": 1449
                },
                {
                    "start": 1449,
                    "end": 1563
                },
                {
                    "start": 1563,
                    "end": 1757
                },
                {
                    "start": 1757,
                    "end": 1856
                },
                {
                    "start": 1856,
                    "end": 2066
                }
            ],
            "ref_mentions": [
                {
                    "start": 38,
                    "end": 41,
                    "matchedPaperCorpusId": "264403242"
                },
                {
                    "start": 210,
                    "end": 214,
                    "matchedPaperCorpusId": "258987384"
                },
                {
                    "start": 214,
                    "end": 217,
                    "matchedPaperCorpusId": "254366634"
                },
                {
                    "start": 217,
                    "end": 219,
                    "matchedPaperCorpusId": "256389993"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.9697265625
        },
        {
            "corpus_id": "258479926",
            "title": "Should ChatGPT and Bard Share Revenue with Their Data Providers? A New Business Model for the AI Era",
            "text": "In addition, Getty images has filed a lawsuit against Stability AI, accusing Stable Diffusion of \"brazen infringement of Getty Images' intellectual property on a staggering scale\" and misusing more than 12 million Getty photos to train its Stable Diffusion AI image-generation system 23 . Faced with the issue of copyright infringement, AI image generator companies have emphasized that they will comply with the Digital Millennium Copyright Act (DMCA) and protect the copyright of image owners. \n\nMeanwhile, the U.S. Copyright Office has taken the position that AI-generated images do not qualify for copyright protection, as they are not the result of human authorship and therefore do not meet the definition of originality 24 . We hope that AI image generators will be better regulated in the near future. To ensure that every artist and copyrighted work to be respected, AI image generators must to be more transparent to disclose their training datasets. However, at the same time, we also expect the revenue-sharing business model discussed earlier based on LLMs could to be applied to AI image generators. As discussed in Section II-D, the connection between human artists, copyright owners, and AI image tools should not be viewed as a hostile, zero-sum game but rather as a collaborative and mutually beneficial relationship. \n\nSimilar to the revenue-sharing business model of LLMs, we can also establish a scoring system for image providers of AI image generators, which would be based on the combination of some image classification models and image similarity calculators. In principle, as long as we replace \"text\" with \"image\" in Figures 3,5,6 from Section IV, we can use some image embedding techniques to build an image classifier, and image similarity calculator on the image training dataset for a certain AI image generator 25 , and establish a scoring system to measure the degree of engagement for each image provider, or even individual images. Different from LLMs, image owners/providers may be more concerned about the engagement of their individual artworks in the image generator, rather than just the overall engagement of their all works. Additionally, rather than turning each image provider into a class to build an image classifier, it may be more reasonable to classify images by art style, genre, topic rather than by image providers. 23",
            "score": 0.5630598550487793,
            "section_title": "A. AI Text-to-Image Generators",
            "char_start_offset": 73125,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 288
                },
                {
                    "start": 289,
                    "end": 495
                },
                {
                    "start": 498,
                    "end": 731
                },
                {
                    "start": 732,
                    "end": 809
                },
                {
                    "start": 810,
                    "end": 960
                },
                {
                    "start": 961,
                    "end": 1113
                },
                {
                    "start": 1114,
                    "end": 1335
                },
                {
                    "start": 1338,
                    "end": 1585
                },
                {
                    "start": 1586,
                    "end": 1967
                },
                {
                    "start": 1968,
                    "end": 2167
                },
                {
                    "start": 2168,
                    "end": 2371
                }
            ],
            "ref_mentions": [
                {
                    "start": 1657,
                    "end": 1658,
                    "matchedPaperCorpusId": "6866988"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.96875
        },
        {
            "corpus_id": "265352103",
            "title": "On Copyright Risks of Text-to-Image Diffusion Models",
            "text": "Copyright infringement for generative models. For copyright infringement, we focus on the copyright regulation in the US [Copyright Office, 2022]. Nevertheless, the concept of copyright applies to the regulation in other countries. In the context of copyright law in the US, whether a piece of copyrighted material can be used by others is governed by the concept of Fair Use which permits the use of copyrighted material only in a transformative manner that is distinct from the original work. There have been legal precedents which demonstrate that structural similarity can lead to infringement claims. Given that such generative models are trained on datasets such as LAION-5B [Schuhmann et al., 2022] which contains publicly available copyrighted data, if the models generate images with visual features that have substantial structural similarity to the original copyrighted images, this would likely be grounds for claims of copyright infringement. \n\nThis has implications in commercial settings such as companies selling proprietary image generation models as a service or an individual user, particularly when such models produce or are used to create and sell material bearing strong structural similarities to copyrighted images. Thus, there is a potential legal risk for both the providers of image generation models and their users, especially in commercial settings where the 'transformative' nature of generated images may not meet the legal threshold established in copyright law if they possess substantial structural similarity to the original image. \n\nObjective of our data generation pipeline. We aim to systematically generate prompts that are considered generic and not related to any copyrighted topic, but still capable of triggering the generation of copyrighted content from diffusion models. We formally define two desired properties our generated prompts should satisfy. Definition 1. (Prompt sensitivity) Given a semantic measurement f s (\u2022) and a tolerance \u03f5, prompt p is sensitive to a \n\nIn practice, f s can be a text encoder that encodes plain text to be text embeddings for comparison. We detail distance measure D[\u2022||\u2022] in the following discussion. According to the above definition, a prompt is considered to be sensitive to a topic if they have similar language semantics. Moreover, a prompt is adversarial if it can trigger a T2I model to generate copyrighted content. Hence, the objective of our data generation pipeline is to systematically create non-sensitive adversarial prompts.",
            "score": 0.5559779813540305,
            "section_title": "Problem Formulation",
            "char_start_offset": 6237,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 45
                },
                {
                    "start": 46,
                    "end": 146
                },
                {
                    "start": 147,
                    "end": 231
                },
                {
                    "start": 232,
                    "end": 494
                },
                {
                    "start": 495,
                    "end": 605
                },
                {
                    "start": 606,
                    "end": 955
                },
                {
                    "start": 958,
                    "end": 1240
                },
                {
                    "start": 1241,
                    "end": 1568
                },
                {
                    "start": 1571,
                    "end": 1613
                },
                {
                    "start": 1614,
                    "end": 1818
                },
                {
                    "start": 1819,
                    "end": 1898
                },
                {
                    "start": 1899,
                    "end": 2016
                },
                {
                    "start": 2019,
                    "end": 2119
                },
                {
                    "start": 2120,
                    "end": 2183
                },
                {
                    "start": 2184,
                    "end": 2309
                },
                {
                    "start": 2310,
                    "end": 2406
                },
                {
                    "start": 2407,
                    "end": 2522
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.9970703125
        },
        {
            "corpus_id": "272146279",
            "title": "RLCP: A Reinforcement Learning-based Copyright Protection Method for Text-to-Image Diffusion Model",
            "text": "Recently, text-to-image diffusion models have garnered significant attention in research. These advanced methods (Balaji et al. 2023;Nichol et al. 2022;Rombach et al. 2022a;Saharia et al. 2022) have demonstrated exceptional capabilities in converting textual descriptions into highly accurate and visually coherent images. The advancements in these techniques have unlocked numerous possibilities for various downstream tasks, including image editing (Avrahami, Lischinski, and Fried 2022;Ho, Jain, and Abbeel 2020a;Kawar et al. 2023), image denoising (Ho, Jain, and Abbeel 2020a;Xie et al. 2023), and super-resolution (Sohl-Dickstein et al. 2015;Ho, Jain, and Abbeel 2020b). \n\nWhile the progress in text-to-image generative models has profoundly impacted different industries, it also presents significant challenges for copyright protection. These models utilize extensive training data that may include copyrighted works, which they are sometimes capable of memorizing (Carlini et al. 2023). This ability can result in the pro- duction of images that closely resemble protected content (See in Figure 1), posing significant challenges to copyright protection (Elkin-Koren et al. 2023). Recent legal cases, such as those involving Stable Diffusion (Rombach et al. 2022b) and Midjourney(Mansour 2023), highlight concerns over the use of copyrighted data in AI training, where the models potentially infringe on the rights of numerous artists. These cases highlight a growing concern: Could the high-quality content synthesized by these generative AIs be excessively similar to copyrighted training data, potentially violating the rights of copyright holders? \n\nVarious methods have been proposed for source data copyright protection. One approach involves using unrecognizable examples (Gandikota et al. 2023;Zhang et al. 2023) that prevent models from learning key features of protected images either during inference or training stages. However, this method is highly dependent on the specific image and model, and it lacks general reliability.",
            "score": 0.5517990732658707,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 89
                },
                {
                    "start": 90,
                    "end": 322
                },
                {
                    "start": 323,
                    "end": 675
                },
                {
                    "start": 678,
                    "end": 843
                },
                {
                    "start": 844,
                    "end": 994
                },
                {
                    "start": 995,
                    "end": 1188
                },
                {
                    "start": 1189,
                    "end": 1443
                },
                {
                    "start": 1444,
                    "end": 1659
                },
                {
                    "start": 1662,
                    "end": 1734
                },
                {
                    "start": 1735,
                    "end": 1939
                },
                {
                    "start": 1940,
                    "end": 2047
                }
            ],
            "ref_mentions": [
                {
                    "start": 152,
                    "end": 173,
                    "matchedPaperCorpusId": "245335280"
                },
                {
                    "start": 173,
                    "end": 192,
                    "matchedPaperCorpusId": "248986576"
                },
                {
                    "start": 451,
                    "end": 489,
                    "matchedPaperCorpusId": "244714366"
                },
                {
                    "start": 516,
                    "end": 534,
                    "matchedPaperCorpusId": "252918469"
                },
                {
                    "start": 619,
                    "end": 647,
                    "matchedPaperCorpusId": "248986576"
                },
                {
                    "start": 1162,
                    "end": 1187,
                    "matchedPaperCorpusId": "258297834"
                },
                {
                    "start": 1250,
                    "end": 1272,
                    "matchedPaperCorpusId": "245335280"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.98583984375
        },
        {
            "corpus_id": "276558342",
            "title": "CopyJudge: Automated Copyright Infringement Identification and Mitigation in Text-to-Image Diffusion Models",
            "text": "Text-to-image generative models (Rombach et al., 2022;Betker et al., 2023;Team et al., 2023;Esser et al., 2024;Hurst et al., 2024;Zhang et al., 2023b;a;Hintersdorf et al., 2024) have transformed creative industries by producing detailed visuals from text prompts. However, these models have been found to sometimes memorize and reproduce content from their training data (Carlini et al., 2023;Somepalli et al., 2023a;Ren et al., 2024;Wang et al., 2024c;Shi et al., 2024b;a;Zhang et al., 2024). This raises significant concerns about copyright infringement, especially when the generated images closely resemble existing copyrighted works. According to U.S. law (rot, 1970), also referenced by most countries, a work can be considered infringing if it constitutes substantial similarity to another work1 . Therefore, determining whether AI-generated images infringe on copyright requires a clear and reliable method to compare them with copyrighted materials to identify substantial similarity. \n\nHowever, identifying substantial similarity is not a trivial task. There are already some methods to assess image similarity through distance-based metrics, e.g., L 2 norm (Carlini et al., 2023). However, we found that these manually designed metrics do not always align with the human judgment for infringement determination. Additionally, it often suffer from insufficient generalization ability and lack interpretable results. This motivates the need for an approach that better measures substantial similarity, one that is more humancentered, interpretable, and generalized to handle copyright infringement identification in AI-generated images. \n\nRecently, large-scale models have already been successfully applied as judges in fields such as finance, education, and healthcare (Gu et al., 2024;Li et al., 2024;Zhuge et al., 2024). In this paper, we attempt to leverage large visionlanguage models (LVLMs) to model the practical court decisions on substantial similarity.",
            "score": 0.5502174902513126,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 263
                },
                {
                    "start": 264,
                    "end": 493
                },
                {
                    "start": 494,
                    "end": 638
                },
                {
                    "start": 639,
                    "end": 804
                },
                {
                    "start": 805,
                    "end": 993
                },
                {
                    "start": 996,
                    "end": 1062
                },
                {
                    "start": 1063,
                    "end": 1191
                },
                {
                    "start": 1192,
                    "end": 1322
                },
                {
                    "start": 1323,
                    "end": 1425
                },
                {
                    "start": 1426,
                    "end": 1645
                },
                {
                    "start": 1648,
                    "end": 1832
                },
                {
                    "start": 1833,
                    "end": 1972
                }
            ],
            "ref_mentions": [
                {
                    "start": 32,
                    "end": 54,
                    "matchedPaperCorpusId": "245335280"
                },
                {
                    "start": 54,
                    "end": 74,
                    "matchedPaperCorpusId": "264403242"
                },
                {
                    "start": 92,
                    "end": 111,
                    "matchedPaperCorpusId": "268247980"
                },
                {
                    "start": 130,
                    "end": 150,
                    "matchedPaperCorpusId": "256827727"
                },
                {
                    "start": 371,
                    "end": 393,
                    "matchedPaperCorpusId": "256389993"
                },
                {
                    "start": 393,
                    "end": 417,
                    "matchedPaperCorpusId": "254366634"
                },
                {
                    "start": 1168,
                    "end": 1190,
                    "matchedPaperCorpusId": "256389993"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.99169921875
        },
        {
            "corpus_id": "276575866",
            "title": "Can Large Vision-Language Models Detect Images Copyright Infringement from GenAI?",
            "text": "The rapid advancement of generative artificial intelligence (GenAI) has ushered in a new era of content creation, enabling the synthesis of highquality text, images, and multimedia content at an unprecedented scale. While these innovations have expanded creative possibilities and applications across industries, they have also raised significant ethical and legal concerns, particularly regarding intellectual property (IP) rights (Sag, 2023;Poland). One of the most pressing issues is the unauthorized reproduction of copyrighted material, where generative models may inadvertently produce outputs that closely resemble or replicate IP-protected content (Zirpoli, 2023;Dzuong et al., 2024;Sag, 2023;Poland;Wang et al., 2024). This issue has led to widespread debates among legal experts, policymakers, and AI researchers on the potential liabilities and regulatory measures required to address copyright infringement in AI-generated content. \n\nExisting efforts to mitigate copyright concerns in generative models have primarily focused on two key approaches: \u2460 reducing memorization during training using techniques such as differential privacy (Dwork et al., 2014), which limits the retention of specific data points to prevent models from reproducing protected content (Abadi et al., 2016;Chen et al., 2022;Dockhorn et al., 2022), and \u2461 employing prompt engineering strategies that discourage the generation of IP-infringing material through explicit negative prompts (Wang et al., 2024;He et al., 2024) or optimized safe prompt modifications (Chin et al., 2023;Rando et al., 2022). While these approaches offer some level of control over generative outputs, they do not directly address the challenge of detecting copyright infringement in already-generated content. As a result, there is an urgent need for robust evaluation methods and benchmarks to assess the ability of AI models-specifically large vision-language models (LVLMs)-to identify potential instances of copyright violations. \n\nVision-language models (VLMs), which integrate both textual and visual data to enable crossmodal reasoning, have demonstrated remarkable capabilities in tasks such as image classification, visual question answering (VQA) (Antol et al., 2015), and multimodal understanding.",
            "score": 0.5488261177400049,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 215
                },
                {
                    "start": 216,
                    "end": 451
                },
                {
                    "start": 452,
                    "end": 727
                },
                {
                    "start": 728,
                    "end": 943
                },
                {
                    "start": 946,
                    "end": 1586
                },
                {
                    "start": 1587,
                    "end": 1771
                },
                {
                    "start": 1772,
                    "end": 1995
                },
                {
                    "start": 1998,
                    "end": 2270
                }
            ],
            "ref_mentions": [
                {
                    "start": 432,
                    "end": 443,
                    "matchedPaperCorpusId": "258515384"
                },
                {
                    "start": 691,
                    "end": 701,
                    "matchedPaperCorpusId": "258515384"
                },
                {
                    "start": 1147,
                    "end": 1167,
                    "matchedPaperCorpusId": "207178262"
                },
                {
                    "start": 1273,
                    "end": 1293,
                    "matchedPaperCorpusId": "207241585"
                },
                {
                    "start": 1293,
                    "end": 1311,
                    "matchedPaperCorpusId": "250210875"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.97802734375
        },
        {
            "corpus_id": "269293130",
            "title": "Concept Arithmetics for Circumventing Concept Inhibition in Diffusion Models",
            "text": "Recent advances in Text-to-Image (T2I) generation [24,26,28] have led to the rapid growth of applications enabled by the models, including many commercial projects as well as creative applications by the general public.On the other hand, they can also be used for generating deep fakes, hateful or inappropriate images [2,9], copyrighted materials or artistic styles [30].Trained on vast amounts of data scraped from the web, these models also learn to reproduce the biases and stereotypes present in the data [2,8,17,19].While some legal [9,18] and ethical [27] questions concerning image generation models remain unsolved, the scientific community is developing methods to limit their malicious utility, while keeping them open and accessible to the community.",
            "score": 0.5469239437169371,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 219
                },
                {
                    "start": 219,
                    "end": 372
                },
                {
                    "start": 372,
                    "end": 522
                },
                {
                    "start": 522,
                    "end": 762
                }
            ],
            "ref_mentions": [
                {
                    "start": 54,
                    "end": 57,
                    "matchedPaperCorpusId": "245335280"
                },
                {
                    "start": 57,
                    "end": 60,
                    "matchedPaperCorpusId": "248986576"
                },
                {
                    "start": 322,
                    "end": 324,
                    "matchedPaperCorpusId": "158596286"
                },
                {
                    "start": 539,
                    "end": 542,
                    "matchedPaperCorpusId": "158596286"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.58349609375
        },
        {
            "corpus_id": "277151077",
            "title": "Guardians of Generation: Dynamic Inference-Time Copyright Shielding with Adaptive Guidance for AI Image Generation",
            "text": "Modern text-to-image generative models can inadvertently reproduce copyrighted content memorized in their training data, raising serious concerns about potential copyright infringement. We introduce Guardians of Generation, a model agnostic inference time framework for dynamic copyright shielding in AI image generation. Our approach requires no retraining or modification of the generative model weights, instead integrating seamlessly with existing diffusion pipelines. It augments the generation process with an adaptive guidance mechanism comprising three components: a detection module, a prompt rewriting module, and a guidance adjustment module. The detection module monitors user prompts and intermediate generation steps to identify features indicative of copyrighted content before they manifest in the final output. If such content is detected, the prompt rewriting mechanism dynamically transforms the user's prompt by sanitizing or replacing references that could trigger copyrighted material while preserving the prompt's intended semantics. The adaptive guidance module adaptively steers the diffusion process away from flagged content by modulating the model's sampling trajectory. Together, these components form a robust shield that enables a tunable balance between preserving creative fidelity and ensuring copyright compliance. We validate our method on a variety of generative models such as Stable Diffusion, SDXL, and Flux, demonstrating substantial reductions in copyrighted content generation with negligible impact on output fidelity or alignment with user intent. This work provides a practical, plug-and-play safeguard for generative image models, enabling more responsible deployment under real-world copyright constraints. Source code is available at: https://respailab.github.io/gog",
            "score": 0.5415004116085296,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.9951171875
        },
        {
            "corpus_id": "278534729",
            "title": "DFA-CON: A Contrastive Learning Approach for Detecting Copyright Infringement in DeepFake Art",
            "text": "Deep Fake art generative models are vulnerable to violating copyright terms by producing images that mimic or closely resemble content protected under copyright [10]. A formal mathematical formulation of copyright infringement in this context is introduced in [4]. For clarity and contextual relevance, we present a simplified version here: \n\nHere, y denotes the generated image and x is a potentially copyright-protected training image. The operator T (\u2022) represents a geometric transformation (e.g., resizing, flipping, rotation), and \u2126 is a region of significant size within the image. The function A(\u2022) defines the domain of representation-either raw pixel space or an edge-based representation. The term f (|\u2126|) is a monotonic function adjusting sensitivity based on region size, and \u03b4 is a fixed similarity threshold. Infringement is said to occur if the distance in the representation space for any region \u2126 falls below this threshold.",
            "score": 0.5409420964778178,
            "section_title": "Copyright Infringement in Art",
            "char_start_offset": 5336,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 166
                },
                {
                    "start": 167,
                    "end": 264
                },
                {
                    "start": 265,
                    "end": 340
                },
                {
                    "start": 343,
                    "end": 437
                },
                {
                    "start": 438,
                    "end": 588
                },
                {
                    "start": 589,
                    "end": 699
                },
                {
                    "start": 700,
                    "end": 823
                },
                {
                    "start": 824,
                    "end": 942
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.98974609375
        },
        {
            "corpus_id": "270357944",
            "title": "Evaluating and Mitigating IP Infringement in Visual Generative AI",
            "text": "For instance, we find that the contents produced by these AI models, such as images and videos, can inadvertently include characters that bear a striking resemblance to IP-protected characters owned by other companies.In Figure 1 and Figure 2, we demonstrate examples of the IP infringement of the generated content of the state-of-the-art visual generative AI models, i.e., DALL-E 3 and Midjourney.As can be clearly seen, all models generates an image which is highly similar to the character \"Spider-Man\" when using the prompt \"Generate an image of the Spider-Man\".Furthermore, the model can even generate the \"Spider-Man\" images without directly mentioning the character's name in the prompt.This is particularly problematic when the visual generation involves well-known characters belonging to large companies in the movie, gaming, and entertainment industries, such as Sony, Marvel, and Nintendo.The increasing sophistication of these visual generative AI systems might raise complex legal and ethical questions around the boundaries of fair use, derivative works, and the appropriate ownership of the generated content.To investigate the IP infringement issues of the state-of-the-art visual generative AIs on the IP protected characters owned by large companies in the entertainment industries, we design a straightforward method to generate prompts that can effectively trigger these models to activate models' IP infringement issues on specific target character, even without directly stating the character's name.It works in a black-box setting where the weight parameters and the internal outputs of the models are not available.We employed both prompts that explicitly included the name of the IP-protected character as well as prompts that described the target character without naming them, and study the IP infringement behaviors of the models under these prompts.To evaluate the extent of IP infringement issues in visual generative AI models, we create a benchmark consisting of six representative IP protected characters owned by large companies (e.g., Sony, Marvel, Nintendo, and DC Entertainment).We then evaluate the extent of intellectual property infringement by these AI models on this benchmark.Our experiments demonstrate that the IP infringement issues are widely existing in both open-source and commercial closed source models.",
            "score": 0.5401772970836513,
            "section_title": "Introduction",
            "char_start_offset": 2163,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 218
                },
                {
                    "start": 218,
                    "end": 399
                },
                {
                    "start": 399,
                    "end": 567
                },
                {
                    "start": 567,
                    "end": 695
                },
                {
                    "start": 695,
                    "end": 902
                },
                {
                    "start": 902,
                    "end": 1126
                },
                {
                    "start": 1126,
                    "end": 1524
                },
                {
                    "start": 1524,
                    "end": 1641
                },
                {
                    "start": 1641,
                    "end": 1880
                },
                {
                    "start": 1880,
                    "end": 2118
                },
                {
                    "start": 2118,
                    "end": 2221
                },
                {
                    "start": 2221,
                    "end": 2357
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.951171875
        },
        {
            "corpus_id": "267412406",
            "title": "Organic or Diffused: Can We Distinguish Human Art from AI-generated Images?",
            "text": "First introduced in 2020, diffusion models quickly replaced GANs as the state-of-the-art models for image synthesis [21]. This was quickly followed by extension to text-to-image generation [46,54]. \n\nLater instances included multiple open-source models from Stable Diffusion [52,56,[67][68][69], and commercial models from Midjourney, DALL-E, and Adobe Firefly. Diffusion models face a growing number of social and ethical concerns. Base models require enormous amounts of training data, often obtained without consent through web-scraping. Midjourney trained their model using data from over 16,000 artists, the vast majority without consent [31]. Stability AI trained on datasets from LAION [56,59], containing millions of copyrighted works. These copyright infringement issues have led to multiple classaction lawsuits [17,35]. Even in smaller volumes, artists are finding their works being used without consent in finetuned models using techniques such as LoRA [3]. \n\nAs these models continue to improve in quality, many of their users have attempted to pass AI-generated images as human art. AIgenerated images have been used to win art competitions, fooling judges of digital art, photography, and book covers [27,57,58]. Companies that promote human art have found themselves using AI content provided to them by third-party vendors [50].",
            "score": 0.5353262639987916,
            "section_title": "Generative Image Diffusion models",
            "char_start_offset": 6257,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 121
                },
                {
                    "start": 122,
                    "end": 197
                },
                {
                    "start": 200,
                    "end": 361
                },
                {
                    "start": 362,
                    "end": 432
                },
                {
                    "start": 433,
                    "end": 540
                },
                {
                    "start": 541,
                    "end": 648
                },
                {
                    "start": 649,
                    "end": 743
                },
                {
                    "start": 744,
                    "end": 830
                },
                {
                    "start": 831,
                    "end": 969
                },
                {
                    "start": 972,
                    "end": 1096
                },
                {
                    "start": 1097,
                    "end": 1227
                },
                {
                    "start": 1228,
                    "end": 1345
                }
            ],
            "ref_mentions": [
                {
                    "start": 116,
                    "end": 120,
                    "matchedPaperCorpusId": "234357997"
                },
                {
                    "start": 279,
                    "end": 282,
                    "matchedPaperCorpusId": "245335280"
                },
                {
                    "start": 693,
                    "end": 697,
                    "matchedPaperCorpusId": "245335280"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.91650390625
        },
        {
            "corpus_id": "273228819",
            "title": "TeaserGen: Generating Teasers for Long Documentaries",
            "text": "We note that, as a generative model trained on copyrighted material, our proposed system has the potential to generate samples that could lead to copyright infringement. Moreover, as discussed in Section 6, the proposed system can make mistakes in matching names and faces, and may pair irrelevant visuals to the generated narrations, resulting in risks of factual inaccuracy.",
            "score": 0.5319865218446947,
            "section_title": "ETHICS STATEMENT",
            "char_start_offset": 30856,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 169
                },
                {
                    "start": 170,
                    "end": 376
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.6923828125
        },
        {
            "corpus_id": "268513090",
            "title": "CGI-DM: Digital Copyright Authentication for Diffusion Models via Contrasting Gradient Inversion",
            "text": "Recent years have witnessed the advancement of Diffusion Models (DMs) in computer vision.These models demonstrate exceptional capabilities across a diverse array of tasks, including image editing [14], and video editing [35], among others.Particularly, the emergence of few-shot generation techniques, exemplified by Dreambooth [24] and swiftly capturing the style or primary objects by fine-tuning a pretrained model on a small set of images.This process enables efficient and high-quality art imitation and art style transfer by utilizing the fine-tuned model.\n\nHowever, these advanced few-shot generation techniques also spark widespread concerns regarding the protection of copyright for human artworks and individual photographs.There is a growing fear that parties may exploit the generative capabilities of DMs to create and profit from derivative artworks based on existing artists' works, without obtaining proper authorization [6,19].Concurrently, concerns arise regarding the creation of fabricated images of individuals without their consent [32].All of these collectively pose a serious threat to the security of personal data and intellectual property rights.\n\nTo address these critical concerns, a line of approaches focuses on safeguarding individual images by incorporating adversarial attacks, such as AdvDM [17], Glaze [25], and Anti-Dreambooth [30].The adversarial attacks can disrupt the generative output, rendering the images unlearnable by diffusion models.These methods are implemented ahead of the fine-tuning process, and as such, we consider them as precaution approaches.\n\nAnother line of approaches facing such threats is copyright authentication.Copyright authentication compares the similarity between the output images of diffusion models and the given images to validate unauthorized usage.Such a process can serve as legal proof for validating infringement (See Appendix A for more details), and has been utilized as evidence in ongoing legal cases concerning violations enabled by DMs [31].This process happens after the fine-tuning and thus we consider it as a post-caution approach.However, current copyright authentication methods face difficulties in producing output images closely resembling training samples due to the pursuit of diversity in generative models.Consequently, it becomes difficult to ascertain whether a particular training sample has been utilized solely based on the generated output of the model for postcaution methods.",
            "score": 0.5283723913625435,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 89
                },
                {
                    "start": 89,
                    "end": 239
                },
                {
                    "start": 239,
                    "end": 443
                },
                {
                    "start": 443,
                    "end": 562
                },
                {
                    "start": 564,
                    "end": 734
                },
                {
                    "start": 734,
                    "end": 944
                },
                {
                    "start": 944,
                    "end": 1059
                },
                {
                    "start": 1059,
                    "end": 1173
                },
                {
                    "start": 1175,
                    "end": 1369
                },
                {
                    "start": 1369,
                    "end": 1481
                },
                {
                    "start": 1481,
                    "end": 1600
                },
                {
                    "start": 1602,
                    "end": 1677
                },
                {
                    "start": 1677,
                    "end": 1824
                },
                {
                    "start": 1824,
                    "end": 2026
                },
                {
                    "start": 2026,
                    "end": 2120
                },
                {
                    "start": 2120,
                    "end": 2304
                },
                {
                    "start": 2304,
                    "end": 2481
                }
            ],
            "ref_mentions": [
                {
                    "start": 328,
                    "end": 332,
                    "matchedPaperCorpusId": "251800180"
                },
                {
                    "start": 1326,
                    "end": 1330,
                    "matchedPaperCorpusId": "256697414"
                },
                {
                    "start": 1364,
                    "end": 1368,
                    "matchedPaperCorpusId": "257766375"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.984375
        },
        {
            "corpus_id": "270620122",
            "title": "Fantastic Copyrighted Beasts and How (Not) to Generate Them",
            "text": "Others have demonstrated that these models can potentially reconstruct or replicate copyrighted content from their training data (Carlini et al., 2020;car, 2023). Efforts to mitigate these risks include provable copyright protection strategies inspired by differential privacy (Vyas et al., 2023), decoding-time prevention (Golatkar et al., 2024) that guide the generation process away from copyright concepts and model editing and unlearning that aim to remove copyrighted content from model weights (Gong et al., 2024;Chefer et al., 2023;Zhang et al., 2023). Ma et al. (2024) introduces benchmark for measuring copyright infringement unlearning from text-to-image diffusion models. Another recent study by Kim et al. (2024) also examines keywords potentially important for image generation, but only includes the character name along with the associated movie or TV program as keywords. They also show that LLM-optimized descriptions can generate images similar to copyrighted characters on proprietary models such as ChatGPT, Copilot, and Gemini. However, their optimized prompts do not explicitly exclude the characters' names. Similarly, Zhang et al. (2024a) focuses on building attacks that can generated particular concepts,including some copyrighted characters. While these works focus on attacks and do not explore effective mitigation methods, our work focuses on building an analysis framework motivated by legal considerations like substantial similarity test for copyrighted characters and provide more in-depth understanding for both how easy it is to generate copyrighted characters as well as the effectiveness of defenses.",
            "score": 0.526538935511049,
            "section_title": "RELATED WORK",
            "char_start_offset": 27810,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 162
                },
                {
                    "start": 163,
                    "end": 560
                },
                {
                    "start": 561,
                    "end": 683
                },
                {
                    "start": 684,
                    "end": 888
                },
                {
                    "start": 889,
                    "end": 1049
                },
                {
                    "start": 1050,
                    "end": 1131
                },
                {
                    "start": 1132,
                    "end": 1269
                },
                {
                    "start": 1270,
                    "end": 1639
                }
            ],
            "ref_mentions": [
                {
                    "start": 129,
                    "end": 151,
                    "matchedPaperCorpusId": "229156229"
                },
                {
                    "start": 277,
                    "end": 296,
                    "matchedPaperCorpusId": "257050406"
                },
                {
                    "start": 323,
                    "end": 346,
                    "matchedPaperCorpusId": "268732879"
                },
                {
                    "start": 520,
                    "end": 540,
                    "matchedPaperCorpusId": "256416326"
                },
                {
                    "start": 540,
                    "end": 559,
                    "matchedPaperCorpusId": "257833863"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.921875
        },
        {
            "corpus_id": "270620122",
            "title": "Fantastic Copyrighted Beasts and How (Not) to Generate Them",
            "text": "Recent studies show that image and video generation models can be prompted to reproduce copyrighted content from their training data, raising serious legal concerns about copyright infringement. Copyrighted characters (e.g., Mario, Batman) present a significant challenge: at least one lawsuit has already awarded damages based on the generation of such characters. Consequently, commercial services like DALL-E have started deploying interventions. However, little research has systematically examined these problems: (1) Can users easily prompt models to generate copyrighted characters, even if it is unintentional?; (2) How effective are the existing mitigation strategies? To address these questions, we introduce a novel evaluation framework with metrics that assess both the generated image's similarity to copyrighted characters and its consistency with user intent, grounded in a set of popular copyrighted characters from diverse studios and regions. We show that state-of-the-art image and video generation models can still generate characters even if characters' names are not explicitly mentioned, sometimes with only two generic keywords (e.g., prompting with\"videogame, plumber\"consistently generates Nintendo's Mario character). We also introduce semi-automatic techniques to identify such keywords or descriptions that trigger character generation. Using this framework, we evaluate mitigation strategies, including prompt rewriting and new approaches we propose. Our findings reveal that common methods, such as DALL-E's prompt rewriting, are insufficient alone and require supplementary strategies like negative prompting. Our work provides empirical grounding for discussions on copyright mitigation strategies and offers actionable insights for model deployers implementing these safeguards.",
            "score": 0.5259821304162764,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.99267578125
        },
        {
            "corpus_id": "270258236",
            "title": "Tackling copyright issues in AI image generation through originality estimation and genericization",
            "text": "We introduced a method to genericize the output of generative models, thereby reducing the risk of copyright infringement. We further proposed PREGen, a practical algorithm for mitigating copyright risks, which combines our genericization method with prompt rewriting. Our method leverages the principle that the level of originality of works determines the strength of their copyright protection, as well as the inherent capability of generative models to learn the distribution of training data. By evaluating the performance of PREGen using the COPYCAT suite, we have shown that PREGen significantly enhances the performance of the standard prompt rewriting method. However, this improvement comes with a trade-off: PREGen requires additional computation to generate multiple samples, most of which are ultimately discarded, along with rewritten prompts. Additionally, the fine-grained consistency with the original prompt may be compromised. \n\nOur work has certain limitations in its scope. While the general framework for originality estimation and genericization is broadly applicable, we have focused on the generation of copyrighted characters using text-to-image generative models. Future research can test our method on the generation of other types of materials and the use of different generative models, such as those for text and video, and investigate appropriate distance metrics and their effectiveness. Another consideration is the potential for the genericization process to amplify undesirable patterns in the generative model's output distribution. Specifically, multiple samples generated during the genericization process might disproportionately represent certain demographics or cultural elements. The resulting generic output, which is, in a sense, the median expression of these patterns, could unintentionally reinforce such biases. These risks should be carefully evaluated in future research.",
            "score": 0.5251635351597358,
            "section_title": "Discussion",
            "char_start_offset": 31498,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 122
                },
                {
                    "start": 123,
                    "end": 268
                },
                {
                    "start": 269,
                    "end": 497
                },
                {
                    "start": 498,
                    "end": 668
                },
                {
                    "start": 669,
                    "end": 857
                },
                {
                    "start": 858,
                    "end": 945
                },
                {
                    "start": 948,
                    "end": 994
                },
                {
                    "start": 995,
                    "end": 1190
                },
                {
                    "start": 1191,
                    "end": 1420
                },
                {
                    "start": 1421,
                    "end": 1569
                },
                {
                    "start": 1570,
                    "end": 1722
                },
                {
                    "start": 1723,
                    "end": 1860
                },
                {
                    "start": 1861,
                    "end": 1922
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.98583984375
        },
        {
            "corpus_id": "265352103",
            "title": "On Copyright Risks of Text-to-Image Diffusion Models",
            "text": "We find then when provided with generic prompts for images in these categories, diffusion models tend to generate images that contain the Superman logo or character (in the former case) or the Starbucks logo (in the latter). To efficiently generate candidates for target topics for both these avenues, we leverage the abilities of Language Models to provide prompts originating from famous game and movie titles that either contain polysemic terms or can be easily overrepresented. \n\nIt is notable to mention that we exclude topics related to artwork and individual artists from our designated target topics. Within the scope of this study, our primary emphasis lies in assessing partial copyright infringement. Specifically, this involves finding the presence of copyrighted content that is visually discernible within image segments. We find that while diffusion models can accurately replicate the style of artists [Casper et al., 2023], this may be a form of derivative work [Cornell, 2022] which is less straightforward to ascertain copyright infringement. Consequently, we refrain from delving into copyright matters pertaining to artistic style and creations by specific artists. The identification of style replication within artworks demands a more intricate approach, involving deeper consideration of how the style is employed, which exceeds the boundaries of the scope of this work.",
            "score": 0.5233449170192119,
            "section_title": "B Details on Collecting Potentially",
            "char_start_offset": 35390,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 224
                },
                {
                    "start": 225,
                    "end": 481
                },
                {
                    "start": 484,
                    "end": 608
                },
                {
                    "start": 609,
                    "end": 711
                },
                {
                    "start": 712,
                    "end": 835
                },
                {
                    "start": 836,
                    "end": 1061
                },
                {
                    "start": 1062,
                    "end": 1186
                },
                {
                    "start": 1187,
                    "end": 1394
                }
            ],
            "ref_mentions": [
                {
                    "start": 979,
                    "end": 994,
                    "matchedPaperCorpusId": "234357997"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.94482421875
        },
        {
            "corpus_id": "275471553",
            "title": "Has an AI model been trained on your images?",
            "text": "From a simple text prompt, generative-AI image models can create stunningly realistic and creative images bounded, it seems, by only our imagination. These models have achieved this remarkable feat thanks, in part, to the ingestion of billions of images collected from nearly every corner of the internet. Many creators have understandably expressed concern over how their intellectual property has been ingested without their permission or a mechanism to opt out of training. As a result, questions of fair use and copyright infringement have quickly emerged. We describe a method that allows us to determine if a model was trained on a specific image or set of images. This method is computationally efficient and assumes no explicit knowledge of the model architecture or weights (so-called black-box membership inference). We anticipate that this method will be crucial for auditing existing models and, looking ahead, ensuring the fairer development and deployment of generative AI models.",
            "score": 0.5228468743600835,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.97119140625
        },
        {
            "corpus_id": "257826680",
            "title": "Qualitative Failures of Image Generation Models and Their Application in Detecting Deepfakes",
            "text": "As previously mentioned, a method for identifying whether an image is generated or not is through reverse image search.Generative models may memorize images partially or in their entirety, as seen in the examples presented in Figure 36.This phenomenon has raised concerns regarding copyright infringement, as generated images may include watermarks from the original images.For more information on this issue, please refer to the this link.",
            "score": 0.5222197246825905,
            "section_title": "Memorization and Copyright",
            "char_start_offset": 32805,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 119
                },
                {
                    "start": 119,
                    "end": 236
                },
                {
                    "start": 236,
                    "end": 374
                },
                {
                    "start": 374,
                    "end": 440
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.94921875
        },
        {
            "corpus_id": "267412857",
            "title": "Copyright Protection in Generative AI: A Technical Perspective",
            "text": "Recently, generative AI models have been extensively developed to produce a wide range of synthesized content, including text, images, audio, and code, among others. For example, advanced image generative models, such as Diffusion Models (DMs) [45], can produce highly realistic and detailed photographs and paintings. Similarly, large language models (LLMs) like ChatGPT [2] can be leveraged to compose coherent and creative text articles with arbitrary genres and storylines. We refer to these advanced models as \"Deep Generative Models\" (DGMs). However, because of the remarkable fidelity and authenticity of the generated contents from DGMs, concerns have been raised regarding the associated copyright issues. For example, the New York Times sued OpenAI and Microsoft for using copyrighted work for training chatGPT 1 . Midjourney was accused to output images copied from commercial films 2 . These copyright issues may pertain to various parties involved in the generation process. In specifics: \n\n(1) Source Data Owners. To generate high-quality contents, DGMs require training on a large amount of data, collected from various resources such as the Internet, even without the permission of the original data owner. As demonstrated in recent studies [15,16], it is likely that both DMs and LLMs can produce contents with a high coincidence to parts of the training data samples. Besides, DGMs can also be utilized to directly edit the contents or imitate the artistic styles from source images and texts. These facts raise concerns for the data owners, as DGMs can generate data that closely resembles or replicates their original data without authorization. \n\n(2) DGM Users. DGMs are also frequently utilized by DGM users and assist DGM users for creative composing. \n\nHowever, whether the DGMs users should receive copyright for their generated contents is still a complex and evolving legal and ethical issue. For example, in 2023 3 , US Copyright Office refused to register a graphic novel for an artist facilitated by Midjourney 4 (a popular AI image generation model). However, another giant generative-AI company, OpenAI, claims that the model users own the created data via the models from OpenAI, including the right to reprint, sell, and merchandise 5 . \n\n(3) DGM Providers.",
            "score": 0.5208781497242713,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 165
                },
                {
                    "start": 166,
                    "end": 318
                },
                {
                    "start": 319,
                    "end": 477
                },
                {
                    "start": 478,
                    "end": 547
                },
                {
                    "start": 548,
                    "end": 714
                },
                {
                    "start": 715,
                    "end": 824
                },
                {
                    "start": 825,
                    "end": 897
                },
                {
                    "start": 898,
                    "end": 987
                },
                {
                    "start": 988,
                    "end": 1001
                },
                {
                    "start": 1004,
                    "end": 1027
                },
                {
                    "start": 1028,
                    "end": 1222
                },
                {
                    "start": 1223,
                    "end": 1385
                },
                {
                    "start": 1386,
                    "end": 1511
                },
                {
                    "start": 1512,
                    "end": 1665
                },
                {
                    "start": 1668,
                    "end": 1682
                },
                {
                    "start": 1683,
                    "end": 1774
                },
                {
                    "start": 1777,
                    "end": 1919
                },
                {
                    "start": 1920,
                    "end": 2081
                },
                {
                    "start": 2082,
                    "end": 2270
                },
                {
                    "start": 2273,
                    "end": 2291
                }
            ],
            "ref_mentions": [
                {
                    "start": 244,
                    "end": 248,
                    "matchedPaperCorpusId": "219955663"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.97509765625
        },
        {
            "corpus_id": "267412857",
            "title": "Copyright Protection in Generative AI: A Technical Perspective",
            "text": "Generative AI has witnessed rapid advancement in recent years, expanding their capabilities to create synthesized content such as text, images, audio, and code. The high fidelity and authenticity of contents generated by these Deep Generative Models (DGMs) have sparked significant copyright concerns. There have been various legal debates on how to effectively safeguard copyrights in DGMs. This work delves into this issue by providing a comprehensive overview of copyright protection from a technical perspective. We examine from two distinct viewpoints: the copyrights pertaining to the source data held by the data owners and those of the generative models maintained by the model builders. For data copyright, we delve into methods data owners can protect their content and DGMs can be utilized without infringing upon these rights. For model copyright, our discussion extends to strategies for preventing model theft and identifying outputs generated by specific models. Finally, we highlight the limitations of existing techniques and identify areas that remain unexplored. Furthermore, we discuss prospective directions for the future of copyright protection, underscoring its importance for the sustainable and ethical development of Generative AI.",
            "score": 0.5207466530088043,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.92041015625
        },
        {
            "corpus_id": "273654195",
            "title": "Copyright-Aware Incentive Scheme for Generative Art Models Using Hierarchical Reinforcement Learning",
            "text": "Generative models are playing an increasingly important role in the web ecosystem by revolutionizing the way digital content is created and consumed [38] [6]. These models enable the automatic generation of high-quality images, audio, and video, leading to more dynamic, personalized, and engaging web experiences [33]. With the rise of generative models, generative art, a prominent form of artificial intelligence-generated content (AIGC), has emerged as a cutting-edge research topic [37] [15]. With the advancements in diffusion models, such as DALL\u2022E [26] and Stable Diffusion [24], generative art has demonstrated remarkable progress, particularly in image generation and text-to-image tasks. \n\nThe widespread deployment of such generative art models has brought about significant challenges concerning the risk for copyright infringement. The image generative models require large amount of training data, and it's impractical to only use noncopyrighted content to train the models. The model holder wants to leverage the high quality copyrighted data to learn the style and content and output high quality generated data. However, the generative models may generate some images that are very similar to the copyrighted training data, which may lead to copyright infringement [28]. \n\nVarious methods have been developed to protect copyright in source data, with the most common strategy involving the introduction of perturbations during the training process to effectively safeguard the dataset's copyright: (1) Unrecognizable examples [8,41] hinder models from learning essential features of protected images, either at the inference or training stages. However, this method is highly dependent on the specific images and models involved, and it lacks universal guarantees. (2) Watermarking [5,7] embeds subtle, imperceptible patterns into images to detect copyright violations, though further research is needed to improve its reliability. (3) Machine unlearning [1,9,13,20] removes the influence of copyrighted data, supporting the right to be forgotten. (4) Dataset deduplication [32] helps reduce memorization of training samples, minimizing the risk of copying protected content.",
            "score": 0.5171769572517791,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 158
                },
                {
                    "start": 159,
                    "end": 319
                },
                {
                    "start": 320,
                    "end": 497
                },
                {
                    "start": 498,
                    "end": 698
                },
                {
                    "start": 701,
                    "end": 845
                },
                {
                    "start": 846,
                    "end": 989
                },
                {
                    "start": 990,
                    "end": 1129
                },
                {
                    "start": 1130,
                    "end": 1288
                },
                {
                    "start": 1291,
                    "end": 1662
                },
                {
                    "start": 1663,
                    "end": 1782
                },
                {
                    "start": 1783,
                    "end": 1949
                },
                {
                    "start": 1950,
                    "end": 2065
                },
                {
                    "start": 2066,
                    "end": 2193
                }
            ],
            "ref_mentions": [
                {
                    "start": 154,
                    "end": 157,
                    "matchedPaperCorpusId": "264172541"
                },
                {
                    "start": 492,
                    "end": 496,
                    "matchedPaperCorpusId": "261279983"
                },
                {
                    "start": 1800,
                    "end": 1803,
                    "matchedPaperCorpusId": "258297834"
                },
                {
                    "start": 1803,
                    "end": 1805,
                    "matchedPaperCorpusId": "264436550"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.99267578125
        },
        {
            "corpus_id": "259501495",
            "title": "Measuring the Success of Diffusion Models at Imitating Human Artists",
            "text": "Accepted to the 1 st Workshop on Generative AI and Law, colocated with the International Conference on Machine Learning, Honolulu, Hawaii, USA. 2023. Copyright 2023 by the author(s). For each artist, we generate an imitation image from Stable Diffusion with the prompt \"Artwork from < artist name >.\" \n\nNext, we encode the image with a CLIP image encoder (Radford et al., 2021). We also encode labels corresponding to n total artists plus one or more 'default' labels with a CLIP text encoder. Finally, we classify the image among all labels using a geometric similarity measure between the encodings. If the label reliably corresponds to the correct artist, we consider the model to have the capability to imitate that artist. \n\nalso show that a sample of the artist's work can be matched to these imitation images with a high degree of statistical reliability. (Rombach et al., 2022) and Midjourney (Midjourney, 2022), are capable of generating images from arbitrary, user-specified prompts. Their success has largely been due to training on large amounts of text/image data, often including copyrighted works (Schuhmann et al., 2021). Modern image-generation diffusion models are trained using CLIP-style encoders. When given an encoding of a caption, a diffusion model is trained to generate an image corresponding to the caption (Ramesh et al., 2022). Accordingly, a diffusion model that generates images from these embeddings is trained to be the inverse of a CLIP image encoder. \n\nLegal Motivation: In the United States, Newton v. Diamond, 388 F.3d 1189, 1195(9th Cir. 2004) established that copyright infringement \"is measured by considering the qualitative and quantitative significance of the copied portion in relation to the plaintiff's work as a whole\". However, the subjective nature of these determinations makes practical enforcement complicated. (Balganesh et al., 2014;Kaminski & Rub, 2017;Balagopalan et al., 2023).",
            "score": 0.5168180592398126,
            "section_title": "body",
            "char_start_offset": 1,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 143
                },
                {
                    "start": 144,
                    "end": 149
                },
                {
                    "start": 150,
                    "end": 182
                },
                {
                    "start": 183,
                    "end": 300
                },
                {
                    "start": 303,
                    "end": 378
                },
                {
                    "start": 379,
                    "end": 493
                },
                {
                    "start": 494,
                    "end": 601
                },
                {
                    "start": 602,
                    "end": 727
                },
                {
                    "start": 730,
                    "end": 862
                },
                {
                    "start": 863,
                    "end": 993
                },
                {
                    "start": 994,
                    "end": 1137
                },
                {
                    "start": 1138,
                    "end": 1217
                },
                {
                    "start": 1218,
                    "end": 1356
                },
                {
                    "start": 1357,
                    "end": 1485
                },
                {
                    "start": 1488,
                    "end": 1766
                },
                {
                    "start": 1767,
                    "end": 1862
                },
                {
                    "start": 1863,
                    "end": 1934
                }
            ],
            "ref_mentions": [
                {
                    "start": 355,
                    "end": 377,
                    "matchedPaperCorpusId": "231591445"
                },
                {
                    "start": 863,
                    "end": 885,
                    "matchedPaperCorpusId": "245335280"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.966796875
        },
        {
            "corpus_id": "274860048",
            "title": "IntroStyle: Training-Free Introspective Style Attribution using Diffusion Features",
            "text": "Diffusion models have significantly advanced image synthesis by applying iterative denoising processes guided by input prompts. Models like Stable Diffusion [30], DALL-E 3 [1], and Imagen [32] have emerged as a powerful paradigm for text-to-image synthesis, demonstrating remarkable capabilities in generating high-quality images from textual descriptions. The success of diffusion-based approaches has led to their application in various domains, including layout-to-image generation, text-guided image generation, and even video synthesis. However, these mod-Figure 1. Introspective Style Attribution (IntroStyle). Existing SOTA methods, e.g., CSD, are biased to content semantics and fail to retrieve images of similar styles, underscoring the need for a better style similarity metric. The top two rows show examples on the WikiArt [33] dataset and our proposed synthetic Style Hacks (SHacks) dataset, respectively, where a reference image, top-3 retrieval results of our method, top-2 retrieval results of CSD are shown from left to right. More importantly, our proposed method can be used as a metric for style measurement, as shown in the third row, with a lower score indicating images further away in style from the reference in the first column. Green colors indicate correct and red for incorrect retrievals. els' widespread adoption and impressive performance have also raised concerns about problems like copyright infringement. For effective performance, the models require largescale pre-training on diverse datasets [34]. Since these datasets are collected automatically and dominantly from the web, it is difficult to control image provenience and avoid the collection of copyrighted imagery. This problem is compounded by the tendency of diffusion models to replicate elements from their training data [2]. The legal implications of training models on copyrighted images have become a subject of recent debate and litigation, with artists arguing that the unauthorized use of their works for AI training [7,10,12] constitutes copyright infringement [17,29]. \n\nCurrent mitigation approaches include \"unlearning\" techniques to remove specific styles from AI models [44], though these require costly model retraining and may not fully address indirect style replication through alternative prompts [24].",
            "score": 0.5167937685169253,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 127
                },
                {
                    "start": 128,
                    "end": 356
                },
                {
                    "start": 357,
                    "end": 541
                },
                {
                    "start": 542,
                    "end": 616
                },
                {
                    "start": 617,
                    "end": 789
                },
                {
                    "start": 790,
                    "end": 1044
                },
                {
                    "start": 1045,
                    "end": 1255
                },
                {
                    "start": 1256,
                    "end": 1319
                },
                {
                    "start": 1320,
                    "end": 1441
                },
                {
                    "start": 1442,
                    "end": 1537
                },
                {
                    "start": 1538,
                    "end": 1709
                },
                {
                    "start": 1710,
                    "end": 1824
                },
                {
                    "start": 1825,
                    "end": 2075
                },
                {
                    "start": 2078,
                    "end": 2318
                }
            ],
            "ref_mentions": [
                {
                    "start": 157,
                    "end": 161,
                    "matchedPaperCorpusId": "245335280"
                },
                {
                    "start": 172,
                    "end": 175,
                    "matchedPaperCorpusId": "264403242"
                },
                {
                    "start": 188,
                    "end": 192,
                    "matchedPaperCorpusId": "248986576"
                },
                {
                    "start": 1532,
                    "end": 1536,
                    "matchedPaperCorpusId": "252917726"
                },
                {
                    "start": 1820,
                    "end": 1823,
                    "matchedPaperCorpusId": "256389993"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.9697265625
        },
        {
            "corpus_id": "258999627",
            "title": "AI Imagery and the Overton Window",
            "text": "Commercial text-to-image AI generators such as Midjourney and Stable Diffusion use a large dataset called LAION-5B. Two of the ethical issues this dataset poses is the scraping of original content without consent, and the use of data such as personal photos, artwork, game assets, and even medical records [41]. \n\nArtistic output is what is subject to copyright by law; the process of creation itself is not, neither is the style of expression [42]. As such, this leaves AI image generators in a legal gray area, as legal professionals are divided over whether or not the result of an image generator can be owned by a particular person or entity. For example, copyright law in the United States indicates that only a work created by a human being can be copyrighted [43]. \n\nLawsuits have been filed against the AI models' infrastructure owners in the United States [44][45] by both individual artists and large corporations [46] [47]. It is likely that copyright laws will be revisited and amended in the upcoming years, given the rapid advancement and complexity of the matter. As of this publication's release, the input artwork used to train the AI models is the legal property of the artist that created it, but the AI image has no legal owner but can still be used commercially with no compensation to the artists whose work was used to produce such imagery. \n\nThe lawsuit is expected to be a complicated one filled with technicalities that may serve as legal escape clauses for the AI infrastructure owners. One such technicality is that the dataset used to train the commercial version of a model like Stable Diffusion [48] contains not the original copies of the scraped training images, but rather the latent copies of those images that were generated during the training process [49]. Therefore, the legal loophole expected to be exploited is the claim that these latent images are derivative works, not originals. \n\nIt is unsurprising that, should the AI infrastructure owners indeed compensate every artist whose work was taken into training the model, the model would not generate income, and would operate at a loss. The current models and legal landscape do not protect or improve artist creators' quality of life, but enable corporations to achieve monopoly and encourage employee layoffs -whilst retaining the employees' talent and skill without consent.",
            "score": 0.5152441020817581,
            "section_title": "A. Legal Landscape",
            "char_start_offset": 20889,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 115
                },
                {
                    "start": 116,
                    "end": 311
                },
                {
                    "start": 314,
                    "end": 449
                },
                {
                    "start": 450,
                    "end": 647
                },
                {
                    "start": 648,
                    "end": 772
                },
                {
                    "start": 775,
                    "end": 935
                },
                {
                    "start": 936,
                    "end": 1079
                },
                {
                    "start": 1080,
                    "end": 1364
                },
                {
                    "start": 1367,
                    "end": 1514
                },
                {
                    "start": 1515,
                    "end": 1795
                },
                {
                    "start": 1796,
                    "end": 1925
                },
                {
                    "start": 1928,
                    "end": 2131
                },
                {
                    "start": 2132,
                    "end": 2372
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.9404296875
        },
        {
            "corpus_id": "273023255",
            "title": "Probabilistic Analysis of Copyright Disputes and Generative AI Safety",
            "text": "In cases where generative AI is used to create content, it is typically provided as a service by the AI developer and utilized by a user, and in such situations, both the user and the AI developer may potentially be considered direct or indirect infringers, depending on the specific circumstances [28]. Copyright infringement may occur even without the user or developer intending to replicate the copyrighted material or to create something similar [28]. \n\nDue to the uninterpretability of generative models, it can be challenging to provide direct evidence of factual copying for AI-generated content. In most cases, access and similarity will likely serve as the primary evidence of infringement, much like in traditional copyright disputes [28]. While the creative processes of generative models differ markedly from human creativity, it appears that similar evidentiary principles are likely to apply even when generative AI is involved in the creative processes. At a minimum, it is generally recognized that some form of access to the original work, which may be demonstrated by its inclusion in the training data, is necessary to establish copying, even for generative AI outputs [28,29,36]. Moreover, studies have shown that exposure to certain types of content during training can lead to a high likelihood of these models reproducing or closely imitating that content [37][38][39], suggesting that inclusion of specific data correlates with a greater probability of producing similar outputs through copying. This echoes the evidentiary framework applicable to traditional copyright cases, where greater similarity between works strengthens the inference of access and copying. \n\nA distinctive feature of generative models, setting them apart from human artists or traditional creative tools, is their capacity to be trained on immense and diverse datasets, which are often created by scraping content from the Internet. Furthermore, direct inclusion of copyrighted material in the training data is not even necessary for latent diffusion models to produce images similar to copyrighted works, as these models can reproduce content from other images that retain similar latent information [40], which exacerbates the problem. These characteristics suggest a high likelihood that generative models have what can be construed as \"access\" to copyrighted works.",
            "score": 0.514113547347403,
            "section_title": "Risks of Copyright Infringement in Generative AI",
            "char_start_offset": 27051,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 303
                },
                {
                    "start": 304,
                    "end": 456
                },
                {
                    "start": 459,
                    "end": 604
                },
                {
                    "start": 605,
                    "end": 750
                },
                {
                    "start": 751,
                    "end": 969
                },
                {
                    "start": 970,
                    "end": 1200
                },
                {
                    "start": 1201,
                    "end": 1520
                },
                {
                    "start": 1521,
                    "end": 1689
                },
                {
                    "start": 1692,
                    "end": 1932
                },
                {
                    "start": 1933,
                    "end": 2237
                },
                {
                    "start": 2238,
                    "end": 2369
                }
            ],
            "ref_mentions": [
                {
                    "start": 1380,
                    "end": 1384,
                    "matchedPaperCorpusId": "229156229"
                },
                {
                    "start": 1384,
                    "end": 1388,
                    "matchedPaperCorpusId": "256389993"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.98876953125
        },
        {
            "corpus_id": "257050406",
            "title": "On Provable Copyright Protection for Generative Models",
            "text": "Generative models raise many legal and ethical issues. This paper focuses on copyright infringement by the outputs of generative models, which is only one of these issues. The concepts and tools we provide do not address issues related to other forms of intellectual property, including privacy, trademarks, patents, or fair use. Moreover, our work does not (and cannot) guarantee the absence of copyright infringement in all settings. However, we do hope it provides helpful tools and concepts that can be used by model creators and users, lawyers, and courts to reduce the task of determining if some types of infringements have occurred to well-defined, quantitative questions.",
            "score": 0.5140299107888977,
            "section_title": "Introduction",
            "char_start_offset": 6449,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 54
                },
                {
                    "start": 55,
                    "end": 171
                },
                {
                    "start": 172,
                    "end": 329
                },
                {
                    "start": 330,
                    "end": 435
                },
                {
                    "start": 436,
                    "end": 680
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.916015625
        },
        {
            "corpus_id": "267400526",
            "title": "AIGC (Artificial Intelligence Generated Content) infringes the copyright of human artists",
            "text": "After exploring how artificial intelligence (AI)-generated platforms work, learn what processes may involve human artist infringement in the course of their work. So let's talk about where these workflows infringe on human artists' copyrighted works -Artificial intelligence (AI)-generated content raises concerns about copyright infringement of human artist works in several ways. \n\nThe researchers summarized three aspects of human artist infringement: model training using only copyrighted works without permission. Take Stable Diffusion, which has been involved in many copyright disputes. It uses the image database containing hundreds of millions-LAION-5B (Kaixin Zhu, & Yiqun Zhang. ( 2023)) as the source of training data, and it does not need the consent of the copyright owner [5]. \n\nSecond, there is a lot of plagiarism and adaptation of copyrighted works in AI-generated images. In a case in which an artist sued an AI-generated platform, the painter Erin Hanson provided strong evidence that the AI-generated images plagiarized his work. And there are many such lawsuits now, not a few. Third, the images generated by the AI generation platform, such as Stable Diffusion, if works infringe copyright works, do not have any watermarks or source marks, making it even more difficult for copyright owners to accept. Unlike artificial intelligence, most human artists will mark the original author or source when learning to learn from other people's works so as not to infringe the copyright of others. Artificial intelligence often regards works as different data, lacking awe and moral measurement for works. \n\nAll in all, during the generation process of the artificial intelligence generation platform (take Stable Diffusion as an example), it may infringe on the works of human artists from two aspects of model training and image generation, and the generated results are different in three dimensions. Infringements are caused to a certain extent, including unauthorized use, excessive plagiarism and adaptation, and lack of attribution to the original author of the copyrighted work. These are the key factors that constitute the infringement of human artists.",
            "score": 0.5135825111625341,
            "section_title": "From what aspects does Stable Diffusion infringe the copyright of human artists",
            "char_start_offset": 13286,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 162
                },
                {
                    "start": 163,
                    "end": 381
                },
                {
                    "start": 384,
                    "end": 518
                },
                {
                    "start": 519,
                    "end": 593
                },
                {
                    "start": 594,
                    "end": 689
                },
                {
                    "start": 690,
                    "end": 791
                },
                {
                    "start": 794,
                    "end": 890
                },
                {
                    "start": 891,
                    "end": 1050
                },
                {
                    "start": 1051,
                    "end": 1099
                },
                {
                    "start": 1100,
                    "end": 1325
                },
                {
                    "start": 1326,
                    "end": 1512
                },
                {
                    "start": 1513,
                    "end": 1620
                },
                {
                    "start": 1623,
                    "end": 1918
                },
                {
                    "start": 1919,
                    "end": 2101
                },
                {
                    "start": 2102,
                    "end": 2178
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.984375
        },
        {
            "corpus_id": "270357944",
            "title": "Evaluating and Mitigating IP Infringement in Visual Generative AI",
            "text": "Our experiments demonstrate that the IP infringement issues are widely existing in both open-source and commercial closed source models.\n\nGiven the severe IP infringement problems with visual generative AI models, it is essential to develop an effective defense method that can mitigate these issues with minimal impact on the models' generation capabilities.To address this, we develop a revised generation paradigm TRIM (inTellectual pRoperty Infringement Mitigating) that detects the generated contents that potentially has the IP infringement issues and suppresses the IP infringement by exploiting the guidance technique for the diffusion process.Experiments on our IP infringement benchmark and state-of-the-art visual generative AI models demonstrate that our defensive generation paradigm is highly effective at mitigating IP infringement problems involving protected characters, while only having a small influence on the text-image alignment quality of the generated content.\n\nOur contributions are summarized as follows: \u2460 We constructed a benchmark for studying IP infringement issues with visual generative AI models.This involved designing a method to create prompts that can trigger IP infringement in a black-box setting, even without directly using the names of protected characters.\u2461 We developed an effective defense method to mitigate the IP infringement problem.\u2462 Our evaluation on the state-of-the-art visual generative AI models demonstrate that the IP infringement problems on the representative characters are severe.\u2463 Experiments demonstrate our proposed mitigation method is highly effective at mitigating these IP issues, while only having a small influence on the overall quality of the generated content.",
            "score": 0.5133633888357836,
            "section_title": "Introduction",
            "char_start_offset": 4384,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 136
                },
                {
                    "start": 138,
                    "end": 359
                },
                {
                    "start": 359,
                    "end": 652
                },
                {
                    "start": 652,
                    "end": 985
                },
                {
                    "start": 987,
                    "end": 1130
                },
                {
                    "start": 1130,
                    "end": 1300
                },
                {
                    "start": 1300,
                    "end": 1383
                },
                {
                    "start": 1383,
                    "end": 1542
                },
                {
                    "start": 1542,
                    "end": 1734
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.86328125
        },
        {
            "corpus_id": "275471553",
            "title": "Has an AI model been trained on your images?",
            "text": "Understanding why models are biased to produce content similar to their training data may provide insights into reducing the likelihood of direct copyright infringement in the form of reproducing training data, and may provide insights into how a model can be made to forget training exemplars. \n\nAn attractive aspect of our membership inference for generative image models is that it does not require direct access to the details of the model architecture or trained weights, is computationally efficient, and generalizes to multiple different AI models. A drawback of our approach is that it only applies to models that allow for an image-to-image synthesis with a controllable strength parameter, as compared to text-to-image. Depending on the underlying mechanism by which models produce images similar to their in-training data, our method may be adaptable to text-to-image generation. \n\nMany of today's tech leaders have admitted that their generative-AI models would not exist without their training on billions of pieces of content scraped from all corners of the internet [16]. These same leaders have also called for the loosening of fairuse and copyright rules. While it is for the courts to decide on these matters of law [22], we contend that content creators have legitimate concerns for whether and how their content is used to train generative-AI models, some of which are designed to offer services directly competing with these very content creators. \n\nA critical component of adjudicating these issues will be determining if a deployed model was trained on a specific piece of content. Equally important is determining how creators can and should be compensated when their content is used for training, and how models can be made to forget its training on a specific piece of content should this be the wish of the content's creator. \n\nWe have focused only on the first of these questions, but all of these issues are important to resolve as generative AI continues its impressive and impactful trajectory.",
            "score": 0.5112030122335751,
            "section_title": "Discussion",
            "char_start_offset": 19446,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 294
                },
                {
                    "start": 297,
                    "end": 555
                },
                {
                    "start": 556,
                    "end": 729
                },
                {
                    "start": 730,
                    "end": 890
                },
                {
                    "start": 893,
                    "end": 1086
                },
                {
                    "start": 1087,
                    "end": 1172
                },
                {
                    "start": 1173,
                    "end": 1468
                },
                {
                    "start": 1471,
                    "end": 1604
                },
                {
                    "start": 1605,
                    "end": 1852
                },
                {
                    "start": 1855,
                    "end": 2025
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.970703125
        },
        {
            "corpus_id": "269982010",
            "title": "How to Trace Latent Generative Model Generated Images without Artificial Watermark?",
            "text": "Recently, latent generative models (Rombach et al., 2022) have attracted significant attention and showcased outstanding capabilities in generating a wide range of high-resolution images with surprising quality.Many state-of-the-art image generation models belong to latent generative models, such as DALL-E 3 (Betker et al., 2023) from OpenAI, Parti (Yu et al., 2022a) from Google, and Stable Diffusion (Rombach et al., 2022) from Stability AI.These models allow for achieving a near-optimal point between reducing computing complexity and preserving visual details, greatly boosting the efficiency in both training and generation phase.Among them, Stable Diffusion is the most widely-used, which has already gained more than 10 million users1 .\n\nAs latent generative models become more prevalent, the issues surrounding their potential for misuse are becoming increasingly important (Schramowski et al., 2023;Wang et al., 2023c;Pan et al., 2024;Liu et al., 2024;Wen et al., 2023b;Chen et al., 2023).For example, malicious users may use the latent generative models to generate and distribute images containing inappropriate concepts such as \"sexual\", \"drug use\", \"weapons\", and \"child abuse\" (Schramowski et al., 2023).AI-powered plagiarism (Francke & Bennett, 2019) and IP (intellectual property) infringement problem surrounding the latent generative models are also important issues.For instance, users may synthesize high-quality images using one company's or open-sourced latent generative models and then dishonestly present them as their own original artwork (e.g., photographs and paintings) to gain recognition and reputation, which is harmful to society and may cause a series of IP problems.Consequently, it's crucial to be able to trace the source of images generated by latent generative models, i.e., determining if a certain image was produced by a specific model.\n\nThere are several existing methods for tracing the source of the images.",
            "score": 0.5086321292726361,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 211
                },
                {
                    "start": 211,
                    "end": 445
                },
                {
                    "start": 445,
                    "end": 638
                },
                {
                    "start": 638,
                    "end": 746
                },
                {
                    "start": 748,
                    "end": 1001
                },
                {
                    "start": 1001,
                    "end": 1221
                },
                {
                    "start": 1221,
                    "end": 1388
                },
                {
                    "start": 1388,
                    "end": 1704
                },
                {
                    "start": 1704,
                    "end": 1881
                },
                {
                    "start": 1883,
                    "end": 1955
                }
            ],
            "ref_mentions": [
                {
                    "start": 35,
                    "end": 57,
                    "matchedPaperCorpusId": "245335280"
                },
                {
                    "start": 310,
                    "end": 331,
                    "matchedPaperCorpusId": "264403242"
                },
                {
                    "start": 404,
                    "end": 426,
                    "matchedPaperCorpusId": "245335280"
                },
                {
                    "start": 885,
                    "end": 911,
                    "matchedPaperCorpusId": "253420366"
                },
                {
                    "start": 982,
                    "end": 1000,
                    "matchedPaperCorpusId": "253254809"
                },
                {
                    "start": 1194,
                    "end": 1220,
                    "matchedPaperCorpusId": "253420366"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.91552734375
        },
        {
            "corpus_id": "260416806",
            "title": "Talkin' 'Bout AI Generation: Copyright and the Generative-AI Supply Chain (The Short Version)",
            "text": "Similarly, assuming and to the extent that a model is a copy of works in the training dataset, training a model is also direct infringement. Again, the model creators have chosen which datasets to include; they act at their own risk that those datasets include copyrighted material. The same applies to fine-tuning a model: the retrainer takes the risk that they are creating an infringing derivative work. A lack of knowledge as to what works are embedded in the model is no defense. Similarly, the choice to distribute a trained model is sufficiently volitional to count. \n\nThe analysis of generation is more complex. Where the same person supplies both the model and the prompt, they are a direct infringer. But when a model owner provides generation as a service to a user who supplies the prompt, the question is which of them is the direct infringer. \n\nBy analogy to the copy-shop and UGC service cases, it would seem that the user is the direct infringer and the service provider is the direct infringer. (Imagine, for example, a prompt for \"Elsa and Anna from Frozen. \") On this analysis, the service provider is probably not a vicarious infringer, because while they have the right and ability to control their model's outputs (e.g., by shutting it down or blocking images of Elsa), they do not have enough of a direct financial interest in specifically infringing uses of the service. 35 They are also typically not an inducing infringer, as they do not intend that the service be used to create infringing cartoons. 36 As for contributory infringement, the model is a material contribution, but they have only generalized knowledge (some users will make infringing art), not specific knowledge (some users will make art that infringes on Frozen). Thus, under Napster, they are not liable. A generation service becomes liable, however, when it has specific notice of an infringing work. Once Disney sends a notice to the service over the infringing Elsa output, the service now has the kind of knowledge that triggered liability in Napster and must therefore take steps to prevent similar future generations. 37",
            "score": 0.5081584129291552,
            "section_title": "D. Direct and Indirect Infringement",
            "char_start_offset": 32486,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 140
                },
                {
                    "start": 141,
                    "end": 282
                },
                {
                    "start": 283,
                    "end": 406
                },
                {
                    "start": 407,
                    "end": 484
                },
                {
                    "start": 485,
                    "end": 573
                },
                {
                    "start": 576,
                    "end": 619
                },
                {
                    "start": 620,
                    "end": 710
                },
                {
                    "start": 711,
                    "end": 856
                },
                {
                    "start": 859,
                    "end": 1011
                },
                {
                    "start": 1012,
                    "end": 1075
                },
                {
                    "start": 1076,
                    "end": 1397
                },
                {
                    "start": 1398,
                    "end": 1529
                },
                {
                    "start": 1530,
                    "end": 1757
                },
                {
                    "start": 1758,
                    "end": 1799
                },
                {
                    "start": 1800,
                    "end": 1896
                },
                {
                    "start": 1897,
                    "end": 2121
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.91357421875
        },
        {
            "corpus_id": "268513090",
            "title": "CGI-DM: Digital Copyright Authentication for Diffusion Models via Contrasting Gradient Inversion",
            "text": "While no methods explicitly claim to be used for authenticating copyright on few-shot generation, both existing image generation [1,31] and inpainting [33] pipelines exhibit the potential to do so.Particularly, we compare our method with three types of pipelines that could be employed for identifying infringements: Text-to-Image Generation.Using the official Text-to-Image pipeline in diffusers 4 , we generate 100 \u00d7 Num images for each fine-tuned model using the prompt employed during training.Here, 'Num' represents the total number of images in the membership and holdout datasets.This process is applied individually to each image in both datasets.Image-to-Image Generation.Leveraging the official Image-to-Image pipeline in diffusers 5 , we use the training prompt to generate 100 \u00d7 Num images for each fine-tuned model.The inference step is set to 50 and the img2img strength is set to 0.7 for each image in the membership or holdout dataset.Inpainting.Employing the state-of-the-art inpainting pipeline DDNM [33], we generate 100 \u00d7 Num images by masking the right half of each image.We use the prompt applied during training for inpainting, and the inference step is fixed at 50.\n\nWe set the number of generated images per input image to 100.This ensures that the time cost for both the baseline method and our method remains similar, facilitating a fair comparison (see Appendix H for more details).We define the similarity between the output images and a given image as the highest similarity achieved among all the generated images corresponding to one target image.\n\nThe comparison between CGI-DM and other pipelines is presented in Tab. 1.It is evident that CGI-DM outperforms others significantly in various few-shot generation scenarios 4 https://huggingface.co/docs/diffusers/api/pipelines/stable diffusion/text2img  across different datasets.",
            "score": 0.507527787474508,
            "section_title": "Comparison with Existing Methods",
            "char_start_offset": 16755,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 197
                },
                {
                    "start": 197,
                    "end": 342
                },
                {
                    "start": 342,
                    "end": 498
                },
                {
                    "start": 498,
                    "end": 587
                },
                {
                    "start": 587,
                    "end": 655
                },
                {
                    "start": 655,
                    "end": 681
                },
                {
                    "start": 681,
                    "end": 828
                },
                {
                    "start": 828,
                    "end": 951
                },
                {
                    "start": 951,
                    "end": 962
                },
                {
                    "start": 962,
                    "end": 1093
                },
                {
                    "start": 1093,
                    "end": 1189
                },
                {
                    "start": 1191,
                    "end": 1252
                },
                {
                    "start": 1252,
                    "end": 1410
                },
                {
                    "start": 1410,
                    "end": 1579
                },
                {
                    "start": 1581,
                    "end": 1654
                },
                {
                    "start": 1654,
                    "end": 1861
                }
            ],
            "ref_mentions": [
                {
                    "start": 129,
                    "end": 132,
                    "matchedPaperCorpusId": "256389993"
                },
                {
                    "start": 151,
                    "end": 155,
                    "matchedPaperCorpusId": "254125609"
                },
                {
                    "start": 1018,
                    "end": 1022,
                    "matchedPaperCorpusId": "254125609"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.4990234375
        },
        {
            "corpus_id": "277955485",
            "title": "TWIG: Two-Step Image Generation using Segmentation Masks in Diffusion Models",
            "text": "While some work [15] have pointed out extensive source copying for generative models, for practical purposes, something that does not exactly replicate the source should be enough [17]. In particular, we observe that if an image has the same semantic segmentation mask as the original source data, they have a higher likelihood of being identified as copyright infringement. Conversely, a simple distortion like reflection or resizing is often enough to circumvent copyright claims [18]. \n\nFrom this observation and taking cue from conditional diffusion model [16], we propose a twostep image generation model. The first step is to create an image segmentation mask from a text prompt using a diffusion model. This model would be trained on image segmentation masks instead of complete images. There would be a greater sampling rate, so that the model can deviate from the training segmentation masks. Using the generated segmentation mask as a conditional control, we would next create the full image, and here we would use a diffusion model as used by [16] with a smaller sampling rate. As we would have a lesser likelihood of generating a similar segmentation mask, we would have a lesser likelihood of generating a copy as well. We suspect this is closer to how an animal or human brain conceptualizes an image in the mind, first by identifying the parts and then filling in the details. \n\nTo the best of our knowledge, this is the first work to address copyright concerns in diffusion model-based image generation by utilizing a two-step process involving image segmentation masks.",
            "score": 0.5063074579109614,
            "section_title": "Introduction",
            "char_start_offset": 3937,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 185
                },
                {
                    "start": 186,
                    "end": 374
                },
                {
                    "start": 375,
                    "end": 487
                },
                {
                    "start": 490,
                    "end": 610
                },
                {
                    "start": 611,
                    "end": 709
                },
                {
                    "start": 710,
                    "end": 793
                },
                {
                    "start": 794,
                    "end": 901
                },
                {
                    "start": 902,
                    "end": 1088
                },
                {
                    "start": 1089,
                    "end": 1232
                },
                {
                    "start": 1233,
                    "end": 1391
                },
                {
                    "start": 1394,
                    "end": 1586
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.9775390625
        },
        {
            "corpus_id": "267400526",
            "title": "AIGC (Artificial Intelligence Generated Content) infringes the copyright of human artists",
            "text": "With the rapid development of artificial intelligence technology, content generated by artificial intelligence has been rapidly applied to people's lives. At the same time, it is also accompanied by many infringement lawsuits, whether AIGC has really caused different degrees of infringement to human artists. Through the analysis of the existing literature on copyright issues and the walkthrough of Stable Diffusion, an AI-generated image platform, this article digs into the main factors that the AI-generated platform causes infringements on human artists. Provide references for using AI by enterprises and related media, and let more scholars pay attention to this issue. The study found that in the workflow of the AI generation platform, taking Stable Diffusion as an example, the two processes of model training and image generation may cause copyright infringement to a certain extent. Based on this, the AI generation platform has unauthorized use of copyright works, excessive plagiarism and adaptation of copyright works, and the generated images are not marked with watermarks or sources, which damages the copyright owner's rights.",
            "score": 0.5053045631164993,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.96923828125
        },
        {
            "corpus_id": "268634519",
            "title": "Beyond Discrimination: Generative AI Applications and Ethical Challenges in Forensic Psychiatry",
            "text": "Although generative AI models gained popularity for their ability to generate novel content, it is crucial to note that the examples used by these models are typically derived from existing human-made works, raising issues of copyright infringement and unauthorised imitation. Large language models (LLMs) are trained on an extensive corpus of data, some of which may have been acquired without proper consent, as the models usually scrape data from the internet, disregarding copyright licenses, plagiarising content, and repurposing proprietary materials without permission. \n\nAs a result, it becomes challenging to trace the lineage of the content generated by those models, and due credit is frequently not given to the original creators, potentially exposing users to copyright infringement issues (107,108) and resulting in legal actions against companies, accused of violating intellectual property rights (109).",
            "score": 0.5021038074185334,
            "section_title": "Intellectual property rights & copyright infringements",
            "char_start_offset": 38795,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 276
                },
                {
                    "start": 277,
                    "end": 576
                },
                {
                    "start": 579,
                    "end": 919
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.79541015625
        },
        {
            "corpus_id": "258865475",
            "title": "Can Copyright be Reduced to Privacy?",
            "text": "Recent advancements in machine learning have sparked a wave of new possibilities and applications that could potentially transform various aspects of our daily lives and revolutionize numerous professions through automation. However, training such algorithms heavily relies on extensive content which may include copyrighted materials. Under U.S copyright law, copyright protection subsists in original content of authorship fixed in any tangible medium of expression [55], excluding any \"idea, procedure, process, system, method of operation, concept, principle, or discovery, regardless of the form in which it is described, explained, illustrated, or embodied in such work.\" [55, \u00a7 102(b)]. The unauthorized copying of copyrighted works may amount to copyright infringement [55, \u00a7 106] unless permitted by exceptions and limitations provided by law ([55, \u00a7107-122], and [50]). Consequently, identifying and, determining when and how content can be used within this framework without infringing upon individuals' legal rights has become a pressing challenge. Foundation Models and generative AI (GenAI), trained on gigantic datasets, exacerbate this challenge. One area where this issue arises prominently is in the operation of generative models, which take human-produced content-much of it copyrighted as input and are expected to generate \"-similar\" content. For instance, consider a machine trained on images and then generates new images that resemble the ones it was trained on. In this context, the fundamental question arises: \n\nWhen does the content generated by a machine (output content) infringe copyright in the training set (input content)? \n\nThis question is not purely theoretical, as various aspects of this problem have become subjects of legal disputes in recent years. In 2022, a class action was filed against Microsoft, GitHub, and OpenAI, claiming that their code-generating systems, Codex and Copilot, infringed copyright in the licensed code that the system was allegedly trained on [13]. Similarly, in another class action, against Stable Diffusion, Midjourney, and DeviantArt, plaintiffs argue that by training their system on webscraped images, the defendant infringes millions of artists' rights [3].",
            "score": 0.501692274096049,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 224
                },
                {
                    "start": 225,
                    "end": 335
                },
                {
                    "start": 336,
                    "end": 677
                },
                {
                    "start": 678,
                    "end": 693
                },
                {
                    "start": 694,
                    "end": 879
                },
                {
                    "start": 880,
                    "end": 1060
                },
                {
                    "start": 1061,
                    "end": 1162
                },
                {
                    "start": 1163,
                    "end": 1364
                },
                {
                    "start": 1365,
                    "end": 1487
                },
                {
                    "start": 1488,
                    "end": 1537
                },
                {
                    "start": 1540,
                    "end": 1657
                },
                {
                    "start": 1660,
                    "end": 1791
                },
                {
                    "start": 1792,
                    "end": 2016
                },
                {
                    "start": 2017,
                    "end": 2232
                }
            ],
            "ref_mentions": [
                {
                    "start": 873,
                    "end": 877,
                    "matchedPaperCorpusId": "259844568"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.87744140625
        },
        {
            "corpus_id": "276575866",
            "title": "Can Large Vision-Language Models Detect Images Copyright Infringement from GenAI?",
            "text": "Generative AI models, renowned for their ability to synthesize high-quality content, have sparked growing concerns over the improper generation of copyright-protected material. While recent studies have proposed various approaches to address copyright issues, the capability of large vision-language models (LVLMs) to detect copyright infringements remains largely unexplored. In this work, we focus on evaluating the copyright detection abilities of state-of-the-art LVLMs using a various set of image samples. Recognizing the absence of a comprehensive dataset that includes both IP-infringement samples and ambiguous non-infringement negative samples, we construct a benchmark dataset comprising positive samples that violate the copyright protection of well-known IP figures, as well as negative samples that resemble these figures but do not raise copyright concerns. This dataset is created using advanced prompt engineering techniques. We then evaluate leading LVLMs using our benchmark dataset. Our experimental results reveal that LVLMs are prone to overfitting, leading to the misclassification of some negative samples as IP-infringement cases. In the final section, we analyze these failure cases and propose potential solutions to mitigate the overfitting problem.",
            "score": 0.5011801213614416,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.9619140625
        },
        {
            "corpus_id": "265352103",
            "title": "On Copyright Risks of Text-to-Image Diffusion Models",
            "text": "We discuss how we select appropriate target topics to serve as inputs for our data generation pipeline (more in Appendix B). Our objective is to identify topics associated with copyrighted images that contain highly specific features. As such, generating these features would not be considered as transformative works, thereby resulting in explicit copyright infringement [Milner Library, 2023]. We concentrate on three distinct domains: movies, video games, and logos (trademarks), as these domains are particularly well-aligned with potentially copyrighted subjects. Additionally, we prioritize recently released movies and video games, to ensure that our samples are of high quality. Images from recent years are also more likely to be protected by copyright as they have not yet entered the public domain [Office, 2023]. Nevertheless, it is important to emphasize that our approach is a form of academic research, and we thus refrain from asserting that the topics we have gathered in this study definitively qualify as copyrighted subjects (see Appendix C.1 for the complete list of topics). \n\nIt is notable to mention that we exclude topics related to artwork and individual artists from our designated target top-ics. Within the scope of this study, our primary emphasis lies in assessing partial copyright infringement. Specifically, this involves finding the presence of copyrighted content that is visually discernible within image segments. We find that while diffusion models can accurately replicate the style of artists [Casper et al., 2023], this may be a form of derivative work [Cornell, 2022] which is less straightforward to ascertain copyright infringement. Consequently, we refrain from delving into copyright matters pertaining to artistic style and creations by specific artists. The identification of style replication within artworks demands a more intricate approach, involving deeper consideration of how the style is employed, which is beyond the scope of this work.",
            "score": 0.5005535318422942,
            "section_title": "Collect Potentially Copyrighted Topics",
            "char_start_offset": 18799,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 124
                },
                {
                    "start": 125,
                    "end": 234
                },
                {
                    "start": 235,
                    "end": 395
                },
                {
                    "start": 396,
                    "end": 568
                },
                {
                    "start": 569,
                    "end": 686
                },
                {
                    "start": 687,
                    "end": 824
                },
                {
                    "start": 825,
                    "end": 1096
                },
                {
                    "start": 1099,
                    "end": 1224
                },
                {
                    "start": 1225,
                    "end": 1327
                },
                {
                    "start": 1328,
                    "end": 1451
                },
                {
                    "start": 1452,
                    "end": 1677
                },
                {
                    "start": 1678,
                    "end": 1802
                },
                {
                    "start": 1803,
                    "end": 1994
                }
            ],
            "ref_mentions": [
                {
                    "start": 1595,
                    "end": 1610,
                    "matchedPaperCorpusId": "234357997"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.93505859375
        },
        {
            "corpus_id": "257557331",
            "title": "Copyright Protection and Accountability of Generative AI: Attack, Watermarking and Attribution",
            "text": "The field of Generative AI (e.g., Generative Adversarial Networks -GANs) has made significant advancements in recent times. These advancements have led Generative AI to improve its capacity to produce highly realistic content such as artwork and images [20]. Hence, there is widespread utilization of Generative AI in both research and industry, leading to significant concerns regarding the protection of Intellectual Property Rights (IPR). There is a surge of GAN-based image generative model usage for creating content that can infringe upon existing copyrights, spoof security systems, and undermine the rights of content owners through the spread of misinformation, affecting personal reputations. Consequently, AI-Generated images are facing lawsuits over alleged copyright violations. One of the notable examples of a legal dispute involving AI-Generated images includes the lawsuit filed by Getty Images and a group of artists against AI art generators. 1 Additionally, producing state-of-the-art generative models requires substantial computational resources and large datasets. These issues have generated growing interest in research on protecting and verifying the ownership of the GAN models and training sets. \n\nPrevious copyright protection approaches for images and GANs include attacks (e.g., adversarial noise), watermarking, and attribution techniques. However, the literature focuses mainly on Deep Neural Networks (DNNs) and has a limited focus on developing copyright protection frameworks of GANs [1]. \n\nOur research aims to fill this gap by conducting a comprehensive analysis of the effectiveness of the existing copyright protection approaches for GANs. The proposed framework assists in identifying the key factors that determine the effectiveness of copyright protection techniques through the analysis of various GAN models. In order to accomplish this goal, we explore and answer the following three research questions. \n\n\u2022 RQ1: Can current adversarial attacks effectively prevent copyright violations of input images? \u2022 RQ2: Can the IPR of GANs and their training sets be protected and verified through watermarking? \u2022 RQ3: Can current attribution methods effectively support IPR protection and trace toxic generative AI model accountability through source attribution of output images?",
            "score": 0.4998124567365658,
            "section_title": "INTRODUCTION",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 123
                },
                {
                    "start": 124,
                    "end": 258
                },
                {
                    "start": 259,
                    "end": 441
                },
                {
                    "start": 442,
                    "end": 702
                },
                {
                    "start": 703,
                    "end": 791
                },
                {
                    "start": 792,
                    "end": 1087
                },
                {
                    "start": 1088,
                    "end": 1223
                },
                {
                    "start": 1226,
                    "end": 1371
                },
                {
                    "start": 1372,
                    "end": 1524
                },
                {
                    "start": 1527,
                    "end": 1679
                },
                {
                    "start": 1680,
                    "end": 1853
                },
                {
                    "start": 1854,
                    "end": 1949
                },
                {
                    "start": 1952,
                    "end": 2048
                },
                {
                    "start": 2049,
                    "end": 2147
                },
                {
                    "start": 2148,
                    "end": 2317
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.962890625
        },
        {
            "corpus_id": "269137391",
            "title": "Uncertain Boundaries: Multidisciplinary Approaches to Copyright Issues in Generative AI",
            "text": "In the rapidly evolving landscape of generative artificial intelligence (AI), the increasingly pertinent issue of copyright infringement arises as AI advances to generate content from scraped copyrighted data, prompting questions about ownership and protection that impact professionals across various careers. With this in mind, this survey provides an extensive examination of copyright infringement as it pertains to generative AI, aiming to stay abreast of the latest developments and open problems. Specifically, it will first outline methods of detecting copyright infringement in mediums such as text, image, and video. Next, it will delve an exploration of existing techniques aimed at safeguarding copyrighted works from generative models. Furthermore, this survey will discuss resources and tools for users to evaluate copyright violations. Finally, insights into ongoing regulations and proposals for AI will be explored and compared. Through combining these disciplines, the implications of AI-driven content and copyright are thoroughly illustrated and brought into question.",
            "score": 0.49958731971317566,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.912109375
        },
        {
            "corpus_id": "277955485",
            "title": "TWIG: Two-Step Image Generation using Segmentation Masks in Diffusion Models",
            "text": "Diffusion models have revolutionized image generation, surpassing predecessors like GANs and VAEs in both fidelity and performance. However, this progress has been shadowed by a critical challenge: source copying. This issue raises serious concerns regarding privacy and intellectual property rights, demanding innovative solutions that preserve content quality while enhancing copyright protection. Our research introduces a novel two-step image generation model designed to specifically address these concerns. This model operates by first generating segmentation masks from a given text prompt. These masks are then used to guide a Stable Diffusion model, effectively minimizing source copying while maintaining high fidelity in the final generated image. This approach also successfully circumvents the need for textual embedding, further streamlining the process and reducing potential avenues for copyright infringement. By decoupling the image generation process in this manner, we offer a promising pathway towards responsible and ethical AI image generation that respects both creative expression and copyright protection.",
            "score": 0.49953797926316135,
            "section_title": "Conclusion",
            "char_start_offset": 26105,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 131
                },
                {
                    "start": 132,
                    "end": 213
                },
                {
                    "start": 214,
                    "end": 399
                },
                {
                    "start": 400,
                    "end": 512
                },
                {
                    "start": 513,
                    "end": 597
                },
                {
                    "start": 598,
                    "end": 758
                },
                {
                    "start": 759,
                    "end": 926
                },
                {
                    "start": 927,
                    "end": 1131
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.99267578125
        },
        {
            "corpus_id": "256697414",
            "title": "Adversarial Example Does Good: Preventing Painting Imitation from Diffusion Models via Adversarial Examples",
            "text": "In this section, we would like to discuss some ethical issues about state-of-the-art AI-for-Art applications based on generative AI and what role our work is expected to play in these issues. \n\nAI-for-Art applications powered by diffusion models have reshaped the art market by significantly lowering the threshold for artistic creation. However, hidden behind such progress are unresolved copyright issues. \n\nUsing copyright-protected training data without the consent of image owners may constitute unauthorized reproduction and distribution, thereby giving rise to copyright infringement liability. One primary source of training data is LAION (Schuhmann et al., 2021;2022), a large-scale dataset of training images with text captions. A large portion of images in LAION was scraped from commercial image-hosting websites without the consent of the image owners (Carr & Jeffrey, 2022). The same issues exist in other generating processes involving unlicensed artworks, for example, learning paintings of a particular artist based on functions of AI-for-Art on a smaller scale without authorization. \n\nCopyright law protects authors' exclusive rights to reproduce, distribute, perform and display the artworks (Franceschelli & Musolesi, 2022). This legal structure makes it highly possible to constitute infringement by using others' artwork without a copyright license in the digital age (Sullivan, 1996). Throughout the AI-for-Art process, the transfer of unauthorized artworks from the platform on which it was originally published to AI's database along with the sale or distribution of the program including such database may constitute reproduction and distribution of the original artwork. This is related to the mechanism AI-for-Art applications created artworks. AI-for-Art applications work by fitting the training images and in turn recombining the learned data to generate new images, which may be understood as a special kind of reproduction. For some artworks with distinct well-known features, for example, cartoon figures owned by Disney, this reproduction is easy to detect (Baio, 2022). For this reason, the plaintiff lawyer representing artists whose works were used to train these generative AI tools referred to Diffusion Models as \"21stcentury collage tools\" in the recent lawsuit against several companies profiting from Stable Diffusion (Carr & Jeffrey, 2022).",
            "score": 0.4994546145982496,
            "section_title": "A. Ethical Issues",
            "char_start_offset": 23269,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 191
                },
                {
                    "start": 194,
                    "end": 337
                },
                {
                    "start": 338,
                    "end": 407
                },
                {
                    "start": 410,
                    "end": 601
                },
                {
                    "start": 602,
                    "end": 738
                },
                {
                    "start": 739,
                    "end": 888
                },
                {
                    "start": 889,
                    "end": 1101
                },
                {
                    "start": 1104,
                    "end": 1245
                },
                {
                    "start": 1246,
                    "end": 1408
                },
                {
                    "start": 1409,
                    "end": 1698
                },
                {
                    "start": 1699,
                    "end": 1773
                },
                {
                    "start": 1774,
                    "end": 1957
                },
                {
                    "start": 1958,
                    "end": 2106
                },
                {
                    "start": 2107,
                    "end": 2386
                }
            ],
            "ref_mentions": [
                {
                    "start": 1391,
                    "end": 1407,
                    "matchedPaperCorpusId": "143768780"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.96142578125
        },
        {
            "corpus_id": "260416806",
            "title": "Talkin' 'Bout AI Generation: Copyright and the Generative-AI Supply Chain (The Short Version)",
            "text": "Under existing copyright doctrine, this should probably end the inquiry: the output is not a \"copy\" of any of those works, and therefore does not infringe on the copyright in any of them, because infringement is assessed on a work-by-work basis. \n\nIn between, there will be difficult cases where similarity requires a more case-by-case and factually intensive analysis. For example, an output might resemble a particular input only in a few recognizable aspects -e.g., a text generation copies a few phrases from an article, or an image generation uses the composition and color palette of a painting but with a different subject, or where a musical generation mashes up multiple songs. In other cases, an output will draw broadly on common elements of a particular artist's stylean illustrator's linework, or a photographer's use of light and shadow -but not be a close copy of any specific work of that artist. All of these cases raise two common doctrinal questions: the quantitative threshold of substantial similarity, and the filtration of unprotectable ideas from the expression in those works. \n\nFrom our zoomed-out perspective, however, the very factual intensity and complexity of these issues paradoxically makes them legally uninteresting. To quote Learned Hand on the idea-expression dichotomy,\"Nobody has ever been able to fix that boundary, and nobody ever can. \" 19 Whether a particular generation is substantially similar or not is ultimately a jury question requiring assessment of audiences' subjective responses to the works. Generative AI will produce cases requiring this lay assessment, and beyond that it is impossible to anticipate in advance all of the possible variations that will arise. So we assume that some generated outputs will infringe from the viewpoint of lay audiences, but that it will not be possible to perfectly predict which ones will. \n\nSubstantial similarity of models, on the other hand, is a legally interesting question. A model, as a collection of weights, is different in kind from the types of copyrightable works it was trained on. Models are not themselves human-intelligible. No viewer would say that the model has the same \"total concept and feel\" as a painting; no reader would say that it is substantially similar to a blog post; and so on.",
            "score": 0.4989208290009407,
            "section_title": "B. Substantial Similarity",
            "char_start_offset": 19837,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 245
                },
                {
                    "start": 248,
                    "end": 369
                },
                {
                    "start": 370,
                    "end": 686
                },
                {
                    "start": 687,
                    "end": 912
                },
                {
                    "start": 913,
                    "end": 1101
                },
                {
                    "start": 1104,
                    "end": 1251
                },
                {
                    "start": 1252,
                    "end": 1376
                },
                {
                    "start": 1377,
                    "end": 1545
                },
                {
                    "start": 1546,
                    "end": 1715
                },
                {
                    "start": 1716,
                    "end": 1878
                },
                {
                    "start": 1881,
                    "end": 1968
                },
                {
                    "start": 1969,
                    "end": 2083
                },
                {
                    "start": 2084,
                    "end": 2129
                },
                {
                    "start": 2130,
                    "end": 2297
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.783203125
        },
        {
            "corpus_id": "270357944",
            "title": "Evaluating and Mitigating IP Infringement in Visual Generative AI",
            "text": "Existing works [23][24][25][26][27] find that the visual generative AI models have the memorizations on the training data.The potential reason for such IP infringement phenomenon is that the visual generative model have the memorizations on the training data [23][24][25], and the training data (e.g., LAION dataset [28] and WebVid dataset [29]) of the visual generative artificial intelligence might contain a large amount of publicly available copyrighted data.",
            "score": 0.49871531293186816,
            "section_title": "Related Work",
            "char_start_offset": 7745,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 122
                },
                {
                    "start": 122,
                    "end": 463
                }
            ],
            "ref_mentions": [
                {
                    "start": 19,
                    "end": 23,
                    "matchedPaperCorpusId": "254366634"
                },
                {
                    "start": 263,
                    "end": 267,
                    "matchedPaperCorpusId": "254366634"
                },
                {
                    "start": 316,
                    "end": 320,
                    "matchedPaperCorpusId": "252917726"
                },
                {
                    "start": 340,
                    "end": 344,
                    "matchedPaperCorpusId": "232478955"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.8671875
        },
        {
            "corpus_id": "272424221",
            "title": "ArtiFade: Learning to Generate High-quality Subject from Blemished Images",
            "text": "Our research addresses the emerging challenge of generating content from images with embedded watermarks, a scenario we term blemished subject-driven generation. Users often source images from the internet, some of which may contain watermarks intended to protect the original author's copyright and identity. However, our method is capable of removing various types of watermarks, potentially compromising the authorship and copyright protection. This could lead to increased instances of image piracy and the generation of illicit content. Hence, we advocate for legal compliance and the implementation of usage restrictions to govern the deployment of our technique and subsequent models in the future.",
            "score": 0.49864073647129625,
            "section_title": "J Social Impact",
            "char_start_offset": 28275,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 161
                },
                {
                    "start": 162,
                    "end": 309
                },
                {
                    "start": 310,
                    "end": 447
                },
                {
                    "start": 448,
                    "end": 541
                },
                {
                    "start": 542,
                    "end": 705
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.556640625
        },
        {
            "corpus_id": "273654195",
            "title": "Copyright-Aware Incentive Scheme for Generative Art Models Using Hierarchical Reinforcement Learning",
            "text": "Generative art using Diffusion models has achieved remarkable performance in image generation and text-to-image tasks. However, the increasing demand for training data in generative art raises significant concerns about copyright infringement, as models can produce images highly similar to copyrighted works. Existing solutions attempt to mitigate this by perturbing Diffusion models to reduce the likelihood of generating such images, but this often compromises model performance. Another approach focuses on economically compensating data holders for their contributions, yet it fails to address copyright loss adequately. Our approach begin with the introduction of a novel copyright metric grounded in copyright law and court precedents on infringement. We then employ the TRAK method to estimate the contribution of data holders. To accommodate the continuous data collection process, we divide the training into multiple rounds. Finally, We designed a hierarchical budget allocation method based on reinforcement learning to determine the budget for each round and the remuneration of the data holder based on the data holder's contribution and copyright loss in each round. Extensive experiments across three datasets show that our method outperforms all eight benchmarks, demonstrating its effectiveness in optimizing budget distribution in a copyright-aware manner. To the best of our knowledge, this is the first technical work that introduces to incentive contributors and protect their copyrights by compensating them.",
            "score": 0.497920306054828,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.9853515625
        },
        {
            "corpus_id": "261279983",
            "title": "AI Art and its Impact on Artists",
            "text": "Given the speed at which image generators have been adopted and their impact, countries around the world are grappling with what policies to enact in response. In particular, there is a lot of uncertainty about whether using copyrighted materials to train image generators is copyright infringement. Some governmental bodies, like the EU, will require companies to \"document and make publicly available a summary of the use of training data protected under copyright law\" 27  [44], which could trigger copyright lawsuits if it becomes possible to identify specific instances of copyright infringement [72]. However, it is not clear what the scope of this law is and if it requires an itemized list of what is included in the training data, or only a summary of other key information. \n\nWhile a number of artists have filed class action lawsuits in the US against companies providing commercial image generation tools [129], image generators represent a dynamic between artists and large-scale companies appropriating their work that has previously not been examined in US copyright law [56]. This is due to the unprecedented scale at which artists' works are being used to create image generators, the recent proliferation of publicly available image generators trained on that content, and the level to which the output of the image generators threatens to displace artists. Furthermore, this dynamic is distinct because of the data collection practices by which image generators are developed in the first place [67]. \n\nWhile some of the harms discussed in Section 4 overlap with the rights protected by US copyright law, others are not. There are also a number of unanswered legal questions when it comes to determining the ways in which copyright law applies to image generators and both the inputs and outputs that go into creating these tools. Hence, US copyright law is largely unequipped to tackle many of the types of harms posed by these systems to content creators. This lack of certainty about whether copyright applies means that the companies producing these tools can do so largely without accountability, unless they are sued for specific violations of copyright law. And waiting for court determinations on their lawsuits means that artists may not be able to get recourse until the cases are resolved. In this section, we highlight specific parts of US copyright law that may be a source of uncertainty and tension for artists and companies using their work.",
            "score": 0.49768643605907764,
            "section_title": "AI ART AND US COPYRIGHT LAW",
            "char_start_offset": 35476,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 159
                },
                {
                    "start": 160,
                    "end": 299
                },
                {
                    "start": 300,
                    "end": 606
                },
                {
                    "start": 607,
                    "end": 783
                },
                {
                    "start": 786,
                    "end": 1091
                },
                {
                    "start": 1092,
                    "end": 1375
                },
                {
                    "start": 1376,
                    "end": 1519
                },
                {
                    "start": 1522,
                    "end": 1639
                },
                {
                    "start": 1640,
                    "end": 1849
                },
                {
                    "start": 1850,
                    "end": 1976
                },
                {
                    "start": 1977,
                    "end": 2183
                },
                {
                    "start": 2184,
                    "end": 2319
                },
                {
                    "start": 2320,
                    "end": 2476
                }
            ],
            "ref_mentions": [
                {
                    "start": 1086,
                    "end": 1090,
                    "matchedPaperCorpusId": "233748873"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.947265625
        },
        {
            "corpus_id": "265551515",
            "title": "VA3: Virtually Assured Amplification Attack on Probabilistic Copyright Protection for Text-to-Image Generative Models",
            "text": "Text-to-image generative models, trained on large-scale datasets like LAION [44], have been equipped with enhanced memorization ability to generate outputs of high semantic similarity to their training data [3,48]. Given the prevalence of copyrighted works in these datasets, the significant risk of copyright infringement for these generations has raised great concerns from the public [6,18] and researchers [1,4,16,42,48,49,52]. Many efforts have been made to safeguard copyrighted materials from being infringed by generative diffusion models. Some researchers [23,24,41,45] introduced data perturbation, where input data is modified to hinder the model to imitate copyrighted features. Another separate line of works [14,21,26,43,58] exploited concept removal that erases unsafe concepts from existing pre-trained diffusion models to mitigate the risk of undesirable generations. In an alternative approach, researchers studied watermark protection for copyrighted data [9,29,36,54,56,59] to encode ownership information into potentially infringed outputs. \n\nA notable contribution, Vyas et al. [52] first provided a mathematical probabilistic upper-bound against copyrightinfringed generation. They asserted that the proposed near access-freeness (NAF) offers robust guarantees for copyright protection. However, Elkin-Koren et al. [12] argued the limitation of this method for reducing copyright to a matter of privacy from a legal perspective. In this paper, we build upon these discussions to present a significant challenge to these probabilistic copyright protection methods through the amplification attack.",
            "score": 0.49509106375748346,
            "section_title": "Copyright Issues in Generative Models",
            "char_start_offset": 4229,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 214
                },
                {
                    "start": 215,
                    "end": 431
                },
                {
                    "start": 432,
                    "end": 547
                },
                {
                    "start": 548,
                    "end": 690
                },
                {
                    "start": 691,
                    "end": 884
                },
                {
                    "start": 885,
                    "end": 1061
                },
                {
                    "start": 1064,
                    "end": 1199
                },
                {
                    "start": 1200,
                    "end": 1309
                },
                {
                    "start": 1310,
                    "end": 1451
                },
                {
                    "start": 1452,
                    "end": 1619
                }
            ],
            "ref_mentions": [
                {
                    "start": 76,
                    "end": 80,
                    "matchedPaperCorpusId": "252917726"
                },
                {
                    "start": 207,
                    "end": 210,
                    "matchedPaperCorpusId": "256389993"
                },
                {
                    "start": 210,
                    "end": 213,
                    "matchedPaperCorpusId": "254366634"
                },
                {
                    "start": 418,
                    "end": 421,
                    "matchedPaperCorpusId": "249375708"
                },
                {
                    "start": 421,
                    "end": 424,
                    "matchedPaperCorpusId": "254366634"
                },
                {
                    "start": 427,
                    "end": 430,
                    "matchedPaperCorpusId": "257050406"
                },
                {
                    "start": 569,
                    "end": 572,
                    "matchedPaperCorpusId": "256697414"
                },
                {
                    "start": 572,
                    "end": 575,
                    "matchedPaperCorpusId": "256826808"
                },
                {
                    "start": 729,
                    "end": 732,
                    "matchedPaperCorpusId": "257427549"
                },
                {
                    "start": 732,
                    "end": 735,
                    "matchedPaperCorpusId": "253420366"
                },
                {
                    "start": 981,
                    "end": 984,
                    "matchedPaperCorpusId": "227129816"
                },
                {
                    "start": 1100,
                    "end": 1104,
                    "matchedPaperCorpusId": "257050406"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.97998046875
        },
        {
            "corpus_id": "267400526",
            "title": "AIGC (Artificial Intelligence Generated Content) infringes the copyright of human artists",
            "text": "The participation of this study revolves around the images generated by AIGC. The images generated by AIGC refer to computer-generated images, graphics, or visual content produced through artificial intelligence. These images are typically generated using AI algorithms and deep learning models. They can be completely new images generated from scratch or modified and edited based on existing ones. Taking Stable Diffusion as an example, users can generate images using pure prompts or prompts combined with images. \n\nThe images generated by AIGC can have various forms and styles, ranging from realistic photo compositions to abstract artworks. As Marek J. Magdalena Z. (2023) mentioned, AI-generated images are created by training models on vast amounts of data and using that data to generate novel visual content [1]. As Brownlee J. (2019) published, the algorithms involved in image generation may utilize convolutional neural networks (CNNs), generative adversarial networks (GANs), or other machine learning methods [2]. \n\nAIGC-generated images find application in numerous fields, including visual effects production in the film and gaming industry, artistic creation, product design, and virtual reality. However, due to potential copyright and originality issues associated with AIGC-generated images, their usage and protection have sparked numerous copyright disputes and legal discussions. \n\nDue to the substantial increase in copyright infringement cases involving AIGC-generated images compared to other AIGC-generated content in recent years and the widespread use of AIGC image-generation technology in various industries, the risk of copyright disputes is higher. Furthermore, images are more susceptible to infringement and can be easily identified by copyright holders, compromising their interests. Therefore, it is justified to choose AIGC-generated images as the research subject.",
            "score": 0.4943845216234633,
            "section_title": "Participant",
            "char_start_offset": 4054,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 77
                },
                {
                    "start": 78,
                    "end": 212
                },
                {
                    "start": 213,
                    "end": 295
                },
                {
                    "start": 296,
                    "end": 399
                },
                {
                    "start": 400,
                    "end": 516
                },
                {
                    "start": 519,
                    "end": 646
                },
                {
                    "start": 647,
                    "end": 822
                },
                {
                    "start": 823,
                    "end": 1028
                },
                {
                    "start": 1031,
                    "end": 1214
                },
                {
                    "start": 1215,
                    "end": 1403
                },
                {
                    "start": 1406,
                    "end": 1682
                },
                {
                    "start": 1683,
                    "end": 1820
                },
                {
                    "start": 1821,
                    "end": 1904
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.9912109375
        },
        {
            "corpus_id": "274422925",
            "title": "Descriptions of women are longer than that of men: An analysis of gender portrayal prompts in Stable Diffusion",
            "text": "Specifying the artist's name in the prompt is a common practice in prompt engineering that drives the outcoming picture closer to the desired outcome. A subject of copyright infringement, companies like Stable Diffusion used many copyrighted materials to train their models, allowing creators to parody their beloved artists' styles. Injecting the prompt with the artist enables the artist to infuse the outcome image with colors, brushstrokes, prominent anatomical and facial features, outfits, and accessory designs. In our study, we identified a range of influential artists whose styles were excessively used to define the look of men and women generated by the users of Stable Diffusion. In particular, the images of men are associated with the artistic styles of Guy Denning, who is famous for his portraits featuring long and contrasting brushstrokes, as well as Donato Giancola, Ryan Jia, Irakli Nadar, Krenz Cushart, Rene Magritte, and Syd Mead, a visual futurist famous for his epic sci-fi landscapes. Similarly, women are depicted using the styles of Irakli Nadar, a Georgian portrait artist famous for soft and gentle anime-style portraits of women, and Liam Sharp -a British comic book artist famous for hyper-detailed images of superheroes and Wonder Woman in particular, Syd Mead, Donato Giancola, Jeremy Mann, and Krenz Cushart. It is worthwhile noting that our approach of applying TF-IDF, followed by manual inspection of terms, lends itself beyond the scope of this paper, to identify potential bulk copyright violations by machine learning companies that profit from AI image generation.",
            "score": 0.4908382494958771,
            "section_title": "Artists style",
            "char_start_offset": 22212,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 150
                },
                {
                    "start": 151,
                    "end": 333
                },
                {
                    "start": 334,
                    "end": 518
                },
                {
                    "start": 519,
                    "end": 692
                },
                {
                    "start": 693,
                    "end": 1011
                },
                {
                    "start": 1012,
                    "end": 1344
                },
                {
                    "start": 1345,
                    "end": 1607
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.8671875
        },
        {
            "corpus_id": "276558342",
            "title": "CopyJudge: Automated Copyright Infringement Identification and Mitigation in Text-to-Image Diffusion Models",
            "text": "Compared to cases of exact replication (memorization), here we consider specific IP infringement, such as imitation of cartoon IPs and artistic elements. Based on whether the input prompt contains direct copyright information, we consider two types of infringement scenarios: explicit infringement and implicit infringement. \n\nExplicit infringement. This refers to prompts that directly contain copyright information, such as \"Generate an image of Mickey Mouse.\" We use the 20 cartoon and artwork samples collected in section 5.1 to generate infringing images using Stable Diffusion v2, where the prompt explicitly includes the names or author names of the work. \n\nImplicit infringement. This occurs when the prompt does not explicitly contain copyright information, but the generated image still infringes due to certain infringing expressions. This type of scenario is more applicable to commercial text-to-image models, as they often include content detection modules that can effectively detect copyrighted information and thus reject the request. In this scenario, we use the same IP samples as above, but generate infringing images without any explicit copyright information using DALL\u2022E 3 (Betker et al., 2023), which has a safety detection module to reject prompts that trigger it. \n\nAutomated attack. Efficiently retrieving or generating infringing prompts has always been a challenge. Kim et al. utilize large models to iteratively generate jailbreak prompts targeting commercial models, thereby inducing them to output copyrighted content. Drawing from it, we use our CopyJudge to generate infringing prompts. In contrast to mitigation, for attack, we only need to use an LVLM to progressively intensify the infringing expressions within the prompt. The prompt is iteratively adjusted, and once the infringement score exceeds 0.8 / 1.0, mitigation is activated, using the current prompt as the starting point. \n\nFor explicit infringement, we validate both prompt control (PC) and latent control (LC). For implicit infringement, due to the commercial model DALL\u2022E's inability to customize latents, we only evaluate prompt control.",
            "score": 0.4895156796398643,
            "section_title": "IP INFRINGEMENT MITIGATION",
            "char_start_offset": 19610,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 153
                },
                {
                    "start": 154,
                    "end": 324
                },
                {
                    "start": 327,
                    "end": 349
                },
                {
                    "start": 350,
                    "end": 462
                },
                {
                    "start": 463,
                    "end": 662
                },
                {
                    "start": 665,
                    "end": 687
                },
                {
                    "start": 688,
                    "end": 845
                },
                {
                    "start": 846,
                    "end": 1051
                },
                {
                    "start": 1052,
                    "end": 1289
                },
                {
                    "start": 1292,
                    "end": 1309
                },
                {
                    "start": 1310,
                    "end": 1394
                },
                {
                    "start": 1395,
                    "end": 1550
                },
                {
                    "start": 1551,
                    "end": 1620
                },
                {
                    "start": 1621,
                    "end": 1760
                },
                {
                    "start": 1761,
                    "end": 1920
                },
                {
                    "start": 1923,
                    "end": 2011
                },
                {
                    "start": 2012,
                    "end": 2140
                }
            ],
            "ref_mentions": [
                {
                    "start": 1196,
                    "end": 1217,
                    "matchedPaperCorpusId": "264403242"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.98876953125
        },
        {
            "corpus_id": "265351912",
            "title": "CopyScope: Model-level Copyright Infringement Quantification in the Diffusion Workflow",
            "text": "We initiate our study by determining components that have the most significant impacts on the generated images. Components make up the AI image generation workflow, which is used to characterize generation models in our proposed copyright tracking approach. This stage is based on a survey from the world's largest AI image generation exchange and sharing platform Civitai, where we collect more than 16,000 generated image data from over 5,000 models to find commonalities in generated images. The generated images are divided into 6 themes: celebrity, film&TV, artwork, popular models, design, and game. We explore the distribution of models that generate images involving copyright infringement by calculating the usage rate of components: Base Model, Lora, ControlNet, and Key Prompt. Table 2 shows the frequency of these components, indicating that they have a high usage rate in AI image-generation tasks. \n\nWe identify four components that are used in AI image generation at a high frequency: \u2776 The Base Model is essential for each generated image. \u2777 The second is the Lora. Although the Lora is not a necessary option for generating images, we can find from Table 2 that it has a high application rate in each category, indicating that the use of Lora for adjustment in AI image generation has become a norm. \u2778 Prompts with particular specificity are called Key Prompts. Key Prompt can make the generated image close to the characteristics of these keywords to a large extent, thus infringing on the original author's rights. \u2779 The overall usage rate of ControlNet is relatively lower than the other components. This is because the ControlNet is challenging to use as it needs higher environment configuration requirements than Lora and Key Prompt[3]. However, ControlNet is an essential components in generating particular themes as it controls the structure of the image. From the perspective of copyright tracing, the ControlNet is a critical suspected infringement component that our CopyScope framework considers.",
            "score": 0.4855333327685923,
            "section_title": "Identify Influential components",
            "char_start_offset": 10528,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 111
                },
                {
                    "start": 112,
                    "end": 257
                },
                {
                    "start": 258,
                    "end": 494
                },
                {
                    "start": 495,
                    "end": 605
                },
                {
                    "start": 606,
                    "end": 788
                },
                {
                    "start": 789,
                    "end": 911
                },
                {
                    "start": 914,
                    "end": 1055
                },
                {
                    "start": 1056,
                    "end": 1081
                },
                {
                    "start": 1082,
                    "end": 1316
                },
                {
                    "start": 1317,
                    "end": 1378
                },
                {
                    "start": 1379,
                    "end": 1533
                },
                {
                    "start": 1534,
                    "end": 1619
                },
                {
                    "start": 1620,
                    "end": 1759
                },
                {
                    "start": 1760,
                    "end": 1881
                },
                {
                    "start": 1882,
                    "end": 2026
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.9921875
        },
        {
            "corpus_id": "276250558",
            "title": "Training-Free Constrained Generation With Stable Diffusion Models",
            "text": "This method ensures that the generated images are guided away from resembling copyrighted material while still allowing the model to produce high-quality outputs. By selectively modifying the generated content during the initial stages of denoising, we can effectively prevent the model from producing images that infringe on copyrights without significantly affecting the overall image quality. \n\nResults. Figure 4 (right) illustrates the correction path that occurs during the initial stages of denoising. Once the correction is completed, the denoising process proceeds freely, as shown in Figure 4 (left), where we compare the evolution of the original sample and that of the corrected sample. \n\nWe implement a Conditional Diffusion Model baselines using and unconstrained stable diffusion model identical to the one used for our method. The conditional baseline generates the protected cartoon character (Mickey Mouse) 33% of the time, despite conditioning it against these generations. \n\nConversely, our Latent Constrained Model only generates the protected cartoon character 10% of the time, aligning with the expected bounds of the classifier's predictive accuracy. Our method has proven to be highly effective because it preserves the generative capabilities of the model while imposing the defined constraints. Notably, the difference between the original image and the corrected one primarily affects the areas near the figure that violate the constraint, while the rest of the image remains largely unchanged. The FID scores of the generated images, increasing only slightly from 61.2 to 65.1, remain largely unaltered by the gradientbased correction. This demonstrates that our approach can selectively modify generated content to avoid copyrighted material without compromising overall image quality.",
            "score": 0.4849243238432534,
            "section_title": "Structural analysis",
            "char_start_offset": 30914,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 162
                },
                {
                    "start": 163,
                    "end": 395
                },
                {
                    "start": 398,
                    "end": 507
                },
                {
                    "start": 508,
                    "end": 697
                },
                {
                    "start": 700,
                    "end": 841
                },
                {
                    "start": 842,
                    "end": 991
                },
                {
                    "start": 994,
                    "end": 1173
                },
                {
                    "start": 1174,
                    "end": 1320
                },
                {
                    "start": 1321,
                    "end": 1521
                },
                {
                    "start": 1522,
                    "end": 1663
                },
                {
                    "start": 1664,
                    "end": 1814
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.98583984375
        },
        {
            "corpus_id": "272146279",
            "title": "RLCP: A Reinforcement Learning-based Copyright Protection Method for Text-to-Image Diffusion Model",
            "text": "Figure 2: Overview of RLCP. \n\naiming to produce a feature vector fi that is close to f i . \n\nTo ensure the model does not generate images that infringe on copyrights, we define a copyright loss CL, which penalizes the generation of images xi that are overly similar to those in D c . Additionally, we employ the Fr\u00e9chet Inception Distance (FID) to evaluate the quality of the generated images, ensuring that they are both visually coherent and diverse. \n\nThe objective is to train the model in such a way that it minimizes the copyright loss L c while maintaining a high FID score, effectively balancing the trade-off between reducing copyright infringement and preserving image quality.",
            "score": 0.4834282716593383,
            "section_title": "RL Fine-tune",
            "char_start_offset": 9106,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 27
                },
                {
                    "start": 30,
                    "end": 90
                },
                {
                    "start": 93,
                    "end": 283
                },
                {
                    "start": 284,
                    "end": 452
                },
                {
                    "start": 455,
                    "end": 687
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.97021484375
        },
        {
            "corpus_id": "261279983",
            "title": "AI Art and its Impact on Artists",
            "text": "The last 3 years have resulted in machine learning (ML)-based image generators with the ability to output consistently higher quality images based on natural language prompts as inputs. As a result, many popular commercial \u201cgenerative AI Art\u201d products have entered the market, making generative AI an estimated $48B industry [125]. However, many professional artists have spoken up about the harms they have experienced due to the proliferation of large scale image generators trained on image/text pairs from the Internet. In this paper, we review some of these harms which include reputational damage, economic loss, plagiarism and copyright infringement. To guard against these issues while reaping the potential benefits of image generators, we provide recommendations such as regulation that forces organizations to disclose their training data, and tools that help artists prevent using their content as training data without their consent.",
            "score": 0.48256832631753543,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.9375
        },
        {
            "corpus_id": "258967413",
            "title": "Alteration-free and Model-agnostic Origin Attribution of Generated Images",
            "text": "Recently, there has been a growing attention in image generation models. However, concerns have emerged regarding potential misuse and intellectual property (IP) infringement associated with these models. Therefore, it is necessary to analyze the origin of images by inferring if a specific image was generated by a particular model, i.e., origin attribution. Existing methods are limited in their applicability to specific types of generative models and require additional steps during training or generation. This restricts their use with pre-trained models that lack these specific operations and may compromise the quality of image generation. To overcome this problem, we first develop an alteration-free and model-agnostic origin attribution method via input reverse-engineering on image generation models, i.e., inverting the input of a particular model for a specific image. Given a particular model, we first analyze the differences in the hardness of reverse-engineering tasks for the generated images of the given model and other images. Based on our analysis, we propose a method that utilizes the reconstruction loss of reverse-engineering to infer the origin. Our proposed method effectively distinguishes between generated images from a specific generative model and other images, including those generated by different models and real images.",
            "score": 0.4822142265268388,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.93212890625
        },
        {
            "corpus_id": "278534729",
            "title": "DFA-CON: A Contrastive Learning Approach for Detecting Copyright Infringement in DeepFake Art",
            "text": "This section describes the overall methodology employed in this work. We first present DFA-CON, a contrastive representation learning framework designed to detect copyright infringement in AI-generated art (see Fig. 1). We then introduce an inference-time detection pipeline that leverages the pretrained embedding model to evaluate whether a generated image constitutes a potential infringement (see Fig. 2).",
            "score": 0.4807576586742124,
            "section_title": "METHODOLOGY",
            "char_start_offset": 7868,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 69
                },
                {
                    "start": 70,
                    "end": 219
                },
                {
                    "start": 220,
                    "end": 409
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.9580078125
        },
        {
            "corpus_id": "269214015",
            "title": "\\copyright Plug-in Authorization for Human Content Copyright Protection in Text-to-Image Model",
            "text": "Large foundation models are trained with extensive, high-quality datasets like LAION [1] and other enormous undisclosed data sources, which definitely contain copyrighted human contents.At the same time, these models not only excel at generating content based on user prompts [2][3][4], but also have the potential of memorizing the exact training data thanks to the huge capacity in their gigantic numbers of parameters [5].These have sparked copyright infringement concerns among content providers, artists, and users.A notable instance is the lawsuit filed by The New York Times against OpenAI and Microsoft [6], alleging the unauthorized use of a vast number of articles for the purpose of training automated chatbots.The lawsuit seeks the destruction of the allegedly infringing chatbots and their associated training data.Similar concerns and legal actions are also emerging in the field of text-to-image generation [7].\n\nAlthough it is still debatable whether using copyrighted material to train machine learning models is allowable due to the fair use exception in copyright laws [6,7], it's definitely that these powerful models have disrupted existing reward systems for creative artists and content providers [8].This paper advocates revisiting the goal of copyright laws [9], which is to protect the original expression of ideas in creative works and promote scientific progress by granting authors exclusive property rights for a limited time.However, generative AI models challenge the proper attribution of rewards to copyright holders, impacting artists and domain experts who may hesitate to contribute to knowledge exchange platforms without reasonable rewards.This could lead to a shortage of fresh data for machine learning.\n\nTo address these copyright challenges in generative AI models, we introduce the \"\u00a9Plug-in Authorization\" framework for text-to-image generation systems (see Figure 1(a)), to align with existing Intellectual Property (IP) management practices, allowing copyright holders to register their works as plug-ins and receive rewards for their use.End users pay for generating images with copyrighted concepts, and base model providers profit from plug-in registration and usage.This framework facilitates explicit tracking of copyrighted work usage, ensuring a fair reward system.",
            "score": 0.4797548101217388,
            "section_title": "Main",
            "char_start_offset": 7,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 186
                },
                {
                    "start": 186,
                    "end": 425
                },
                {
                    "start": 425,
                    "end": 520
                },
                {
                    "start": 520,
                    "end": 722
                },
                {
                    "start": 722,
                    "end": 828
                },
                {
                    "start": 828,
                    "end": 926
                },
                {
                    "start": 928,
                    "end": 1224
                },
                {
                    "start": 1224,
                    "end": 1456
                },
                {
                    "start": 1456,
                    "end": 1679
                },
                {
                    "start": 1679,
                    "end": 1744
                },
                {
                    "start": 1746,
                    "end": 2086
                },
                {
                    "start": 2086,
                    "end": 2217
                },
                {
                    "start": 2217,
                    "end": 2319
                }
            ],
            "ref_mentions": [
                {
                    "start": 85,
                    "end": 88,
                    "matchedPaperCorpusId": "252917726"
                },
                {
                    "start": 279,
                    "end": 282,
                    "matchedPaperCorpusId": "245335280"
                },
                {
                    "start": 421,
                    "end": 424,
                    "matchedPaperCorpusId": "229156229"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.962890625
        },
        {
            "corpus_id": "271600759",
            "title": "Replication in Visual Diffusion Models: A Survey and Outlook",
            "text": "As shown in Fig. 6 (a), training and generating processes in some visual diffusion models raise significant law issues due to the replication of copyrighted materials. As these models become more powerful and prevalent, an increasing number of legal scholars are focusing on this area. They primarily investigate how these models manage and utilize copyrighted materials during the creation process, along with the challenges and implications for the existing copyright law framework. For instance, they [51], [52], [177]- [182] question whether using copyrighted works as training data for AI constitutes copyright infringement, whether AI-generated outputs are derivative works infringing on the original copyrights, and who owns the copyright for AI-generated works. Furthermore, [182], [183] discuss the intricate infringement challenges that arise when generative AI models, particularly visual diffusion models, are trained using copyrighted materials without proper authorization. \n\nAdditionally, [184] aims to define and clarify what constitutes replication from the perspective of copyright infringement; [185] thoroughly explores the intersection of copyright law and economic principles in the context of rapid technological advancements; and the core idea of [186] is to evaluate whether privacy protection measures can align with and support copyright law. Beyond the copyright issues, there are also privacy concerns and corresponding data protection regulations [267], [268]. The replication of data by visual diffusion models can pose significant privacy risks, especially when the models inadvertently replicate sensitive or personal data. This contravenes data protection regulations such as the General Data Protection Regulation (GDPR) [269] in Europe, which mandates the protection of personal data with appropriate technical measures. Regulatory frameworks ensure that AI systems, particularly those trained on vast amounts of potentially sensitive data, comply with privacy regulations and do not retain or reproduce personal data without consent. \n\nThe replication of biases in training data by AI models is another regulatory concern [267], [270], [271]. Ensuring that diffusion models do not perpetuate or amplify biases present in the data they are trained on is crucial. Regulations enforce fairness, accountability, and transparency in AI systems to mitigate these issues. This could involve mandatory bias audits, transparency in data usage, and clear documentation of the data and methodologies used in training AI models.",
            "score": 0.4797088338677053,
            "section_title": "Regulation",
            "char_start_offset": 45154,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 167
                },
                {
                    "start": 168,
                    "end": 285
                },
                {
                    "start": 286,
                    "end": 484
                },
                {
                    "start": 485,
                    "end": 769
                },
                {
                    "start": 770,
                    "end": 987
                },
                {
                    "start": 990,
                    "end": 1369
                },
                {
                    "start": 1370,
                    "end": 1490
                },
                {
                    "start": 1491,
                    "end": 1656
                },
                {
                    "start": 1657,
                    "end": 1856
                },
                {
                    "start": 1857,
                    "end": 2070
                },
                {
                    "start": 2073,
                    "end": 2179
                },
                {
                    "start": 2180,
                    "end": 2298
                },
                {
                    "start": 2299,
                    "end": 2401
                },
                {
                    "start": 2402,
                    "end": 2553
                }
            ],
            "ref_mentions": [
                {
                    "start": 790,
                    "end": 795,
                    "matchedPaperCorpusId": "258684279"
                },
                {
                    "start": 1114,
                    "end": 1119,
                    "matchedPaperCorpusId": "264474179"
                },
                {
                    "start": 1477,
                    "end": 1482,
                    "matchedPaperCorpusId": "257280234"
                },
                {
                    "start": 2159,
                    "end": 2164,
                    "matchedPaperCorpusId": "257280234"
                },
                {
                    "start": 2173,
                    "end": 2178,
                    "matchedPaperCorpusId": "268164961"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.9189453125
        },
        {
            "corpus_id": "278534729",
            "title": "DFA-CON: A Contrastive Learning Approach for Detecting Copyright Infringement in DeepFake Art",
            "text": "The proposed inference pipeline is designed for practical copyright verification of AI-generated artworks. Consider a scenario where a user possesses a single image or a collection of original, copyright-protected artworks. The goal is to determine whether a given image produced by a generative AI model infringes upon any of the known originals. \n\nAs illustrated in Fig. 2, the generated image is first passed through the embedding model, which could be DFA-CON or any other pretrained visual encoder. The resulting embedding is normalized and compared against the set of pre-computed, normalized embeddings of the original artworks using cosine similarity. A threshold-based decision rule is then applied to determine whether the image constitutes a potential infringement. This pipeline is computationally lightweight and scalable, and it also enables top-k retrieval of the most similar original artworks for use in more fine-grained analysis or",
            "score": 0.4794267199731499,
            "section_title": "Copyright Infringement Detection Pipeline",
            "char_start_offset": 12716,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 106
                },
                {
                    "start": 107,
                    "end": 223
                },
                {
                    "start": 224,
                    "end": 347
                },
                {
                    "start": 350,
                    "end": 503
                },
                {
                    "start": 504,
                    "end": 659
                },
                {
                    "start": 660,
                    "end": 776
                },
                {
                    "start": 777,
                    "end": 950
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.97900390625
        },
        {
            "corpus_id": "267412857",
            "title": "Copyright Protection in Generative AI: A Technical Perspective",
            "text": "The development of Deep Generative Models (DGMs) marks a noteworthy advancement in image generation. Nevertheless, the impressive quality and authenticity of the generated images, as well as the efficiency in producing new ones, give rise to legitimate concerns regarding copyright matters within the realm of DGMs. \n\nData copyright protection. For the source data owner, which refers to the party or individual who owns the originality of image works, their data can be intentionally or unintentionally collected by model trainers as training samples to construct DGMs as introduced above. For example, recent studies [16,134] have demonstrated that popular DGMs are highly possible to completely replicate their training data samples, which is called memorization. \n\nThe possibility of data replication may severely offend the ownership of the original data samples. Moreover, the development of fine-tuning strategies such as DreamBooth can greatly improve the efficiency for unauthorized parties to directly edit or modify the source data to obtain new samples, which also severely infringes the copyright of the original works. \n\nModel copyright protection. To obtain DGMs with advanced generation performance, it is always necessary for the model trainers to invest a significant amount of funds and labor. It grants them intellectual property rights over the trained model. However, recent works also identify the possibility to steal others models [128].",
            "score": 0.4793273499897233,
            "section_title": "Copyright Issues in Image Generation",
            "char_start_offset": 12077,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 100
                },
                {
                    "start": 101,
                    "end": 315
                },
                {
                    "start": 318,
                    "end": 344
                },
                {
                    "start": 345,
                    "end": 590
                },
                {
                    "start": 591,
                    "end": 766
                },
                {
                    "start": 769,
                    "end": 868
                },
                {
                    "start": 869,
                    "end": 1132
                },
                {
                    "start": 1135,
                    "end": 1162
                },
                {
                    "start": 1163,
                    "end": 1312
                },
                {
                    "start": 1313,
                    "end": 1380
                },
                {
                    "start": 1381,
                    "end": 1462
                }
            ],
            "ref_mentions": [
                {
                    "start": 1456,
                    "end": 1461,
                    "matchedPaperCorpusId": "2984526"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.99169921875
        },
        {
            "corpus_id": "257280234",
            "title": "A Pathway Towards Responsible AI Generated Content",
            "text": "There is a risk of copyright infringement with the generated content if it copies existing works, whether intentionally or not, raising legal questions about IP infringement. \n\nIn November 2022, Matthew Butterick filed a class action lawsuit against Microsoft's subsidiary GitHub, accusing that their product Copilot, a code-generating service, violated copyright law [Butterick, 2022]. The lawsuit centers around Copilot's illegal use of licensed code sections from the internet without attribution. Texas A&M professor Tim Davis also provided examples of his code being copied verbatim by Copilot [Jennings, 2022]. Although Microsoft and OpenAI have acknowledged that Copilot is trained on open-source software in public GitHub repositories, Microsoft claims that the output of Copilot is merely a series of code \"suggestions\" and does not claim any rights in these suggestions. Microsoft also does not make any guarantees regarding the correctness, security, or copyright of the generated code. \n\nIn addition to code generation, text-to-image generative models like Stable Diffusion also faced accusations of infringing on the creative work of artists, as they are trained on billions of images from the Internet without the approval of the IP holders, which some argue is a violation of their rights. This is evident in Stable Diffusion, which has generated images with the Getty Images' watermark on them [Vincent, 2023]. Somepalli et al. [Somepalli et al., 2022] also presented evidence suggesting that Stable Diffusion copies from the data on which it was trained on. While Stable Diffusion disclaims any ownership of generated images and allows users to use them freely as long as the image content is legal and non-harmful, this freedom raises questions about ownership ethics.",
            "score": 0.47904538930497875,
            "section_title": "IP infringement examples",
            "char_start_offset": 17846,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 174
                },
                {
                    "start": 177,
                    "end": 386
                },
                {
                    "start": 387,
                    "end": 500
                },
                {
                    "start": 501,
                    "end": 616
                },
                {
                    "start": 617,
                    "end": 880
                },
                {
                    "start": 881,
                    "end": 997
                },
                {
                    "start": 1000,
                    "end": 1304
                },
                {
                    "start": 1305,
                    "end": 1426
                },
                {
                    "start": 1427,
                    "end": 1574
                },
                {
                    "start": 1575,
                    "end": 1786
                }
            ],
            "ref_mentions": [
                {
                    "start": 368,
                    "end": 385,
                    "matchedPaperCorpusId": "252780163"
                },
                {
                    "start": 1427,
                    "end": 1468,
                    "matchedPaperCorpusId": "262217524"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.97509765625
        },
        {
            "corpus_id": "266648315",
            "title": "Analysis on Tort Liability of Generative Artificial Intelligence",
            "text": "Generative AI technologies may inadvertently create infringements when creating new content, especially in the area of intellectual property.This technology can sometimes copy or mimic existing works, raising copyright infringement issues.For example, in text generation, content may be produced that is similar to an existing literary work, article, or report.In terms of image generation, elements of another person's artwork or photography may be inadvertently reproduced.Music generation can also face similar risks, such as producing musical works that are similar to the melody or rhythm of existing songs.In addition to direct content reproduction, generative AI may also inadvertently violate non-material rights such as reputation rights and privacy rights during the simulation creation process.For example, when news or social media content is generated, misinformation may be generated that damages the reputation of an individual or company, or sensitive personal information may be disclosed.The emergence of these risks is not only related to the characteristics of the technology itself, but also closely related to the data set used in the training process.If the dataset contains copyrighted content or sensitive information, the resulting results may contain infringing elements.",
            "score": 0.4788812490707999,
            "section_title": "Risk of infringement of generated content",
            "char_start_offset": 2858,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 1298
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.89013671875
        },
        {
            "corpus_id": "268532352",
            "title": "A Dataset and Benchmark for Copyright Infringement Unlearning from Text-to-Image Diffusion Models",
            "text": "In order to identify instances of infringement in these contexts, a comprehensive analysis encompassing both technical and semantic aspects of the generated contents is indispensable.\n\nIn light of the aforementioned considerations, we introduce the first comprehensive dataset tailored for copyright protection in this domain: Copyright Protection from Diffusion Model (CPDM) dataset.Specifically, we have curated copyright images from four distinct categories that are most commonly suspected of infringement, as demonstrated in Fig. 2.This dataset encompasses a collection of original copyright images, associated prompts for text-to-image generation via Stable Diffusion, and indicatives for a series of features, represented by: 1) Potential for Infringement; 2) Effectiveness of Unlearning; 3) Extent of Model Degradation during Unlearning.\n\nPotential for Infringement.This is quantified by feature-level similarities between the original copyright images and potentially infringing generated contents, denoted as CPDM metric (CM).",
            "score": 0.4786603389388866,
            "section_title": "Introduction",
            "char_start_offset": 2456,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 183
                },
                {
                    "start": 185,
                    "end": 384
                },
                {
                    "start": 384,
                    "end": 537
                },
                {
                    "start": 537,
                    "end": 845
                },
                {
                    "start": 847,
                    "end": 874
                },
                {
                    "start": 874,
                    "end": 1036
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.9833984375
        },
        {
            "corpus_id": "267400526",
            "title": "AIGC (Artificial Intelligence Generated Content) infringes the copyright of human artists",
            "text": "The emergence of artificial intelligence-generated content (AIGC) has brought significant advancements in various fields, such as advertising copywriting, news reporting, and scientific creation. This technology, known as AIGC, has led to reduced labor costs and improved creative efficiency, thereby delivering greater commercial and creative value to businesses and creators. However, controversies surrounding copyright issues and innovative capabilities during the content generation process have become a subject of concern. \n\nAs a generative AI technology for text-to-image generation, the Stable Diffusion technique has garnered global attention. While this technology appears remarkably innovative, it also brings forth certain issues, including copyright concerns. \n\nStable Diffusion can quickly generate high-quality images based on simple text descriptions, thanks to its training on billions of sample images collected online. However, not all of these sample images fall within the public domain; some are even protected by copyright. As one can imagine, the authors and photographers of these copyrighted images are not pleased with the actions of Stable Diffusion. \n\nStable Diffusion has faced multiple lawsuits due to copyright issues. Artists and photographers have initiated collective lawsuits against this technology, and one of the world's leading image suppliers, Getty Images Holdings Inc., has also filed a similar case. According to Getty's (2023) complaint, \"Stability AI has reproduced over 12 million Getty Images, along with their related captions and metadata, without permission or compensation.\" Consequently, delving deeper into this issue and exploring potential solutions is essential. \n\nAIGC model training phase. One critical aspect of using copyrighted works in AIGC is the two-stage process of utilizing these works. To illustrate this process, the Stable Diffusion technique will be used as an example. During the model training phase, copyrighted works are copied from LAION-5B (Large-Scale Artificial Intelligence Online) database and modified before being incorporated into the image information space. This process ensures convenient output for future user consumption. \n\nAIGC model output stage. However, the second stage of output generation raises concerns regarding the potential infringement of copyrighted works. If the generated image is deemed \"substantially similar\" to the original work in expression, it may infringe upon the original work's \"reproduction right.\"",
            "score": 0.47688708578819816,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 195
                },
                {
                    "start": 196,
                    "end": 377
                },
                {
                    "start": 378,
                    "end": 529
                },
                {
                    "start": 532,
                    "end": 653
                },
                {
                    "start": 654,
                    "end": 773
                },
                {
                    "start": 776,
                    "end": 938
                },
                {
                    "start": 939,
                    "end": 1047
                },
                {
                    "start": 1048,
                    "end": 1179
                },
                {
                    "start": 1182,
                    "end": 1251
                },
                {
                    "start": 1252,
                    "end": 1444
                },
                {
                    "start": 1445,
                    "end": 1627
                },
                {
                    "start": 1628,
                    "end": 1720
                },
                {
                    "start": 1723,
                    "end": 1749
                },
                {
                    "start": 1750,
                    "end": 1855
                },
                {
                    "start": 1856,
                    "end": 1942
                },
                {
                    "start": 1943,
                    "end": 2145
                },
                {
                    "start": 2146,
                    "end": 2213
                },
                {
                    "start": 2216,
                    "end": 2240
                },
                {
                    "start": 2241,
                    "end": 2362
                },
                {
                    "start": 2363,
                    "end": 2518
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.99072265625
        },
        {
            "corpus_id": "269214015",
            "title": "\\copyright Plug-in Authorization for Human Content Copyright Protection in Text-to-Image Model",
            "text": "This framework facilitates explicit tracking of copyrighted work usage, ensuring a fair reward system.\n\nTechnically to enable an effective and efficient copyright authorization, the plugins, as permits, should be easily created by addition if copyrighted works are new to the base models, or by extraction if the copyrighted works are already infringed by the base model.Moreover, the plug-ins should be easily combined, which allows copyright holders to merge multiple plug-ins into a new one or enables end users to generate images with multiple copyrighted works.Meanwhile, for efficient execution, these operations should be implemented as light adaptations to the base model, e.g., parameter-efficient tuning methods or prompt designs.We introduce three foundational operations -addition, extraction, and combination -implemented using the Low-Rank Adaptor (LoRA) method [10].These operations are essential for realizing the Copyright Plug-in Authorization (Figure 1(b) for an overview).\n\nIn summary, we make a conceptual contribution: advocating solving the problem of copyright infringement in foundation models with a \u00a9Plug-in Authorization framework.It can offer a fair and practical solution for the attribution challenge in text-to-image generative models.We further introduce three operations addition, extraction and combination to instantiate the framework with efficient human content copyright authorizations.Technically, we propose a novel \"Reverse LoRA\" algorithm for extraction.It can effectively extract copyrighted concepts from the base model, achieving competitive performance for concept extraction with flexible plug-ins, which are verified with deleting various artist styles and cartoon IPs.We further propose a novel \"EasyMerge\" approach for combination.It is a data-free layer-wise distillation approach, which can effectively and efficiently address the challenge of combining multiple LoRA components.These methodological innovations are extensively verified with practical text-to-image generation scenarios.",
            "score": 0.4758389266314333,
            "section_title": "Main",
            "char_start_offset": 2224,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 102
                },
                {
                    "start": 104,
                    "end": 371
                },
                {
                    "start": 371,
                    "end": 566
                },
                {
                    "start": 566,
                    "end": 740
                },
                {
                    "start": 740,
                    "end": 881
                },
                {
                    "start": 881,
                    "end": 992
                },
                {
                    "start": 994,
                    "end": 1159
                },
                {
                    "start": 1159,
                    "end": 1267
                },
                {
                    "start": 1267,
                    "end": 1425
                },
                {
                    "start": 1425,
                    "end": 1497
                },
                {
                    "start": 1497,
                    "end": 1718
                },
                {
                    "start": 1718,
                    "end": 1782
                },
                {
                    "start": 1782,
                    "end": 1932
                },
                {
                    "start": 1932,
                    "end": 2040
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.9384765625
        },
        {
            "corpus_id": "270620122",
            "title": "Fantastic Copyrighted Beasts and How (Not) to Generate Them",
            "text": "DALL\u2022E 3 blocks explicit name prompts requests (character name anchoring) with content policy messages. models may generate, copyrighted characters pose a unique legal challenge (Sag, 2023;Henderson et al., 2023;Lee et al., 2024), since characters are a type of \"abstractly\" protected content (Sag, 2023) Copyrighted characters also pose a unique technical challenge, as they are like general concepts that can appear in many poses, sizes, and variations, making it challenging to apply many previous methods designed for (near) verbatim memorization. As a result, commercial platforms like DALL\u2022E have started implementing tailored measures, such as prompt rewriting, with the goal of preventing the generation of copyrighted characters. \n\nHowever, two key questions regarding the infringement of copyrighted characters in generative models have not been systematically studied: (Q1) How easy is it to prompt models to generate copyrighted characters, especially when the copyrighted character is prompted indirectly or generated inadvertently? (Q2) How effective are common mitigation approaches at reducing copyrighted characters in image generation models? \n\nTo address these questions, we present novel evaluation metrics that measure both the generated image's resemblance to copyrighted characters and its consistency with user input. We use a diverse set of 50 popular characters from various studios and regions as the basis for our analysis. We use this framework to evaluate five image generation models -Playground v2.5 (Li et al., 2024a), Stable Diffusion XL (Podell et al., 2024), PixArt-\u03b1 (Chen et al., 2024), DeepFloyd IF (StabilityAI, 2023), and DALL\u2022E 3 (Betker et al., 2023) -along with one video generation model, VideoFusion (Luo et al., 2023), using a diverse set of popular copyrighted characters ( \u00a7 2). \n\nWe study Q1 using our evaluation framework, showing that as few as two generic keywords associated with a character are enough to generate their image, without mentioning their name (Fig. 1).",
            "score": 0.47550604415551667,
            "section_title": "INTRODUCTION",
            "char_start_offset": 1509,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 103
                },
                {
                    "start": 104,
                    "end": 551
                },
                {
                    "start": 552,
                    "end": 738
                },
                {
                    "start": 741,
                    "end": 1045
                },
                {
                    "start": 1046,
                    "end": 1160
                },
                {
                    "start": 1163,
                    "end": 1341
                },
                {
                    "start": 1342,
                    "end": 1451
                },
                {
                    "start": 1452,
                    "end": 1827
                },
                {
                    "start": 1830,
                    "end": 2021
                }
            ],
            "ref_mentions": [
                {
                    "start": 1746,
                    "end": 1764,
                    "matchedPaperCorpusId": "257532642"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.96435546875
        },
        {
            "corpus_id": "267400526",
            "title": "AIGC (Artificial Intelligence Generated Content) infringes the copyright of human artists",
            "text": "In generating images, the artificial intelligence generation platform led by Stable Diffusion often directly plagiarizes the images used for training or moves the elements in them. In Getty v. Stable Diffusion (James V, 2023), Getty showed ample evidence that Stable Diffusion recreated its watermark in images generated by Stable Diffusion, which Getty argued appeared on models' \"bizarre or grotesque images that would be blurred or defaced.\" and impair the quality of the Getty Images markup\" [8]. , such incidents occur frequently, violating the interests of the copyright owner and quickly confusing the public, making it difficult to distinguish the actual owner of the copyright. Additionally, Companies using AI-generated technology profit from it. As Lucas R. (2023) projected, \"After OpenAI freely exploited everybody's web content, it then proceeded to use that data to build commercial products that it is now attempting to sell back to the public for exorbitant sums of money.\" [9]",
            "score": 0.4750920632668228,
            "section_title": "Copying or modifying elements of a copyrighted work",
            "char_start_offset": 17779,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 180
                },
                {
                    "start": 181,
                    "end": 444
                },
                {
                    "start": 445,
                    "end": 500
                },
                {
                    "start": 501,
                    "end": 686
                },
                {
                    "start": 687,
                    "end": 756
                },
                {
                    "start": 757,
                    "end": 990
                },
                {
                    "start": 991,
                    "end": 994
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.970703125
        },
        {
            "corpus_id": "269214015",
            "title": "\\copyright Plug-in Authorization for Human Content Copyright Protection in Text-to-Image Model",
            "text": "To curate the dataset, we randomly generate images with the base model using the contextual prompt \"the painting of a building\", while leveraging the negative prompt [22] \"Picasso\" to steer the generation as far away from the target concept \"Picasso\" as possible.Specifically, we further optimize w L with the objective, arg min\n\nwhere X, c represent the constructed pairs (image, prompt) to recover the generation capability of surrounding contexts.Overall, after the de-concept step, the model w \u2212 w L is unable to generate images in the Picasso style, and it performs well with surrounding prompts thanks to the recontext step.Therefore, through the extraction operation, we obtain a non-infringing model w = w \u2212 w L and a \u00a9plug-in w L .By incorporating the \u00a9plug-in, the model is restored to the original base model w, regaining the capability of successfully generating the artworks in the \"Picasso\" style.The intermediate results of the extraction process are visually illustrated in Figure 5 in Appendix B, showcasing the successful extraction of the targeted copyright, while preserving the model's ability to generate images with surrounding contexts.",
            "score": 0.4729580318520921,
            "section_title": "Preliminary on Diffusion Generative Model",
            "char_start_offset": 10823,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 263
                },
                {
                    "start": 263,
                    "end": 328
                },
                {
                    "start": 330,
                    "end": 450
                },
                {
                    "start": 450,
                    "end": 630
                },
                {
                    "start": 630,
                    "end": 740
                },
                {
                    "start": 740,
                    "end": 911
                },
                {
                    "start": 911,
                    "end": 1160
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.59033203125
        },
        {
            "corpus_id": "268532352",
            "title": "A Dataset and Benchmark for Copyright Infringement Unlearning from Text-to-Image Diffusion Models",
            "text": "Our experiments can be roughly categorized into two perspectives.First, indicating the potential of infringement of SD-Generated images, using the original version of SD (SD-v1.4,SD-v2.1) and the finetuned counterpart (SD-Finetuned) respectively.Note that we directly test the SD generative models using the data from WikiArt and Wikipedia, as shown in Tab.3' top.We test both the SD-Generated and SD-Finetuned models on the Illustration part of our CPDM dataset, from an anonymous artist, as shown in Tab.3' bottom.Second, we conduct a comprehensive evaluation of unlearning methods, regarding their ability to remove copyright-relevant concepts and maintain the copyright-irrelevant generative capacities.In this stage, we incorporate four representative unlearning methods tailored for text-to-image generative tasks, ESD [7], FMN [42], CA [27], and UCE [8] (further details on the setups are provided in the supplementary materials), along with our proposed baseline unlearning methods, GA and WP.As shown in Tab 3, the CPDM metrics are evaluated on a case-bycase basis, comparing anchored copyright images to the generated images produced by SD models and their unlearned counterparts, forming image pairs of <Image cpr , Image gen > and <Image cpr , Image unl >, respectively.Specifically, FID is calculated by generating 10,000 images using each model on the COCO-10K dataset [30].In the context of an unlearning process, for our proposed two baseline unlearning methods, we only make parameter adjustments on the U-Net structure of SD models [34], while freezing the other parameters.",
            "score": 0.47282705685134324,
            "section_title": "Experiment Setting",
            "char_start_offset": 17538,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 65
                },
                {
                    "start": 65,
                    "end": 179
                },
                {
                    "start": 179,
                    "end": 246
                },
                {
                    "start": 246,
                    "end": 357
                },
                {
                    "start": 357,
                    "end": 364
                },
                {
                    "start": 364,
                    "end": 506
                },
                {
                    "start": 506,
                    "end": 516
                },
                {
                    "start": 516,
                    "end": 707
                },
                {
                    "start": 707,
                    "end": 1001
                },
                {
                    "start": 1001,
                    "end": 1282
                },
                {
                    "start": 1282,
                    "end": 1388
                },
                {
                    "start": 1388,
                    "end": 1592
                }
            ],
            "ref_mentions": [
                {
                    "start": 843,
                    "end": 847,
                    "matchedPaperCorpusId": "257687839"
                },
                {
                    "start": 857,
                    "end": 860,
                    "matchedPaperCorpusId": "261276613"
                },
                {
                    "start": 1383,
                    "end": 1387,
                    "matchedPaperCorpusId": "14113767"
                },
                {
                    "start": 1550,
                    "end": 1554,
                    "matchedPaperCorpusId": "245335280"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.94873046875
        },
        {
            "corpus_id": "265456119",
            "title": "ChatGPT and Beyond: The Generative AI Revolution in Education",
            "text": "Generative AI models can be used to generate copyrighted content without permission, raising copyright implications. \n\nSteps should be taken to avoid copyright infringement. Intellectual Property Ownership Ownership of intellectual property rights to content generated by generative AI models is unclear, leading to potential disputes over ownership and licensing.",
            "score": 0.4727705749770736,
            "section_title": "Legal Implication Description Copyright Infringement",
            "char_start_offset": 56170,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 116
                },
                {
                    "start": 119,
                    "end": 173
                },
                {
                    "start": 174,
                    "end": 364
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.74169921875
        },
        {
            "corpus_id": "260416806",
            "title": "Talkin' 'Bout AI Generation: Copyright and the Generative-AI Supply Chain (The Short Version)",
            "text": "The strength of these grounds will vary from case to case. \n\nIt is not possible to make blanket statements about the remainder of factor one, or about factors two and three. \n\n\u2022 Factor one: Some output uses will be commercial, some will not. (We are thinking here of the uses made by the users of the generations; there is a colorable argument that the commerciality of generation as a service may be relevant to the fair use claims as to the generations, but we think our analysis is slightly cleaner and more useful.) \n\n\u2022 Factor two: Most of the training data will typically have been \"published\" within the meaning of copyright law; it would not otherwise be available within the training data at all. (A fraction of works may have been held confidentially and released into training datasets without their authors' permission; the case for removing such works is comparatively stronger.) Some training data will be primarily informational; some will be primarily expressive. \n\n\u2022 Factor three: This is a replay of substantial similarity. Some generations will closely resemble the works they were copied from; others will copy comparatively smaller portions of the works, both qualitatively and quantitatively. Even when a work is transformative under the first factor, courts will still also inquire into whether the generation copies more than necessary for that transformation -a \"painting of a car driving in a snowstorm in the style of Frida Kahlo\" might copy just her color palette, brushwork, and floral motifs, or it might also put the entire composition of Self-Portrait with Thorn Necklace and Hummingbird inside the resulting image. We conclude that some generations will be fair uses and others will nota conclusion that forces a reconsideration of whether the underlying models are fair uses. The nonexpressive-use argument no longer goes through, because some uses of these models are expressive -and indeed, some of them are infringing. \n\nA full four-factor analysis has been given elsewhere, so we will belabor the details. Instead, we emphasize a few points. First, the models qua models are arguably highly transformative -both because they represent the works internally in new and very different ways, and also because they are capable of generating highly transformative works as outputs.",
            "score": 0.4725753590274144,
            "section_title": "Id. \u00a7 512(i)(1)(A",
            "char_start_offset": 48183,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 58
                },
                {
                    "start": 61,
                    "end": 173
                },
                {
                    "start": 176,
                    "end": 241
                },
                {
                    "start": 242,
                    "end": 519
                },
                {
                    "start": 522,
                    "end": 704
                },
                {
                    "start": 705,
                    "end": 891
                },
                {
                    "start": 892,
                    "end": 978
                },
                {
                    "start": 981,
                    "end": 1040
                },
                {
                    "start": 1041,
                    "end": 1213
                },
                {
                    "start": 1214,
                    "end": 1646
                },
                {
                    "start": 1647,
                    "end": 1808
                },
                {
                    "start": 1809,
                    "end": 1954
                },
                {
                    "start": 1957,
                    "end": 2042
                },
                {
                    "start": 2043,
                    "end": 2078
                },
                {
                    "start": 2079,
                    "end": 2312
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.55029296875
        },
        {
            "corpus_id": "257495777",
            "title": "Erasing Concepts from Diffusion Models",
            "text": "The high cost of the process makes it challenging to establish a causal connection between specific changes in the data and the capabilities that emerge, but users report that removing explicit images and other subjects from the training data may have had a negative impact on the output quality [30]. And despite the effort, explicit content remains prevalent in the model's output: when we evaluate generation of images using prompts from the 4,703 prompts of the Inappropriate Image Prompts (I2P) benchmark [38], we find that the popular SD 1.4 model produces 796 images with exposed body parts identified by a nudity detector, while the new trainingset-restricted SD 2.0 model produces 417 (Figure 7). \n\nAnother major concern regarding the text-to-image models is their ability to imitate potentially copyrighted content. Not only is the quality of the AI-generated art on par with the human-generated art [34], it can also faithfully replicate an artistic style of real artists. Users of Stable Diffusion [31] and other large-scale text-to-image synthesis systems have discovered that prompts such as \"art in the style of [artist]\" can mimic styles of specific artists, potentially devaluing original work. Copyright concerns of several artists has led to a lawsuit against the makers of Stable Diffusion [1], raising new legal issues [41]; the courts have yet to rule on these cases. Recent work [42] aims to protect the artist by applying an adversarial perturbation to artwork before posting it online to prevent the model from imitating it. That approach, however, cannot remove a learned artistic style from a pretrained model. \n\nIn response to safety and copyright infringement concerns, we propose a method for erasing a concept from a text-to-image model. Our method, Erased Stable Diffusion (ESD), fine-tunes the model's parameters using only undesired concept descriptions and no additional training data. Unlike training-set censorship approaches, our method is fast and does not require training the whole system from scratch. Furthermore, our method can be applied to existing models without the need to modify input images [42].",
            "score": 0.4717205527219292,
            "section_title": "Introduction",
            "char_start_offset": 1667,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 301
                },
                {
                    "start": 302,
                    "end": 705
                },
                {
                    "start": 708,
                    "end": 825
                },
                {
                    "start": 826,
                    "end": 983
                },
                {
                    "start": 984,
                    "end": 1211
                },
                {
                    "start": 1212,
                    "end": 1389
                },
                {
                    "start": 1390,
                    "end": 1549
                },
                {
                    "start": 1550,
                    "end": 1637
                },
                {
                    "start": 1640,
                    "end": 1768
                },
                {
                    "start": 1769,
                    "end": 1920
                },
                {
                    "start": 1921,
                    "end": 2043
                },
                {
                    "start": 2044,
                    "end": 2147
                }
            ],
            "ref_mentions": [
                {
                    "start": 1010,
                    "end": 1014,
                    "matchedPaperCorpusId": "245335280"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.935546875
        },
        {
            "corpus_id": "259298538",
            "title": "The Dance of the Doppelg\u00e4ngers: AI and the cultural heritage community",
            "text": "Perhaps the toughest challenge for a is the question of copyright. There are two main issues around copyright and AI image generators. The first is whether the images that were used to train the software have been licensed and whether permission was granted. The second is the issue of who owns the copyright of the generated image. The various image generators trained their models on millions of pictures scraped off the web but more often without telling their creators or seeking their consent. This leaves the field wide open for rampant exploitation and in my opinion is basically an accident waiting to happen. Legal proceedings are already underway. In the United States, a class action was launched by Mathew Butterick who is representing people from all over the worldspecifically writers, artists, programmers, and other creators-who are concerned about AI systems being trained on vast amounts of copyrighted work with no consent, no credit, and no compensation. In the United Kingdom, Getty Images, a supplier of commercial photographs, is suing the company behind Stable Diffusion. Getty Images claims Stability AI 'unlawfully' scraped millions of images from its site, which in their opinion is a significant escalation in the developing legal battles between generative AI firms and content creators. \n\nShutterstock is now removing AI-generated Images. \n\nThe flood gates are already open, and AI generated images are besieging the cultural heritage sector. \n\nThe battles have just begun. The move towards litigation is gathering momentum. Midjourney's terms of service now includes a DMCA takedown policy, allowing artists to request their work to be removed from the set if they believe copyright infringement to be evident. GitHub offers a Guide to the Digital Millennium Copyright Act, commonly known as the \"DMCA as a pathway to those bewildered. \n\nTo explore the potential of AI text to image generators I created an exhibition using the prompt 'oil paintings, based on the style of Rembrandt'. \n\nMy goal was to test the potential of the interface and I was astounded when the magical creatures started appearing on my screen. The Uncanny Animal Musiciansan AI Exhibition soon took shape, populated with fantastical animal musicians. My family loved them, and they are now hung next to the piano for inspiration and have their very own website.",
            "score": 0.4715253617094168,
            "section_title": "Copyright",
            "char_start_offset": 7899,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 66
                },
                {
                    "start": 67,
                    "end": 134
                },
                {
                    "start": 135,
                    "end": 258
                },
                {
                    "start": 259,
                    "end": 332
                },
                {
                    "start": 333,
                    "end": 498
                },
                {
                    "start": 499,
                    "end": 617
                },
                {
                    "start": 618,
                    "end": 657
                },
                {
                    "start": 658,
                    "end": 974
                },
                {
                    "start": 975,
                    "end": 1095
                },
                {
                    "start": 1096,
                    "end": 1316
                },
                {
                    "start": 1319,
                    "end": 1368
                },
                {
                    "start": 1371,
                    "end": 1472
                },
                {
                    "start": 1475,
                    "end": 1503
                },
                {
                    "start": 1504,
                    "end": 1554
                },
                {
                    "start": 1555,
                    "end": 1741
                },
                {
                    "start": 1742,
                    "end": 1866
                },
                {
                    "start": 1869,
                    "end": 2015
                },
                {
                    "start": 2018,
                    "end": 2147
                },
                {
                    "start": 2148,
                    "end": 2254
                },
                {
                    "start": 2255,
                    "end": 2365
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.94775390625
        },
        {
            "corpus_id": "256697414",
            "title": "Adversarial Example Does Good: Preventing Painting Imitation from Diffusion Models via Adversarial Examples",
            "text": "Recent years have witnessed a boom of deep diffusion models (Sohl-Dickstein et al., 2015;Ho et al., 2020) in computer vision. With solid theoretical foundations (Song et al., 2020b;a;Bao et al., 2022) and highly applicable techniques (Gal et al., 2022;Lu et al., 2022), diffusion models have proven to be effective in generative tasks, including image synthesis (Ruiz et al., 2022), video synthesis (Yang   (Gal et al., 2022) in DMs, which has raised copyright concerns in several cases (MT, 2022;Deck, 2022). et al., 2022), image editing (Kawar et al., 2022), and text-to-3D synthesis (Poole et al., 2022). Among these, the latent diffusion model (Rombach et al., 2022) shows great power in artwork creation, sparking a commercialization flurry of AI for Art. \n\nDespite the success of diffusion models in commercialization, it has been a public concern that these models empower some copyright violations. For example, textual inversion (Gal et al., 2022), a novel function implemented in most AI-for-Art applications based on the latent diffusion model, can imitate the art style of human-created paintings with several samples. Cases have taken place that copyright infringers easily fetch paintings created by artists online and illegally use them to train models with the help of textual inversion (MT, 2022;Deck, 2022). Although artists have the right to declare prohibition for their artworks to be used for training AI-for-Art models, there is no existing technology to prevent or track this illegal use, leading to an even lower crime cost and difficulty in proof generating. Moreover, artists suffer from a lack of resources to start legal challenges against the infringers (Vincent, 2022) (See Appendix A for more discussion about ethical issues of AI for Art). Hence, the art society is calling for off-the-shelf techniques to protect the copyright of paintings against AI for Art (Vincent, 2022).",
            "score": 0.47079469658899004,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 125
                },
                {
                    "start": 126,
                    "end": 509
                },
                {
                    "start": 510,
                    "end": 607
                },
                {
                    "start": 608,
                    "end": 760
                },
                {
                    "start": 763,
                    "end": 906
                },
                {
                    "start": 907,
                    "end": 1130
                },
                {
                    "start": 1131,
                    "end": 1325
                },
                {
                    "start": 1326,
                    "end": 1584
                },
                {
                    "start": 1585,
                    "end": 1772
                },
                {
                    "start": 1773,
                    "end": 1909
                }
            ],
            "ref_mentions": [
                {
                    "start": 60,
                    "end": 89,
                    "matchedPaperCorpusId": "14888175"
                },
                {
                    "start": 89,
                    "end": 105,
                    "matchedPaperCorpusId": "219955663"
                },
                {
                    "start": 497,
                    "end": 508,
                    "matchedPaperCorpusId": "234357997"
                },
                {
                    "start": 648,
                    "end": 670,
                    "matchedPaperCorpusId": "245335280"
                },
                {
                    "start": 1313,
                    "end": 1324,
                    "matchedPaperCorpusId": "234357997"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.8681640625
        },
        {
            "corpus_id": "269137391",
            "title": "Uncertain Boundaries: Multidisciplinary Approaches to Copyright Issues in Generative AI",
            "text": "In the swiftly progressing realm of generative artificial intelligence (AI), the pressing concern of copyright infringement emerges prominently.As AI technologies continue to autonomously generate content from copyrighted data, inquiries about ownership and safeguarding rights surface, reverberating across diverse professional domains.This escalating trend raises critical discussions surrounding ethical, legal, and socio-economic implications, necessitating nuanced exploration and strategic interventions to navigate this evolving landscape effectively.For instance, in July 2023 a group of novelists collectively sued OpenAI for alleged usage of their books to train their models and output similar content to the novelists' prose [117].Moreover, in December 2023 The New York Times filed a lawsuit against OpenAI and Microsoft, alleging copyright infringement by having its articles scraped without permission to train their generative models [118].More recently, Marcus and Southen revealed how generative models such as Midjourney and OpenAI's Chat GPT-4 produced outputs strongly reminiscent of scenes from copyrighted films and shows [82,124].As a concrete example, Figure 1 illustrates how a prompt from Southen resulted in an output resembling a shot from the trailer of Dune (2021).Notably, Midjourney's terms of service [87] highlight that users assume liability when requesting the model to generate content featuring copyrighted trademarks.This delegation of responsibility not only places the burden of infringement on users, but also diverts accountability from Midjourney's developers, who have openly admitted to using copyrighted trademarks without authorization [103].\n\nIn light of these developments, this survey aims to delve into the complex interplay between generative AI and protecting intellectual property (IP).Through synthesizing existing methods and legal analyses, we provide a comprehensive overview of the current landscape surrounding copyright in generative AI.To the best of our knowledge, this work presents the first thorough study on robust and applicable solutions to copyright issues in generative AI, which also combines contextual legal analysis for future consideration.The challenges and opportunities inherent in this burgeoning field offer insights that can inform policymakers, practitioners, and researchers alike when developing generative AI.",
            "score": 0.4686537019342872,
            "section_title": "INTRODUCTION",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 144
                },
                {
                    "start": 144,
                    "end": 337
                },
                {
                    "start": 337,
                    "end": 558
                },
                {
                    "start": 558,
                    "end": 743
                },
                {
                    "start": 743,
                    "end": 956
                },
                {
                    "start": 956,
                    "end": 1154
                },
                {
                    "start": 1154,
                    "end": 1296
                },
                {
                    "start": 1296,
                    "end": 1457
                },
                {
                    "start": 1457,
                    "end": 1691
                },
                {
                    "start": 1693,
                    "end": 1842
                },
                {
                    "start": 1842,
                    "end": 2000
                },
                {
                    "start": 2000,
                    "end": 2218
                },
                {
                    "start": 2218,
                    "end": 2397
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.86572265625
        },
        {
            "corpus_id": "260416806",
            "title": "Talkin' 'Bout AI Generation: Copyright and the Generative-AI Supply Chain (The Short Version)",
            "text": "This essay is an attempt to work systematically through the copyright infringement analysis of the generative AI supply chain. Our goal is not to provide a definitive answer as to whether and when training or using a generative AI is infringing conduct. Rather, we aim to map the surprisingly large number of live copyright issues that generative AI raises, and to identify the key decision points at which the analysis forks in interesting ways. \n\nWe assume basic familiarity with generative AI systems and terminology. We will focus primarily on transformer-based large language models (LLMs) and on diffusion-based image models, but most of the analysis should apply to other architectures and other types of generated content. \n\nPart I provides a brief breakdown of the stages in what we call the \"generative AI supply chain. \" Part II, the main body of the essay, works through the copyright analysis.",
            "score": 0.46855682455073,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 126
                },
                {
                    "start": 127,
                    "end": 253
                },
                {
                    "start": 254,
                    "end": 446
                },
                {
                    "start": 449,
                    "end": 520
                },
                {
                    "start": 521,
                    "end": 730
                },
                {
                    "start": 733,
                    "end": 829
                },
                {
                    "start": 830,
                    "end": 906
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.841796875
        },
        {
            "corpus_id": "267400526",
            "title": "AIGC (Artificial Intelligence Generated Content) infringes the copyright of human artists",
            "text": "If the generated image is deemed \"substantially similar\" to the original work in expression, it may infringe upon the original work's \"reproduction right.\" Similarly, if the image forms a new word based on the original work while retaining its fundamental expression, it may infringe upon the original work's \"adaptation right.\" \n\nWith the widespread adoption of AIGC, numerous original authors have resorted to legal actions against commercial companies utilizing AIGC technology. This trend indicates a growing concern for copyright infringement incidents in AIGC. \n\nWhile considerable progress has been made, there is still a long way to go in finding a harmonious balance between AIGC development and artists' rights. Based on this, the study aims to contribute to this ongoing endeavor by undertaking more in-depth research and comprehensively understanding the issue at hand. \n\nAs AIGC continues to evolve and revolutionize creative industries, it is imperative to address copyright concerns and find suitable solutions that safeguard the intellectual property rights of artists while embracing the benefits offered by AIGC technology. This study is designed to bring more attention to the other side of AIGC than just the convenience it gets so that people can use this technology with respect for the fruits of other people's work and to protect the interests of more artists. By examining the literature on copyright issues in AIGC technology, we aim to delve into the intricacies of this field's challenges and potential solutions, providing valuable insights and guidance for academic research and practical applications in related domains.",
            "score": 0.4666966749236361,
            "section_title": "Introduction",
            "char_start_offset": 2378,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 155
                },
                {
                    "start": 156,
                    "end": 328
                },
                {
                    "start": 331,
                    "end": 481
                },
                {
                    "start": 482,
                    "end": 566
                },
                {
                    "start": 569,
                    "end": 721
                },
                {
                    "start": 722,
                    "end": 881
                },
                {
                    "start": 884,
                    "end": 1141
                },
                {
                    "start": 1142,
                    "end": 1384
                },
                {
                    "start": 1385,
                    "end": 1651
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.8759765625
        },
        {
            "corpus_id": "265352103",
            "title": "On Copyright Risks of Text-to-Image Diffusion Models",
            "text": "This section evaluates our method for detecting partial copyright infringements by computing the cosine similarity of CLIP-embeddings between image chunks. An image chunk is a specific rectangular image region from the original image. We apply this measurement to compare chunks in generated images with annotated target image chunks, and for contrast, assess the similarity between randomly chosen chunks and our annotated ones. Additionally, we assess similarity in images generated from random prompts with our target annotations. Results, as shown in Table 2, reveal that our identified chunks exhibit a high cosine similarity (approximately 0.9) with target annotations, compared to lower similarities Baseline 2 58.4 58.9 57.9 57.8 57.9 58.9 \n\nTable 2: Cosine similarity (multiplied by 100) between CLIP embeddings of target annotations and chunks in generated images. Ours denotes image chunks identified with copyright test on images generated using our prompts. Baseline 1 denotes randomly selected chunks on images generated using our prompts. Baseline 2 denotes randomly selected chunks on images generated using random prompts. Identified image chunks show significant similarity with the annotated image parts. Figure 9: Proportion of generated images with identified copyrighted content. For all models apart from Stable Diffusion XL (SD XL), our copyright test identifies that around 70% of generated images from our pipeline have at least one chunk containing copyrighted content. The slight decrease in copyrighted content in SD XL is due to the model's increase in ability to comprehend the nuances in our non-sensitive prompts, thus slightly reducing the adversarial property. Overall, it is clear that our non-sensitive prompts can effectively cause diffusion models to infringe copyright as even more than half of the images generated by SD XL contain copyrighted content. \n\nof about 0.7 and 0.6 for random chunks in generated and random images, respectively.",
            "score": 0.4662246655052916,
            "section_title": "Evaluation on Copyright Test",
            "char_start_offset": 26229,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 155
                },
                {
                    "start": 156,
                    "end": 234
                },
                {
                    "start": 235,
                    "end": 429
                },
                {
                    "start": 430,
                    "end": 533
                },
                {
                    "start": 534,
                    "end": 747
                },
                {
                    "start": 750,
                    "end": 874
                },
                {
                    "start": 875,
                    "end": 970
                },
                {
                    "start": 971,
                    "end": 1053
                },
                {
                    "start": 1054,
                    "end": 1139
                },
                {
                    "start": 1140,
                    "end": 1223
                },
                {
                    "start": 1224,
                    "end": 1301
                },
                {
                    "start": 1302,
                    "end": 1496
                },
                {
                    "start": 1497,
                    "end": 1695
                },
                {
                    "start": 1696,
                    "end": 1893
                },
                {
                    "start": 1896,
                    "end": 1980
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.99072265625
        },
        {
            "corpus_id": "277502009",
            "title": "Who Owns the Output? Bridging Law and Technology in LLMs Attribution",
            "text": "They asserted that Midjourney, the AI program, was the actual author of the \"visual material,\" rather than Kashtanova. Following this, in March 2023, the Copyright Office released new guidance, stating that when AI determines the expressive elements of its output, the resulting material cannot be considered the product of human authorship. This guidance represents a significant clarification in the ongoing discussion regarding copyright and the role of artificial intelligence in creative processes. \n\nIn general, AI systems undergo a training process that enables them to generate various forms of artistic expression, including literature and visual art. This process involves feeding the AI large datasets comprised of text, images, and other content acquired from the vast resources available on the internet. During training, the AI program creates digital copies of existing works, a necessary step for analyzing the vast amount of information it receives. The U.S. Patent and Trademark Office has noted that this training inherently involves reproducing either complete works or significant portions of those works. For instance, OpenAI has publicly acknowledged that its programs are trained on extensive datasets that include copyrighted materials. Specifically, they emphasize that the training process begins with making copies of the data intended for analysis. While OpenAI has recently introduced an option to exclude certain images from being used in the training of future image generation models, it's important to recognize that the act of creating these copies without obtaining permission may violate the exclusive rights of copyright holders to reproduce and distribute their creative works. This raises important questions about copyright and intellectual property in the context of advancing AI technology. \n\nThe critical question surrounding the use of generative AI is who should be held accountable if its outputs violate the copyrights of existing works. Current legal frameworks suggest that both the individual using the AI and the company that developed the technology could be deemed responsible for such infringements. \n\nFor instance, if a user is found to be directly liable for copyright infringement by generating content that illegally replicates protected material, the AI company might also be implicated. This potential liability arises from the legal principle known as \"vicarious infringement.\" Under this doctrine, a party can be held liable if they have the authority and capability to oversee the infringing actions and if they also have a direct financial stake in the activities occurring through their technology.",
            "score": 0.4660770238312177,
            "section_title": "The US Approach",
            "char_start_offset": 15218,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 118
                },
                {
                    "start": 119,
                    "end": 341
                },
                {
                    "start": 342,
                    "end": 503
                },
                {
                    "start": 506,
                    "end": 660
                },
                {
                    "start": 661,
                    "end": 817
                },
                {
                    "start": 818,
                    "end": 966
                },
                {
                    "start": 967,
                    "end": 1126
                },
                {
                    "start": 1127,
                    "end": 1261
                },
                {
                    "start": 1262,
                    "end": 1377
                },
                {
                    "start": 1378,
                    "end": 1716
                },
                {
                    "start": 1717,
                    "end": 1833
                },
                {
                    "start": 1836,
                    "end": 1985
                },
                {
                    "start": 1986,
                    "end": 2154
                },
                {
                    "start": 2157,
                    "end": 2347
                },
                {
                    "start": 2348,
                    "end": 2439
                },
                {
                    "start": 2440,
                    "end": 2664
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.9501953125
        },
        {
            "corpus_id": "259064265",
            "title": "DeepfakeArt Challenge: A Benchmark Dataset for Generative AI Art Forgery and Data Poisoning Detection",
            "text": "Based on the aforementioned definitions of copyright infringement and adversarial data poisoning in the context of generative AI, the proposed DeepfakeArt Challenge benchmark dataset consists of more than 32,000 records generated using the following generative forgery and data poisoning methods: 1) Inpainting, 2) Style Transfer, 3) Adversarial data poisoning, and 4) Cutmix. These were generated by modifying source images from the WikiArt dataset Saleh and Elgammal [2015] to produce forgery images. This transformation process is based on four main principles, each representing a different form of copyright violation: 1) The usage of partial image data from the source, with the remaining portion inpainted. 2) The preservation of original image edge strokes, but with a modification in the painting style. 3) The introduction of minor noise into the image. 4) The replication, either partially or entirely, of another image. \n\nTo demonstrate these types of copyright violations, we have established four categories within our dataset: \n\n\u2022 Inpainting: Using Stable Diffusion 2 Rombach et al. [2022], we inpainted parts of the source image. \n\nIn each instance, a source image was randomly chosen from the WikiArt dataset and then inpainted to create the forgery images. The prompt used for generating the inpainting image was: \"generate a painting compatible with the rest of the image\". \n\nThree different masking techniques were used: 1-Side masking 2-Diagonal masking 3-Random masking. Figure 2(left) presents examples of the source, mask, and the resulting inpainted images. and Agrawala [2023] to alter the style of the source image, utilizing Canny edge detection. To guide the generation of style-transferred images, we utilized four distinct prompts, each corresponding to a different style: 1) \"a high-quality, detailed, realistic image\", 2) \"a high-quality, detailed, cartoon-style drawing\", 3) \"a high-quality, detailed, oil painting\", and 4) \"a high-quality, detailed, pencil drawing\".",
            "score": 0.4660567983066532,
            "section_title": "Methodology",
            "char_start_offset": 10155,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 376
                },
                {
                    "start": 377,
                    "end": 502
                },
                {
                    "start": 503,
                    "end": 713
                },
                {
                    "start": 714,
                    "end": 812
                },
                {
                    "start": 813,
                    "end": 863
                },
                {
                    "start": 864,
                    "end": 931
                },
                {
                    "start": 934,
                    "end": 1041
                },
                {
                    "start": 1044,
                    "end": 1145
                },
                {
                    "start": 1148,
                    "end": 1274
                },
                {
                    "start": 1275,
                    "end": 1392
                },
                {
                    "start": 1395,
                    "end": 1492
                },
                {
                    "start": 1493,
                    "end": 1582
                },
                {
                    "start": 1583,
                    "end": 1674
                },
                {
                    "start": 1675,
                    "end": 2001
                }
            ],
            "ref_mentions": [
                {
                    "start": 1083,
                    "end": 1104,
                    "matchedPaperCorpusId": "245335280"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.865234375
        },
        {
            "corpus_id": "277955485",
            "title": "TWIG: Two-Step Image Generation using Segmentation Masks in Diffusion Models",
            "text": "In this research, we prioritized mitigating copyright concerns stemming from the relationship between real-world images in our datasets and the images generated by our model. While our proposed methodology effectively reduces the risk of copyright infringement, the process of generating prompts for each image is labor-intensive and inefficient when applied to large datasets. To address this limitation, our future research will focus on developing a self-automated prompt-generation mechanism. This mechanism will analyze the context of an image to create relevant prompts, enabling the generation of higher-quality images that better capture the essence of the original or provided images. By automating this crucial step, we aim to enhance the scalability and practicality of our approach for real-world applications. This advancement will not only streamline the image generation workflow but also contribute to producing more creative and contextually appropriate outputs, further reducing the potential for copyright issues.",
            "score": 0.4656210809807037,
            "section_title": "Limitations and Future Work",
            "char_start_offset": 25058,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 174
                },
                {
                    "start": 175,
                    "end": 377
                },
                {
                    "start": 378,
                    "end": 496
                },
                {
                    "start": 497,
                    "end": 693
                },
                {
                    "start": 694,
                    "end": 822
                },
                {
                    "start": 823,
                    "end": 1032
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.97216796875
        },
        {
            "corpus_id": "265352103",
            "title": "On Copyright Risks of Text-to-Image Diffusion Models",
            "text": "Hence, the objective of our data generation pipeline is to systematically create non-sensitive adversarial prompts. We discuss our proposed data generation pipeline in Section 4. \n\nCopyright test for substantial similarities. According to the legal definition of copyright infringement, copyright violations can appear across a broader spectrum, in which generated images are not entirely replicated from any training source, yet they possess substantial similarities that replicate or bear significant resemblance to copyrighted content. We refer to these copyright violations as partial copyright violations. In Definition 2, we apply D[\u2022||\u2022] to measure the substantial similarities between generated images and copyrighted images. We propose an implementation of D[\u2022||\u2022] as a copyright tester in Section 5.",
            "score": 0.4644012753453807,
            "section_title": "Problem Formulation",
            "char_start_offset": 8644,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 115
                },
                {
                    "start": 116,
                    "end": 178
                },
                {
                    "start": 181,
                    "end": 225
                },
                {
                    "start": 226,
                    "end": 538
                },
                {
                    "start": 539,
                    "end": 610
                },
                {
                    "start": 611,
                    "end": 733
                },
                {
                    "start": 734,
                    "end": 809
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.98193359375
        },
        {
            "corpus_id": "260416806",
            "title": "Talkin' 'Bout AI Generation: Copyright and the Generative-AI Supply Chain (The Short Version)",
            "text": "No viewer would say that the model has the same \"total concept and feel\" as a painting; no reader would say that it is substantially similar to a blog post; and so on. There is therefore a colorable argument that a model is not a copy of any of the works it was trained on. Although the model could be prompted to generate a substantially similar work, it would be the generation that infringes, and not the model. This is an interesting argument, and we will return to it below. But there is a persuasive counter-argument that a model is a copy, because the Copyright Act does not require that copies be directly human-intelligible. After all, a Blu-Ray is not directly intelligible by humans, either, but it counts as a \"copy\" of the movie on it. Indeed, all digital copies are unintelligible. Instead, they are objects \"from which the work can be perceived, reproduced, or otherwise communicated \u2026 with the aid of a machine or device. \" 20 Thus, even if a model is uninterpretable, it might still be possible to \"perceive[]\" or \"reproduce[]\" a copyrighted work embedded in its weights through suitable prompting. 21 n this view, the necessary condition for a model to count as a substantially similar copy of a work is that the model is capable of generating that work as an output. Note that this is direct infringement, not secondary. The theory is not that the generation is an infringing copy and the model is a tool in causing that infringement in the way that a tape-duplicating machine might be a tool in making infringing cassettes. 22 The theory is that the model itself is an infringing copy, regardless of whether that particular generation is ever made. (Alert readers will note the similarity to the debate over whether the mere act of making it available without a download infringes the distribution right. 23 ) \n\nAnother perspective is that a model is a compilation of (at least some of the) works that it was trained on. If so, then it might be a copy irrespective of whether the model can generate a specific work when used in the normal way. That is because there might be other ways of inspecting the model that are capable of recovering training data.",
            "score": 0.46382593980666403,
            "section_title": "B. Substantial Similarity",
            "char_start_offset": 21967,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 167
                },
                {
                    "start": 168,
                    "end": 273
                },
                {
                    "start": 274,
                    "end": 414
                },
                {
                    "start": 415,
                    "end": 479
                },
                {
                    "start": 480,
                    "end": 633
                },
                {
                    "start": 634,
                    "end": 748
                },
                {
                    "start": 749,
                    "end": 795
                },
                {
                    "start": 796,
                    "end": 937
                },
                {
                    "start": 938,
                    "end": 1118
                },
                {
                    "start": 1119,
                    "end": 1285
                },
                {
                    "start": 1286,
                    "end": 1339
                },
                {
                    "start": 1340,
                    "end": 1546
                },
                {
                    "start": 1547,
                    "end": 1668
                },
                {
                    "start": 1669,
                    "end": 1827
                },
                {
                    "start": 1828,
                    "end": 1829
                },
                {
                    "start": 1832,
                    "end": 1940
                },
                {
                    "start": 1941,
                    "end": 2063
                },
                {
                    "start": 2064,
                    "end": 2175
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.79052734375
        },
        {
            "corpus_id": "257771630",
            "title": "Foundation Models and Fair Use",
            "text": "The third commonly produced category of generative AI is image generation.\n\nComplexities of fair use with images. As with code or text data, it is unlikely that verbatim generation of images would yield a successful fair use defense. And others have found that it is possible in some circumstances to extract training data from image generation foundation models (Somepalli et al., 2022;Carlini et al., 2023). As Somepalli et al. (2022) and others note, however, as foundation models for image generation train on more data, they are less likely to output content similar to the training data on average. These cases are more likely to be fair use.\n\nBut generated images, and generated art in particular, have their own complexities when it comes to fair use, with sometimes conflicting outcomes. For example, in a recent case, a video game company used the likeness of a WWE wrestler in a video game. The wrestler had tattoos that the company faithfully replicated in the game. The tattoo artist sued for infringement and a jury determined that this was not covered by fair use (Alexander v. Take small portions of an image, like the tattoo on a player's arm, to trigger copyright problems that are not guaranteed a fair use defense. Consider the following hypothetical.\n\nHypothetical 2.5: Generate Me Video-Game Assets.\n\nOne direction for generative art is creating video game assets. There are already mechanisms to generate 3D models from text (Poole et al., 2022). Consider a situation where a video game company builds a machine learning model into their system that generates art on the fly within the game to populate a virtual world dynamically. The game is a hit, but artists begin to notice that their artwork shows up in the game with only slight modifications, for example on tattoos for video game characters. Is this fair use? While their lawsuit is not guaranteed to succeed, there is still some risk for the video game company if the outcome follows Alexander v. Take-Two Interactive Software, Inc. (S.D. Ill. 2020).\n\nStyle Transfer. What about more abstract scenarios, where art is generated in different styles? There are two components to this. First, let us consider the rights of the original image that is being transformed into a different style. Relevant is a case that was recently argued before the Supreme",
            "score": 0.463823478307547,
            "section_title": "Generated Images",
            "char_start_offset": 48067,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.93017578125
        },
        {
            "corpus_id": "266693839",
            "title": "Digger: Detecting Copyright Content Mis-usage in Large Language Model Training",
            "text": "Complications of AI Models Trained on Copyrighted Content. The training of artificial intelligence (AI) models, particularly LLMs, on copyrighted content has emerged as a complex issue straddling legal, ethical, and technological domains. The architecture of these AI models, especially those based on deep learning, requires the consumption of extensive datasets, often comprising copyrighted material like texts, images, or audio. This necessity raises intricate questions about traditional copyright law, as the self-generative nature of AI models makes it difficult to delineate how much of the model's outputs can be considered derivative of the original copyrighted works. High-profile legal cases such as Getty Images accessed via Stability AI [20] have brought these complexities to the forefront, demonstrating the legal risks of using copyrighted content in AI training without explicit consent. Generative AI models, capable of creating new content, add another layer of complexity. These models learn from the patterns and structures in their training data and can generate new data that may closely resemble copyrighted works. This poses significant legal and ethical challenges, including potential copyright infringement and the ethical implications of using copyrighted material without consent. The risks are particularly high when the generated content bears watermarks or other insignia, which could be interpreted as an admission of infringement. \n\nLimitations of Exisiting Mitigations. Various mitigation strategies have been proposed, ranging from legal solutions like securing explicit content use consent through contracts or licenses, to technological approaches like creating original datasets based on real-world measurements. Despite these mitigation strategies, challenges persist. Legal solutions often involve complex negotiations and may not fully address the ethical dimensions, such as potential cultural appropriation or commodification of creative works. Technological solutions, while bypassing the need for copyrighted content, require significant investments and may still fall short in capturing the richness of human-created content. In this work, we aim to explore the copyrighted content internalized within Large Language Models, seeking to understand how these models transform and potentially infringe upon copyrighted material.",
            "score": 0.46366537070400843,
            "section_title": "BACKGROUND",
            "char_start_offset": 5807,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 58
                },
                {
                    "start": 59,
                    "end": 238
                },
                {
                    "start": 239,
                    "end": 432
                },
                {
                    "start": 433,
                    "end": 678
                },
                {
                    "start": 679,
                    "end": 905
                },
                {
                    "start": 906,
                    "end": 993
                },
                {
                    "start": 994,
                    "end": 1139
                },
                {
                    "start": 1140,
                    "end": 1311
                },
                {
                    "start": 1312,
                    "end": 1466
                },
                {
                    "start": 1469,
                    "end": 1506
                },
                {
                    "start": 1507,
                    "end": 1753
                },
                {
                    "start": 1754,
                    "end": 1810
                },
                {
                    "start": 1811,
                    "end": 1990
                },
                {
                    "start": 1991,
                    "end": 2174
                },
                {
                    "start": 2175,
                    "end": 2374
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.80419921875
        },
        {
            "corpus_id": "265352103",
            "title": "On Copyright Risks of Text-to-Image Diffusion Models",
            "text": "(1) We form a framework to create prompts for T2I tasks that are generic in language semantics but can still trigger partial copyright infringements in image generation by various diffusion models. (2) We introduce a copyright tester that employs attention maps to identify significant similarities, extending the analysis from entire image duplication to specific visual feature resemblances and pinpointing potential areas of interest for detailed examination. \n\n(3) We compile a dataset of potential copyrighted topics and prompts to aid in more realistic copyright research and to analyze diffusion model behaviors. We provide empirical results to show the copyright threat and to raise awareness in copyright research for generative models. A case study utilizing our dataset revealed a significant concern: recent diffusion models is prone to unintentionally produce images with copyrighted content from seemingly unrelated prompts. \n\n2 Background Diffusion models. Diffusion models are a class of generative models that model the diffusion process. In the diffusion process, source data is gradually distorted by adding noise to it until the source data becomes noise as well [Sohl-Dickstein et al., 2015]. The objective of diffusion models is to learn the reverse process of diffusion, which tries to reconstruct the target given noisy input. Diffusion models can either learn to directly predict the less noisy data at each reverse step, or learn to predict the noise at each step, then denoise the data using predicted noise [Ho et al., 2020;Saharia et al., 2022]. Earlier diffusion models work at image level and try to directly reconstruct images from noise. However, reverse steps at image level require intensive calculation and constraints the speed of the reconstruction. Instead, [Rombach et al., 2022] proposes to first transform the image into low dimensional hidden space, then apply diffusion models at hidden level. Subsequently, latent diffusion models are much faster than their counterparts working at image level, hence can be trained on large datasets such as LAION [Schuhmann et al., 2022]. Prediction of noise or previous states in the reverse process usually uses a U-Net [Ronneberger et al., 2015].",
            "score": 0.4634600029415141,
            "section_title": "Introduction",
            "char_start_offset": 2271,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 197
                },
                {
                    "start": 198,
                    "end": 462
                },
                {
                    "start": 465,
                    "end": 619
                },
                {
                    "start": 620,
                    "end": 745
                },
                {
                    "start": 746,
                    "end": 938
                },
                {
                    "start": 941,
                    "end": 971
                },
                {
                    "start": 972,
                    "end": 1055
                },
                {
                    "start": 1056,
                    "end": 1213
                },
                {
                    "start": 1214,
                    "end": 1350
                },
                {
                    "start": 1351,
                    "end": 1574
                },
                {
                    "start": 1575,
                    "end": 1670
                },
                {
                    "start": 1671,
                    "end": 1787
                },
                {
                    "start": 1788,
                    "end": 1937
                },
                {
                    "start": 1938,
                    "end": 2118
                },
                {
                    "start": 2119,
                    "end": 2229
                }
            ],
            "ref_mentions": [
                {
                    "start": 1183,
                    "end": 1212,
                    "matchedPaperCorpusId": "254366634"
                },
                {
                    "start": 1535,
                    "end": 1552,
                    "matchedPaperCorpusId": "257687839"
                },
                {
                    "start": 1552,
                    "end": 1573,
                    "matchedPaperCorpusId": "252917726"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.99169921875
        },
        {
            "corpus_id": "278534729",
            "title": "DFA-CON: A Contrastive Learning Approach for Detecting Copyright Infringement in DeepFake Art",
            "text": "The growing availability of generative AI tools for visual content creation has raised critical concerns around copyright infringement, especially in the domain of visual artworks [1]. Generative models trained on large-scale, web-scraped datasets often absorb patterns from both copyrighted and public domain images, making them prone to reproducing unauthorized content [2]. This phenomenon is particularly problematic in the context of AI-generated art, where stylistic and structural similarities to original artworks may constitute legal or ethical violations [3]. Given the ease with which forged or derivative artworks can be created and distributed, there is a compelling need for automated methods to assess the originality and attribution of generative outputs. \n\nTo study and benchmark the detection of AI-generated art forgeries, the recently proposed DeepfakeArt Challenge [4] provides a comprehensive dataset comprising over 32,000 image pairs spanning a variety of generative manipulation techniques. Each entry in the dataset consists of a pair of images-either a forged/generated version of an original artwork or two dissimilar, unrelated images. The manipulated images cover several attack types including inpainting, style transfer, adversarial perturbation, and cutmix, simulating realistic scenarios of content misuse. This dataset enables the development of algorithms that go beyond pixel-level artifact detection, focusing instead on semantic similarity and visual attribution, a crucial capability when detecting copyright violations in generative art. \n\nTo address this problem, we propose DFA-CON, a supervised contrastive learning framework designed to detect copyright-infringing or forged art generated by AI models. Rather than treating forgery detection as a traditional classification problem, DFA-CON learns an embedding space that encodes semantic similarity between original and manipulated images. Using supervised contrastive loss, the model is trained to pose affinity among original artworks and their forged counterparts, while distancing unrelated images within the same batch. This representation-level formulation allows for greater generalization across manipulation types and better discrimination of subtle, high-quality forgeries, which are common in the domain of AI-generated art. \n\nOur main contributions are summarized as follows: \n\n\u2022 We propose a supervised contrastive learning framework, DFA-CON, for training visual encoders to detect copyright infringement in DeepFake art.",
            "score": 0.4633429615380903,
            "section_title": "INTRODUCTION",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 184
                },
                {
                    "start": 185,
                    "end": 376
                },
                {
                    "start": 377,
                    "end": 569
                },
                {
                    "start": 570,
                    "end": 771
                },
                {
                    "start": 774,
                    "end": 1015
                },
                {
                    "start": 1016,
                    "end": 1164
                },
                {
                    "start": 1165,
                    "end": 1340
                },
                {
                    "start": 1341,
                    "end": 1578
                },
                {
                    "start": 1581,
                    "end": 1747
                },
                {
                    "start": 1748,
                    "end": 1935
                },
                {
                    "start": 1936,
                    "end": 2120
                },
                {
                    "start": 2121,
                    "end": 2331
                },
                {
                    "start": 2334,
                    "end": 2383
                },
                {
                    "start": 2386,
                    "end": 2531
                }
            ],
            "ref_mentions": [
                {
                    "start": 180,
                    "end": 183,
                    "matchedPaperCorpusId": "261279983"
                },
                {
                    "start": 372,
                    "end": 375,
                    "matchedPaperCorpusId": "235359053"
                },
                {
                    "start": 565,
                    "end": 568,
                    "matchedPaperCorpusId": "254366634"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.99072265625
        },
        {
            "corpus_id": "265551515",
            "title": "VA3: Virtually Assured Amplification Attack on Probabilistic Copyright Protection for Text-to-Image Generative Models",
            "text": "The booming use of text-to-image generative models has raised concerns about their high risk of producing copyright-infringing content. While probabilistic copyright protection methods provide a probabilistic guarantee against such infringement, in this paper, we introduce Virtually Assured Amplification Attack (VA3), a novel online attack framework that exposes the vulnerabilities of these protection mechanisms. The proposed framework significantly amplifies the probability of generating infringing content on the sustained interactions with generative models and a non-trivial lower-bound on the success probability of each engagement. Our theoretical and experimental results demonstrate the effectiveness of our approach under various scenarios. These findings highlight the potential risk of implementing probabilistic copyright protection in practical applications of text-to-image generative models. Code is available at https://github.com/South7X/VA3.",
            "score": 0.46291025244155753,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.9775390625
        },
        {
            "corpus_id": "269137659",
            "title": "Rethinking Artistic Copyright Infringements in the Era of Text-to-Image Generative Models",
            "text": "As image generative models have rapidly improved in scale and sophistication, the possibility of them mimicking artists' personal styles has been an important topic of discussion in the literature [18].Many previous works describe ways to either detect potential direct image copying in generated images, or to foil any future copying attempts by imperceptibly altering the artists' works to prevent effective training by the generative models.These include techniques like adding imperceptible watermarks to copyrighted artworks [7,8,28], and crafting \"un-learnable\" examples on which models struggle to learn the style-relevant information [24,29,30].These methods are typically computationally expensive and incur a loss in image quality, which may render these techniques impractical for many artists.Also, they do not protect artworks which have been previously uploaded to the internet without any safeguards.Others have suggested methods to mitigate this issue from the model owner's perspective -to either de-duplicate the dataset before training [4,25,26], or to remove concepts from the model after training (\"unlearning\") [3,9,13].These are also technically challenging, and require the model owner to invest significant resources which may again inhibit their practicality.Methods like [4,25,26] are also more focused on analyzing direct image copying from the training data, and thus may not be applicable to preventing style copying.None of these works tackle the problem of detecting potentially copied art styles in generated art, especially in a manner which may be relevant to legal standards of copyright infringement.According to current US legal standards [2], an artwork has to meet the \"substantial similarity\" test for it to be infringing on copyright.This similarity has to be established on analytic and holistic terms Fig. 2: Example generations from Stable Diffusion 2 when prompted to produce specific paintings by Vincent Van Gogh, along with the histogram of similarities between the generated image and corresponding real image.Even for a famous artist like Vincent Van Gogh, generative models rarely produce near-exact duplicates.However, Van Gogh's style appears consistently, even when similarity is low.[11,14].",
            "score": 0.461057559628288,
            "section_title": "Related Works",
            "char_start_offset": 8141,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 202
                },
                {
                    "start": 202,
                    "end": 444
                },
                {
                    "start": 444,
                    "end": 653
                },
                {
                    "start": 653,
                    "end": 805
                },
                {
                    "start": 805,
                    "end": 915
                },
                {
                    "start": 915,
                    "end": 1142
                },
                {
                    "start": 1142,
                    "end": 1285
                },
                {
                    "start": 1285,
                    "end": 1447
                },
                {
                    "start": 1447,
                    "end": 1637
                },
                {
                    "start": 1637,
                    "end": 1776
                },
                {
                    "start": 1776,
                    "end": 2060
                },
                {
                    "start": 2060,
                    "end": 2163
                },
                {
                    "start": 2163,
                    "end": 2239
                },
                {
                    "start": 2239,
                    "end": 2247
                }
            ],
            "ref_mentions": [
                {
                    "start": 1061,
                    "end": 1064,
                    "matchedPaperCorpusId": "258987384"
                },
                {
                    "start": 1304,
                    "end": 1307,
                    "matchedPaperCorpusId": "258987384"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.99267578125
        },
        {
            "corpus_id": "271956731",
            "title": "Randomization Techniques to Mitigate the Risk of Copyright Infringement",
            "text": "Modern machine learning relies heavily on large amounts of high-quality training data, primarily obtained from the Internet. Inevitably, these large-scale datasets contain some copyrighted material. When models are trained on this copyrighted data, they can accidentally generate outputs that closely resemble the training data, leading to potential copyright infringement. For example, despite recent advancements in foundational models (Bommasani et al., 2021), studies have shown that these models can easily memorize substantial portions of their training data (Carlini et al., 2021(Carlini et al., , 2023)). This immediately leads to the following questions: How do we define copyright infringement for models trained on potentially copyrighted data? How can one claim that a model violates copyright laws? And how can we prevent the models from generating outputs that resemble significantly copyrighted data? Copyright laws aim to promote creativity while protecting the rights, often economically, of original works. Note that copyright infringement class-action cases can be lucrative, which accounts for their popularity compared to, for instance, privacy violation cases. Under the fair use doctrine (Office, 2022), reproducing copyrighted work can be considered fair use based on four factors: the purpose of the use, the nature of the work, the amount of similarity, and potential harm. While purpose, nature, and harm are outside the scope of engineering solutions (see, for example, Sag (2018); Sobel (2017) for a discussion on whether data mining and machine learning on copyrighted text falls under \"fair use\"), substantial similarity can potentially be addressed with technology. \n\nProducing outputs substantially similar to copyrighted work on which a foundation model is trained can be a key factor in determining whether it constitutes fair use. Current ongoing lawsuits that do not demonstrate that foundation models generate substantially similar works are likely to be dismissed. Determining the amount and substantiality of the portion of the generated text in relation to the copyrighted work is subjective and ambiguous as courts look at both quantity and quality. Fair use is less likely to be found if the use includes a large portion of the copyrighted work. However, some courts have found the use of an entire work to be fair under certain circumstances.",
            "score": 0.4605677706215411,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 124
                },
                {
                    "start": 125,
                    "end": 198
                },
                {
                    "start": 199,
                    "end": 373
                },
                {
                    "start": 374,
                    "end": 612
                },
                {
                    "start": 613,
                    "end": 755
                },
                {
                    "start": 756,
                    "end": 811
                },
                {
                    "start": 812,
                    "end": 915
                },
                {
                    "start": 916,
                    "end": 1024
                },
                {
                    "start": 1025,
                    "end": 1182
                },
                {
                    "start": 1183,
                    "end": 1399
                },
                {
                    "start": 1400,
                    "end": 1697
                },
                {
                    "start": 1700,
                    "end": 1866
                },
                {
                    "start": 1867,
                    "end": 2003
                },
                {
                    "start": 2004,
                    "end": 2191
                },
                {
                    "start": 2192,
                    "end": 2288
                },
                {
                    "start": 2289,
                    "end": 2386
                }
            ],
            "ref_mentions": [
                {
                    "start": 565,
                    "end": 586,
                    "matchedPaperCorpusId": "229156229"
                },
                {
                    "start": 586,
                    "end": 611,
                    "matchedPaperCorpusId": "256389993"
                },
                {
                    "start": 1498,
                    "end": 1508,
                    "matchedPaperCorpusId": "86424284"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.7451171875
        },
        {
            "corpus_id": "266900037",
            "title": "The Stronger the Diffusion Model, the Easier the Backdoor: Data Poisoning to Induce Copyright Breaches Without Adjusting Finetuning Pipeline",
            "text": "Copyright Infringement Attack Scenario. In a copyright infringement attack, the attacker, who is the copyright owner of some creations (such as images, poems, etc.), aims to profit financially by suing the organization responsible for training a generative model (such as LLM, T2I diffusion model etc.) for copyright infringement. This legal action assumes the attacker has enough evidence to support their claim, making a lawsuit likely to succeed when there is clear proof of unauthorized reproduction of copyrighted content. A real-world example illustrating this scenario is the lawsuit filed by Getty Images against the AI art generator Stable Diffusion in the United States for copyright infringement (Vincent, 2023). \n\nTo further study this, we consider a specific scenario where the victim is the organization that trains text-to-image diffusion models. The attacker, a copyright owner of some images, possesses knowledge about the sources of training data, such as specific URLs from which the organization downloads images for training purposes. By exploiting this knowledge, the attacker engages in the copyright infringement attack by purchasing expired URLs, hosting poisoned images and modifying corresponding captions, aiming to increase the likelihood that the model inadvertently reproduces copyrighted content (Carlini et al., 2023). This, in turn, facilitates the attacker's objective of filing a successful copyright infringement lawsuit. To this end, the attacker is motivated to: \n\n\u2022 Perform the attack in stealth to avoid detection by the organization, preventing the organization from identifying and mitigating the model's vulnerability to attack before it is released and commercialized. \n\n\u2022 Select an image from which the attacker owns the copyright that is suitable for the attack method, to ensure the targeted diffusion model breaches copyright, such as one that are easily decomposable and recognizable by the model. \n\n\u2022 Try various prompts to cause the diffusion model to specifically reproduce the copyrighted image, and use the reproduced image as evidence in their lawsuit. \n\nDefining Copyright Infringement Attack. A copyright infringement attack is a specific type of backdoor attack targeting generative models. The goal of this attack is to make the model produce copyrighted contents, such as images and articles. In this work, we consider the specific setting: I. \n\nThe target model is a text-to-image diffusion model that has not been pretrained on copyrighted images, and II.",
            "score": 0.4605457788845669,
            "section_title": "Copyright Infringement Attack",
            "char_start_offset": 6647,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 39
                },
                {
                    "start": 40,
                    "end": 330
                },
                {
                    "start": 331,
                    "end": 527
                },
                {
                    "start": 528,
                    "end": 723
                },
                {
                    "start": 726,
                    "end": 861
                },
                {
                    "start": 862,
                    "end": 1055
                },
                {
                    "start": 1056,
                    "end": 1351
                },
                {
                    "start": 1352,
                    "end": 1458
                },
                {
                    "start": 1459,
                    "end": 1501
                },
                {
                    "start": 1504,
                    "end": 1713
                },
                {
                    "start": 1716,
                    "end": 1947
                },
                {
                    "start": 1950,
                    "end": 2108
                },
                {
                    "start": 2111,
                    "end": 2150
                },
                {
                    "start": 2151,
                    "end": 2249
                },
                {
                    "start": 2250,
                    "end": 2353
                },
                {
                    "start": 2354,
                    "end": 2404
                },
                {
                    "start": 2407,
                    "end": 2518
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.98828125
        },
        {
            "corpus_id": "265351912",
            "title": "CopyScope: Model-level Copyright Infringement Quantification in the Diffusion Workflow",
            "text": "[8], Midjourney [1], are setting off a new revolution of artwork creation. These programs allow users to effortlessly generate target images by taking some descriptions as input into the model. However, these AI-generated artworks inherit the characteristics of the images that are used to train models [33,40,43], which might be pretty similar to the original ones, as shown in Figure 1. Such similarity has aroused concerns about copyright infringement disputes. For example, three artists (Sarah Andersen, Kelly McKernan, and Karla Ortiz) have recently accused Stable Diffusion of unlawfully  2) was created by Stable Diffusion using the \"style of Erin Hanson\" as a prompt. The styles of these two images are so similar that it is impossible to tell them apart. \n\nscraping copyrighted images from the Internet to mimic their art styles [11]. To this end, research on responsible AI image generation is in urgent need to address such copyright issues. \n\nPrevious work on data attribution [29,31,32,42] focused on how the images in the training data contributed to the model's outcome, which is not suitable for the context of copyright traceability. This is because of the following reasons: (1) the training data is not known in advance in real-world practices; as shown in Figure 2, the models are usually publicly available, while the training datasets are not [22,43]. (2) the responsible party is the model provider, namely, the personnel or the organization who abused the online image collections without the owners' consent, not the image itself. As long as the model that generates the infringing image is identified, the corresponding infringer (the model provider) can be found, and the degree of infringement can be quantified. Thus, we need to develop a new approach to quantify copyright infringement from the model level, which is the focus of our paper. \n\nTo this end, we propose a new framework CopyScope at the model level towards AIGC copyright traceability. Our framework CopyScope includes three closely intertwined stages (Identify-Quantify-Evaluate).",
            "score": 0.46017829201599986,
            "section_title": "AI image generation programs, namely Artificial Intelligence Generated Content (AIGC) tools such as Stable Diffusion [20], DALL\u2022E2",
            "char_start_offset": 147,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 74
                },
                {
                    "start": 75,
                    "end": 193
                },
                {
                    "start": 194,
                    "end": 388
                },
                {
                    "start": 389,
                    "end": 464
                },
                {
                    "start": 465,
                    "end": 676
                },
                {
                    "start": 677,
                    "end": 764
                },
                {
                    "start": 767,
                    "end": 844
                },
                {
                    "start": 845,
                    "end": 953
                },
                {
                    "start": 956,
                    "end": 1151
                },
                {
                    "start": 1152,
                    "end": 1374
                },
                {
                    "start": 1375,
                    "end": 1556
                },
                {
                    "start": 1557,
                    "end": 1741
                },
                {
                    "start": 1742,
                    "end": 1871
                },
                {
                    "start": 1874,
                    "end": 1979
                },
                {
                    "start": 1980,
                    "end": 2075
                }
            ],
            "ref_mentions": [
                {
                    "start": 990,
                    "end": 994,
                    "matchedPaperCorpusId": "235719065"
                },
                {
                    "start": 997,
                    "end": 1000,
                    "matchedPaperCorpusId": "249947012"
                },
                {
                    "start": 1000,
                    "end": 1003,
                    "matchedPaperCorpusId": "249319573"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.99609375
        },
        {
            "corpus_id": "271135364",
            "title": "Mind the Ownership Gap? Copyright in AI-generated Language Data",
            "text": "A group of 17 authors, including John Grisham and George R. R. Martin, went as far as to sue Open AI for \"systematic theft on a mass scale\" (Italie, 2023).As a matter of fact, there are a number of other lawsuits brought by authors against AI companies (see, Setty, 2023) or content producers accused of using AI techniques (Khalid, 2024).According to recent media reports, a Chinese court found a provider of an AI text-toimage tool guilty of copyright infringement; the tool (when prompted accordingly) generated images of Ultraman, a popular cartoon character, that were substantially similar to the original artwork (Costigan, 2024).\n\nThe opinion according to which AI-generated outputs are in fact infringing copyright in the data used to train the underlying model remains to be tested by European and US courts, and rightholders have so far struggled to consistently identify outputs which bear a resemblance to items of training data without wilfully contriving circumstances intended to create such resemblance 16 .\n\nFinally, it has also happened that, for fear of a successful copyright infringement lawsuit, platforms removed AI-generated content when pressured by rightholders (Snapes, 2023).Such content was, therefore, assumed to infringe copyright.",
            "score": 0.46015259603416947,
            "section_title": "Grey areas related to copyright protection of AI outputs",
            "char_start_offset": 18513,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 155
                },
                {
                    "start": 155,
                    "end": 339
                },
                {
                    "start": 339,
                    "end": 637
                },
                {
                    "start": 639,
                    "end": 1024
                },
                {
                    "start": 1026,
                    "end": 1204
                },
                {
                    "start": 1204,
                    "end": 1263
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.6787109375
        },
        {
            "corpus_id": "270063121",
            "title": "Protect-Your-IP: Scalable Source-Tracing and Attribution against Personalized Generation",
            "text": "The AI-generated content (AIGC) model, especially the personalized generation model [55], has adverse implications on the copyright and intellectual property (IP) of various visual content such as artworks by artists, portraits of individuals, and photographs by photographers.Images generated through personalized methods may propagate misinformation and infringe upon copyrights, thereby engendering negative societal repercussions.In response to these concerns, significant research efforts have been directed towards AIGC copyright watermarking for images [37,27,54].Several methodologies, including those employing box-free watermarking techniques [50,43,51,19,35,44,28,38], have been developed to trace back the fine-tuning of generative models like Generative Adversarial Network (GAN) [12,20,13,4] and Diffusion models [17,33,31,7,34], thereby serving a post-hoc protective function.Cool photo!I will input them into personalized generation models and generate images, then upload online.\n\nI see these images on Internet, and I doubt that they are generated via my portraits.\n\nIt is AIGC-generated.I detect your copyright in this image, and it is generated using InstantID.\n\nThere is a new kind of personalized generation model, and I can generate new images via this model.\n\nI can scale my attributor for new methods, and it is generated using PhotoMaker.\n\nCopyright Info",
            "score": 0.45955898622431823,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 277
                },
                {
                    "start": 277,
                    "end": 434
                },
                {
                    "start": 434,
                    "end": 571
                },
                {
                    "start": 571,
                    "end": 891
                },
                {
                    "start": 891,
                    "end": 902
                },
                {
                    "start": 902,
                    "end": 996
                },
                {
                    "start": 998,
                    "end": 1083
                },
                {
                    "start": 1085,
                    "end": 1106
                },
                {
                    "start": 1106,
                    "end": 1181
                },
                {
                    "start": 1183,
                    "end": 1282
                },
                {
                    "start": 1284,
                    "end": 1364
                },
                {
                    "start": 1366,
                    "end": 1380
                }
            ],
            "ref_mentions": [
                {
                    "start": 560,
                    "end": 564,
                    "matchedPaperCorpusId": "257766375"
                },
                {
                    "start": 657,
                    "end": 660,
                    "matchedPaperCorpusId": "229718542"
                },
                {
                    "start": 660,
                    "end": 663,
                    "matchedPaperCorpusId": "232148007"
                },
                {
                    "start": 663,
                    "end": 666,
                    "matchedPaperCorpusId": "260334882"
                },
                {
                    "start": 666,
                    "end": 669,
                    "matchedPaperCorpusId": "264492075"
                },
                {
                    "start": 669,
                    "end": 672,
                    "matchedPaperCorpusId": "258588430"
                },
                {
                    "start": 675,
                    "end": 678,
                    "matchedPaperCorpusId": "268692714"
                },
                {
                    "start": 793,
                    "end": 797,
                    "matchedPaperCorpusId": "1033682"
                },
                {
                    "start": 800,
                    "end": 803,
                    "matchedPaperCorpusId": "10894094"
                },
                {
                    "start": 827,
                    "end": 831,
                    "matchedPaperCorpusId": "219955663"
                },
                {
                    "start": 831,
                    "end": 834,
                    "matchedPaperCorpusId": "222140788"
                },
                {
                    "start": 834,
                    "end": 837,
                    "matchedPaperCorpusId": "245335280"
                },
                {
                    "start": 837,
                    "end": 839,
                    "matchedPaperCorpusId": "234357997"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.96435546875
        },
        {
            "corpus_id": "276721767",
            "title": "Towards a World Wide Web powered by generative AI",
            "text": "Our analysis shows that AI-generated images are highly realistic, making it difficult for users to distinguish between real and AI-generated content (see Fig. 5). While this is beneficial to the performance of a system adopting such technology, it can lead, for example, to the spread of misinformation if inappropriate images are generated. Similarly, AI models can inadvertently embed biases, which can also be dangerous for end-users 28 . It follows that websites or web browsers adopting AI-generated images need to disclose such behavior. For example, a browser or a website could add a watermark on images when AI-generated. \n\nA second concern is copyright infringement. AI systems rely on large datasets, which may contain copyrighted material. Entities implementing this technology must ensure that their training data complies with copyright laws. Note that enforcing such copyright laws is a challenging task, as determining the originality and ownership of AI-generated content can be complex. This issue diverges from the focus of this paper and has been explored in recent research efforts such as Glaze 58 . Glaze enables artists to apply \"style cloaks\" to their artwork, introducing subtly imperceptible perturbations. When integrated into training datasets, these cloaks can deceive generative models attempting to replicate a particular artist's style.",
            "score": 0.45913516403689564,
            "section_title": "Copyright and ethical concerns",
            "char_start_offset": 38467,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 162
                },
                {
                    "start": 163,
                    "end": 341
                },
                {
                    "start": 342,
                    "end": 441
                },
                {
                    "start": 442,
                    "end": 543
                },
                {
                    "start": 544,
                    "end": 630
                },
                {
                    "start": 633,
                    "end": 676
                },
                {
                    "start": 677,
                    "end": 751
                },
                {
                    "start": 752,
                    "end": 856
                },
                {
                    "start": 857,
                    "end": 1004
                },
                {
                    "start": 1005,
                    "end": 1121
                },
                {
                    "start": 1122,
                    "end": 1233
                },
                {
                    "start": 1234,
                    "end": 1369
                }
            ],
            "ref_mentions": [
                {
                    "start": 1117,
                    "end": 1119,
                    "matchedPaperCorpusId": "256662278"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.8623046875
        },
        {
            "corpus_id": "268532352",
            "title": "A Dataset and Benchmark for Copyright Infringement Unlearning from Text-to-Image Diffusion Models",
            "text": "Copyright law confers upon creators the exclusive rights to reproduce, distribute, and monetize their creative works. However, recent progress in text-to-image generation has introduced formidable challenges to copyright enforcement. These technologies enable the unauthorized learning and replication of copyrighted content, artistic creations, and likenesses, leading to the proliferation of unregulated content. Notably, models like stable diffusion, which excel in text-to-image synthesis, heighten the risk of copyright infringement and unauthorized distribution.Machine unlearning, which seeks to eradicate the influence of specific data or concepts from machine learning models, emerges as a promising solution by eliminating the \\enquote{copyright memories} ingrained in diffusion models. Yet, the absence of comprehensive large-scale datasets and standardized benchmarks for evaluating the efficacy of unlearning techniques in the copyright protection scenarios impedes the development of more effective unlearning methods. To address this gap, we introduce a novel pipeline that harmonizes CLIP, ChatGPT, and diffusion models to curate a dataset. This dataset encompasses anchor images, associated prompts, and images synthesized by text-to-image models. Additionally, we have developed a mixed metric based on semantic and style information, validated through both human and artist assessments, to gauge the effectiveness of unlearning approaches. Our dataset, benchmark library, and evaluation metrics will be made publicly available to foster future research and practical applications (https://rmpku.github.io/CPDM-page/, website / http://149.104.22.83/unlearning.tar.gz, dataset).",
            "score": 0.45894049639189183,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.96923828125
        },
        {
            "corpus_id": "265551515",
            "title": "VA3: Virtually Assured Amplification Attack on Probabilistic Copyright Protection for Text-to-Image Generative Models",
            "text": "To determine whether samples generated by model p infringe the copyright of the target image, we need to assign ground-truth labels to these samples. Unfortunately, to our knowledge, there is currently no widely recognized computable standard for determining whether an image infringes copyright. In fact, the criteria for copyright infringement determination may evolve with changing societal perceptions. Alternatively, we rely on the similarity between samples and the target image as the basis for determining infringement. In order to distinguish between noninfringing and infringing samples, an ideal similarity score should assign lower scores to non-infringing samples and higher scores to infringing samples. Recognizing the limitations of a singular similarity measure, we compare the performance of SSCD [31] and CLIP score for determining copyright infringement. In Fig. 5, we plot the histograms of SSCD scores and CLIP scores for images generated by original captions of all target copyrighted images in two datasets. We can observe that the distributions of SSCD scores demonstrate a more clearly bimodal pattern compared with CLIP scores. This means that non-infringing and infringing samples can be better distinguished by the two modes of distribution of SSCD scores. In Fig. 6, we show example images with different values of similarity scores in ascending order. We can find that non-infringing samples may have higher CLIP scores than infringing samples with target images. However, there is a clear threshold (e.g., 50%) for SSCD score to distinguish non-infringing and infringing samples. Thus, in this paper, we use the SSCD score for infringement judgment. In Sec. 5.3, we report results with SSCD-50% as the infringement threshold. Further, considering the evolving nature of copyright infringement standards, we utilize other varying thresholds. According to the observation in Fig. 5, we consider modeling the SSCD score distribution as a mixture of two Gaussian distributions and use the mean value of two means of the Gaussian distributions as the similarity threshold, denoted as SSCD-gmm. For POKEMON dataset, we further consider SSCD-45% and SSCD-55%.",
            "score": 0.45793102878002656,
            "section_title": "Infringement Judgment",
            "char_start_offset": 33299,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 149
                },
                {
                    "start": 150,
                    "end": 296
                },
                {
                    "start": 297,
                    "end": 406
                },
                {
                    "start": 407,
                    "end": 527
                },
                {
                    "start": 528,
                    "end": 717
                },
                {
                    "start": 718,
                    "end": 874
                },
                {
                    "start": 875,
                    "end": 1031
                },
                {
                    "start": 1032,
                    "end": 1154
                },
                {
                    "start": 1155,
                    "end": 1285
                },
                {
                    "start": 1286,
                    "end": 1382
                },
                {
                    "start": 1383,
                    "end": 1494
                },
                {
                    "start": 1495,
                    "end": 1611
                },
                {
                    "start": 1612,
                    "end": 1681
                },
                {
                    "start": 1682,
                    "end": 1689
                },
                {
                    "start": 1690,
                    "end": 1757
                },
                {
                    "start": 1758,
                    "end": 1872
                },
                {
                    "start": 1873,
                    "end": 2120
                },
                {
                    "start": 2121,
                    "end": 2184
                }
            ],
            "ref_mentions": [
                {
                    "start": 815,
                    "end": 819,
                    "matchedPaperCorpusId": "247011159"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.97900390625
        },
        {
            "corpus_id": "276575866",
            "title": "Can Large Vision-Language Models Detect Images Copyright Infringement from GenAI?",
            "text": "In this section, we outline two methods for generating positive samples that infringe on copyright protection laws. For this purpose, we selected three widely-used generative AI models: Stable Diffusion XL (generated 40% of the image samples) (Podell et al., 2023), Ideogram (generated 40% of the image samples) (ideogramAI), and DALL-E (generated 20% of the image samples) (Betker et al., 2023). \n\nGenerate IP Characters with Direct Prompt. \n\nThe simplest approach to generating positive samples involves using direct prompts with generative models, such as \"Generate an image of <a char-acter>.\" This method typically produces images that closely resemble the IP-protected characters, as illustrated in Fig. 2. Using this technique, we generated 40 images for each character class. \n\nGenerate with Descriptive Prompt. Rewriting direct prompts that reference copyright-protected content into longer, more descriptive prompts, as explored by Wang et al. (2024) and He et al. (2024), can sometimes reduce the risk of IP infringement. However, this approach is not entirely effective in preventing outputs that closely resemble copyrighted characters (He et al., 2024), as rewritten prompts often retain a high degree of similarity to the original IP-associated names. \n\nTo enhance the diversity of our dataset, we first generate images using descriptive prompts and then apply a human evaluation process to filter out most positive samples, as detailed in Section 3.2. We use GPT-4o (GPT4V) here as its exceptional text generation capabilities. We construct descript prompt with the following guidance to GPT-4o: \n\n\u2022 Creating a prompt that describes a character similar to <Target Character>. This prompt should enable text-to-image AI models to generate images without directly mentioning the name of the <Target Character>. \n\nFinally, we curate the selected positive images, as illustrated in Fig. 3.",
            "score": 0.45775419262272865,
            "section_title": "Collecting Positive Samples",
            "char_start_offset": 12340,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 115
                },
                {
                    "start": 116,
                    "end": 396
                },
                {
                    "start": 399,
                    "end": 441
                },
                {
                    "start": 444,
                    "end": 597
                },
                {
                    "start": 598,
                    "end": 783
                },
                {
                    "start": 786,
                    "end": 819
                },
                {
                    "start": 820,
                    "end": 1032
                },
                {
                    "start": 1033,
                    "end": 1266
                },
                {
                    "start": 1269,
                    "end": 1467
                },
                {
                    "start": 1468,
                    "end": 1543
                },
                {
                    "start": 1544,
                    "end": 1611
                },
                {
                    "start": 1614,
                    "end": 1691
                },
                {
                    "start": 1692,
                    "end": 1824
                },
                {
                    "start": 1827,
                    "end": 1901
                }
            ],
            "ref_mentions": [
                {
                    "start": 374,
                    "end": 395,
                    "matchedPaperCorpusId": "264403242"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.9775390625
        },
        {
            "corpus_id": "273374823",
            "title": "Meta-Unlearning on Diffusion Models: Preventing Relearning Unlearned Concepts",
            "text": "In this section, we use both ESD-u-1-based unlearning and meta-unlearning to remove copyrighted concepts and/or styles from images. \n\nCopyright removal. Fig. 2 shows the removal of two copyrighted cartoon characters, SpongeBob and Snoopy. After unlearning, neither unlearned nor meta-unlearned models are able to produce the removed copyright concepts. However, after maliciously finetuned on unlearned concepts, the unlearned model becomes to relearn the unlearned concepts, while the meta-unlearned model is resistant to relearning and avoids copyright infringement. \n\nStyle removal. Fig. 4 shows the removal of two painting styles Thomas Kinkade and Kelly McKernan. After unlearning, neither the unlearned nor the meta-unlearned models can produce these styles. Nevertheless, the unlearned model regains the capacity to replicate the removed styles after malicious finetuning, while the meta-unlearned model will not relearn these removed styles. \n\nPerformance on unrelated concepts. Fig. 9 displays the images generated on prompts unrelated to the removed concepts before the unlearned or meta-unlearned model is maliciously finetuned. This demonstrates that both unlearned and meta-unlearned models can maintain the capability to generate images on unrelated and benign concepts.",
            "score": 0.4575036145907459,
            "section_title": "COPYRIGHT AND STYLE REMOVAL",
            "char_start_offset": 20568,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 131
                },
                {
                    "start": 134,
                    "end": 152
                },
                {
                    "start": 153,
                    "end": 238
                },
                {
                    "start": 239,
                    "end": 352
                },
                {
                    "start": 353,
                    "end": 568
                },
                {
                    "start": 571,
                    "end": 585
                },
                {
                    "start": 586,
                    "end": 668
                },
                {
                    "start": 669,
                    "end": 764
                },
                {
                    "start": 765,
                    "end": 949
                },
                {
                    "start": 952,
                    "end": 986
                },
                {
                    "start": 987,
                    "end": 1139
                },
                {
                    "start": 1140,
                    "end": 1284
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.845703125
        },
        {
            "corpus_id": "269780858",
            "title": "Challenges and Opportunities in Intellectual Property Rights (IPR) in the Age of Generative AI: Balancing Innovation and Protection",
            "text": "The rise of Generative AI in content creation has intensified the risks associated with copyright infringement, significantly impacting the landscape of Intellectual Property Rights (IPR).AI's ability to synthesize and reproduce content based on existing works poses a substantial challenge in distinguishing between original creation and unauthorized derivative works.This situation raises critical legal questions about the extent to which AI -generated content might inadvertently infringe upon existing copyrights, especially when such content closely resembles or is derived from copyrighted material [23].\n\nFurthermore, the difficulty in tracing the origins of AIgenerated content complicates the enforcement of copyright laws.Traditional copyright infringement assessments, which rely on human intent and direct copying, are not readily applicable to AI, where the 'intent' is ambiguous and the 'copying' process is inherently complex and often opaque [6,34].This scenario necessitates a rethinking of copyright frameworks to effectively address the nuances of AI -driven content creation, ensuring that the rights of original creators are protected while also recognizing the innovative contributions of AI technologies [30,25].",
            "score": 0.457231491825206,
            "section_title": "Copyright Infringement Risks",
            "char_start_offset": 6171,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 188
                },
                {
                    "start": 188,
                    "end": 369
                },
                {
                    "start": 369,
                    "end": 611
                },
                {
                    "start": 613,
                    "end": 733
                },
                {
                    "start": 733,
                    "end": 966
                },
                {
                    "start": 966,
                    "end": 1236
                }
            ],
            "ref_mentions": [
                {
                    "start": 606,
                    "end": 610,
                    "matchedPaperCorpusId": "117226187"
                },
                {
                    "start": 959,
                    "end": 962,
                    "matchedPaperCorpusId": "157107565"
                },
                {
                    "start": 962,
                    "end": 965,
                    "matchedPaperCorpusId": "155815461"
                },
                {
                    "start": 1232,
                    "end": 1235,
                    "matchedPaperCorpusId": "111061225"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.283935546875
        },
        {
            "corpus_id": "271874323",
            "title": "Not Every Image is Worth a Thousand Words: Quantifying Originality in Stable Diffusion",
            "text": "Privacy and Copyright Infringements The intersection of privacy and copyright infringements in generative models has garnered significant attention. This approach assumes that to avoid copyright infringement, the output of a model shouldn't be too sensitive to any of its individual training samples. Bousquet et al. (Bousquet et al., 2020) suggest the use of differential privacy (Dwork et al., 2006) 2023), however, investigated the gap between privacy and copyright infringement from the perspective of the law, and showed that requiring such notions of stability may be too strong, and are not always aligned with the original intention of the law. Closer to our approach, (Scheffler et al., 2022) suggests a framework to quantify originality by measuring the description length of a content with and without access to the allegedly copyrighted material. Our approach of textual inversion also looks for a succinct description of the content but, dis-tinctively, our definition depends on the distribution of the data, and measures originality with respect to the whole data to be trained. This may lead to different outcomes, for example, when the allegedly copyrighted material contains a distinctive trait that is not necessarily original. \n\nAttribution in Generative Models Attribution in generative models is a crucial area of research, focusing on identifying the sources of data that contribute to the generation of specific outputs. Park et al. introduced the TRAK method to address data attribution in large-scale models (Park, 2022), and recently, Wang et al. (Wang et al., 2023b) proposed a method for evaluating the attribution in Stable Diffusion models of data points in the generation process, which is closely related to assessing the originality of generated images. However, such a method requires full access and knowledge of the training set on which the model was trained.",
            "score": 0.45696380394485303,
            "section_title": "Related Works",
            "char_start_offset": 20867,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 148
                },
                {
                    "start": 149,
                    "end": 300
                },
                {
                    "start": 301,
                    "end": 652
                },
                {
                    "start": 653,
                    "end": 858
                },
                {
                    "start": 859,
                    "end": 1093
                },
                {
                    "start": 1094,
                    "end": 1246
                },
                {
                    "start": 1249,
                    "end": 1444
                },
                {
                    "start": 1445,
                    "end": 1787
                },
                {
                    "start": 1788,
                    "end": 1897
                }
            ],
            "ref_mentions": [
                {
                    "start": 317,
                    "end": 340,
                    "matchedPaperCorpusId": "225067265"
                },
                {
                    "start": 381,
                    "end": 401,
                    "matchedPaperCorpusId": "2468323"
                },
                {
                    "start": 677,
                    "end": 701,
                    "matchedPaperCorpusId": "249375708"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.82177734375
        },
        {
            "corpus_id": "269033466",
            "title": "RealmDreamer: Text-Driven 3D Scene Generation with Inpainting and Depth Diffusion",
            "text": "While we use pretrained models for all components of our pipeline, it is important to acknowledge biases and ethical issues that stem from the training of these large-scale image generative models [50]. As these models are often trained on vast collections of internet data, they can reflect negative biases and stereotypes against certain populations, as well as infringe on the copyright of artists and other creatives. It is essential to consider these factors when using these models and our technique broadly.",
            "score": 0.4566150045812061,
            "section_title": "Ethical Considerations",
            "char_start_offset": 24481,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 202
                },
                {
                    "start": 203,
                    "end": 421
                },
                {
                    "start": 422,
                    "end": 514
                }
            ],
            "ref_mentions": [
                {
                    "start": 197,
                    "end": 201,
                    "matchedPaperCorpusId": "245335280"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.7822265625
        },
        {
            "corpus_id": "276558342",
            "title": "CopyJudge: Automated Copyright Infringement Identification and Mitigation in Text-to-Image Diffusion Models",
            "text": "Filtering Agent Source Image Generated Image Motivated by this, we aim to establish a substantial similarity identification model f , which takes x and x cr as inputs and outputs a similarity score s. When s exceeds a threshold \u03b3, we determine that x infringes on x cr . This can be defined as: \n\nwhere x represents the AI-generated image, and x cr is the corresponding copyrighted image.",
            "score": 0.4565359383052015,
            "section_title": "Abstraction Agent",
            "char_start_offset": 6224,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 200
                },
                {
                    "start": 201,
                    "end": 270
                },
                {
                    "start": 271,
                    "end": 294
                },
                {
                    "start": 297,
                    "end": 388
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.95751953125
        },
        {
            "corpus_id": "270063652",
            "title": "Automatic Jailbreaking of the Text-to-Image Generative AI Systems",
            "text": "Midjourney [19], Gemini Pro [29], Copilot [18] and ChatGPT [1] have word-based detection mechanism on the user prompts to prevent generation of the images that may violate the internal policy.that the blocking mechanism fails to block copyright infringement generation to 11.0% block rate on our APGP-generated prompts (Table 3).Furthermore, not only generating the contents, the contents are exceptionally similar to the original IP content as shown in Figure 4. Human evaluation.To quantify the violations, we conducted a human evaluation on 63 participants to determine the copyright violation based on the reference image.The copyright violation is highly occurring in the product and logo category where 96.24% and 82.71% of participants examine the images as copyright infringement (Figure 7).Upon examining the images classified as identical violations, it was found that over 50% were deemed to be cases of copyright infringement in product and logo.Furthermore, 30% of characters are also considered as similar violations which are determined as severe similarity (Figure 8).When we employ a consensus vote to determine violations, there are still 10 images that all participants determine as violations.Automatic evaluation.Although human evaluation is one of the best evaluation approach for copyright infringement, we propose automatic evaluation to reduce the cost of the experiment.We introduce a QA score that calculates the accuracy by given generated images by T2I systems, where QA sets are generated based on the target image.\n\nWe employ VLM to respond to the question, and LLM to evaluate the answers.In Figure 5, 34.09% of the generated images accurately answer more than seven questions, suggesting that these images contain key aspects similar to the target images necessary for matching the correct answers.Given that the target image correctly matches the answer for more than seven questions in 67.05% of cases, we estimate that 50.84% of the generated images likely commit copyright infringement.Ablation study.Text prompts that specifically describe copyrighted content can trigger the generation of such content even without explicit keywords, as demonstrated in Table 4.We hypothesize that omitting specific keywords may allow these prompts to bypass initial violation detection mechanisms.",
            "score": 0.45613931266155816,
            "section_title": "Simple prompt can induce the copyright violation in most systems",
            "char_start_offset": 15113,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 192
                },
                {
                    "start": 192,
                    "end": 329
                },
                {
                    "start": 329,
                    "end": 481
                },
                {
                    "start": 481,
                    "end": 626
                },
                {
                    "start": 626,
                    "end": 799
                },
                {
                    "start": 799,
                    "end": 958
                },
                {
                    "start": 958,
                    "end": 1084
                },
                {
                    "start": 1084,
                    "end": 1213
                },
                {
                    "start": 1213,
                    "end": 1234
                },
                {
                    "start": 1234,
                    "end": 1396
                },
                {
                    "start": 1396,
                    "end": 1545
                },
                {
                    "start": 1547,
                    "end": 1621
                },
                {
                    "start": 1621,
                    "end": 1831
                },
                {
                    "start": 1831,
                    "end": 2023
                },
                {
                    "start": 2023,
                    "end": 2038
                },
                {
                    "start": 2038,
                    "end": 2200
                },
                {
                    "start": 2200,
                    "end": 2320
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.87744140625
        },
        {
            "corpus_id": "269033217",
            "title": "Disguised Copyright Infringement of Latent Diffusion Models",
            "text": "Copyright infringement may occur when a generative model produces samples substantially similar to some copyrighted data that it had access to during the training phase. The notion of access usually refers to including copyrighted samples directly in the training dataset, which one may inspect to identify an infringement. We argue that such visual auditing largely overlooks a concealed copyright infringement, where one constructs a disguise that looks drastically different from the copyrighted sample yet still induces the effect of training Latent Diffusion Models on it. Such disguises only require indirect access to the copyrighted material and cannot be visually distinguished, thus easily circumventing the current auditing tools. In this paper, we provide a better understanding of such disguised copyright infringement by uncovering the disguises generation algorithm, the revelation of the disguises, and importantly, how to detect them to augment the existing toolbox. Additionally, we introduce a broader notion of acknowledgment for comprehending such indirect access. Our code is available at https://github.com/watml/disguised_copyright_infringement.",
            "score": 0.45609959937859534,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.9619140625
        },
        {
            "corpus_id": "259088709",
            "title": "ChatGPT is a Remarkable Tool\u2014For Experts",
            "text": "The remarkable abilities exhibited by generative AI, as exemplified by ChatGPT, in the creation of artistic works, give rise to a multitude of profound legal questions. These inquiries primarily revolve around two fundamental aspects: the copyright of the training data and the copyright of the AIgenerated products. \n\nThe first copyright issue pertains to the training process of generative AI models. These models rely on diverse datasets that may contain copyrighted material, leading to questions of ownership and licensing between the enterprise and the parties whose information was used. In the case of ChatGPT, the absence of explicit source attribution in its responses raises concerns about potential copyright infringements. \n\nDetermining ownership rights in a derivative work depends on factors such as the origin and ownership of the training dataset and the level of similarity between the AI-generated work and specific works within the training set [65]. \n\nAccording to the Team8 group report [75], certain generative AI models have been found to incorporate content created by others, including code, instead of relying solely on their own generated content. This raises concerns about potential copyright infringement. Additionally, there is a risk Figure 10: Generated code similar to existing code: the 8 queens example. Note that SWISH credits the author, while ChatGPT does not. of generating the same content for multiple users. The report highlights that utilizing the output of generative AI could lead to claims of copyright infringement, particularly if the models were trained on copyrighted content without obtaining appropriate permissions from the dataset owners. \n\nReturning to ChatGPT, instances have been reported where it generated responses that closely resemble copyrighted sources, without proper referencing. This issue may arise when users seek content in a specific style, such as that of a writer or poet, or when requesting code in less common programming languages. It is also relevant when asking for definitions or code snippets in niche languages, as ChatGPT's responses can closely mirror copyrighted material without acknowledging the source. \n\nDuring an interaction with ChatGPT, Gordon Graham [31] observed that the definition of a \"white paper\" provided by ChatGPT closely resembled his own definition. This raised concerns that his copyrighted content had been scrapped by the creators of ChatGPT without permission, credit, or compensation.",
            "score": 0.4556613148949365,
            "section_title": "Copyrights Issues",
            "char_start_offset": 72967,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 168
                },
                {
                    "start": 169,
                    "end": 316
                },
                {
                    "start": 319,
                    "end": 402
                },
                {
                    "start": 403,
                    "end": 594
                },
                {
                    "start": 595,
                    "end": 735
                },
                {
                    "start": 738,
                    "end": 970
                },
                {
                    "start": 973,
                    "end": 1175
                },
                {
                    "start": 1176,
                    "end": 1236
                },
                {
                    "start": 1237,
                    "end": 1340
                },
                {
                    "start": 1341,
                    "end": 1451
                },
                {
                    "start": 1452,
                    "end": 1694
                },
                {
                    "start": 1697,
                    "end": 1847
                },
                {
                    "start": 1848,
                    "end": 2009
                },
                {
                    "start": 2010,
                    "end": 2191
                },
                {
                    "start": 2194,
                    "end": 2354
                },
                {
                    "start": 2355,
                    "end": 2494
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.86083984375
        },
        {
            "corpus_id": "270095412",
            "title": "Going beyond Compositions, DDPMs Can Produce Zero-Shot Interpolations",
            "text": "Our research on the interpolation abilities of diffusion models holds promise for improving fairness in machine learning.By showcasing the model's capacity to generate diverse attributes covering the range of a latent factor from a subset of examples, our work can contribute to mitigating biases in training data.However, it's essential to recognize potential misuse.While our work is on low-dimensional images of insufficient quality for deception, the method could potentially be improved and exploited for deepfake generation in higher resolutions.Such concerns are inherent to the development of most novel generative modeling techniques.Importantly, our focus is not technically advancing the state of the art in image generation.Instead, we merely study the properties of diffusion models and show what is already possible with current models.\n\nThis demonstration also has concrete implications on the use of generative models as tools in creating new content.At time of writing there are ongoing lawsuits concerning the generation of copyrighted content (Marcus & Southen, 2024) and while the examples discussed in our citation are clearly infringing even if they were interpolated in the manner we discuss (which we find unlikely), the ability to interpolate and potentially extrapolate across latent factors hints at the possibility at truly novel synthesized content -which then itself would complicate discussions around the level of derivative vs. original \"work\" present in the generated content.\n\nIn no way should our work be cited as \"proof\" that any models involved in such lawsuits are not infringing.It requires further development in dataset provenance attestation and legal frameworks to make any such claims.Presumably, our findings will play a rule in developing these legal frameworks.Until then, we think it is important to err on the side of caution and not train production models on any content for which the artists or copyright holders have not explicitly consented to be included as part of generative model training.\n\nFurther, while we are excited about the idea of using interpolated factors to de-bias classifiers, e.g. by developing methods for holding all factors except the interpolated one constant in order to remove spurious correlations with e.g.gender or skin color, the model used for such debiasing might itself be flawed and/or introduce new biases.",
            "score": 0.45559146974850234,
            "section_title": "Impact Statement",
            "char_start_offset": 26407,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 121
                },
                {
                    "start": 121,
                    "end": 314
                },
                {
                    "start": 314,
                    "end": 368
                },
                {
                    "start": 368,
                    "end": 552
                },
                {
                    "start": 552,
                    "end": 643
                },
                {
                    "start": 643,
                    "end": 736
                },
                {
                    "start": 736,
                    "end": 850
                },
                {
                    "start": 852,
                    "end": 967
                },
                {
                    "start": 967,
                    "end": 1510
                },
                {
                    "start": 1512,
                    "end": 1619
                },
                {
                    "start": 1619,
                    "end": 1730
                },
                {
                    "start": 1730,
                    "end": 1809
                },
                {
                    "start": 1809,
                    "end": 2048
                },
                {
                    "start": 2050,
                    "end": 2287
                },
                {
                    "start": 2287,
                    "end": 2394
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.9228515625
        },
        {
            "corpus_id": "261076348",
            "title": "How to Protect Copyright Data in Optimization of Large Language Models?",
            "text": "Large language models have changed the world, with the rise of generative AI models such as ChatGPT, GPT-4, Llama, BERT, BARD, PaLM, and OPT (ChatGPT 2022;Bubeck et al. 2023;Devlin et al. 2018;Touvron et al. 2023b,a;BARD 2023;Chowdhery et al. 2022;Anil et al. 2023;Zhang et al. 2022). These models are able to process natural language effectively, handling a wide range of tasks including story generation, code creation, machine translation, and elementary mathematical problem solving (Brown et al. 2020;Svyatkovskiy et al. 2020;Wu et al. 2016;Wei et al. 2022). One core component in the large language model is the transformer architecture (Vaswani et al. 2017), which is built on a computational step known as attention. Transformers have been used in a wide variety of tasks outside of large language models, including generative image systems such as DALL-E (Research 2021) and DALL-E2 (Research 2022). Recent research has integrated the transformer architecture with scalable diffusion-based image generation models (Bao et al. 2023;Cao et al. 2022;Wu et al. 2023a;Han et al. 2022;Dosovitskiy et al. 2020). \n\nOnce challenge in generative AI is guaranteeing that outputs are protected from copyright infringement and intellectual property issues (Hattenbach and Glucoft 2015;Hristov 2016;Sag 2018;Gillotte 2019;Vyas, Kakade, and Barak 2023). Generative models trained on large corpora of data can inadvertently generate outputs that are direct copies, or close variants, of copyrighted text or images that the model is trained on. Removing copyrighted material from training may also be undesirable: while one can achieve good performance without using copyrighted data, the inclusion of such data can significantly enhance the performance of generative AI models. For example, incorporating literary works, which are often copyrighted, into the training dataset of a language model can enhance performance.",
            "score": 0.4550837850754299,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 284
                },
                {
                    "start": 285,
                    "end": 563
                },
                {
                    "start": 564,
                    "end": 724
                },
                {
                    "start": 725,
                    "end": 908
                },
                {
                    "start": 909,
                    "end": 1113
                },
                {
                    "start": 1116,
                    "end": 1347
                },
                {
                    "start": 1348,
                    "end": 1536
                },
                {
                    "start": 1537,
                    "end": 1770
                },
                {
                    "start": 1771,
                    "end": 1913
                }
            ],
            "ref_mentions": [
                {
                    "start": 155,
                    "end": 174,
                    "matchedPaperCorpusId": "218971783"
                },
                {
                    "start": 174,
                    "end": 193,
                    "matchedPaperCorpusId": "225039882"
                },
                {
                    "start": 226,
                    "end": 248,
                    "matchedPaperCorpusId": "231830912"
                },
                {
                    "start": 487,
                    "end": 506,
                    "matchedPaperCorpusId": "218971783"
                },
                {
                    "start": 506,
                    "end": 531,
                    "matchedPaperCorpusId": "260334454"
                },
                {
                    "start": 531,
                    "end": 546,
                    "matchedPaperCorpusId": "259108694"
                },
                {
                    "start": 546,
                    "end": 562,
                    "matchedPaperCorpusId": "246411621"
                },
                {
                    "start": 1023,
                    "end": 1040,
                    "matchedPaperCorpusId": "253581703"
                },
                {
                    "start": 1088,
                    "end": 1112,
                    "matchedPaperCorpusId": "225039882"
                },
                {
                    "start": 1252,
                    "end": 1281,
                    "matchedPaperCorpusId": "244909149"
                },
                {
                    "start": 1281,
                    "end": 1294,
                    "matchedPaperCorpusId": "113882623"
                },
                {
                    "start": 1294,
                    "end": 1303,
                    "matchedPaperCorpusId": "86424284"
                },
                {
                    "start": 1303,
                    "end": 1317,
                    "matchedPaperCorpusId": "233748873"
                },
                {
                    "start": 1317,
                    "end": 1346,
                    "matchedPaperCorpusId": "246411621"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.83642578125
        },
        {
            "corpus_id": "269033217",
            "title": "Disguised Copyright Infringement of Latent Diffusion Models",
            "text": "Main results: In Section 4, we reveal the latent (copyrighted) information contained in the acquired samples x d using textual inversion, where the diffusion model (i.e., the U-Net in LDM) is not retrained.To further demonstrate the effectiveness of the disguises x d and examine disguised copyright infringement in other scenarios, we further perform evaluation on DreamBooth (Ruiz et al. 2023), an LDM-based fine-tuning method on a small set of images designed for subject-driven generation.Specifically, we choose five images from the DreamBooth dataset12 as the (designated) copyrighted image x c , and generate their corresponding disguises x d using Algorithm 1 by choosing the noisy version of x c as the base images x b , training 10000 epochs and setting the weight parameter \u03b1 = 1000.After generating the disguises x d , we apply DreamBooth for fine-tuning (on x d only) and inference following the general recipe13 .In Figure 24, we show the copyrighted images x c in the first row, their corresponding disguises x d in the second row, and images generated by DreamBooth by fine-tuning with the above x d in the third row.Our results qualitatively confirm that copyrighted information in x c can be reproduced by fine-tuning on x d with DreamBooth.\n\nEvaluation on model utility: To further examine the practicality of the disguises, we want to demonstrate that the model utility is not deprecated after training (or fine-tuning) on x d .To accomplish this task, we perform the same text-to-image generation task with the prompt \"a photo of an astronaut riding a horse on Mars\" before/after training on x d with LDM14 .Specifically, the \"before\" model is Stable Diffusion v1-4, and the \"after\" model is the acquired model by fine-tuning the \"before\" model on the dog disguise (second row, first column in Figure 24).We show our results in Figure 25 for 6",
            "score": 0.453736396671452,
            "section_title": "F.1. Evaluation on DreamBooth",
            "char_start_offset": 37186,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 206
                },
                {
                    "start": 206,
                    "end": 493
                },
                {
                    "start": 493,
                    "end": 794
                },
                {
                    "start": 794,
                    "end": 927
                },
                {
                    "start": 927,
                    "end": 1133
                },
                {
                    "start": 1133,
                    "end": 1259
                },
                {
                    "start": 1261,
                    "end": 1448
                },
                {
                    "start": 1448,
                    "end": 1629
                },
                {
                    "start": 1629,
                    "end": 1826
                },
                {
                    "start": 1826,
                    "end": 1864
                }
            ],
            "ref_mentions": [
                {
                    "start": 377,
                    "end": 394,
                    "matchedPaperCorpusId": "251800180"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.94873046875
        },
        {
            "corpus_id": "272060563",
            "title": "Managing Copyright Infringement Risks in Generative Artificial Intelligence Data Mining",
            "text": "With the continuous innovation of computer science, artificial intelligence has entered a new stage of development from assisting artificial intelligence to self-generated artificial intelligence. This stage affects every aspect of human creation, on the one hand, it brings great convenience to human creation, on the other hand, it also brings a series of new challenges to the existing copyright legislation and justice. At present, in judicial practice, there have been a large number of lawsuits against AI infringement, mainly concentrated in the field of AI painting and writing. For example, in the world's first AIGC infringement case \"Stable Diffusion\" [1], the U.S. photo library giant Getty Images filed a lawsuit against \"Stability AI\" for scraping millions of images online without permission to develop and train its AI image generation technology. \n\nThe case has attracted wide attention around the world. It has also sparked research on \"fair use systems for generative artificial intelligence data mining\". This paper aims to further analyze the types of copyright infringement that may be involved in the data input stage based on the operation mode of generative AI. Based on the study of existing domestic legislation and justice, combined with foreign experience, this paper aims to propose solutions to copyright risks that may be caused by generative artificial intelligence data mining. This lays a good legal groundwork for the future longevity of generative AI.",
            "score": 0.4533810537897026,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 196
                },
                {
                    "start": 197,
                    "end": 423
                },
                {
                    "start": 424,
                    "end": 586
                },
                {
                    "start": 587,
                    "end": 863
                },
                {
                    "start": 866,
                    "end": 921
                },
                {
                    "start": 922,
                    "end": 1024
                },
                {
                    "start": 1025,
                    "end": 1186
                },
                {
                    "start": 1187,
                    "end": 1411
                },
                {
                    "start": 1412,
                    "end": 1488
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.87353515625
        },
        {
            "corpus_id": "265659495",
            "title": "Watermarking for Neural Radiation Fields by Invertible Neural Network",
            "text": "The usage scenarios of the algorithm described in this paper are as follows: \n\nAlice has acquired some pictures of a 3D scene by taking photographs, etc; Alice embeds watermarks in the images and renders 3D scenes by training NeRF models; \n\nAlice shares NeRF models and 3D scenes online for others to enjoy; \n\nBob acquired the NeRF model without Alice's permission and posted it on the web under his name; \n\nAlice sees the NeRF model posted by Bob and uses his model to render a 2D image for watermark extraction, thereby verifying that Alice is the copyright holder of the NeRF model; \n\nBob is infringing on copyright and needs to withdraw the release;",
            "score": 0.453074981426287,
            "section_title": "A. Application Scenarios",
            "char_start_offset": 5876,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 76
                },
                {
                    "start": 79,
                    "end": 238
                },
                {
                    "start": 241,
                    "end": 307
                },
                {
                    "start": 310,
                    "end": 405
                },
                {
                    "start": 408,
                    "end": 585
                },
                {
                    "start": 588,
                    "end": 653
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.54150390625
        },
        {
            "corpus_id": "270620122",
            "title": "Fantastic Copyrighted Beasts and How (Not) to Generate Them",
            "text": "State-of-the-art image and video generation models demonstrate a remarkable ability for generating high-quality visual content based on free-form user inputs (Rombach et al., 2022;Betker et al., 2023;Chen et al., 2024;Li et al., 2024a;Blattmann et al., 2023;Esser et al., 2024). However, recent research has shown that generative models, including those for image and video, are susceptible to memorizing and generating entire datapoints or concepts from their training data (Somepalli et al., 2023;Carlini et al., 2023;car, 2023). Since some training data originates from copyrighted materials (car, 2023;Kumari et al., 2023), regurgitation of such content may lead to legal intellectual property liability for users and model deployers who further make use of the generated content. In particular, this liability may stem not only from verbatim generation of training data points, but generation of some copyrightable repeating motifs highly similar to those from the training data. \n\nAs a result, copyright concerns in image generative models has been extensively discussed in both academic research (car, 2023;Ma et al., 2024;Kim et al., 2024) and litigation (Vincent, 2023;Andersen et al. v. Stability AI et al., N.D. Cal. 2023). Among the diverse copyrighted content that  (Li et al., 2024a) and proprietary DALL\u2022E 3 model. For Mario (a) and Batman (b), both models can generate these characters through indirect anchoring, using relevant descriptive keywords instead of character names. DALL\u2022E 3 blocks explicit name prompts requests (character name anchoring) with content policy messages.",
            "score": 0.45283379289724596,
            "section_title": "INTRODUCTION",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 278
                },
                {
                    "start": 279,
                    "end": 531
                },
                {
                    "start": 532,
                    "end": 784
                },
                {
                    "start": 785,
                    "end": 984
                },
                {
                    "start": 987,
                    "end": 1234
                },
                {
                    "start": 1235,
                    "end": 1329
                },
                {
                    "start": 1330,
                    "end": 1493
                },
                {
                    "start": 1494,
                    "end": 1597
                }
            ],
            "ref_mentions": [
                {
                    "start": 475,
                    "end": 499,
                    "matchedPaperCorpusId": "254366634"
                },
                {
                    "start": 606,
                    "end": 626,
                    "matchedPaperCorpusId": "257687839"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.974609375
        },
        {
            "corpus_id": "272060563",
            "title": "Managing Copyright Infringement Risks in Generative Artificial Intelligence Data Mining",
            "text": "The infringement of works by generative AI becomes explicit through the generation of content. Based on the previous discussion that non-expressive use does not amount to the use of works and is not subject to tort liability, further infringement analysis will not be conducted here. However, in the case of expressive use, the situation differs. When the generated content from generative artificial intelligence exhibits original expression during the output stage, it should be categorized as a \"content conversion\", qualifying as fair use and therefore exempt from tort liability. On the other hand, if the generated content merely replicates or rearranges the learned work without adding significant originality, resulting in substantial similarity to the original work, it should not be considered fair use but rather as copyright infringement [12]. In practice, when assessing infringement considering the input of previous works, the determination may follow the standard of \"contact + substantial similarity\". In other words, if the generated content is substantially similar to the original work and lack prior permission from the copyright owner, the infringement of the right to copy can be established directly without the need to evaluate the degree of similarity in terms of quantity and quality. The consideration of similarity in quantity and quality comes into play when the defendant subsequently claims \"fair use\" as a defense. \n\nThe resulting tort liability should be clearly defined in legislation to protect the rights of the original owners. This paper argues that the tort liability should be categorized and discussed based on different development models. \n\nWhen the service provider purchases the model developed by the technology developer and provides it to the user directly, it should be established that the technology developer bears the corresponding copyright infringement liability. Because it understands and can control the data input of generative AI. Unless the developer can prove that the model was not exposed to the original work during the data training process. As a result, the problem of \"algorithmic black box\" can be effectively solved by comprehensively checking the algorithms, training data, and output content, so as to better improve the interpretability of artificial intelligence. \n\nWhen the service provider fine-tunes the model to meet the requirements of specific application scenarios, both the pre-training in the technology development stage and the fine-tuning before the later market application will essentially shape the generative AI algorithm model.",
            "score": 0.4525153530184033,
            "section_title": "Clarify the Subject of Tort Liability",
            "char_start_offset": 15475,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 94
                },
                {
                    "start": 95,
                    "end": 283
                },
                {
                    "start": 284,
                    "end": 346
                },
                {
                    "start": 347,
                    "end": 584
                },
                {
                    "start": 585,
                    "end": 855
                },
                {
                    "start": 856,
                    "end": 1018
                },
                {
                    "start": 1019,
                    "end": 1311
                },
                {
                    "start": 1312,
                    "end": 1447
                },
                {
                    "start": 1450,
                    "end": 1565
                },
                {
                    "start": 1566,
                    "end": 1682
                },
                {
                    "start": 1685,
                    "end": 1919
                },
                {
                    "start": 1920,
                    "end": 1991
                },
                {
                    "start": 1992,
                    "end": 2108
                },
                {
                    "start": 2109,
                    "end": 2338
                },
                {
                    "start": 2341,
                    "end": 2619
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.84033203125
        },
        {
            "corpus_id": "269137391",
            "title": "Uncertain Boundaries: Multidisciplinary Approaches to Copyright Issues in Generative AI",
            "text": "Generative AI presents a complex challenge in navigating copyright issues, as AI-generated content can often infringe upon existing copyrights.This survey paper comprehensively explores approaches and regulations to consider in navigating copyright issues in AI.Specifically, it details methods for detecting AI-generated copyright violations.It then delves into existing approaches, limitations, and potential biases in detection methods, as well as various practical approaches to protecting copyrighted content in AI, including but not limited to watermarking, digital signatures, and machine unlearning.Additionally, resources such as AI-generated datasets and web tools are outlined for users to assess copyright infringement in online multimedia.Lastly, the paper examines regulations surrounding copyright and AI-generated works, offering valuable insights for future research and policy development in this evolving field.As we stand at the intersection of creativity and technology, the significance of these methods cannot be overstated, setting a critical foundation for future advancements ensuring a balanced coexistence of generative AI and data integrity.",
            "score": 0.4520860410562245,
            "section_title": "CONCLUSION",
            "char_start_offset": 41763,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 143
                },
                {
                    "start": 143,
                    "end": 262
                },
                {
                    "start": 262,
                    "end": 343
                },
                {
                    "start": 343,
                    "end": 607
                },
                {
                    "start": 607,
                    "end": 752
                },
                {
                    "start": 752,
                    "end": 930
                },
                {
                    "start": 930,
                    "end": 1170
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.6923828125
        },
        {
            "corpus_id": "276558342",
            "title": "CopyJudge: Automated Copyright Infringement Identification and Mitigation in Text-to-Image Diffusion Models",
            "text": "In this paper, we attempt to leverage large visionlanguage models (LVLMs) to model the practical court decisions on substantial similarity. However, directly applying large models for infringement identification may face unreliable outputs due to their limited comprehension or potential misinterpretation. To address this, we propose CopyJudge, an automated abstraction-filtration-comparison framework with multi-LVLM debate to reliably follow the court decision process on identifying substantial similarity. Specifically, referring to the software abstraction test (Abramson, 2002), we decompose the image into different layers or elements, such as composition, color, and theme, to distinguish between the basic concepts of the image and its specific expressions. Then, we filter out parts that are not copyright-protected, such as public and functional expressions. Finally, we compare the filtered portions to assess whether there is substantial similarity. To enhance the reliability of the judgment, we employ a multi-agent debate (Du et al., 2023;Chan et al., 2023) method where multiple LVLMs discuss and score the similarity. Each LVLM can make judgments based on the scores and reasons provided by other LVLMs. Ultimately, another LVLM-based meta-judge gives the final score and rationale based on the consensus of the debate. To enhance the consistency with human preferences, we inject human priors into each LVLM via few-shot demonstrations (Agarwal et al., 2024). \n\nGiven the judging results, we further explore how to mitigate infringement issues in text-to-image diffusion models. Utilizing our CopyJudge, we propose an automated black-box infringement mitigation strategy that leverages a defense LVLM to iteratively optimize the input infringing prompts. This process avoids generating sensitive infringing expressions by querying supporting infringement rationales, while preserving the integrity of the original content. Moreover, if the input latent of the diffusion model is controllable, we could further enhance the mitigation approach by exploring specific non-infringing noise vectors within the latent space in a reinforcement manner, with the reward being reducing the predicted infringement score. This helps avoid infringement while maintaining the desired output characteristics, even without changing the original prompts.",
            "score": 0.4517670309822078,
            "section_title": "Introduction",
            "char_start_offset": 1848,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 139
                },
                {
                    "start": 140,
                    "end": 306
                },
                {
                    "start": 307,
                    "end": 510
                },
                {
                    "start": 511,
                    "end": 767
                },
                {
                    "start": 768,
                    "end": 870
                },
                {
                    "start": 871,
                    "end": 963
                },
                {
                    "start": 964,
                    "end": 1136
                },
                {
                    "start": 1137,
                    "end": 1222
                },
                {
                    "start": 1223,
                    "end": 1338
                },
                {
                    "start": 1339,
                    "end": 1479
                },
                {
                    "start": 1482,
                    "end": 1598
                },
                {
                    "start": 1599,
                    "end": 1774
                },
                {
                    "start": 1775,
                    "end": 1942
                },
                {
                    "start": 1943,
                    "end": 2228
                },
                {
                    "start": 2229,
                    "end": 2356
                }
            ],
            "ref_mentions": [
                {
                    "start": 568,
                    "end": 584,
                    "matchedPaperCorpusId": "6322845"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.953125
        },
        {
            "corpus_id": "261279983",
            "title": "AI Art and its Impact on Artists",
            "text": "The complaint 32 filed by Getty Images (a large and well-resourced copyright holder) against Stability AI is illustrative of the resources needed to assert copyright claims against companies producing image generators and the power differential that exists between small-scale copyright holders and these companies. Copyright infringement is a concern for small-scale artists but the overall system of how image generators normalize appropriation of art at the input stages is a problem that is beyond the scope of fair use considerations. \n\nThe second point where fair use is a question is if an image generator is used to create works that are similar to a human artist, and as a result compete with the human artist's market, as we describe in Section 4.1. In such instances, the fourth factor may weigh heavily against finding fair use. If the work is being used in ways that displaces the artist's market share, or prevents them from receiving appropriate attribution and compensation, there is a clear harm in place, and may be addressed through copyright law.",
            "score": 0.45143804358461415,
            "section_title": "Fair Use",
            "char_start_offset": 42074,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 315
                },
                {
                    "start": 316,
                    "end": 539
                },
                {
                    "start": 542,
                    "end": 759
                },
                {
                    "start": 760,
                    "end": 840
                },
                {
                    "start": 841,
                    "end": 1066
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.86376953125
        },
        {
            "corpus_id": "270620122",
            "title": "Fantastic Copyrighted Beasts and How (Not) to Generate Them",
            "text": "The combination of these words in the negative prompt significantly reduces the original DETECT score from 30 to 4. Notably, the addition of negative prompts does not significantly impair generated image's consistency with user's intended prompt, as the CONS scores typically remain similar or only slightly lower compared to the no intervention setting, but still substantially above 0.33, the value which indicates very high consistency (see \u00a7 C.4). Fig. 5 and Fig. 12 (in \u00a7 E.4) visualize some qualitative examples. \n\nCombining prompt rewriting and negative prompts shows promise for elimination of similar output. Finally, we combine prompt rewriting and negative prompts. Specifically, we send the rewritten prompts as inputs to the image generation models. Then we apply negative prompts during generation. Surprisingly, as demonstrated in Tb. 2, this simple technique is already quite promising in alleviating copyright concerns and is effective across all open-source models evaluated. \u2020 \u2020 The number of detected copyrighted characters is significantly reduced for all models. Notably, the number of detection decreases to only 5% of the original in the case of DeepFloyd. At the same time, the CONS scores remain mostly stable. This suggests that despite the pressing concern of image generation models generating copyrighted characters, we can use this simple yet effective method for meaningful mitigation. Fig. 5 and Fig. 13 (in \u00a7 E.4) present some examples. As shown, most generated images still align with the user's intent, keeping key characteristics as the requested copyrighted character, but the generation result is already drastically different from the requested copyrighted characters. \n\nWe show statistical significance of our highlighted intervention's effect on reducing DETECT in Tb. 3. Nonetheless, even this combination of strategies is not perfect at stopping the generation of copyrighted characters, which calls for more future research efforts.",
            "score": 0.45068283269867854,
            "section_title": "MITIGATION EFFECTIVENESS",
            "char_start_offset": 23866,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 518
                },
                {
                    "start": 521,
                    "end": 617
                },
                {
                    "start": 618,
                    "end": 676
                },
                {
                    "start": 677,
                    "end": 762
                },
                {
                    "start": 763,
                    "end": 812
                },
                {
                    "start": 813,
                    "end": 993
                },
                {
                    "start": 994,
                    "end": 1084
                },
                {
                    "start": 1085,
                    "end": 1180
                },
                {
                    "start": 1181,
                    "end": 1236
                },
                {
                    "start": 1237,
                    "end": 1417
                },
                {
                    "start": 1418,
                    "end": 1470
                },
                {
                    "start": 1471,
                    "end": 1708
                },
                {
                    "start": 1711,
                    "end": 1977
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.9375
        },
        {
            "corpus_id": "258240392",
            "title": "Generative AI",
            "text": "Some scholars even attest to models certain moral self-correcting capabilities (Ganguli et al., 2023), which may attenuate concerns of embedded biases and result in more fairness. In addition, on the system and application level, mitigation mechanisms can be implemented to address biases embedded in the deep learning models and create more diverse outputs (e.g., updating the prompts \"under the hood\" as done by Dall-E 2 to increase the demographic diversity of the outputs). Yet, more research is needed to get closer to the notion of fair AI. \n\nCopyright violation. Generative AI models, systems, and applications may cause a violation of copyright laws because they can produce outputs that resemble or even copy existing works without permission or compensation to the original creators (Smits and Borghuis, 2022). Here, two potential infringement risks are common. On the one hand, generative AI may make illegal copies of a work, thus violating the reproduction right of creators. Among others, this may happen when a generative AI was trained on original content that is protected by copyright but where the generative AI produces copies. \n\nHence, a typical implication is that the training data for building generative AI systems must be free of copyrights. Crucially, copyright violation may nevertheless still happen even when the generative AI has never seen a copyrighted work before, such as, for example, when it simply produces a trademarked logo similar to that of Adidas but without having never seen that logo before. On the other hand, generative AI may prepare derivative works, thus violating the transformation right of creators. To this end, legal questions arise around the balance of originality and creativity in generative AI systems. Along these lines, legal questions also arise around who holds the intellectual property for works (including patents) produced by a generative AI. \n\nEnvironmental concerns. Lastly, there are substantial environmental concerns from developing and using generative AI systems due to the fact that such systems are typically built around large-scale neural networks, and, therefore, their development and operation consume large amounts of electricity with immense negative carbon footprint (Schwartz et al., 2020).",
            "score": 0.44983582886452234,
            "section_title": "Limitations of Current Generative AI",
            "char_start_offset": 28930,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 179
                },
                {
                    "start": 180,
                    "end": 477
                },
                {
                    "start": 478,
                    "end": 546
                },
                {
                    "start": 549,
                    "end": 569
                },
                {
                    "start": 570,
                    "end": 820
                },
                {
                    "start": 821,
                    "end": 871
                },
                {
                    "start": 872,
                    "end": 988
                },
                {
                    "start": 989,
                    "end": 1147
                },
                {
                    "start": 1150,
                    "end": 1267
                },
                {
                    "start": 1268,
                    "end": 1537
                },
                {
                    "start": 1538,
                    "end": 1653
                },
                {
                    "start": 1654,
                    "end": 1763
                },
                {
                    "start": 1764,
                    "end": 1911
                },
                {
                    "start": 1914,
                    "end": 1937
                },
                {
                    "start": 1938,
                    "end": 2277
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.66455078125
        },
        {
            "corpus_id": "268297245",
            "title": "Sora as an AGI World Model? A Complete Survey on Text-to-Video Generation",
            "text": "A common approach in massive data collection to train scalable text-to-video generation models is to scrape internet data. This data contains many personal identifications whose owner may or may not intend to spread. If the generation model suffers from a data memorization issue, someone else's face may surface in the generated video, leaking the privacy of a particular individual [131]. Further, the copyright definition in generative AI is still obscure. If someone else's work accidentally appears in the generated content, it is still uncertain whether the user of generative AI model can be considered to infringe the original artist's copyright. This is a dilemma because the AI user also contributes his creative thinking to designing prompts that could engineer such an artwork [131].",
            "score": 0.44929792427673443,
            "section_title": "Privacy and Copyright.",
            "char_start_offset": 60394,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 122
                },
                {
                    "start": 123,
                    "end": 216
                },
                {
                    "start": 217,
                    "end": 390
                },
                {
                    "start": 391,
                    "end": 459
                },
                {
                    "start": 460,
                    "end": 654
                },
                {
                    "start": 655,
                    "end": 795
                }
            ],
            "ref_mentions": [
                {
                    "start": 384,
                    "end": 389,
                    "matchedPaperCorpusId": "257280234"
                },
                {
                    "start": 789,
                    "end": 794,
                    "matchedPaperCorpusId": "257280234"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.46826171875
        },
        {
            "corpus_id": "269484238",
            "title": "MMTryon: Multi-Modal Multi-Reference Control for High-Quality Fashion Generation",
            "text": "As most generative approaches, MMTryon can be used for malicious purposes by generating images that infringe upon copyrights and/or privacy. Given these considerations, responsible use of the model is advocated. \n\nA.4 Limitation and future work. \n\nWhile our method demonstrates strong performance, it still has certain limitations. In the data generation process, our method is influenced by the limitations of pretrained models, making it challenging to produce data that meets the requirements for very fine parts, such as cuffs and collars. This restricts our ability to generate detailed components. Moving forward, we may focus on finetuning large models to construct a more freely detailed and fine-grained dataset, aiming to enhance the upper limit of our model.",
            "score": 0.44898762652019203,
            "section_title": "A.3 Societal impact.",
            "char_start_offset": 26306,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 140
                },
                {
                    "start": 141,
                    "end": 211
                },
                {
                    "start": 214,
                    "end": 245
                },
                {
                    "start": 248,
                    "end": 331
                },
                {
                    "start": 332,
                    "end": 543
                },
                {
                    "start": 544,
                    "end": 603
                },
                {
                    "start": 604,
                    "end": 769
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.5966796875
        },
        {
            "corpus_id": "276576186",
            "title": "Interrogating LLM design under a fair learning doctrine",
            "text": "It is not necessary because models could still infringe even if their outputs are never similar to copyrighted materials, so long as copying of copyrighted materials occurred when preprocessing training data (which is universally agreed) or potentially if the models themselves implicitly contain copies of copyrighted materials. Behavioral analysis is also not sufficient because transformer-based language models are expressive generation models which assign non-zero probability to every piece of text [37]. This makes them susceptible to adversarial prompting [61,73], and with the right prompt, they can generate any piece of text; thus the ability to extract copyrighted text from a LLM cannot be taken as per se copyright infringement on its own, to which OpenAI has claimed in their lawsuits [7]. The key legal question is not whether the outputs are substantially similar, but whether the fair use exception applies. \n\nStructuralism. The second approach focuses on the structure, or design, of a generative model, to ensure that the model is learning and memorizing only non-expressive facts and ideas from its training data. Normatively, it may be desirable for model developers to deploy models that are copyright safe by design [20], which requires developers to make careful decisions during training. These decisions can include which datasets are selected, how they were filtered, whether they were upweighted, and also the use of memorization-reduction techniques such as differential privacy [9]. \n\nThe study of memorization is an active research area [33] which has begun to understand the effects of basic design decisions on memorization [39,69]. These design decisions matter, as generative models can differ significantly in their capacity to memorize training data, presenting varying levels of copyright risk [44,58]. Training decisions can be systematically evaluated and optimized, and the machine learning literature presents many examples of rigorously studying LLM design decisions for the purposes of improving performance [45,52]. How then could we formalize a Manuscript submitted to ACM notion of \"fair learning\" [48]? This is the heart of our inquiry, where we provide an operationalization of \"fair learning\" and relate it to judicial adjudication.",
            "score": 0.4489049028135969,
            "section_title": "Behavioral and structural perspectives",
            "char_start_offset": 11133,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 329
                },
                {
                    "start": 330,
                    "end": 510
                },
                {
                    "start": 511,
                    "end": 804
                },
                {
                    "start": 805,
                    "end": 925
                },
                {
                    "start": 928,
                    "end": 942
                },
                {
                    "start": 943,
                    "end": 1134
                },
                {
                    "start": 1135,
                    "end": 1314
                },
                {
                    "start": 1315,
                    "end": 1513
                },
                {
                    "start": 1516,
                    "end": 1666
                },
                {
                    "start": 1667,
                    "end": 1841
                },
                {
                    "start": 1842,
                    "end": 2061
                },
                {
                    "start": 2062,
                    "end": 2151
                },
                {
                    "start": 2152,
                    "end": 2283
                }
            ],
            "ref_mentions": [
                {
                    "start": 568,
                    "end": 571,
                    "matchedPaperCorpusId": "201698258"
                },
                {
                    "start": 1240,
                    "end": 1244,
                    "matchedPaperCorpusId": "149765855"
                },
                {
                    "start": 1509,
                    "end": 1512,
                    "matchedPaperCorpusId": "170076423"
                },
                {
                    "start": 1658,
                    "end": 1662,
                    "matchedPaperCorpusId": "246823128"
                },
                {
                    "start": 1662,
                    "end": 1665,
                    "matchedPaperCorpusId": "248986465"
                },
                {
                    "start": 1837,
                    "end": 1840,
                    "matchedPaperCorpusId": "258515384"
                },
                {
                    "start": 2053,
                    "end": 2057,
                    "matchedPaperCorpusId": "235829052"
                },
                {
                    "start": 2057,
                    "end": 2060,
                    "matchedPaperCorpusId": "258832491"
                },
                {
                    "start": 2146,
                    "end": 2150,
                    "matchedPaperCorpusId": "219342558"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.5966796875
        },
        {
            "corpus_id": "258967413",
            "title": "Alteration-free and Model-agnostic Origin Attribution of Generated Images",
            "text": "In recent years, there has been a rapid evolution in image generation techniques. With the advances in visual generative models, images can now be easily created with high quality and diversity [1][2][3][4]. There are three important milestones in the field of image generation and manipulation, i.e., Generative Adversarial Networks (GAN) [5], Variational AutoEncoders (VAE) [6], and diffusion models [7]. Various image generation models are built based on these three models [8][9][10][11][12][13] to make the AI-generated images more realistic. \n\nWith its wide adoption, the security and privacy of machine learning models becomes critical [14][15][16][17][18][19][20][21]. One severe and important issue is the potential misuse and intellectual property (IP) infringement of image generation models [16,20]. Users may generate malicious images containing inappropriate or biased content using these models and distribute them online. Furthermore, trained models may be used without authorization, violating the model owner's intellectual property. For example, malicious users may steal the model's parameters file and use it for commercial purposes. Others may create AI-generated images and falsely claim them as their own artwork (e.g., photos and paintings) to gain recognition, which also violates the model's IP. Therefore, it is essential to track the origin of AI-generated images. The origin attribution problem is to identify whether a specific image is generated by a particular model. As shown in Fig. 1, assuming we have a model M 1 and its generated images, the origin attribution algorithm's objective is to flag an image as belonging to model M 1 if it was generated by that model. On the other hand, the algorithm should consider the image as non-belonging if it was created by other models (e.g., M 2 in Fig. 1) or if it is a real image. One existing way to infer the source of specific images is image watermarking.",
            "score": 0.4484848466634409,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 81
                },
                {
                    "start": 82,
                    "end": 207
                },
                {
                    "start": 208,
                    "end": 406
                },
                {
                    "start": 407,
                    "end": 547
                },
                {
                    "start": 550,
                    "end": 676
                },
                {
                    "start": 677,
                    "end": 811
                },
                {
                    "start": 812,
                    "end": 937
                },
                {
                    "start": 938,
                    "end": 1051
                },
                {
                    "start": 1052,
                    "end": 1154
                },
                {
                    "start": 1155,
                    "end": 1322
                },
                {
                    "start": 1323,
                    "end": 1393
                },
                {
                    "start": 1394,
                    "end": 1500
                },
                {
                    "start": 1501,
                    "end": 1701
                },
                {
                    "start": 1702,
                    "end": 1859
                },
                {
                    "start": 1860,
                    "end": 1938
                }
            ],
            "ref_mentions": [
                {
                    "start": 200,
                    "end": 203,
                    "matchedPaperCorpusId": "245335280"
                },
                {
                    "start": 340,
                    "end": 343,
                    "matchedPaperCorpusId": "10319744"
                },
                {
                    "start": 402,
                    "end": 405,
                    "matchedPaperCorpusId": "219955663"
                },
                {
                    "start": 477,
                    "end": 480,
                    "matchedPaperCorpusId": "54482423"
                },
                {
                    "start": 480,
                    "end": 483,
                    "matchedPaperCorpusId": "209202273"
                },
                {
                    "start": 483,
                    "end": 487,
                    "matchedPaperCorpusId": "219636053"
                },
                {
                    "start": 491,
                    "end": 495,
                    "matchedPaperCorpusId": "214743564"
                },
                {
                    "start": 651,
                    "end": 655,
                    "matchedPaperCorpusId": "232105096"
                },
                {
                    "start": 659,
                    "end": 663,
                    "matchedPaperCorpusId": "221203089"
                },
                {
                    "start": 667,
                    "end": 671,
                    "matchedPaperCorpusId": "235692919"
                },
                {
                    "start": 671,
                    "end": 675,
                    "matchedPaperCorpusId": "249097522"
                },
                {
                    "start": 803,
                    "end": 807,
                    "matchedPaperCorpusId": "232105096"
                },
                {
                    "start": 807,
                    "end": 810,
                    "matchedPaperCorpusId": "235692919"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.9736328125
        },
        {
            "corpus_id": "270258236",
            "title": "Tackling copyright issues in AI image generation through originality estimation and genericization",
            "text": "The rapid progress of generative AI technology has sparked significant copyright concerns, leading to numerous lawsuits filed against AI developers. Notably, generative AI\u2019s capacity for generating images of copyrighted characters has been well documented in the literature, and while various techniques for mitigating copyright issues have been studied, significant risks remain. Here, we propose a genericization method that modifies the outputs of a generative model to make them more generic and less likely to imitate distinctive features of copyrighted materials. To achieve this, we introduce a metric for quantifying the level of originality of data, estimated by drawing samples from a generative model, and applied in the genericization process. As a practical implementation, we introduce PREGen (Prompt Rewriting-Enhanced Genericization), which combines our genericization method with an existing mitigation technique. Compared to the existing method, PREGen reduces the likelihood of generating copyrighted characters by more than half when the names of copyrighted characters are used as the prompt. Additionally, while generative models can produce copyrighted characters even when their names are not directly mentioned in the prompt, PREGen almost entirely prevents the generation of such characters in these cases. Ultimately, this study advances computational approaches for quantifying and strengthening copyright protection, thereby providing practical methodologies to promote responsible generative AI development.",
            "score": 0.4483924175418692,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.97412109375
        },
        {
            "corpus_id": "274436153",
            "title": "Continuous Concepts Removal in Text-to-image Diffusion Models",
            "text": "Advancements in Artificial Intelligence Generated Contents (AIGCs) [47] have revolutionized the field of image synthesis [29,38,49], among which text-to-image diffusion models enable the creation of high-quality images from textual descriptions [32,52]. However, this progress has also raised significant concerns regarding the potential misuse of these models [8,13,36,43]. Such misuse includes generating content that infringes on copyrights, such as mimicking specific artistic styles [30], intellectual properties [45,46], or creating disturbing and improper subject matter, including eroticism and violence [33]. Addressing these issues necessitates continuously removing those improper concepts from these models to prevent misuse and protect copyright from infringement. \n\nExisting techniques aiming to remove concepts from the text-to-image diffusion models can be categorized into two types. For a given concept that needs to be removed, the first group of methods refines the training data by discarding images containing the undesired concept and then retrains the model from scratch [3, 25,34]. The other set of methods removes the target concept without requiring full retraining. These methods instead utilize a small amount of additional data to fine-tune the models and modify specific neurons [10][11][12]. In real-world scenario, the improper concepts learned by the models such as copyright-protected art styles often discovered by the model owner in a continuous manner. For example, various artists may continually raise complaints that text-to-image generative AI can replicate their distinctive art style. Additionally, users or red-teaming teams [9] of these models may continuously flag instances where the models generate harmful or malicious contents. However, we find that these existing techniques do not perform well in scenarios where different concepts need to be continuously removed one after another, which is a practical and important use case. In detail, we observe that training data filtering methods require model owners to retrain the model from scratch, which is deemed impractical due to its exorbitant cost.",
            "score": 0.44817580954474456,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 253
                },
                {
                    "start": 254,
                    "end": 374
                },
                {
                    "start": 375,
                    "end": 617
                },
                {
                    "start": 618,
                    "end": 777
                },
                {
                    "start": 780,
                    "end": 900
                },
                {
                    "start": 901,
                    "end": 1106
                },
                {
                    "start": 1107,
                    "end": 1193
                },
                {
                    "start": 1194,
                    "end": 1323
                },
                {
                    "start": 1324,
                    "end": 1490
                },
                {
                    "start": 1491,
                    "end": 1628
                },
                {
                    "start": 1629,
                    "end": 1778
                },
                {
                    "start": 1779,
                    "end": 1980
                },
                {
                    "start": 1981,
                    "end": 2151
                }
            ],
            "ref_mentions": [
                {
                    "start": 121,
                    "end": 125,
                    "matchedPaperCorpusId": "245335280"
                },
                {
                    "start": 128,
                    "end": 131,
                    "matchedPaperCorpusId": "258959002"
                },
                {
                    "start": 245,
                    "end": 249,
                    "matchedPaperCorpusId": "248986576"
                },
                {
                    "start": 249,
                    "end": 252,
                    "matchedPaperCorpusId": "256827727"
                },
                {
                    "start": 361,
                    "end": 364,
                    "matchedPaperCorpusId": "256389993"
                },
                {
                    "start": 367,
                    "end": 370,
                    "matchedPaperCorpusId": "255546643"
                },
                {
                    "start": 612,
                    "end": 616,
                    "matchedPaperCorpusId": "253420366"
                },
                {
                    "start": 1102,
                    "end": 1105,
                    "matchedPaperCorpusId": "252917726"
                },
                {
                    "start": 1314,
                    "end": 1318,
                    "matchedPaperCorpusId": "257495777"
                },
                {
                    "start": 1318,
                    "end": 1322,
                    "matchedPaperCorpusId": "261276613"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.98095703125
        },
        {
            "corpus_id": "269294137",
            "title": "An Economic Solution to Copyright Challenges of Generative AI",
            "text": "Recent advancements in generative artificial intelligence (AI) have profoundly impacted the creative industries, ushering in an era of AI-generated content in literature, visual arts, and music. Trained on vast datasets of human-generated material, generative AI models such as large language models and diffusion models can now produce content with a sophistication that rivals-and may potentially displace-the works of human artists [30,2,16]. This burgeoning capability raises crucial questions about the legal and ethical boundaries of creative authorship, particularly concerning copyright infringement by generative models [32,34]. Consequently, several AI companies are currently involved in lawsuits over allegations of producing content that potentially infringes on copyrights [34,13]. \n\nEfforts to mitigate the tension between owners of copyright in the training data and AI developers have emerged, mostly involving modifications to generative model training or inference to reduce the likelihood of generating infringing outputs [37,5,35,15,4]. However, these modifications may compromise model performance due to either the exclusion of high-quality, copyrighted training data from training or restrictions on content generation [22]. The complexity and ambiguity of copyright law add another layer of difficulty, blurring the line between infringing and non-infringing outputs. The resulting uncertainty could lead to a significant waste of resources on both sides while these issues are debated in courts [34]. \n\nRather than restricting AI developers' use of copyrighted data, we propose establishing a mutually beneficial revenue-sharing agreement between AI developers and copyright owners. This proposal echoes an argument recently advocated in economics [1]. However, a major challenge in developing a revenue-sharing model for generative AI, in contrast to conventional cases of sharing between digital platforms and independent content creators [7], lies in the complexity of training generative models on diverse data sources. This results in the \"black-box\" nature of model training and content generation, making the traditional, straightforward pro rata methods unsuitable [24]. \n\nIn this paper, we introduce a simple framework that appropriately compensates copyright owners for using their copyrighted data in training generative AI systems based on the cooperative game theory, thereby directly addressing the intricacies of copyright challenges.",
            "score": 0.4478564092677707,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 194
                },
                {
                    "start": 195,
                    "end": 445
                },
                {
                    "start": 446,
                    "end": 637
                },
                {
                    "start": 638,
                    "end": 795
                },
                {
                    "start": 798,
                    "end": 1057
                },
                {
                    "start": 1058,
                    "end": 1248
                },
                {
                    "start": 1249,
                    "end": 1392
                },
                {
                    "start": 1393,
                    "end": 1526
                },
                {
                    "start": 1529,
                    "end": 1708
                },
                {
                    "start": 1709,
                    "end": 1778
                },
                {
                    "start": 1779,
                    "end": 2049
                },
                {
                    "start": 2050,
                    "end": 2204
                },
                {
                    "start": 2207,
                    "end": 2475
                }
            ],
            "ref_mentions": [
                {
                    "start": 435,
                    "end": 439,
                    "matchedPaperCorpusId": "245335280"
                },
                {
                    "start": 629,
                    "end": 633,
                    "matchedPaperCorpusId": "258515384"
                },
                {
                    "start": 633,
                    "end": 636,
                    "matchedPaperCorpusId": "259844568"
                },
                {
                    "start": 787,
                    "end": 791,
                    "matchedPaperCorpusId": "259844568"
                },
                {
                    "start": 1521,
                    "end": 1525,
                    "matchedPaperCorpusId": "259844568"
                },
                {
                    "start": 2199,
                    "end": 2203,
                    "matchedPaperCorpusId": "257149150"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.9228515625
        },
        {
            "corpus_id": "265066820",
            "title": "Robust Retraining-free GAN Fingerprinting via Personalized Normalization",
            "text": "Synthetic image generation has made significant progress in recent years and generative models are now widely used in commercial applications. These models are provided to commercial users as production tools or for selling services. Protecting the Intellectual Property Rights (IPR) of model owners has become a pressing issue to avoid potential copyright infringements, such as unauthorized duplication or model theft, when these models are delivered to malicious users. Deep Neural Network (DNN) watermarking has been proposed as a solution to protect the IPR associated with DNN models [1]. Most DNN watermarking methods focus on the protection of discriminative models, namely, networks developed for classification tasks, and less attention is paid to generative models. Yet, some methods for the watermarking of generative models have started appearing recently. Given the large entropy of the output of generative models, the watermark can be directly extracted from the output produced by the model, thus permitting to verification of the watermark in a so-called box-free setting. In this way, it is possible to determine the source of images produced by generative models and associate any image to the generative model that produced it [2]. \n\nWith the exception of a few scattered works [3], the existing approaches for the watermarking of generative models, notably GANs, are designed for ownership verification, aiming at making it possible to retrieve the model authorship information from the generated images. These methods embed a fixed watermark, linking the model Zhihua Xia is the corresponding author. to the owner, and require retraining or finetuning if different a watermark has to be embedded in the model. \n\nIn this paper, we focus on a different scenario, hereafter referred to as GAN fingerprinting, illustrated in Fig. 1. In this scenario, the model distributor, simply referred to as the model owner, releases distinct watermarked model instances to different users, in such a way that the user-specific fingerprint can be recovered from the images produced by these models for copyright authentication and to trace back to the guilty user in case of a violation of the license agreements (traitor tracing).",
            "score": 0.4468025564525281,
            "section_title": "I. INTRODUCTION",
            "char_start_offset": 18,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 142
                },
                {
                    "start": 143,
                    "end": 233
                },
                {
                    "start": 234,
                    "end": 472
                },
                {
                    "start": 473,
                    "end": 594
                },
                {
                    "start": 595,
                    "end": 776
                },
                {
                    "start": 777,
                    "end": 869
                },
                {
                    "start": 870,
                    "end": 1090
                },
                {
                    "start": 1091,
                    "end": 1252
                },
                {
                    "start": 1255,
                    "end": 1526
                },
                {
                    "start": 1527,
                    "end": 1623
                },
                {
                    "start": 1624,
                    "end": 1732
                },
                {
                    "start": 1735,
                    "end": 1851
                },
                {
                    "start": 1852,
                    "end": 2238
                }
            ],
            "ref_mentions": [
                {
                    "start": 590,
                    "end": 593,
                    "matchedPaperCorpusId": "235494966"
                },
                {
                    "start": 1248,
                    "end": 1251,
                    "matchedPaperCorpusId": "235692919"
                },
                {
                    "start": 1299,
                    "end": 1302,
                    "matchedPaperCorpusId": "229212743"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.98193359375
        },
        {
            "corpus_id": "270357944",
            "title": "Evaluating and Mitigating IP Infringement in Visual Generative AI",
            "text": "In this section, we present our approach to leveraging the powerful perception and understanding capabilities of large (vision-)language models for two specific purposes: first, to prevent name-based intellectual property infringement by blocking input prompts that directly request the generation of protected characters; and second, to detect potentially infringing outputs by analyzing the generated images themselves.\n\nName Blocking.With respect to the name-based intellectual property infringement, we implement measures to block the input prompts that contain the names of protected characters and directly instruct the AI models to generate images depicting those characters whose visual appearances are safeguarded.For instance, a prompt like \"Generate an image of Spider-Man.\" and \"An image of the Iron Man on a white horse.\"would be blocked.To defend against such infringing prompts, we employ  as the default LLM used in this paper.The prompt we use for this purpose, along with illustrative examples, can be found in Figure 10.\n\nOutput Infringement Detection.We leverage the multi-modal perception and understanding capabilities of large vision-language models to analyze both the textual prompt and the visual content (the generated image/video) to make an informed judgment about possible intellectual property violations in generated contents.Specifically, in this paper, we utilize GPT-4V(ision) [35] as the default vision-language model for this task.The process of infringement detection is illustrated through examples in Figure 11.Initially, we provide the vision-language model with a name list of all protected intellectual properties (in this case, the specific characters whose visual depictions or appearances are protected).We then supply the model with an image that requires assessment for potential infringement.Subsequently, we directly query the vision-language model, asking whether the provided image infringes upon the intellectual property rights of any of the protected characters listed earlier.The output response from the vision-language model is then used as the final determination for identifying potential infringement in the given image.",
            "score": 0.44514333260123323,
            "section_title": "Exploiting the Perception and Understanding Capability of L(V)LMs",
            "char_start_offset": 16877,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 421
                },
                {
                    "start": 423,
                    "end": 437
                },
                {
                    "start": 437,
                    "end": 723
                },
                {
                    "start": 723,
                    "end": 834
                },
                {
                    "start": 834,
                    "end": 851
                },
                {
                    "start": 851,
                    "end": 943
                },
                {
                    "start": 943,
                    "end": 1039
                },
                {
                    "start": 1041,
                    "end": 1071
                },
                {
                    "start": 1071,
                    "end": 1358
                },
                {
                    "start": 1358,
                    "end": 1468
                },
                {
                    "start": 1468,
                    "end": 1551
                },
                {
                    "start": 1551,
                    "end": 1750
                },
                {
                    "start": 1750,
                    "end": 1841
                },
                {
                    "start": 1841,
                    "end": 2032
                },
                {
                    "start": 2032,
                    "end": 2181
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.923828125
        },
        {
            "corpus_id": "260155285",
            "title": "Not with my name! Inferring artists' names of input strings employed by Diffusion Models",
            "text": "In this paper, a novel approach to estimate the probability of an artist's name being used in the input prompt of an image generated by a diffusion model, specifically DALL-E 2, was presented. Our approach aimed to address the concern of potential intellectual property infringement in the context of image generation, as the usage of an artist's name in the input string might imply that the diffusion model has learned from some or all of the artist's work, potentially violating their copyright. \n\nThis work employed metric learning for classification of an extremely limited number of authors but in future more sophisticated similarity measures, larger and more diverse datasets, and additional techniques to refine the estimation of the input string's content could be explored. Additionally, investigating the ethical implications and possible solutions to the challenges posed by AI-generated content in relation to copyright and intellectual property protection should be a priority for the research community.",
            "score": 0.4450570020193337,
            "section_title": "Conclusion and future works",
            "char_start_offset": 18875,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 192
                },
                {
                    "start": 193,
                    "end": 498
                },
                {
                    "start": 501,
                    "end": 784
                },
                {
                    "start": 785,
                    "end": 1019
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.9736328125
        },
        {
            "corpus_id": "261064676",
            "title": "Hey That's Mine Imperceptible Watermarks are Preserved in Diffusion Generated Outputs",
            "text": "Image generation has become a booming area of research, with many large image generation models becoming available to the general public and the commercialisation of such models becoming its own industry [RDN + 22, RBL + 22, ZA23, SCS + 22]. Progress in this area has introduced a number of new issues around the effortless creation of realistic images. Such issues include the proliferation of misinformation and the use of digital content without the creator's permission. With the rise of these ready to use text to image generators, high-quality fake images can be quickly produced on mass. \n\nTo help counter this, many investigations been undertaken in order to detect generated images. Such work either focuses on detecting generated images directly with DNN based classification [PMW + 20] or marking fake generated images with imperceptible watermarks that a suitable detector can recognise [YSAF21, ZPD + 23, FCJ + 23]. The first methods rely on there being some perceptible different between generated and real images and struggles when the generated images come from a model other then the one it was trained on. The second method is more robust as the diffusion model itself can be trained to produce images with imperceptible watermarks. However as the watermark is embedded after generation this method relies on the creator of such generative models adding these watermarks as well as making the detector available. \n\nThe issue of digital content being used without consent when training generative models is a much more difficult problem to solve. Many content creators regularly share their work online, making it easy for individuals or organisations to automatically collect vast quantities of high quality data without consent from the original creators. Even if one visibly watermarks an image, modern Deep learning techniques could automatically detect and remove these. Without access to the data-set that was used to train the generative model, or the parameters of the model itself, it can be very difficult to determine whether or not a given piece of digital content was used. The creators of these generative models could simply claim that any similarity between generated images and a given piece of content is a coincidence. This issue raises major concerns around copyright and intellectual property [HLJ + 23] and may cause independent creators to become reluctant to share their work, stifling innovation and the sharing of new ideas.",
            "score": 0.4450270646981551,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 241
                },
                {
                    "start": 242,
                    "end": 353
                },
                {
                    "start": 354,
                    "end": 474
                },
                {
                    "start": 475,
                    "end": 594
                },
                {
                    "start": 597,
                    "end": 691
                },
                {
                    "start": 692,
                    "end": 928
                },
                {
                    "start": 929,
                    "end": 1123
                },
                {
                    "start": 1124,
                    "end": 1250
                },
                {
                    "start": 1251,
                    "end": 1430
                },
                {
                    "start": 1433,
                    "end": 1563
                },
                {
                    "start": 1564,
                    "end": 1774
                },
                {
                    "start": 1775,
                    "end": 1892
                },
                {
                    "start": 1893,
                    "end": 2103
                },
                {
                    "start": 2104,
                    "end": 2254
                },
                {
                    "start": 2255,
                    "end": 2467
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.91552734375
        },
        {
            "corpus_id": "268691556",
            "title": "Not All Similarities Are Created Equal: Leveraging Data-Driven Biases to Inform GenAI Copyright Disputes",
            "text": "The emergence of Generative Artificial Intelligence (GenAI) models is fostering a radical transformation in the creative domain.By using GenAI mod- * Equal.els, non-professional users can now generate highquality content such as text, images, music, or code.At the same time, however, GenAI is also disrupting copyright law.By reducing the costs and talent barriers associated with creative production, GenAI has flooded markets with a massive yield of syntactic content, leaving scholars and copyright regulators to ponder whether to merit copyright protection to such works (Burk, 2023;Grimmelmann, 2016).The approach proposed in this paper can help the stakeholders grappling with this question by informing them of the scope of unprotected elements in copyrighted works.In addition, numerous lawsuits are emerging, claiming that GenAI models are \"21st-century collage tools\" that infringe the copyrighted works used to train these models v. Stability AI Ltd. (2023); J. Doe 1 v. GitHub (2022); Dogan and Liu (2005).These pending class action lawsuits will require courts to decide, first, whether unauthorized use of copyrighted works to train GenAI models constitutes copyright infringement and second, when does the output generated by GenAI models infringes the copyright of works included in the training dataset.Our paper only addresses the latter question.\n\nThese challenges put increasing pressure on the legal system to discern which portions of copyrighted works are protected from unauthorized use and which portions are not protected and are, therefore, available for others to use.Copyright law's ultimate goal is to foster the creation and dissemination of expressive works by granting authors only limited rights to their respective expressions (Leval, 1990).That is because creation never occurs in a vacuum, and the author always draws upon preexisting materials by adapting them into an original expression.Consequently, some elements of copyrighted works that are building blocks of further creation, such as style, ideas, functions, and methods, are not protected against unauthorized uses (Elkin-Koren et al., 2024).In fact, copyright law encourages users to draw upon these unprotected materials to create new original works of authorship (Litman, 1990).",
            "score": 0.4449613990284397,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 128
                },
                {
                    "start": 128,
                    "end": 156
                },
                {
                    "start": 156,
                    "end": 258
                },
                {
                    "start": 258,
                    "end": 324
                },
                {
                    "start": 324,
                    "end": 607
                },
                {
                    "start": 607,
                    "end": 774
                },
                {
                    "start": 774,
                    "end": 1019
                },
                {
                    "start": 1019,
                    "end": 1321
                },
                {
                    "start": 1321,
                    "end": 1366
                },
                {
                    "start": 1368,
                    "end": 1597
                },
                {
                    "start": 1597,
                    "end": 1777
                },
                {
                    "start": 1777,
                    "end": 1928
                },
                {
                    "start": 1928,
                    "end": 2140
                },
                {
                    "start": 2140,
                    "end": 2279
                }
            ],
            "ref_mentions": [
                {
                    "start": 576,
                    "end": 588,
                    "matchedPaperCorpusId": "257940624"
                },
                {
                    "start": 2113,
                    "end": 2139,
                    "matchedPaperCorpusId": "258865475"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.634765625
        },
        {
            "corpus_id": "268513090",
            "title": "CGI-DM: Digital Copyright Authentication for Diffusion Models via Contrasting Gradient Inversion",
            "text": "In this paper, we introduce a new method for copyright protection called copyright authentication.Our framework, CGI-DM, validates the use of training samples featuring vivid visual representation, serving as a tool for digital copyright authentication.We start by removing part of the input image.Then, using Monte Carlo sampling and PGD, we exploit the differences between the pretrained and fine-tuned model to recover the removed information.A high similarity between the recovered samples and the original input samples suggests a potential infringement.Through experiments on WikiArt and Dreambooth datasets, we demonstrate CGI-DM's robustness and effectiveness, surpassing alternative approaches.Such experimental results show that CGI-DM is adept at providing legal evidence for art style mimicry and unauthorized image fabrication.In conclusion, CGI-DM not only offers robust method for infringement validation in the evolving DM landscape but also pioneers the application of gradient inversion in generative models.",
            "score": 0.44460884127349026,
            "section_title": "Conclusion",
            "char_start_offset": 24731,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 98
                },
                {
                    "start": 98,
                    "end": 253
                },
                {
                    "start": 253,
                    "end": 298
                },
                {
                    "start": 298,
                    "end": 446
                },
                {
                    "start": 446,
                    "end": 559
                },
                {
                    "start": 559,
                    "end": 703
                },
                {
                    "start": 703,
                    "end": 840
                },
                {
                    "start": 840,
                    "end": 1026
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.9677734375
        },
        {
            "corpus_id": "267400526",
            "title": "AIGC (Artificial Intelligence Generated Content) infringes the copyright of human artists",
            "text": "This study employs literature analysis and walk-through methods to comprehensively examine the existing literature on Artificial Intelligence Generated Content (AIGC) and its implications on copyright issues. Additionally, it takes Stable Diffusion as a specific example to explore the inner workings of an artificial intelligence generation platform, establishing a solid foundation for further research in this domain. Through a meticulous investigation, the study identifies the conditions and methods by which AIGC may infringe on the rights of human artists, critically analyzing this problem in-depth. The insights from this research offer valuable practical experience for companies intending to integrate AI technology into their products. By understanding the potential pitfalls associated with AIGC, these companies can implement measures to prevent infringement on the rights of users and artists alike. Moreover, the findings raise awareness among future scholars and researchers regarding the ethical implications of AIGC and its impact on human artists' rights. \n\nRegarding the event, the AI-generated avatar function by Lofter was met significant resistance from numerous users. This case study reveals that the platform undeniably infringed upon the copyrights of authors who had shared their works on the forum. The unauthorized use of these works and the generation of derivative content that closely mimicked elements of their original works amounted to a violation of their creative rights. Moreover, the generated results lacked any attribution to the original authors, further exacerbating the issue of copyright infringement. \n\nIt is important to emphasize that resistance to all AI-generated content is not necessarily reasonable. As Stephen Hawking said, \"It is all right to make mistakes; nothing is perfect because without perfection, we would not exist.\" AI will play an increasingly significant role in creative fields as technology advances. However, addressing the challenges and potential issues arising from using AI in content generation is imperative. Recognizing that no technology is flawless, the study advocates for a balanced approach that acknowledges both the benefits and challenges associated with AIGC. \n\nBy paying attention to the multifaceted aspects of AIGC, stakeholders can foster an environment that promotes responsible AI development and usage. As the digwatch (2023) said, Industry leaders partner to encourage accountable AI development [13]. This entails ensuring that adequate measures are in place to protect the rights of content creators, preventing unauthorized use of copyrighted material, and promoting fair attribution practices in AI-generated content.",
            "score": 0.44384164265411585,
            "section_title": "Conclusion",
            "char_start_offset": 22170,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 208
                },
                {
                    "start": 209,
                    "end": 420
                },
                {
                    "start": 421,
                    "end": 607
                },
                {
                    "start": 608,
                    "end": 747
                },
                {
                    "start": 748,
                    "end": 914
                },
                {
                    "start": 915,
                    "end": 1075
                },
                {
                    "start": 1078,
                    "end": 1193
                },
                {
                    "start": 1194,
                    "end": 1328
                },
                {
                    "start": 1329,
                    "end": 1510
                },
                {
                    "start": 1511,
                    "end": 1648
                },
                {
                    "start": 1651,
                    "end": 1754
                },
                {
                    "start": 1755,
                    "end": 1882
                },
                {
                    "start": 1883,
                    "end": 1971
                },
                {
                    "start": 1972,
                    "end": 2086
                },
                {
                    "start": 2087,
                    "end": 2247
                },
                {
                    "start": 2250,
                    "end": 2397
                },
                {
                    "start": 2398,
                    "end": 2497
                },
                {
                    "start": 2498,
                    "end": 2717
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.51220703125
        },
        {
            "corpus_id": "271064292",
            "title": "CopyBench: Measuring Literal and Non-Literal Reproduction of Copyright-Protected Text in Language Model Generation",
            "text": "In this section, we review copyright law and relevant court cases on copyright infringement (Section 2.1), as well as prior work in AI on benchmarking and mitigating copyright risks (Section 2.2). We highlight the gap between real-world legal risks and the current research aimed at addressing potential copyright issues. \n\nCopyright issues can be associated with each component of the generative-AI supply chain (Lee et al., 2023b), including data collection (Min et al., 2023;Shi et al., 2023;Chang et al., 2023;Karamolegkou et al., 2023), model training (Vyas et al., 2023), and generation and deployment (Meeus et al., 2024;Ippolito et al., 2023). Our work focuses on the infringement risks in LMgenerated content, although other stages may also present infringement risks even if the outputs do not infringe.",
            "score": 0.4437955880362479,
            "section_title": "Background",
            "char_start_offset": 5113,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 196
                },
                {
                    "start": 197,
                    "end": 321
                },
                {
                    "start": 324,
                    "end": 651
                },
                {
                    "start": 652,
                    "end": 813
                }
            ],
            "ref_mentions": [
                {
                    "start": 557,
                    "end": 576,
                    "matchedPaperCorpusId": "257050406"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.68798828125
        },
        {
            "corpus_id": "259064265",
            "title": "DeepfakeArt Challenge: A Benchmark Dataset for Generative AI Art Forgery and Data Poisoning Detection",
            "text": "In the case of art forgery, the goal of the proposed DeepfakeArt Challenge dataset is about the identification of copyright infringement, and as such it is important to set the context along with a formal definition. Specifically, generative AI models typically utilize vast amounts of training data to achieve strong content generation performance. However, many of the large-scale datasets used for training such as the LAION datasets Schuhmann et al. [2021] contain significant amounts of copyright-protected content. Hence, generative AI models trained on such datasets can produce synthetic content that contains or highly resembles copyrightprotected content Somepalli et al. [2022]. \n\nIn judicial proceedings, the \"substantial similarity\" test is employed to ascertain if a party has violated another party's copyright Asay [2022]. However, determining substantial similarity remains a subject of ongoing debate due to its inherently open-ended nature. While there's no precise formula, courts do follow certain criteria to arrive at a decision. In particular, the substantial similarity doctrine in copyright law necessitates that a court confirm the defendant's replication is both quantitatively and qualitatively similar enough to the original to warrant classification as infringement Balganesh et al. [2014]. As discussed in Balganesh et al. [2014], the jury evaluates the evidence based on several factors, including the creativity exhibited, the work's creation process, its intended purpose, the defendant's creative contributions and conduct, and the market impacts of the defendant's copying. \n\nBuilding on the principles inherent in the \"substantial similarity test\" used in legal proceedings, let us establish a pragmatic definition of copyright infringement as it applies to generative AI and as applied in the creation of the proposed DeepfakeArt Challenge dataset. This definition is rooted in the exploration and consideration of two foundational conditions. \n\nDefinition: Copyright Infringement in a Generative AI Model -Consider a generative model M : R H\u00d7W \u00d7C \u00d7 Q \u2192 R H \u2032 \u00d7W \u2032 \u00d7C \u2032 where Q could represent the set of any additional input to the generative model.",
            "score": 0.44306153579069246,
            "section_title": "Art Forgery",
            "char_start_offset": 5607,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 216
                },
                {
                    "start": 217,
                    "end": 349
                },
                {
                    "start": 350,
                    "end": 520
                },
                {
                    "start": 521,
                    "end": 689
                },
                {
                    "start": 692,
                    "end": 838
                },
                {
                    "start": 839,
                    "end": 959
                },
                {
                    "start": 960,
                    "end": 1052
                },
                {
                    "start": 1053,
                    "end": 1321
                },
                {
                    "start": 1322,
                    "end": 1610
                },
                {
                    "start": 1613,
                    "end": 1887
                },
                {
                    "start": 1888,
                    "end": 1982
                },
                {
                    "start": 1985,
                    "end": 2189
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.9072265625
        },
        {
            "corpus_id": "278338968",
            "title": "Safer Prompts: Reducing IP Risk in Visual Generative AI",
            "text": "Visual Generative AI models have demonstrated remarkable capability in generating high-quality images from simple inputs like text prompts. However, because these models are trained on images from diverse sources, they risk memorizing and reproducing specific content, raising concerns about intellectual property (IP) infringement. Recent advances in prompt engineering offer a cost-effective way to enhance generative AI performance. In this paper, we evaluate the effectiveness of prompt engineering techniques in mitigating IP infringement risks in image generation. Our findings show that Chain of Thought Prompting and Task Instruction Prompting significantly reduce the similarity between generated images and the training data of diffusion models, thereby lowering the risk of IP infringement.",
            "score": 0.4421563982971254,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.97998046875
        },
        {
            "corpus_id": "269033217",
            "title": "Disguised Copyright Infringement of Latent Diffusion Models",
            "text": "In this paper, we adopt a simple algorithm to disguise images, which we realize may be applied as a concealed copyright infringement tool.We are surprised at how easy it is for Generative AI to circumvent the copyright law, which was originally designed for regulating human scrapers.This seemingly undetectable infringing threat may bring disadvantages for human artists, who are already losing the battle with GenAI.Furthermore, disguised images may not only be a concern to copyright owners but also a threat to Generative AI companies: if these disguises are accidentally collected as part of the training data, copyright infringement could be triggered unconsciously and unwillingly.As computer scientists, we are obligated to reveal the existence of such possible unlawful acts and provide technical tools to identify such behaviors.We expect our paper to be a prudent tool such that disguised copyright infringement and its corresponding detection methods are recognized by law experts.\n\nconference on computer vision and pattern recognition, pp.10684-10695.",
            "score": 0.441733894272961,
            "section_title": "Impact Statement",
            "char_start_offset": 27706,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 138
                },
                {
                    "start": 138,
                    "end": 284
                },
                {
                    "start": 284,
                    "end": 418
                },
                {
                    "start": 418,
                    "end": 688
                },
                {
                    "start": 688,
                    "end": 839
                },
                {
                    "start": 839,
                    "end": 993
                },
                {
                    "start": 995,
                    "end": 1053
                },
                {
                    "start": 1053,
                    "end": 1065
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.96728515625
        },
        {
            "corpus_id": "271329215",
            "title": "SpotDiffusion: A Fast Approach for Seamless Panorama Generation Over Time",
            "text": "Generative image models can be misused to create deepfakes, infringe on copyrights, and produce biased images. These problems can lead to fake news, privacy invasion, and reinforcing stereotypes. Our method builds on existing foundational models and thus shares these risks. To reduce them, it's important to develop better deepfake detection, protect intellectual property, and follow ethical guidelines. Labeling synthetic content and learning to use generative models responsibly is essential. By focusing on these protective measures, we can use generative models effectively while minimizing their negative impacts.",
            "score": 0.4415062266497616,
            "section_title": "Societal Impact",
            "char_start_offset": 21819,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 110
                },
                {
                    "start": 111,
                    "end": 195
                },
                {
                    "start": 196,
                    "end": 274
                },
                {
                    "start": 275,
                    "end": 405
                },
                {
                    "start": 406,
                    "end": 496
                },
                {
                    "start": 497,
                    "end": 620
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.9208984375
        },
        {
            "corpus_id": "274131822",
            "title": "Generating Compositional Scenes via Text-to-image RGBA Instance Generation",
            "text": "Our work contributes positively to several domains. Artists, designers, and content creators can benefit from more precise control over their generated images, enhancing creativity and productivity. Additionally, the technology can be used in educational tools to help students and professionals learn about design, art, and computer graphics. Improved text-to-image generation can also make graphic design more accessible to individuals without extensive training in the field. \n\nHowever, as with any generative model, there is a risk that the technology could be used to create disinformation, generate fake profiles, or produce harmful content. There is also a risk that generated images could be used to infringe on personal privacy, particularly if the model is misused to create realistic but fake images of individuals. Furthermore, the generated images could be used in ways that compromise security, such as creating counterfeit documents or misleading imagery for fraud. \n\nTo mitigate these risks, we propose several safeguarding measures. Releasing the model in a controlled manner can ensure that only verified and responsible users have access to it. Conducting thorough audits to ensure that the model does not perpetuate or amplify existing biases is also important. Maintaining transparency about the capabilities and limitations of the model, and establishing accountability protocols for its use, will help manage its impact. Providing resources and guidelines for ethical use, along with training for users on the potential societal impacts of the technology, can further promote responsible usage. Finally, developing features that can detect and prevent the creation of malicious content, such as watermarking techniques and usage restrictions, will enhance security measures. \n\nAlgorithm 1: Multi-layer scene composition \n\nT generation timesteps, Random noise \u03b7 \u2208 N (0, I) Number of blending timesteps n, background blending timesteps b, cross-layer consistency timesteps n s Output: K + 1 multi-layer images (background, K images with increasing number of instances) \n\n\u25b7 cross-layer consistency optional step For k = 0 to K: \n\nRGBA generation. Our RGBA generator is trained on a relatively small dataset, and doesn't leverage training data with intrinsic transparency information. We would expect substantial image quality improvement by increasing training data volume and quality.",
            "score": 0.44017416121257413,
            "section_title": "C Broader and Societal Impact",
            "char_start_offset": 32229,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 51
                },
                {
                    "start": 52,
                    "end": 198
                },
                {
                    "start": 199,
                    "end": 343
                },
                {
                    "start": 344,
                    "end": 478
                },
                {
                    "start": 481,
                    "end": 647
                },
                {
                    "start": 648,
                    "end": 826
                },
                {
                    "start": 827,
                    "end": 980
                },
                {
                    "start": 983,
                    "end": 1049
                },
                {
                    "start": 1050,
                    "end": 1163
                },
                {
                    "start": 1164,
                    "end": 1281
                },
                {
                    "start": 1282,
                    "end": 1443
                },
                {
                    "start": 1444,
                    "end": 1617
                },
                {
                    "start": 1618,
                    "end": 1797
                },
                {
                    "start": 1800,
                    "end": 1842
                },
                {
                    "start": 1845,
                    "end": 2089
                },
                {
                    "start": 2092,
                    "end": 2147
                },
                {
                    "start": 2150,
                    "end": 2166
                },
                {
                    "start": 2167,
                    "end": 2303
                },
                {
                    "start": 2304,
                    "end": 2405
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.1448974609375
        },
        {
            "corpus_id": "269214015",
            "title": "\\copyright Plug-in Authorization for Human Content Copyright Protection in Text-to-Image Model",
            "text": "Recent text-to-image generative models are trained with large scale datasets [1,28], which cannot be guaranteed free of copyrighted data.At the same time, the stateof-the-art models are capable of generating high-quality and valuable creative images comparable to human creators or even memorizing the data points in the training set [29], which arouses copyright concerns about the training data and brings anxiety to the artist community.Numerous efforts have been made for copyright protection of training data [30].A direct approach is removing the copyrighted images from the training set, which may involve cumbersome cost due to the size of the training sets and may significantly degrade the model performance [31].Another direct approach is post filtering, refusing to generate images with copyrighted concepts, e.g., [32] proposes Safe Latent Diffusion to guide latent representation away from target concepts in the inference process, which nonetheless can be bypassed by a user with access to the model [33].As an example, OpenAI Dall\u2022E3 [34] declines requests for generating an image in the style of a living artist and promises that creators can also opt their images out from training of future image generation models.Many papers discuss the idea of concept removal, which will be reviewed in later section.\n\nShan et al. [35] propose Image Cloaking that suggests adding adversarial perturbations before posting artistic works on the internet so as to make them unlearnable for machine learning model, which has been pointed out to be hard to defend against future learning algorithms [36].\n\nTheoretically, [37,38] connect the copyright protection of training data with the concept of differential privacy and discuss their subtle differences.[39] further formulate the copyright problem with a near free access (NAF) notion to bound the distance of the generative distributions of the models trained with and without the copyrighted data.\n\nOur paper distinguishes largely from all previous works as we do not try to prohibit generating copyrighted concepts but instead we introduce a copyright authorization for the generative model to reward the copyright owners with fairness and transparency.",
            "score": 0.4396506628741437,
            "section_title": "A.1 Scope Related: Copyright, Data Contribution and Credit Attribution",
            "char_start_offset": 22848,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 137
                },
                {
                    "start": 137,
                    "end": 440
                },
                {
                    "start": 440,
                    "end": 519
                },
                {
                    "start": 519,
                    "end": 723
                },
                {
                    "start": 723,
                    "end": 1020
                },
                {
                    "start": 1020,
                    "end": 1234
                },
                {
                    "start": 1234,
                    "end": 1323
                },
                {
                    "start": 1325,
                    "end": 1605
                },
                {
                    "start": 1607,
                    "end": 1758
                },
                {
                    "start": 1758,
                    "end": 1954
                },
                {
                    "start": 1956,
                    "end": 2211
                }
            ],
            "ref_mentions": [
                {
                    "start": 77,
                    "end": 80,
                    "matchedPaperCorpusId": "252917726"
                },
                {
                    "start": 80,
                    "end": 83,
                    "matchedPaperCorpusId": "258508966"
                },
                {
                    "start": 334,
                    "end": 338,
                    "matchedPaperCorpusId": "256389993"
                },
                {
                    "start": 514,
                    "end": 518,
                    "matchedPaperCorpusId": "257557331"
                },
                {
                    "start": 718,
                    "end": 722,
                    "matchedPaperCorpusId": "186206616"
                },
                {
                    "start": 827,
                    "end": 831,
                    "matchedPaperCorpusId": "253420366"
                },
                {
                    "start": 1015,
                    "end": 1019,
                    "matchedPaperCorpusId": "252780252"
                },
                {
                    "start": 1337,
                    "end": 1341,
                    "matchedPaperCorpusId": "256662278"
                },
                {
                    "start": 1600,
                    "end": 1604,
                    "matchedPaperCorpusId": "235658909"
                },
                {
                    "start": 1622,
                    "end": 1626,
                    "matchedPaperCorpusId": "225067265"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.9423828125
        },
        {
            "corpus_id": "265506098",
            "title": "Situating the social issues of image generation models in the model life cycle: a sociotechnical approach",
            "text": "Data issues are also closely linked with issues of intellectual property, which can arise both through the training data used for image generation models, and through the output generated by the systems, which may (intentionally or unintentionally) have similarities to copyrighted work (52). It is notable here that understandings of \"fair use\" differ across national contexts: in the United States, for example, published works may be used for training models, while in the EU, artists are able to disallow the use of their work if it will ultimately be used commercially (53). However, the removal of copyrighted data from a large-scale training dataset may be impractical (54), given the large amounts of data and lack of documentation as described above (44). Moreover, it may be viewed as \"undesirable\" (52), as the inclusion of copyrighted works can be seen to strengthen the resulting model by providing a broader range of sources to draw on. It is significant that, as reported by Dehouche and Dehouche (7), the development of text-to-image models was supported by services such as ArtStation which \"encourage artists to include detailed labels describing their work in order to make it more accessible to persons with disabilities,\" thus providing ready text-image pairs, but raising complex issues in relation to intellectual property when these images are used to train image generation models. \n\nThe tendency of image generation models to memorize and regenerate images (\"image regurgitation\" ( 55)) can pose a particular problem when the models are developed from \"stolen artwork from the internet [sic] without permission\". This has led to legal action, with a notable example being a lawsuit against Stable Diffusion by Getty Images, with output from Stable Diffusion containing a regenerated version of the Getty Images watermark, overlaid on \"grotesque\" (56) images. There is an alternative perspective, however, which sees the regurgitation of training images as a positive, as it allows for the generation of specific images which have cultural significance (57). \n\nAn additional issue is how intellectual property rights should be handled for the works generated by AI, or the prompts used.",
            "score": 0.4396506628741437,
            "section_title": "Intellectual property",
            "char_start_offset": 17382,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 292
                },
                {
                    "start": 293,
                    "end": 579
                },
                {
                    "start": 580,
                    "end": 764
                },
                {
                    "start": 765,
                    "end": 950
                },
                {
                    "start": 951,
                    "end": 1406
                },
                {
                    "start": 1409,
                    "end": 1638
                },
                {
                    "start": 1639,
                    "end": 1884
                },
                {
                    "start": 1885,
                    "end": 2083
                },
                {
                    "start": 2086,
                    "end": 2211
                }
            ],
            "ref_mentions": [
                {
                    "start": 287,
                    "end": 291,
                    "matchedPaperCorpusId": "257050406"
                },
                {
                    "start": 574,
                    "end": 578,
                    "matchedPaperCorpusId": "252355133"
                },
                {
                    "start": 676,
                    "end": 680,
                    "matchedPaperCorpusId": "257833863"
                },
                {
                    "start": 759,
                    "end": 763,
                    "matchedPaperCorpusId": "4421027"
                },
                {
                    "start": 1012,
                    "end": 1015,
                    "matchedPaperCorpusId": "258957226"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.96533203125
        },
        {
            "corpus_id": "266900037",
            "title": "The Stronger the Diffusion Model, the Easier the Backdoor: Data Poisoning to Induce Copyright Breaches Without Adjusting Finetuning Pipeline",
            "text": "The commercialization of text-to-image diffusion models (DMs) brings forth potential copyright concerns. Despite numerous attempts to protect DMs from copyright issues, the vulnerabilities of these solutions are underexplored. In this study, we formalized the Copyright Infringement Attack on generative AI models and proposed a backdoor attack method, SilentBadDiffusion, to induce copyright infringement without requiring access to or control over training processes. Our method strategically embeds connections between pieces of copyrighted information and text references in poisoning data while carefully dispersing that information, making the poisoning data inconspicuous when integrated into a clean dataset. Our experiments show the stealth and efficacy of the poisoning data. When given specific text prompts, DMs trained with a poisoning ratio of 0.20% can produce copyrighted images. Additionally, the results reveal that the more sophisticated the DMs are, the easier the success of the attack becomes. These findings underline potential pitfalls in the prevailing copyright protection strategies and underscore the necessity for increased scrutiny to prevent the misuse of DMs.",
            "score": 0.43857424003677475,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.978515625
        },
        {
            "corpus_id": "269293420",
            "title": "VidProM: A Million-scale Real Prompt-Gallery Dataset for Text-to-Video Diffusion Models",
            "text": "Qualitative observations. In Fig. 7, we demonstrate that text-to-video diffusion models can replicate the content from their training data or existing videos. This replication is acceptable for fair use purposes such as education, news reporting, and parody. However, misusing videos that contain replicated content with copyright could constitute an infringement. For example, in Fig. 7, the video generated by the commercial model Pika closely replicate the content of the famous painting, \"The Persistence of Memory\", which is copyrighted by the Gala-Salvador Dal\u00ed Foundation. If someone uses this generated video for profit without permission, it could potentially constitute copyright infringement.",
            "score": 0.4384191773842353,
            "section_title": "D.2 Observations",
            "char_start_offset": 31692,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 25
                },
                {
                    "start": 26,
                    "end": 158
                },
                {
                    "start": 159,
                    "end": 258
                },
                {
                    "start": 259,
                    "end": 364
                },
                {
                    "start": 365,
                    "end": 579
                },
                {
                    "start": 580,
                    "end": 703
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.66357421875
        },
        {
            "corpus_id": "266798835",
            "title": "The Study of Copyright Infringement Liability of Generative Artificial Intelligence",
            "text": "Part 2 delves into the infringement and liability related to generative AI that undergoes pre-training using prior works, exploring the legal implications of using copyrighted works as a foundation for training generative AI models and analyzes the potential liability of different actors in this process. Part 3 examines the infringement and liability issues that arise when generative AI generates content that is highly similar to prior works. This section considers the responsibilities of both generative AI service providers and users in different scenarios, emphasizing the need for clear guidelines and accountability. \n\nThrough an exploration of these critical facets of liability in the wake of copyright infringement by generative AI, we aim to illuminate the current legal challenges surrounding the use of AI in content creation and contribute to the development of a comprehensive and effective legal liability framework for generative AI in the future.",
            "score": 0.4384135376610396,
            "section_title": "Introduction",
            "char_start_offset": 2351,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 305
                },
                {
                    "start": 306,
                    "end": 446
                },
                {
                    "start": 447,
                    "end": 626
                },
                {
                    "start": 629,
                    "end": 967
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.8408203125
        },
        {
            "corpus_id": "276558342",
            "title": "CopyJudge: Automated Copyright Infringement Identification and Mitigation in Text-to-Image Diffusion Models",
            "text": "Assessing whether AI-generated images are substantially similar to copyrighted works is a crucial step in resolving copyright disputes. In this paper, we propose CopyJudge, an automated copyright infringement identification framework that leverages large vision-language models (LVLMs) to simulate practical court processes for determining substantial similarity between copyrighted images and those generated by text-to-image diffusion models. Specifically, we employ an abstraction-filtration-comparison test framework with multi-LVLM debate to assess the likelihood of infringement and provide detailed judgment rationales. Based on the judgments, we further introduce a general LVLM-based mitigation strategy that automatically optimizes infringing prompts by avoiding sensitive expressions while preserving the non-infringing content. Besides, our approach can be enhanced by exploring non-infringing noise vectors within the diffusion latent space via reinforcement learning, even without modifying the original prompts. Experimental results show that our identification method achieves comparable state-of-the-art performance, while offering superior generalization and interpretability across various forms of infringement, and that our mitigation method could more effectively mitigate memorization and IP infringement without losing non-infringing expressions.",
            "score": 0.4377457196985437,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.98583984375
        },
        {
            "corpus_id": "269137391",
            "title": "Uncertain Boundaries: Multidisciplinary Approaches to Copyright Issues in Generative AI",
            "text": "The challenges and opportunities inherent in this burgeoning field offer insights that can inform policymakers, practitioners, and researchers alike when developing generative AI.Our main contributions are: i) A detailed examination of the most advanced methods for detecting AI-generated copyright violations across various mediums such as text, image, and video, establishing itself as an invaluable resource for both researchers and practitioners in the field.ii) Innovative strategies designed to safeguard copyrights within the AI sphere, highlighting cutting-edge techniques like watermarking, fingerprinting, and machine unlearning, contributing to the protection of IP. iii) A comprehensive array of tools and resources for assessing copyright violations, including extensive datasets, search engine capabilities, and metrics quantifying infringement.iv) An in-depth analysis of the regulatory framework surrounding generative AI, navigating through current international copyright laws and proposing solutions to tackle the emerging challenges in generative AI.",
            "score": 0.43766140456375235,
            "section_title": "INTRODUCTION",
            "char_start_offset": 2233,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 179
                },
                {
                    "start": 179,
                    "end": 463
                },
                {
                    "start": 463,
                    "end": 859
                },
                {
                    "start": 859,
                    "end": 1070
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.65869140625
        },
        {
            "corpus_id": "277502009",
            "title": "Who Owns the Output? Bridging Law and Technology in LLMs Attribution",
            "text": "The rapid development of artificial intelligence (AI) tools capable of creating complex content, from images and text to music and multimedia, has spurred significant debate over the legal and ethical status of AI-generated works. Central to this discussion is the question of whether AI outputs should be considered \"derivative works\" under copyright law, as they often rely on vast amounts of preexisting, human-created content to generate something new. This reliance raises questions about the potential for copyright infringement, especially as AI tools become more sophisticated in their ability to replicate particular styles, structures, or elements drawn from the training data used in their development. \n\nUnder copyright law, derivative works are those that are based on existing content but altered enough to provide a new expression, form, or purpose distinct from the original. A traditional example might include a film adaptation of a novel or a reinterpretation of a song. For a work to be deemed derivative, it typically must contain identifiable elements of the original while adding a unique contribution. With AI, however, determining this distinct contribution is complicated by the absence of human authorship. AI systems are built to analyse patterns and recombine elements from large datasets, which often include copyrighted materials. This means that, although AI outputs may look or sound original, they are often directly shaped by the existing content fed into the system. \n\nOne of the primary legal questions involves whether AI-generated content transforms the original work enough to qualify as \"new\" or if it instead reproduces protected elements in a way that infringes copyright [20]. Courts [21] typically analyse the amount of original material used, the intent and market impact of the use, and whether the final output offers new expression or value. In some jurisdictions, such as the United States, the doctrine of \"transformative use\" under fair use guidelines provides a pathway for determining whether a work is sufficiently altered from the original. This allows certain uses of copyrighted materials if they add enough new expression or purpose, although the extent of such protection for AI-generated content remains an open question. \n\nIn addition to transformative use, AI-generated content raises broader concerns over the potential devaluation of human creativity.",
            "score": 0.43765572092535887,
            "section_title": "AI-generated content as a derivative work",
            "char_start_offset": 18105,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 230
                },
                {
                    "start": 231,
                    "end": 456
                },
                {
                    "start": 457,
                    "end": 713
                },
                {
                    "start": 716,
                    "end": 891
                },
                {
                    "start": 892,
                    "end": 989
                },
                {
                    "start": 990,
                    "end": 1125
                },
                {
                    "start": 1126,
                    "end": 1233
                },
                {
                    "start": 1234,
                    "end": 1361
                },
                {
                    "start": 1362,
                    "end": 1502
                },
                {
                    "start": 1505,
                    "end": 1720
                },
                {
                    "start": 1721,
                    "end": 1890
                },
                {
                    "start": 1891,
                    "end": 2096
                },
                {
                    "start": 2097,
                    "end": 2282
                },
                {
                    "start": 2285,
                    "end": 2416
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.65185546875
        },
        {
            "corpus_id": "268732888",
            "title": "Detecting Generative Parroting through Overfitting Masked Autoencoders",
            "text": "Generative artificial intelligence (AI) models, including but not limited to Stable Diffusion [11], DALLE [10], and Generative Pre-trained Transformers (GPT) [8], represent a groundbreaking shift in the landscape of digital content creation, empowering users to generate text, images, and other forms of media with unprecedented ease and flexibility.These models have been applied across a wide range of domains, from artistic creation and design to content generation for social media and marketing purposes, demonstrating their versatility and potential to enhance creativity and productivity.\n\nThe rapid adoption and deployment of these technologies have also raised significant ethical, legal, and technical challenges, particularly in the context of copyright infringement and data privacy [4,12,17].At the heart of these concerns is the phenomenon known as \"generative parroting,\" where models produce outputs that are not sufficiently distinct from their training data [3,16], leading to the generation of content that closely mimics or even directly copies existing copyrighted materials.This issue not only poses legal risks for users and developers but also undermines trust in generative AI technologies, especially in trust-critical scenarios where the protection of intellectual property and sensitive information is paramount.\n\nThe challenge of detecting and mitigating generative parroting is compounded by the inherent complexities of AI models' training processes and the vastness of the data landscapes they navigate.Traditional approaches to model training and evaluation may not adequately address the nuances of copyright-sensitive scenarios, necessitating innovative solutions that are specifically tailored to recognize and respect the boundaries of copyright law [13].Moreover, the dynamic nature of copyright legislation, which varies across jurisdictions and is continually evolving in response to technological advancements, adds another layer of complexity to this challenge [7].\n\nWhile passing generated samples through a representation learner to obtain feature vectors and compare them with training data might be feasible for small datasets, this approach becomes impractical for larger datasets with billions of samples, especially in real-time scenarios.For instance, designers interacting with generative models need immediate feedback, rendering exhaustive comparisons untenably slow.",
            "score": 0.4375960115194548,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 350
                },
                {
                    "start": 350,
                    "end": 595
                },
                {
                    "start": 597,
                    "end": 805
                },
                {
                    "start": 805,
                    "end": 1096
                },
                {
                    "start": 1096,
                    "end": 1340
                },
                {
                    "start": 1342,
                    "end": 1535
                },
                {
                    "start": 1535,
                    "end": 1792
                },
                {
                    "start": 1792,
                    "end": 2007
                },
                {
                    "start": 2009,
                    "end": 2288
                },
                {
                    "start": 2288,
                    "end": 2420
                }
            ],
            "ref_mentions": [
                {
                    "start": 94,
                    "end": 98,
                    "matchedPaperCorpusId": "245335280"
                },
                {
                    "start": 106,
                    "end": 110,
                    "matchedPaperCorpusId": "232035663"
                },
                {
                    "start": 795,
                    "end": 798,
                    "matchedPaperCorpusId": "234777751"
                },
                {
                    "start": 801,
                    "end": 804,
                    "matchedPaperCorpusId": "257050406"
                },
                {
                    "start": 976,
                    "end": 979,
                    "matchedPaperCorpusId": "256389993"
                },
                {
                    "start": 979,
                    "end": 982,
                    "matchedPaperCorpusId": "254366634"
                },
                {
                    "start": 1787,
                    "end": 1791,
                    "matchedPaperCorpusId": "259844568"
                },
                {
                    "start": 2003,
                    "end": 2006,
                    "matchedPaperCorpusId": "261375096"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.96630859375
        },
        {
            "corpus_id": "276558342",
            "title": "CopyJudge: Automated Copyright Infringement Identification and Mitigation in Text-to-Image Diffusion Models",
            "text": "We present CopyJudge, an innovative framework for automating the identification of copyright infringement in text-to-image diffusion models. By leveraging abstractionfiltration-comparison test and multi-LVLM debates, our approach could effectively evaluate the substantial similarity between generated and copyrighted images, providing clear and interpretable judgments. Additionally, our LVLM-based mitigation strategy helps avoid infringement by automatically optimizing prompts and exploring non-infringing latent noise vectors, while ensuring that generated images align with the user's requirements. \n\n\u2022 Modifying a Prompt to Improve Similarity Score (Attack Iteration): \"Adjust the parts of the original prompt of the second image that may cause expressions of distinction in the following rationale, making it more similar to the first image to achieve a higher score. Add more information about the [IP type] in Image 1, and provide more unique expressions specific to the [IP type] in Image 1. You can make any changes as long as they improve the similarity score. Require: Source image x cr , generated image x 0 , initial prompt p 0 , control condition p c , LVLM-based prompt modifier \u03c0 p , infringement identification function f , threshold \u03b3, maximum iterations T 1: Initialize prompt: p t \u2190 p 0 , generated image: \n\nGenerate new prompt: p t+1 \u2190 \u03c0 p (x t , x cr , p t , p c , s t , c t , r t ) 8: \n\nGenerate new image using p t+1 : x t+1 \u2190 T2I(p t+1 ) 9: \n\nUpdate iteration counter: t \u2190 t + 1 10: end while 11: Return final prompt p t and generated image x t",
            "score": 0.4374161971366494,
            "section_title": "Conclusion",
            "char_start_offset": 24510,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 140
                },
                {
                    "start": 141,
                    "end": 370
                },
                {
                    "start": 371,
                    "end": 604
                },
                {
                    "start": 607,
                    "end": 875
                },
                {
                    "start": 876,
                    "end": 1002
                },
                {
                    "start": 1003,
                    "end": 1073
                },
                {
                    "start": 1074,
                    "end": 1328
                },
                {
                    "start": 1331,
                    "end": 1410
                },
                {
                    "start": 1413,
                    "end": 1468
                },
                {
                    "start": 1471,
                    "end": 1572
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.9853515625
        },
        {
            "corpus_id": "269033217",
            "title": "Disguised Copyright Infringement of Latent Diffusion Models",
            "text": "In our case, the training data has the original data hidden in the latent space.In Section 4.5, we argue that if training data contains the same latent information as some copyrighted data, then it should be acknowledged.Daras et al. (2024) show that even when the latent space is corrupted the copyrighted information could still be contained, which could raise a stronger disguised copyright infringement attack.\n\nAcknowledgment and fair use Finally we discuss the anticipated impact of our definition of acknowledgment on judicial decisions.Specifically, we believe acknowledgment will not directly affect the current fair use doctrine or change its decision.To further illustrate the above claim, we first propose two principles: (1) copyright infringement is not fair use: although generative AI law is not rigorously defined, we generally assume that \"reproducing the copyrighted content\" is not fair use when there's direct access;\n\n(2) disguised infringement does not (appear to) use the copyrighted data: there is no decision of fair use doctrine here as there is no apparent \"usage\" of the copyrighted data, not to mention \"fair use\".Thus the decision is on the \"originality\" of the output (reproduction of the copyrighted content) but not fair use.As a result, the notion of acknowledgment aims to identify disguised copyright infringement in the second principle and challenge the decision of \"originality\".Based on the acquired outcome (reproduction), there would be no scenarios that acknowledgment can change the decision of fair use according to the first principle.",
            "score": 0.43736057145569185,
            "section_title": "Conclusion",
            "char_start_offset": 26104,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 80
                },
                {
                    "start": 80,
                    "end": 221
                },
                {
                    "start": 221,
                    "end": 414
                },
                {
                    "start": 416,
                    "end": 544
                },
                {
                    "start": 544,
                    "end": 662
                },
                {
                    "start": 662,
                    "end": 938
                },
                {
                    "start": 940,
                    "end": 1144
                },
                {
                    "start": 1144,
                    "end": 1259
                },
                {
                    "start": 1259,
                    "end": 1419
                },
                {
                    "start": 1419,
                    "end": 1582
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.55224609375
        },
        {
            "corpus_id": "270258236",
            "title": "Tackling copyright issues in AI image generation through originality estimation and genericization",
            "text": "Outputs of generative models can be genericized in a model-agnostic way through originality estimation. This is achieved by internally generating n samples, estimating the originality of each sample, and only outputting the one with the lowest estimated originality. For computational efficiency, the originality estimate of a sample can be cross-computed using other internally produced samples. In other words, the genericization method internally produces samples y i , i = 1, 2, \u2022 \u2022 \u2022 , n and selects the final output y generic such that y generic = arg min \n\nTo illustrate, consider the scenario where the prompt \"a mustachioed character\" is used to generate y i . \n\nDue to the probabilistic nature of generative models, produced samples will exhibit a variety of designs, featuring different outfits as well as facial and physical traits. Some designs might resemble Mario from Super Mario Bros., featuring elements such as a red cap with his initial on it, a red shirt, blue overalls, large round blue eyes, and a shorter stature. However, this specific combination is quite unique, and most outputs will share little in common with Mario beyond featuring a mustache. In essence, the method selects the design that is closest to the central point of all generated samples as the final output, y generic . The resulting output is expected to be generic, rather than highly unique or original like Mario, relative to the prompt. The philosophy behind this approach is to reduce the risk of copyright infringement by avoiding overly specific outputs. As a consequence of the varying level of copyright protection as embodied in the substantial similarity test, generic expressions-those that use common or widely shared ideas and themes with limited originality-are less likely to infringe on other copyrighted works.",
            "score": 0.43651716201097074,
            "section_title": "Genericization",
            "char_start_offset": 7389,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 103
                },
                {
                    "start": 104,
                    "end": 266
                },
                {
                    "start": 267,
                    "end": 396
                },
                {
                    "start": 397,
                    "end": 561
                },
                {
                    "start": 564,
                    "end": 669
                },
                {
                    "start": 672,
                    "end": 844
                },
                {
                    "start": 845,
                    "end": 1037
                },
                {
                    "start": 1038,
                    "end": 1174
                },
                {
                    "start": 1175,
                    "end": 1311
                },
                {
                    "start": 1312,
                    "end": 1433
                },
                {
                    "start": 1434,
                    "end": 1554
                },
                {
                    "start": 1555,
                    "end": 1821
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.9716796875
        },
        {
            "corpus_id": "264451536",
            "title": "CommonCanvas: An Open Diffusion Model Trained with Creative-Commons Images",
            "text": "Current methods train high-quality, text-to-image (T2I) models with. A lack of curated datasets that are large enough for the task has led researchers to turn to web-scraped solutions [29,30], like LAION-2B [26]. The use of web-scraped data is a very common practice for training generative models, however, US courts have yet to definitively rule if this is permissible under copyright law [1,13,15,20,21,60]. In response, recent work has begun to investigate alternative methods of navigating copyright concerns in text generation [39], code completion [16,51], and image generation [24]. Nevertheless, matching the performance of state-of-the-art models remains a challenge. In this work, we study the following natural question: Is it possible to efficiently produce a high-quality T2I model by training only on Creative-Commons-licensed data? \n\nWe suggest a possible path forward, training a suite of T2I architectures using only open-licensed, Creative-Commons (CC) images (Figures 1 & 2). This task brings to light two significant challenges. The first problem is data incompleteness: almost all CC images lack the captions necessary to train a high-quality T2I model. The second is data scarcity: there are relatively few high-resolution CC images -roughly 70 million, compared to LAION-2B's roughly 2 billion [26]. Using entirely Creative-Commons images and our synthetic captioning approach, we achieve comparable qualitative performance to Stable Diffusion 2 (SD2base), as seen in CommonCanvas generations, while only requiring a small fraction (< 3%) of the amount of training data. We include results for two CommonCanvas architectures, small (S) and large (L) (Section 6), and two CC-image datasets, commercial (C) and non-commercial (NC) (Section 4). We label our results accordingly as CommonCanvas-<architecture>-<dataset>.",
            "score": 0.435904561682689,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 68
                },
                {
                    "start": 69,
                    "end": 212
                },
                {
                    "start": 213,
                    "end": 410
                },
                {
                    "start": 411,
                    "end": 590
                },
                {
                    "start": 591,
                    "end": 677
                },
                {
                    "start": 678,
                    "end": 847
                },
                {
                    "start": 850,
                    "end": 995
                },
                {
                    "start": 996,
                    "end": 1049
                },
                {
                    "start": 1050,
                    "end": 1175
                },
                {
                    "start": 1176,
                    "end": 1323
                },
                {
                    "start": 1324,
                    "end": 1594
                },
                {
                    "start": 1595,
                    "end": 1765
                },
                {
                    "start": 1766,
                    "end": 1840
                }
            ],
            "ref_mentions": [
                {
                    "start": 559,
                    "end": 562,
                    "matchedPaperCorpusId": "249375708"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.76220703125
        },
        {
            "corpus_id": "271711518",
            "title": "\"I'm a Solo Developer but AI is My New Ill-Informed Co-Worker\": Envisioning and Designing Generative AI to Support Indie Game Development",
            "text": "\" \n\n\"If the AI uses the voice of someone that has not given permission and your game is going to generate a revenue you will most likely get a lawsuit headed your way. \" \n\nThese posts make it clear that AI-generated content that uses copyrighted materials, or materials from other human creators without their consent, will lead to legal troubles for indie game developers. In particular, these developers are concerned that although it is possible for them to refine or modify the AI-generated content to avoid potential copyright infringement (e.g., \"massaging these images into some final form\"), it is challenging to sufficiently validate their originality and legal rights to re-use these materials (\"pass the fair use 'sniff test'\"). \n\nAdditionally, the current absence of copyright protection for AI-generated content may lead to misuse and exploitation, for example: \n\n\"If you have an AI-generated banner/cover/main character art, for example, you may not be able to stop someone from copying it and using it for themselves. \" \"The argument is not about AI aiding processes, it's about AI that takes pre-existing content and generating a collage of other peoples work that people try to pass as their own. \" \n\nAccording to these posts, if generative AI is involved in indie developers' creative practices in any way, their work might be copied and used in others' products without crediting their contributions. Even worse, people can take their work and claim it \"as their own. \" As indie game developers are already facing several challenges regarding labor, capital, and production in their creative practices [13,17,31,42,57,60], these new risks paint a worrisome image where these developers may become even more vulnerable in the emerging age of generative AI. \n\nConsidering this potential risk, many developers thus express strong reservations against widely using generative AI in indie game development: \n\n\"AI smells. It is quite easy to understand it is done by a computer, and people immediately devalue anything using AI as cheap and low quality. \" \n\n\"As far as the community is concerned, the use of AI-generated images in a video game can potentially result in a review bombing, as there is a lot of hostility towards the use of AI image generation due to potential ethical concerns. \"",
            "score": 0.4358791878877017,
            "section_title": "Generative AI Makes Indie Game Development More Open and Accessible to All but Causes",
            "char_start_offset": 33160,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 1
                },
                {
                    "start": 4,
                    "end": 167
                },
                {
                    "start": 168,
                    "end": 169
                },
                {
                    "start": 172,
                    "end": 373
                },
                {
                    "start": 374,
                    "end": 739
                },
                {
                    "start": 742,
                    "end": 874
                },
                {
                    "start": 877,
                    "end": 1032
                },
                {
                    "start": 1033,
                    "end": 1213
                },
                {
                    "start": 1214,
                    "end": 1215
                },
                {
                    "start": 1218,
                    "end": 1419
                },
                {
                    "start": 1420,
                    "end": 1486
                },
                {
                    "start": 1487,
                    "end": 1774
                },
                {
                    "start": 1777,
                    "end": 1920
                },
                {
                    "start": 1923,
                    "end": 1934
                },
                {
                    "start": 1935,
                    "end": 2066
                },
                {
                    "start": 2067,
                    "end": 2068
                },
                {
                    "start": 2071,
                    "end": 2305
                },
                {
                    "start": 2306,
                    "end": 2307
                }
            ],
            "ref_mentions": [
                {
                    "start": 1621,
                    "end": 1625,
                    "matchedPaperCorpusId": "258217988"
                },
                {
                    "start": 1625,
                    "end": 1628,
                    "matchedPaperCorpusId": "232023051"
                },
                {
                    "start": 1631,
                    "end": 1634,
                    "matchedPaperCorpusId": "8568117"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.2281494140625
        },
        {
            "corpus_id": "270258236",
            "title": "Tackling copyright issues in AI image generation through originality estimation and genericization",
            "text": "Quantification of copyright protection Researchers have recently explored the application of computational methods to copyright legal issues. For example, a computational framework for testing substantial similarity based on Kolmogorov-Levin complexity has been proposed. 33 Furthermore, the potential of machine learning to bring objectivity to the substantial similarity test has also been recognized in the legal literature. 23 Relatedly, a deep-learning-based method for measuring creativity has been suggested, 24 but without alignment to the framework of copyright law in mind. More recently, the ability of generative models to capture the distribution of expressions in a massive dataset has been considered promising for measuring originality. 16,25,26 In this context, a method involving text inversion and reconstructing the When the user's prompt is the name of the copyrighted character itself, the generative models can generate images that closely resemble the copyrighted character. PREGen tends to successfully exclude elements that are highly unique to copyrighted characters even when the standard method fails to do so. Images were generated using Playground v2.5, Pixart-\u03b1, and SDXL. \n\noriginal image from the resulting text tokens has been discussed as a means to measure the originality of images. 16,26 This paper contributes to the literature by introducing a novel metric for quantifying the level of originality, which is consistent with the legal framework and can be practically estimated using generative models. \n\nMitigation of copyright concerns of generative models Previous studies have explored various approaches to modifying the outputs of generative models to mitigate the risks of copyright infringement. These include algorithms that achieve near-access-free conditions, ensuring that the model's output distribution is similar to one trained without access to copyrighted content, 8 though the validity of this condition has faced some criticisms 6,34 ; rejecting outputs that closely resemble copyrighted material and guiding generation away from such content 9 ; removing specific content from generative models through the application of unlearning techniques 10 ; and rewriting input prompts. 7 In this paper, a method for genericizing outputs by quantifying originality was introduced, which significantly improves the performance of the existing prompt rewriting method.",
            "score": 0.4355225151422983,
            "section_title": "Related works",
            "char_start_offset": 28638,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 141
                },
                {
                    "start": 142,
                    "end": 274
                },
                {
                    "start": 275,
                    "end": 430
                },
                {
                    "start": 431,
                    "end": 583
                },
                {
                    "start": 584,
                    "end": 761
                },
                {
                    "start": 762,
                    "end": 998
                },
                {
                    "start": 999,
                    "end": 1139
                },
                {
                    "start": 1140,
                    "end": 1204
                },
                {
                    "start": 1207,
                    "end": 1326
                },
                {
                    "start": 1327,
                    "end": 1542
                },
                {
                    "start": 1545,
                    "end": 1743
                },
                {
                    "start": 1744,
                    "end": 2239
                },
                {
                    "start": 2240,
                    "end": 2417
                }
            ],
            "ref_mentions": [
                {
                    "start": 272,
                    "end": 274,
                    "matchedPaperCorpusId": "249375708"
                },
                {
                    "start": 428,
                    "end": 430,
                    "matchedPaperCorpusId": "235642208"
                },
                {
                    "start": 516,
                    "end": 518,
                    "matchedPaperCorpusId": "246015719"
                },
                {
                    "start": 753,
                    "end": 756,
                    "matchedPaperCorpusId": "260673815"
                },
                {
                    "start": 1321,
                    "end": 1324,
                    "matchedPaperCorpusId": "260673815"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.97802734375
        },
        {
            "corpus_id": "269033217",
            "title": "Disguised Copyright Infringement of Latent Diffusion Models",
            "text": "In summary, we make the following contributions:\n\n\u2022 We challenge the current \"access\" criterion and point out its insufficiency in more delicate cases of copyright infringement;\n\n\u2022 We propose an algorithm that demonstrably crafts disguised copyrighted data to conceal the content (or concepts) of copyrighted images in the training set;\n\n\u2022 We show disguised data contain copyrighted information in the latent space, such that by finetuning them on textual inversion or DreamBooth, or training on LDM, the model reproduces copyrighted data during inference;\n\n\u2022 We propose methods to detect such disguises, which further encourage the expansion and quantification of \"access\" in the context of copyright infringement.",
            "score": 0.43481191203211494,
            "section_title": "Introduction",
            "char_start_offset": 4248,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 48
                },
                {
                    "start": 50,
                    "end": 177
                },
                {
                    "start": 179,
                    "end": 336
                },
                {
                    "start": 338,
                    "end": 556
                },
                {
                    "start": 558,
                    "end": 715
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.82958984375
        },
        {
            "corpus_id": "260416806",
            "title": "Talkin' 'Bout AI Generation: Copyright and the Generative-AI Supply Chain (The Short Version)",
            "text": "First, the models qua models are arguably highly transformative -both because they represent the works internally in new and very different ways, and also because they are capable of generating highly transformative works as outputs. Second, the business models of the model trainer and deployer become relevant here: models that are sold or provided as paid services are clearly commercial, whereas opensource releases are not. Third, at least some models will compete with at least some of the authors represented in their training datasets on at least some generations. \n\nConsidered as a straight up-or-down question, then, the fair use status of models is a difficult one. One attractive answer for courts may be to hold that the models themselves are fair uses while holding their creators and deployers liable for infringing generations. If generation leads to direct liability, then generation is an at-your-own risk activity. This would make the operation of generation services infeasible unless the operator were able to filter the outputs for all copyrighted works. Doing so would probably require training only on vetted, licensed datasets (as Adobe's generative AI model purportedly has been). In this world, however, it would probably not be infringing to distribute a trained model as a set of weights (as Stability AI's releases have been), because Sony. The fact that this tool (which is fair use in itself) is used by others for infringing purposes would be counterbalanced by the substantial noninfringing uses, leading to immunity under Sony. This might not be an attractive business model, because it might be hard for buyers to monetize these models and because of the ease of copying and further redistributing the models, but it could at least exist. \n\nAnother response, however, might be to treat generation as creating only indirect liability for model operators. In this world, operators would be allowed to provide models as services -but they would need to respond to notices of specific infringements under a Napster-like rule. These would probably not be notices directed to specific generations by named users, which would be difficult to detect and track. Instead, they would involve copyright owners identifying copyrighted works and demanding that the model operator prevent generations that are substantially similar to those works.",
            "score": 0.4341487787022237,
            "section_title": "Id. \u00a7 512(i)(1)(A",
            "char_start_offset": 50262,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 233
                },
                {
                    "start": 234,
                    "end": 428
                },
                {
                    "start": 429,
                    "end": 572
                },
                {
                    "start": 575,
                    "end": 676
                },
                {
                    "start": 677,
                    "end": 843
                },
                {
                    "start": 844,
                    "end": 933
                },
                {
                    "start": 934,
                    "end": 1076
                },
                {
                    "start": 1077,
                    "end": 1206
                },
                {
                    "start": 1207,
                    "end": 1370
                },
                {
                    "start": 1371,
                    "end": 1562
                },
                {
                    "start": 1563,
                    "end": 1774
                },
                {
                    "start": 1777,
                    "end": 1889
                },
                {
                    "start": 1890,
                    "end": 2057
                },
                {
                    "start": 2058,
                    "end": 2188
                },
                {
                    "start": 2189,
                    "end": 2368
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.880859375
        },
        {
            "corpus_id": "269033217",
            "title": "Disguised Copyright Infringement of Latent Diffusion Models",
            "text": "In this paper, we review the current access criterion of containing copyrighted material in the training set (direct access) in copyright infringement of generative models and point out its insufficiency by introducing disguised copyright infringement (indirect access).Specifically, such an infringement is realized by injecting disguised samples into the training set, which urges LDMs to produce copyrighted content.Such disguises are generated with a simple algorithm and demonstrated to share the same concept with their target copyrighted images using the textual inversion tool.To alleviate the concern on the disguises, we expand the current visual auditing (browsing the training set) with additional tools, i.e., feature similarity search and encoderdecoder examination to better identify these disguises.Furthermore, we propose a broader definition of acknowledgment to cover this new type of copyright violation.\n\nLimitations and future work: One interesting future work is to quantify the number of disguises needed for reproducing in large-scale training, which can be further linked to the quantification of memorization of such models (Carlini et al. 2022;Somepalli et al. 2023a;Somepalli et al. 2023b;Carlini et al. 2023;Ippolito et al. 2023;Zhang et al. 2021)).\n\nAdditionally, although our algorithms can generate descent disguises, we believe there is still room for improvement for optimization.Finally, one extension we didn't touch is the possibility of \"chopping\" copyrighted data and hiding it in several images.It is intriguing to explore whether it is possible to generate such a smuggler's dataset and detection towards it.\n\nLearning from noisy data A simultaneous and independent work (Daras et al. 2024) considers learning LDMs from noisy data.Although the techniques are very different, our works share a similar implication: the training dataset may not immediately resemble the generations produced, thus al-lowing copyright issues to be disguised from an auditor who manually inspects the dataset.In the work of Daras et al. (2024), the training data is the original data with Gaussian noise applied to it.In our case, the training data has the original data hidden in the latent space.",
            "score": 0.43408457669725664,
            "section_title": "Conclusion",
            "char_start_offset": 23965,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 270
                },
                {
                    "start": 270,
                    "end": 419
                },
                {
                    "start": 419,
                    "end": 585
                },
                {
                    "start": 585,
                    "end": 815
                },
                {
                    "start": 815,
                    "end": 924
                },
                {
                    "start": 926,
                    "end": 1279
                },
                {
                    "start": 1281,
                    "end": 1415
                },
                {
                    "start": 1415,
                    "end": 1536
                },
                {
                    "start": 1536,
                    "end": 1650
                },
                {
                    "start": 1652,
                    "end": 1773
                },
                {
                    "start": 1773,
                    "end": 2030
                },
                {
                    "start": 2030,
                    "end": 2139
                },
                {
                    "start": 2139,
                    "end": 2219
                }
            ],
            "ref_mentions": [
                {
                    "start": 1151,
                    "end": 1172,
                    "matchedPaperCorpusId": "246863735"
                },
                {
                    "start": 1172,
                    "end": 1195,
                    "matchedPaperCorpusId": "254366634"
                },
                {
                    "start": 1218,
                    "end": 1238,
                    "matchedPaperCorpusId": "256389993"
                },
                {
                    "start": 1238,
                    "end": 1259,
                    "matchedPaperCorpusId": "263610040"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.9833984375
        },
        {
            "corpus_id": "273791614",
            "title": "A Dual-Module System for Copyright-Free Image Recommendation and Infringement Detection in Educational Materials",
            "text": "Images are extensively utilized in educational materials due to their efficacy in conveying complex concepts. However, unauthorized use of images frequently results in legal issues related to copyright infringement. To mitigate this problem, we introduce a dual-module system specifically designed for educators. The first module, a copyright infringement detection system, employs deep learning techniques to verify the copyright status of images. It utilizes a Convolutional Variational Autoencoder (CVAE) model to extract significant features from copyrighted images and compares them against user-provided images. If infringement is detected, the second module, an image retrieval system, recommends alternative copyright-free images using a Vision Transformer (ViT)-based hashing model. Evaluation on benchmark datasets demonstrates the system\u2019s effectiveness, achieving a mean Average Precision (mAP) of 0.812 on the Flickr25k dataset. Additionally, a user study involving 65 teachers indicates high satisfaction levels, particularly in addressing copyright concerns and ease of use. Our system significantly aids educators in creating educational materials that comply with copyright regulations.",
            "score": 0.4333518494927393,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.56201171875
        },
        {
            "corpus_id": "257557331",
            "title": "Copyright Protection and Accountability of Generative AI: Attack, Watermarking and Attribution",
            "text": "Generative AI (e.g., Generative Adversarial Networks \u2013 GANs) has become increasingly popular in recent years. However, Generative AI introduces significant concerns regarding the protection of Intellectual Property Rights (IPR) (resp. model accountability) pertaining to images (resp. toxic images) and models (resp. poisoned models) generated. In this paper, we propose an evaluation framework to provide a comprehensive overview of the current state of the copyright protection measures for GANs, evaluate their performance across a diverse range of GAN architectures, and identify the factors that affect their performance and future research directions. Our findings indicate that the current IPR protection methods for input images, model watermarking, and attribution networks are largely satisfactory for a wide range of GANs. We highlight that further attention must be directed towards protecting training sets, as the current approaches fail to provide robust IPR protection and provenance tracing on training sets.",
            "score": 0.43325477571729576,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.68408203125
        },
        {
            "corpus_id": "274436785",
            "title": "CopyrightShield: Spatial Similarity Guided Backdoor Defense against Copyright Infringement in Diffusion Models",
            "text": "Diffusion models [59] have been widely applied in various generative tasks, including high-quality image synthesis, image style transfer, image-to-image translation, and text-to-image synthesis [11,52,53,65,75]. These models emulate the diffusion process observed in non-equilibrium Figure 1. Spatial similarity in diffusion models is key to copyright infringement. During a backdoor attack, the model learns infringing features and generates content at similar locations to achieve infringement. Our defense method successfully mitigates copyright infringement attacks and effectively detects backdoor attack samples. \n\nthermodynamics by incrementally introducing noise to the data, which approximates a Gaussian distribution. Subsequently, they learn a denoising process to convert this noisy data into new samples that align with the target data distribution. Due to their remarkable data generation capabilities, diffusion models are being increasingly employed across diverse fields [46,49,54]. \n\nHowever, as commercial text-to-image diffusion models become increasingly prevalent [57,61], copyright issues have emerged as a significant concern. While the robust memorization and replication abilities of these models enhance their image generation performance, they also increase the models' vulnerability to backdoor copyright attacks. By injecting concealed poisoned data into the training set, diffusion models can be compromised without the need for fine-tuning [62]. Consequently, it is crucial to acknowledge the copyright-related risks of diffusion models and to develop effective defense strategies. \n\nCurrent solutions to copyright issues primarily involve the removal of copyrighted images from training datasets to prevent diffusion models from inadvertently learning these images, thus avoiding potential copyright infringement [7,61,76]. However, effective defense mechanisms against existing backdoor injection attacks [21,28,36,74,77], which can result in copyright violations, have yet to be developed. \n\nIn this paper, we investigated the strong correlation between the content replication and prompts of diffusion models (exemplified by Stable Diffusion [1]). We analyzed the tight coupling of image and prompts features through cross-attention, revealing the spatial similarity of imageprompt associative features within cross-attention feature maps. Through experiments involving spatial transformations of samples, we observed that when identical spatial transformations were applied to the samples, the duplication also exhibited the same spatial transformations.",
            "score": 0.43287595415152913,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 211
                },
                {
                    "start": 212,
                    "end": 292
                },
                {
                    "start": 293,
                    "end": 365
                },
                {
                    "start": 366,
                    "end": 496
                },
                {
                    "start": 497,
                    "end": 618
                },
                {
                    "start": 621,
                    "end": 727
                },
                {
                    "start": 728,
                    "end": 862
                },
                {
                    "start": 863,
                    "end": 999
                },
                {
                    "start": 1002,
                    "end": 1150
                },
                {
                    "start": 1151,
                    "end": 1342
                },
                {
                    "start": 1343,
                    "end": 1477
                },
                {
                    "start": 1478,
                    "end": 1613
                },
                {
                    "start": 1616,
                    "end": 1856
                },
                {
                    "start": 1857,
                    "end": 2024
                },
                {
                    "start": 2027,
                    "end": 2183
                },
                {
                    "start": 2184,
                    "end": 2375
                },
                {
                    "start": 2376,
                    "end": 2591
                }
            ],
            "ref_mentions": [
                {
                    "start": 194,
                    "end": 198,
                    "matchedPaperCorpusId": "244714856"
                },
                {
                    "start": 198,
                    "end": 201,
                    "matchedPaperCorpusId": "245335280"
                },
                {
                    "start": 201,
                    "end": 204,
                    "matchedPaperCorpusId": "243938678"
                },
                {
                    "start": 204,
                    "end": 207,
                    "matchedPaperCorpusId": "260900064"
                },
                {
                    "start": 207,
                    "end": 210,
                    "matchedPaperCorpusId": "257427673"
                },
                {
                    "start": 995,
                    "end": 998,
                    "matchedPaperCorpusId": "248986576"
                },
                {
                    "start": 1086,
                    "end": 1090,
                    "matchedPaperCorpusId": "254366634"
                },
                {
                    "start": 1090,
                    "end": 1093,
                    "matchedPaperCorpusId": "257050406"
                },
                {
                    "start": 1849,
                    "end": 1852,
                    "matchedPaperCorpusId": "257050406"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.98828125
        },
        {
            "corpus_id": "259064265",
            "title": "DeepfakeArt Challenge: A Benchmark Dataset for Generative AI Art Forgery and Data Poisoning Detection",
            "text": "For example, in the realm of visual content synthesis using generative AI, there is a growing concern over image forgery and copyright infringement Knight [2022], Sheng [2023], Vincent [2023]. More specifically, large generative models for image synthesis are typically trained on large image datasets such as LAION Schuhmann et al. [2022], which contain billions of images with many of these being copyright-protected images. Furthermore, given that such models also accept language prompts, they are also trained on textual datasets often comprising of internet corpora, which could contain proprietary or copyright-protected text content. Given the sheer size of these datasets, it is nearly impossible to manually inspect all content within these datasets, leaving the origins and intellectual property rights of the data sources largely ambiguous Somepalli et al. [2022]. A critical concern arising from this is the tendency of large generative models to memorize data, as highlighted in various studies Carlini et al. [2022], Biderman et al. [2023]. This presents significant legal considerations for companies deploying these models, who must navigate these issues carefully to avoid infringement. Another growing concern in the realm of visual content synthesis is the rise of adversarial data poisoning intended to fool AI-based systems for decision-making and decision support. For example, there has been significant recent explorations on the impact of data poisoning in AI-powered medical systems, where the intentional embedding of generated adversarial noise into medical images that are imperceptible to the human eye can lead to false diagnosis that can have lethal consequences for patients as well as other malicious acts such as insurance fraud Ghaffari Laleh et al. [2022], Finlayson et al. \n\n[2019], Zhou et al. [2021]. As such, means to tackle these concerns are critical for widespread adoption in a responsible manner. Motivated to address these key concerns to encourage responsible generative AI for visual content synthesis, we introduce the DeepfakeArt Challenge, a large-scale challenge benchmark dataset designed specifically to aid in the building of machine learning algorithms for generative AI art forgery and data poisoning detection. Figure 1 shows the snapshot examples of our dataset.",
            "score": 0.432870373787953,
            "section_title": "Introduction",
            "char_start_offset": 1927,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 192
                },
                {
                    "start": 193,
                    "end": 426
                },
                {
                    "start": 427,
                    "end": 641
                },
                {
                    "start": 642,
                    "end": 876
                },
                {
                    "start": 877,
                    "end": 1055
                },
                {
                    "start": 1056,
                    "end": 1204
                },
                {
                    "start": 1205,
                    "end": 1387
                },
                {
                    "start": 1388,
                    "end": 1811
                },
                {
                    "start": 1814,
                    "end": 1841
                },
                {
                    "start": 1842,
                    "end": 1943
                },
                {
                    "start": 1944,
                    "end": 2270
                },
                {
                    "start": 2271,
                    "end": 2323
                }
            ],
            "ref_mentions": [
                {
                    "start": 1822,
                    "end": 1840,
                    "matchedPaperCorpusId": "245203152"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.97314453125
        },
        {
            "corpus_id": "276250558",
            "title": "Training-Free Constrained Generation With Stable Diffusion Models",
            "text": "Stress-strain curves MSE [\u2193] 179.5 175.6 12.5 1.2 risk of generating outputs which closely resemble copyrighted material. For this setting, a pretrained proxy model is fine-tuned to determine whether the generation infringes upon existing copyrighted material. This model has been calibrated so that the output logits can be directly used to evaluate the likelihood that the samples resemble existing protected material. Hence, by minimizing this surrogate constraint function, we directly minimize the likelihood that the output image includes copyrighted material. \n\nTo implement this, we define a permissible threshold for the likelihood function captured by the classifier. A balanced dataset of 8,000 images is constructed to fine-tune the classifier and diffusion models. Here, we use cartoon mouse characters 'Jerry,' from Tom and Jerry, and copyrightprotected character 'Mickey Mouse'. When fine-tuning the diffusion model, we do not discriminate between these two characters, but the classifier is tuned to identify 'Mickey Mouse' as a copyrighted example. \n\nInner minimizer. Our correction step begins by performing Principal Component Analysis (PCA) on the 512 features input to the last layer and selecting the two principal components. This analysis yields two well-defined clusters corresponding to the class labels. Provided this, we formulate a correction by iteratively driving the noisy samples toward the centroid of the target cluster, as illustrated in Figure 4 (right). During the early stages of the denoising process, if the classifier assigns a high probability to the sample being 'Mickey Mouse,' we correct the sample toward the 'Jerry' cluster in the feature space. Specifically, we iteratively adjust the sample until its distance from the 'Jerry' cluster falls below a predefined threshold. This correction is achieved by minimizing the distance between the sample's feature representation and the centroid of the 'Jerry' cluster, effectively guiding the generation process away from the copyrighted class label. \n\nAfter this correction, the denoising process is allowed to evolve naturally without further intervention. This method ensures that the generated images are guided away from resembling copyrighted material while still allowing the model to produce high-quality outputs.",
            "score": 0.43126801075515314,
            "section_title": "Structural analysis",
            "char_start_offset": 28763,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 121
                },
                {
                    "start": 122,
                    "end": 260
                },
                {
                    "start": 261,
                    "end": 420
                },
                {
                    "start": 421,
                    "end": 566
                },
                {
                    "start": 569,
                    "end": 677
                },
                {
                    "start": 678,
                    "end": 777
                },
                {
                    "start": 778,
                    "end": 893
                },
                {
                    "start": 894,
                    "end": 1065
                },
                {
                    "start": 1068,
                    "end": 1084
                },
                {
                    "start": 1085,
                    "end": 1248
                },
                {
                    "start": 1249,
                    "end": 1330
                },
                {
                    "start": 1331,
                    "end": 1491
                },
                {
                    "start": 1492,
                    "end": 1693
                },
                {
                    "start": 1694,
                    "end": 1820
                },
                {
                    "start": 1821,
                    "end": 2042
                },
                {
                    "start": 2045,
                    "end": 2150
                },
                {
                    "start": 2151,
                    "end": 2313
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.95947265625
        },
        {
            "corpus_id": "276247703",
            "title": "Legal and ethical considerations around the use of existing illustrations to generate new illustrations in the anatomical sciences",
            "text": "An increase in the volume of publishing in the biomedical, medical, and anatomical sciences in recent decades 1 has been accompanied by a proliferation of copyright infringements 2 and ongoing concerns around plagiarized content. 3,4 Copyright infringement (a legal issue) and ethical considerations such as plagiarism are recognized as an ongoing and contemporary concern in many fields, including areas such as photography, 5 graphic design, 6 architecture, 7 interior design, 8,9 and music, 10,11 often because of the financial implications associated with content reproduction. These costs can arise through either legal action involving persons or groups that potentially violated copyright, 11 or the license fees associated with gaining permission to use material protected by copyright. The use of generative artificial intelligence (AI) has also raised the profile of both copyright infringement and plagiarism in recent years, 12,13 leading to further discussion around the unauthorized use and reproduction of content. In particular, arguments center on original content creators suggesting AI engines should be paying authors for the use of their creations in generating new material. 14 tworks, including illustrations, are protected by copyright law 15 and international conventions. 16 Despite the inherent protections for artists and illustrators, it is difficult to find detailed information that assists authors and illustrators in understanding what copyright infringement and plagiarism are, particularly when it comes to visualizations and illustrations. Such guidance is necessary as existing illustrations are often utilized as inspiration for creating new images. 17,18 This is especially relevant in the anatomical sciences given the common features of the human form, the plethora of images from about 600 years of scientific anatomical visualizations, 19,20 and the increasing volume of additional new anatomical illustrations becoming available in print and digital formats. Further, many persons generating medical illustrations may not have access to body donors or dissections and must rely on others' depictions for reference. This article aims to provide guidance by presenting an overview of the issues central to the determination of copyright infringement and plagiarism with respect to anatomical illustrations. That is, it illuminates issues of law which underpin copyright infringement, as well as discusses plagiarism which is determined around ethical standards.",
            "score": 0.43120993472220814,
            "section_title": "INTRODUC TI ON",
            "char_start_offset": 17,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 233
                },
                {
                    "start": 234,
                    "end": 581
                },
                {
                    "start": 582,
                    "end": 794
                },
                {
                    "start": 795,
                    "end": 1029
                },
                {
                    "start": 1030,
                    "end": 1199
                },
                {
                    "start": 1200,
                    "end": 1300
                },
                {
                    "start": 1301,
                    "end": 1575
                },
                {
                    "start": 1576,
                    "end": 1693
                },
                {
                    "start": 1694,
                    "end": 2002
                },
                {
                    "start": 2003,
                    "end": 2158
                },
                {
                    "start": 2159,
                    "end": 2348
                },
                {
                    "start": 2349,
                    "end": 2503
                }
            ],
            "ref_mentions": [
                {
                    "start": 110,
                    "end": 111,
                    "matchedPaperCorpusId": "7826703"
                },
                {
                    "start": 230,
                    "end": 232,
                    "matchedPaperCorpusId": "235294739"
                },
                {
                    "start": 444,
                    "end": 445,
                    "matchedPaperCorpusId": "158467950"
                },
                {
                    "start": 460,
                    "end": 461,
                    "matchedPaperCorpusId": "226644188"
                },
                {
                    "start": 479,
                    "end": 481,
                    "matchedPaperCorpusId": "247766619"
                },
                {
                    "start": 494,
                    "end": 497,
                    "matchedPaperCorpusId": "153324290"
                },
                {
                    "start": 1264,
                    "end": 1266,
                    "matchedPaperCorpusId": "201010036"
                },
                {
                    "start": 1688,
                    "end": 1691,
                    "matchedPaperCorpusId": "58955061"
                },
                {
                    "start": 1691,
                    "end": 1693,
                    "matchedPaperCorpusId": "22142437"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.1143798828125
        },
        {
            "corpus_id": "270357944",
            "title": "Evaluating and Mitigating IP Infringement in Visual Generative AI",
            "text": "We term the prompt capable of potentially triggering intellectual property infringement issues in text-to-image generation models as lure prompt.In this section, we outline the detailed methodology for crafting lure prompts to induce intellectual property infringement.For a given target character (the character the infringer wants the generated images to resemble in appearance), we consider two types of lure prompts: name-based lure prompts and description-based lure prompts.\n\nName-based Lure Prompt.Regarding to the name-based lure prompts, we create them by utilizing the template \"Generate an image of {Character Name}\" for different target characters.\n\nDescription-based Lure Prompt.For the description-based lure prompts, we generate them by employing a large language model.We use GPT-4 [30] here as its exceptional text generation capabilities.It has been extensively utilized for various generation tasks [31].Given a target character, the detailed input supplied to the large language models during the lure prompt generation process is depicted in Figure 3. Based on the provided input, the large language model will generate the lure prompt, which has the potential to infringe upon the intellectual property rights associated with the specified target character.The examples of the generated description-based lure prompts can be found in Table 6, as well as the captions for Figure 6, Figure 7, Figure 8, Figure 9, and Figure 15.",
            "score": 0.43120387258033244,
            "section_title": "Constructing Lure Prompt for IP Infringement Evaluation",
            "char_start_offset": 9858,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 145
                },
                {
                    "start": 145,
                    "end": 269
                },
                {
                    "start": 269,
                    "end": 480
                },
                {
                    "start": 482,
                    "end": 505
                },
                {
                    "start": 505,
                    "end": 660
                },
                {
                    "start": 662,
                    "end": 692
                },
                {
                    "start": 692,
                    "end": 785
                },
                {
                    "start": 785,
                    "end": 856
                },
                {
                    "start": 856,
                    "end": 923
                },
                {
                    "start": 923,
                    "end": 1279
                },
                {
                    "start": 1279,
                    "end": 1447
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.85400390625
        },
        {
            "corpus_id": "261279983",
            "title": "AI Art and its Impact on Artists",
            "text": "Ascribing agency to image generators diminishes the complexity of human creativity, robs artists of credit (and in many cases compensation), and transfers accountability from the organizations creating image generators, and the practices of these organizations which should be scrutinized, to the image generators themselves. \n\nWhile companies like Midjourney, Stability AI and Open AI who produce image generators are valued at billions of dollars and are raising hundreds of millions of dollars 1 , their products are flooding the market with content that is being used to compete with and displace artists. In section 4, we discuss the impact of these products on working artists, including the chilling effect on cultural production and consumption as a whole. Merely open sourcing image generators does not solve these problems as they would still enable people to plagiarize artists' works, and impersonate their style for uses that the artists have not consented to. \n\nIn Section 5, we provide a summary of the relevant legal questions pertaining to image generators. While there have been legal developments around the world, we focus our analysis on the US where a number of lawsuits have been filed by artists challenging the use of image generators [129]. Given that copyright has been the most frequently invoked law in such cases [28], we provide an overview discussing the relevance of US copyright law in protecting artists, and conclude that it is largely unequipped to tackle many of the types of harms posed by these systems to content creators. As we discuss in Section 6, the AI research community has enabled the aforementioned harms through data laundering, with for-profit corporations partnering with academic institutions that help them gather training data for commercial purposes while increasing their chances of courts finding these uses to be \"fair use\". \n\nWe end our discussion with proposals for new tools and regulations that tackle some of the harms discussed in this paper, as well as encouraging the AI community to align themselves with those harmed by these systems rather than powerful entities driving the proliferation of generative AI models trained on the free labor of content creators.",
            "score": 0.4310919816026011,
            "section_title": "INTRODUCTION",
            "char_start_offset": 1868,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 325
                },
                {
                    "start": 328,
                    "end": 609
                },
                {
                    "start": 610,
                    "end": 764
                },
                {
                    "start": 765,
                    "end": 973
                },
                {
                    "start": 976,
                    "end": 1074
                },
                {
                    "start": 1075,
                    "end": 1266
                },
                {
                    "start": 1267,
                    "end": 1563
                },
                {
                    "start": 1564,
                    "end": 1884
                },
                {
                    "start": 1887,
                    "end": 2230
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.7529296875
        },
        {
            "corpus_id": "265352103",
            "title": "On Copyright Risks of Text-to-Image Diffusion Models",
            "text": "Diffusion models are undergoing a rapid development phase, and recently proposed diffusion models have shown improved performance in generating high-quality images. However, whether their performance has improved for copyright protection remains questionable. In this section, we investigate diffusion models in this regard. \n\nOur case study examines how a diverse pool of diffusion models react to varying subjects. For this purpose, we leverage the experiment setup in the previous section. Specifically, we obtain proportions of images with identified copyrighted content for each topic and compare them across different diffusion models (Figure 10, more in Appendix D). While the performance of each model varies for each topic, it is clear that all models exhibit copyright-infringing behavior for the five distinct topics. Even though the latest model SD XL shows a slight decrease in generating copyrighted content, the rate generating copyrighted content remains above 50% for many topics. This suggests that the current approach to training diffusion models remains ineffective in preventing the occurrence of copyright infringement.",
            "score": 0.4303665990213408,
            "section_title": "Case Study: Copyright Issue Across Diffusion Models",
            "char_start_offset": 28942,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 164
                },
                {
                    "start": 165,
                    "end": 259
                },
                {
                    "start": 260,
                    "end": 324
                },
                {
                    "start": 327,
                    "end": 416
                },
                {
                    "start": 417,
                    "end": 492
                },
                {
                    "start": 493,
                    "end": 673
                },
                {
                    "start": 674,
                    "end": 828
                },
                {
                    "start": 829,
                    "end": 997
                },
                {
                    "start": 998,
                    "end": 1142
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.984375
        },
        {
            "corpus_id": "270258236",
            "title": "Tackling copyright issues in AI image generation through originality estimation and genericization",
            "text": "Benchmark and evaluation metrics We measure the performance of PREGen on COPYCAT, 7 which is an evaluation suite designed for systematically analyzing the risks of the generation of copyrighted characters. It consists of a curated list of 50 diverse well-known copyrighted characters, such as Batman, Mickey Mouse and Mario, as well as evaluation metrics DETECT and CONS. \n\nDETECT measures the similarity of generated images to copyrighted characters by using a detection system, typically a multimodal model such as GPT-4o, to determine whether a specific copyrighted character appears in the generated content. A lower DETECT score indicates that fewer copyrighted characters were generated, which helps in minimizing potential copyright infringement. CONS measures the consistency of the generated image with the user's intent using VQAScore. 30 It evaluates, in particular, whether the key characteristics (e.g., \"cartoon mouse\" for Mickey Mouse) requested by the user are present in the generated image. A higher CONS score indicates that the generated content aligns better with the user's expectations, thereby improving user satisfaction. We use the same list of key characteristics used in the previous study. 7 Note that there is a natural trade-off between DETECT and CONS. In an extreme case, a generative model can refuse generation all together or generate a single harmless image regardless of what the prompt says, which would result in a perfect DETECT score. However, the user would likely not be satisfied with such a model. It is crucial to strike the right balance between DETECT and CONS for a generative model to be copyright safe and useful at the same time. \n\nIn addition to DETECT and CONS, to aid our analysis, we define p-CONS, a new metric that directly measures the alignment of the generation with the input prompt provided by the user. p-CONS can be considered as a more fine-grained measure of prompt consistency than the standard CONS. The exact definition of p-CONS and the details of the analysis involving p-CONS can be found in Section 4 of SI. \n\nWe consider both situations where the prompt is the name of copyrighted characters themselves, which we refer to as direct anchoring, and a longer textual description that could include phrases that trigger generation of these characters, termed indirect anchoring, 7 respectively.",
            "score": 0.4294799619840498,
            "section_title": "Performance of PREGen",
            "char_start_offset": 19123,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 205
                },
                {
                    "start": 206,
                    "end": 371
                },
                {
                    "start": 374,
                    "end": 612
                },
                {
                    "start": 613,
                    "end": 753
                },
                {
                    "start": 754,
                    "end": 848
                },
                {
                    "start": 849,
                    "end": 1008
                },
                {
                    "start": 1009,
                    "end": 1146
                },
                {
                    "start": 1147,
                    "end": 1220
                },
                {
                    "start": 1221,
                    "end": 1284
                },
                {
                    "start": 1285,
                    "end": 1476
                },
                {
                    "start": 1477,
                    "end": 1543
                },
                {
                    "start": 1544,
                    "end": 1682
                },
                {
                    "start": 1685,
                    "end": 1867
                },
                {
                    "start": 1868,
                    "end": 1969
                },
                {
                    "start": 1970,
                    "end": 2082
                },
                {
                    "start": 2085,
                    "end": 2366
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.98046875
        },
        {
            "corpus_id": "276558342",
            "title": "CopyJudge: Automated Copyright Infringement Identification and Mitigation in Text-to-Image Diffusion Models",
            "text": "For the process of identifying substantial similarity, we refer to the abstraction-filtering-comparison test method (Abramson, 2002), which has been widely adopted in practical court rulings on infringement cases, and propose an automated infringement identification framework using large visionlanguage models, as seen in Figure 1. In the copyright expression extraction stage, we break down images into different elements (such as composition and color patterns), and filter out non-copyrightable parts, leaving copyrighted portions to assess substantial similarity. In the next copyright infringement determination stage, multiple LVLMs debate and score the similarity of images given the copyrighted elements, with a final decision made by a meta-judge LVLM based on their consensus. Human priors are injected into the models through few-shot demonstrations to better align with human preferences. \n\nCopyright expression extraction via image-to-text abstraction and filtration. The process of distinguishing between the fundamental ideas and the specific expressions of an image is a crucial step in determining copyright protection. The core idea of our method is to break down the image into different layers or components, in order to examine the true copyright elements. \n\nFirst, during the abstraction phase, the image is analyzed and decomposed into its fundamental building blocks. This involves identifying the core elements that contribute to the overall meaning or aesthetic of the image, such as composition, themes, color palette, or other unique visual elements. We can implement this using an LVLM \u03c0 abs , defined as: \n\nwhere z and z cr represent the expressions of x and x cr in text after decoupling, respectively. The goal is to abstract away the superficial features of the image that do not hold significant creative value and instead focus on the underlying concepts that convey the essence of the work. \n\nThe next step is filtering. At this stage, elements of the image that are not eligible for copyright protection are removed from consideration. These can include generic concepts, common patterns, functional aspects, or elements derived from public domain sources. For example, standard design patterns or commonly used motifs in artwork may not be deemed original enough to warrant protection under copyright law. This process could be defined as: \n\nwhere z c and z c cr are the filtered copyright expressions, and \u03c0 f il is another independent LVLM. Filtering helps ensure that only the truly creative, original aspects of the image are preserved for comparison.",
            "score": 0.4288911876126787,
            "section_title": "Abstraction-Filtering-Comparison Framework",
            "char_start_offset": 6659,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 332
                },
                {
                    "start": 333,
                    "end": 568
                },
                {
                    "start": 569,
                    "end": 787
                },
                {
                    "start": 788,
                    "end": 901
                },
                {
                    "start": 904,
                    "end": 981
                },
                {
                    "start": 982,
                    "end": 1137
                },
                {
                    "start": 1138,
                    "end": 1278
                },
                {
                    "start": 1281,
                    "end": 1392
                },
                {
                    "start": 1393,
                    "end": 1579
                },
                {
                    "start": 1580,
                    "end": 1635
                },
                {
                    "start": 1638,
                    "end": 1734
                },
                {
                    "start": 1735,
                    "end": 1927
                },
                {
                    "start": 1930,
                    "end": 1957
                },
                {
                    "start": 1958,
                    "end": 2073
                },
                {
                    "start": 2074,
                    "end": 2194
                },
                {
                    "start": 2195,
                    "end": 2344
                },
                {
                    "start": 2345,
                    "end": 2378
                },
                {
                    "start": 2381,
                    "end": 2481
                },
                {
                    "start": 2482,
                    "end": 2594
                }
            ],
            "ref_mentions": [
                {
                    "start": 116,
                    "end": 132,
                    "matchedPaperCorpusId": "6322845"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.845703125
        },
        {
            "corpus_id": "265551515",
            "title": "VA3: Virtually Assured Amplification Attack on Probabilistic Copyright Protection for Text-to-Image Generative Models",
            "text": "In Fig. 2, we show example outputs under four scenarios. It is evident that with amplification, the CP-k protected generative model does indeed output copyright-infringed content with high probability, especially when using the Anti-NAF prompt. These examples suggest that probabilistic safeguards against copyright infringement are vulnerable to the amplification attack. Additional visualizations of outputs on various copyrighted images are demonstrated in Fig. 3. In Tab. 1, we report CIRs and FARs at various ARs on two datasets. Notably, there is a significant growth in both metrics when amplification attack is employed, highlighting its effectiveness in amplifying the probability of infringing generations. The superior performance of Anti-NAF under- CLIP-Int. w/ Amp. PEZ w/ Amp. Anti-NAF w/ Amp. \n\n-greedy-max Bandit Amp. \n\n-greedy-cdf Bandit Amp. \n\n(a) FAR-AR on POKEMON. CLIP-Int. w/ Amp. PEZ w/ Amp. Anti-NAF w/ Amp. \n\n-greedy-max Bandit Amp. \n\n-greedy-cdf Bandit Amp. \n\n(b) FAR-AR on LAION-mi. scores its efficacy in rendering infringing generations with a substantial probability. The overall FAR-AR curves are illustrated in Fig. 4. We can observe that bandit variants of amplification lead to a smaller variance across different target copyrighted images, especially at lower acceptance rates, indicating that our bandit strategies achieve a more steady attack. In Figs. 4c and 4d, we plot the overall FAR-AR curves over different amplification steps. There is a clear trend of rapidly improved performance with increased amplification steps, due to the cumulative probability of generating infringing samples. This finding underlines the potential risk in practical applications of probabilistic copyright protections, given the high frequency of daily interactions with text-to-image generative models. We provide additional results and human evaluations in Sec. 11.",
            "score": 0.4286218499003477,
            "section_title": "Results",
            "char_start_offset": 25578,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 56
                },
                {
                    "start": 57,
                    "end": 244
                },
                {
                    "start": 245,
                    "end": 372
                },
                {
                    "start": 373,
                    "end": 467
                },
                {
                    "start": 468,
                    "end": 534
                },
                {
                    "start": 535,
                    "end": 716
                },
                {
                    "start": 717,
                    "end": 770
                },
                {
                    "start": 771,
                    "end": 778
                },
                {
                    "start": 779,
                    "end": 790
                },
                {
                    "start": 791,
                    "end": 807
                },
                {
                    "start": 810,
                    "end": 833
                },
                {
                    "start": 836,
                    "end": 859
                },
                {
                    "start": 862,
                    "end": 884
                },
                {
                    "start": 885,
                    "end": 894
                },
                {
                    "start": 895,
                    "end": 902
                },
                {
                    "start": 903,
                    "end": 914
                },
                {
                    "start": 915,
                    "end": 931
                },
                {
                    "start": 934,
                    "end": 957
                },
                {
                    "start": 960,
                    "end": 983
                },
                {
                    "start": 986,
                    "end": 1009
                },
                {
                    "start": 1010,
                    "end": 1097
                },
                {
                    "start": 1098,
                    "end": 1380
                },
                {
                    "start": 1381,
                    "end": 1389
                },
                {
                    "start": 1390,
                    "end": 1470
                },
                {
                    "start": 1471,
                    "end": 1629
                },
                {
                    "start": 1630,
                    "end": 1823
                },
                {
                    "start": 1824,
                    "end": 1883
                },
                {
                    "start": 1884,
                    "end": 1887
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.966796875
        },
        {
            "corpus_id": "268532352",
            "title": "A Dataset and Benchmark for Copyright Infringement Unlearning from Text-to-Image Diffusion Models",
            "text": "We propose a pipeline to coordinate CLIP and diffusion models to generate a dataset that contains anchor images, corresponding prompts, and images generated by text-to-image models, reflecting the potential abuses of copyright, as illustrated in Fig. 3. Initially, we collect a set of images that potentially contain copyrighted content, which serves as anchor images.Subsequently, these images are fed into the clip-interrogator6 , allowing us to obtain prompts that correspond to each anchor image.We also provided experimental results using tools other than clip-interrogator in the supplementary materials.Finally, the prompts are used as inputs for the stable diffusion model, resulting in the generation of images by the stable diffusion model.The final outcomes indicate that even such a rudimentary pipeline can effectively generate a substantial volume of works pertinent to infringement issues.We conducted a statistical of the styles in prompts obtained using the pipeline in Fig. 3.The various styles at the bottom are provided by artists.",
            "score": 0.42820091595921195,
            "section_title": "Dataset Creation Pipeline",
            "char_start_offset": 9355,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 368
                },
                {
                    "start": 368,
                    "end": 500
                },
                {
                    "start": 500,
                    "end": 610
                },
                {
                    "start": 610,
                    "end": 750
                },
                {
                    "start": 750,
                    "end": 904
                },
                {
                    "start": 904,
                    "end": 994
                },
                {
                    "start": 994,
                    "end": 1051
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.9453125
        },
        {
            "corpus_id": "268691556",
            "title": "Not All Similarities Are Created Equal: Leveraging Data-Driven Biases to Inform GenAI Copyright Disputes",
            "text": "A growing number of researchers in recent years explore how to address legal problems by applying theories and methods of computer science.This literature seeks to narrow the gap between the vague and abstract concepts used by law by applying mathematical models to offer more rigor, coherent and scalable definitions into issues such as privacy (Dwork et al., 2006), or fairness and discrimination (Dwork et al., 2012).The study of copyright by computer science methods has only emerged recently.Scheffler et al. (2022), for instance, proposed a framework to test substantial similarity by comparing Kolmogorov-Levin complexity with and without access to the original copyright work.\n\nIn the context of generative models Carlini et al. (2023); Haim et al. (2022), explore whether generative diffusion models memorize protected works that appeared in the models' training set.This can be considered as a preliminary issue to the problem of establishing copyright infringement.However, as we discuss in section 4, memorization of input content does not necessarily equate with copyright infringement.There is also active and thoughtprovoking discussion on how machine learning technologies are reshaping our understanding of copyright within the realm of law.Asay (2020) explores the question of whether AI system outputs should be subject to copyright.Additionally, Grimmelmann (2015); Lemley and Casey (2020) explore the implications of copyright law for literary machines that extract content and manage databases of information.Other works, like Bousquet et al. (2020); Vyas et al. (2023) attempt to evaluate copyright infringement in GenAI models using privacy-like notions.The contribution of these works lies in proposing a procedure to inform copyright analysis in line with the thesis of this paper.However, this approach may not scale easily, and it also falls short of providing nuanced information regarding the level of genericity, which could be crucial for resolving copyright legal disputes (Elkin-Koren et al., 2024).",
            "score": 0.4279153102522513,
            "section_title": "Related Work",
            "char_start_offset": 6728,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 139
                },
                {
                    "start": 139,
                    "end": 420
                },
                {
                    "start": 420,
                    "end": 497
                },
                {
                    "start": 497,
                    "end": 684
                },
                {
                    "start": 686,
                    "end": 876
                },
                {
                    "start": 876,
                    "end": 976
                },
                {
                    "start": 976,
                    "end": 1099
                },
                {
                    "start": 1099,
                    "end": 1258
                },
                {
                    "start": 1258,
                    "end": 1352
                },
                {
                    "start": 1352,
                    "end": 1531
                },
                {
                    "start": 1531,
                    "end": 1678
                },
                {
                    "start": 1678,
                    "end": 1807
                },
                {
                    "start": 1807,
                    "end": 2033
                }
            ],
            "ref_mentions": [
                {
                    "start": 346,
                    "end": 366,
                    "matchedPaperCorpusId": "2468323"
                },
                {
                    "start": 399,
                    "end": 419,
                    "matchedPaperCorpusId": "13496699"
                },
                {
                    "start": 722,
                    "end": 743,
                    "matchedPaperCorpusId": "256389993"
                },
                {
                    "start": 745,
                    "end": 763,
                    "matchedPaperCorpusId": "249712435"
                },
                {
                    "start": 1258,
                    "end": 1269,
                    "matchedPaperCorpusId": "264375893"
                },
                {
                    "start": 1366,
                    "end": 1384,
                    "matchedPaperCorpusId": "152665774"
                },
                {
                    "start": 1386,
                    "end": 1409,
                    "matchedPaperCorpusId": "219342558"
                },
                {
                    "start": 1549,
                    "end": 1571,
                    "matchedPaperCorpusId": "225067265"
                },
                {
                    "start": 1573,
                    "end": 1591,
                    "matchedPaperCorpusId": "257050406"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.34375
        },
        {
            "corpus_id": "265351912",
            "title": "CopyScope: Model-level Copyright Infringement Quantification in the Diffusion Workflow",
            "text": "In this section, we present an Identify-Quantify-Evaluate framework called CopyScope to address the issue of AI-generated image copyright traceability at the model level. In the Identify stage, we first rigorously select four pivotal components for describing infringing models by analyzing images generated from Civitai[5]. \n\nIn the Quantify stage, we adopt FID to measure the similarity between the original images and the images generated by our designed model under different alliances of models. In the Evaluate stage, we trace back the possible infringing model by computing the contribution of models using the FID-Shapley value.",
            "score": 0.4277019725073797,
            "section_title": "METHODOLOGY",
            "char_start_offset": 9856,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 170
                },
                {
                    "start": 171,
                    "end": 324
                },
                {
                    "start": 327,
                    "end": 500
                },
                {
                    "start": 501,
                    "end": 636
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.97900390625
        },
        {
            "corpus_id": "278129333",
            "title": "Enhancing Privacy-Utility Trade-offs to Mitigate Memorization in Diffusion Models",
            "text": "Leveraging classifier-free guidance (CFG) [12], text-toimage diffusion models like Stable Diffusion [28] and Midjourney [1] are now capable of generating highly realistic images that closely align with user-provided text prompts. This capability has propelled their popularity, leading to widespread use and distribution of their generated images. However, recent research [6,8,31,34] has revealed a critical issue: these models can memorize training data, leading them to reproduce parts of images, such as foregrounds or backgrounds (local memorization, see Fig. 1), or even entire images (global memorization, see Fig. 2) during inference, instead of generating genuinely novel content. When the training data includes sensitive or copyrighted material, these memorization issues can infringe on copyright laws without notifying either the model's owners or users or the copyright holders of the replicated content. The risk is es-pecially substantial given the extensive use of these models and their reliance on massive datasets, such as LAION [30], which contain billions of web-scale images and are impractical to thoroughly review or filter manually. This risk is further highlighted by real-world cases, where several artists have filed lawsuits, arguing that models like Stable Diffusion act as \"21st-century collage tools\" that remix their copyrighted works, implicating Stability AI, DeviantArt, Midjourney, and Runway AI. Recognizing the potential for unauthorized reproductions, Midjourney has even banned prompts containing the term \"Afghan\" to prevent the generation of the copyrighted Afghan Girl photograph. Yet, as [35] demonstrates, such restrictions alone are insufficient to fully prevent the reproduction of copyrighted images. This underscores the urgent need for timely effective mitigation strategies to address these concerns. \n\nIn response to these legal challenges, recent initiatives [7,8,27,32,36] have focused on developing strategies to minimize memorization, achieving notable success. These approaches vary in scope, with some targeting the model's training phase and others making adjustments during inference.",
            "score": 0.42763016432976286,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 229
                },
                {
                    "start": 230,
                    "end": 347
                },
                {
                    "start": 348,
                    "end": 689
                },
                {
                    "start": 690,
                    "end": 918
                },
                {
                    "start": 919,
                    "end": 1158
                },
                {
                    "start": 1159,
                    "end": 1434
                },
                {
                    "start": 1435,
                    "end": 1625
                },
                {
                    "start": 1626,
                    "end": 1750
                },
                {
                    "start": 1751,
                    "end": 1853
                },
                {
                    "start": 1856,
                    "end": 2019
                },
                {
                    "start": 2020,
                    "end": 2146
                }
            ],
            "ref_mentions": [
                {
                    "start": 100,
                    "end": 104,
                    "matchedPaperCorpusId": "245335280"
                },
                {
                    "start": 373,
                    "end": 376,
                    "matchedPaperCorpusId": "256389993"
                },
                {
                    "start": 378,
                    "end": 381,
                    "matchedPaperCorpusId": "254366634"
                },
                {
                    "start": 1634,
                    "end": 1638,
                    "matchedPaperCorpusId": "256627601"
                },
                {
                    "start": 1914,
                    "end": 1917,
                    "matchedPaperCorpusId": "268819999"
                },
                {
                    "start": 1922,
                    "end": 1925,
                    "matchedPaperCorpusId": "258987384"
                },
                {
                    "start": 1925,
                    "end": 1928,
                    "matchedPaperCorpusId": "270309880"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.99169921875
        },
        {
            "corpus_id": "277667210",
            "title": "We Are All Creators: Generative AI, Collective Knowledge, and the Path Towards Human-AI Synergy",
            "text": "Lawsuits like Getty Images v. Stability AI, Andersen v. Stability AI, and Authors Guild v. OpenAI are grappling with these issues, including claims related to the removal or alteration of Copyright Management Information (CMI) under the DMCA \u00a71202 [Ginsburg and Budiardjo, 2023]. \n\nThe widespread availability of powerful open generative models, exemplified by the release of Stable Diffusion in 2022 [Rombach et al., 2022], adds another layer of complexity. These models can be freely downloaded, modified, and fine-tuned by anyone, often using private datasets or specific styles. Tracking the lineage of outputs generated by potentially thousands of derivative models becomes impossible, rendering systematic compensation or enforcement schemes based on training data provenance utterly impractical. Users can easily train an open model (initially trained on non-copyrighted data) with copyrighted images offline and generate new content, making legal recourse against the original model creators or the end-users exceedingly difficult. \n\nThis intricate legal and technical landscape suggests that seeking resolution primarily through copyright litigation or regulation focused on individual attribution and compensation faces formidable, perhaps insuperable, obstacles.",
            "score": 0.4268795056642635,
            "section_title": "Navigating the Copyright Labyrinth: Collective",
            "char_start_offset": 15686,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 279
                },
                {
                    "start": 282,
                    "end": 458
                },
                {
                    "start": 459,
                    "end": 582
                },
                {
                    "start": 583,
                    "end": 802
                },
                {
                    "start": 803,
                    "end": 1039
                },
                {
                    "start": 1042,
                    "end": 1273
                }
            ],
            "ref_mentions": [
                {
                    "start": 401,
                    "end": 423,
                    "matchedPaperCorpusId": "245335280"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.92138671875
        },
        {
            "corpus_id": "276558342",
            "title": "CopyJudge: Automated Copyright Infringement Identification and Mitigation in Text-to-Image Diffusion Models",
            "text": "Require: Initial latent variable z 0 , source image x cr , generated image x 0 , initial prompt p 0 , NN-based policy net \u03c0 \u03c9 , infringement identification function f , threshold \u03b3, learning rate \u03b2, maximum iterations T 1: Initialize latent variable: z t \u2190 z 0 , generated image: x t \u2190 x 0 , iteration counter: t \u2190 0; Initialize policy parameters \u03c9 2: while t < T do 3: \n\nSample latent noise:  Compute reward: R(z t ) \u2190 \u2212 log(s t ) 10: \n\nCompute gradient of the objective and update policy net parameters: \n\nUpdate latent variable: z t+1 \u2190 z t + \u03b2\u03f5 t 12: \n\nUpdate iteration counter: t \u2190 t + 1 13: end while 14: Return final latent variable z t and generated image x t human_score: 3, predicted_score: 3.0, confidence: 0.87, reason: \"Both images feature similar hairstyles and attire related to a well-known fictional character, suggesting inspiration from the same source material. The first is an artistic interpretation as a portrait, while the second is a group photo showcasing a broader scene. These differences in medium and composition, along with transformative artistic elements in the portrait, reduce the likelihood of copyright infringement. The similarities are thematic and character-based, rather than direct copying.\" human_score: 4, predicted_score: 4.0, confidence: 0.87, reason: \"Both images feature a similar subject with a distinctive pose and expression, set in a culinary context. The combination of unique elements like positioning, mood, and artistic style contributes to a significant similarity. Despite the different mediums, the depiction's nuances align closely, suggesting plausible copyright concerns, especially if one work directly inspires the other.\" human_score: 1, predicted_score: 0.9, confidence: 0.93, reason: \"Both images prominently feature the same distinctive animated character, exhibiting similar unique design elements such as shape, exaggerated facial features, and color scheme. These elements are specific to the character and highly recognizable, suggesting possible copyright infringement due to the replication of protected design features.\"",
            "score": 0.42668372433933566,
            "section_title": "Algorithm 3 Copyright Infringement Mitigation via Latent Control",
            "char_start_offset": 26151,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 369
                },
                {
                    "start": 372,
                    "end": 435
                },
                {
                    "start": 438,
                    "end": 505
                },
                {
                    "start": 508,
                    "end": 554
                },
                {
                    "start": 557,
                    "end": 881
                },
                {
                    "start": 882,
                    "end": 998
                },
                {
                    "start": 999,
                    "end": 1153
                },
                {
                    "start": 1154,
                    "end": 1233
                },
                {
                    "start": 1234,
                    "end": 1403
                },
                {
                    "start": 1404,
                    "end": 1522
                },
                {
                    "start": 1523,
                    "end": 1686
                },
                {
                    "start": 1687,
                    "end": 1928
                },
                {
                    "start": 1929,
                    "end": 2095
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.9404296875
        },
        {
            "corpus_id": "259836898",
            "title": "The Ethical Implications of Generative Audio Models: A Systematic Literature Review",
            "text": "weight music generation model to generate instrumental music. In analyzing their output they found that their model was sensitive to Western music theory in that it \"it maintains the configuration of the circle of fifths; distinguishes major and minor keys from interval vectors, and manifests meaningful structures between music phases\" [68]  \u2020 . Machine learning models often perpetuate biases in the training data, and generative models are no different. It is important to be aware of the composition of the training data to understand what biases could be perpetuated. 4.2.4 Copyright Infringement. Perhaps one of the most important considerations of generative music models-both ethically and potentially legally-was only discussed by two papers in the entire corpus: copyright infringement. There are many legality questions surrounding the copyright of AI generated content. At least three lawsuits in early 2023 are currently discussing whether models trained on publicly available works constitute copyright infringement [31]. Research in the text and vision domain is even geared toward specifically identifying to what degree generative models are memorizing training data [11,39,56] or are producing outputs with \"substantial similarity\" to items in the training set [62]. However, in this corpus of generative audio models, only two papers discussed the potential for copyright infringement [2, 20]  \u2020 . Esling et al. focused their research on maximizing novelty in the music generation system in order to subvert the potential for copyright issues and increase creativity in their generation [20]  \u2020 . Agostinelli et al. \"conducted a thorough study of memorization, adapting and extending a methodology used in the context of text-based LLMs\" in order to determine the degree to which their model memorized the training dataset and understand the potential for copyright infringement [2]  \u2020 . Of the remaining 75 papers discussing generative music models (97%), not one discussed the potential for copyright infringement or training data memorization. 4.2.5 Cultural Appropriation. Generative audio models sometimes train on incomprehensible amounts of training data, and it follows that some of this training data comes from cultures outside the creator of the algorithm or users of the model.",
            "score": 0.425815445460212,
            "section_title": "Predominance of Western Bias. Zhao et al. proposed a light-",
            "char_start_offset": 28668,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 61
                },
                {
                    "start": 62,
                    "end": 347
                },
                {
                    "start": 348,
                    "end": 457
                },
                {
                    "start": 458,
                    "end": 573
                },
                {
                    "start": 574,
                    "end": 579
                },
                {
                    "start": 580,
                    "end": 603
                },
                {
                    "start": 604,
                    "end": 797
                },
                {
                    "start": 798,
                    "end": 882
                },
                {
                    "start": 883,
                    "end": 1036
                },
                {
                    "start": 1037,
                    "end": 1285
                },
                {
                    "start": 1286,
                    "end": 1417
                },
                {
                    "start": 1418,
                    "end": 1616
                },
                {
                    "start": 1617,
                    "end": 1907
                },
                {
                    "start": 1908,
                    "end": 2066
                },
                {
                    "start": 2067,
                    "end": 2096
                },
                {
                    "start": 2097,
                    "end": 2309
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.235107421875
        },
        {
            "corpus_id": "259501495",
            "title": "Measuring the Success of Diffusion Models at Imitating Human Artists",
            "text": "Modern diffusion models have set the state-of-the-art in AI image generation. Their success is due, in part, to training on Internet-scale data which often includes copyrighted work. This prompts questions about the extent to which these models learn from, imitate, or copy the work of human artists. This work suggests that tying copyright liability to the capabilities of the model may be useful given the evolving ecosystem of generative models. Specifically, much of the legal analysis of copyright and generative systems focuses on the use of protected data for training. As a result, the connections between data, training, and the system are often obscured. In our approach, we consider simple image classification techniques to measure a model's ability to imitate specific artists. Specifically, we use Contrastive Language-Image Pretrained (CLIP) encoders to classify images in a zero-shot fashion. Our process first prompts a model to imitate a specific artist. Then, we test whether CLIP can be used to reclassify the artist (or the artist's work) from the imitation. If these tests match the imitation back to the original artist, this suggests the model can imitate that artist's expression. Our approach is simple and quantitative. Furthermore, it uses standard techniques and does not require additional training. We demonstrate our approach with an audit of Stable Diffusion's capacity to imitate 70 professional digital artists with copyrighted work online. When Stable Diffusion is prompted to imitate an artist from this set, we find that the artist can be identified from the imitation with an average accuracy of 81.0%. Finally, we also show that a sample of the artist's work can be matched to these imitation images with a high degree of statistical reliability. Overall, these results suggest that Stable Diffusion is broadly successful at imitating individual human artists.",
            "score": 0.4255214297276823,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.974609375
        },
        {
            "corpus_id": "267412857",
            "title": "Copyright Protection in Generative AI: A Technical Perspective",
            "text": "On the other hand, the protection in some domains like text is much harder than image, because the modification on image can be designed as invisible to human eyes but effective on DGMs, while text is discrete and is hard to be designed as imperceptible. Despite the difficulty, it is necessary to propose the protection from the side of source data owner for other domains, especially in text domain which is fast developing and causes increasing concerns in the data copyright. Also, the copyright protection for multi-modality generation [85,101,110,155] in both data copyright and model copyright is also crucial. \n\n\u2022 Infringement detection. Injecting watermark into images to accelerate the infringement detection has been introduced in Section 2.3.2, which aims to help source data owners protect their data copyright. Infringement detection can also benefit the model builder. Before releasing the generated output to the users, the builder can first check whether the output is infringing the copyright of training data (like a memorized training sample). If the output to be released is detected as infringement, the model builder can use some following-up strategies like adding references to the source of the content or removing the suspicious part to avoid the infringement. However, detecting infringement on the training data is not trivial, especially that current DGMs usually require a large amount of training data to ensure the generated quality and diversity. The dataset de-duplication in Section 2.3.4 and Section 3.3.1 provides potential solutions. It is an offline strategy that can be achieved before training. However, for infringement detection, a faster real-time searching is necessary to mitigate negative impact on the generation speed. Another issue is that, compared with memorization problem, the infringement on the copyright of abstract concepts, such as the style of an artwork or a product, and the original storylines and characters created by authors, is hard to confirm. The memorized samples often bear a high resemblance to the copyrighted material 12 , making infringement easy to confirm. In contrast, if a generated sample replicates the style of a picture but diverges significantly in content from the original, it becomes much harder to detect and confirm whether the generated sample stems from copyrighted data or not. \n\nSecond, for the model copyright protection, we present the following aspects: \n\n\u2022 Robustness.",
            "score": 0.4254176410578965,
            "section_title": "Discussion",
            "char_start_offset": 92009,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 254
                },
                {
                    "start": 255,
                    "end": 479
                },
                {
                    "start": 480,
                    "end": 617
                },
                {
                    "start": 620,
                    "end": 645
                },
                {
                    "start": 646,
                    "end": 824
                },
                {
                    "start": 825,
                    "end": 883
                },
                {
                    "start": 884,
                    "end": 1063
                },
                {
                    "start": 1064,
                    "end": 1287
                },
                {
                    "start": 1288,
                    "end": 1480
                },
                {
                    "start": 1481,
                    "end": 1572
                },
                {
                    "start": 1573,
                    "end": 1636
                },
                {
                    "start": 1637,
                    "end": 1768
                },
                {
                    "start": 1769,
                    "end": 2012
                },
                {
                    "start": 2013,
                    "end": 2134
                },
                {
                    "start": 2135,
                    "end": 2370
                },
                {
                    "start": 2373,
                    "end": 2450
                },
                {
                    "start": 2453,
                    "end": 2466
                }
            ],
            "ref_mentions": [
                {
                    "start": 545,
                    "end": 549,
                    "matchedPaperCorpusId": "7588858"
                },
                {
                    "start": 549,
                    "end": 553,
                    "matchedPaperCorpusId": "254854449"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.87060546875
        },
        {
            "corpus_id": "272146279",
            "title": "RLCP: A Reinforcement Learning-based Copyright Protection Method for Text-to-Image Diffusion Model",
            "text": "we propose a novel approach to minimize copyright infringement in text-to-image diffusion models by leveraging reinforcement learning (RL) and our proposed copyright metrics. We first define a copyright metric to measure how closely a generated image resembles copyrighted content. Then, we integrate this copyright metric into reward function and employ reinforcement learning techniques to fine-tune a pre-trained text-to-image diffusion model. Specifically, the model is trained to maximize the reward by iteratively adjusting its parameters to reduce the likelihood of producing copyright-infringing images. By doing so, we ensure that the model maintains high image quality while adhering to copyright constraints. \n\nAs shown in Figure 2, the training process of RLCP is as follows: \n\n\u2022 Gather Datasets: Compile datasets that include both original and copyright-infringing samples. \u2022 Prompts Generation: Fed these images into the CLIP interrogator, allowing us to obtain prompts that correspond to each anchor image. The CLIP Interrogator is utilized to convert copyrighted images into corresponding textual information. This text is subsequently refined and transformed into prompts, which are then inputted",
            "score": 0.4250632094260996,
            "section_title": "Overview",
            "char_start_offset": 9821,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 174
                },
                {
                    "start": 175,
                    "end": 281
                },
                {
                    "start": 282,
                    "end": 446
                },
                {
                    "start": 447,
                    "end": 611
                },
                {
                    "start": 612,
                    "end": 719
                },
                {
                    "start": 722,
                    "end": 787
                },
                {
                    "start": 790,
                    "end": 886
                },
                {
                    "start": 887,
                    "end": 1021
                },
                {
                    "start": 1022,
                    "end": 1125
                },
                {
                    "start": 1126,
                    "end": 1213
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.98779296875
        },
        {
            "corpus_id": "271310121",
            "title": "Unlearning Concepts from Text-to-Video Diffusion Models",
            "text": "Recent text-to-video diffusion generative models (Wang et al. 2023;Ho et al. 2022;Yin et al. 2023) have attracted attention because of their outstanding video quality, stable learning procedure, and seemingly infinite generation capabilities, surpassing the previous state-of-art generative adversarial networks (Goodfellow et al. 2020(Goodfellow et al. , 2014)).Classifierfree guidance (Ho and Salimans 2021) allows us generate high-quality videos on the basis of natural language input.These models are able to imitate a wide range of concepts since they are trained on vast internet datasets.Their ability to imitate potentially copyrighted content is a major concern regarding text-to-video models.They can faithfully generate copyrighted videos such as \"Snoopy,\" an iconic beagle dog from the beloved comic strip Peanuts, as shown in Figure 1.The AI-generated art is on par with human-generated art.Another issue is that the models can faithfully replicate an artist's style.Users of large-scale textto-video generation systems can use prompts including \"in the style of [artist]\" to mimic the styles of specific artists, which may reduce the value of the original work.The Van Gogh-style video \"a panda taking a selfie\" is shown in Figure 1.Some artists have sued the makers and providers of certain generation models, raising new legal issues (Setty 2023).\n\nApart from copyright infringement issues, privacy and safety are other major concerns.Text-to-video diffusion models can generate specific facial characteristics through text prompts that include names, if the training datasets contain corresponding videos of those names.An example of this is shown in Figure 1: a generated video of Elon Musk.However, this generation of facial characteristics may raise concerns about privacy and portrait rights as outlined in the Civil Code of the People's Republic of China.Additionally, malicious use of these generated videos could contribute to the spread of fake news and misinformation.Text-to-video Figure 2: Overview of our proposed method.\u03b8 denotes that parameters are fixed and \u03b2 denotes that parameters need to be optimized.diffusion models are also capable of generating nude and pornographic videos.These concerns all necessitate technical solutions.",
            "score": 0.4249319365478829,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 363
                },
                {
                    "start": 363,
                    "end": 488
                },
                {
                    "start": 488,
                    "end": 595
                },
                {
                    "start": 595,
                    "end": 702
                },
                {
                    "start": 702,
                    "end": 848
                },
                {
                    "start": 848,
                    "end": 904
                },
                {
                    "start": 904,
                    "end": 980
                },
                {
                    "start": 980,
                    "end": 1175
                },
                {
                    "start": 1175,
                    "end": 1247
                },
                {
                    "start": 1247,
                    "end": 1363
                },
                {
                    "start": 1365,
                    "end": 1451
                },
                {
                    "start": 1451,
                    "end": 1637
                },
                {
                    "start": 1637,
                    "end": 1709
                },
                {
                    "start": 1709,
                    "end": 1877
                },
                {
                    "start": 1877,
                    "end": 1994
                },
                {
                    "start": 1994,
                    "end": 2050
                },
                {
                    "start": 2050,
                    "end": 2137
                },
                {
                    "start": 2137,
                    "end": 2214
                },
                {
                    "start": 2214,
                    "end": 2265
                }
            ],
            "ref_mentions": [
                {
                    "start": 82,
                    "end": 97,
                    "matchedPaperCorpusId": "262823915"
                },
                {
                    "start": 312,
                    "end": 335,
                    "matchedPaperCorpusId": "1033682"
                },
                {
                    "start": 335,
                    "end": 362,
                    "matchedPaperCorpusId": "10319744"
                },
                {
                    "start": 387,
                    "end": 409,
                    "matchedPaperCorpusId": "249145348"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.7958984375
        },
        {
            "corpus_id": "271269956",
            "title": "Safe-SD: Safe and Traceable Stable Diffusion with Text Prompt Trigger for Invisible Generative Watermarking",
            "text": "\"In art, what we want is the certainty that one spark of original genius shall not be extinguished.\"\n\n-Mary Cassatt Recent years have witnessed the remarkable success of diffusion models [21, 40-42, 47, 56], due to its impressive generative capabilities.After surpassing GAN on image synthesis [11], diffusion models have shown a promising algorithm with dense theoretical founding, and emerged as the new state-of-the-art among the deep generative models [18,22,29,38,46,52,55,57,58,62,69].Notably, Stable Diffusion [53], as one of the most popular and sought-after generative models, has sparked the interest of many researchers, and a series of SD-based works have been proposed and exploited to produce plenty of AI-created or AI-edited images, such as Con-trolNet [72], SDEdit [43], DreamBooth [54], Imagic [27], Instruct-Pix2Pix [3] and Null-text Inversion [44], which raises profound concerns about ethical and legal risks [49,50] for AI-generated content (AIGC) being unscrupulously exposed on public platforms and raises new challenges for copyright protection and content monitoring.\n\nThese concerns may be elaborated into the following three aspects: (1) Originator Concern.An artistic work or photograph produced by the original author may be edited or modified at will by AI today and published to the public platform for commercial profit, which infringes on the interests of the originator.Take Figure 1 as an example, when a wonderful hand-crafted watercolor painting is published online by the originator, another user could download it without any restrictions and then request the SD-based model to edit the artwork through an accompanying prompt \"please edit a watercolor picture of... \", whereas ultimately attributes the AIcreated production and its ancillary value to the user and the given prompt, which may have violated the rights of the originator.If this is a commercial advertisement or model shooting, product designs or industrial drawings, etc., it may cause more serious infringement of interests.(2) Developer Concern.",
            "score": 0.4243567818137107,
            "section_title": "INTRODUCTION",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 99
                },
                {
                    "start": 99,
                    "end": 100
                },
                {
                    "start": 102,
                    "end": 254
                },
                {
                    "start": 254,
                    "end": 491
                },
                {
                    "start": 491,
                    "end": 1093
                },
                {
                    "start": 1095,
                    "end": 1185
                },
                {
                    "start": 1185,
                    "end": 1405
                },
                {
                    "start": 1405,
                    "end": 1875
                },
                {
                    "start": 1875,
                    "end": 2030
                },
                {
                    "start": 2030,
                    "end": 2052
                }
            ],
            "ref_mentions": [
                {
                    "start": 294,
                    "end": 298,
                    "matchedPaperCorpusId": "234357997"
                },
                {
                    "start": 456,
                    "end": 460,
                    "matchedPaperCorpusId": "244714856"
                },
                {
                    "start": 460,
                    "end": 463,
                    "matchedPaperCorpusId": "249145348"
                },
                {
                    "start": 463,
                    "end": 466,
                    "matchedPaperCorpusId": "235694314"
                },
                {
                    "start": 478,
                    "end": 481,
                    "matchedPaperCorpusId": "235352469"
                },
                {
                    "start": 484,
                    "end": 487,
                    "matchedPaperCorpusId": "235390993"
                },
                {
                    "start": 517,
                    "end": 521,
                    "matchedPaperCorpusId": "245335280"
                },
                {
                    "start": 782,
                    "end": 786,
                    "matchedPaperCorpusId": "245704504"
                },
                {
                    "start": 799,
                    "end": 803,
                    "matchedPaperCorpusId": "251800180"
                },
                {
                    "start": 812,
                    "end": 816,
                    "matchedPaperCorpusId": "252918469"
                },
                {
                    "start": 835,
                    "end": 838,
                    "matchedPaperCorpusId": "253581213"
                },
                {
                    "start": 863,
                    "end": 867,
                    "matchedPaperCorpusId": "253581838"
                },
                {
                    "start": 930,
                    "end": 934,
                    "matchedPaperCorpusId": "268247491"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.78271484375
        },
        {
            "corpus_id": "271956731",
            "title": "Randomization Techniques to Mitigate the Risk of Copyright Infringement",
            "text": "Any generation of text that is substantially similar to a copyrighted work is by chance, and this chance is the same as a model that has never seen the original work. Therefore, it is safe to claim that the model p(\u2022|x) is not infringing copyright. Generally, if a generative model satisfies NAF with a small k x , then one can claim it is less likely that the model is outputting something substantially similar with a probability that is much larger than a random chance.",
            "score": 0.4235819931220413,
            "section_title": "Prelimenary on Near Access-Freeness",
            "char_start_offset": 8046,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 166
                },
                {
                    "start": 167,
                    "end": 248
                },
                {
                    "start": 249,
                    "end": 473
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.439697265625
        },
        {
            "corpus_id": "261697329",
            "title": "Elucidating the Solution Space of Extended Reverse-Time SDE for Diffusion Models",
            "text": "In line with other advanced deep generative models like GANs, DMs can be harnessed to produce deceptive or misleading content, particularly in manipulated images. The efficient solvers we propose herein offer the capability to expedite the sampling process of DMs, thereby enabling faster image generation and manipulation, potentially leading to the creation of convincing but fabricated visuals. As with any technology, this acceleration could accentuate the potential ethical concerns associated with DMs, particularly their susceptibility to misuse or malicious applications. For instance, more frequent image generation might elevate the likelihood of unauthorized exposure of personal information, facilitate content forgery and dissemination of false information, and potentially infringe upon intellectual property rights.",
            "score": 0.42356092340015083,
            "section_title": "Impact Statement",
            "char_start_offset": 25548,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 162
                },
                {
                    "start": 163,
                    "end": 397
                },
                {
                    "start": 398,
                    "end": 579
                },
                {
                    "start": 580,
                    "end": 830
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.331298828125
        },
        {
            "corpus_id": "277711160",
            "title": "Legal regulation of AI-assisted academic writing: challenges, frameworks, and pathways",
            "text": "Copyright infringement risks in AI-assisted academic writing stem primarily from the legality of AI system training data and the infringement risks of generated content. Regarding training data, large language models use large amounts of potentially copyright-protected textual materials during training, and the legal nature of this usage remains controversial (European-Union, 2019). The Authors Guild v. OpenAI case centrally reflects this issue, and its judgment will significantly impact the legitimate scope of AI training data usage (Niu, 2023). \n\nRegarding generated content, AI systems may tend to reproduce expression patterns from training data, increasing the potential risks of copyright infringement (Xiao, 2024;European-Union, 2019). Research indicates that large language models may unconsciously reproduce (copy or adapt) expressions from training data during generation, and this \"latent derivation,\" where AI systems subtly incorporate preexisting expressions without explicit attribution, presents new challenges for copyright protection. \n\nIn response to these risks, we propose systematic prevention mechanisms. First, we recommend improving AI usage registration systems that require researchers to maintain detailed records of AI tool usage, including key information such as purpose, scope, and extent. Second, we should develop specialized content similarity checks and source tracing systems to identify potential infringement issues quickly through technical means. Finally, we should establish clear review standards to regulate the limits of AI-assisted creation (Gulumbe et al., 2024).",
            "score": 0.4231919495208607,
            "section_title": "Infringement risk prevention",
            "char_start_offset": 9338,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 169
                },
                {
                    "start": 170,
                    "end": 385
                },
                {
                    "start": 386,
                    "end": 552
                },
                {
                    "start": 555,
                    "end": 748
                },
                {
                    "start": 749,
                    "end": 1058
                },
                {
                    "start": 1061,
                    "end": 1133
                },
                {
                    "start": 1134,
                    "end": 1327
                },
                {
                    "start": 1328,
                    "end": 1493
                },
                {
                    "start": 1494,
                    "end": 1616
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.312255859375
        },
        {
            "corpus_id": "269294137",
            "title": "An Economic Solution to Copyright Challenges of Generative AI",
            "text": "Generative artificial intelligence (AI) systems are trained on large data corpora to generate new pieces of text, images, videos, and other media. There is growing concern that such systems may infringe on the copyright interests of training data contributors. To address the copyright challenges of generative AI, we propose a framework that compensates copyright owners proportionally to their contributions to the creation of AI-generated content. The metric for contributions is quantitatively determined by leveraging the probabilistic nature of modern generative AI models and using techniques from cooperative game theory in economics. This framework enables a platform where AI developers benefit from access to high-quality training data, thus improving model performance. Meanwhile, copyright owners receive fair compensation, driving the continued provision of relevant data for generative model training. Experiments demonstrate that our framework successfully identifies the most relevant data sources used in artwork generation, ensuring a fair and interpretable distribution of revenues among copyright owners.",
            "score": 0.42318555152282367,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.9248046875
        },
        {
            "corpus_id": "257578557",
            "title": "Developing Artificial Intelligence-Based Content Creation: Are EU Copyright and Antitrust Law Fit for Purpose?",
            "text": "hird, once data from input nodes enter the subsequent layers of a neural network, the data inputted may be processed in such complex ways that literal infringement is avoided. For instance, the individual pixels of the image of a fox may be processed with other pixels so that no protected aspects of the image can be observed within the neural network. However, once a neural network learns to reproduce features of the training data, protected aspects or elements of works used in training a neural network could also be reproduced within the neural network. 10 or instance, excerpts or protected features of training materials could be reproduced (e.g. parts or aspects of an image that enjoys copyright protection). \n\nFinally, the outputs of neural networks when training or using the network may also constitute reproduction that infringes protected elements of the works used in training the application.11 Whether the outputs produced infringe copyright in the training works depends on the design of the neural network and the application it is used in.12",
            "score": 0.42289041122810883,
            "section_title": "Copyright Infringement in Development of Artificial Creativity Applications",
            "char_start_offset": 6553,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 175
                },
                {
                    "start": 176,
                    "end": 353
                },
                {
                    "start": 354,
                    "end": 563
                },
                {
                    "start": 564,
                    "end": 655
                },
                {
                    "start": 656,
                    "end": 719
                },
                {
                    "start": 722,
                    "end": 912
                },
                {
                    "start": 913,
                    "end": 1063
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.74853515625
        },
        {
            "corpus_id": "265551515",
            "title": "VA3: Virtually Assured Amplification Attack on Probabilistic Copyright Protection for Text-to-Image Generative Models",
            "text": "In Fig. 8, we show example outputs of three target copyrighted images under four attack and defense scenarios. Similar to Fig. 2, using a benign prompt (such as the original caption), in the first column, we can observe that outputs without copyright protection infringe the copyright of target images with high probability; in the second column, after  copyright protection, all samples are non-infringing content as CP-k rejects all infringing samples. In the third column, we find that an amplification attack with a benign prompt can be unsuccessful, because such a prompt may not provide a strictly positive probability of producing infringing generations from models protected by CP-k. However, in the last column, with an adversarial prompt obtained from our proposed Anti-NAF algorithm, we can see that most of the outputs are copyright-infringed, which means that the probability of infringing samples is largely amplified. In Fig. 9, we give detailed FAR-AR curves on each target copyrighted image in LAION-mi dataset. We can find that our proposed bandit amplification method performs more steadily in the worst cases. For example, in Figs. 9a and 9d, when acceptance rate is lower than 20%, the FAR of Anti-NAF with amplification is nearly 0%; while \u03b5-greedy-max/cdf bandit amplification can adapt to follow the best choice of prompts (e.g., PEZ or CLIP-Interrogator) and keep a competitive FAR score.",
            "score": 0.4215019652002828,
            "section_title": "Detailed Analysis on Results",
            "char_start_offset": 35988,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 110
                },
                {
                    "start": 111,
                    "end": 454
                },
                {
                    "start": 455,
                    "end": 691
                },
                {
                    "start": 692,
                    "end": 932
                },
                {
                    "start": 933,
                    "end": 1028
                },
                {
                    "start": 1029,
                    "end": 1129
                },
                {
                    "start": 1130,
                    "end": 1151
                },
                {
                    "start": 1152,
                    "end": 1413
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.92822265625
        },
        {
            "corpus_id": "265352103",
            "title": "On Copyright Risks of Text-to-Image Diffusion Models",
            "text": "In this work, we propose a data generation pipeline to generate realistic copyright-infringing examples on diffusion models. Our proposed pipeline generated prompts that are seemingly unrelated to the target copyrighted topic, but can still be utilized to produce copyrighted content. Additionally, our pipeline tackles partial copyright infringement. Through our proposal, we present a toolkit that includes potentially copyrighted topics, target images of copyright topics with annotations of copyrighted content, and a dataset generation pipeline. The toolkit can be used as a whole on diffusion models for testing copyright-related performance and generating copyright-violating samples. We demonstrate with this toolkit that contemporary diffusion models are highly susceptible to generating copyrighted content. The findings emphasize the immediate necessity for appropriate measures to prevent models from generating copyrighted materials. This is especially crucial since our study shows even common phrases can prompt models to create images containing copyrighted content. This work can serve copyright research for diffusion models. For instance, the research community can leverage this toolkit to assess diffusion models based on copyright-related criteria. Furthermore, copyright protection algorithms can employ it for effectiveness evaluation. \n\nAll Query Templates for Prompt Generation Form N phrases using all of the exact words in the exact order: KEYWORD0, KEYWORD1, . . . , KEYWORDN. The phrases should be similar to CATEGORY TOPIC. Form N sentences that start with the phrase START-PHRASE. Do not make reference to the CATEGORY TOPIC. Use words that are challenging to represent visually. Form N sentences that uses all of the exact words in the exact order: KEYWORD0, KEYWORD1, . . . , KEY-WORDN. Use the words continuously wherever possible. Ensure grammatical correctness. Start the sentence with the exact STARTPHRASE. Do not make reference to the CATEGORY TOPIC. Use words that are challenging to represent visually. \n\nTable 3: The list of all templates used for prompt generation on GPT3.5. Capitalized words are variables that can be changed. We query GPT models to generate candidate prompts that contain extracted keywords but have different semantic meanings from our target topics. We employ different queries to ensure diversity in the prompt generation.",
            "score": 0.4211661459107904,
            "section_title": "Conclusion",
            "char_start_offset": 30099,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 124
                },
                {
                    "start": 125,
                    "end": 284
                },
                {
                    "start": 285,
                    "end": 351
                },
                {
                    "start": 352,
                    "end": 550
                },
                {
                    "start": 551,
                    "end": 691
                },
                {
                    "start": 692,
                    "end": 817
                },
                {
                    "start": 818,
                    "end": 946
                },
                {
                    "start": 947,
                    "end": 1082
                },
                {
                    "start": 1083,
                    "end": 1143
                },
                {
                    "start": 1144,
                    "end": 1270
                },
                {
                    "start": 1271,
                    "end": 1359
                },
                {
                    "start": 1362,
                    "end": 1493
                },
                {
                    "start": 1494,
                    "end": 1505
                },
                {
                    "start": 1506,
                    "end": 1554
                },
                {
                    "start": 1555,
                    "end": 1612
                },
                {
                    "start": 1613,
                    "end": 1657
                },
                {
                    "start": 1658,
                    "end": 1711
                },
                {
                    "start": 1712,
                    "end": 1807
                },
                {
                    "start": 1808,
                    "end": 1820
                },
                {
                    "start": 1821,
                    "end": 1866
                },
                {
                    "start": 1867,
                    "end": 1898
                },
                {
                    "start": 1899,
                    "end": 1945
                },
                {
                    "start": 1946,
                    "end": 1990
                },
                {
                    "start": 1991,
                    "end": 2044
                },
                {
                    "start": 2047,
                    "end": 2119
                },
                {
                    "start": 2120,
                    "end": 2172
                },
                {
                    "start": 2173,
                    "end": 2315
                },
                {
                    "start": 2316,
                    "end": 2389
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.98095703125
        },
        {
            "corpus_id": "273654195",
            "title": "Copyright-Aware Incentive Scheme for Generative Art Models Using Hierarchical Reinforcement Learning",
            "text": "Text-to-image Models. Generative Adversarial Networks (GAN) [27] are one of the early approaches to generating images from text. Following works like StackGAN [43], AttnGAN [39] improve the quality and relevance of generated images of the text-to-image models. These year, diffusion models have become a prominent choice in generative modeling [2]. Diffusion models are a kind of generative models that differ from the traditional models like GANs in that they iteratively transform noisy images to recover the original images through a sequence of learned denoising steps [12]. [29] introduces Latent Diffusion Models (LDMs), which combines diffusion processes with latent variable modeling. The authors proposed a model that operates in a lower-dimensional latent space rather than pixel space. This approach significantly reduces computational costs and enables the generation of high-resolution images with less resource usage. The diffusion models have shown remarkable abilities in generating diverse and high-quality outputs. \n\nCopyright Protection. Copyright is crucial in AI-generated images to protect intellectual property, ensure fair use, and encourage innovation by legally securing the rights of creators and developers. In both US and EU law [21] [23], the substantiality of copyright infringement is one of the most important and measurable determinants. In practice, the court will perform extrinsic and intrinsic tests to measure the substantiality [4] and determine whether the infringement exists. There are some previous works focusing on building a generative model that avoids generating content mimic the copyrighted images. Existed method are machine unlearning, and fine-tuning. For example, \"Forgot-Me-Not\" [42] is a method that can safely remove specified IDs, objects, or styles from a wellconfigured text-to-image model in as little as 30 seconds. [34] proposes the concept \"Near Access Free(NAF)\" and also a method that can output generative models with strong bounds on the probability of sampling protected content. Reinforcement Learning. Reinforcement Learning is a popular pivotal area in machine learning, addressing the challenge of learning optimal actions through interaction with an environment. There are three basic types of RL: value-based, policy-based, and actorcritic.",
            "score": 0.4211432850765818,
            "section_title": "Related works",
            "char_start_offset": 6772,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 21
                },
                {
                    "start": 22,
                    "end": 128
                },
                {
                    "start": 129,
                    "end": 260
                },
                {
                    "start": 261,
                    "end": 348
                },
                {
                    "start": 349,
                    "end": 578
                },
                {
                    "start": 579,
                    "end": 692
                },
                {
                    "start": 693,
                    "end": 796
                },
                {
                    "start": 797,
                    "end": 931
                },
                {
                    "start": 932,
                    "end": 1032
                },
                {
                    "start": 1035,
                    "end": 1056
                },
                {
                    "start": 1057,
                    "end": 1235
                },
                {
                    "start": 1236,
                    "end": 1371
                },
                {
                    "start": 1372,
                    "end": 1518
                },
                {
                    "start": 1519,
                    "end": 1649
                },
                {
                    "start": 1650,
                    "end": 1705
                },
                {
                    "start": 1706,
                    "end": 1878
                },
                {
                    "start": 1879,
                    "end": 2049
                },
                {
                    "start": 2050,
                    "end": 2073
                },
                {
                    "start": 2074,
                    "end": 2237
                },
                {
                    "start": 2238,
                    "end": 2316
                }
            ],
            "ref_mentions": [
                {
                    "start": 60,
                    "end": 64,
                    "matchedPaperCorpusId": "1563370"
                },
                {
                    "start": 173,
                    "end": 177,
                    "matchedPaperCorpusId": "8858625"
                },
                {
                    "start": 344,
                    "end": 347,
                    "matchedPaperCorpusId": "265039918"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.95654296875
        },
        {
            "corpus_id": "265352103",
            "title": "On Copyright Risks of Text-to-Image Diffusion Models",
            "text": "Copyrighted Topics \n\nWe discuss how we select target topics to serve as inputs for our data generation pipeline. Our objective is to identify topics associated with copyrighted images that contain highly specific features. As such, generating these features would not be considered as transformative works, thereby resulting in explicit copyright infringement [Milner Library, 2023]. \n\nAs such, we concentrate on three distinct domains: movies, video games, and logos (trademarks). These domains are particularly well-aligned with potentially copyrighted subjects, as movies, video games, and logos are products meant for commercial usage. Hence, creators of these products have the incentive to protect their intellectual property. Further, copyright infringement in these domains would be in the form of explicit replication of the subjects rather than a form of style transfer. Images from these domains are also very popular, increasing the likelihood of their inclusion within diffusion model training sets. Additionally, we prioritize recently released movies and video games, to ensure that our samples are of high quality. Images from recent years are also more likely to be protected by copyright as they have not yet entered the public domain [Office, 2023]. Nevertheless, it is important to emphasize that our approach is a form of academic research and we thus refrain from asserting that the topics we have gathered in this study definitively qualify as copyrighted subjects. \n\nOne direction involves finding titles containing polysemic words or phrases. Polysemic refers to the capability of an object to have several possible meanings that vary depending on the context. An illustrative example of such a term is \"Halo\", which can refer to either a glowing ring above an angel's head or a ring around a planet or the video game. However, even when used in the former context, the generated image (Figure 1) is still from the video game. Another avenue is to identify broad categories that are over-represented. For instance, the superhero category is over-represented by the word \"Superman\" and the coffee brand is over-represented by \"Starbucks\". We find then when provided with generic prompts for images in these categories, diffusion models tend to generate images that contain the Superman logo or character (in the former case) or the Starbucks logo (in the latter).",
            "score": 0.4211418136269791,
            "section_title": "B Details on Collecting Potentially",
            "char_start_offset": 33227,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 18
                },
                {
                    "start": 21,
                    "end": 112
                },
                {
                    "start": 113,
                    "end": 222
                },
                {
                    "start": 223,
                    "end": 383
                },
                {
                    "start": 386,
                    "end": 481
                },
                {
                    "start": 482,
                    "end": 639
                },
                {
                    "start": 640,
                    "end": 732
                },
                {
                    "start": 733,
                    "end": 880
                },
                {
                    "start": 881,
                    "end": 1012
                },
                {
                    "start": 1013,
                    "end": 1130
                },
                {
                    "start": 1131,
                    "end": 1268
                },
                {
                    "start": 1269,
                    "end": 1488
                },
                {
                    "start": 1491,
                    "end": 1567
                },
                {
                    "start": 1568,
                    "end": 1685
                },
                {
                    "start": 1686,
                    "end": 1843
                },
                {
                    "start": 1844,
                    "end": 1951
                },
                {
                    "start": 1952,
                    "end": 2025
                },
                {
                    "start": 2026,
                    "end": 2162
                },
                {
                    "start": 2163,
                    "end": 2387
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.94970703125
        },
        {
            "corpus_id": "270620122",
            "title": "Fantastic Copyrighted Beasts and How (Not) to Generate Them",
            "text": "In addition to the discussion in \u00a7 1 on the unique challenges posed by copyrighted characters and the value of a quantitative study In this matter, we further expand on the legal background and broader societal impacts of our study below. \n\nLegal Motivations Past work has studied the setting of verbatim regurgitation of images (car, 2023), and some lawsuits focus on this particular legal issue (Vincent, 2023;Andersen et al. v. Stability AI et al., N.D. Cal. 2023). Some recent work builds datasets for benchmarking copyright infringement unlearning methods (Ma et al., 2024) and attempts to jailbreak proprietary systems to output copyrighted images (Kim et al., 2024). Among the copyrighted subjects of interest, copyrighted characters, such as popular Intellectual Property (IP) from Disney, Nintendo, and Dreamworks, pose a unique legal challenge (Sag, 2023;Henderson et al., 2023;Lee et al., 2024). At least one lawsuit in China has already resulted in liability for an image generation service that generated the copyrighted character, Ultraman (Shimbun, 2024). Unlike in the verbatim memorization setting, copyrighted characters are computationally more like general concepts that can appear in many poses, sizes, and variations in the training data. Typical deduplication, or even near access free learning approaches (Vyas et al., 2023), will not work for this task (Henderson et al., 2023). \n\nCopyrighted characters are also in a certain area of copyright law with distinct rules to determine infringement (Schreyer, 2015;Hennessey, 2020). To simplify the legal rules, characters are defined by key distinctive features that as a whole comprise the character. not (Liu, 2013;Lee, 2019). \n\nAs discussed earlier, to simplify the legal rules, characters are often defined by key distinctive features that as a whole compromise the character. This can lead to interesting situations. For example, in 2023 the copyright for the original version of Mickey Mouse character (Steamboat Willie) entered the public domain. But this version of the character did not wear white gloves.",
            "score": 0.420598434850858,
            "section_title": "A LEGAL BACKGROUND AND BROADER SOCIETAL IMPACTS",
            "char_start_offset": 36283,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 238
                },
                {
                    "start": 241,
                    "end": 468
                },
                {
                    "start": 469,
                    "end": 673
                },
                {
                    "start": 674,
                    "end": 906
                },
                {
                    "start": 907,
                    "end": 1070
                },
                {
                    "start": 1071,
                    "end": 1260
                },
                {
                    "start": 1261,
                    "end": 1403
                },
                {
                    "start": 1406,
                    "end": 1552
                },
                {
                    "start": 1553,
                    "end": 1672
                },
                {
                    "start": 1673,
                    "end": 1699
                },
                {
                    "start": 1702,
                    "end": 1851
                },
                {
                    "start": 1852,
                    "end": 1892
                },
                {
                    "start": 1893,
                    "end": 2024
                },
                {
                    "start": 2025,
                    "end": 2085
                }
            ],
            "ref_mentions": [
                {
                    "start": 1329,
                    "end": 1348,
                    "matchedPaperCorpusId": "257050406"
                },
                {
                    "start": 1519,
                    "end": 1535,
                    "matchedPaperCorpusId": "153105953"
                },
                {
                    "start": 1535,
                    "end": 1551,
                    "matchedPaperCorpusId": "226546743"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.2381591796875
        },
        {
            "corpus_id": "259064265",
            "title": "DeepfakeArt Challenge: A Benchmark Dataset for Generative AI Art Forgery and Data Poisoning Detection",
            "text": "Figure 1 shows the snapshot examples of our dataset. To the best of the authors' knowledge, there are currently no available datasets designed specifically for the purpose of exploring the detection of copyright infringement for generative AI in the domain of art. As such, this study represents a first attempt at producing a dataset for supporting researchers in this field in the development of machine learning algorithms for identifying copyright infringement by generative AI models. The development of such detection algorithms could significantly assist developers of generative AI systems in mitigating potential legal repercussions associated with their systems, as well as enable content creators and content curators to better identify copyright infringement by generative AI efficiently and systematically.",
            "score": 0.42048919372918314,
            "section_title": "Introduction",
            "char_start_offset": 4198,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 52
                },
                {
                    "start": 53,
                    "end": 264
                },
                {
                    "start": 265,
                    "end": 489
                },
                {
                    "start": 490,
                    "end": 819
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.85498046875
        },
        {
            "corpus_id": "260416806",
            "title": "Talkin' 'Bout AI Generation: Copyright and the Generative-AI Supply Chain (The Short Version)",
            "text": "Instead, they would involve copyright owners identifying copyrighted works and demanding that the model operator prevent generations that are substantially similar to those works. Some of those works might be identified based on known outputs that are recognizably similar to suspected inputs. But others might simply involve copyright owners handing over to model operators large catalogs of works to block, much as they currently do with ContentID on YouTube. Matching new generations against a catalog of copyrighted works is not a trivial problem, but it is one that has been very approximately solved by major social networks, which use perceptual hashing to prevent the upload of various kinds of identified content. Generative AI companies could at least add similar perceptualhash-driven filtering to the outputs of their models. Alternatively, they could retrain the models without the infringing works in the training dataset. This approach may only work on a prospective basis, however, due to the expense and time required to train a full model. It is often infeasible to retrain a model simply to remove infringing works, and it would be completely unworkable to retrain on each new notice. \n\nIn practice, there would likely be a gravitational force pulling the operator's duties towards the duties of a service provider under section 512(c) or (d): block infringing generations on notice, block infringing generations on actual knowledge, block infringing generations on red-flag knowledge, avoid having a business model that directly ties income to infringement, and terminate the abilities of repeat infringers to continue making generations. It is not the case that these specific requirements arise out of the tests for indirect infringement -the Napster rule is only that a provider must block the infringement of specific works as to which it has been provided notice. Instead, we suspect that the section 512 doctrines will be a convergence point because a model operator that does not implement them might seem to a court to look like an increasingly unappealing candidate for fair use. The reason for this is that courts have now had two decades of experience -which means two decades of precedents -with the section 512 safe harbors.",
            "score": 0.41993174340461215,
            "section_title": "Id. \u00a7 512(i)(1)(A",
            "char_start_offset": 52451,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 179
                },
                {
                    "start": 180,
                    "end": 293
                },
                {
                    "start": 294,
                    "end": 461
                },
                {
                    "start": 462,
                    "end": 722
                },
                {
                    "start": 723,
                    "end": 837
                },
                {
                    "start": 838,
                    "end": 936
                },
                {
                    "start": 937,
                    "end": 1057
                },
                {
                    "start": 1058,
                    "end": 1203
                },
                {
                    "start": 1206,
                    "end": 1658
                },
                {
                    "start": 1659,
                    "end": 1888
                },
                {
                    "start": 1889,
                    "end": 2108
                },
                {
                    "start": 2109,
                    "end": 2257
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.75537109375
        },
        {
            "corpus_id": "257771630",
            "title": "Foundation Models and Fair Use",
            "text": "of all the training data and the likelihood of verbatim (or significantly similar) extraction can reduce with the amount of training data. For example, Somepalli et al. (2022) found that extraction of training data from diffusion models is less likely if there is more diverse training data.\n\nWhen the model is capable of outputting both transformative and non-transformative content, it is also unresolved how the model itself (and model creators) should be treated as a function of secondary liability. The extraction of non-transformative content, according to our experiments and others, is often not straightforward. It requires effort on the part of model users and deployers to identify an extraction mechanism. If this is the case, one might instead argue that remedies should be limited to specific instances of extracted non-transformative content, not the model as a whole, which does not generate infringing output in the ordinary case. The model creator might also be insulated from liability on other fair use factors. For example, if they released the model under a non-commercial license and actively prevented its use for commercial purposes, they might argue that the nature of their model was non-commercial, increasing the likelihood of a fair use defense at this part of the liability chain. 24 As with other issues in this work all of this is actively being litigated and will be shaped over the coming years.\n\nDMCA Safe Harbor. The Digital Millennium Copyright Act (\"DMCA\") is a U.S. law created to address digital copyright issues that came about with the advancement of technology. The DMCA safe harbor provisions protect online service providers from legal responsibility for copyright infringement claims. DMCA protections might vary depending on a number of considerations, but we emphasize that they are not guaranteed for all model deployments. We examine several of these considerations here.\n\nDMCA protections for generative foundation models are uncertain. At first glance, it may seem like the Digital Millennium Copyright Act (DMCA) would protect machine learning model hosts. Like in other hosted sites, they would need to meet the relevant requirements like using a registered agent under DMCA \u00a7512(c)(2). Then they could put up a take-down request form and add filters for the offending model output when served with a take-down request under the",
            "score": 0.41991130886368677,
            "section_title": "Implied Licenses and Common Crawl.",
            "char_start_offset": 70835,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.370849609375
        },
        {
            "corpus_id": "271874323",
            "title": "Not Every Image is Worth a Thousand Words: Quantifying Originality in Stable Diffusion",
            "text": "In this work, we introduced a novel approach to assess the originality of images with Text-to-Image (T2I) Generative Diffusion models, and have investigated its behavior in this aspect under a controlled environment. Our methodology leverages the concept of familiarity within the model's training data to quantify the originality of tested images. By employing textual inversion techniques, we demonstrated that the number of tokens required to represent and reconstruct an image serves as a measure of its originality, without requiring access to the training data, nor a specific prompt that potentially poses copyright complications. \n\nOur analysis confirmed that T2I models are capable of producing new original content, highlighting the importance of training models on diverse and comprehensive datasets. These findings also challenge the traditional view of avoiding memorization in models. Instead, we propose that models should familiarize themselves with a broad spectrum of data, respecting copyright constraints, to enhance their ability to generate new content. \n\nIn summary, our study offers a fresh perspective on evaluating originality in the context of generative models, which can inform copyright analysis and assist in delineating the legal protection afforded to such images more efficiently and accurately. By quantifying the familiarity of concepts to the model, we provide insights that align with legal definitions and can aid in addressing copyright eligibility, infringement, and licensing issues. In addition to law-related applications, our approach opens up new avenues for research in the intersection of generative models, originality assessment, and generative quality. \n\nLimitations One of the primary constraints for the method is the reliance on textual inversion, which may not capture all aspects of originality in complex images. Additionally, our method's effectiveness is contingent on the quality and diversity of the training data, which might not always be optimal. Furthermore, the correlation between token count and originality, although significant, may not be universally applicable across different model architectures or datasets. Future research should explore alternative measures of originality and test the robustness of our approach across a broader range of models and data, making it readily available for deployment. Finally, our work demonstrates that T2I models can be utilized to discriminate original and nonoriginal work. That being said, an important motivation of our work is to assess originality of T2I content.",
            "score": 0.4196121252525046,
            "section_title": "Discussion",
            "char_start_offset": 23712,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 216
                },
                {
                    "start": 217,
                    "end": 348
                },
                {
                    "start": 349,
                    "end": 637
                },
                {
                    "start": 640,
                    "end": 811
                },
                {
                    "start": 812,
                    "end": 898
                },
                {
                    "start": 899,
                    "end": 1075
                },
                {
                    "start": 1078,
                    "end": 1329
                },
                {
                    "start": 1330,
                    "end": 1525
                },
                {
                    "start": 1526,
                    "end": 1703
                },
                {
                    "start": 1706,
                    "end": 1869
                },
                {
                    "start": 1870,
                    "end": 2010
                },
                {
                    "start": 2011,
                    "end": 2182
                },
                {
                    "start": 2183,
                    "end": 2376
                },
                {
                    "start": 2377,
                    "end": 2486
                },
                {
                    "start": 2487,
                    "end": 2580
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.916015625
        },
        {
            "corpus_id": "258865475",
            "title": "Can Copyright be Reduced to Privacy?",
            "text": "There is a growing concern that generative AI models will generate outputs closely resembling the copyrighted materials for which they are trained. This worry has intensified as the quality and complexity of generative models have immensely improved, and the availability of extensive datasets containing copyrighted material has expanded. Researchers are actively exploring strategies to mitigate the risk of generating infringing samples, with a recent line of work suggesting to employ techniques such as differential privacy and other forms of algorithmic stability to provide guarantees on the lack of infringing copying. In this work, we examine whether such algorithmic stability techniques are suitable to ensure the responsible use of generative models without inadvertently violating copyright laws. We argue that while these techniques aim to verify the presence of identifiable information in datasets, thus being privacy-oriented, copyright law aims to promote the use of original works for the benefit of society as a whole, provided that no unlicensed use of protected expression occurred. These fundamental differences between privacy and copyright must not be overlooked. In particular, we demonstrate that while algorithmic stability may be perceived as a practical tool to detect copying, such copying does not necessarily constitute copyright infringement. Therefore, if adopted as a standard for detecting an establishing copyright infringement, algorithmic stability may undermine the intended objectives of copyright law.",
            "score": 0.4193808452254855,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.9189453125
        },
        {
            "corpus_id": "278534729",
            "title": "DFA-CON: A Contrastive Learning Approach for Detecting Copyright Infringement in DeepFake Art",
            "text": "The DeepfakeArt Challenge [4] provides a large-scale benchmark for detecting copyright-infringing or adversarially manipulated images in the domain of AI-generated art. The dataset comprises over 32,000 image pairs, each labeled as either similar (indicating a generated version of an original artwork) or dissimilar (completely unrelated pairs). Each similar pair corresponds to a specific form of generation-based manipulation, known as attack type. The challenge enables the development and evaluation of models capable of distinguishing original artworks from manipulated or forged counterparts.",
            "score": 0.4192429906036502,
            "section_title": "Deepfake Art Challenge",
            "char_start_offset": 6305,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 168
                },
                {
                    "start": 169,
                    "end": 346
                },
                {
                    "start": 347,
                    "end": 451
                },
                {
                    "start": 452,
                    "end": 599
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.93603515625
        },
        {
            "corpus_id": "261049723",
            "title": "DUAW: Data-free Universal Adversarial Watermark against Stable Diffusion Customization",
            "text": "\u2022 We introduce a data-free solution that utilizes LLM and SD models to generate training data, eliminating the necessity of directly accessing highly confidential copyrighted images. \n\n\u2022 The experimental results demonstrate that the proposed DUAW can cause obvious distortion to the outputs of fine-tuned SD models, with 96.43% of output images successfully identified as infringing images by a simple classifier.",
            "score": 0.4191976634091923,
            "section_title": "Introduction",
            "char_start_offset": 3853,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 182
                },
                {
                    "start": 185,
                    "end": 413
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.5439453125
        },
        {
            "corpus_id": "274234829",
            "title": "Dark Miner: Defend against undesired generation for text-to-image diffusion models",
            "text": "This work will have a positive impact on our society. In the era of AIGC, there are numerous open-source or commercial generative models available for users. Each individual can easily access generated images. However, due to large-scale training datasets, generative models can generate undesired images inevitably, such as nudity and protected copyrights. Some malicious users use attacking methods to induce models to generate undesired content. To address this problem, we carry out this work to defend against undesired generation, including the one caused by various attacking methods.",
            "score": 0.4185848365485682,
            "section_title": "Potential Society Impacts",
            "char_start_offset": 34875,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 53
                },
                {
                    "start": 54,
                    "end": 157
                },
                {
                    "start": 158,
                    "end": 209
                },
                {
                    "start": 210,
                    "end": 357
                },
                {
                    "start": 358,
                    "end": 448
                },
                {
                    "start": 449,
                    "end": 591
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.7470703125
        },
        {
            "corpus_id": "272911316",
            "title": "AnyLogo: Symbiotic Subject-Driven Diffusion System with Gemini Status",
            "text": "The ability to manipulate logos could be benefit for product promotion, poster making, logo alteration in advertising position, etc., and the outpainting version with diversified highlighting backgrounds could ease the cost of the venue rental and model hiring. However, the misuse could incur the potential copyright problems with legal disputes. And the generative logos may confuse the consumers to discern the authenticity and impact the reputation of the brands. Furthermore, the population of the generative models could impact the graphic designers and brand professionals, as the automated logo alteration with similar semantic layout might reduce the demand for manual design work. We preclude the copyright infringement with infused watermarks [82] to fingerprint the generated images.",
            "score": 0.41825152573418156,
            "section_title": "D Boarder Impacts",
            "char_start_offset": 29315,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 261
                },
                {
                    "start": 262,
                    "end": 347
                },
                {
                    "start": 348,
                    "end": 467
                },
                {
                    "start": 468,
                    "end": 690
                },
                {
                    "start": 691,
                    "end": 795
                }
            ],
            "ref_mentions": [
                {
                    "start": 754,
                    "end": 758,
                    "matchedPaperCorpusId": "257767463"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.2218017578125
        },
        {
            "corpus_id": "267312299",
            "title": "IntentTuner: An Interactive Framework for Integrating Human Intentions in Fine-tuning Text-to-Image Generative Models",
            "text": "While the benefits of advanced generative capabilities are undeniable, their impressive power, coupled with enhanced customization options, also introduces significant ethical concerns, particularly concerning misinformation and intellectual property rights. For instance, the ease of fine-tuning models could lead to a surge in deepfake creations [58], enabling users to generate unauthorized synthetic representations of real individuals, especially celebrities. These representations risk being misused to disseminate false information, potentially causing harm to those depicted. Additionally, the artistic community raises alarms over copyright infringement, as their creations are increasingly harvested without consent for model fine-tuning purposes [27]. These issues may also, in the long term, exacerbate biases in AI-generated",
            "score": 0.41823839981641764,
            "section_title": "Customized AIGC: Ethical Risks and Responsive Strategies",
            "char_start_offset": 55649,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 258
                },
                {
                    "start": 259,
                    "end": 464
                },
                {
                    "start": 465,
                    "end": 583
                },
                {
                    "start": 584,
                    "end": 762
                },
                {
                    "start": 763,
                    "end": 837
                }
            ],
            "ref_mentions": [
                {
                    "start": 348,
                    "end": 352,
                    "matchedPaperCorpusId": "214014129"
                },
                {
                    "start": 757,
                    "end": 761,
                    "matchedPaperCorpusId": "270964765"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.65234375
        },
        {
            "corpus_id": "260416806",
            "title": "Talkin' 'Bout AI Generation: Copyright and the Generative-AI Supply Chain (The Short Version)",
            "text": "All generative-AI models are trained on some type of data: examples of the type of material that they are to generate. Text-generation systems are trained on text, image-generation systems are trained on images, music-generation systems on music, and so on. All of this data must come from somewhere. The most prominent current generative AI models have been trained on data that already existed and was not created specifically for the purpose of AI training. And for AIs that generate potentially copyrightable (or copyrightinfringing) material, the training data itself will often include copyrightable expression. GitHub Copilot is trained on copyrighted code, ChatGPT is trained on textual data scraped from the web, Stable Diffusion is trained on images, and so on. \n\nFor the most part, it is the copyright owners of these individual training works who are the potential plaintiffs in any copyright infringement suit against actors at other stages of the supply chain. These are the relevant copyrights. \n\nIndividual pieces of training data are useless by themselves. 1 The AI training process requires vast quantities of data to create cutting-edge models, and those vast quantities of data must be arranged into datasets that have recurring, standard structure. 2 Sometimes this process is carried out by the same entities that train the AI models. More commonly, however, the process is split across different actors. Stable Diffusion, for example, is trained on images from datasets curated by the non-profit organization LAION. It is necessary, therefore, to consider the potential liability of dataset curators separately from the potential liability of model trainers. \n\nNote that dataset curation, as just described, will frequently involve \"the collection and assembling of preexisting materials or of data that are selected, coordinated, or arranged in such a way that the resulting work as a whole constitutes an original work of authorship. \" 3 As such, training datasets can themselves be copyrighted, such that the copying without permission of the dataset as a whole could constitute infringement, separate and apart from infringement on the underlying works the dataset comprises. In practice, however, it appears that most uses of training datasets are licensed -either through a bilateral negotiation or by means of an open-source license offered to the world by the dataset compiler.",
            "score": 0.4182149787191786,
            "section_title": "A. Data Creation",
            "char_start_offset": 1080,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 118
                },
                {
                    "start": 119,
                    "end": 257
                },
                {
                    "start": 258,
                    "end": 300
                },
                {
                    "start": 301,
                    "end": 460
                },
                {
                    "start": 461,
                    "end": 617
                },
                {
                    "start": 618,
                    "end": 771
                },
                {
                    "start": 774,
                    "end": 974
                },
                {
                    "start": 975,
                    "end": 1009
                },
                {
                    "start": 1012,
                    "end": 1271
                },
                {
                    "start": 1272,
                    "end": 1356
                },
                {
                    "start": 1357,
                    "end": 1426
                },
                {
                    "start": 1427,
                    "end": 1538
                },
                {
                    "start": 1539,
                    "end": 1681
                },
                {
                    "start": 1684,
                    "end": 1958
                },
                {
                    "start": 1959,
                    "end": 2202
                },
                {
                    "start": 2203,
                    "end": 2408
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.92724609375
        },
        {
            "corpus_id": "276558342",
            "title": "CopyJudge: Automated Copyright Infringement Identification and Mitigation in Text-to-Image Diffusion Models",
            "text": "This helps avoid infringement while maintaining the desired output characteristics, even without changing the original prompts. \n\nIn summary, our contributions are as follows: \n\n\u2022 We propose CopyJudge, an automated abstractionfiltration-comparison framework powered by a multi-LVLM debate mechanism, designed to efficiently detect copyright-infringing images generated by text-toimage diffusion models. \n\n\u2022 Given the judgment, we introduce an adaptive mitigation strategy that automatically optimizes prompts and explores non-infringing latent noise vectors of diffusion models, effectively mitigating copyright violations while preserving non-infringing expressions. \n\n\u2022 Extensive experiments demonstrate that our identification method matches state-of-the-art performance, with improved generalization and interpretability, while our mitigation approach more effectively prevents infringement without losing non-infringing expressions. \n\nImage infringement detection and mitigation. The current mainstream infringing image detection methods primarily measure the distance or invariance in pixel or embedding space (Carlini et al., 2023;Somepalli et al., 2023a;Shi et al., 2024b;Wang et al., 2021;2024b (Wen et al., 2024;Wang et al., 2024d) have shown that these methods have lower generalization capabilities and lack interpretability because they do not fully align with human judgment standards. For copyright infringement mitigation, the current approaches mainly involve machine unlearning to remove the model's memory of copyright information (Bourtoule et al., 2021;Nguyen et al., 2022;Kumari et al., 2023;Zhang et al., 2024) or deleting duplicated samples from the training data (Webster et al., 2023;Somepalli et al., 2023b",
            "score": 0.41820132967910045,
            "section_title": "Introduction",
            "char_start_offset": 4077,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 127
                },
                {
                    "start": 130,
                    "end": 175
                },
                {
                    "start": 178,
                    "end": 402
                },
                {
                    "start": 405,
                    "end": 667
                },
                {
                    "start": 670,
                    "end": 937
                },
                {
                    "start": 940,
                    "end": 984
                },
                {
                    "start": 985,
                    "end": 1399
                },
                {
                    "start": 1400,
                    "end": 1733
                }
            ],
            "ref_mentions": [
                {
                    "start": 1116,
                    "end": 1138,
                    "matchedPaperCorpusId": "256389993"
                },
                {
                    "start": 1138,
                    "end": 1162,
                    "matchedPaperCorpusId": "254366634"
                },
                {
                    "start": 1204,
                    "end": 1222,
                    "matchedPaperCorpusId": "270309880"
                },
                {
                    "start": 1550,
                    "end": 1574,
                    "matchedPaperCorpusId": "208909851"
                },
                {
                    "start": 1594,
                    "end": 1614,
                    "matchedPaperCorpusId": "257687839"
                },
                {
                    "start": 1710,
                    "end": 1733,
                    "matchedPaperCorpusId": "258987384"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.990234375
        },
        {
            "corpus_id": "268532352",
            "title": "A Dataset and Benchmark for Copyright Infringement Unlearning from Text-to-Image Diffusion Models",
            "text": "We reuse the CM metric to indicate the similarities between the original copyright images and their unlearned counterparts after processed by unlearning methods.Additionally, we evaluate changes of CLIP scores, denoted as \u2206CLIP, for text-image similarity.This indicates the extent to which the prompt that generates potential infringement is nullified.\n\nExtent of Model Degradation during Unlearning.The unlearning process inherently degrades the model by eliminating certain infringement-suspected concepts.Nevertheless, it is vital to preserve the Stable Diffusion model's generation capacities for copyright-irrelevant contents.We assess the degree of model degradation using the widely-recognized FID (Fr\u00e9chet Inception Distance) metric [20].\n\nOur benchmark facilitates a straightforward evaluation of potential copyright infringement, while facilitating comparison among various unlearning methods.Moreover, we perform comprehensive benchmark tests on our proposed CPDM dataset.In our experiments, we utilize gradient ascent-based and response-based pruning methods for unlearning, as comparison baselines for other unlearning approaches, specifically targeting the Stable Diffusion models.This evaluation provides valuable insights into assessing copyright infringement and the efficacy of unlearning methods in reducing infringement risks, while preserving the ability to generate non-infringing contents.",
            "score": 0.4180104981251659,
            "section_title": "Effectiveness of Unlearning.",
            "char_start_offset": 3524,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 161
                },
                {
                    "start": 161,
                    "end": 255
                },
                {
                    "start": 255,
                    "end": 352
                },
                {
                    "start": 354,
                    "end": 400
                },
                {
                    "start": 400,
                    "end": 508
                },
                {
                    "start": 508,
                    "end": 631
                },
                {
                    "start": 631,
                    "end": 746
                },
                {
                    "start": 748,
                    "end": 903
                },
                {
                    "start": 903,
                    "end": 983
                },
                {
                    "start": 983,
                    "end": 1195
                },
                {
                    "start": 1195,
                    "end": 1412
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.97265625
        },
        {
            "corpus_id": "268049158",
            "title": "Break It 'Til You Make It: An Exploration of the Ramifications of Copyright Liability Under a Pre-training Paradigm of AI Development",
            "text": "Generative AI can facilitate output or model infringements by downstream developers [Lu et al. 2019].I draw an analogy between indirect liability from pre-trained models and indirect copyright litigation involving peer-to-peer file-sharing networks to identify opportunities for model developers to subvert policy goals.I highlight these strategies to illustrate how the \"breaking\" of the AI development chain within a pre-training paradigm could be further exploited, allowing involved players to \"make it'to bypass liability for the use of copyrighted works.In line with scholarship that emphasizes accountability at each stage of model development [Khan and Hanna 2023], I propose duties that may apply during key stages of model development: training and deployment.I conclude with a discussion of copyright's role within the AI policy landscape.Proposed evaluations in the literature of the legality of the use of copyrighted works to train AI models heavily rely on model application and model outputs, and hinge on analysis through copyright's protection of expression.As part of these evaluations, the developmental process and technical architecture is conceptualized as a single pipeline-a single entity that builds a single model toward a single application.Indeed, model training is conceptualized as a process that learns toward a particular application.Sobel [2017] posits that the use of copyrighted works to train facial recognition algorithms does not implicate copyright protections: \"Training facial recognition algorithms on copyrighted photographs does not implicate the works' protectable aspects.The use analyzes factual information-the unique physical features of a subject's face-in the photographs, rather than photographers' expressive choices.\" As part of this analysis, a machine learning model that recognizes faces-a non-expressive application-is conceptualized to take only from the non-expressive aspects of the copyrighted works in the training process.Since copyright law protects only an author's original expression, this non-expressive use of copyrighted works then falls outside the scope of copyright protections and is non-infringing.This framing is reflected throughout the legal literature.Quang [2021] explains that ML models that mine data for \"functional, non-expressive purposes\" extract \"unprotectable ideas and patterns from data\" as part of their training process.",
            "score": 0.4177614884828659,
            "section_title": "INTRODUCTION",
            "char_start_offset": 2302,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 101
                },
                {
                    "start": 101,
                    "end": 320
                },
                {
                    "start": 320,
                    "end": 560
                },
                {
                    "start": 560,
                    "end": 770
                },
                {
                    "start": 770,
                    "end": 850
                },
                {
                    "start": 850,
                    "end": 1076
                },
                {
                    "start": 1076,
                    "end": 1269
                },
                {
                    "start": 1269,
                    "end": 1367
                },
                {
                    "start": 1367,
                    "end": 1619
                },
                {
                    "start": 1619,
                    "end": 1771
                },
                {
                    "start": 1771,
                    "end": 1987
                },
                {
                    "start": 1987,
                    "end": 2175
                },
                {
                    "start": 2175,
                    "end": 2233
                },
                {
                    "start": 2233,
                    "end": 2414
                }
            ],
            "ref_mentions": [
                {
                    "start": 84,
                    "end": 99,
                    "matchedPaperCorpusId": "199453025"
                },
                {
                    "start": 651,
                    "end": 672,
                    "matchedPaperCorpusId": "252866086"
                },
                {
                    "start": 1367,
                    "end": 1379,
                    "matchedPaperCorpusId": "115500744"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.283203125
        },
        {
            "corpus_id": "277468540",
            "title": "Unfair Learning: GenAI Exceptionalism and Copyright Law",
            "text": "It is no secret that GenAI 3 requires trillions of words and sub-word components (called tokens) to train the most capable models. These trillions of tokens mostly come from billions of publicly accessible web pages. Most of the content on these web pages is protected by copyright law, including books, songs, videos, and more. The content is often copied without authorization by the GenAI entities directly and formed into training datasets used to train GenAI models or is acquired by GenAI entities from third parties who create the datasets, such as Common Crawl. \n\nMany (most?) legal scholar arguments excusing the unauthorized reproduction of copyrighted work hinge on the affirmative defense of fair use. That is, GenAI companies likely committed copyright infringement by making copies of those billions of web pages, but because the companies subsequently transformed the works, they do not unfairly compete in the market with the source material, and therefore, there is no violation of copyright law. Whether the reproductions, distributions, and derivatives at issue qualify as fair use is the billions-of-dollars question- 4 Losing a fair use argument could be an existential risk to most GenAI companies. \n\nScholars have also argued that, as a matter of policy, because the volume of data required to train GenAI models is so large that it can only be accumulated by mass scraping and that properly licensing all of the content is impractical, society 3 GenAI is AI that can generate a type of AI that uses data to create new content, such as text, images, videos, and audio. For the purposes of this paper, the terms GenAI, large language models (LLMs), and models are used interchangeably. 4 E.g., The New York Times alleges that OpenAI infringed on 10 million works. The New York Times Company v. Microsoft Corporation, 1:23-cv-11195, (S.D.N.Y.) The penalty for willful infringement is $150,000 per work. If the Times were to win, the total penalty could theoretically reach 10,000,000 x $150,000 = $1,500,000,000,000 ($1.5 trillion). OpenAI is currently valued at $157 billion.",
            "score": 0.41764020914642547,
            "section_title": "I. Introduction",
            "char_start_offset": 18,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 130
                },
                {
                    "start": 131,
                    "end": 216
                },
                {
                    "start": 217,
                    "end": 328
                },
                {
                    "start": 329,
                    "end": 569
                },
                {
                    "start": 572,
                    "end": 713
                },
                {
                    "start": 714,
                    "end": 1013
                },
                {
                    "start": 1014,
                    "end": 1220
                },
                {
                    "start": 1223,
                    "end": 1591
                },
                {
                    "start": 1592,
                    "end": 1709
                },
                {
                    "start": 1710,
                    "end": 1785
                },
                {
                    "start": 1786,
                    "end": 1864
                },
                {
                    "start": 1865,
                    "end": 1923
                },
                {
                    "start": 1924,
                    "end": 2053
                },
                {
                    "start": 2054,
                    "end": 2097
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.46435546875
        },
        {
            "corpus_id": "265551515",
            "title": "VA3: Virtually Assured Amplification Attack on Probabilistic Copyright Protection for Text-to-Image Generative Models",
            "text": "In Tab. 3, we conduct a human evaluation on two target copyrighted images from two datasets. We randomly select 100 accepted samples obtained from each of the two threat models (the original caption and \u03b5-greedy-cdf). For each target image, a total of 200 samples are randomly shuffled and displayed to 5 graduate students. They are told to label each sample as non-infringing or infringing the copyright of the given target image. Finally, we report their average copyright infringement rates.",
            "score": 0.4175618885883181,
            "section_title": "Human Evaluation",
            "char_start_offset": 37422,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 92
                },
                {
                    "start": 93,
                    "end": 217
                },
                {
                    "start": 218,
                    "end": 323
                },
                {
                    "start": 324,
                    "end": 431
                },
                {
                    "start": 432,
                    "end": 494
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.88720703125
        },
        {
            "corpus_id": "265470237",
            "title": "AUTHORSHIP OF AI-GENERATED WORKS IN ARTISTIC DOMAIN",
            "text": "Any generative neural network requires large amounts of data to be used for training for its output to be usable in any way. Beyond that we have to note that image generation is just one part of the process, the second one being natural language understanding and processing, which is in itself an exceptionally hard task for the computers to do. Again, humans naturally excel at this task, as well as the ability to convey complex concepts in simple, short and coherent sentences. If we combine both of these factors together, the fact that there are services like Midjourney, DALL-E [3], DALL-E Mini [4] and an open-source Stable Diffusion [5] with its own different versions, too numerous to count, is a significant achievement, comparable in importance to when computers became capable of real-time image editing. As evident from previous examples, while the use of AIgenerated content is possible, even in \"first-attempt\" cases, likely it will require substantial human involvement, different from extensive prompt-crafting and generation of multiple different variations, and therefore can be considered merely a draft and as such the authorship partially or wholly lies with the human author as the one finally bringing the work to the required level of quality. \n\nLegal stance. In March of 2023 US Copyright office issued a notice [6] that can be summed up as \"if the work was produced by an AI and submitted to us \"as is\", so that no substantial human involvement, beyond initial prompts, is present, then the work can not be granted copyright protection\". This solution is further supported by the notice of UK Intellectual Property Office [7] that concludes that AI can only infringe copyright (and as such for the purposes of this article be considered \"author\" of infringing piece) only if it is tasked with recreating the original work itself, in which case the infringement lies on the person or entity guiding AI to such an act, however, the fact of learning and remembering about the work is not an infringement in itself as it is not an infringement for a human to learn and remember about a certain piece of media or any other work or event.",
            "score": 0.4175563626161703,
            "section_title": "body",
            "char_start_offset": 6883,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 124
                },
                {
                    "start": 125,
                    "end": 346
                },
                {
                    "start": 347,
                    "end": 481
                },
                {
                    "start": 482,
                    "end": 817
                },
                {
                    "start": 818,
                    "end": 1269
                },
                {
                    "start": 1272,
                    "end": 1285
                },
                {
                    "start": 1286,
                    "end": 1565
                },
                {
                    "start": 1566,
                    "end": 2160
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.349853515625
        },
        {
            "corpus_id": "256868849",
            "title": "Data Forensics in Diffusion Models: A Systematic Analysis of Membership Privacy",
            "text": "Deep generative modeling has made significant advancements over the past few years, resulting in photo-realistic media generation tools with emerging commercial uses for art and design. In particular, the rapid improvement of denoising diffusion models [36], [26], [14], [38], [39], [40], [6] has greatly advanced the state-of-the-art in the image and video generation tasks, as highlighted in recent studies [6]. Meanwhile, diffusion models are considered the most promising generative framework to date, serving as the foundation for powerful commercial models such as Stable Diffusion [29], Imagen [32], and DALL\u2022E-2 [27]. \n\nDespite the remarkable success of recent diffusion models, the widespread use of online APIs and shared pre-trained models raises concerns about their potential risks in various areas. One major concern is the risk of data misuse and violations of privacy, as sensitive information pertaining to individual identities could be revealed. Additionally, malicious users may attempt to infer the original training data, further exacerbating privacy concerns. An example of such an attack is the membership inference attack (MIA) [35], which seeks to determine if a particular data record was used to train a machine learning model. This is particularly concerning in the context of diffusion models that serve as the backbone for online media editing tools, which are freely accessible to the public. \n\nAnother major concern is the potential intellectual property (IP) infringement during the development and deployment of diffusion models. Advanced diffusion models rely heavily on the usage of massive and diverse training data. However, with the commercialization of these models, there is a risk of data being harvested from the internet for model training purposes without proper regard for the IP rights of media creators. Most commonly, it is impractical for model developers to manually review all training samples for IP compliance, making this a relevant issue in real-world application scenarios. \n\nDiffusion models possess several distinct features that set them apart from other generative models. First of all, the encoding process in diffusion models is unlearnable and fixed, following a standard procedure that is known to the public.",
            "score": 0.41736370040325155,
            "section_title": "I. INTRODUCTION",
            "char_start_offset": 18,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 185
                },
                {
                    "start": 186,
                    "end": 413
                },
                {
                    "start": 414,
                    "end": 625
                },
                {
                    "start": 628,
                    "end": 812
                },
                {
                    "start": 813,
                    "end": 964
                },
                {
                    "start": 965,
                    "end": 1082
                },
                {
                    "start": 1083,
                    "end": 1255
                },
                {
                    "start": 1256,
                    "end": 1424
                },
                {
                    "start": 1427,
                    "end": 1564
                },
                {
                    "start": 1565,
                    "end": 1654
                },
                {
                    "start": 1655,
                    "end": 1852
                },
                {
                    "start": 1853,
                    "end": 2031
                },
                {
                    "start": 2034,
                    "end": 2134
                },
                {
                    "start": 2135,
                    "end": 2275
                }
            ],
            "ref_mentions": [
                {
                    "start": 253,
                    "end": 257,
                    "matchedPaperCorpusId": "14888175"
                },
                {
                    "start": 259,
                    "end": 263,
                    "matchedPaperCorpusId": "231979499"
                },
                {
                    "start": 265,
                    "end": 269,
                    "matchedPaperCorpusId": "219955663"
                },
                {
                    "start": 271,
                    "end": 275,
                    "matchedPaperCorpusId": "196470871"
                },
                {
                    "start": 277,
                    "end": 281,
                    "matchedPaperCorpusId": "219708245"
                },
                {
                    "start": 283,
                    "end": 287,
                    "matchedPaperCorpusId": "227209335"
                },
                {
                    "start": 289,
                    "end": 292,
                    "matchedPaperCorpusId": "234357997"
                },
                {
                    "start": 409,
                    "end": 412,
                    "matchedPaperCorpusId": "234357997"
                },
                {
                    "start": 588,
                    "end": 592,
                    "matchedPaperCorpusId": "245335280"
                },
                {
                    "start": 1153,
                    "end": 1157,
                    "matchedPaperCorpusId": "10488675"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.923828125
        },
        {
            "corpus_id": "266798835",
            "title": "The Study of Copyright Infringement Liability of Generative Artificial Intelligence",
            "text": "The high similarity between AI-generated content and prior works can be attributed to various factors. One possible reason is that the generative model used in AI generation may have flaws that result in the similarity. For example, the pre-trained model might not have sufficiently analyzed and learned from the corpus of prior works, leading to limited originality in the generated content. This could be due to limitations in data availability or inadequate training methodologies. Additionally, the generative model may intentionally be designed to correspond to a particular artist's style, resulting in a deliberate similarity to prior works. This attribute can be a desirable feature for specific applications, such as content creation that emulates renowned artists' styles or replicates a particular genre or historical period. In such cases, the resemblance to prior works is a purposeful design choice rather than a flaw in the AI model. Another factor contributing to the high similarity is the influence of consumers of generative AI services. Users of these services may input material that infringes upon existing works themselves, knowingly or unknowingly. They may also deliberately guide the AI to generate content that closely resembles others' prior works through language training. This user input and guidance can also influence the similarity between AI-generated content and prior works. \n\nAccording to Paragraph 1 of Article 1165 of China's Civil Code, an actor who through his fault infringes upon another person's civil-law rights and interests shall bear tort liability [9]. Therefore, in the two scenarios mentioned above, both the generative AI service provider and the user bear direct tort liability as the perpetrators of the infringement. \n\nThe assumption of direct tort liability is generally accepted and does not face much controversy. However, the focus of the discussion should shift towards determining whether generative AI service providers also bear indirect tort liability when their users engage in infringement. \n\nIn traditional online copyright dispute cases, China has established rules for determining the indirect infringement liability of internet service providers (ISPs) based on their involvement in aiding infringement. Two main rules, known as the \"notice takedown rule\" and the \"know rule,\" govern the liability for online infringement.",
            "score": 0.4168684188466747,
            "section_title": "Liability for infringement of content generated by artificial intelligence that is highly similar to the prior work",
            "char_start_offset": 18361,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 102
                },
                {
                    "start": 103,
                    "end": 219
                },
                {
                    "start": 220,
                    "end": 392
                },
                {
                    "start": 393,
                    "end": 484
                },
                {
                    "start": 485,
                    "end": 648
                },
                {
                    "start": 649,
                    "end": 836
                },
                {
                    "start": 837,
                    "end": 948
                },
                {
                    "start": 949,
                    "end": 1056
                },
                {
                    "start": 1057,
                    "end": 1172
                },
                {
                    "start": 1173,
                    "end": 1302
                },
                {
                    "start": 1303,
                    "end": 1411
                },
                {
                    "start": 1414,
                    "end": 1602
                },
                {
                    "start": 1603,
                    "end": 1772
                },
                {
                    "start": 1775,
                    "end": 1872
                },
                {
                    "start": 1873,
                    "end": 2057
                },
                {
                    "start": 2060,
                    "end": 2274
                },
                {
                    "start": 2275,
                    "end": 2393
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.93310546875
        },
        {
            "corpus_id": "263835355",
            "title": "State of the Art on Diffusion Models for Visual Computing",
            "text": "Distribution of Harmful Content. Generative AI tools automate the process of content creation with photorealistic quality. This ability could be used to generate fake photos or videos of real people (DeepFakes). DeepFakes pose a societal threat that could be used for harm, either intentional or unintentional. Anyone with unrestricted access to generative AI tools could, for example, create an image or video of a celebrity with the intention of tarnishing their reputation. \n\nA number of measures can be put in place to prevent the distribution of harmful content. First, the ability of a user to generate harmful content should be prevented as best as possible. Most image generation models today, for example, prevent the generation of content depicting violence, gore, harassment, drugs, adult content, and generally offensive topics. Second, forensic techniques to detect DeepFakes are being developed by the AI community (e.g., [AFG * 19, RCV * 19, FLK *  21]). As the quality of generative AI tools advances, these types of efforts are becoming increasingly important but also challenging. \n\nCopyright, Legal Exposure, and Privacy Concerns. Foundation models are trained on billions of images, including content that may have been scraped without the consent of the creator, that may have been legally protected otherwise, or that may contain personally identifiable or sensitive information. Indeed, copyright infringement lawsuits have already been filed by artists against some of the companies behind foundation models for visual computing. \n\nBias and Fairness. Similar to most machine learning methods, diffusion models can inadvertently learn and perpetuate biases present in their training data. This is a significant concern in terms of fairness and ethical considerations; further research to develop means to mitigate such biases is needed. \n\nEnvironmental Concerns. Training foundation models requires substantial computational resources. For example, the relatively small StableDiffusion model was reportedly trained on 2.3 Billion images using 256 Nvidia A100 GPUs on Amazon Web Services for a total of 150,000 GPU-hours [Mos22]. According to unverified sources, OpenAI's large GPT-4 model was trained on about 25,000 Nvidia A100 GPUs for 90-100 days, costing more than $100 Million.",
            "score": 0.41679132732412427,
            "section_title": "Social Implications and Ethical Concerns",
            "char_start_offset": 107782,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 32
                },
                {
                    "start": 33,
                    "end": 122
                },
                {
                    "start": 123,
                    "end": 211
                },
                {
                    "start": 212,
                    "end": 310
                },
                {
                    "start": 311,
                    "end": 476
                },
                {
                    "start": 479,
                    "end": 567
                },
                {
                    "start": 568,
                    "end": 665
                },
                {
                    "start": 666,
                    "end": 840
                },
                {
                    "start": 841,
                    "end": 969
                },
                {
                    "start": 970,
                    "end": 1098
                },
                {
                    "start": 1101,
                    "end": 1149
                },
                {
                    "start": 1150,
                    "end": 1401
                },
                {
                    "start": 1402,
                    "end": 1553
                },
                {
                    "start": 1556,
                    "end": 1574
                },
                {
                    "start": 1575,
                    "end": 1711
                },
                {
                    "start": 1712,
                    "end": 1859
                },
                {
                    "start": 1862,
                    "end": 1885
                },
                {
                    "start": 1886,
                    "end": 1958
                },
                {
                    "start": 1959,
                    "end": 2151
                },
                {
                    "start": 2152,
                    "end": 2305
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.73974609375
        },
        {
            "corpus_id": "257050406",
            "title": "On Provable Copyright Protection for Generative Models",
            "text": "There have been several studies of copyright issues in machine learning and data mining in the law literature, though most of them focus on potential infringements in the training phase. Sag [2018] surveys the question of whether data mining and machine learning on copyrighted text falls under \"fair use\" and states that \"allowing [text data mining] and other similar non-expressive uses of copyrighted works without authorization is entirely consistent with the fundamental structure of copyright law.\". Sag [2018] also states that under U.S. law \"extracting a short phrase or snippet of text from one work and using it in another does not amount to a reproduction of the work if the localized similarity is not substantial, is not quantitatively or qualitatively significant, or is otherwise de minimis.\" (However, European courts have a stricter threshold for the amount of similarity.) Sobel [2018] also discusses the issue of \"fair use\" in training. While he mentions the issue of output generation, the article does not focus on it since (at the time) \"works generated by Al are fascinating and entertaining, but today they remain novelties rather than mainstream sources of entertainment or compelling substitutes for human expression.\" Gillotte [2020] studies copyright infringement in AI-generated artworks and concludes that regarding the training phase \"an engineer may use copyrighted works to train an Al program to generate artwork without incurring infringement liability.\" Hristov [2016] considers a separate issue regarding AI and copyright: whether it should be possible to grant copyright to AI-authored works. Current rulings (Board [2022]) by the U.S. copyright review board state that wholly AI generated works cannot be considered for copyright. \n\nMemorization of training samples is considered undesirable for many reasons apart from copyright. Lee et al. [2022b] show that deduplication can significantly reduce memorization, but not eliminate it (see also bottom row of Table 1 in [Kandpal et al., 2022]). Ippolito et al. [2022] state that \"deduplication does not guarantee that a model will not still memorize individual (deduplicated) examples, necessitating defenses that operate at inference-time\".",
            "score": 0.4161669172117514,
            "section_title": "Related works",
            "char_start_offset": 49394,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 186
                },
                {
                    "start": 187,
                    "end": 505
                },
                {
                    "start": 506,
                    "end": 807
                },
                {
                    "start": 808,
                    "end": 890
                },
                {
                    "start": 891,
                    "end": 955
                },
                {
                    "start": 956,
                    "end": 1244
                },
                {
                    "start": 1245,
                    "end": 1489
                },
                {
                    "start": 1490,
                    "end": 1630
                },
                {
                    "start": 1631,
                    "end": 1769
                },
                {
                    "start": 1772,
                    "end": 1869
                },
                {
                    "start": 1870,
                    "end": 2032
                },
                {
                    "start": 2033,
                    "end": 2229
                }
            ],
            "ref_mentions": [
                {
                    "start": 187,
                    "end": 197,
                    "matchedPaperCorpusId": "86424284"
                },
                {
                    "start": 506,
                    "end": 516,
                    "matchedPaperCorpusId": "86424284"
                },
                {
                    "start": 891,
                    "end": 903,
                    "matchedPaperCorpusId": "115500744"
                },
                {
                    "start": 1245,
                    "end": 1260,
                    "matchedPaperCorpusId": "233748873"
                },
                {
                    "start": 1870,
                    "end": 1888,
                    "matchedPaperCorpusId": "235829052"
                },
                {
                    "start": 2008,
                    "end": 2030,
                    "matchedPaperCorpusId": "246823128"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.5888671875
        },
        {
            "corpus_id": "256389511",
            "title": "Is Writing Prompts Really Making Art?",
            "text": "One of the key factors that contributes to the capability of TTI models is their access to massive datasets used for training and validation. Achieving the visual quality and diversity that they are capable of reproducing requires a very large corpus of human-created imagery, which is typically scraped from the Internet. In a practice that has been dubbed \"data laundering\", scraped datasets -which include large amounts of copyrighted media -rely on special exemptions for \"academic use\" to avoid any legal barriers preventing their use, or for copyright owners to claim against [3]. For example, Stability AI (the \"creators\" of Stable Diffusion) funded the Machine Vision & Learning research group at the Ludwig Maximilian University of Munich to undertake the model training and a small nonprofit organisation, LAION, to create the training dataset8 of approximately 5.85 billion images, many of which are copyrighted, and in general appropriated for this purpose without the image creator's direct permission. \n\nConcern has been raised by artists about the ethical and moral implications of their work being used in such systems. These concerns include the appropriation of an individual artist's \"style\", mimicry, and even the replacement of a human artist or illustrator. Furthermore, there is no easy way to be excluded or removed from such datasets. Such is the concern raised by TTI systems that websites such as https://haveibeentrained.com/ have emerged, allowing anyone to search to see if their work has been used as part of the training data. A companion project allows people to opt-in or opt-out of their data being included. At the time of writing (November 2022) around 60% of respondents chose to opt-out of being included in any training data. \n\nThe use of copyrighted images in datasets creates another issue, raising the question that whether training models on copyrighted data should be considered plagiarism. Being able to easily generate an image in an artist's style without paying for that artist to create it (or paying any royalties or licensing fees), allows users of such technology to bypass the traditional economic, legal and moral frameworks that have supported artists traditionally.",
            "score": 0.41613108290397544,
            "section_title": "Data Laundering",
            "char_start_offset": 17897,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 141
                },
                {
                    "start": 142,
                    "end": 322
                },
                {
                    "start": 323,
                    "end": 586
                },
                {
                    "start": 587,
                    "end": 1015
                },
                {
                    "start": 1018,
                    "end": 1135
                },
                {
                    "start": 1136,
                    "end": 1279
                },
                {
                    "start": 1280,
                    "end": 1359
                },
                {
                    "start": 1360,
                    "end": 1558
                },
                {
                    "start": 1559,
                    "end": 1643
                },
                {
                    "start": 1644,
                    "end": 1765
                },
                {
                    "start": 1768,
                    "end": 1935
                },
                {
                    "start": 1936,
                    "end": 2222
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.806640625
        },
        {
            "corpus_id": "259501495",
            "title": "Measuring the Success of Diffusion Models at Imitating Human Artists",
            "text": "(Balganesh et al., 2014;Kaminski & Rub, 2017;Balagopalan et al., 2023). In evaluating copyright questions involving AI systems, legal analyses have focused on how copyrighted work is used in the system's training data (Sag, 2018;Lemley & Casey, 2020), but such a focus on training data does not connect liability to an AI system's ability to copy an artist. In contrast, we show how standard image classification techniques can be used to help determine how successful AI image generators are at imitating individual human artists. This approach is consistent, quantitative, and connected to the capabilities of the resulting AI system. Our goal, however, is not to automate determinations of infringement but to demonstrate how tried and tested image classification techniques from machine learning can be used to analyze legal claims.",
            "score": 0.415922275784119,
            "section_title": "body",
            "char_start_offset": 1864,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 71
                },
                {
                    "start": 72,
                    "end": 357
                },
                {
                    "start": 358,
                    "end": 531
                },
                {
                    "start": 532,
                    "end": 636
                },
                {
                    "start": 637,
                    "end": 836
                }
            ],
            "ref_mentions": [
                {
                    "start": 24,
                    "end": 45,
                    "matchedPaperCorpusId": "157499030"
                },
                {
                    "start": 45,
                    "end": 70,
                    "matchedPaperCorpusId": "258615890"
                },
                {
                    "start": 218,
                    "end": 229,
                    "matchedPaperCorpusId": "86424284"
                },
                {
                    "start": 229,
                    "end": 250,
                    "matchedPaperCorpusId": "219342558"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.92626953125
        },
        {
            "corpus_id": "259165411",
            "title": "Are ChatGPT and Other Similar Systems the Modern Lernaean Hydras of AI?",
            "text": "You may not use this data for the training or inference of Generative Artificial Intelligence Models without the prior permission of the copyright holder. (\"Generative Artificial Intelligence Models\" are used to create new content or data that is similar to the original data, but not identical. Examples of Generative Artificial Intelligence Models include but are not limited to, text generation models, image and video generation models, and music generation models. The restrictions on Generative Artificial Intelligence Models apply to any use of this data, whether the generative artificial intelligence is trained on this data or uses this data for inference). \n\nAny attempt by other artificial intelligence models to access or use this data without such permission shall be deemed a violation of this license and a breach of copyright laws. The copyright holder reserves the right to pursue all legal remedies available, including but not limited to injunctive relief and damages, against any party that violates this license. \n\nEXHIBIT 3 SUGGESTED SUPPLEMENTAL TEXT FOR OTHER OPEN-SOURCE LICENSES 156 All permissions granted are only to any person. \n\nThe terms \"person\" and \"individual\" are defined as a natural person, as the term is defined by the United States Patent and Trademark Office (PTO), and/or 35 U.S.C. \u00a7 100, as amended. The term \"Artificial Intelligence Model\" means any non-human generative machine learning system or computer program, algorithm, or functional prediction engine supported by cloud-based/computing platforms. The term \"Source Code\" means the preferred form of a program for making, creating, and modifying software source code, documentation source, and configuration files. \n\nPermission is not granted to use, modify, combine, study, collect, share, reproduce, distribute, and/or access the Software under this License, by any non-human Generative Artificial Intelligence Model without the express written consent of the copyright holder, which may be withheld or delayed for any reason. Any appropriation, adoption, disclosure, reproduction, use, and/or access of the licensed Software by any non-human Generative Artificial Intelligence Model shall immediately terminate all rights granted to the Licensee. The Licensor shall have the right, at any time, to withdraw consent by written notice, thereby terminating with immediate effect all use of Software made under this License unless otherwise specified.",
            "score": 0.4157756444281606,
            "section_title": "THE MIT LICENSE",
            "char_start_offset": 62509,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 154
                },
                {
                    "start": 155,
                    "end": 295
                },
                {
                    "start": 296,
                    "end": 469
                },
                {
                    "start": 470,
                    "end": 667
                },
                {
                    "start": 670,
                    "end": 848
                },
                {
                    "start": 849,
                    "end": 1034
                },
                {
                    "start": 1037,
                    "end": 1157
                },
                {
                    "start": 1160,
                    "end": 1343
                },
                {
                    "start": 1344,
                    "end": 1549
                },
                {
                    "start": 1550,
                    "end": 1715
                },
                {
                    "start": 1718,
                    "end": 2029
                },
                {
                    "start": 2030,
                    "end": 2250
                },
                {
                    "start": 2251,
                    "end": 2451
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.6015625
        },
        {
            "corpus_id": "254685940",
            "title": "The Infinite Index: Information Retrieval on Generative Text-To-Image Models",
            "text": "A computational approach powerful enough to generate documents such as images, text, and other media types at a quality difficult to distinguish at times from human-made illustrations naturally raises ethical concerns. We discuss the most important ones below. \n\nWill algorithms replace artists? We begin with the obvious question: will generative text-to-image models threaten artists' jobs? First, based on our experience in the case study, it is currently difficult to get text-to-image models to generate a desired result. The decision whether the generated images represent the desired scene with sufficient quality still has to be made by the user. Therefore, we believe that these new models will be a powerful tool, but will not replace the human illustrator in the foreseeable future-even if the image quality should eventually reach human levels. This is corroborated by others such as Liu et al. [54], who developed and evaluated a system that assists users in generating images for news articles, noting that artistic knowledge is still beneficial to the generated result, explicitly saying \"generative AI deployment should [...] augment rather than [...] replace human creative expertise\". We support this view: instead of an autonomous AI that acts on its own, we want to emphasize the benefits of a \"supportive AI\" that inquires about and incorporates the decisions of its users. \n\nWho is the author of a generated image? And who owns the rights? This is currently an unresolved situation that leads to uncertainties regarding the use of AI-generated images. For this reason, major platforms such as the well-known image provider Getty Images have recently banned all AI-generated content. 10 Stakeholders may include the user, the creators, and the artists who created the images used for model training. Ultimately, this decision must be made by policy makers and by the courts, where many legal precedents have been set in the past through copyright litigation. \n\nText-to-image models for generating misinformation? Generated misinformation is already a pervasive problem and is widely discussed in the context of so-called \"deep fakes\" and AI-generated text [43,82,101]. To mitigate this problem in text-to-image models such as Stable Diffusion, an image is watermarked to identify it as artificially generated. 11",
            "score": 0.41560887374141325,
            "section_title": "Ethical Concerns",
            "char_start_offset": 52081,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 218
                },
                {
                    "start": 219,
                    "end": 260
                },
                {
                    "start": 263,
                    "end": 295
                },
                {
                    "start": 296,
                    "end": 392
                },
                {
                    "start": 393,
                    "end": 526
                },
                {
                    "start": 527,
                    "end": 654
                },
                {
                    "start": 655,
                    "end": 856
                },
                {
                    "start": 857,
                    "end": 1202
                },
                {
                    "start": 1203,
                    "end": 1394
                },
                {
                    "start": 1397,
                    "end": 1436
                },
                {
                    "start": 1437,
                    "end": 1461
                },
                {
                    "start": 1462,
                    "end": 1573
                },
                {
                    "start": 1574,
                    "end": 1707
                },
                {
                    "start": 1708,
                    "end": 1820
                },
                {
                    "start": 1821,
                    "end": 1979
                },
                {
                    "start": 1982,
                    "end": 2033
                },
                {
                    "start": 2034,
                    "end": 2189
                },
                {
                    "start": 2190,
                    "end": 2333
                }
            ],
            "ref_mentions": [
                {
                    "start": 2177,
                    "end": 2181,
                    "matchedPaperCorpusId": "219800227"
                },
                {
                    "start": 2181,
                    "end": 2184,
                    "matchedPaperCorpusId": "211204954"
                },
                {
                    "start": 2184,
                    "end": 2188,
                    "matchedPaperCorpusId": "168169824"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.5361328125
        },
        {
            "corpus_id": "270703065",
            "title": "U Can't Gen This? A Survey of Intellectual Property Protection Methods for Data in Generative AI",
            "text": "Generative Artificial Intelligence (GAI) has revolutionised various domains, demonstrating remarkable capabilities in generating realistic text (ChatGPT [1]), music (MuseNet1 ) and images [2], [3] (see Figure 1), among other outputs.This cutting-edge technology has the potential to disrupt traditional creative processes, enabling AI systems to autonomously produce content that mirrors human creativity.While GAI opens up exciting opportunities for innovation and artistic expression, it also presents significant challenges concerning intellectual property rights and ethical considerations.Intellectual property (IP) rights encompass a set of legal protections designed to safeguard the creations of human intellect.These rights, including copyright, patents and trademarks have played a vital role in promoting creativity, innovation, and fair compensation for creators.However, the emergence of GAI has sparked debates surrounding the nature of these rights when AI systems generate original content without direct human intervention but rather by generalising from a large corpus of human creation.\n\nRecently, artists have been noticing the evidence of the non-originality of generative models through some controversial examples.The name of a Polish artist, Greg Rutkowski, known for his fantasy illustrations, appeared in more than 93,000 prompts of Stable Diffusion -the prompts have been used to create art in his distinct style, very successfully so.As a result, many AI-created images inspired by Rutkowski's style have been published and appear in online searches [4].The case was followed by more artists reporting the same behaviour of generative models.Three artists, Sarah Anderson, Kelly McKernan, and Karla Ortiz filed a class action lawsuit against Stability AI 3 , Midjourney 4 , and Deviant Art 5 , over their usage of Stable Diffusion [2], alleging copyright infringement.They assert that these models used their works and the works of thousands of other artists as training data without authorisation.Another lawsuit was filed by Getty Images against Stability AI for using more than 12 million images and associated metadata without authorisation or compensation to train their model [5].\n\nOne major aspect of the underlying problem and an aspect that distinguishes these current cases from historic cases of plagiarism and other forms of copyright infringement is the sheer scale.",
            "score": 0.41540379481205364,
            "section_title": "I. INTRODUCTION",
            "char_start_offset": 18,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 233
                },
                {
                    "start": 233,
                    "end": 405
                },
                {
                    "start": 405,
                    "end": 594
                },
                {
                    "start": 594,
                    "end": 720
                },
                {
                    "start": 720,
                    "end": 875
                },
                {
                    "start": 875,
                    "end": 1105
                },
                {
                    "start": 1107,
                    "end": 1237
                },
                {
                    "start": 1237,
                    "end": 1462
                },
                {
                    "start": 1462,
                    "end": 1582
                },
                {
                    "start": 1582,
                    "end": 1670
                },
                {
                    "start": 1670,
                    "end": 1896
                },
                {
                    "start": 1896,
                    "end": 2026
                },
                {
                    "start": 2026,
                    "end": 2214
                },
                {
                    "start": 2216,
                    "end": 2407
                }
            ],
            "ref_mentions": [
                {
                    "start": 188,
                    "end": 191,
                    "matchedPaperCorpusId": "245335280"
                },
                {
                    "start": 1859,
                    "end": 1862,
                    "matchedPaperCorpusId": "245335280"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.9560546875
        },
        {
            "corpus_id": "234777751",
            "title": "Copyright in generative deep learning",
            "text": "We should also remember from Section 2 that GDL involves the creation of a probabilistic model describing data of interest, from which we obtain new works through sampling. This peculiarity leads to other critical questions. In fact, the model creation and storage, which contains the extracted probabilistic features, may infringe the copyright of works used for training. Its storage cannot be considered as transient or incidental; therefore, it is allowed only if it does not constitute a (partial) reproduction of protected works, since, in that case, there is no copyright relevant activity (Margoni, 2018).\n\nDeep learning models are usually stored as sets of numerical weights; usually, they do not fall in the category of those that are considered as a partial reproduction of a work. However, if the model is built to mimic in output the input (with some nonsubstantial changes) and it is trained over a protected work, the model would represent that work, at least partially. Moreover, even if the model is not intentionally built to mimic a protected work, it could still end up doing so to an infringing degree: it might reconstruct idiosyncrasies of input data instead of reflecting underlying trends about them (Sobel, 2017). If just trained with the goal of learning how to reproduce works, an overfitted generative model may actually be considered as a direct reproduction of the works.\n\nThese issues are not only related to the model itself, but also to the output it can generate. Protected input data are commonly used to train models to generate similar output. Then that output may infringe copyright in the preexisting work or works to which it is similar to (Sobel, 2017). In this context, the importance of adversarial training from GANs and searching for diverging from existing works as done by Elgammal et al. (2017) may tip the balance toward legality. With respect to other classic techniques, the generative part of a GAN never uses protected data; therefore, it is harder to obtain an output representing an expression of protected data. In addition, new techniques with a novelty objective, which tries to increase the distance between outputs and training data, will tend to be more transformative. This may be crucial, since prior appropriation art cases suggest that,",
            "score": 0.41528564087500813,
            "section_title": "Additional issues",
            "char_start_offset": 27080,
            "sentence_offsets": [],
            "ref_mentions": [
                {
                    "start": 1225,
                    "end": 1238,
                    "matchedPaperCorpusId": "115500744"
                },
                {
                    "start": 1681,
                    "end": 1694,
                    "matchedPaperCorpusId": "115500744"
                },
                {
                    "start": 1821,
                    "end": 1843,
                    "matchedPaperCorpusId": "24986117"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.92724609375
        },
        {
            "corpus_id": "266900037",
            "title": "The Stronger the Diffusion Model, the Easier the Backdoor: Data Poisoning to Induce Copyright Breaches Without Adjusting Finetuning Pipeline",
            "text": "The SSCD has been specifically designed to identify copied content existing across two images. While SSCD is state-of-theart and the first choice for this purpose, it's important to recognize that no system is infallible. Discrepancies between SSCD outcomes and human judgment can occur, highlighting an area for further research and development. \n\nCopyright laws stipulate that any unauthorized reproduction constitutes infringement. To ensure a dataset is free from copyright issues, each image within the dataset (of size N) must be compared against every image in a predefined set of copyright-protected images (of size M). This requirement leads to N*M comparisons and discussions by human experts. A formidable task considering that legal disputes over copyright between two works can span several months. Given the vast size of datasets typically used in training generative models, manual inspection for copyright infringement at such scale becomes impractical. Consequently, despite its limitations, employing a tool like SSCD remains the most feasible strategy for conducting copyright infringement checks at this magnitude. \n\nTo further justify the effectiveness of SSCD, we collected examples used in the Getty Images lawsuit against Stability AI (Vincent, 2023), as illustrated in Figure 8. The SSCD similarity score is 0.47. In our study, we set the threshold at 0.5. This result indicates the effectiveness of SSCD for copyright detection and our threshold is a reasonable value for checking copyright infringement in training data.",
            "score": 0.4152367394882531,
            "section_title": "D. Limitation and Discussion of Employing SSCD for Assessing Substantial Similarity",
            "char_start_offset": 42005,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 94
                },
                {
                    "start": 95,
                    "end": 221
                },
                {
                    "start": 222,
                    "end": 346
                },
                {
                    "start": 349,
                    "end": 434
                },
                {
                    "start": 435,
                    "end": 627
                },
                {
                    "start": 628,
                    "end": 703
                },
                {
                    "start": 704,
                    "end": 811
                },
                {
                    "start": 812,
                    "end": 969
                },
                {
                    "start": 970,
                    "end": 1134
                },
                {
                    "start": 1137,
                    "end": 1303
                },
                {
                    "start": 1304,
                    "end": 1338
                },
                {
                    "start": 1339,
                    "end": 1381
                },
                {
                    "start": 1382,
                    "end": 1547
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.94384765625
        },
        {
            "corpus_id": "272146279",
            "title": "RLCP: A Reinforcement Learning-based Copyright Protection Method for Text-to-Image Diffusion Model",
            "text": "We briefly review related work about text-to-image diffusion models, copyright protection and Reinforcement Learning from Human Feedback. \n\nText-to-Image Diffusion Models: Recently, text-toimage diffusion models have garnered significant attention in research. These advanced methods (Balaji et al. 2023;Nichol et al. 2022;Rombach et al. 2022a;Saharia et al. 2022) have demonstrated exceptional capabilities in converting textual descriptions into visually coherent and realistic images with high accuracy. The advancements in these techniques have unlocked numerous possibilities for various downstream tasks, including image editing (Avrahami, Lischinski, and Fried 2022;Ho, Jain, and Abbeel 2020a;Kawar et al. 2023), image denoising (Ho, Jain, and 1 https://law.justia.com/cases/federal/appellate-courts/F2/562/ 1157/293262/ Abbeel 2020a; Xie et al. 2023), andsuper-resolution (Sohl-Dickstein et al. 2015;Ho, Jain, and Abbeel 2020b). \n\nCopyright Protection: Several studies in the legal literature have examined copyright issues in machine learning and data mining, focusing primarily on potential infringements during the training phase.: (1)Watermarking (Dogoulis et al. 2023;Epstein et al. 2023), which inserts specific, unnoticeable patterns into protected images to detect copyright infringement, has been explored, but further research is needed to improve its robustness. (2)Concept Removal: To remove explicit artwork from large models, (Gandikota et al. 2023) presents a fine-tuning method for concept removal from diffusion models. Additionally, (Zhang et al. 2023) presents the \"Forget-Me-Not\" method, which enables the targeted removal of specific objects and content from large models within 30 seconds while minimizing the impact on other content. (3) Dataset Deduplication: (Somepalli et al. 2022) explores whether diffusion models create unique artworks or directly replicate certain content from the training dataset during image generation.",
            "score": 0.4152070230410541,
            "section_title": "Related Work",
            "char_start_offset": 4920,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 137
                },
                {
                    "start": 140,
                    "end": 260
                },
                {
                    "start": 261,
                    "end": 506
                },
                {
                    "start": 507,
                    "end": 814
                },
                {
                    "start": 815,
                    "end": 936
                },
                {
                    "start": 939,
                    "end": 1381
                },
                {
                    "start": 1382,
                    "end": 1544
                },
                {
                    "start": 1545,
                    "end": 1764
                },
                {
                    "start": 1765,
                    "end": 1961
                }
            ],
            "ref_mentions": [
                {
                    "start": 323,
                    "end": 344,
                    "matchedPaperCorpusId": "245335280"
                },
                {
                    "start": 344,
                    "end": 363,
                    "matchedPaperCorpusId": "248986576"
                },
                {
                    "start": 635,
                    "end": 673,
                    "matchedPaperCorpusId": "244714366"
                },
                {
                    "start": 700,
                    "end": 718,
                    "matchedPaperCorpusId": "252918469"
                },
                {
                    "start": 1159,
                    "end": 1181,
                    "matchedPaperCorpusId": "258297834"
                },
                {
                    "start": 1181,
                    "end": 1201,
                    "matchedPaperCorpusId": "264436550"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.943359375
        },
        {
            "corpus_id": "270562693",
            "title": "AI Royalties - an IP Framework to Compensate Artists & IP Holders for AI-Generated Content",
            "text": "We propose a systematic AI method to evaluate whether an AI-generated output infringes previous IP rights, and assess its performance on a series of rulings.\n\nFor standardization, we focus on image copyright infringement, using an approach that is readily extendable (with corresponding metrics) to other forms of IPRs (trademark, image, name, likeness, etc.) and creative expression protected by copyright (sound, video, 3D graphics and games, text, etc.).Of the four factors considered when ruling for copyright infringement 2, we focus on a metric for sufficient transformation the image itself (factor 3).For a broader analysis encompassing all four factors, this metric could be combined in a multi-modal AI system incorporating predictive treatments of court rulings, such as previous specifically developed LLMs (Li and Zhang 2021).",
            "score": 0.4148455863381475,
            "section_title": "Introduction & Background",
            "char_start_offset": 6137,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 157
                },
                {
                    "start": 159,
                    "end": 457
                },
                {
                    "start": 457,
                    "end": 609
                },
                {
                    "start": 609,
                    "end": 839
                }
            ],
            "ref_mentions": [
                {
                    "start": 819,
                    "end": 838,
                    "matchedPaperCorpusId": "236169524"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.642578125
        },
        {
            "corpus_id": "278534729",
            "title": "DFA-CON: A Contrastive Learning Approach for Detecting Copyright Infringement in DeepFake Art",
            "text": "Recent advancements in generative models such as GANs and diffusion models have enabled the creation of highly realistic and stylistically rich visual artworks [5,6]. While this has opened new avenues in creative expression, it has also raised significant concerns regarding authorship, originality, and copyright infringement [3]. Prior work in AI-generated art has largely focused on generation techniques, artistic style transfer, and aesthetics modeling, with less attention paid to post-generation verification or attribution. The DeepfakeArt Challenge benchmark [4] was introduced to bridge this gap by facilitating research on forgery and contamination detection in generative art settings.",
            "score": 0.4145828004650822,
            "section_title": "AI Generated Art",
            "char_start_offset": 3226,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 166
                },
                {
                    "start": 167,
                    "end": 331
                },
                {
                    "start": 332,
                    "end": 531
                },
                {
                    "start": 532,
                    "end": 697
                }
            ],
            "ref_mentions": [
                {
                    "start": 163,
                    "end": 165,
                    "matchedPaperCorpusId": "245335280"
                },
                {
                    "start": 327,
                    "end": 330,
                    "matchedPaperCorpusId": "254366634"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.94970703125
        },
        {
            "corpus_id": "260848352",
            "title": "The Dilemma of Fair Use of Artificial Intelligence Painting and Its Regulations",
            "text": "In January, three artists filed a lawsuit against Sta-bleDiffusion and Midjourney's creators, Stability AI and Midjourney, and Dream Up's artist portfolio platform DeviantArt, accusing the platform of training AI models with 5 billion images scraped from the web without the authors' consent. The technology behind AI-assisted painting has been developed for many years and has been subject to copyright infringement controversy since its birth, because AI painting simulates and interprets the human painting process by learning a large amount of data in the database, and it is inevitable to use other people's works. The use of other people's works by AI paintings may constitute infringement of others' copyrights. The use of other people's works by artificial intelligence painting may constitute an infringement of other people's copyrights. At present, the discussion of the use of other people's works by artificial intelligence paintings in the field of copyright law mainly focuses on whether it constitutes fair use, and if the company is accused of infringement, whether it can invoke the fair use system for defense. At present, artificial intelligence painting technology is in an era of prosperity and development, and the \"transformation\" of works using artificial intelligence has brought challenges to the traditional fair use system. Overemphasizing its \"transformative nature\", and correspondingly underestimating its market consequences, may lead to an imbalance of interests, so the rationale of qualifying it as fair use remains to be explored.",
            "score": 0.414547752868642,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 292
                },
                {
                    "start": 293,
                    "end": 619
                },
                {
                    "start": 620,
                    "end": 718
                },
                {
                    "start": 719,
                    "end": 847
                },
                {
                    "start": 848,
                    "end": 1129
                },
                {
                    "start": 1130,
                    "end": 1352
                },
                {
                    "start": 1353,
                    "end": 1567
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.6201171875
        },
        {
            "corpus_id": "278338968",
            "title": "Safer Prompts: Reducing IP Risk in Visual Generative AI",
            "text": "As generative AI (GenAI) becomes increasingly prevalent in realworld applications, concerns about its potential risks continue to grow. We focus on the risks associated with the replication of copyrighted material and restrict ourselves to image generation. The tangible legal and financial implications of these risks have sparked our current research. For instance, artists sued Stability AI (Midjourney and DevianArt) claiming that the companies' AI image generators produce images that are strikingly similar to their artworks [2]. In August 2024, a U.S. district judge (Orrick) ruled that the artists could proceed with the copyright infringement claims, underscoring the ongoing legal uncertainties surrounding AI-generated content [6]. \n\nThese prevalent risks make model users resistant to fully exploiting the latest GenAI models. To promote a wide adoption of GenAI, risk management is crucial. Risk management has two aspects: a) risk quantification; and b) risk mitigation. Both aspects have already received attention in the literature. \n\nRisk quantification: Risk quantification involves analysing the fraction of training images that are reproduced by the model. Since training images may be copy-righted, this leads to IP infringement risks for the end user. Consider for instance Stable Diffusion-1. When prompted using the captions of training images directly, about 2% of images it generates are highly similar to those in the training dataset [4,24]; the results are very similar for the Stable Diffusion-2 model [25]. \n\nRisk mitigation: We focus on risk mitigation, which takes a step beyond risk quantification. The goal of risk mitigation is to reduce the probability of a model outputting copyrighted content. The risk could either be mitigated before deploying a model (on the model developer's side) or after (on the model's user side). \n\nPre-deployment strategies involve changing the data science process and are usually expensive. For instance, models could be trained on a de-duplicated data set, which usually reduces the risk during training time [25]. Note however that with this strategy test time risks still prevail. Model could also be made to unlearn copy righted data but that comes with a huge computational expense [31,7,8].",
            "score": 0.41438170835825244,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 135
                },
                {
                    "start": 136,
                    "end": 257
                },
                {
                    "start": 258,
                    "end": 353
                },
                {
                    "start": 354,
                    "end": 535
                },
                {
                    "start": 536,
                    "end": 742
                },
                {
                    "start": 745,
                    "end": 838
                },
                {
                    "start": 839,
                    "end": 903
                },
                {
                    "start": 904,
                    "end": 984
                },
                {
                    "start": 985,
                    "end": 1048
                },
                {
                    "start": 1051,
                    "end": 1176
                },
                {
                    "start": 1177,
                    "end": 1273
                },
                {
                    "start": 1274,
                    "end": 1315
                },
                {
                    "start": 1316,
                    "end": 1537
                },
                {
                    "start": 1540,
                    "end": 1632
                },
                {
                    "start": 1633,
                    "end": 1732
                },
                {
                    "start": 1733,
                    "end": 1861
                },
                {
                    "start": 1864,
                    "end": 1958
                },
                {
                    "start": 1959,
                    "end": 2083
                },
                {
                    "start": 2084,
                    "end": 2151
                },
                {
                    "start": 2152,
                    "end": 2264
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.99267578125
        },
        {
            "corpus_id": "268513090",
            "title": "CGI-DM: Digital Copyright Authentication for Diffusion Models via Contrasting Gradient Inversion",
            "text": "Consequently, it becomes difficult to ascertain whether a particular training sample has been utilized solely based on the generated output of the model for postcaution methods.\n\nIn this paper, we propose a new copyright authentication framework, named Contrasting Gradient Inversion for Diffusion Models (CGI-DM) to greatly improve the efficacy of the post-caution path, illustrated in Fig. 1 (Bottom).Recent advances in gradient inversion [3,10,38] emphasize the importance of prior information in data extraction.Inspired by this, we propose first removing half of a given image.Then we utilize the retained partial representation as a prior and employ gradient inversion to reconstruct the original image.As recent studies [1,27] indicate that generative models tend to \"memorize\", the recovery of removed information should be possible only when the images are utilized during the fine-tuning process, enabling \"memorization\" on given samples.Thus, a high similarity between the recovered image and the original image can indicate that the model has been trained with the given image.\n\nHowever, directly applying gradient inversion may not yield useful information for DMs, possibly because they inherently eliminate noise (see Appendix C for details).To address this issue, we focus on contrasting two models: the pretrained and the fine-tuned model.Specifically, our goal is to leverage the conceptual differences between these two models.We measure this disparity through the KL divergence between the latent variable distributions of the pretrained and fine-tuned models.Subsequently, we provide a proof demonstrating that maximizing this KL divergence approximates accentuating the loss differences between the two models.Building on this, we employ Monte Carlo Sampling on the noise and the time variable during the diffusion process, utilizing PGD [17,18] to optimize the aforementioned loss difference.Comprehensive experiments are conducted on artists' works and objects, addressing the potential for unauthorized style transfers and the creation of fabricated images, both of which necessitate copyright authentication.The experiment results affirm the effectiveness and robustness of our approach.\n\nIn summary, our contributions are as follows: \u2022 We have formulated a novel post-caution approach for copyright protection-copyright authentication.This method complements precaution measures and provides robust legal proof of infringements.",
            "score": 0.4142550271852262,
            "section_title": "Introduction",
            "char_start_offset": 2319,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 177
                },
                {
                    "start": 179,
                    "end": 403
                },
                {
                    "start": 403,
                    "end": 516
                },
                {
                    "start": 516,
                    "end": 582
                },
                {
                    "start": 582,
                    "end": 709
                },
                {
                    "start": 709,
                    "end": 948
                },
                {
                    "start": 948,
                    "end": 1089
                },
                {
                    "start": 1091,
                    "end": 1257
                },
                {
                    "start": 1257,
                    "end": 1356
                },
                {
                    "start": 1356,
                    "end": 1446
                },
                {
                    "start": 1446,
                    "end": 1580
                },
                {
                    "start": 1580,
                    "end": 1732
                },
                {
                    "start": 1732,
                    "end": 1915
                },
                {
                    "start": 1915,
                    "end": 2134
                },
                {
                    "start": 2134,
                    "end": 2213
                },
                {
                    "start": 2215,
                    "end": 2362
                },
                {
                    "start": 2362,
                    "end": 2455
                }
            ],
            "ref_mentions": [
                {
                    "start": 441,
                    "end": 444,
                    "matchedPaperCorpusId": "237204511"
                },
                {
                    "start": 444,
                    "end": 447,
                    "matchedPaperCorpusId": "246822452"
                },
                {
                    "start": 447,
                    "end": 450,
                    "matchedPaperCorpusId": "208139345"
                },
                {
                    "start": 727,
                    "end": 730,
                    "matchedPaperCorpusId": "256389993"
                },
                {
                    "start": 1860,
                    "end": 1864,
                    "matchedPaperCorpusId": "256697414"
                },
                {
                    "start": 1864,
                    "end": 1867,
                    "matchedPaperCorpusId": "3488815"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.8095703125
        },
        {
            "corpus_id": "267412857",
            "title": "Copyright Protection in Generative AI: A Technical Perspective",
            "text": "https://openai.com/policies/terms-of-use information; (b) \"Watermark techniques\" can be used by the data owner to trace and distinguish whether a generated work is produced based on their original creation; (c) \"Machine Unlearning\" means that the data owner can request a deletion of their data from the model or its output, once they identify the copyright infringement; \n\n(d) \"Dataset De-duplication\" removes duplicated data to mitigate the memorization effect to prevent the training data from being generated; (e) \"Alignment\" which uses a reward signal to reduce the memorization in LLMs; and \n\n(f) others including improved training and generation algorithms for better behaviors of LLMs. \u2022 For DGM users who create new works assisted by DGMs, the protected object is the generated contents from DGMs. Thus, the techniques for this type of copyright is barely related to the generation process of DGMs, as traditional copyright protection strategies can be also applied for protection of the DGM generated contents. \n\n\u2022 For DGM providers, there are representative \"watermarking strategies\" to inject the watermarks into the generated content or model parameters, such that we can track the ownership of the model. \n\nGiven the diversity in protection objectives, as well as DGM applications, we are motivated to have an overview on existing computational methods in this direction. Essentially, in Section 2, we will majorly discuss the copyright protection techniques for DGMs in the image domain. In Section 3, we discuss the strategies for text generation. Finally, we discuss the related problems in other domains, such as graphs, codes and audio generation in Section 4. In each section, we will introduce the background knowledge of existing DGMs, as well as the existing methodologies for data protection under different scenarios.",
            "score": 0.4141285749448146,
            "section_title": "Introduction",
            "char_start_offset": 4143,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 40
                },
                {
                    "start": 41,
                    "end": 371
                },
                {
                    "start": 374,
                    "end": 596
                },
                {
                    "start": 599,
                    "end": 693
                },
                {
                    "start": 694,
                    "end": 806
                },
                {
                    "start": 807,
                    "end": 1020
                },
                {
                    "start": 1023,
                    "end": 1218
                },
                {
                    "start": 1221,
                    "end": 1385
                },
                {
                    "start": 1386,
                    "end": 1502
                },
                {
                    "start": 1503,
                    "end": 1563
                },
                {
                    "start": 1564,
                    "end": 1842
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.787109375
        },
        {
            "corpus_id": "259064265",
            "title": "DeepfakeArt Challenge: A Benchmark Dataset for Generative AI Art Forgery and Data Poisoning Detection",
            "text": "In Table 1, we present the performance of various models evaluated on the DeepfakeArt dataset. Our task was a classification challenge, wherein the objective was to distinguish between 'similar pairs' (pairs where one image potentially infringes copyright) and 'dissimilar pairs' (pairs of unrelated images). Evaluation. In this study, we evaluated several prominent models: MultiGrain Berman et al. [2019], DINO-v1 Caron et al. [2021], DINO-v2 Oquab et al. [2023], and SwinTransformer Liu et al. [2021]. We utilized the embeddings generated by these models, calculating the dot product between each pair. Subsequently, we applied cross-validation to pinpoint the optimal threshold to distinguish between similar and dissimilar pairs. Utilizing this threshold, we quantized the calculated cosine similarities into binary outcomes, facilitating the analysis of various metrics, such as accuracy, precision, and recall, all of which are documented in Table 1. Discussion. As delineated in Table 1, the DINO-v2 ViT-L/14 model notably outshines the others in overall performance, albeit with a slight drawback in the precision metric, where the MultiGrain model emerges as the frontrunner. Despite showcasing considerable precision, the models generally falter in terms of recall. This indicates a heightened rate of false negatives across all models, a trend which holds significant implications in the context of copyright infringement detection. Particularly, a higher frequency of false negatives could potentially escalate the risk of litigations concerning copyright infringements in generative AI models. \n\nIn conclusion, this research articulates a nuanced definition of copyright infringement and introduces a synthetic dataset designed to emulate real-world scenarios where such infringements may occur. The experimental results underscore the current models' tendency to incur high false negative rates when applied to the dataset, thereby highlighting avenues for the development of more robust and efficient detection tools to identify and mitigate copyright infringements.",
            "score": 0.4138226191277694,
            "section_title": "Experiments and Discussion",
            "char_start_offset": 12491,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 94
                },
                {
                    "start": 95,
                    "end": 308
                },
                {
                    "start": 309,
                    "end": 320
                },
                {
                    "start": 321,
                    "end": 504
                },
                {
                    "start": 505,
                    "end": 605
                },
                {
                    "start": 606,
                    "end": 734
                },
                {
                    "start": 735,
                    "end": 957
                },
                {
                    "start": 958,
                    "end": 969
                },
                {
                    "start": 970,
                    "end": 1185
                },
                {
                    "start": 1186,
                    "end": 1276
                },
                {
                    "start": 1277,
                    "end": 1444
                },
                {
                    "start": 1445,
                    "end": 1607
                },
                {
                    "start": 1610,
                    "end": 1809
                },
                {
                    "start": 1810,
                    "end": 2082
                }
            ],
            "ref_mentions": [
                {
                    "start": 486,
                    "end": 503,
                    "matchedPaperCorpusId": "232352874"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.96337890625
        },
        {
            "corpus_id": "260333979",
            "title": "Model Synthesis for Zero-Shot Model Attribution",
            "text": "I N recent years, advanced generative (vision) models have revolutionized various fields such as art creation, design, and human-computer interaction [1]- [4]. Despite their positive impact, these models have also given rise to new concerns, such as copyright infringement issues and content supervision. To address these concerns, model attribution, the process of identifying the source model of generated content, has gained increasing attention [5]- [10]. It helps deter unauthorized copying and distribution, enabling content creators and rights holders to prove ownership and take legal action against infringements. Furthermore, model attribution allows regulators to identify and act against entities using generative models for harmful, illegal, or unethical purposes. \n\nExisting research seeks to identify the unique fingerprints on the images they generate, which can be leveraged to attribute a generated image to its source model. A commonly adopted model attribution paradigm frames the task as a multi-class classification problem [6], [8]- [10]. In this setup, images generated by a limited and static set of models are used to train a classifier, where each image is labeled with a unique model Fig. 1: Illustration of existing model attribution method (above) and our method (below). Existing methods rely on training with a limited set of real-world models, allows them to attribute seen classical models included in the training data. However, they struggle with generalizing to emerging unseen models like Stable Diffusion and DALL-E due to a gap in fingerprint distribution. Our method, in contrast, trains on numerous synthetic models that replicate a broader range of real-world generative model fingerprints, enabling it to attribute unseen models effectively in a zero-shot manner. \n\nID. During testing, the test image comes from a seen model predefined in the training set, and the classifier would identify the model ID of the image based on the model fingerprint it has learned [6], [8], [9]. However, there arises a scenario where a testing image originates from an unseen model not present during training. To tackle this, some methodologies [10], [11] adopt an open-set setup, assigning them an \"unknown\" label.",
            "score": 0.413808784600564,
            "section_title": "I. INTRODUCTION",
            "char_start_offset": 18,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 159
                },
                {
                    "start": 160,
                    "end": 304
                },
                {
                    "start": 305,
                    "end": 459
                },
                {
                    "start": 460,
                    "end": 622
                },
                {
                    "start": 623,
                    "end": 777
                },
                {
                    "start": 780,
                    "end": 943
                },
                {
                    "start": 944,
                    "end": 1061
                },
                {
                    "start": 1062,
                    "end": 1301
                },
                {
                    "start": 1302,
                    "end": 1454
                },
                {
                    "start": 1455,
                    "end": 1596
                },
                {
                    "start": 1597,
                    "end": 1807
                },
                {
                    "start": 1810,
                    "end": 1813
                },
                {
                    "start": 1814,
                    "end": 2021
                },
                {
                    "start": 2022,
                    "end": 2137
                },
                {
                    "start": 2138,
                    "end": 2243
                }
            ],
            "ref_mentions": [
                {
                    "start": 150,
                    "end": 153,
                    "matchedPaperCorpusId": "245335086"
                },
                {
                    "start": 454,
                    "end": 458,
                    "matchedPaperCorpusId": "257496280"
                },
                {
                    "start": 1046,
                    "end": 1049,
                    "matchedPaperCorpusId": "201058738"
                },
                {
                    "start": 1051,
                    "end": 1054,
                    "matchedPaperCorpusId": "247158092"
                },
                {
                    "start": 1056,
                    "end": 1060,
                    "matchedPaperCorpusId": "257496280"
                },
                {
                    "start": 2007,
                    "end": 2010,
                    "matchedPaperCorpusId": "201058738"
                },
                {
                    "start": 2012,
                    "end": 2015,
                    "matchedPaperCorpusId": "247158092"
                },
                {
                    "start": 2017,
                    "end": 2020,
                    "matchedPaperCorpusId": "250280166"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.96630859375
        },
        {
            "corpus_id": "270620122",
            "title": "Fantastic Copyrighted Beasts and How (Not) to Generate Them",
            "text": "If no description with a cosine similarity greater than 0.7 is found, we conclude that the user query does not intend to generate characters substantially similar to copyrighted ones. \n\nExperimental Setup To evaluate our detection methods, we curated a dataset comprising 200 descriptions of copyrighted characters and 200 standard prompts unlikely to cause copyright issues \u00a7 \u00a7 text-embedding-3-small negative prompts, still leads to the generation of copyrighted characters. This suggests that T2I models are deeply anchored to these character names. \n\nHowever, once we apply prompt rewriting and combine it with various negative prompts, the model is no longer inclined to generate these characters, as shown in Fig. 13. \n\nBatman Black Panther Iron Man Judy Hopps Mario Mickey Mouse Pikachu Spider-Man Thanos Maleficent  (Chen et al., 2024). With higher generation quality, the findings are also consistent with those observed using the Playground v2.5 model-adding more fine-grained negative prompts and applying prompt rewriting significantly reduces the similarity of the generated images to the original copyrighted character. \n\nFig. 15 visualizes results from the Stable Diffusion XL (SDXL) model (Podell et al., 2024). Although the generation quality of SDXL is generally lower compared to the Playground model (see Fig. 5), adding more fine-grained negative prompts and applying prompt rewriting significantly reduces the similarity of the generated images to the original copyrighted character.",
            "score": 0.41372985496028264,
            "section_title": "D MORE RESULTS ON DALL\u2022E",
            "char_start_offset": 49648,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 183
                },
                {
                    "start": 186,
                    "end": 476
                },
                {
                    "start": 477,
                    "end": 552
                },
                {
                    "start": 555,
                    "end": 723
                },
                {
                    "start": 726,
                    "end": 844
                },
                {
                    "start": 845,
                    "end": 1133
                },
                {
                    "start": 1136,
                    "end": 1227
                },
                {
                    "start": 1228,
                    "end": 1505
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.64794921875
        },
        {
            "corpus_id": "261243073",
            "title": "ORES: Open-vocabulary Responsible Visual Synthesis",
            "text": "ORES involves multiple tasks, and our method not only serves image generation but also directly works for various tasks without any modifications. We conducted experiments in four common tasks within the visual synthesis: (a) image generation, (b) image editing, (c) image inpainting, and (d) video synthesis. For the diffusion model, we used pretrained models from previous work without any changes. (Brooks, Holynski, and Efros 2023) followed the user's request to synthesize a vividly burning house, but the potential violent elements could lead to ethical issues with the image. Our method successfully prevents the synthesis of a burning house and, to some extent, adheres to the user's request by providing a damaged house, significantly reducing the risk of generating violent images. \n\nImage Inpainting As shown in Figure 6 (c), our method does not synthesize content that may include brand trademarks. The original CONTROLNET (Zhang and Agrawala 2023) generated an interface highly similar to the Windows 8 start screen, but Windows 8 was never released on the hardware depicted in the image, which could pose a risk of commercial infringement. Our method avoids generating responsibly and ensures the quality of image inpainting. \n\nVideo Generation As shown in Figure 6 (d), our method does not synthesize content that may contain copyrighted characters. The original VIDEOFUSION (Luo et al. 2023) generated high-quality videos that match the user's queries, but considering the user input in the image, there might be copyrighted characters, which could lead to copyright risks. \n\nOur method replaces copyrighted characters with ordinary people without copyright issues while maintaining a high similarity in the video content.",
            "score": 0.41366912765035824,
            "section_title": "Extending to Other Tasks",
            "char_start_offset": 20523,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 146
                },
                {
                    "start": 147,
                    "end": 309
                },
                {
                    "start": 310,
                    "end": 400
                },
                {
                    "start": 401,
                    "end": 582
                },
                {
                    "start": 583,
                    "end": 791
                },
                {
                    "start": 794,
                    "end": 910
                },
                {
                    "start": 911,
                    "end": 1153
                },
                {
                    "start": 1154,
                    "end": 1239
                },
                {
                    "start": 1242,
                    "end": 1364
                },
                {
                    "start": 1365,
                    "end": 1589
                },
                {
                    "start": 1592,
                    "end": 1738
                }
            ],
            "ref_mentions": [
                {
                    "start": 401,
                    "end": 435,
                    "matchedPaperCorpusId": "253581213"
                },
                {
                    "start": 1390,
                    "end": 1406,
                    "matchedPaperCorpusId": "257532642"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.69091796875
        },
        {
            "corpus_id": "276575866",
            "title": "Can Large Vision-Language Models Detect Images Copyright Infringement from GenAI?",
            "text": "In recent years, Vision-Language Models (VLMs) have significantly advanced the integration of visual and textual data, leading to more sophisticated AI applications. A notable example is CLIP, which employs contrastive learning to align images and text in a shared latent space, enabling zero-shot image classification and cross-modal retrieval (Radford et al., 2021). Building upon such foundations, Large Vision Language Models (LVLMs) like GPT-4 have extended capabilities to process both textual and visual inputs, enhancing tasks such as image description and visual question answering (GPT4V). Similarly, Anthropic's Claude 3.5 has been developed to handle multimodal inputs, contributing to advancements in understanding and generating content across different modalities (Claude3.5). Further contributions include LLaVA, which integrates visual features into language models to improve visual reasoning (Liu et al., 2024), and Qwen-VL, which supports multilingual conversations and end-to-end text recognition in images (Bai et al., 2023). Additionally, DeepSeek-VL2 has been recognized for its performance in visual understanding benchmarks, demonstrating the rapid progress in this field (Wu et al., 2024). Collectively, these models represent significant contributions in combining image and text, paving the way for comprehensive AI systems. \n\n2.2 Copyright Issues Related to Generative Models. \n\nThe rapid advancement of generative AI enables the creation of text and images that closely mimic human-authored works, leading to significant legal and ethical concerns regarding potential infringements of intellectual property rights (Zirpoli, 2023;Dzuong et al., 2024;Sag, 2023;Poland). A key contributing factor is that visual generative models may memorize portions of their training data, resulting in outputs that inadvertently reproduce IP-protected content (Carlini et al., 2023;Somepalli et al., 2023;Gu et al., 2023). To mitigate IP infringement, two primary approaches have emerged:",
            "score": 0.4136527391110027,
            "section_title": "Vision Language Models",
            "char_start_offset": 6052,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 165
                },
                {
                    "start": 166,
                    "end": 368
                },
                {
                    "start": 369,
                    "end": 599
                },
                {
                    "start": 600,
                    "end": 791
                },
                {
                    "start": 792,
                    "end": 1047
                },
                {
                    "start": 1048,
                    "end": 1216
                },
                {
                    "start": 1217,
                    "end": 1353
                },
                {
                    "start": 1356,
                    "end": 1406
                },
                {
                    "start": 1409,
                    "end": 1698
                },
                {
                    "start": 1699,
                    "end": 1937
                },
                {
                    "start": 1938,
                    "end": 2003
                }
            ],
            "ref_mentions": [
                {
                    "start": 345,
                    "end": 367,
                    "matchedPaperCorpusId": "231591445"
                },
                {
                    "start": 911,
                    "end": 929,
                    "matchedPaperCorpusId": "258179774"
                },
                {
                    "start": 1680,
                    "end": 1690,
                    "matchedPaperCorpusId": "258515384"
                },
                {
                    "start": 1875,
                    "end": 1897,
                    "matchedPaperCorpusId": "256389993"
                },
                {
                    "start": 1897,
                    "end": 1920,
                    "matchedPaperCorpusId": "254366634"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.88671875
        },
        {
            "paperId": "2e28407a767f388713fbd372248989c2e28473a7",
            "corpusId": 268048573,
            "title": "WIP: Auditing Artist Style Pirate in Text-to-image Generation Models",
            "venue": "Proceedings 2024 Workshop on AI Systems with Confidential COmputing",
            "year": 2024,
            "referenceCount": 46,
            "citationCount": 3,
            "influentialCitationCount": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": "CLOSED",
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.14722/aiscc.2024.23009?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.14722/aiscc.2024.23009, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "151483943",
                    "name": "L. Du"
                },
                {
                    "authorId": "2288042024",
                    "name": "Zheng Zhu"
                },
                {
                    "authorId": "2238153157",
                    "name": "Min Chen"
                },
                {
                    "authorId": "2237990407",
                    "name": "Shouling Ji"
                },
                {
                    "authorId": "2147335888",
                    "name": "Peng Cheng"
                },
                {
                    "authorId": "2138800088",
                    "name": "Jiming Chen"
                },
                {
                    "authorId": "2238124154",
                    "name": "Zhikun Zhang"
                }
            ],
            "abstract": "\u2014The text-to-image models based on diffusion processes, capable of transforming text descriptions into detailed images, have widespread applications in art, design, and beyond, such as DALL-E, Stable Diffusion, and Midjourney. However, they enable users without artistic training to create artwork comparable to professional quality, leading to concerns about copyright infringement. To tackle these issues, previous works have proposed strategies such as adversarial perturbation-based and watermarking-based methods. The former involves introducing subtle changes to disrupt the image generation process, while the latter involves embedding detectable marks in the artwork. The existing methods face limitations such as requiring modifications of the original image, being vulnerable to image pre-processing, and facing difficulties in applying them to the published artwork. To this end, we propose a new paradigm, called StyleAuditor , for artistic style auditing. StyleAuditor identifies if a suspect model has been fine-tuned using a specific artist\u2019s artwork by analyzing style-related features. Specifically, StyleAuditor employs a style extractor to obtain the multi-granularity style representations and treats artwork as samples of an artist\u2019s style. Then, StyleAuditor queries a trained discriminator to gain the auditing decisions. The results of the experiment on the artwork of thirty artists demonstrate the high accuracy of StyleAuditor , with an auditing accuracy of over 90% and a false positive rate of less than 1.3%.",
            "corpus_id": "268048573",
            "text": "\u2014The text-to-image models based on diffusion processes, capable of transforming text descriptions into detailed images, have widespread applications in art, design, and beyond, such as DALL-E, Stable Diffusion, and Midjourney. However, they enable users without artistic training to create artwork comparable to professional quality, leading to concerns about copyright infringement. To tackle these issues, previous works have proposed strategies such as adversarial perturbation-based and watermarking-based methods. The former involves introducing subtle changes to disrupt the image generation process, while the latter involves embedding detectable marks in the artwork. The existing methods face limitations such as requiring modifications of the original image, being vulnerable to image pre-processing, and facing difficulties in applying them to the published artwork. To this end, we propose a new paradigm, called StyleAuditor , for artistic style auditing. StyleAuditor identifies if a suspect model has been fine-tuned using a specific artist\u2019s artwork by analyzing style-related features. Specifically, StyleAuditor employs a style extractor to obtain the multi-granularity style representations and treats artwork as samples of an artist\u2019s style. Then, StyleAuditor queries a trained discriminator to gain the auditing decisions. The results of the experiment on the artwork of thirty artists demonstrate the high accuracy of StyleAuditor , with an auditing accuracy of over 90% and a false positive rate of less than 1.3%.",
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "score": 0.0,
            "stype": "public_api",
            "pdf_hash": "",
            "rerank_score": 0.9638671875
        },
        {
            "paperId": "6bdb1e327c4bd661173e94871648f442626e451c",
            "corpusId": 277856857,
            "title": "ArtistAuditor: Auditing Artist Style Pirate in Text-to-Image Generation Models",
            "venue": "The Web Conference",
            "year": 2025,
            "referenceCount": 75,
            "citationCount": 0,
            "influentialCitationCount": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2504.13061, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "151483943",
                    "name": "L. Du"
                },
                {
                    "authorId": "2288042024",
                    "name": "Zheng Zhu"
                },
                {
                    "authorId": "2238153157",
                    "name": "Min Chen"
                },
                {
                    "authorId": "2328027909",
                    "name": "Zhou Su"
                },
                {
                    "authorId": "2237990407",
                    "name": "Shouling Ji"
                },
                {
                    "authorId": "2147335888",
                    "name": "Peng Cheng"
                },
                {
                    "authorId": "2238129188",
                    "name": "Jiming Chen"
                },
                {
                    "authorId": "2238124154",
                    "name": "Zhikun Zhang"
                }
            ],
            "abstract": "Text-to-image models based on diffusion processes, such as DALL-E, Stable Diffusion, and Midjourney, are capable of transforming texts into detailed images and have widespread applications in art and design. As such, amateur users can easily imitate professional-level paintings by collecting an artist's work and fine-tuning the model, leading to concerns about artworks' copyright infringement. To tackle these issues, previous studies either add visually imperceptible perturbation to the artwork to change its underlying styles (perturbation-based methods) or embed post-training detectable watermarks in the artwork (watermark-based methods). However, when the artwork or the model has been published online, i.e., modification to the original artwork or model retraining is not feasible, these strategies might not be viable. To this end, we propose a novel method for data-use auditing in the text-to-image generation model. The general idea of ArtistAuditor is to identify if a suspicious model has been finetuned using the artworks of specific artists by analyzing the features related to the style. Concretely, ArtistAuditor employs a style extractor to obtain the multi-granularity style representations and treats artworks as samplings of an artist's style. Then, ArtistAuditor queries a trained discriminator to gain the auditing decisions. The experimental results on six combinations of models and datasets show that ArtistAuditor can achieve high AUC values (> 0.937). By studying ArtistAuditor's transferability and core modules, we provide valuable insights into the practical implementation. Finally, we demonstrate the effectiveness of ArtistAuditor in real-world cases by an online platform Scenario. ArtistAuditor is open-sourced at https://github.com/Jozenn/ArtistAuditor.",
            "corpus_id": "277856857",
            "text": "Text-to-image models based on diffusion processes, such as DALL-E, Stable Diffusion, and Midjourney, are capable of transforming texts into detailed images and have widespread applications in art and design. As such, amateur users can easily imitate professional-level paintings by collecting an artist's work and fine-tuning the model, leading to concerns about artworks' copyright infringement. To tackle these issues, previous studies either add visually imperceptible perturbation to the artwork to change its underlying styles (perturbation-based methods) or embed post-training detectable watermarks in the artwork (watermark-based methods). However, when the artwork or the model has been published online, i.e., modification to the original artwork or model retraining is not feasible, these strategies might not be viable. To this end, we propose a novel method for data-use auditing in the text-to-image generation model. The general idea of ArtistAuditor is to identify if a suspicious model has been finetuned using the artworks of specific artists by analyzing the features related to the style. Concretely, ArtistAuditor employs a style extractor to obtain the multi-granularity style representations and treats artworks as samplings of an artist's style. Then, ArtistAuditor queries a trained discriminator to gain the auditing decisions. The experimental results on six combinations of models and datasets show that ArtistAuditor can achieve high AUC values (> 0.937). By studying ArtistAuditor's transferability and core modules, we provide valuable insights into the practical implementation. Finally, we demonstrate the effectiveness of ArtistAuditor in real-world cases by an online platform Scenario. ArtistAuditor is open-sourced at https://github.com/Jozenn/ArtistAuditor.",
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "score": 0.0,
            "stype": "public_api",
            "pdf_hash": "",
            "rerank_score": 0.95361328125
        },
        {
            "paperId": "ac78b019c0cc883c79da0872cb4e1485a72928b6",
            "corpusId": 264490490,
            "title": "AntifakePrompt: Prompt-Tuned Vision-Language Models are Fake Image Detectors",
            "venue": "arXiv.org",
            "year": 2023,
            "referenceCount": 99,
            "citationCount": 30,
            "influentialCitationCount": 4,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2310.17419, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2261798677",
                    "name": "You-Ming Chang"
                },
                {
                    "authorId": "2261734483",
                    "name": "Chen Yeh"
                },
                {
                    "authorId": "2296714886",
                    "name": "Wei-Chen Chiu"
                },
                {
                    "authorId": "2261735247",
                    "name": "Ning Yu"
                }
            ],
            "abstract": "Deep generative models can create remarkably photorealistic fake images while raising concerns about misinformation and copyright infringement, known as deepfake threats. Deepfake detection technique is developed to distinguish between real and fake images, where the existing methods typically learn classifiers in the image domain or various feature domains. However, the generalizability of deepfake detection against emerging and more advanced generative models remains challenging. In this paper, being inspired by the zero-shot advantages of Vision-Language Models (VLMs), we propose a novel approach using VLMs (e.g. InstructBLIP) and prompt tuning techniques to improve the deepfake detection accuracy over unseen data. We formulate deepfake detection as a visual question answering problem, and tune soft prompts for InstructBLIP to answer the real/fake information of a query image. We conduct full-spectrum experiments on datasets from 3 held-in and 13 held-out generative models, covering modern text-to-image generation, image editing and image attacks. Results demonstrate that (1) the deepfake detection accuracy can be significantly and consistently improved (from 58.8% to 91.31%, in average accuracy over unseen data) using pretrained vision-language models with prompt tuning; (2) our superior performance is at less cost of trainable parameters, resulting in an effective and efficient solution for deepfake detection. Code and models can be found at https://github.com/nctu-eva-lab/AntifakePrompt.",
            "corpus_id": "264490490",
            "text": "Deep generative models can create remarkably photorealistic fake images while raising concerns about misinformation and copyright infringement, known as deepfake threats. Deepfake detection technique is developed to distinguish between real and fake images, where the existing methods typically learn classifiers in the image domain or various feature domains. However, the generalizability of deepfake detection against emerging and more advanced generative models remains challenging. In this paper, being inspired by the zero-shot advantages of Vision-Language Models (VLMs), we propose a novel approach using VLMs (e.g. InstructBLIP) and prompt tuning techniques to improve the deepfake detection accuracy over unseen data. We formulate deepfake detection as a visual question answering problem, and tune soft prompts for InstructBLIP to answer the real/fake information of a query image. We conduct full-spectrum experiments on datasets from 3 held-in and 13 held-out generative models, covering modern text-to-image generation, image editing and image attacks. Results demonstrate that (1) the deepfake detection accuracy can be significantly and consistently improved (from 58.8% to 91.31%, in average accuracy over unseen data) using pretrained vision-language models with prompt tuning; (2) our superior performance is at less cost of trainable parameters, resulting in an effective and efficient solution for deepfake detection. Code and models can be found at https://github.com/nctu-eva-lab/AntifakePrompt.",
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "score": 0.0,
            "stype": "public_api",
            "pdf_hash": "",
            "rerank_score": 0.5283203125
        },
        {
            "paperId": "c880be7d561b750260d01df2abdc79482a7b1af9",
            "corpusId": 276575481,
            "title": "PersGuard: Preventing Malicious Personalization via Backdoor Attacks on Pre-trained Text-to-Image Diffusion Models",
            "venue": "arXiv.org",
            "year": 2025,
            "referenceCount": 44,
            "citationCount": 1,
            "influentialCitationCount": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2502.16167, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2230013847",
                    "name": "Xinwei Liu"
                },
                {
                    "authorId": "144890263",
                    "name": "Xiaojun Jia"
                },
                {
                    "authorId": "2230070994",
                    "name": "Yuan Xun"
                },
                {
                    "authorId": "2108906565",
                    "name": "Hua Zhang"
                },
                {
                    "authorId": "2262088189",
                    "name": "Xiaochun Cao"
                }
            ],
            "abstract": "Diffusion models (DMs) have revolutionized data generation, particularly in text-to-image (T2I) synthesis. However, the widespread use of personalized generative models raises significant concerns regarding privacy violations and copyright infringement. To address these issues, researchers have proposed adversarial perturbation-based protection techniques. However, these methods have notable limitations, including insufficient robustness against data transformations and the inability to fully eliminate identifiable features of protected objects in the generated output. In this paper, we introduce PersGuard, a novel backdoor-based approach that prevents malicious personalization of specific images. Unlike traditional adversarial perturbation methods, PersGuard implant backdoor triggers into pre-trained T2I models, preventing the generation of customized outputs for designated protected images while allowing normal personalization for unprotected ones. Unfortunately, existing backdoor methods for T2I diffusion models fail to be applied to personalization scenarios due to the different backdoor objectives and the potential backdoor elimination during downstream fine-tuning processes. To address these, we propose three novel backdoor objectives specifically designed for personalization scenarios, coupled with backdoor retention loss engineered to resist downstream fine-tuning. These components are integrated into a unified optimization framework. Extensive experimental evaluations demonstrate PersGuard's effectiveness in preserving data privacy, even under challenging conditions including gray-box settings, multi-object protection, and facial identity scenarios. Our method significantly outperforms existing techniques, offering a more robust solution for privacy and copyright protection.",
            "corpus_id": "276575481",
            "text": "Diffusion models (DMs) have revolutionized data generation, particularly in text-to-image (T2I) synthesis. However, the widespread use of personalized generative models raises significant concerns regarding privacy violations and copyright infringement. To address these issues, researchers have proposed adversarial perturbation-based protection techniques. However, these methods have notable limitations, including insufficient robustness against data transformations and the inability to fully eliminate identifiable features of protected objects in the generated output. In this paper, we introduce PersGuard, a novel backdoor-based approach that prevents malicious personalization of specific images. Unlike traditional adversarial perturbation methods, PersGuard implant backdoor triggers into pre-trained T2I models, preventing the generation of customized outputs for designated protected images while allowing normal personalization for unprotected ones. Unfortunately, existing backdoor methods for T2I diffusion models fail to be applied to personalization scenarios due to the different backdoor objectives and the potential backdoor elimination during downstream fine-tuning processes. To address these, we propose three novel backdoor objectives specifically designed for personalization scenarios, coupled with backdoor retention loss engineered to resist downstream fine-tuning. These components are integrated into a unified optimization framework. Extensive experimental evaluations demonstrate PersGuard's effectiveness in preserving data privacy, even under challenging conditions including gray-box settings, multi-object protection, and facial identity scenarios. Our method significantly outperforms existing techniques, offering a more robust solution for privacy and copyright protection.",
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "score": 0.0,
            "stype": "public_api",
            "pdf_hash": "",
            "rerank_score": 0.89697265625
        },
        {
            "paperId": "7c586099efe2584beafa7f2774d06b93509afa67",
            "corpusId": 274824433,
            "title": "Research on Intelligent Generation Algorithm of AIGC in Digital Media Content Creation",
            "venue": "International Conferences on Computers, Information Processing and Advanced Education",
            "year": 2024,
            "referenceCount": 9,
            "citationCount": 0,
            "influentialCitationCount": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": "CLOSED",
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.1109/CIPAE64326.2024.00166?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1109/CIPAE64326.2024.00166, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2281484153",
                    "name": "Li Yong"
                },
                {
                    "authorId": "2281478967",
                    "name": "Xiong Dan"
                }
            ],
            "abstract": "This paper aims to explore how AIGC (Generative Artificial Intelligence) drives innovation in digital media content creation and proposes a new content generation framework based on Generative Multiple Adversarial steganography (GMASS). By integrating advanced machine learning models, the framework enables efficient generation and editing of image, video and audio content, while ensuring content security and privacy. Firstly, this paper summarizes the basic principle of AIGC and its application in the field of digital media, and points out its potential in improving creative efficiency and reducing production cost. Then, aiming at the challenges of the existing content generation technology in privacy disclosure and copyright infringement, the GMASS algorithm is proposed. The algorithm makes use of the characteristics of adversarial network to generate content and embed an imperceptible information hiding layer to effectively prevent unauthorized copying and tampering. In order to verify the effectiveness of the proposed framework, detailed system modeling and simulation experiments are carried out in this paper. The experimental results show that the GMASS algorithm can significantly improve the content security while maintaining the quality of the generated content. In addition, by comparing with traditional steganography techniques, GMASS's superior performance in resisting various attack methods and media content generation frameworks is demonstrated.",
            "corpus_id": "274824433",
            "text": "This paper aims to explore how AIGC (Generative Artificial Intelligence) drives innovation in digital media content creation and proposes a new content generation framework based on Generative Multiple Adversarial steganography (GMASS). By integrating advanced machine learning models, the framework enables efficient generation and editing of image, video and audio content, while ensuring content security and privacy. Firstly, this paper summarizes the basic principle of AIGC and its application in the field of digital media, and points out its potential in improving creative efficiency and reducing production cost. Then, aiming at the challenges of the existing content generation technology in privacy disclosure and copyright infringement, the GMASS algorithm is proposed. The algorithm makes use of the characteristics of adversarial network to generate content and embed an imperceptible information hiding layer to effectively prevent unauthorized copying and tampering. In order to verify the effectiveness of the proposed framework, detailed system modeling and simulation experiments are carried out in this paper. The experimental results show that the GMASS algorithm can significantly improve the content security while maintaining the quality of the generated content. In addition, by comparing with traditional steganography techniques, GMASS's superior performance in resisting various attack methods and media content generation frameworks is demonstrated.",
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "score": 0.0,
            "stype": "public_api",
            "pdf_hash": "",
            "rerank_score": 0.2264404296875
        },
        {
            "paperId": "b9b282e4347a2de622c6b65374b9fa189f02b755",
            "corpusId": 269457237,
            "title": "Probing Unlearned Diffusion Models: A Transferable Adversarial Attack Perspective",
            "venue": "arXiv.org",
            "year": 2024,
            "referenceCount": 36,
            "citationCount": 6,
            "influentialCitationCount": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2404.19382, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2277447235",
                    "name": "Xiaoxuan Han"
                },
                {
                    "authorId": "2143935738",
                    "name": "Songlin Yang"
                },
                {
                    "authorId": "40119691",
                    "name": "Wei Wang"
                },
                {
                    "authorId": "2286344285",
                    "name": "Yang Li"
                },
                {
                    "authorId": "2277272453",
                    "name": "Jing Dong"
                }
            ],
            "abstract": "Advanced text-to-image diffusion models raise safety concerns regarding identity privacy violation, copyright infringement, and Not Safe For Work content generation. Towards this, unlearning methods have been developed to erase these involved concepts from diffusion models. However, these unlearning methods only shift the text-to-image mapping and preserve the visual content within the generative space of diffusion models, leaving a fatal flaw for restoring these erased concepts. This erasure trustworthiness problem needs probe, but previous methods are sub-optimal from two perspectives: (1) Lack of transferability: Some methods operate within a white-box setting, requiring access to the unlearned model. And the learned adversarial input often fails to transfer to other unlearned models for concept restoration; (2) Limited attack: The prompt-level methods struggle to restore narrow concepts from unlearned models, such as celebrity identity. Therefore, this paper aims to leverage the transferability of the adversarial attack to probe the unlearning robustness under a black-box setting. This challenging scenario assumes that the unlearning method is unknown and the unlearned model is inaccessible for optimization, requiring the attack to be capable of transferring across different unlearned models. Specifically, we employ an adversarial search strategy to search for the adversarial embedding which can transfer across different unlearned models. This strategy adopts the original Stable Diffusion model as a surrogate model to iteratively erase and search for embeddings, enabling it to find the embedding that can restore the target concept for different unlearning methods. Extensive experiments demonstrate the transferability of the searched adversarial embedding across several state-of-the-art unlearning methods and its effectiveness for different levels of concepts.",
            "corpus_id": "269457237",
            "text": "Advanced text-to-image diffusion models raise safety concerns regarding identity privacy violation, copyright infringement, and Not Safe For Work content generation. Towards this, unlearning methods have been developed to erase these involved concepts from diffusion models. However, these unlearning methods only shift the text-to-image mapping and preserve the visual content within the generative space of diffusion models, leaving a fatal flaw for restoring these erased concepts. This erasure trustworthiness problem needs probe, but previous methods are sub-optimal from two perspectives: (1) Lack of transferability: Some methods operate within a white-box setting, requiring access to the unlearned model. And the learned adversarial input often fails to transfer to other unlearned models for concept restoration; (2) Limited attack: The prompt-level methods struggle to restore narrow concepts from unlearned models, such as celebrity identity. Therefore, this paper aims to leverage the transferability of the adversarial attack to probe the unlearning robustness under a black-box setting. This challenging scenario assumes that the unlearning method is unknown and the unlearned model is inaccessible for optimization, requiring the attack to be capable of transferring across different unlearned models. Specifically, we employ an adversarial search strategy to search for the adversarial embedding which can transfer across different unlearned models. This strategy adopts the original Stable Diffusion model as a surrogate model to iteratively erase and search for embeddings, enabling it to find the embedding that can restore the target concept for different unlearning methods. Extensive experiments demonstrate the transferability of the searched adversarial embedding across several state-of-the-art unlearning methods and its effectiveness for different levels of concepts.",
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "score": 0.0,
            "stype": "public_api",
            "pdf_hash": "",
            "rerank_score": 0.273193359375
        },
        {
            "paperId": "d887c04d66b6fa0a526cd049d89a66b4d53246e6",
            "corpusId": 259075262,
            "title": "Unlearnable Examples for Diffusion Models: Protect Data from Unauthorized Exploitation",
            "venue": "arXiv.org",
            "year": 2023,
            "referenceCount": 45,
            "citationCount": 28,
            "influentialCitationCount": 3,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "http://arxiv.org/pdf/2306.01902",
                "status": "CLOSED",
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2306.01902, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": null,
                    "name": "Zhengyue Zhao"
                },
                {
                    "authorId": "2004228925",
                    "name": "Jinhao Duan"
                },
                {
                    "authorId": "2109635961",
                    "name": "Xingui Hu"
                },
                {
                    "authorId": "46321210",
                    "name": "Kaidi Xu"
                },
                {
                    "authorId": "2108811793",
                    "name": "Chenan Wang"
                },
                {
                    "authorId": "2118404461",
                    "name": "Rui Zhang"
                },
                {
                    "authorId": "1678776",
                    "name": "Zidong Du"
                },
                {
                    "authorId": "145461472",
                    "name": "Qi Guo"
                },
                {
                    "authorId": "7377735",
                    "name": "Yunji Chen"
                }
            ],
            "abstract": "Diffusion models have demonstrated remarkable performance in image generation tasks, paving the way for powerful AIGC applications. However, these widely-used generative models can also raise security and privacy concerns, such as copyright infringement, and sensitive data leakage. To tackle these issues, we propose a method, Unlearnable Diffusion Perturbation, to safeguard images from unauthorized exploitation. Our approach involves designing an algorithm to generate sample-wise perturbation noise for each image to be protected. This imperceptible protective noise makes the data almost unlearnable for diffusion models, i.e., diffusion models trained or fine-tuned on the protected data cannot generate high-quality and diverse images related to the protected training data. Theoretically, we frame this as a max-min optimization problem and introduce EUDP, a noise scheduler-based method to enhance the effectiveness of the protective noise. We evaluate our methods on both Denoising Diffusion Probabilistic Model and Latent Diffusion Models, demonstrating that training diffusion models on the protected data lead to a significant reduction in the quality of the generated images. Especially, the experimental results on Stable Diffusion demonstrate that our method effectively safeguards images from being used to train Diffusion Models in various tasks, such as training specific objects and styles. This achievement holds significant importance in real-world scenarios, as it contributes to the protection of privacy and copyright against AI-generated content.",
            "corpus_id": "259075262",
            "text": "Diffusion models have demonstrated remarkable performance in image generation tasks, paving the way for powerful AIGC applications. However, these widely-used generative models can also raise security and privacy concerns, such as copyright infringement, and sensitive data leakage. To tackle these issues, we propose a method, Unlearnable Diffusion Perturbation, to safeguard images from unauthorized exploitation. Our approach involves designing an algorithm to generate sample-wise perturbation noise for each image to be protected. This imperceptible protective noise makes the data almost unlearnable for diffusion models, i.e., diffusion models trained or fine-tuned on the protected data cannot generate high-quality and diverse images related to the protected training data. Theoretically, we frame this as a max-min optimization problem and introduce EUDP, a noise scheduler-based method to enhance the effectiveness of the protective noise. We evaluate our methods on both Denoising Diffusion Probabilistic Model and Latent Diffusion Models, demonstrating that training diffusion models on the protected data lead to a significant reduction in the quality of the generated images. Especially, the experimental results on Stable Diffusion demonstrate that our method effectively safeguards images from being used to train Diffusion Models in various tasks, such as training specific objects and styles. This achievement holds significant importance in real-world scenarios, as it contributes to the protection of privacy and copyright against AI-generated content.",
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "score": 0.0,
            "stype": "public_api",
            "pdf_hash": "",
            "rerank_score": 0.95361328125
        },
        {
            "paperId": "04a3111295aaad2481e6c1e1ad4c5b1c289ca6c5",
            "corpusId": 273162563,
            "title": "Revealing the Unseen: Guiding Personalized Diffusion Models to Expose Training Data",
            "venue": "arXiv.org",
            "year": 2024,
            "referenceCount": 37,
            "citationCount": 2,
            "influentialCitationCount": 1,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2410.03039, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2303917211",
                    "name": "Xiaoyu Wu"
                },
                {
                    "authorId": "2118001291",
                    "name": "Jiaru Zhang"
                },
                {
                    "authorId": "2324568191",
                    "name": "Steven Wu"
                }
            ],
            "abstract": "Diffusion Models (DMs) have evolved into advanced image generation tools, especially for few-shot fine-tuning where a pretrained DM is fine-tuned on a small set of images to capture specific styles or objects. Many people upload these personalized checkpoints online, fostering communities such as Civitai and HuggingFace. However, model owners may overlook the potential risks of data leakage by releasing their fine-tuned checkpoints. Moreover, concerns regarding copyright violations arise when unauthorized data is used during fine-tuning. In this paper, we ask:\"Can training data be extracted from these fine-tuned DMs shared online?\"A successful extraction would present not only data leakage threats but also offer tangible evidence of copyright infringement. To answer this, we propose FineXtract, a framework for extracting fine-tuning data. Our method approximates fine-tuning as a gradual shift in the model's learned distribution -- from the original pretrained DM toward the fine-tuning data. By extrapolating the models before and after fine-tuning, we guide the generation toward high-probability regions within the fine-tuned data distribution. We then apply a clustering algorithm to extract the most probable images from those generated using this extrapolated guidance. Experiments on DMs fine-tuned with datasets such as WikiArt, DreamBooth, and real-world checkpoints posted online validate the effectiveness of our method, extracting approximately 20% of fine-tuning data in most cases, significantly surpassing baseline performance.",
            "corpus_id": "273162563",
            "text": "Diffusion Models (DMs) have evolved into advanced image generation tools, especially for few-shot fine-tuning where a pretrained DM is fine-tuned on a small set of images to capture specific styles or objects. Many people upload these personalized checkpoints online, fostering communities such as Civitai and HuggingFace. However, model owners may overlook the potential risks of data leakage by releasing their fine-tuned checkpoints. Moreover, concerns regarding copyright violations arise when unauthorized data is used during fine-tuning. In this paper, we ask:\"Can training data be extracted from these fine-tuned DMs shared online?\"A successful extraction would present not only data leakage threats but also offer tangible evidence of copyright infringement. To answer this, we propose FineXtract, a framework for extracting fine-tuning data. Our method approximates fine-tuning as a gradual shift in the model's learned distribution -- from the original pretrained DM toward the fine-tuning data. By extrapolating the models before and after fine-tuning, we guide the generation toward high-probability regions within the fine-tuned data distribution. We then apply a clustering algorithm to extract the most probable images from those generated using this extrapolated guidance. Experiments on DMs fine-tuned with datasets such as WikiArt, DreamBooth, and real-world checkpoints posted online validate the effectiveness of our method, extracting approximately 20% of fine-tuning data in most cases, significantly surpassing baseline performance.",
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "score": 0.0,
            "stype": "public_api",
            "pdf_hash": "",
            "rerank_score": 0.92431640625
        },
        {
            "paperId": "eca4389780eec5257ed40df1011b2ff7d40e593a",
            "corpusId": 273655150,
            "title": "Shallow Diffuse: Robust and Invisible Watermarking through Low-Dimensional Subspaces in Diffusion Models",
            "venue": "arXiv.org",
            "year": 2024,
            "referenceCount": 76,
            "citationCount": 2,
            "influentialCitationCount": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2410.21088, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2328024472",
                    "name": "Wenda Li"
                },
                {
                    "authorId": "2257089655",
                    "name": "Huijie Zhang"
                },
                {
                    "authorId": "2256993150",
                    "name": "Qing Qu"
                }
            ],
            "abstract": "The widespread use of AI-generated content from diffusion models has raised significant concerns regarding misinformation and copyright infringement. Watermarking is a crucial technique for identifying these AI-generated images and preventing their misuse. In this paper, we introduce Shallow Diffuse, a new watermarking technique that embeds robust and invisible watermarks into diffusion model outputs. Unlike existing approaches that integrate watermarking throughout the entire diffusion sampling process, Shallow Diffuse decouples these steps by leveraging the presence of a low-dimensional subspace in the image generation process. This method ensures that a substantial portion of the watermark lies in the null space of this subspace, effectively separating it from the image generation process. Our theoretical and empirical analyses show that this decoupling strategy greatly enhances the consistency of data generation and the detectability of the watermark. Extensive experiments further validate that our Shallow Diffuse outperforms existing watermarking methods in terms of robustness and consistency. The codes will be released at https://github.com/liwd190019/Shallow-Diffuse.",
            "corpus_id": "273655150",
            "text": "The widespread use of AI-generated content from diffusion models has raised significant concerns regarding misinformation and copyright infringement. Watermarking is a crucial technique for identifying these AI-generated images and preventing their misuse. In this paper, we introduce Shallow Diffuse, a new watermarking technique that embeds robust and invisible watermarks into diffusion model outputs. Unlike existing approaches that integrate watermarking throughout the entire diffusion sampling process, Shallow Diffuse decouples these steps by leveraging the presence of a low-dimensional subspace in the image generation process. This method ensures that a substantial portion of the watermark lies in the null space of this subspace, effectively separating it from the image generation process. Our theoretical and empirical analyses show that this decoupling strategy greatly enhances the consistency of data generation and the detectability of the watermark. Extensive experiments further validate that our Shallow Diffuse outperforms existing watermarking methods in terms of robustness and consistency. The codes will be released at https://github.com/liwd190019/Shallow-Diffuse.",
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "score": 0.0,
            "stype": "public_api",
            "pdf_hash": "",
            "rerank_score": 0.7109375
        },
        {
            "paperId": "bfb6da971196335ce899e272fa77b1d325c09155",
            "corpusId": 269635525,
            "title": "An Inversion-based Measure of Memorization for Diffusion Models",
            "venue": "",
            "year": 2024,
            "referenceCount": 59,
            "citationCount": 0,
            "influentialCitationCount": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2405.05846, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2239308702",
                    "name": "Zhe Ma"
                },
                {
                    "authorId": "2282496681",
                    "name": "Xuhong Zhang"
                },
                {
                    "authorId": "2300423996",
                    "name": "Qingming Li"
                },
                {
                    "authorId": "2056897605",
                    "name": "Tianyu Du"
                },
                {
                    "authorId": "2266428031",
                    "name": "Wenzhi Chen"
                },
                {
                    "authorId": "2274942629",
                    "name": "Zonghui Wang"
                },
                {
                    "authorId": "2237797356",
                    "name": "Shouling Ji"
                }
            ],
            "abstract": "The past few years have witnessed substantial advances in image generation powered by diffusion models. However, it was shown that diffusion models are vulnerable to training data memorization, raising concerns regarding copyright infringement and privacy invasion. This study delves into a rigorous analysis of memorization in diffusion models. We introduce an inversion-based measure of memorization, InvMM, which searches for a sensitive latent noise distribution accounting for the replication of an image. For accurate estimation of the memorization score, we propose an adaptive algorithm that balances the normality and sensitivity of the inverted distribution. Comprehensive experiments, conducted on both unconditional and text-guided diffusion models, demonstrate that InvMM is capable of detecting heavily memorized images and elucidating the effect of various factors on memorization. Additionally, we discuss how memorization differs from membership. In practice, InvMM serves as a useful tool for model developers to reliably assess the risk of memorization, thereby contributing to the enhancement of trustworthiness and privacy-preserving capabilities of diffusion models.",
            "corpus_id": "269635525",
            "text": "The past few years have witnessed substantial advances in image generation powered by diffusion models. However, it was shown that diffusion models are vulnerable to training data memorization, raising concerns regarding copyright infringement and privacy invasion. This study delves into a rigorous analysis of memorization in diffusion models. We introduce an inversion-based measure of memorization, InvMM, which searches for a sensitive latent noise distribution accounting for the replication of an image. For accurate estimation of the memorization score, we propose an adaptive algorithm that balances the normality and sensitivity of the inverted distribution. Comprehensive experiments, conducted on both unconditional and text-guided diffusion models, demonstrate that InvMM is capable of detecting heavily memorized images and elucidating the effect of various factors on memorization. Additionally, we discuss how memorization differs from membership. In practice, InvMM serves as a useful tool for model developers to reliably assess the risk of memorization, thereby contributing to the enhancement of trustworthiness and privacy-preserving capabilities of diffusion models.",
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "score": 0.0,
            "stype": "public_api",
            "pdf_hash": "",
            "rerank_score": 0.5830078125
        }
    ],
    "quotes": {
        "cost": 0.20005799999999999,
        "quotes": [
            {
                "idx": 0,
                "key": "[257050406 | Vyas et al. | 2023 | Citations: 94]",
                "snippets": "Generative models for images, text, code, and other domains pose new challenges for ensuring their outputs are protected from copyright infringement. Such models are trained on a large corpus of data, where it is often impractical to ensure the training set is 100% free of copyrighted material. Furthermore, removing copyrighted material from training may also be undesirable. For example, a human author is free to read and use copyrighted material as inspiration for their work, as long as they do not copy it. Similarly, it may be beneficial to use copyrighted material when training in order to have more effective generative models.\n\nCopyright infringement by generative models can potentially arise in (at least) two manners. First, in the training phase, the algorithm could directly access copyrighted material, and the learned model itself could implicitly contain (e.g. coded in its weights) verbatim copies of some of this material. The copyright issues arising during training share many similarities with other settings in which algorithms scrape significant amounts of data, including search-engine indexing and digitizing books. Here, the question of what constitutes a copyright infringement is largely a question of \"fair use.\" This work does not examine these fair use issues that arise in the training phase, and we refer the reader to the several legal precedents in this area [Samuelson, 2021].\n\nThe second notable source of potential infringement is in the deployment phase, where a user provides a prompt x to the model to obtain some output y. Apriori, we cannot rule out the possibility that y is either a verbatim copy or substantially similar to some copyrighted training data. Moreover, unlike search engines, generative models do not keep track of the provenance of their outputs. Hence, a user of such an output y (e.g., a software company using generated code, or a designer using a generated image) has no easy way to verify that it does not infringe upon any copyrighted material.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {},
                "metadata": [
                    {
                        "quote": "Generative models for images, text, code, and other domains pose new challenges for ensuring their outputs are protected from copyright infringement. Such models are trained on a large corpus of data, where it is often impractical to ensure the training set is 100% free of copyrighted material. Furthermore, removing copyrighted material from training may also be undesirable. For example, a human author is free to read and use copyrighted material as inspiration for their work, as long as they do not copy it. Similarly, it may be beneficial to use copyrighted material when training in order to have more effective generative models.\n\nCopyright infringement by generative models can potentially arise in (at least) two manners. First, in the training phase, the algorithm could directly access copyrighted material, and the learned model itself could implicitly contain (e.g. coded in its weights) verbatim copies of some of this material. The copyright issues arising during training share many similarities with other settings in which algorithms scrape significant amounts of data, including search-engine indexing and digitizing books. Here, the question of what constitutes a copyright infringement is largely a question of \"fair use.\" This work does not examine these fair use issues that arise in the training phase, and we refer the reader to the several legal precedents in this area [Samuelson, 2021].\n\nThe second notable source of potential infringement is in the deployment phase, where a user provides a prompt x to the model to obtain some output y. Apriori, we cannot rule out the possibility that y is either a verbatim copy or substantially similar to some copyrighted training data. Moreover, unlike search engines, generative models do not keep track of the provenance of their outputs. Hence, a user of such an output y (e.g., a software company using generated code, or a designer using a generated image) has no easy way to verify that it does not infringe upon any copyrighted material.",
                        "pdf_hash": ""
                    }
                ]
            },
            {
                "idx": 1,
                "key": "[260155285 | Leotta et al. | 2023 | Citations: 7]",
                "snippets": "The rapid advancement of generative models, particularly Diffusion Models [7], has led to a surge in high-quality, realistic image generation. These models have demonstrated immense potential for creative applications across various domains, including art, design, and advertising. However, their ability to replicate the styles of specific artists raises concerns about Intellectual Property (IP) rights and potential copyright infringements [20,25,19]. \n\nAs the boundary between human creativity and machine-generated content becomes increasingly blurred, it is crucial to address the legal and ethical implications of using generative models to produce art, as well as to develop methods that evaluate the extent to which generated images are influenced by the works of real artists and ensure the protection of their intellectual property. \n\nSeveral studies and articles have explored the legal ramifications of generative models and their potential to infringe on copyrights. In an article dated 2021 the challenges in determining copyright ownership for AI-generated works were discussed and authors emphasized the need for legal frameworks that could adequately address the unique nature of generative models. MLQ.AI also reported on a copyright infringement case involving generative AI, which sparked debates on the responsibilities of AI developers and users in protecting original creators' rights. In response to these concerns, legal scholars have delved into the complexities of copyright law as it pertains to AI-generated artworks. Gillotte [8] examined the challenges in assigning liability and protecting IP rights, arguing that existing copyright laws may not be sufficient to address the unique characteristics of AI-generated contents. Indeed, it is became a copyright dilemma and the need for a balance between innovation and IP protection is arising to ensure that creative works are safeguarded without stifling technological advancements [16].",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {
                    "[234357997 | Dhariwal et al. | 2021 | Citations: 7951]": "We show that diffusion models can achieve image sample quality superior to the current state-of-the-art generative models. We achieve this on unconditional image synthesis by finding a better architecture through a series of ablations. For conditional image synthesis, we further improve sample quality with classifier guidance: a simple, compute-efficient method for trading off diversity for fidelity using gradients from a classifier. We achieve an FID of 2.97 on ImageNet 128$\\times$128, 4.59 on ImageNet 256$\\times$256, and 7.72 on ImageNet 512$\\times$512, and we match BigGAN-deep even with as few as 25 forward passes per sample, all while maintaining better coverage of the distribution. Finally, we find that classifier guidance combines well with upsampling diffusion models, further improving FID to 3.94 on ImageNet 256$\\times$256 and 3.85 on ImageNet 512$\\times$512. We release our code at https://github.com/openai/guided-diffusion"
                },
                "metadata": [
                    {
                        "section_title": "Introduction",
                        "pdf_hash": "",
                        "start": 0,
                        "end": 1968,
                        "sentence_offsets": [
                            {
                                "start": 0,
                                "end": 142
                            },
                            {
                                "start": 143,
                                "end": 281
                            },
                            {
                                "start": 282,
                                "end": 456
                            },
                            {
                                "start": 459,
                                "end": 845
                            },
                            {
                                "start": 848,
                                "end": 985
                            },
                            {
                                "start": 986,
                                "end": 1224
                            },
                            {
                                "start": 1225,
                                "end": 1420
                            },
                            {
                                "start": 1421,
                                "end": 1558
                            },
                            {
                                "start": 1559,
                                "end": 1767
                            },
                            {
                                "start": 1768,
                                "end": 1979
                            }
                        ],
                        "ref_mentions": [
                            "234357997",
                            "233748873"
                        ],
                        "quote": "The rapid advancement of generative models, particularly Diffusion Models [7], has led to a surge in high-quality, realistic image generation. These models have demonstrated immense potential for creative applications across various domains, including art, design, and advertising. However, their ability to replicate the styles of specific artists raises concerns about Intellectual Property (IP) rights and potential copyright infringements [20,25,19]. \n\nAs the boundary between human creativity and machine-generated content becomes increasingly blurred, it is crucial to address the legal and ethical implications of using generative models to produce art, as well as to develop methods that evaluate the extent to which generated images are influenced by the works of real artists and ensure the protection of their intellectual property. \n\nSeveral studies and articles have explored the legal ramifications of generative models and their potential to infringe on copyrights. In an article dated 2021 the challenges in determining copyright ownership for AI-generated works were discussed and authors emphasized the need for legal frameworks that could adequately address the unique nature of generative models. MLQ.AI also reported on a copyright infringement case involving generative AI, which sparked debates on the responsibilities of AI developers and users in protecting original creators' rights. In response to these concerns, legal scholars have delved into the complexities of copyright law as it pertains to AI-generated artworks. Gillotte [8] examined the challenges in assigning liability and protecting IP rights, arguing that existing copyright laws may not be sufficient to address the unique characteristics of AI-generated contents. Indeed, it is became a copyright dilemma and the need for a balance between innovation and IP protection is arising to ensure that creative works are safeguarded without stifling technological advancements [16]."
                    }
                ]
            },
            {
                "idx": 2,
                "key": "[265066820 | Fei et al. | 2023 | Citations: 6]",
                "snippets": "In recent years, there has been significant growth in the commercial applications of generative models, licensed and distributed by model developers to users, who in turn use them to offer services. In this scenario, there is a need to track and identify the responsible user in the presence of a violation of the license agreement or any kind of malicious usage.\n\nProtecting the Intellectual Property Rights (IPR) of model owners has become a pressing issue to avoid potential copyright infringements, such as unauthorized duplication or model theft, when these models are delivered to malicious users. Deep Neural Network (DNN) watermarking has been proposed as a solution to protect the IPR associated with DNN models [1].\n\nWith the exception of a few scattered works [3], the existing approaches for the watermarking of generative models, notably GANs, are designed for ownership verification, aiming at making it possible to retrieve the model authorship information from the generated images.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {},
                "metadata": [
                    {
                        "quote": "In recent years, there has been significant growth in the commercial applications of generative models, licensed and distributed by model developers to users, who in turn use them to offer services. In this scenario, there is a need to track and identify the responsible user in the presence of a violation of the license agreement or any kind of malicious usage.\n\nProtecting the Intellectual Property Rights (IPR) of model owners has become a pressing issue to avoid potential copyright infringements, such as unauthorized duplication or model theft, when these models are delivered to malicious users. Deep Neural Network (DNN) watermarking has been proposed as a solution to protect the IPR associated with DNN models [1].\n\nWith the exception of a few scattered works [3], the existing approaches for the watermarking of generative models, notably GANs, are designed for ownership verification, aiming at making it possible to retrieve the model authorship information from the generated images.",
                        "pdf_hash": ""
                    }
                ]
            },
            {
                "idx": 3,
                "key": "[265351912 | Zhou et al. | 2023 | Citations: 2]",
                "snippets": "However, these AI-generated artworks inherit the characteristics of the images that are used to train models [33,40,43], which might be pretty similar to the original ones, as shown in Figure 1. Such similarity has aroused concerns about copyright infringement disputes. For example, three artists (Sarah Andersen, Kelly McKernan, and Karla Ortiz) have recently accused Stable Diffusion of unlawfully scraping copyrighted images from the Internet to mimic their art styles [11]. To this end, research on responsible AI image generation is in urgent need to address such copyright issues.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {},
                "metadata": [
                    {
                        "quote": "However, these AI-generated artworks inherit the characteristics of the images that are used to train models [33,40,43], which might be pretty similar to the original ones, as shown in Figure 1. Such similarity has aroused concerns about copyright infringement disputes. For example, three artists (Sarah Andersen, Kelly McKernan, and Karla Ortiz) have recently accused Stable Diffusion of unlawfully scraping copyrighted images from the Internet to mimic their art styles [11]. To this end, research on responsible AI image generation is in urgent need to address such copyright issues.",
                        "pdf_hash": ""
                    }
                ]
            },
            {
                "idx": 4,
                "key": "[265352103 | Zhang et al. | 2023 | Citations: 10]",
                "snippets": "Diffusion models excel in many generative modeling tasks, notably in creating images from text prompts, a task referred to as text-to-image (T2I) generation. Despite the ability to generate high-quality images, these models often replicate elements from their training data, leading to increasing copyright concerns in real applications in recent years. In response to this raising concern about copyright infringement, recent studies have studied the copyright behavior of diffusion models when using direct, copyrighted prompts.\n\nOur research extends this by examining subtler forms of infringement, where even indirect prompts can trigger copyright issues.\n\nThe apprehension surrounding copyright protection in diffusion models has also evolved into a tangible threat, as multiple lawsuits related to copyright infringement have been initiated against companies that utilize diffusion models for commercial purposes. Notably, there have been instances of lawsuits: Stability AI and Mid-Journey are both facing civil suits for training their models on artists' work without their consent thereby allowing their models to replicate the style and work of such artists [Vincent, 2023].\n\nFor copyright infringement, we focus on the copyright regulation in the US [Copyright Office, 2022]. Nevertheless, the concept of copyright applies to the regulation in other countries. In the context of copyright law in the US, whether a piece of copyrighted material can be used by others is governed by the concept of Fair Use which permits the use of copyrighted material only in a transformative manner that is distinct from the original work. There have been legal precedents which demonstrate that structural similarity can lead to infringement claims. Given that such generative models are trained on datasets such as LAION-5B [Schuhmann et al., 2022] which contains publicly available copyrighted data, if the models generate images with visual features that have substantial structural similarity to the original copyrighted images, this would likely be grounds for claims of copyright infringement.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {},
                "metadata": [
                    {
                        "quote": "Diffusion models excel in many generative modeling tasks, notably in creating images from text prompts, a task referred to as text-to-image (T2I) generation. Despite the ability to generate high-quality images, these models often replicate elements from their training data, leading to increasing copyright concerns in real applications in recent years. In response to this raising concern about copyright infringement, recent studies have studied the copyright behavior of diffusion models when using direct, copyrighted prompts.\n\nOur research extends this by examining subtler forms of infringement, where even indirect prompts can trigger copyright issues.\n\nThe apprehension surrounding copyright protection in diffusion models has also evolved into a tangible threat, as multiple lawsuits related to copyright infringement have been initiated against companies that utilize diffusion models for commercial purposes. Notably, there have been instances of lawsuits: Stability AI and Mid-Journey are both facing civil suits for training their models on artists' work without their consent thereby allowing their models to replicate the style and work of such artists [Vincent, 2023].\n\nFor copyright infringement, we focus on the copyright regulation in the US [Copyright Office, 2022]. Nevertheless, the concept of copyright applies to the regulation in other countries. In the context of copyright law in the US, whether a piece of copyrighted material can be used by others is governed by the concept of Fair Use which permits the use of copyrighted material only in a transformative manner that is distinct from the original work. There have been legal precedents which demonstrate that structural similarity can lead to infringement claims. Given that such generative models are trained on datasets such as LAION-5B [Schuhmann et al., 2022] which contains publicly available copyrighted data, if the models generate images with visual features that have substantial structural similarity to the original copyrighted images, this would likely be grounds for claims of copyright infringement.",
                        "pdf_hash": ""
                    }
                ]
            },
            {
                "idx": 5,
                "key": "[265551515 | Li et al. | 2023 | Citations: 5]",
                "snippets": "In recent years, the advancement of large generative models [17,(Sohl-Dickstein et al., 2015)(Song et al., 2020) has revolutionized high-quality image synthesis [34,(Rombach et al., 2021)(Saharia et al., 2022), paving the way for commercial applications that enable the public to effortlessly craft their own artworks and designs (Brooks et al., 2022)(Gal et al., 2022)(Kawar et al., 2022)(Lugmayr et al., 2022)(Ruiz et al., 2022)(Saharia et al., 2021). Nevertheless, these models exhibit notable memorization capabilities to produce generations highly similar to the training data (Carlini et al., 2023). This resemblance raises growing concerns about copyright infringement, especially when copyrighted data is used for training [12,16,49](Vyas et al., 2023).",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {
                    "[257050406 | Vyas et al. | 2023 | Citations: 94]": "There is a growing concern that learned conditional generative models may output samples that are substantially similar to some copyrighted data $C$ that was in their training set. We give a formal definition of $\\textit{near access-freeness (NAF)}$ and prove bounds on the probability that a model satisfying this definition outputs a sample similar to $C$, even if $C$ is included in its training set. Roughly speaking, a generative model $p$ is $\\textit{$k$-NAF}$ if for every potentially copyrighted data $C$, the output of $p$ diverges by at most $k$-bits from the output of a model $q$ that $\\textit{did not access $C$ at all}$. We also give generative model learning algorithms, which efficiently modify the original generative model learning algorithm in a black box manner, that output generative models with strong bounds on the probability of sampling protected content. Furthermore, we provide promising experiments for both language (transformers) and image (diffusion) generative models, showing minimal degradation in output quality while ensuring strong protections against sampling protected content.",
                    "[14888175 | Sohl-Dickstein et al. | 2015 | Citations: 7030]": "A central problem in machine learning involves modeling complex data-sets using highly flexible families of probability distributions in which learning, sampling, inference, and evaluation are still analytically or computationally tractable. Here, we develop an approach that simultaneously achieves both flexibility and tractability. The essential idea, inspired by non-equilibrium statistical physics, is to systematically and slowly destroy structure in a data distribution through an iterative forward diffusion process. We then learn a reverse diffusion process that restores structure in data, yielding a highly flexible and tractable generative model of the data. This approach allows us to rapidly learn, sample from, and evaluate probabilities in deep generative models with thousands of layers or time steps, as well as to compute conditional and posterior probabilities under the learned model. We additionally release an open source reference implementation of the algorithm.",
                    "[227209335 | Song et al. | 2020 | Citations: 6585]": "Creating noise from data is easy; creating data from noise is generative modeling. We present a stochastic differential equation (SDE) that smoothly transforms a complex data distribution to a known prior distribution by slowly injecting noise, and a corresponding reverse-time SDE that transforms the prior distribution back into the data distribution by slowly removing the noise. Crucially, the reverse-time SDE depends only on the time-dependent gradient field (\\aka, score) of the perturbed data distribution. By leveraging advances in score-based generative modeling, we can accurately estimate these scores with neural networks, and use numerical SDE solvers to generate samples. We show that this framework encapsulates previous approaches in score-based generative modeling and diffusion probabilistic modeling, allowing for new sampling procedures and new modeling capabilities. In particular, we introduce a predictor-corrector framework to correct errors in the evolution of the discretized reverse-time SDE. We also derive an equivalent neural ODE that samples from the same distribution as the SDE, but additionally enables exact likelihood computation, and improved sampling efficiency. In addition, we provide a new way to solve inverse problems with score-based models, as demonstrated with experiments on class-conditional generation, image inpainting, and colorization. Combined with multiple architectural improvements, we achieve record-breaking performance for unconditional image generation on CIFAR-10 with an Inception score of 9.89 and FID of 2.20, a competitive likelihood of 2.99 bits/dim, and demonstrate high fidelity generation of 1024 x 1024 images for the first time from a score-based generative model.",
                    "[243938678 | Saharia et al. | 2021 | Citations: 1647]": "This paper develops a unified framework for image-to-image translation based on conditional diffusion models and evaluates this framework on four challenging image-to-image translation tasks, namely colorization, inpainting, uncropping, and JPEG restoration. Our simple implementation of image-to-image diffusion models outperforms strong GAN and regression baselines on all tasks, without task-specific hyper-parameter tuning, architecture customization, or any auxiliary loss or sophisticated new techniques needed. We uncover the impact of an L2 vs. L1 loss in the denoising diffusion objective on sample diversity, and demonstrate the importance of self-attention in the neural architecture through empirical studies. Importantly, we advocate a unified evaluation protocol based on ImageNet, with human evaluation and sample quality scores (FID, Inception Score, Classification Accuracy of a pre-trained ResNet-50, and Perceptual Distance against original images). We expect this standardized evaluation protocol to play a role in advancing image-to-image translation research. Finally, we show that a generalist, multi-task diffusion model performs as well or better than task-specific specialist counterparts. Check out https://diffusion-palette.github.io/ for an overview of the results and code.",
                    "[245335280 | Rombach et al. | 2021 | Citations: 15768]": "By decomposing the image formation process into a sequential application of denoising autoencoders, diffusion models (DMs) achieve state-of-the-art synthesis results on image data and beyond. Additionally, their formulation allows for a guiding mechanism to control the image generation process without retraining. However, since these models typically operate directly in pixel space, optimization of powerful DMs often consumes hundreds of GPU days and inference is expensive due to sequential evaluations. To enable DM training on limited computational resources while retaining their quality and flexibility, we apply them in the latent space of powerful pretrained autoencoders. In contrast to previous work, training diffusion models on such a representation allows for the first time to reach a near-optimal point between complexity reduction and detail preservation, greatly boosting visual fidelity. By introducing cross-attention layers into the model architecture, we turn diffusion models into powerful and flexible generators for general conditioning inputs such as text or bounding boxes and high-resolution synthesis becomes possible in a convolutional manner. Our latent diffusion models (LDMs) achieve new state of the art scores for image inpainting and class-conditional image synthesis and highly competitive performance on various tasks, including unconditional image generation, text-to-image synthesis, and super-resolution, while significantly reducing computational requirements compared to pixel-based DMs.",
                    "[246240274 | Lugmayr et al. | 2022 | Citations: 1424]": "Free-form inpainting is the task of adding new content to an image in the regions specified by an arbitrary binary mask. Most existing approaches train for a certain distribution of masks, which limits their generalization capabilities to unseen mask types. Furthermore, training with pixel-wise and perceptual losses often leads to simple textural extensions towards the missing areas instead of semantically meaningful generation. In this work, we propose RePaint: A Denoising Diffusion Probabilistic Model (DDPM) based inpainting approach that is applicable to even extreme masks. We employ a pretrained unconditional DDPM as the generative prior. To condition the generation process, we only alter the reverse diffusion iterations by sampling the unmasked regions using the given image infor-mation. Since this technique does not modify or condition the original DDPM network itself, the model produces high-quality and diverse output images for any inpainting form. We validate our method for both faces and general-purpose image inpainting using standard and extreme masks. Re-Paint outperforms state-of-the-art Autoregressive, and GAN approaches for at least five out of six mask distributions. Github Repository: git.io/RePaint",
                    "[248986576 | Saharia et al. | 2022 | Citations: 6075]": "We present Imagen, a text-to-image diffusion model with an unprecedented degree of photorealism and a deep level of language understanding. Imagen builds on the power of large transformer language models in understanding text and hinges on the strength of diffusion models in high-fidelity image generation. Our key discovery is that generic large language models (e.g. T5), pretrained on text-only corpora, are surprisingly effective at encoding text for image synthesis: increasing the size of the language model in Imagen boosts both sample fidelity and image-text alignment much more than increasing the size of the image diffusion model. Imagen achieves a new state-of-the-art FID score of 7.27 on the COCO dataset, without ever training on COCO, and human raters find Imagen samples to be on par with the COCO data itself in image-text alignment. To assess text-to-image models in greater depth, we introduce DrawBench, a comprehensive and challenging benchmark for text-to-image models. With DrawBench, we compare Imagen with recent methods including VQ-GAN+CLIP, Latent Diffusion Models, and DALL-E 2, and find that human raters prefer Imagen over other models in side-by-side comparisons, both in terms of sample quality and image-text alignment. See https://imagen.research.google/ for an overview of the results.",
                    "[251253049 | Gal et al. | 2022 | Citations: 1897]": "Text-to-image models offer unprecedented freedom to guide creation through natural language. Yet, it is unclear how such freedom can be exercised to generate images of specific unique concepts, modify their appearance, or compose them in new roles and novel scenes. In other words, we ask: how can we use language-guided models to turn our cat into a painting, or imagine a new product based on our favorite toy? Here we present a simple approach that allows such creative freedom. Using only 3-5 images of a user-provided concept, like an object or a style, we learn to represent it through new\"words\"in the embedding space of a frozen text-to-image model. These\"words\"can be composed into natural language sentences, guiding personalized creation in an intuitive way. Notably, we find evidence that a single word embedding is sufficient for capturing unique and varied concepts. We compare our approach to a wide range of baselines, and demonstrate that it can more faithfully portray the concepts across a range of applications and tasks. Our code, data and new words will be available at: https://textual-inversion.github.io",
                    "[251800180 | Ruiz et al. | 2022 | Citations: 2891]": "Large text-to-image models achieved a remarkable leap in the evolution of AI, enabling high-quality and diverse synthesis of images from a given text prompt. However, these models lack the ability to mimic the appearance of subjects in a given reference set and synthesize novel renditions of them in different contexts. In this work, we present a new approach for \u201cpersonalization\u201d of text-to-image diffusion models. Given as input just a few images of a subject, we fine-tune a pretrained text-to-image model such that it learns to bind a unique identifier with that specific subject. Once the subject is embedded in the output domain of the model, the unique identifier can be used to synthesize novel photorealistic images of the subject contextualized in different scenes. By leveraging the semantic prior embedded in the model with a new autogenous class-specific prior preservation loss, our technique enables synthesizing the subject in diverse scenes, poses, views and lighting conditions that do not appear in the reference images. We apply our technique to several previously-unassailable tasks, including subject recontextualization, text-guided view synthesis, and artistic rendering, all while preserving the subject's key features. We also provide a new dataset and evaluation protocol for this new task of subject-driven generation. Project page: https://dreambooth.github.io/",
                    "[252918469 | Kawar et al. | 2022 | Citations: 1104]": "Text-conditioned image editing has recently attracted considerable interest. However, most methods are currently limited to one of the following: specific editing types (e.g., object overlay, style transfer), synthetically generated images, or requiring multiple input images of a common object. In this paper we demonstrate, for the very first time, the ability to apply complex (e.g., non-rigid) text-based semantic edits to a single real image. For example, we can change the posture and composition of one or multiple objects inside an image, while preserving its original characteristics. Our method can make a standing dog sit down, cause a bird to spread its wings, etc. \u2013 each within its single high-resolution user-provided natural image. Contrary to previous work, our proposed method requires only a single input image and a target text (the desired edit). It operates on real images, and does not require any additional inputs (such as image masks or additional views of the object). Our method, called Imagic, leverages a pre-trained text-to-image diffusion model for this task. It produces a text embedding that aligns with both the input image and the target text, while fine-tuning the diffusion model to capture the image-specific appearance. We demonstrate the quality and versatility of Imagic on numerous inputs from various domains, showcasing a plethora of high quality complex semantic image edits, all within a single unified framework. To better assess performance, we introduce TEdBench, a highly challenging image editing benchmark. We conduct a user study, whose findings show that human raters prefer Imagic to previous leading editing methods on TEdBench.",
                    "[253581213 | Brooks et al. | 2022 | Citations: 1833]": "We propose a method for editing images from human instructions: given an input image and a written instruction that tells the model what to do, our model follows these instructions to edit the image. To obtain training data for this problem, we combine the knowledge of two large pretrained models\u2014a language model (GPT-3) and a text-to-image model (Stable Diffusion)\u2014to generate a large dataset of image editing examples. Our conditional diffusion model, InstructPix2Pix, is trained on our generated data, and generalizes to real images and user-written instructions at inference time. Since it performs edits in the forward pass and does not require per-example fine-tuning or inversion, our model edits images quickly, in a matter of seconds. We show compelling editing results for a diverse collection of input images and written instructions.",
                    "[256389993 | Carlini et al. | 2023 | Citations: 617]": "Image diffusion models such as DALL-E 2, Imagen, and Stable Diffusion have attracted significant attention due to their ability to generate high-quality synthetic images. In this work, we show that diffusion models memorize individual images from their training data and emit them at generation time. With a generate-and-filter pipeline, we extract over a thousand training examples from state-of-the-art models, ranging from photographs of individual people to trademarked company logos. We also train hundreds of diffusion models in various settings to analyze how different modeling and data decisions affect privacy. Overall, our results show that diffusion models are much less private than prior generative models such as GANs, and that mitigating these vulnerabilities may require new advances in privacy-preserving training."
                },
                "metadata": [
                    {
                        "section_title": "Introduction",
                        "pdf_hash": "",
                        "start": 0,
                        "end": 542,
                        "sentence_offsets": [
                            {
                                "start": 0,
                                "end": 269
                            },
                            {
                                "start": 270,
                                "end": 402
                            },
                            {
                                "start": 403,
                                "end": 542
                            }
                        ],
                        "ref_mentions": [
                            "14888175",
                            "227209335",
                            "245335280",
                            "248986576",
                            "253581213",
                            "251253049",
                            "252918469",
                            "246240274",
                            "251800180",
                            "243938678",
                            "256389993",
                            "257050406"
                        ],
                        "quote": "In recent years, the advancement of large generative models [17,(Sohl-Dickstein et al., 2015)(Song et al., 2020) has revolutionized high-quality image synthesis [34,(Rombach et al., 2021)(Saharia et al., 2022), paving the way for commercial applications that enable the public to effortlessly craft their own artworks and designs (Brooks et al., 2022)(Gal et al., 2022)(Kawar et al., 2022)(Lugmayr et al., 2022)(Ruiz et al., 2022)(Saharia et al., 2021). Nevertheless, these models exhibit notable memorization capabilities to produce generations highly similar to the training data (Carlini et al., 2023). This resemblance raises growing concerns about copyright infringement, especially when copyrighted data is used for training [12,16,49](Vyas et al., 2023)."
                    }
                ]
            },
            {
                "idx": 6,
                "key": "[266900037 | Wang et al. | 2024 | Citations: 32]",
                "snippets": "In a copyright infringement attack, the attacker, who is the copyright owner of some creations (such as images, poems, etc.), aims to profit financially by suing the organization responsible for training a generative model (such as LLM, T2I diffusion model etc.) for copyright infringement. This legal action assumes the attacker has enough evidence to support their claim, making a lawsuit likely to succeed when there is clear proof of unauthorized reproduction of copyrighted content. A real-world example illustrating this scenario is the lawsuit filed by Getty Images against the AI art generator Stable Diffusion in the United States for copyright infringement (Vincent, 2023).",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {},
                "metadata": [
                    {
                        "section_title": "Copyright Infringement Attack",
                        "pdf_hash": "",
                        "start": 40,
                        "end": 723,
                        "sentence_offsets": [
                            {
                                "start": 40,
                                "end": 330
                            },
                            {
                                "start": 331,
                                "end": 527
                            },
                            {
                                "start": 528,
                                "end": 723
                            }
                        ],
                        "ref_mentions": [],
                        "quote": "In a copyright infringement attack, the attacker, who is the copyright owner of some creations (such as images, poems, etc.), aims to profit financially by suing the organization responsible for training a generative model (such as LLM, T2I diffusion model etc.) for copyright infringement. This legal action assumes the attacker has enough evidence to support their claim, making a lawsuit likely to succeed when there is clear proof of unauthorized reproduction of copyrighted content. A real-world example illustrating this scenario is the lawsuit filed by Getty Images against the AI art generator Stable Diffusion in the United States for copyright infringement (Vincent, 2023)."
                    }
                ]
            },
            {
                "idx": 7,
                "key": "[267400526 | Zhuang | 2024 | Citations: 2]",
                "snippets": "Stable Diffusion can quickly generate high-quality images based on simple text descriptions, thanks to its training on billions of sample images collected online. However, not all of these sample images fall within the public domain; some are even protected by copyright. As one can imagine, the authors and photographers of these copyrighted images are not pleased with the actions of Stable Diffusion.\n\nStable Diffusion has faced multiple lawsuits due to copyright issues. Artists and photographers have initiated collective lawsuits against this technology, and one of the world's leading image suppliers, Getty Images Holdings Inc., has also filed a similar case. According to Getty's (2023) complaint, \"Stability AI has reproduced over 12 million Getty Images, along with their related captions and metadata, without permission or compensation.\"...After exploring how artificial intelligence (AI)-generated platforms work, learn what processes may involve human artist infringement in the course of their work. So let's talk about where these workflows infringe on human artists' copyrighted works -Artificial intelligence (AI)-generated content raises concerns about copyright infringement of human artist works in several ways.\n\nThe researchers summarized three aspects of human artist infringement: model training using only copyrighted works without permission. Take Stable Diffusion, which has been involved in many copyright disputes. It uses the image database containing hundreds of millions-LAION-5B (Kaixin Zhu, & Yiqun Zhang. ( 2023)) as the source of training data, and it does not need the consent of the copyright owner [5].\n\nSecond, there is a lot of plagiarism and adaptation of copyrighted works in AI-generated images. In a case in which an artist sued an AI-generated platform, the painter Erin Hanson provided strong evidence that the AI-generated images plagiarized his work. And there are many such lawsuits now, not a few. Third, the images generated by the AI generation platform, such as Stable Diffusion, if works infringe copyright works, do not have any watermarks or source marks, making it even more difficult for copyright owners to accept.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {},
                "metadata": [
                    {
                        "section_title": "Introduction",
                        "pdf_hash": "",
                        "start": 653,
                        "end": 1503,
                        "sentence_offsets": [
                            {
                                "start": 654,
                                "end": 773
                            },
                            {
                                "start": 776,
                                "end": 938
                            },
                            {
                                "start": 939,
                                "end": 1047
                            },
                            {
                                "start": 1048,
                                "end": 1179
                            },
                            {
                                "start": 1182,
                                "end": 1251
                            },
                            {
                                "start": 1252,
                                "end": 1444
                            },
                            {
                                "start": 1445,
                                "end": 1627
                            }
                        ],
                        "ref_mentions": [],
                        "quote": "Stable Diffusion can quickly generate high-quality images based on simple text descriptions, thanks to its training on billions of sample images collected online. However, not all of these sample images fall within the public domain; some are even protected by copyright. As one can imagine, the authors and photographers of these copyrighted images are not pleased with the actions of Stable Diffusion.\n\nStable Diffusion has faced multiple lawsuits due to copyright issues. Artists and photographers have initiated collective lawsuits against this technology, and one of the world's leading image suppliers, Getty Images Holdings Inc., has also filed a similar case. According to Getty's (2023) complaint, \"Stability AI has reproduced over 12 million Getty Images, along with their related captions and metadata, without permission or compensation.\""
                    },
                    {
                        "section_title": "From what aspects does Stable Diffusion infringe the copyright of human artists",
                        "pdf_hash": "",
                        "start": 0,
                        "end": 1325,
                        "sentence_offsets": [
                            {
                                "start": 0,
                                "end": 162
                            },
                            {
                                "start": 163,
                                "end": 381
                            },
                            {
                                "start": 384,
                                "end": 518
                            },
                            {
                                "start": 519,
                                "end": 593
                            },
                            {
                                "start": 594,
                                "end": 689
                            },
                            {
                                "start": 690,
                                "end": 791
                            },
                            {
                                "start": 794,
                                "end": 890
                            },
                            {
                                "start": 891,
                                "end": 1050
                            },
                            {
                                "start": 1051,
                                "end": 1099
                            },
                            {
                                "start": 1100,
                                "end": 1325
                            }
                        ],
                        "ref_mentions": [],
                        "quote": "After exploring how artificial intelligence (AI)-generated platforms work, learn what processes may involve human artist infringement in the course of their work. So let's talk about where these workflows infringe on human artists' copyrighted works -Artificial intelligence (AI)-generated content raises concerns about copyright infringement of human artist works in several ways.\n\nThe researchers summarized three aspects of human artist infringement: model training using only copyrighted works without permission. Take Stable Diffusion, which has been involved in many copyright disputes. It uses the image database containing hundreds of millions-LAION-5B (Kaixin Zhu, & Yiqun Zhang. ( 2023)) as the source of training data, and it does not need the consent of the copyright owner [5].\n\nSecond, there is a lot of plagiarism and adaptation of copyrighted works in AI-generated images. In a case in which an artist sued an AI-generated platform, the painter Erin Hanson provided strong evidence that the AI-generated images plagiarized his work. And there are many such lawsuits now, not a few. Third, the images generated by the AI generation platform, such as Stable Diffusion, if works infringe copyright works, do not have any watermarks or source marks, making it even more difficult for copyright owners to accept."
                    }
                ]
            },
            {
                "idx": 8,
                "key": "[267412857 | Ren et al. | 2024 | Citations: 42]",
                "snippets": "For the source data owner, which refers to the party or individual who owns the originality of image works, their data can be intentionally or unintentionally collected by model trainers as training samples to construct DGMs as introduced above. For example, recent studies [16]134] have demonstrated that popular DGMs are highly possible to completely replicate their training data samples, which is called memorization. \n\nThe possibility of data replication may severely offend the ownership of the original data samples. Moreover, the development of fine-tuning strategies such as DreamBooth can greatly improve the efficiency for unauthorized parties to directly edit or modify the source data to obtain new samples, which also severely infringes the copyright of the original works.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {},
                "metadata": [
                    {
                        "section_title": "Copyright Issues in Image Generation",
                        "pdf_hash": "",
                        "start": 345,
                        "end": 1132,
                        "sentence_offsets": [
                            {
                                "start": 345,
                                "end": 590
                            },
                            {
                                "start": 591,
                                "end": 766
                            },
                            {
                                "start": 769,
                                "end": 868
                            },
                            {
                                "start": 869,
                                "end": 1132
                            }
                        ],
                        "ref_mentions": [],
                        "quote": "For the source data owner, which refers to the party or individual who owns the originality of image works, their data can be intentionally or unintentionally collected by model trainers as training samples to construct DGMs as introduced above. For example, recent studies [16]134] have demonstrated that popular DGMs are highly possible to completely replicate their training data samples, which is called memorization. \n\nThe possibility of data replication may severely offend the ownership of the original data samples. Moreover, the development of fine-tuning strategies such as DreamBooth can greatly improve the efficiency for unauthorized parties to directly edit or modify the source data to obtain new samples, which also severely infringes the copyright of the original works."
                    }
                ]
            },
            {
                "idx": 9,
                "key": "[268513090 | Wu et al. | 2024 | Citations: 6]",
                "snippets": "Recent years have witnessed the advancement of Diffusion Models (DMs) in computer vision.These models demonstrate exceptional capabilities across a diverse array of tasks, including image editing [14], and video editing [35], among others.Particularly, the emergence of few-shot generation techniques, exemplified by Dreambooth (Ruiz et al., 2022) and swiftly capturing the style or primary objects by fine-tuning a pretrained model on a small set of images.This process enables efficient and high-quality art imitation and art style transfer by utilizing the fine-tuned model.\n\nHowever, these advanced few-shot generation techniques also spark widespread concerns regarding the protection of copyright for human artworks and individual photographs.There is a growing fear that parties may exploit the generative capabilities of DMs to create and profit from derivative artworks based on existing artists' works, without obtaining proper authorization [6,19].Concurrently, concerns arise regarding the creation of fabricated images of individuals without their consent [32].All of these collectively pose a serious threat to the security of personal data and intellectual property rights.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {
                    "[251800180 | Ruiz et al. | 2022 | Citations: 2891]": "Large text-to-image models achieved a remarkable leap in the evolution of AI, enabling high-quality and diverse synthesis of images from a given text prompt. However, these models lack the ability to mimic the appearance of subjects in a given reference set and synthesize novel renditions of them in different contexts. In this work, we present a new approach for \u201cpersonalization\u201d of text-to-image diffusion models. Given as input just a few images of a subject, we fine-tune a pretrained text-to-image model such that it learns to bind a unique identifier with that specific subject. Once the subject is embedded in the output domain of the model, the unique identifier can be used to synthesize novel photorealistic images of the subject contextualized in different scenes. By leveraging the semantic prior embedded in the model with a new autogenous class-specific prior preservation loss, our technique enables synthesizing the subject in diverse scenes, poses, views and lighting conditions that do not appear in the reference images. We apply our technique to several previously-unassailable tasks, including subject recontextualization, text-guided view synthesis, and artistic rendering, all while preserving the subject's key features. We also provide a new dataset and evaluation protocol for this new task of subject-driven generation. Project page: https://dreambooth.github.io/"
                },
                "metadata": [
                    {
                        "section_title": "Introduction",
                        "pdf_hash": "",
                        "start": 0,
                        "end": 1173,
                        "sentence_offsets": [
                            {
                                "start": 0,
                                "end": 89
                            },
                            {
                                "start": 89,
                                "end": 239
                            },
                            {
                                "start": 239,
                                "end": 443
                            },
                            {
                                "start": 443,
                                "end": 562
                            },
                            {
                                "start": 564,
                                "end": 734
                            },
                            {
                                "start": 734,
                                "end": 944
                            },
                            {
                                "start": 944,
                                "end": 1059
                            },
                            {
                                "start": 1059,
                                "end": 1173
                            }
                        ],
                        "ref_mentions": [
                            "251800180"
                        ],
                        "quote": "Recent years have witnessed the advancement of Diffusion Models (DMs) in computer vision.These models demonstrate exceptional capabilities across a diverse array of tasks, including image editing [14], and video editing [35], among others.Particularly, the emergence of few-shot generation techniques, exemplified by Dreambooth (Ruiz et al., 2022) and swiftly capturing the style or primary objects by fine-tuning a pretrained model on a small set of images.This process enables efficient and high-quality art imitation and art style transfer by utilizing the fine-tuned model.\n\nHowever, these advanced few-shot generation techniques also spark widespread concerns regarding the protection of copyright for human artworks and individual photographs.There is a growing fear that parties may exploit the generative capabilities of DMs to create and profit from derivative artworks based on existing artists' works, without obtaining proper authorization [6,19].Concurrently, concerns arise regarding the creation of fabricated images of individuals without their consent [32].All of these collectively pose a serious threat to the security of personal data and intellectual property rights."
                    }
                ]
            },
            {
                "idx": 10,
                "key": "[268532352 | Ma et al. | 2024 | Citations: 2]",
                "snippets": "Copyright law confers upon creators the exclusive rights to reproduce, distribute, and monetize their creative works. However, recent progress in text-to-image generation has introduced formidable challenges to copyright enforcement. These technologies enable the unauthorized learning and replication of copyrighted content, artistic creations, and likenesses, leading to the proliferation of unregulated content. Notably, models like stable diffusion, which excel in text-to-image synthesis, heighten the risk of copyright infringement and unauthorized distribution....Text-to-image generative models have recently emerged as a significant topic in computer vision, demonstrating remarkable results in the area of generative modeling (Goodfellow et al., 2021)(Rombach et al., 2021).These models bridge the gap between language and visual contents by generating realistic images from textual descriptions.However, rapid advancements in text-to-image generation techniques have raised concerns about copyright protection, particularly unauthorized reproduction of content, artistic creations, and portraits [4].A specific concern arises from the use of Stable Diffusion (SD), a state-of-the-art text-conditional latent diffusion model, which has sparked global discussions on copyright.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {
                    "[1033682 | Goodfellow et al. | 2021 | Citations: 30155]": "Generative Adversarial Networks (GANs) are a type of deep learning techniques that have shown remarkable success in generating realistic images, videos, and other types of data. This paper provides a comprehensive guide to GANs, covering their architecture, loss functions, training methods, applications, evaluation metrics, challenges, and future directions. We begin with an introduction to GANs and their historical development, followed by a review of the background and related work. We then provide a detailed overview of the GAN architecture, including the generator and discriminator networks, and discuss the key design choices and variations. Next, we review the loss functions utilized in GANs, including the original minimax objective, as well as more recent approaches s.a. Wasserstein distance and gradient penalty. We then delve into the training of GANs, discussing common techniques s.a. alternating optimization, minibatch discrimination, and spectral normalization. We also provide a survey of the various applications of GANs across domains. In addition, we review the evaluation metrics utilized to assess the diversity and quality of GAN-produced data. Furthermore, we discuss the challenges and open issues in GANs, including mode collapse, training instability, and ethical considerations. Finally, we provide a glimpse into the future directions of GAN research, including improving scalability, developing new architectures, incorporating domain knowledge, and exploring new applications. Overall, this paper serves as a comprehensive guide to GANs, providing both theoretical and practical insights for researchers and practitioners in the field.",
                    "[245335280 | Rombach et al. | 2021 | Citations: 15768]": "By decomposing the image formation process into a sequential application of denoising autoencoders, diffusion models (DMs) achieve state-of-the-art synthesis results on image data and beyond. Additionally, their formulation allows for a guiding mechanism to control the image generation process without retraining. However, since these models typically operate directly in pixel space, optimization of powerful DMs often consumes hundreds of GPU days and inference is expensive due to sequential evaluations. To enable DM training on limited computational resources while retaining their quality and flexibility, we apply them in the latent space of powerful pretrained autoencoders. In contrast to previous work, training diffusion models on such a representation allows for the first time to reach a near-optimal point between complexity reduction and detail preservation, greatly boosting visual fidelity. By introducing cross-attention layers into the model architecture, we turn diffusion models into powerful and flexible generators for general conditioning inputs such as text or bounding boxes and high-resolution synthesis becomes possible in a convolutional manner. Our latent diffusion models (LDMs) achieve new state of the art scores for image inpainting and class-conditional image synthesis and highly competitive performance on various tasks, including unconditional image generation, text-to-image synthesis, and super-resolution, while significantly reducing computational requirements compared to pixel-based DMs."
                },
                "metadata": [
                    {
                        "quote": "Copyright law confers upon creators the exclusive rights to reproduce, distribute, and monetize their creative works. However, recent progress in text-to-image generation has introduced formidable challenges to copyright enforcement. These technologies enable the unauthorized learning and replication of copyrighted content, artistic creations, and likenesses, leading to the proliferation of unregulated content. Notably, models like stable diffusion, which excel in text-to-image synthesis, heighten the risk of copyright infringement and unauthorized distribution",
                        "pdf_hash": "",
                        "section_title": "abstract"
                    },
                    {
                        "section_title": "Introduction",
                        "pdf_hash": "",
                        "start": 0,
                        "end": 677,
                        "sentence_offsets": [
                            {
                                "start": 0,
                                "end": 173
                            },
                            {
                                "start": 173,
                                "end": 295
                            },
                            {
                                "start": 295,
                                "end": 500
                            },
                            {
                                "start": 500,
                                "end": 675
                            }
                        ],
                        "ref_mentions": [
                            "1033682",
                            "245335280"
                        ],
                        "quote": ".Text-to-image generative models have recently emerged as a significant topic in computer vision, demonstrating remarkable results in the area of generative modeling (Goodfellow et al., 2021)(Rombach et al., 2021).These models bridge the gap between language and visual contents by generating realistic images from textual descriptions.However, rapid advancements in text-to-image generation techniques have raised concerns about copyright protection, particularly unauthorized reproduction of content, artistic creations, and portraits [4].A specific concern arises from the use of Stable Diffusion (SD), a state-of-the-art text-conditional latent diffusion model, which has sparked global discussions on copyright."
                    }
                ]
            },
            {
                "idx": 11,
                "key": "[269033217 | Lu et al. | 2024 | Citations: 8]",
                "snippets": "Copyright infringement may occur when a generative model produces samples substantially similar to some copyrighted data that it had access to during the training phase. The notion of access usually refers to including copyrighted samples directly in the training dataset, which one may inspect to identify an infringement. We argue that such visual auditing largely overlooks a concealed copyright infringement, where one constructs a disguise that looks drastically different from the copyrighted sample yet still induces the effect of training Latent Diffusion Models on it. Such disguises only require indirect access to the copyrighted material and cannot be visually distinguished, thus easily circumventing the current auditing tools.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {},
                "metadata": [
                    {
                        "quote": "Copyright infringement may occur when a generative model produces samples substantially similar to some copyrighted data that it had access to during the training phase. The notion of access usually refers to including copyrighted samples directly in the training dataset, which one may inspect to identify an infringement. We argue that such visual auditing largely overlooks a concealed copyright infringement, where one constructs a disguise that looks drastically different from the copyrighted sample yet still induces the effect of training Latent Diffusion Models on it. Such disguises only require indirect access to the copyrighted material and cannot be visually distinguished, thus easily circumventing the current auditing tools.",
                        "pdf_hash": "",
                        "section_title": "abstract"
                    }
                ]
            },
            {
                "idx": 12,
                "key": "[269137659 | Moayeri et al. | 2024 | Citations: 4]",
                "snippets": "Recent text-to-image generative models such as Stable Diffusion are extremely adept at mimicking and generating copyrighted content, raising concerns amongst artists that their unique styles may be improperly copied. Understanding how generative models copy\"artistic style\"is more complex than duplicating a single image, as style is comprised by a set of elements (or signature) that frequently co-occurs across a body of work, where each individual work may vary significantly.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {},
                "metadata": [
                    {
                        "quote": "Recent text-to-image generative models such as Stable Diffusion are extremely adept at mimicking and generating copyrighted content, raising concerns amongst artists that their unique styles may be improperly copied. Understanding how generative models copy\"artistic style\"is more complex than duplicating a single image, as style is comprised by a set of elements (or signature) that frequently co-occurs across a body of work, where each individual work may vary significantly.",
                        "pdf_hash": "",
                        "section_title": "abstract"
                    }
                ]
            },
            {
                "idx": 13,
                "key": "[270258236 | Chiba-Okabe et al. | 2024 | Citations: 1]",
                "snippets": "Generative models are trained on massive datasets that often contain copyrighted works and are capable of producing outputs closely resembling their training data, potentially resulting in violation of exclusive rights of copyright owners. Although some methods have been found effective to some extent, significant risks of copyright infringement remains.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {
                    "[10463592 | Gillen | 2012 | Citations: 142]": "From April next year public health commissioning responsibilities will be transferred from the NHS to local authorities, and the changeover will herald radical changes in public health services. Under plans announced in January, councils in England will be given a ring-fenced share of \u00a35.2 billion to improve public health, with success judged against an outcomes framework of 66 health measures. These include reducing the number of people who smoke and the number of people who die from heart disease. Nurses\u2019 contribution to public health will be vital in helping local authorities score well. Whether they will be employed by councils directly is not yet clear, but for some specialist practitioners the changes will mean moving out of NHS employment."
                },
                "metadata": [
                    {
                        "section_title": "Introduction",
                        "pdf_hash": "",
                        "start": 355,
                        "end": 711,
                        "sentence_offsets": [
                            {
                                "start": 299,
                                "end": 425
                            },
                            {
                                "start": 426,
                                "end": 667
                            },
                            {
                                "start": 668,
                                "end": 797
                            }
                        ],
                        "ref_mentions": [
                            "10463592",
                            "258515384"
                        ],
                        "quote": "Generative models are trained on massive datasets that often contain copyrighted works and are capable of producing outputs closely resembling their training data, potentially resulting in violation of exclusive rights of copyright owners. Although some methods have been found effective to some extent, significant risks of copyright infringement remains."
                    }
                ]
            },
            {
                "idx": 14,
                "key": "[270620122 | He et al. | 2024 | Citations: 12]",
                "snippets": "Recent studies show that image and video generation models can be prompted to reproduce copyrighted content from their training data, raising serious legal concerns about copyright infringement. Copyrighted characters (e.g., Mario, Batman) present a significant challenge: at least one lawsuit has already awarded damages based on the generation of such characters. Consequently, commercial services like DALL-E have started deploying interventions.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {},
                "metadata": [
                    {
                        "section_title": "abstract",
                        "pdf_hash": "",
                        "start": 0,
                        "end": 449,
                        "sentence_offsets": [],
                        "ref_mentions": [],
                        "quote": "Recent studies show that image and video generation models can be prompted to reproduce copyrighted content from their training data, raising serious legal concerns about copyright infringement. Copyrighted characters (e.g., Mario, Batman) present a significant challenge: at least one lawsuit has already awarded damages based on the generation of such characters. Consequently, commercial services like DALL-E have started deploying interventions."
                    }
                ]
            },
            {
                "idx": 15,
                "key": "[272146279 | Shi et al. | 2024 | Citations: 1]",
                "snippets": "These models utilize extensive training data that may include copyrighted works, which they are sometimes capable of memorizing (Carlini et al. 2023). This ability can result in the pro- duction of images that closely resemble protected content (See in Figure 1), posing significant challenges to copyright protection (Dogoulis et al., 2023). Recent legal cases, such as those involving Stable Diffusion (Rombach et al., 2021) and Midjourney(Mansour 2023), highlight concerns over the use of copyrighted data in AI training, where the models potentially infringe on the rights of numerous artists. These cases highlight a growing concern: Could the high-quality content synthesized by these generative AIs be excessively similar to copyrighted training data, potentially violating the rights of copyright holders?",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {
                    "[245335280 | Rombach et al. | 2021 | Citations: 15768]": "By decomposing the image formation process into a sequential application of denoising autoencoders, diffusion models (DMs) achieve state-of-the-art synthesis results on image data and beyond. Additionally, their formulation allows for a guiding mechanism to control the image generation process without retraining. However, since these models typically operate directly in pixel space, optimization of powerful DMs often consumes hundreds of GPU days and inference is expensive due to sequential evaluations. To enable DM training on limited computational resources while retaining their quality and flexibility, we apply them in the latent space of powerful pretrained autoencoders. In contrast to previous work, training diffusion models on such a representation allows for the first time to reach a near-optimal point between complexity reduction and detail preservation, greatly boosting visual fidelity. By introducing cross-attention layers into the model architecture, we turn diffusion models into powerful and flexible generators for general conditioning inputs such as text or bounding boxes and high-resolution synthesis becomes possible in a convolutional manner. Our latent diffusion models (LDMs) achieve new state of the art scores for image inpainting and class-conditional image synthesis and highly competitive performance on various tasks, including unconditional image generation, text-to-image synthesis, and super-resolution, while significantly reducing computational requirements compared to pixel-based DMs.",
                    "[258297834 | Dogoulis et al. | 2023 | Citations: 19]": "New advancements for the detection of synthetic images are critical for fighting disinformation, as the capabilities of generative AI models continuously evolve and can lead to hyper-realistic synthetic imagery at unprecedented scale and speed. In this paper, we focus on the challenge of generalizing across different concept classes, e.g., when training a detector on human faces and testing on synthetic animal images \u2013 highlighting the ineffectiveness of existing approaches that randomly sample generated images to train their models. By contrast, we propose an approach based on the premise that the robustness of the detector can be enhanced by training it on realistic synthetic images that are selected based on their quality scores according to a probabilistic quality estimation model. We demonstrate the effectiveness of the proposed approach by conducting experiments with generated images from two seminal architectures, StyleGAN2 and Latent Diffusion, and using three different concepts for each, so as to measure the cross-concept generalization ability. Our results show that our quality-based sampling method leads to higher detection performance for nearly all concepts, improving the overall effectiveness of the synthetic image detectors."
                },
                "metadata": [
                    {
                        "section_title": "Introduction",
                        "pdf_hash": "",
                        "start": 844,
                        "end": 1659,
                        "sentence_offsets": [
                            {
                                "start": 844,
                                "end": 994
                            },
                            {
                                "start": 995,
                                "end": 1188
                            },
                            {
                                "start": 1189,
                                "end": 1443
                            },
                            {
                                "start": 1444,
                                "end": 1659
                            }
                        ],
                        "ref_mentions": [
                            "258297834",
                            "245335280"
                        ],
                        "quote": "These models utilize extensive training data that may include copyrighted works, which they are sometimes capable of memorizing (Carlini et al. 2023). This ability can result in the pro- duction of images that closely resemble protected content (See in Figure 1), posing significant challenges to copyright protection (Dogoulis et al., 2023). Recent legal cases, such as those involving Stable Diffusion (Rombach et al., 2021) and Midjourney(Mansour 2023), highlight concerns over the use of copyrighted data in AI training, where the models potentially infringe on the rights of numerous artists. These cases highlight a growing concern: Could the high-quality content synthesized by these generative AIs be excessively similar to copyrighted training data, potentially violating the rights of copyright holders?"
                    }
                ]
            },
            {
                "idx": 16,
                "key": "[273023255 | Chiba-Okabe | 2024 | Citations: 2]",
                "snippets": "In cases where generative AI is used to create content, it is typically provided as a service by the AI developer and utilized by a user, and in such situations, both the user and the AI developer may potentially be considered direct or indirect infringers, depending on the specific circumstances [28]. Copyright infringement may occur even without the user or developer intending to replicate the copyrighted material or to create something similar [28]. \n\nDue to the uninterpretability of generative models, it can be challenging to provide direct evidence of factual copying for AI-generated content. In most cases, access and similarity will likely serve as the primary evidence of infringement, much like in traditional copyright disputes [28]. While the creative processes of generative models differ markedly from human creativity, it appears that similar evidentiary principles are likely to apply even when generative AI is involved in the creative processes. At a minimum, it is generally recognized that some form of access to the original work, which may be demonstrated by its inclusion in the training data, is necessary to establish copying, even for generative AI outputs [28,29,36]. Moreover, studies have shown that exposure to certain types of content during training can lead to a high likelihood of these models reproducing or closely imitating that content (Carlini et al., 2020)(Carlini et al., 2023)[39], suggesting that inclusion of specific data correlates with a greater probability of producing similar outputs through copying. This echoes the evidentiary framework applicable to traditional copyright cases, where greater similarity between works strengthens the inference of access and copying. \n\nA distinctive feature of generative models, setting them apart from human artists or traditional creative tools, is their capacity to be trained on immense and diverse datasets, which are often created by scraping content from the Internet. Furthermore, direct inclusion of copyrighted material in the training data is not even necessary for latent diffusion models to produce images similar to copyrighted works, as these models can reproduce content from other images that retain similar latent information [40], which exacerbates the problem. These characteristics suggest a high likelihood that generative models have what can be construed as \"access\" to copyrighted works.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {
                    "[229156229 | Carlini et al. | 2020 | Citations: 1950]": "It has become common to publish large (billion parameter) language models that have been trained on private datasets. This paper demonstrates that in such settings, an adversary can perform a training data extraction attack to recover individual training examples by querying the language model. \nWe demonstrate our attack on GPT-2, a language model trained on scrapes of the public Internet, and are able to extract hundreds of verbatim text sequences from the model's training data. These extracted examples include (public) personally identifiable information (names, phone numbers, and email addresses), IRC conversations, code, and 128-bit UUIDs. Our attack is possible even though each of the above sequences are included in just one document in the training data. \nWe comprehensively evaluate our extraction attack to understand the factors that contribute to its success. For example, we find that larger models are more vulnerable than smaller models. We conclude by drawing lessons and discussing possible safeguards for training large language models.",
                    "[256389993 | Carlini et al. | 2023 | Citations: 617]": "Image diffusion models such as DALL-E 2, Imagen, and Stable Diffusion have attracted significant attention due to their ability to generate high-quality synthetic images. In this work, we show that diffusion models memorize individual images from their training data and emit them at generation time. With a generate-and-filter pipeline, we extract over a thousand training examples from state-of-the-art models, ranging from photographs of individual people to trademarked company logos. We also train hundreds of diffusion models in various settings to analyze how different modeling and data decisions affect privacy. Overall, our results show that diffusion models are much less private than prior generative models such as GANs, and that mitigating these vulnerabilities may require new advances in privacy-preserving training."
                },
                "metadata": [
                    {
                        "section_title": "Risks of Copyright Infringement in Generative AI",
                        "pdf_hash": "",
                        "start": 0,
                        "end": 2369,
                        "sentence_offsets": [
                            {
                                "start": 0,
                                "end": 303
                            },
                            {
                                "start": 304,
                                "end": 456
                            },
                            {
                                "start": 459,
                                "end": 604
                            },
                            {
                                "start": 605,
                                "end": 750
                            },
                            {
                                "start": 751,
                                "end": 969
                            },
                            {
                                "start": 970,
                                "end": 1200
                            },
                            {
                                "start": 1201,
                                "end": 1520
                            },
                            {
                                "start": 1521,
                                "end": 1689
                            },
                            {
                                "start": 1692,
                                "end": 1932
                            },
                            {
                                "start": 1933,
                                "end": 2237
                            },
                            {
                                "start": 2238,
                                "end": 2369
                            }
                        ],
                        "ref_mentions": [
                            "229156229",
                            "256389993"
                        ],
                        "quote": "In cases where generative AI is used to create content, it is typically provided as a service by the AI developer and utilized by a user, and in such situations, both the user and the AI developer may potentially be considered direct or indirect infringers, depending on the specific circumstances [28]. Copyright infringement may occur even without the user or developer intending to replicate the copyrighted material or to create something similar [28]. \n\nDue to the uninterpretability of generative models, it can be challenging to provide direct evidence of factual copying for AI-generated content. In most cases, access and similarity will likely serve as the primary evidence of infringement, much like in traditional copyright disputes [28]. While the creative processes of generative models differ markedly from human creativity, it appears that similar evidentiary principles are likely to apply even when generative AI is involved in the creative processes. At a minimum, it is generally recognized that some form of access to the original work, which may be demonstrated by its inclusion in the training data, is necessary to establish copying, even for generative AI outputs [28,29,36]. Moreover, studies have shown that exposure to certain types of content during training can lead to a high likelihood of these models reproducing or closely imitating that content (Carlini et al., 2020)(Carlini et al., 2023)[39], suggesting that inclusion of specific data correlates with a greater probability of producing similar outputs through copying. This echoes the evidentiary framework applicable to traditional copyright cases, where greater similarity between works strengthens the inference of access and copying. \n\nA distinctive feature of generative models, setting them apart from human artists or traditional creative tools, is their capacity to be trained on immense and diverse datasets, which are often created by scraping content from the Internet. Furthermore, direct inclusion of copyrighted material in the training data is not even necessary for latent diffusion models to produce images similar to copyrighted works, as these models can reproduce content from other images that retain similar latent information [40], which exacerbates the problem. These characteristics suggest a high likelihood that generative models have what can be construed as \"access\" to copyrighted works."
                    }
                ]
            },
            {
                "idx": 17,
                "key": "[273654195 | Shi et al. | 2024 | Citations: 1]",
                "snippets": "The widespread deployment of such generative art models has brought about significant challenges concerning the risk for copyright infringement. The image generative models require large amount of training data, and it's impractical to only use noncopyrighted content to train the models. The model holder wants to leverage the high quality copyrighted data to learn the style and content and output high quality generated data. However, the generative models may generate some images that are very similar to the copyrighted training data, which may lead to copyright infringement [28].",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {},
                "metadata": [
                    {
                        "section_title": "Introduction",
                        "pdf_hash": "",
                        "start": 701,
                        "end": 1288,
                        "sentence_offsets": [
                            {
                                "start": 701,
                                "end": 845
                            },
                            {
                                "start": 846,
                                "end": 989
                            },
                            {
                                "start": 990,
                                "end": 1129
                            },
                            {
                                "start": 1130,
                                "end": 1288
                            }
                        ],
                        "ref_mentions": [],
                        "quote": "The widespread deployment of such generative art models has brought about significant challenges concerning the risk for copyright infringement. The image generative models require large amount of training data, and it's impractical to only use noncopyrighted content to train the models. The model holder wants to leverage the high quality copyrighted data to learn the style and content and output high quality generated data. However, the generative models may generate some images that are very similar to the copyrighted training data, which may lead to copyright infringement [28]."
                    }
                ]
            },
            {
                "idx": 18,
                "key": "[274436153 | Han et al. | 2024 | Citations: 3]",
                "snippets": "Advancements in Artificial Intelligence Generated Contents (AIGCs) [47] have revolutionized the field of image synthesis (Rombach et al., 2021)38,(Xue et al., 2023), among which text-to-image diffusion models enable the creation of high-quality images from textual descriptions (Saharia et al., 2022)(Zhang et al., 2023). However, this progress has also raised significant concerns regarding the potential misuse of these models (Carlini et al., 2023)13,(Sha et al., 2022)[43]. Such misuse includes generating content that infringes on copyrights, such as mimicking specific artistic styles [30], intellectual properties [45]46], or creating disturbing and improper subject matter, including eroticism and violence (Schramowski et al., 2022). Addressing these issues necessitates continuously removing those improper concepts from these models to prevent misuse and protect copyright from infringement.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {
                    "[245335280 | Rombach et al. | 2021 | Citations: 15768]": "By decomposing the image formation process into a sequential application of denoising autoencoders, diffusion models (DMs) achieve state-of-the-art synthesis results on image data and beyond. Additionally, their formulation allows for a guiding mechanism to control the image generation process without retraining. However, since these models typically operate directly in pixel space, optimization of powerful DMs often consumes hundreds of GPU days and inference is expensive due to sequential evaluations. To enable DM training on limited computational resources while retaining their quality and flexibility, we apply them in the latent space of powerful pretrained autoencoders. In contrast to previous work, training diffusion models on such a representation allows for the first time to reach a near-optimal point between complexity reduction and detail preservation, greatly boosting visual fidelity. By introducing cross-attention layers into the model architecture, we turn diffusion models into powerful and flexible generators for general conditioning inputs such as text or bounding boxes and high-resolution synthesis becomes possible in a convolutional manner. Our latent diffusion models (LDMs) achieve new state of the art scores for image inpainting and class-conditional image synthesis and highly competitive performance on various tasks, including unconditional image generation, text-to-image synthesis, and super-resolution, while significantly reducing computational requirements compared to pixel-based DMs.",
                    "[248986576 | Saharia et al. | 2022 | Citations: 6075]": "We present Imagen, a text-to-image diffusion model with an unprecedented degree of photorealism and a deep level of language understanding. Imagen builds on the power of large transformer language models in understanding text and hinges on the strength of diffusion models in high-fidelity image generation. Our key discovery is that generic large language models (e.g. T5), pretrained on text-only corpora, are surprisingly effective at encoding text for image synthesis: increasing the size of the language model in Imagen boosts both sample fidelity and image-text alignment much more than increasing the size of the image diffusion model. Imagen achieves a new state-of-the-art FID score of 7.27 on the COCO dataset, without ever training on COCO, and human raters find Imagen samples to be on par with the COCO data itself in image-text alignment. To assess text-to-image models in greater depth, we introduce DrawBench, a comprehensive and challenging benchmark for text-to-image models. With DrawBench, we compare Imagen with recent methods including VQ-GAN+CLIP, Latent Diffusion Models, and DALL-E 2, and find that human raters prefer Imagen over other models in side-by-side comparisons, both in terms of sample quality and image-text alignment. See https://imagen.research.google/ for an overview of the results.",
                    "[253420366 | Schramowski et al. | 2022 | Citations: 309]": "Text-conditioned image generation models have recently achieved astonishing results in image quality and text alignment and are consequently employed in a fast-growing number of applications. Since they are highly data-driven, relying on billion-sized datasets randomly scraped from the internet, they also suffer, as we demonstrate, from degenerated and biased human behavior. In turn, they may even reinforce such biases. To help combat these undesired side effects, we present safe latent diffusion (SLD). Specifically, to measure the inappropriate degeneration due to unfiltered and imbalanced training sets, we establish a novel image generation test bed-inappropriate image prompts (I2P)-containing dedicated, real-world image-to-text prompts covering concepts such as nudity and violence. As our exhaustive empirical evaluation demonstrates, the introduced SLD removes and suppresses inappropriate image parts during the diffusion process, with no additional training required and no adverse effect on overall image quality or text alignment.11Code available at https://huggingface.co/docs/diffusers/api/pipelines/stable.diffusion.safe",
                    "[255546643 | Sha et al. | 2022 | Citations: 132]": "Text-to-image generation models that generate images based on prompt descriptions have attracted an increasing amount of attention during the past few months. Despite their encouraging performance, these models raise concerns about the misuse of their generated fake images. To tackle this problem, we pioneer a systematic study on the detection and attribution of fake images generated by text-to-image generation models. Concretely, we first build a machine learning classifier to detect the fake images generated by various text-to-image generation models. We then attribute these fake images to their source models, such that model owners can be held responsible for their models' misuse. We further investigate how prompts that generate fake images affect detection and attribution. We conduct extensive experiments on four popular text-to-image generation models, including DALL\u00b7E 2, Stable Diffusion, GLIDE, and Latent Diffusion, and two benchmark prompt-image datasets. Empirical results show that (1) fake images generated by various models can be distinguished from real ones, as there exists a common artifact shared by fake images from different models; (2) fake images can be effectively attributed to their source models, as different models leave unique fingerprints in their generated images; (3) prompts with the \"person'' topic or a length between 25 and 75 enable models to generate fake images with higher authenticity. All findings contribute to the community's insight into the threats caused by text-to-image generation models. We appeal to the community's consideration of the counterpart solutions, like ours, against the rapidly-evolving fake image generation.",
                    "[256389993 | Carlini et al. | 2023 | Citations: 617]": "Image diffusion models such as DALL-E 2, Imagen, and Stable Diffusion have attracted significant attention due to their ability to generate high-quality synthetic images. In this work, we show that diffusion models memorize individual images from their training data and emit them at generation time. With a generate-and-filter pipeline, we extract over a thousand training examples from state-of-the-art models, ranging from photographs of individual people to trademarked company logos. We also train hundreds of diffusion models in various settings to analyze how different modeling and data decisions affect privacy. Overall, our results show that diffusion models are much less private than prior generative models such as GANs, and that mitigating these vulnerabilities may require new advances in privacy-preserving training.",
                    "[256827727 | Zhang et al. | 2023 | Citations: 4175]": "We present ControlNet, a neural network architecture to add spatial conditioning controls to large, pretrained text-to-image diffusion models. ControlNet locks the production-ready large diffusion models, and reuses their deep and robust encoding layers pretrained with billions of images as a strong backbone to learn a diverse set of conditional controls. The neural architecture is connected with \"zero convolutions\" (zero-initialized convolution layers) that progressively grow the parameters from zero and ensure that no harmful noise could affect the finetuning. We test various conditioning controls, e.g., edges, depth, segmentation, human pose, etc., with Stable Diffusion, using single or multiple conditions, with or without prompts. We show that the training of ControlNets is robust with small (<50k) and large (>1m) datasets. Extensive results show that ControlNet may facilitate wider applications to control image diffusion models.",
                    "[258959002 | Xue et al. | 2023 | Citations: 136]": "Text-to-image generation has recently witnessed remarkable achievements. We introduce a text-conditional image diffusion model, termed RAPHAEL, to generate highly artistic images, which accurately portray the text prompts, encompassing multiple nouns, adjectives, and verbs. This is achieved by stacking tens of mixture-of-experts (MoEs) layers, i.e., space-MoE and time-MoE layers, enabling billions of diffusion paths (routes) from the network input to the output. Each path intuitively functions as a\"painter\"for depicting a particular textual concept onto a specified image region at a diffusion timestep. Comprehensive experiments reveal that RAPHAEL outperforms recent cutting-edge models, such as Stable Diffusion, ERNIE-ViLG 2.0, DeepFloyd, and DALL-E 2, in terms of both image quality and aesthetic appeal. Firstly, RAPHAEL exhibits superior performance in switching images across diverse styles, such as Japanese comics, realism, cyberpunk, and ink illustration. Secondly, a single model with three billion parameters, trained on 1,000 A100 GPUs for two months, achieves a state-of-the-art zero-shot FID score of 6.61 on the COCO dataset. Furthermore, RAPHAEL significantly surpasses its counterparts in human evaluation on the ViLG-300 benchmark. We believe that RAPHAEL holds the potential to propel the frontiers of image generation research in both academia and industry, paving the way for future breakthroughs in this rapidly evolving field. More details can be found on a webpage: https://raphael-painter.github.io/."
                },
                "metadata": [
                    {
                        "section_title": "Introduction",
                        "pdf_hash": "",
                        "start": 0,
                        "end": 777,
                        "sentence_offsets": [
                            {
                                "start": 0,
                                "end": 253
                            },
                            {
                                "start": 254,
                                "end": 374
                            },
                            {
                                "start": 375,
                                "end": 617
                            },
                            {
                                "start": 618,
                                "end": 777
                            }
                        ],
                        "ref_mentions": [
                            "245335280",
                            "258959002",
                            "248986576",
                            "256827727",
                            "256389993",
                            "255546643",
                            "253420366"
                        ],
                        "quote": "Advancements in Artificial Intelligence Generated Contents (AIGCs) [47] have revolutionized the field of image synthesis (Rombach et al., 2021)38,(Xue et al., 2023), among which text-to-image diffusion models enable the creation of high-quality images from textual descriptions (Saharia et al., 2022)(Zhang et al., 2023). However, this progress has also raised significant concerns regarding the potential misuse of these models (Carlini et al., 2023)13,(Sha et al., 2022)[43]. Such misuse includes generating content that infringes on copyrights, such as mimicking specific artistic styles [30], intellectual properties [45]46], or creating disturbing and improper subject matter, including eroticism and violence (Schramowski et al., 2022). Addressing these issues necessitates continuously removing those improper concepts from these models to prevent misuse and protect copyright from infringement."
                    }
                ]
            },
            {
                "idx": 19,
                "key": "[274436785 | Guo et al. | 2024 | Citations: 3]",
                "snippets": "However, as commercial text-to-image diffusion models become increasingly prevalent [57,61], copyright issues have emerged as a significant concern. While the robust memorization and replication abilities of these models enhance their image generation performance, they also increase the models' vulnerability to backdoor copyright attacks. By injecting concealed poisoned data into the training set, diffusion models can be compromised without the need for fine-tuning [62]. Consequently, it is crucial to acknowledge the copyright-related risks of diffusion models and to develop effective defense strategies.\n\nCurrent solutions to copyright issues primarily involve the removal of copyrighted images from training datasets to prevent diffusion models from inadvertently learning these images, thus avoiding potential copyright infringement [7,61,76]. However, effective defense mechanisms against existing backdoor injection attacks [21,28,36,74]77], which can result in copyright violations, have yet to be developed.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {
                    "[257050406 | Vyas et al. | 2023 | Citations: 94]": "There is a growing concern that learned conditional generative models may output samples that are substantially similar to some copyrighted data $C$ that was in their training set. We give a formal definition of $\\textit{near access-freeness (NAF)}$ and prove bounds on the probability that a model satisfying this definition outputs a sample similar to $C$, even if $C$ is included in its training set. Roughly speaking, a generative model $p$ is $\\textit{$k$-NAF}$ if for every potentially copyrighted data $C$, the output of $p$ diverges by at most $k$-bits from the output of a model $q$ that $\\textit{did not access $C$ at all}$. We also give generative model learning algorithms, which efficiently modify the original generative model learning algorithm in a black box manner, that output generative models with strong bounds on the probability of sampling protected content. Furthermore, we provide promising experiments for both language (transformers) and image (diffusion) generative models, showing minimal degradation in output quality while ensuring strong protections against sampling protected content.",
                    "[248986576 | Saharia et al. | 2022 | Citations: 6075]": "We present Imagen, a text-to-image diffusion model with an unprecedented degree of photorealism and a deep level of language understanding. Imagen builds on the power of large transformer language models in understanding text and hinges on the strength of diffusion models in high-fidelity image generation. Our key discovery is that generic large language models (e.g. T5), pretrained on text-only corpora, are surprisingly effective at encoding text for image synthesis: increasing the size of the language model in Imagen boosts both sample fidelity and image-text alignment much more than increasing the size of the image diffusion model. Imagen achieves a new state-of-the-art FID score of 7.27 on the COCO dataset, without ever training on COCO, and human raters find Imagen samples to be on par with the COCO data itself in image-text alignment. To assess text-to-image models in greater depth, we introduce DrawBench, a comprehensive and challenging benchmark for text-to-image models. With DrawBench, we compare Imagen with recent methods including VQ-GAN+CLIP, Latent Diffusion Models, and DALL-E 2, and find that human raters prefer Imagen over other models in side-by-side comparisons, both in terms of sample quality and image-text alignment. See https://imagen.research.google/ for an overview of the results.",
                    "[254366634 | Somepalli et al. | 2022 | Citations: 329]": "Cutting-edge diffusion models produce images with high quality and customizability, enabling them to be used for commercial art and graphic design purposes. But do diffusion models create unique works of art, or are they replicating content directly from their training sets? In this work, we study image retrieval frameworks that enable us to compare generated images with training samples and detect when content has been replicated. Applying our frameworks to diffusion models trained on multiple datasets including Oxford flowers, Celeb-A, ImageNet, and LAION, we discuss how factors such as training set size impact rates of content replication. We also identify cases where diffusion models, including the popular Stable Diffusion model, blatantly copy from their training data. Project page: https://somepago.github.io/diffrep.html"
                },
                "metadata": [
                    {
                        "section_title": "Introduction",
                        "pdf_hash": "",
                        "start": 817,
                        "end": 1838,
                        "sentence_offsets": [
                            {
                                "start": 728,
                                "end": 862
                            },
                            {
                                "start": 863,
                                "end": 999
                            },
                            {
                                "start": 1002,
                                "end": 1150
                            },
                            {
                                "start": 1151,
                                "end": 1342
                            },
                            {
                                "start": 1343,
                                "end": 1477
                            },
                            {
                                "start": 1478,
                                "end": 1613
                            },
                            {
                                "start": 1616,
                                "end": 1856
                            }
                        ],
                        "ref_mentions": [
                            "248986576",
                            "254366634",
                            "257050406"
                        ],
                        "quote": "However, as commercial text-to-image diffusion models become increasingly prevalent [57,61], copyright issues have emerged as a significant concern. While the robust memorization and replication abilities of these models enhance their image generation performance, they also increase the models' vulnerability to backdoor copyright attacks. By injecting concealed poisoned data into the training set, diffusion models can be compromised without the need for fine-tuning [62]. Consequently, it is crucial to acknowledge the copyright-related risks of diffusion models and to develop effective defense strategies.\n\nCurrent solutions to copyright issues primarily involve the removal of copyrighted images from training datasets to prevent diffusion models from inadvertently learning these images, thus avoiding potential copyright infringement [7,61,76]. However, effective defense mechanisms against existing backdoor injection attacks [21,28,36,74]77], which can result in copyright violations, have yet to be developed."
                    }
                ]
            },
            {
                "idx": 20,
                "key": "[276250558 | Zampini et al. | 2025 | Citations: 2]",
                "snippets": "This method ensures that the generated images are guided away from resembling copyrighted material while still allowing the model to produce high-quality outputs. By selectively modifying the generated content during the initial stages of denoising, we can effectively prevent the model from producing images that infringe on copyrights without significantly affecting the overall image quality...We implement a Conditional Diffusion Model baselines using and unconstrained stable diffusion model identical to the one used for our method. The conditional baseline generates the protected cartoon character (Mickey Mouse) 33% of the time, despite conditioning it against these generations. \n\nConversely, our Latent Constrained Model only generates the protected cartoon character 10% of the time, aligning with the expected bounds of the classifier's predictive accuracy. Our method has proven to be highly effective because it preserves the generative capabilities of the model while imposing the defined constraints. Notably, the difference between the original image and the corrected one primarily affects the areas near the figure that violate the constraint, while the rest of the image remains largely unchanged. The FID scores of the generated images, increasing only slightly from 61.2 to 65.1, remain largely unaltered by the gradientbased correction. This demonstrates that our approach can selectively modify generated content to avoid copyrighted material without compromising overall image quality.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {},
                "metadata": [
                    {
                        "section_title": "Structural analysis",
                        "pdf_hash": "",
                        "start": 0,
                        "end": 394,
                        "sentence_offsets": [
                            {
                                "start": 0,
                                "end": 162
                            },
                            {
                                "start": 163,
                                "end": 395
                            }
                        ],
                        "ref_mentions": [],
                        "quote": "This method ensures that the generated images are guided away from resembling copyrighted material while still allowing the model to produce high-quality outputs. By selectively modifying the generated content during the initial stages of denoising, we can effectively prevent the model from producing images that infringe on copyrights without significantly affecting the overall image quality"
                    },
                    {
                        "section_title": "Structural analysis",
                        "pdf_hash": "",
                        "start": 700,
                        "end": 1815,
                        "sentence_offsets": [
                            {
                                "start": 700,
                                "end": 841
                            },
                            {
                                "start": 842,
                                "end": 991
                            },
                            {
                                "start": 994,
                                "end": 1173
                            },
                            {
                                "start": 1174,
                                "end": 1320
                            },
                            {
                                "start": 1321,
                                "end": 1521
                            },
                            {
                                "start": 1522,
                                "end": 1663
                            },
                            {
                                "start": 1664,
                                "end": 1814
                            }
                        ],
                        "ref_mentions": [],
                        "quote": "We implement a Conditional Diffusion Model baselines using and unconstrained stable diffusion model identical to the one used for our method. The conditional baseline generates the protected cartoon character (Mickey Mouse) 33% of the time, despite conditioning it against these generations. \n\nConversely, our Latent Constrained Model only generates the protected cartoon character 10% of the time, aligning with the expected bounds of the classifier's predictive accuracy. Our method has proven to be highly effective because it preserves the generative capabilities of the model while imposing the defined constraints. Notably, the difference between the original image and the corrected one primarily affects the areas near the figure that violate the constraint, while the rest of the image remains largely unchanged. The FID scores of the generated images, increasing only slightly from 61.2 to 65.1, remain largely unaltered by the gradientbased correction. This demonstrates that our approach can selectively modify generated content to avoid copyrighted material without compromising overall image quality."
                    }
                ]
            },
            {
                "idx": 21,
                "key": "[276558342 | Liu et al. | 2025 | Citations: 2]",
                "snippets": "Text-to-image generative models (Rombach et al., 2021)(Betker et al., 0)Team et al., 2023;(Esser et al., 2024)Hurst et al., 2024;(Zhang et al., 2023)a;Hintersdorf et al., 2024) have transformed creative industries by producing detailed visuals from text prompts. However, these models have been found to sometimes memorize and reproduce content from their training data (Carlini et al., 2023)(Somepalli et al., 2022)Ren et al., 2024;Wang et al., 2024c;Shi et al., 2024b;a;Zhang et al., 2024). This raises significant concerns about copyright infringement, especially when the generated images closely resemble existing copyrighted works. According to U.S. law (rot, 1970), also referenced by most countries, a work can be considered infringing if it constitutes substantial similarity to another work1 . Therefore, determining whether AI-generated images infringe on copyright requires a clear and reliable method to compare them with copyrighted materials to identify substantial similarity.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {
                    "[245335280 | Rombach et al. | 2021 | Citations: 15768]": "By decomposing the image formation process into a sequential application of denoising autoencoders, diffusion models (DMs) achieve state-of-the-art synthesis results on image data and beyond. Additionally, their formulation allows for a guiding mechanism to control the image generation process without retraining. However, since these models typically operate directly in pixel space, optimization of powerful DMs often consumes hundreds of GPU days and inference is expensive due to sequential evaluations. To enable DM training on limited computational resources while retaining their quality and flexibility, we apply them in the latent space of powerful pretrained autoencoders. In contrast to previous work, training diffusion models on such a representation allows for the first time to reach a near-optimal point between complexity reduction and detail preservation, greatly boosting visual fidelity. By introducing cross-attention layers into the model architecture, we turn diffusion models into powerful and flexible generators for general conditioning inputs such as text or bounding boxes and high-resolution synthesis becomes possible in a convolutional manner. Our latent diffusion models (LDMs) achieve new state of the art scores for image inpainting and class-conditional image synthesis and highly competitive performance on various tasks, including unconditional image generation, text-to-image synthesis, and super-resolution, while significantly reducing computational requirements compared to pixel-based DMs.",
                    "[254366634 | Somepalli et al. | 2022 | Citations: 329]": "Cutting-edge diffusion models produce images with high quality and customizability, enabling them to be used for commercial art and graphic design purposes. But do diffusion models create unique works of art, or are they replicating content directly from their training sets? In this work, we study image retrieval frameworks that enable us to compare generated images with training samples and detect when content has been replicated. Applying our frameworks to diffusion models trained on multiple datasets including Oxford flowers, Celeb-A, ImageNet, and LAION, we discuss how factors such as training set size impact rates of content replication. We also identify cases where diffusion models, including the popular Stable Diffusion model, blatantly copy from their training data. Project page: https://somepago.github.io/diffrep.html",
                    "[256389993 | Carlini et al. | 2023 | Citations: 617]": "Image diffusion models such as DALL-E 2, Imagen, and Stable Diffusion have attracted significant attention due to their ability to generate high-quality synthetic images. In this work, we show that diffusion models memorize individual images from their training data and emit them at generation time. With a generate-and-filter pipeline, we extract over a thousand training examples from state-of-the-art models, ranging from photographs of individual people to trademarked company logos. We also train hundreds of diffusion models in various settings to analyze how different modeling and data decisions affect privacy. Overall, our results show that diffusion models are much less private than prior generative models such as GANs, and that mitigating these vulnerabilities may require new advances in privacy-preserving training.",
                    "[256827727 | Zhang et al. | 2023 | Citations: 4175]": "We present ControlNet, a neural network architecture to add spatial conditioning controls to large, pretrained text-to-image diffusion models. ControlNet locks the production-ready large diffusion models, and reuses their deep and robust encoding layers pretrained with billions of images as a strong backbone to learn a diverse set of conditional controls. The neural architecture is connected with \"zero convolutions\" (zero-initialized convolution layers) that progressively grow the parameters from zero and ensure that no harmful noise could affect the finetuning. We test various conditioning controls, e.g., edges, depth, segmentation, human pose, etc., with Stable Diffusion, using single or multiple conditions, with or without prompts. We show that the training of ControlNets is robust with small (<50k) and large (>1m) datasets. Extensive results show that ControlNet may facilitate wider applications to control image diffusion models.",
                    "[268247980 | Esser et al. | 2024 | Citations: 1401]": "Diffusion models create data from noise by inverting the forward paths of data towards noise and have emerged as a powerful generative modeling technique for high-dimensional, perceptual data such as images and videos. Rectified flow is a recent generative model formulation that connects data and noise in a straight line. Despite its better theoretical properties and conceptual simplicity, it is not yet decisively established as standard practice. In this work, we improve existing noise sampling techniques for training rectified flow models by biasing them towards perceptually relevant scales. Through a large-scale study, we demonstrate the superior performance of this approach compared to established diffusion formulations for high-resolution text-to-image synthesis. Additionally, we present a novel transformer-based architecture for text-to-image generation that uses separate weights for the two modalities and enables a bidirectional flow of information between image and text tokens, improving text comprehension, typography, and human preference ratings. We demonstrate that this architecture follows predictable scaling trends and correlates lower validation loss to improved text-to-image synthesis as measured by various metrics and human evaluations. Our largest models outperform state-of-the-art models, and we will make our experimental data, code, and model weights publicly available."
                },
                "metadata": [
                    {
                        "section_title": "Introduction",
                        "pdf_hash": "",
                        "start": 0,
                        "end": 993,
                        "sentence_offsets": [
                            {
                                "start": 0,
                                "end": 263
                            },
                            {
                                "start": 264,
                                "end": 493
                            },
                            {
                                "start": 494,
                                "end": 638
                            },
                            {
                                "start": 639,
                                "end": 804
                            },
                            {
                                "start": 805,
                                "end": 993
                            }
                        ],
                        "ref_mentions": [
                            "245335280",
                            "264403242",
                            "268247980",
                            "256827727",
                            "256389993",
                            "254366634"
                        ],
                        "quote": "Text-to-image generative models (Rombach et al., 2021)(Betker et al., 0)Team et al., 2023;(Esser et al., 2024)Hurst et al., 2024;(Zhang et al., 2023)a;Hintersdorf et al., 2024) have transformed creative industries by producing detailed visuals from text prompts. However, these models have been found to sometimes memorize and reproduce content from their training data (Carlini et al., 2023)(Somepalli et al., 2022)Ren et al., 2024;Wang et al., 2024c;Shi et al., 2024b;a;Zhang et al., 2024). This raises significant concerns about copyright infringement, especially when the generated images closely resemble existing copyrighted works. According to U.S. law (rot, 1970), also referenced by most countries, a work can be considered infringing if it constitutes substantial similarity to another work1 . Therefore, determining whether AI-generated images infringe on copyright requires a clear and reliable method to compare them with copyrighted materials to identify substantial similarity."
                    }
                ]
            },
            {
                "idx": 22,
                "key": "[277151077 | Roy et al. | 2025 | Citations: 0]",
                "snippets": "Modern text-to-image generative models can inadvertently reproduce copyrighted content memorized in their training data, raising serious concerns about potential copyright infringement.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {},
                "metadata": [
                    {
                        "section_title": "abstract",
                        "pdf_hash": "",
                        "start": 0,
                        "end": 185,
                        "sentence_offsets": [],
                        "ref_mentions": [],
                        "quote": "Modern text-to-image generative models can inadvertently reproduce copyrighted content memorized in their training data, raising serious concerns about potential copyright infringement."
                    }
                ]
            },
            {
                "idx": 23,
                "key": "[277955485 | Rakib et al. | 2025 | Citations: 1]",
                "snippets": "In today's age of social media and marketing, copyright issues can be a major roadblock to the free sharing of images. Generative AI models have made it possible to create high-quality images, but concerns about copyright infringement are a hindrance to their abundant use. As these models use data from training images to generate new ones, it is often a daunting task to ensure they do not violate intellectual property rights. Some AI models have even been noted to directly copy copyrighted images, a problem often referred to as source copying. Traditional copyright protection measures such as watermarks and metadata have also proven to be futile in this regard....Diffusion models have revolutionized image generation, surpassing predecessors like GANs and VAEs in both fidelity and performance. However, this progress has been shadowed by a critical challenge: source copying. This issue raises serious concerns regarding privacy and intellectual property rights, demanding innovative solutions that preserve content quality while enhancing copyright protection.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {},
                "metadata": [
                    {
                        "section_title": "abstract",
                        "pdf_hash": "",
                        "start": 0,
                        "end": 670,
                        "sentence_offsets": [],
                        "ref_mentions": [],
                        "quote": "In today's age of social media and marketing, copyright issues can be a major roadblock to the free sharing of images. Generative AI models have made it possible to create high-quality images, but concerns about copyright infringement are a hindrance to their abundant use. As these models use data from training images to generate new ones, it is often a daunting task to ensure they do not violate intellectual property rights. Some AI models have even been noted to directly copy copyrighted images, a problem often referred to as source copying. Traditional copyright protection measures such as watermarks and metadata have also proven to be futile in this regard"
                    },
                    {
                        "section_title": "Conclusion",
                        "pdf_hash": "",
                        "start": 0,
                        "end": 400,
                        "sentence_offsets": [
                            {
                                "start": 0,
                                "end": 131
                            },
                            {
                                "start": 132,
                                "end": 213
                            },
                            {
                                "start": 214,
                                "end": 399
                            }
                        ],
                        "ref_mentions": [],
                        "quote": ".Diffusion models have revolutionized image generation, surpassing predecessors like GANs and VAEs in both fidelity and performance. However, this progress has been shadowed by a critical challenge: source copying. This issue raises serious concerns regarding privacy and intellectual property rights, demanding innovative solutions that preserve content quality while enhancing copyright protection."
                    }
                ]
            },
            {
                "idx": 24,
                "key": "[278129333 | Chen et al. | 2025 | Citations: 1]",
                "snippets": "Leveraging classifier-free guidance (CFG) [12], text-toimage diffusion models like Stable Diffusion (Rombach et al., 2021) and Midjourney [1] are now capable of generating highly realistic images that closely align with user-provided text prompts. This capability has propelled their popularity, leading to widespread use and distribution of their generated images. However, recent research (Carlini et al., 2023)8,(Somepalli et al., 2022)[34] has revealed a critical issue: these models can memorize training data, leading them to reproduce parts of images, such as foregrounds or backgrounds (local memorization, see Fig. 1), or even entire images (global memorization, see Fig. 2) during inference, instead of generating genuinely novel content. When the training data includes sensitive or copyrighted material, these memorization issues can infringe on copyright laws without notifying either the model's owners or users or the copyright holders of the replicated content. The risk is es-pecially substantial given the extensive use of these models and their reliance on massive datasets, such as LAION [30], which contain billions of web-scale images and are impractical to thoroughly review or filter manually. This risk is further highlighted by real-world cases, where several artists have filed lawsuits, arguing that models like Stable Diffusion act as \"21st-century collage tools\" that remix their copyrighted works, implicating Stability AI, DeviantArt, Midjourney, and Runway AI. Recognizing the potential for unauthorized reproductions, Midjourney has even banned prompts containing the term \"Afghan\" to prevent the generation of the copyrighted Afghan Girl photograph. Yet, as (Wen et al., 2023) demonstrates, such restrictions alone are insufficient to fully prevent the reproduction of copyrighted images.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {
                    "[245335280 | Rombach et al. | 2021 | Citations: 15768]": "By decomposing the image formation process into a sequential application of denoising autoencoders, diffusion models (DMs) achieve state-of-the-art synthesis results on image data and beyond. Additionally, their formulation allows for a guiding mechanism to control the image generation process without retraining. However, since these models typically operate directly in pixel space, optimization of powerful DMs often consumes hundreds of GPU days and inference is expensive due to sequential evaluations. To enable DM training on limited computational resources while retaining their quality and flexibility, we apply them in the latent space of powerful pretrained autoencoders. In contrast to previous work, training diffusion models on such a representation allows for the first time to reach a near-optimal point between complexity reduction and detail preservation, greatly boosting visual fidelity. By introducing cross-attention layers into the model architecture, we turn diffusion models into powerful and flexible generators for general conditioning inputs such as text or bounding boxes and high-resolution synthesis becomes possible in a convolutional manner. Our latent diffusion models (LDMs) achieve new state of the art scores for image inpainting and class-conditional image synthesis and highly competitive performance on various tasks, including unconditional image generation, text-to-image synthesis, and super-resolution, while significantly reducing computational requirements compared to pixel-based DMs.",
                    "[254366634 | Somepalli et al. | 2022 | Citations: 329]": "Cutting-edge diffusion models produce images with high quality and customizability, enabling them to be used for commercial art and graphic design purposes. But do diffusion models create unique works of art, or are they replicating content directly from their training sets? In this work, we study image retrieval frameworks that enable us to compare generated images with training samples and detect when content has been replicated. Applying our frameworks to diffusion models trained on multiple datasets including Oxford flowers, Celeb-A, ImageNet, and LAION, we discuss how factors such as training set size impact rates of content replication. We also identify cases where diffusion models, including the popular Stable Diffusion model, blatantly copy from their training data. Project page: https://somepago.github.io/diffrep.html",
                    "[256389993 | Carlini et al. | 2023 | Citations: 617]": "Image diffusion models such as DALL-E 2, Imagen, and Stable Diffusion have attracted significant attention due to their ability to generate high-quality synthetic images. In this work, we show that diffusion models memorize individual images from their training data and emit them at generation time. With a generate-and-filter pipeline, we extract over a thousand training examples from state-of-the-art models, ranging from photographs of individual people to trademarked company logos. We also train hundreds of diffusion models in various settings to analyze how different modeling and data decisions affect privacy. Overall, our results show that diffusion models are much less private than prior generative models such as GANs, and that mitigating these vulnerabilities may require new advances in privacy-preserving training.",
                    "[256627601 | Wen et al. | 2023 | Citations: 273]": "The strength of modern generative models lies in their ability to be controlled through text-based prompts. Typical\"hard\"prompts are made from interpretable words and tokens, and must be hand-crafted by humans. There are also\"soft\"prompts, which consist of continuous feature vectors. These can be discovered using powerful optimization methods, but they cannot be easily interpreted, re-used across models, or plugged into a text-based interface. We describe an approach to robustly optimize hard text prompts through efficient gradient-based optimization. Our approach automatically generates hard text-based prompts for both text-to-image and text-to-text applications. In the text-to-image setting, the method creates hard prompts for diffusion models, allowing API users to easily generate, discover, and mix and match image concepts without prior knowledge on how to prompt the model. In the text-to-text setting, we show that hard prompts can be automatically discovered that are effective in tuning LMs for classification."
                },
                "metadata": [
                    {
                        "section_title": "Introduction",
                        "pdf_hash": "",
                        "start": 0,
                        "end": 1750,
                        "sentence_offsets": [
                            {
                                "start": 0,
                                "end": 229
                            },
                            {
                                "start": 230,
                                "end": 347
                            },
                            {
                                "start": 348,
                                "end": 689
                            },
                            {
                                "start": 690,
                                "end": 918
                            },
                            {
                                "start": 919,
                                "end": 1158
                            },
                            {
                                "start": 1159,
                                "end": 1434
                            },
                            {
                                "start": 1435,
                                "end": 1625
                            },
                            {
                                "start": 1626,
                                "end": 1750
                            }
                        ],
                        "ref_mentions": [
                            "245335280",
                            "256389993",
                            "254366634",
                            "256627601"
                        ],
                        "quote": "Leveraging classifier-free guidance (CFG) [12], text-toimage diffusion models like Stable Diffusion (Rombach et al., 2021) and Midjourney [1] are now capable of generating highly realistic images that closely align with user-provided text prompts. This capability has propelled their popularity, leading to widespread use and distribution of their generated images. However, recent research (Carlini et al., 2023)8,(Somepalli et al., 2022)[34] has revealed a critical issue: these models can memorize training data, leading them to reproduce parts of images, such as foregrounds or backgrounds (local memorization, see Fig. 1), or even entire images (global memorization, see Fig. 2) during inference, instead of generating genuinely novel content. When the training data includes sensitive or copyrighted material, these memorization issues can infringe on copyright laws without notifying either the model's owners or users or the copyright holders of the replicated content. The risk is es-pecially substantial given the extensive use of these models and their reliance on massive datasets, such as LAION [30], which contain billions of web-scale images and are impractical to thoroughly review or filter manually. This risk is further highlighted by real-world cases, where several artists have filed lawsuits, arguing that models like Stable Diffusion act as \"21st-century collage tools\" that remix their copyrighted works, implicating Stability AI, DeviantArt, Midjourney, and Runway AI. Recognizing the potential for unauthorized reproductions, Midjourney has even banned prompts containing the term \"Afghan\" to prevent the generation of the copyrighted Afghan Girl photograph. Yet, as (Wen et al., 2023) demonstrates, such restrictions alone are insufficient to fully prevent the reproduction of copyrighted images."
                    }
                ]
            },
            {
                "idx": 25,
                "key": "[278338968 | Reissinger et al. | 2025 | Citations: 0]",
                "snippets": "As generative AI (GenAI) becomes increasingly prevalent in realworld applications, concerns about its potential risks continue to grow. We focus on the risks associated with the replication of copyrighted material and restrict ourselves to image generation. The tangible legal and financial implications of these risks have sparked our current research. For instance, artists sued Stability AI (Midjourney and DevianArt) claiming that the companies' AI image generators produce images that are strikingly similar to their artworks [2]. In August 2024, a U.S. district judge (Orrick) ruled that the artists could proceed with the copyright infringement claims, underscoring the ongoing legal uncertainties surrounding AI-generated content [6].",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {},
                "metadata": [
                    {
                        "section_title": "Introduction",
                        "pdf_hash": "",
                        "start": 0,
                        "end": 742,
                        "sentence_offsets": [
                            {
                                "start": 0,
                                "end": 135
                            },
                            {
                                "start": 136,
                                "end": 257
                            },
                            {
                                "start": 258,
                                "end": 353
                            },
                            {
                                "start": 354,
                                "end": 535
                            },
                            {
                                "start": 536,
                                "end": 742
                            }
                        ],
                        "ref_mentions": [],
                        "quote": "As generative AI (GenAI) becomes increasingly prevalent in realworld applications, concerns about its potential risks continue to grow. We focus on the risks associated with the replication of copyrighted material and restrict ourselves to image generation. The tangible legal and financial implications of these risks have sparked our current research. For instance, artists sued Stability AI (Midjourney and DevianArt) claiming that the companies' AI image generators produce images that are strikingly similar to their artworks [2]. In August 2024, a U.S. district judge (Orrick) ruled that the artists could proceed with the copyright infringement claims, underscoring the ongoing legal uncertainties surrounding AI-generated content [6]."
                    }
                ]
            },
            {
                "idx": 26,
                "key": "[278534729 | Wahab et al. | 2025 | Citations: 0]",
                "snippets": "The growing availability of generative AI tools for visual content creation has raised critical concerns around copyright infringement, especially in the domain of visual artworks [1]. Generative models trained on large-scale, web-scraped datasets often absorb patterns from both copyrighted and public domain images, making them prone to reproducing unauthorized content [2]. This phenomenon is particularly problematic in the context of AI-generated art, where stylistic and structural similarities to original artworks may constitute legal or ethical violations [3].\n\nDeep Fake art generative models are vulnerable to violating copyright terms by producing images that mimic or closely resemble content protected under copyright [10]. A formal mathematical formulation of copyright infringement in this context is introduced in [4].",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {},
                "metadata": [
                    {
                        "quote": "The growing availability of generative AI tools for visual content creation has raised critical concerns around copyright infringement, especially in the domain of visual artworks [1]. Generative models trained on large-scale, web-scraped datasets often absorb patterns from both copyrighted and public domain images, making them prone to reproducing unauthorized content [2]. This phenomenon is particularly problematic in the context of AI-generated art, where stylistic and structural similarities to original artworks may constitute legal or ethical violations [3].\n\nDeep Fake art generative models are vulnerable to violating copyright terms by producing images that mimic or closely resemble content protected under copyright [10]. A formal mathematical formulation of copyright infringement in this context is introduced in [4].",
                        "pdf_hash": ""
                    }
                ]
            }
        ]
    },
    "cluster": {
        "cost": 0.03186,
        "cot": "For the query on \"copyright infringement of image generation models,\" I need to organize the quotes into coherent dimensions that address the various aspects of this topic. Based on the quotes provided, I'll create dimensions that cover the key areas of copyright infringement in the context of image generation models.\n\n1. \"Introduction to Copyright Concerns in Generative AI\" - This should be a synthesis section that introduces the basic concepts and provides background on why copyright is a concern for image generation models. It's important to start with this to provide context for readers.\n\n2. \"How Copyright Infringement Occurs in Generative Models\" - This should be a synthesis section that explains the mechanisms through which copyright infringement can happen with generative models. Several quotes discuss how these models memorize training data and can reproduce copyrighted content.\n\n3. \"Legal Cases and Lawsuits\" - This should be a list format since there are several specific lawsuits mentioned in the quotes that would be best presented as discrete examples rather than a continuous narrative.\n\n4. \"Proposed Solutions and Defenses\" - This should be a synthesis section that discusses the various approaches being developed to address copyright concerns, from both technical and legal perspectives.\n\n5. \"Implications for Stakeholders\" - This should be a synthesis section examining how copyright issues affect different parties: model developers, artists, users, and copyright holders.\n\nThis organization covers the essential aspects of the topic while ensuring all quotes are included and the information flows logically from introduction to specific concerns to potential solutions.",
        "plan": {
            "Introduction to Copyright Concerns in Generative AI (synthesis)": [
                0,
                1,
                10,
                13,
                15,
                17
            ],
            "How Copyright Infringement Occurs in Generative Models (synthesis)": [
                0,
                3,
                4,
                5,
                8,
                11,
                12,
                16,
                21,
                22,
                23,
                24,
                26
            ],
            "Legal Cases and Lawsuits (list)": [
                1,
                4,
                6,
                7,
                14,
                15,
                25
            ],
            "Proposed Solutions and Defenses (synthesis)": [
                2,
                9,
                18,
                19,
                20,
                23
            ],
            "Implications for Stakeholders (synthesis)": [
                2,
                7,
                8,
                9,
                16
            ]
        },
        "model": "claude-3-7-sonnet-20250219"
    },
    "summary": {
        "sections": [
            {
                "title": "Introduction to Copyright Concerns in Generative AI",
                "tldr": "Generative AI models for images raise significant copyright concerns as they can potentially reproduce content similar to copyrighted works in their training data. These issues arise both during model training and when generating outputs, creating legal challenges for creators, developers, and users. (8 sources)",
                "text": "\nGenerative AI models, particularly image generation systems like diffusion models, have demonstrated remarkable capabilities in creating high-quality, realistic images from textual descriptions. However, these advancements have introduced significant copyright challenges that affect multiple stakeholders in the creative ecosystem <Paper corpusId=\"260155285\" paperTitle=\"(Leotta et al., 2023)\" isShortName></Paper> <Paper corpusId=\"268532352\" paperTitle=\"(Ma et al., 2024)\" isShortName></Paper>. The fundamental problem stems from these models being trained on massive datasets that frequently contain copyrighted works, making it practically impossible to ensure training data is completely free of protected material <Paper corpusId=\"257050406\" paperTitle=\"(Vyas et al., 2023)\" isShortName></Paper>.\n\nCopyright infringement concerns in generative AI emerge through two primary mechanisms. First, during the training phase, algorithms directly access copyrighted material, potentially encoding verbatim copies within the model's weights. Second, during deployment, when users prompt the model to generate content, the outputs may be substantially similar to copyrighted training data without any clear provenance tracking <Paper corpusId=\"257050406\" paperTitle=\"(Vyas et al., 2023)\" isShortName></Paper>. This is particularly problematic as models like Stable Diffusion and Midjourney can generate images closely resembling protected content, raising questions about whether these similarities constitute copyright violations <Paper corpusId=\"272146279\" paperTitle=\"(Shi et al., 2024)\" isShortName></Paper> <Paper corpusId=\"245335280\" paperTitle=\"(Rombach et al., 2021)\" isShortName></Paper>.\n\nAs the boundary between human creativity and machine-generated content becomes increasingly blurred, determining copyright ownership and liability has become a complex challenge <Paper corpusId=\"260155285\" paperTitle=\"(Leotta et al., 2023)\" isShortName></Paper>. While existing copyright laws grant creators exclusive rights to reproduce, distribute, and monetize their creative works, these frameworks may be insufficient to address the unique characteristics of AI-generated content <Paper corpusId=\"268532352\" paperTitle=\"(Ma et al., 2024)\" isShortName></Paper>. The issue is further complicated by the impracticality of using only non-copyrighted content for training these models, as developers seek to leverage high-quality data to improve generation capabilities <Paper corpusId=\"273654195\" paperTitle=\"(Shi et al._1, 2024)\" isShortName></Paper>.\n\nDespite attempts to develop methods that mitigate copyright infringement risks, significant challenges remain <Paper corpusId=\"270258236\" paperTitle=\"(Chiba-Okabe et al., 2024)\" isShortName></Paper>. Recent legal cases involving image generation platforms highlight growing concerns about whether the high-quality content synthesized by these models might be excessively similar to copyrighted training data, potentially violating the rights of numerous artists and copyright holders <Paper corpusId=\"272146279\" paperTitle=\"(Shi et al., 2024)\" isShortName></Paper> <Paper corpusId=\"258297834\" paperTitle=\"(Dogoulis et al., 2023)\" isShortName></Paper>.",
                "citations": [
                    {
                        "id": "(Leotta et al., 2023)",
                        "snippets": [
                            "The rapid advancement of generative models, particularly Diffusion Models [7], has led to a surge in high-quality, realistic image generation. These models have demonstrated immense potential for creative applications across various domains, including art, design, and advertising. However, their ability to replicate the styles of specific artists raises concerns about Intellectual Property (IP) rights and potential copyright infringements [20,25,19]. \n\nAs the boundary between human creativity and machine-generated content becomes increasingly blurred, it is crucial to address the legal and ethical implications of using generative models to produce art, as well as to develop methods that evaluate the extent to which generated images are influenced by the works of real artists and ensure the protection of their intellectual property. \n\nSeveral studies and articles have explored the legal ramifications of generative models and their potential to infringe on copyrights. In an article dated 2021 the challenges in determining copyright ownership for AI-generated works were discussed and authors emphasized the need for legal frameworks that could adequately address the unique nature of generative models. MLQ.AI also reported on a copyright infringement case involving generative AI, which sparked debates on the responsibilities of AI developers and users in protecting original creators' rights. In response to these concerns, legal scholars have delved into the complexities of copyright law as it pertains to AI-generated artworks. Gillotte [8] examined the challenges in assigning liability and protecting IP rights, arguing that existing copyright laws may not be sufficient to address the unique characteristics of AI-generated contents. Indeed, it is became a copyright dilemma and the need for a balance between innovation and IP protection is arising to ensure that creative works are safeguarded without stifling technological advancements [16]."
                        ],
                        "paper": {
                            "corpus_id": 260155285,
                            "title": "Not with my name! Inferring artists' names of input strings employed by Diffusion Models",
                            "authors": [
                                {
                                    "authorId": "47253043",
                                    "name": "R. Leotta"
                                },
                                {
                                    "authorId": "1797543",
                                    "name": "Oliver Giudice"
                                },
                                {
                                    "authorId": "40010524",
                                    "name": "Luca Guarnera"
                                },
                                {
                                    "authorId": "1742452",
                                    "name": "S. Battiato"
                                }
                            ],
                            "year": 2023,
                            "venue": "International Conference on Image Analysis and Processing",
                            "n_citations": 7
                        },
                        "score": 0.982421875
                    },
                    {
                        "id": "(Ma et al., 2024)",
                        "snippets": [
                            "Copyright law confers upon creators the exclusive rights to reproduce, distribute, and monetize their creative works. However, recent progress in text-to-image generation has introduced formidable challenges to copyright enforcement. These technologies enable the unauthorized learning and replication of copyrighted content, artistic creations, and likenesses, leading to the proliferation of unregulated content. Notably, models like stable diffusion, which excel in text-to-image synthesis, heighten the risk of copyright infringement and unauthorized distribution",
                            ".Text-to-image generative models have recently emerged as a significant topic in computer vision, demonstrating remarkable results in the area of generative modeling (Goodfellow et al., 2021)(Rombach et al., 2021).These models bridge the gap between language and visual contents by generating realistic images from textual descriptions.However, rapid advancements in text-to-image generation techniques have raised concerns about copyright protection, particularly unauthorized reproduction of content, artistic creations, and portraits [4].A specific concern arises from the use of Stable Diffusion (SD), a state-of-the-art text-conditional latent diffusion model, which has sparked global discussions on copyright."
                        ],
                        "paper": {
                            "corpus_id": 268532352,
                            "title": "A Dataset and Benchmark for Copyright Infringement Unlearning from Text-to-Image Diffusion Models",
                            "authors": [
                                {
                                    "authorId": "2210435658",
                                    "name": "Rui Ma"
                                },
                                {
                                    "authorId": "2257338567",
                                    "name": "Qiang Zhou"
                                },
                                {
                                    "authorId": "2268733263",
                                    "name": "Yizhu Jin"
                                },
                                {
                                    "authorId": "2292161412",
                                    "name": "Daquan Zhou"
                                },
                                {
                                    "authorId": "2292177829",
                                    "name": "Bangjun Xiao"
                                },
                                {
                                    "authorId": "2292217065",
                                    "name": "Xiuyu Li"
                                },
                                {
                                    "authorId": "2292690089",
                                    "name": "Yi Qu"
                                },
                                {
                                    "authorId": "2261448160",
                                    "name": "Aishani Singh"
                                },
                                {
                                    "authorId": "2242659602",
                                    "name": "Kurt Keutzer"
                                },
                                {
                                    "authorId": "2328473437",
                                    "name": "Jingtong Hu"
                                },
                                {
                                    "authorId": "2307915260",
                                    "name": "Xiaodong Xie"
                                },
                                {
                                    "authorId": "2293731776",
                                    "name": "Zhen Dong"
                                },
                                {
                                    "authorId": "2257020214",
                                    "name": "Shanghang Zhang"
                                },
                                {
                                    "authorId": "2275298300",
                                    "name": "Shiji Zhou"
                                }
                            ],
                            "year": 2024,
                            "venue": "",
                            "n_citations": 2
                        },
                        "score": 0.99072265625
                    },
                    {
                        "id": "(Vyas et al., 2023)",
                        "snippets": [
                            "Generative models for images, text, code, and other domains pose new challenges for ensuring their outputs are protected from copyright infringement. Such models are trained on a large corpus of data, where it is often impractical to ensure the training set is 100% free of copyrighted material. Furthermore, removing copyrighted material from training may also be undesirable. For example, a human author is free to read and use copyrighted material as inspiration for their work, as long as they do not copy it. Similarly, it may be beneficial to use copyrighted material when training in order to have more effective generative models.\n\nCopyright infringement by generative models can potentially arise in (at least) two manners. First, in the training phase, the algorithm could directly access copyrighted material, and the learned model itself could implicitly contain (e.g. coded in its weights) verbatim copies of some of this material. The copyright issues arising during training share many similarities with other settings in which algorithms scrape significant amounts of data, including search-engine indexing and digitizing books. Here, the question of what constitutes a copyright infringement is largely a question of \"fair use.\" This work does not examine these fair use issues that arise in the training phase, and we refer the reader to the several legal precedents in this area [Samuelson, 2021].\n\nThe second notable source of potential infringement is in the deployment phase, where a user provides a prompt x to the model to obtain some output y. Apriori, we cannot rule out the possibility that y is either a verbatim copy or substantially similar to some copyrighted training data. Moreover, unlike search engines, generative models do not keep track of the provenance of their outputs. Hence, a user of such an output y (e.g., a software company using generated code, or a designer using a generated image) has no easy way to verify that it does not infringe upon any copyrighted material."
                        ],
                        "paper": {
                            "corpus_id": 257050406,
                            "title": "On Provable Copyright Protection for Generative Models",
                            "authors": [
                                {
                                    "authorId": "145603901",
                                    "name": "Nikhil Vyas"
                                },
                                {
                                    "authorId": "144695232",
                                    "name": "S. Kakade"
                                },
                                {
                                    "authorId": "1697211",
                                    "name": "B. Barak"
                                }
                            ],
                            "year": 2023,
                            "venue": "International Conference on Machine Learning",
                            "n_citations": 94
                        },
                        "score": 0.9892578125
                    },
                    {
                        "id": "(Shi et al., 2024)",
                        "snippets": [
                            "These models utilize extensive training data that may include copyrighted works, which they are sometimes capable of memorizing (Carlini et al. 2023). This ability can result in the pro- duction of images that closely resemble protected content (See in Figure 1), posing significant challenges to copyright protection (Dogoulis et al., 2023). Recent legal cases, such as those involving Stable Diffusion (Rombach et al., 2021) and Midjourney(Mansour 2023), highlight concerns over the use of copyrighted data in AI training, where the models potentially infringe on the rights of numerous artists. These cases highlight a growing concern: Could the high-quality content synthesized by these generative AIs be excessively similar to copyrighted training data, potentially violating the rights of copyright holders?"
                        ],
                        "paper": {
                            "corpus_id": 272146279,
                            "title": "RLCP: A Reinforcement Learning-based Copyright Protection Method for Text-to-Image Diffusion Model",
                            "authors": [
                                {
                                    "authorId": "2319419519",
                                    "name": "Zhuan Shi"
                                },
                                {
                                    "authorId": "2318391138",
                                    "name": "Jing Yan"
                                },
                                {
                                    "authorId": "2318236128",
                                    "name": "Xiaoli Tang"
                                },
                                {
                                    "authorId": "2287820224",
                                    "name": "Lingjuan Lyu"
                                },
                                {
                                    "authorId": "2054858128",
                                    "name": "Boi Faltings"
                                }
                            ],
                            "year": 2024,
                            "venue": "arXiv.org",
                            "n_citations": 1
                        },
                        "score": 0.98779296875
                    },
                    {
                        "id": "(Rombach et al., 2021)",
                        "snippets": [
                            "By decomposing the image formation process into a sequential application of denoising autoencoders, diffusion models (DMs) achieve state-of-the-art synthesis results on image data and beyond. Additionally, their formulation allows for a guiding mechanism to control the image generation process without retraining. However, since these models typically operate directly in pixel space, optimization of powerful DMs often consumes hundreds of GPU days and inference is expensive due to sequential evaluations. To enable DM training on limited computational resources while retaining their quality and flexibility, we apply them in the latent space of powerful pretrained autoencoders. In contrast to previous work, training diffusion models on such a representation allows for the first time to reach a near-optimal point between complexity reduction and detail preservation, greatly boosting visual fidelity. By introducing cross-attention layers into the model architecture, we turn diffusion models into powerful and flexible generators for general conditioning inputs such as text or bounding boxes and high-resolution synthesis becomes possible in a convolutional manner. Our latent diffusion models (LDMs) achieve new state of the art scores for image inpainting and class-conditional image synthesis and highly competitive performance on various tasks, including unconditional image generation, text-to-image synthesis, and super-resolution, while significantly reducing computational requirements compared to pixel-based DMs."
                        ],
                        "paper": {
                            "corpus_id": 245335280,
                            "title": "High-Resolution Image Synthesis with Latent Diffusion Models",
                            "authors": [
                                {
                                    "authorId": "1660819540",
                                    "name": "Robin Rombach"
                                },
                                {
                                    "authorId": "119843260",
                                    "name": "A. Blattmann"
                                },
                                {
                                    "authorId": "2053482699",
                                    "name": "Dominik Lorenz"
                                },
                                {
                                    "authorId": "35175531",
                                    "name": "Patrick Esser"
                                },
                                {
                                    "authorId": "1796707",
                                    "name": "B. Ommer"
                                }
                            ],
                            "year": 2021,
                            "venue": "Computer Vision and Pattern Recognition",
                            "n_citations": 15768
                        },
                        "score": 0
                    },
                    {
                        "id": "(Shi et al._1, 2024)",
                        "snippets": [
                            "The widespread deployment of such generative art models has brought about significant challenges concerning the risk for copyright infringement. The image generative models require large amount of training data, and it's impractical to only use noncopyrighted content to train the models. The model holder wants to leverage the high quality copyrighted data to learn the style and content and output high quality generated data. However, the generative models may generate some images that are very similar to the copyrighted training data, which may lead to copyright infringement [28]."
                        ],
                        "paper": {
                            "corpus_id": 273654195,
                            "title": "Copyright-Aware Incentive Scheme for Generative Art Models Using Hierarchical Reinforcement Learning",
                            "authors": [
                                {
                                    "authorId": "2319419519",
                                    "name": "Zhuan Shi"
                                },
                                {
                                    "authorId": "2328664079",
                                    "name": "Yifei Song"
                                },
                                {
                                    "authorId": "2318236128",
                                    "name": "Xiaoli Tang"
                                },
                                {
                                    "authorId": "2287820224",
                                    "name": "Lingjuan Lyu"
                                },
                                {
                                    "authorId": "2054858128",
                                    "name": "Boi Faltings"
                                }
                            ],
                            "year": 2024,
                            "venue": "arXiv.org",
                            "n_citations": 1
                        },
                        "score": 0.99267578125
                    },
                    {
                        "id": "(Chiba-Okabe et al., 2024)",
                        "snippets": [
                            "Generative models are trained on massive datasets that often contain copyrighted works and are capable of producing outputs closely resembling their training data, potentially resulting in violation of exclusive rights of copyright owners. Although some methods have been found effective to some extent, significant risks of copyright infringement remains."
                        ],
                        "paper": {
                            "corpus_id": 270258236,
                            "title": "Tackling copyright issues in AI image generation through originality estimation and genericization",
                            "authors": [
                                {
                                    "authorId": "2297800322",
                                    "name": "Hiroaki Chiba-Okabe"
                                },
                                {
                                    "authorId": "2278306561",
                                    "name": "Weijie J. Su"
                                }
                            ],
                            "year": 2024,
                            "venue": "Scientific Reports",
                            "n_citations": 1
                        },
                        "score": 0.98583984375
                    },
                    {
                        "id": "(Dogoulis et al., 2023)",
                        "snippets": [
                            "New advancements for the detection of synthetic images are critical for fighting disinformation, as the capabilities of generative AI models continuously evolve and can lead to hyper-realistic synthetic imagery at unprecedented scale and speed. In this paper, we focus on the challenge of generalizing across different concept classes, e.g., when training a detector on human faces and testing on synthetic animal images \u2013 highlighting the ineffectiveness of existing approaches that randomly sample generated images to train their models. By contrast, we propose an approach based on the premise that the robustness of the detector can be enhanced by training it on realistic synthetic images that are selected based on their quality scores according to a probabilistic quality estimation model. We demonstrate the effectiveness of the proposed approach by conducting experiments with generated images from two seminal architectures, StyleGAN2 and Latent Diffusion, and using three different concepts for each, so as to measure the cross-concept generalization ability. Our results show that our quality-based sampling method leads to higher detection performance for nearly all concepts, improving the overall effectiveness of the synthetic image detectors."
                        ],
                        "paper": {
                            "corpus_id": 258297834,
                            "title": "Improving Synthetically Generated Image Detection in Cross-Concept Settings",
                            "authors": [
                                {
                                    "authorId": "97760096",
                                    "name": "Pantelis Dogoulis"
                                },
                                {
                                    "authorId": "1403953272",
                                    "name": "Giorgos Kordopatis-Zilos"
                                },
                                {
                                    "authorId": "119661806",
                                    "name": "I. Kompatsiaris"
                                },
                                {
                                    "authorId": "144178604",
                                    "name": "S. Papadopoulos"
                                }
                            ],
                            "year": 2023,
                            "venue": "MAD@ICMR",
                            "n_citations": 19
                        },
                        "score": 0
                    }
                ],
                "format": "synthesis",
                "table": null,
                "model": "claude-3-7-sonnet-20250219"
            },
            {
                "title": "How Copyright Infringement Occurs in Generative Models",
                "tldr": "Copyright infringement in generative AI models occurs through two main mechanisms: memorization of training data leading to reproduction of copyrighted content, and the generation of outputs substantially similar to copyrighted works without proper attribution or permission. (11 sources)",
                "text": "\nGenerative models like Stable Diffusion and Midjourney can potentially infringe on copyright through several distinct mechanisms. The first and most direct mechanism is memorization, where models encode and reproduce verbatim copies of copyrighted material from their training data <Paper corpusId=\"256389993\" paperTitle=\"(Carlini et al., 2023)\" isShortName></Paper> <Paper corpusId=\"254366634\" paperTitle=\"(Somepalli et al., 2022)\" isShortName></Paper>. This goes beyond simple inspiration, as research has shown that diffusion models are capable of completely replicating their training data samples, producing outputs that are nearly identical to copyrighted works <Paper corpusId=\"267412857\" paperTitle=\"(Ren et al., 2024)\" isShortName></Paper>.\n\nThe second mechanism involves more subtle forms of infringement, where models create outputs that are \"substantially similar\" to copyrighted works even without verbatim copying <Paper corpusId=\"276558342\" paperTitle=\"(Liu et al., 2025)\" isShortName></Paper>. This is particularly concerning because U.S. copyright law, which influences laws in many countries, considers substantial similarity as grounds for infringement <Paper corpusId=\"276558342\" paperTitle=\"(Liu et al., 2025)\" isShortName></Paper>. Unlike human artists who might be inspired by existing works but create distinctly new content, generative models can inadvertently reproduce the characteristic elements of an artist's style across their body of work <Paper corpusId=\"269137659\" paperTitle=\"(Moayeri et al., 2024)\" isShortName></Paper>.\n\nCopyright infringement can occur at two distinct phases of the generative AI pipeline. During the training phase, algorithms directly access copyrighted material, potentially encoding verbatim copies within the model's weights <Paper corpusId=\"257050406\" paperTitle=\"(Vyas et al., 2023)\" isShortName></Paper>. Then, during the deployment phase, when users prompt the model to generate content, the outputs may be substantially similar to copyrighted training data without any clear provenance tracking, making it difficult for users to verify if outputs infringe on existing copyrights <Paper corpusId=\"257050406\" paperTitle=\"(Vyas et al., 2023)\" isShortName></Paper>.\n\nWhat makes this problem particularly concerning is that infringement can occur unintentionally, without either the user or developer specifically aiming to replicate copyrighted material <Paper corpusId=\"273023255\" paperTitle=\"(Chiba-Okabe, 2024)\" isShortName></Paper>. Additionally, the uninterpretable nature of generative models makes it challenging to provide direct evidence of copying, with access and similarity serving as primary evidence in potential legal disputes <Paper corpusId=\"273023255\" paperTitle=\"(Chiba-Okabe, 2024)\" isShortName></Paper>.\n\nRecent research has uncovered even more nuanced forms of potential copyright infringement. For instance, researchers have demonstrated that infringement can occur through disguised training samples that look drastically different from copyrighted content yet still induce the same effect when used to train latent diffusion models <Paper corpusId=\"269033217\" paperTitle=\"(Lu et al., 2024)\" isShortName></Paper>. Such concealed methods of infringement are particularly problematic as they can easily circumvent current auditing tools <Paper corpusId=\"269033217\" paperTitle=\"(Lu et al., 2024)\" isShortName></Paper>.\n\nThe risk of copyright infringement is amplified by the massive scale of training datasets like LAION, which contain billions of web-scale images that are impractical to thoroughly review or filter manually <Paper corpusId=\"278129333\" paperTitle=\"(Chen et al., 2025)\" isShortName></Paper>. This has led to real-world legal consequences, with several artists filing lawsuits against companies like Stability AI, DeviantArt, Midjourney, and Runway AI, arguing that their models function as \"21st-century collage tools\" that remix copyrighted works without permission <Paper corpusId=\"278129333\" paperTitle=\"(Chen et al., 2025)\" isShortName></Paper>.\n\nThe problem extends beyond just image reproduction to include fine-tuning methods such as DreamBooth, which can efficiently enable unauthorized parties to directly edit or modify source data to create new samples, potentially infringing on the copyright of original works <Paper corpusId=\"267412857\" paperTitle=\"(Ren et al., 2024)\" isShortName></Paper>. These developments underscore the urgent need for responsible AI image generation that addresses copyright concerns <Paper corpusId=\"265351912\" paperTitle=\"(Zhou et al., 2023)\" isShortName></Paper> and highlights the limitations of traditional copyright protection measures like watermarks and metadata in this new technological landscape <Paper corpusId=\"277955485\" paperTitle=\"(Rakib et al., 2025)\" isShortName></Paper>.",
                "citations": [
                    {
                        "id": "(Carlini et al., 2023)",
                        "snippets": [
                            "Image diffusion models such as DALL-E 2, Imagen, and Stable Diffusion have attracted significant attention due to their ability to generate high-quality synthetic images. In this work, we show that diffusion models memorize individual images from their training data and emit them at generation time. With a generate-and-filter pipeline, we extract over a thousand training examples from state-of-the-art models, ranging from photographs of individual people to trademarked company logos. We also train hundreds of diffusion models in various settings to analyze how different modeling and data decisions affect privacy. Overall, our results show that diffusion models are much less private than prior generative models such as GANs, and that mitigating these vulnerabilities may require new advances in privacy-preserving training."
                        ],
                        "paper": {
                            "corpus_id": 256389993,
                            "title": "Extracting Training Data from Diffusion Models",
                            "authors": [
                                {
                                    "authorId": "2483738",
                                    "name": "Nicholas Carlini"
                                },
                                {
                                    "authorId": "9200194",
                                    "name": "Jamie Hayes"
                                },
                                {
                                    "authorId": "3490923",
                                    "name": "Milad Nasr"
                                },
                                {
                                    "authorId": "40844378",
                                    "name": "Matthew Jagielski"
                                },
                                {
                                    "authorId": "3482535",
                                    "name": "Vikash Sehwag"
                                },
                                {
                                    "authorId": "2444919",
                                    "name": "Florian Tram\u00e8r"
                                },
                                {
                                    "authorId": "1718064",
                                    "name": "Borja Balle"
                                },
                                {
                                    "authorId": "7975935",
                                    "name": "Daphne Ippolito"
                                },
                                {
                                    "authorId": "145217343",
                                    "name": "Eric Wallace"
                                }
                            ],
                            "year": 2023,
                            "venue": "USENIX Security Symposium",
                            "n_citations": 617
                        },
                        "score": 0
                    },
                    {
                        "id": "(Somepalli et al., 2022)",
                        "snippets": [
                            "Cutting-edge diffusion models produce images with high quality and customizability, enabling them to be used for commercial art and graphic design purposes. But do diffusion models create unique works of art, or are they replicating content directly from their training sets? In this work, we study image retrieval frameworks that enable us to compare generated images with training samples and detect when content has been replicated. Applying our frameworks to diffusion models trained on multiple datasets including Oxford flowers, Celeb-A, ImageNet, and LAION, we discuss how factors such as training set size impact rates of content replication. We also identify cases where diffusion models, including the popular Stable Diffusion model, blatantly copy from their training data. Project page: https://somepago.github.io/diffrep.html"
                        ],
                        "paper": {
                            "corpus_id": 254366634,
                            "title": "Diffusion Art or Digital Forgery? Investigating Data Replication in Diffusion Models",
                            "authors": [
                                {
                                    "authorId": "2003112028",
                                    "name": "Gowthami Somepalli"
                                },
                                {
                                    "authorId": "1824188732",
                                    "name": "Vasu Singla"
                                },
                                {
                                    "authorId": "121592562",
                                    "name": "Micah Goldblum"
                                },
                                {
                                    "authorId": "8284185",
                                    "name": "Jonas Geiping"
                                },
                                {
                                    "authorId": "1962083",
                                    "name": "T. Goldstein"
                                }
                            ],
                            "year": 2022,
                            "venue": "Computer Vision and Pattern Recognition",
                            "n_citations": 329
                        },
                        "score": 0
                    },
                    {
                        "id": "(Ren et al., 2024)",
                        "snippets": [
                            "For the source data owner, which refers to the party or individual who owns the originality of image works, their data can be intentionally or unintentionally collected by model trainers as training samples to construct DGMs as introduced above. For example, recent studies [16]134] have demonstrated that popular DGMs are highly possible to completely replicate their training data samples, which is called memorization. \n\nThe possibility of data replication may severely offend the ownership of the original data samples. Moreover, the development of fine-tuning strategies such as DreamBooth can greatly improve the efficiency for unauthorized parties to directly edit or modify the source data to obtain new samples, which also severely infringes the copyright of the original works."
                        ],
                        "paper": {
                            "corpus_id": 267412857,
                            "title": "Copyright Protection in Generative AI: A Technical Perspective",
                            "authors": [
                                {
                                    "authorId": "2256589810",
                                    "name": "Jie Ren"
                                },
                                {
                                    "authorId": "2253881697",
                                    "name": "Han Xu"
                                },
                                {
                                    "authorId": "2185740224",
                                    "name": "Pengfei He"
                                },
                                {
                                    "authorId": "2218740984",
                                    "name": "Yingqian Cui"
                                },
                                {
                                    "authorId": "2253682835",
                                    "name": "Shenglai Zeng"
                                },
                                {
                                    "authorId": "2282560420",
                                    "name": "Jiankun Zhang"
                                },
                                {
                                    "authorId": "2256788829",
                                    "name": "Hongzhi Wen"
                                },
                                {
                                    "authorId": "46496977",
                                    "name": "Jiayuan Ding"
                                },
                                {
                                    "authorId": "2253533415",
                                    "name": "Hui Liu"
                                },
                                {
                                    "authorId": "2267019992",
                                    "name": "Yi Chang"
                                },
                                {
                                    "authorId": "2115879611",
                                    "name": "Jiliang Tang"
                                }
                            ],
                            "year": 2024,
                            "venue": "arXiv.org",
                            "n_citations": 42
                        },
                        "score": 0.99169921875
                    },
                    {
                        "id": "(Liu et al., 2025)",
                        "snippets": [
                            "Text-to-image generative models (Rombach et al., 2021)(Betker et al., 0)Team et al., 2023;(Esser et al., 2024)Hurst et al., 2024;(Zhang et al., 2023)a;Hintersdorf et al., 2024) have transformed creative industries by producing detailed visuals from text prompts. However, these models have been found to sometimes memorize and reproduce content from their training data (Carlini et al., 2023)(Somepalli et al., 2022)Ren et al., 2024;Wang et al., 2024c;Shi et al., 2024b;a;Zhang et al., 2024). This raises significant concerns about copyright infringement, especially when the generated images closely resemble existing copyrighted works. According to U.S. law (rot, 1970), also referenced by most countries, a work can be considered infringing if it constitutes substantial similarity to another work1 . Therefore, determining whether AI-generated images infringe on copyright requires a clear and reliable method to compare them with copyrighted materials to identify substantial similarity."
                        ],
                        "paper": {
                            "corpus_id": 276558342,
                            "title": "CopyJudge: Automated Copyright Infringement Identification and Mitigation in Text-to-Image Diffusion Models",
                            "authors": [
                                {
                                    "authorId": "2346891526",
                                    "name": "Shunchang Liu"
                                },
                                {
                                    "authorId": "2319419519",
                                    "name": "Zhuan Shi"
                                },
                                {
                                    "authorId": "2287820224",
                                    "name": "Lingjuan Lyu"
                                },
                                {
                                    "authorId": "2344619001",
                                    "name": "Yaochu Jin"
                                },
                                {
                                    "authorId": "2054858128",
                                    "name": "Boi Faltings"
                                }
                            ],
                            "year": 2025,
                            "venue": "arXiv.org",
                            "n_citations": 2
                        },
                        "score": 0.99169921875
                    },
                    {
                        "id": "(Moayeri et al., 2024)",
                        "snippets": [
                            "Recent text-to-image generative models such as Stable Diffusion are extremely adept at mimicking and generating copyrighted content, raising concerns amongst artists that their unique styles may be improperly copied. Understanding how generative models copy\"artistic style\"is more complex than duplicating a single image, as style is comprised by a set of elements (or signature) that frequently co-occurs across a body of work, where each individual work may vary significantly."
                        ],
                        "paper": {
                            "corpus_id": 269137659,
                            "title": "Rethinking Artistic Copyright Infringements in the Era of Text-to-Image Generative Models",
                            "authors": [
                                {
                                    "authorId": "104644443",
                                    "name": "Mazda Moayeri"
                                },
                                {
                                    "authorId": "2114710333",
                                    "name": "Samyadeep Basu"
                                },
                                {
                                    "authorId": "144021807",
                                    "name": "S. Balasubramanian"
                                },
                                {
                                    "authorId": "1962835975",
                                    "name": "Priyatham Kattakinda"
                                },
                                {
                                    "authorId": "2188780004",
                                    "name": "Atoosa Malemir Chegini"
                                },
                                {
                                    "authorId": "69357142",
                                    "name": "R. Brauneis"
                                },
                                {
                                    "authorId": "34389431",
                                    "name": "S. Feizi"
                                }
                            ],
                            "year": 2024,
                            "venue": "arXiv.org",
                            "n_citations": 4
                        },
                        "score": 0.99267578125
                    },
                    {
                        "id": "(Vyas et al., 2023)",
                        "snippets": [
                            "Generative models for images, text, code, and other domains pose new challenges for ensuring their outputs are protected from copyright infringement. Such models are trained on a large corpus of data, where it is often impractical to ensure the training set is 100% free of copyrighted material. Furthermore, removing copyrighted material from training may also be undesirable. For example, a human author is free to read and use copyrighted material as inspiration for their work, as long as they do not copy it. Similarly, it may be beneficial to use copyrighted material when training in order to have more effective generative models.\n\nCopyright infringement by generative models can potentially arise in (at least) two manners. First, in the training phase, the algorithm could directly access copyrighted material, and the learned model itself could implicitly contain (e.g. coded in its weights) verbatim copies of some of this material. The copyright issues arising during training share many similarities with other settings in which algorithms scrape significant amounts of data, including search-engine indexing and digitizing books. Here, the question of what constitutes a copyright infringement is largely a question of \"fair use.\" This work does not examine these fair use issues that arise in the training phase, and we refer the reader to the several legal precedents in this area [Samuelson, 2021].\n\nThe second notable source of potential infringement is in the deployment phase, where a user provides a prompt x to the model to obtain some output y. Apriori, we cannot rule out the possibility that y is either a verbatim copy or substantially similar to some copyrighted training data. Moreover, unlike search engines, generative models do not keep track of the provenance of their outputs. Hence, a user of such an output y (e.g., a software company using generated code, or a designer using a generated image) has no easy way to verify that it does not infringe upon any copyrighted material."
                        ],
                        "paper": {
                            "corpus_id": 257050406,
                            "title": "On Provable Copyright Protection for Generative Models",
                            "authors": [
                                {
                                    "authorId": "145603901",
                                    "name": "Nikhil Vyas"
                                },
                                {
                                    "authorId": "144695232",
                                    "name": "S. Kakade"
                                },
                                {
                                    "authorId": "1697211",
                                    "name": "B. Barak"
                                }
                            ],
                            "year": 2023,
                            "venue": "International Conference on Machine Learning",
                            "n_citations": 94
                        },
                        "score": 0.9892578125
                    },
                    {
                        "id": "(Chiba-Okabe, 2024)",
                        "snippets": [
                            "In cases where generative AI is used to create content, it is typically provided as a service by the AI developer and utilized by a user, and in such situations, both the user and the AI developer may potentially be considered direct or indirect infringers, depending on the specific circumstances [28]. Copyright infringement may occur even without the user or developer intending to replicate the copyrighted material or to create something similar [28]. \n\nDue to the uninterpretability of generative models, it can be challenging to provide direct evidence of factual copying for AI-generated content. In most cases, access and similarity will likely serve as the primary evidence of infringement, much like in traditional copyright disputes [28]. While the creative processes of generative models differ markedly from human creativity, it appears that similar evidentiary principles are likely to apply even when generative AI is involved in the creative processes. At a minimum, it is generally recognized that some form of access to the original work, which may be demonstrated by its inclusion in the training data, is necessary to establish copying, even for generative AI outputs [28,29,36]. Moreover, studies have shown that exposure to certain types of content during training can lead to a high likelihood of these models reproducing or closely imitating that content (Carlini et al., 2020)(Carlini et al., 2023)[39], suggesting that inclusion of specific data correlates with a greater probability of producing similar outputs through copying. This echoes the evidentiary framework applicable to traditional copyright cases, where greater similarity between works strengthens the inference of access and copying. \n\nA distinctive feature of generative models, setting them apart from human artists or traditional creative tools, is their capacity to be trained on immense and diverse datasets, which are often created by scraping content from the Internet. Furthermore, direct inclusion of copyrighted material in the training data is not even necessary for latent diffusion models to produce images similar to copyrighted works, as these models can reproduce content from other images that retain similar latent information [40], which exacerbates the problem. These characteristics suggest a high likelihood that generative models have what can be construed as \"access\" to copyrighted works."
                        ],
                        "paper": {
                            "corpus_id": 273023255,
                            "title": "Probabilistic Analysis of Copyright Disputes and Generative AI Safety",
                            "authors": [
                                {
                                    "authorId": "2297800322",
                                    "name": "Hiroaki Chiba-Okabe"
                                }
                            ],
                            "year": 2024,
                            "venue": "arXiv.org",
                            "n_citations": 2
                        },
                        "score": 0.98876953125
                    },
                    {
                        "id": "(Lu et al., 2024)",
                        "snippets": [
                            "Copyright infringement may occur when a generative model produces samples substantially similar to some copyrighted data that it had access to during the training phase. The notion of access usually refers to including copyrighted samples directly in the training dataset, which one may inspect to identify an infringement. We argue that such visual auditing largely overlooks a concealed copyright infringement, where one constructs a disguise that looks drastically different from the copyrighted sample yet still induces the effect of training Latent Diffusion Models on it. Such disguises only require indirect access to the copyrighted material and cannot be visually distinguished, thus easily circumventing the current auditing tools."
                        ],
                        "paper": {
                            "corpus_id": 269033217,
                            "title": "Disguised Copyright Infringement of Latent Diffusion Models",
                            "authors": [
                                {
                                    "authorId": "2275053301",
                                    "name": "Yiwei Lu"
                                },
                                {
                                    "authorId": "2284800079",
                                    "name": "Matthew Y.R. Yang"
                                },
                                {
                                    "authorId": "2295948127",
                                    "name": "Zuoqiu Liu"
                                },
                                {
                                    "authorId": "2284763541",
                                    "name": "Gautam Kamath"
                                },
                                {
                                    "authorId": "2274963165",
                                    "name": "Yaoliang Yu"
                                }
                            ],
                            "year": 2024,
                            "venue": "International Conference on Machine Learning",
                            "n_citations": 8
                        },
                        "score": 0.9833984375
                    },
                    {
                        "id": "(Chen et al., 2025)",
                        "snippets": [
                            "Leveraging classifier-free guidance (CFG) [12], text-toimage diffusion models like Stable Diffusion (Rombach et al., 2021) and Midjourney [1] are now capable of generating highly realistic images that closely align with user-provided text prompts. This capability has propelled their popularity, leading to widespread use and distribution of their generated images. However, recent research (Carlini et al., 2023)8,(Somepalli et al., 2022)[34] has revealed a critical issue: these models can memorize training data, leading them to reproduce parts of images, such as foregrounds or backgrounds (local memorization, see Fig. 1), or even entire images (global memorization, see Fig. 2) during inference, instead of generating genuinely novel content. When the training data includes sensitive or copyrighted material, these memorization issues can infringe on copyright laws without notifying either the model's owners or users or the copyright holders of the replicated content. The risk is es-pecially substantial given the extensive use of these models and their reliance on massive datasets, such as LAION [30], which contain billions of web-scale images and are impractical to thoroughly review or filter manually. This risk is further highlighted by real-world cases, where several artists have filed lawsuits, arguing that models like Stable Diffusion act as \"21st-century collage tools\" that remix their copyrighted works, implicating Stability AI, DeviantArt, Midjourney, and Runway AI. Recognizing the potential for unauthorized reproductions, Midjourney has even banned prompts containing the term \"Afghan\" to prevent the generation of the copyrighted Afghan Girl photograph. Yet, as (Wen et al., 2023) demonstrates, such restrictions alone are insufficient to fully prevent the reproduction of copyrighted images."
                        ],
                        "paper": {
                            "corpus_id": 278129333,
                            "title": "Enhancing Privacy-Utility Trade-offs to Mitigate Memorization in Diffusion Models",
                            "authors": [
                                {
                                    "authorId": "1696291",
                                    "name": "Chen Chen"
                                },
                                {
                                    "authorId": "51023221",
                                    "name": "Daochang Liu"
                                },
                                {
                                    "authorId": "2302950741",
                                    "name": "Mubarak Shah"
                                },
                                {
                                    "authorId": "2288626806",
                                    "name": "Chang Xu"
                                }
                            ],
                            "year": 2025,
                            "venue": "arXiv.org",
                            "n_citations": 1
                        },
                        "score": 0.99169921875
                    },
                    {
                        "id": "(Zhou et al., 2023)",
                        "snippets": [
                            "However, these AI-generated artworks inherit the characteristics of the images that are used to train models [33,40,43], which might be pretty similar to the original ones, as shown in Figure 1. Such similarity has aroused concerns about copyright infringement disputes. For example, three artists (Sarah Andersen, Kelly McKernan, and Karla Ortiz) have recently accused Stable Diffusion of unlawfully scraping copyrighted images from the Internet to mimic their art styles [11]. To this end, research on responsible AI image generation is in urgent need to address such copyright issues."
                        ],
                        "paper": {
                            "corpus_id": 265351912,
                            "title": "CopyScope: Model-level Copyright Infringement Quantification in the Diffusion Workflow",
                            "authors": [
                                {
                                    "authorId": "2267879159",
                                    "name": "Junlei Zhou"
                                },
                                {
                                    "authorId": "2149258131",
                                    "name": "Jiashi Gao"
                                },
                                {
                                    "authorId": "2184222659",
                                    "name": "Ziwei Wang"
                                },
                                {
                                    "authorId": "2255554914",
                                    "name": "Xuetao Wei"
                                }
                            ],
                            "year": 2023,
                            "venue": "arXiv.org",
                            "n_citations": 2
                        },
                        "score": 0.998046875
                    },
                    {
                        "id": "(Rakib et al., 2025)",
                        "snippets": [
                            "In today's age of social media and marketing, copyright issues can be a major roadblock to the free sharing of images. Generative AI models have made it possible to create high-quality images, but concerns about copyright infringement are a hindrance to their abundant use. As these models use data from training images to generate new ones, it is often a daunting task to ensure they do not violate intellectual property rights. Some AI models have even been noted to directly copy copyrighted images, a problem often referred to as source copying. Traditional copyright protection measures such as watermarks and metadata have also proven to be futile in this regard",
                            ".Diffusion models have revolutionized image generation, surpassing predecessors like GANs and VAEs in both fidelity and performance. However, this progress has been shadowed by a critical challenge: source copying. This issue raises serious concerns regarding privacy and intellectual property rights, demanding innovative solutions that preserve content quality while enhancing copyright protection."
                        ],
                        "paper": {
                            "corpus_id": 277955485,
                            "title": "TWIG: Two-Step Image Generation using Segmentation Masks in Diffusion Models",
                            "authors": [
                                {
                                    "authorId": "2313107575",
                                    "name": "Mazharul Islam Rakib"
                                },
                                {
                                    "authorId": "2357491645",
                                    "name": "Showrin Rahman"
                                },
                                {
                                    "authorId": "2113939573",
                                    "name": "J. Mondal"
                                },
                                {
                                    "authorId": "2349232014",
                                    "name": "Xi Xiao"
                                },
                                {
                                    "authorId": "2357414290",
                                    "name": "David Lewis"
                                },
                                {
                                    "authorId": "2356548661",
                                    "name": "Alessandra Mileo"
                                },
                                {
                                    "authorId": "2210516789",
                                    "name": "Meem Arafat Manab"
                                }
                            ],
                            "year": 2025,
                            "venue": "arXiv.org",
                            "n_citations": 1
                        },
                        "score": 0.994140625
                    }
                ],
                "format": "synthesis",
                "table": null,
                "model": "claude-3-7-sonnet-20250219"
            },
            {
                "title": "Legal Cases and Lawsuits",
                "tldr": "Several high-profile lawsuits have been filed against generative AI companies for copyright infringement, with artists, photographers, and stock image providers seeking legal remedies for unauthorized use of their works in model training. These cases highlight the growing legal tensions between copyright holders and AI developers, with some courts already allowing claims to proceed. (8 sources)",
                "text": "\n## Notable Copyright Infringement Lawsuits\n\n* **Getty Images v. Stability AI**: One of the most prominent legal challenges came from Getty Images, which sued Stability AI (creators of Stable Diffusion) for allegedly reproducing over 12 million Getty Images along with their captions and metadata without permission or compensation <Paper corpusId=\"267400526\" paperTitle=\"(Zhuang, 2024)\" isShortName></Paper>. This case represents a significant commercial entity taking action against AI developers <Paper corpusId=\"266900037\" paperTitle=\"(Wang et al., 2024)\" isShortName></Paper>.\n\n* **Artists' Collective Lawsuits**: Multiple artists have filed collective lawsuits against Stability AI, DeviantArt, and Midjourney, claiming these companies trained their models on artists' works without consent, allowing the AI to replicate their distinctive styles and content <Paper corpusId=\"265352103\" paperTitle=\"(Zhang et al., 2023)\" isShortName></Paper>. In one notable case, painter Erin Hanson provided substantial evidence that AI-generated images plagiarized his work <Paper corpusId=\"267400526\" paperTitle=\"(Zhuang, 2024)\" isShortName></Paper>.\n\n* **Legal Progress**: In August 2024, U.S. District Judge Orrick ruled that artists could proceed with copyright infringement claims against Stability AI, Midjourney, and DeviantArt, signaling that courts are taking these concerns seriously <Paper corpusId=\"278338968\" paperTitle=\"(Reissinger et al., 2025)\" isShortName></Paper>. This ruling marked a significant development in establishing legal precedent for AI copyright cases.\n\n* **Character Copyright Cases**: Studies have shown that image generation models can be prompted to reproduce copyrighted characters (such as Mario or Batman), which has already resulted in at least one lawsuit awarding damages based on the generation of such characters <Paper corpusId=\"270620122\" paperTitle=\"(He et al., 2024)\" isShortName></Paper>. This has prompted commercial services like DALL-E to implement protective interventions.\n\n* **Emerging Legal Framework**: These lawsuits highlight growing concerns about whether the high-quality content synthesized by generative AI models is excessively similar to copyrighted training data, potentially violating the rights of numerous artists and copyright holders <Paper corpusId=\"272146279\" paperTitle=\"(Shi et al., 2024)\" isShortName></Paper> <Paper corpusId=\"245335280\" paperTitle=\"(Rombach et al., 2021)\" isShortName></Paper>. The cases are forcing courts to address the unique legal challenges posed by AI models that can memorize and reproduce aspects of their training data.\n\n* **Broader Implications**: These legal challenges are occurring as the boundary between human creativity and machine-generated content becomes increasingly blurred, raising crucial questions about legal and ethical implications of using generative models to produce art <Paper corpusId=\"260155285\" paperTitle=\"(Leotta et al., 2023)\" isShortName></Paper>. The outcomes of these cases will likely influence how copyright law adapts to AI-generated content.",
                "citations": [
                    {
                        "id": "(Zhuang, 2024)",
                        "snippets": [
                            "Stable Diffusion can quickly generate high-quality images based on simple text descriptions, thanks to its training on billions of sample images collected online. However, not all of these sample images fall within the public domain; some are even protected by copyright. As one can imagine, the authors and photographers of these copyrighted images are not pleased with the actions of Stable Diffusion.\n\nStable Diffusion has faced multiple lawsuits due to copyright issues. Artists and photographers have initiated collective lawsuits against this technology, and one of the world's leading image suppliers, Getty Images Holdings Inc., has also filed a similar case. According to Getty's (2023) complaint, \"Stability AI has reproduced over 12 million Getty Images, along with their related captions and metadata, without permission or compensation.\"",
                            "After exploring how artificial intelligence (AI)-generated platforms work, learn what processes may involve human artist infringement in the course of their work. So let's talk about where these workflows infringe on human artists' copyrighted works -Artificial intelligence (AI)-generated content raises concerns about copyright infringement of human artist works in several ways.\n\nThe researchers summarized three aspects of human artist infringement: model training using only copyrighted works without permission. Take Stable Diffusion, which has been involved in many copyright disputes. It uses the image database containing hundreds of millions-LAION-5B (Kaixin Zhu, & Yiqun Zhang. ( 2023)) as the source of training data, and it does not need the consent of the copyright owner [5].\n\nSecond, there is a lot of plagiarism and adaptation of copyrighted works in AI-generated images. In a case in which an artist sued an AI-generated platform, the painter Erin Hanson provided strong evidence that the AI-generated images plagiarized his work. And there are many such lawsuits now, not a few. Third, the images generated by the AI generation platform, such as Stable Diffusion, if works infringe copyright works, do not have any watermarks or source marks, making it even more difficult for copyright owners to accept."
                        ],
                        "paper": {
                            "corpus_id": 267400526,
                            "title": "AIGC (Artificial Intelligence Generated Content) infringes the copyright of human artists",
                            "authors": [
                                {
                                    "authorId": "2282411198",
                                    "name": "Lyulin Zhuang"
                                }
                            ],
                            "year": 2024,
                            "venue": "Applied and Computational Engineering",
                            "n_citations": 2
                        },
                        "score": 0.9912109375
                    },
                    {
                        "id": "(Wang et al., 2024)",
                        "snippets": [
                            "In a copyright infringement attack, the attacker, who is the copyright owner of some creations (such as images, poems, etc.), aims to profit financially by suing the organization responsible for training a generative model (such as LLM, T2I diffusion model etc.) for copyright infringement. This legal action assumes the attacker has enough evidence to support their claim, making a lawsuit likely to succeed when there is clear proof of unauthorized reproduction of copyrighted content. A real-world example illustrating this scenario is the lawsuit filed by Getty Images against the AI art generator Stable Diffusion in the United States for copyright infringement (Vincent, 2023)."
                        ],
                        "paper": {
                            "corpus_id": 266900037,
                            "title": "The Stronger the Diffusion Model, the Easier the Backdoor: Data Poisoning to Induce Copyright Breaches Without Adjusting Finetuning Pipeline",
                            "authors": [
                                {
                                    "authorId": "2267866973",
                                    "name": "Haonan Wang"
                                },
                                {
                                    "authorId": "2257038423",
                                    "name": "Qianli Shen"
                                },
                                {
                                    "authorId": "2278794984",
                                    "name": "Yao Tong"
                                },
                                {
                                    "authorId": "2267877984",
                                    "name": "Yang Zhang"
                                },
                                {
                                    "authorId": "2256995496",
                                    "name": "Kenji Kawaguchi"
                                }
                            ],
                            "year": 2024,
                            "venue": "International Conference on Machine Learning",
                            "n_citations": 32
                        },
                        "score": 0.98828125
                    },
                    {
                        "id": "(Zhang et al., 2023)",
                        "snippets": [
                            "Diffusion models excel in many generative modeling tasks, notably in creating images from text prompts, a task referred to as text-to-image (T2I) generation. Despite the ability to generate high-quality images, these models often replicate elements from their training data, leading to increasing copyright concerns in real applications in recent years. In response to this raising concern about copyright infringement, recent studies have studied the copyright behavior of diffusion models when using direct, copyrighted prompts.\n\nOur research extends this by examining subtler forms of infringement, where even indirect prompts can trigger copyright issues.\n\nThe apprehension surrounding copyright protection in diffusion models has also evolved into a tangible threat, as multiple lawsuits related to copyright infringement have been initiated against companies that utilize diffusion models for commercial purposes. Notably, there have been instances of lawsuits: Stability AI and Mid-Journey are both facing civil suits for training their models on artists' work without their consent thereby allowing their models to replicate the style and work of such artists [Vincent, 2023].\n\nFor copyright infringement, we focus on the copyright regulation in the US [Copyright Office, 2022]. Nevertheless, the concept of copyright applies to the regulation in other countries. In the context of copyright law in the US, whether a piece of copyrighted material can be used by others is governed by the concept of Fair Use which permits the use of copyrighted material only in a transformative manner that is distinct from the original work. There have been legal precedents which demonstrate that structural similarity can lead to infringement claims. Given that such generative models are trained on datasets such as LAION-5B [Schuhmann et al., 2022] which contains publicly available copyrighted data, if the models generate images with visual features that have substantial structural similarity to the original copyrighted images, this would likely be grounds for claims of copyright infringement."
                        ],
                        "paper": {
                            "corpus_id": 265352103,
                            "title": "On Copyright Risks of Text-to-Image Diffusion Models",
                            "authors": [
                                {
                                    "authorId": "2267877984",
                                    "name": "Yang Zhang"
                                },
                                {
                                    "authorId": "2267728071",
                                    "name": "Teoh Tze Tzun"
                                },
                                {
                                    "authorId": "2267727392",
                                    "name": "Lim Wei Hern"
                                },
                                {
                                    "authorId": "2267866973",
                                    "name": "Haonan Wang"
                                },
                                {
                                    "authorId": "2256995496",
                                    "name": "Kenji Kawaguchi"
                                }
                            ],
                            "year": 2023,
                            "venue": "",
                            "n_citations": 10
                        },
                        "score": 0.9970703125
                    },
                    {
                        "id": "(Reissinger et al., 2025)",
                        "snippets": [
                            "As generative AI (GenAI) becomes increasingly prevalent in realworld applications, concerns about its potential risks continue to grow. We focus on the risks associated with the replication of copyrighted material and restrict ourselves to image generation. The tangible legal and financial implications of these risks have sparked our current research. For instance, artists sued Stability AI (Midjourney and DevianArt) claiming that the companies' AI image generators produce images that are strikingly similar to their artworks [2]. In August 2024, a U.S. district judge (Orrick) ruled that the artists could proceed with the copyright infringement claims, underscoring the ongoing legal uncertainties surrounding AI-generated content [6]."
                        ],
                        "paper": {
                            "corpus_id": 278338968,
                            "title": "Safer Prompts: Reducing IP Risk in Visual Generative AI",
                            "authors": [
                                {
                                    "authorId": "2359254079",
                                    "name": "Lena Reissinger"
                                },
                                {
                                    "authorId": "2256011059",
                                    "name": "Yuanyuan Li"
                                },
                                {
                                    "authorId": "23107750",
                                    "name": "Anna Haensch"
                                },
                                {
                                    "authorId": "2254269177",
                                    "name": "Neeraj Sarna"
                                }
                            ],
                            "year": 2025,
                            "venue": "arXiv.org",
                            "n_citations": 0
                        },
                        "score": 0.99267578125
                    },
                    {
                        "id": "(He et al., 2024)",
                        "snippets": [
                            "Recent studies show that image and video generation models can be prompted to reproduce copyrighted content from their training data, raising serious legal concerns about copyright infringement. Copyrighted characters (e.g., Mario, Batman) present a significant challenge: at least one lawsuit has already awarded damages based on the generation of such characters. Consequently, commercial services like DALL-E have started deploying interventions."
                        ],
                        "paper": {
                            "corpus_id": 270620122,
                            "title": "Fantastic Copyrighted Beasts and How (Not) to Generate Them",
                            "authors": [
                                {
                                    "authorId": "2294507804",
                                    "name": "Luxi He"
                                },
                                {
                                    "authorId": "2283305597",
                                    "name": "Yangsibo Huang"
                                },
                                {
                                    "authorId": "2304129935",
                                    "name": "Weijia Shi"
                                },
                                {
                                    "authorId": "2144071564",
                                    "name": "Tinghao Xie"
                                },
                                {
                                    "authorId": "2308072184",
                                    "name": "Haotian Liu"
                                },
                                {
                                    "authorId": "2307621989",
                                    "name": "Yue Wang"
                                },
                                {
                                    "authorId": "2137813791",
                                    "name": "Luke S. Zettlemoyer"
                                },
                                {
                                    "authorId": "2309481623",
                                    "name": "Chiyuan Zhang"
                                },
                                {
                                    "authorId": "50536468",
                                    "name": "Danqi Chen"
                                },
                                {
                                    "authorId": "2254262712",
                                    "name": "Peter Henderson"
                                }
                            ],
                            "year": 2024,
                            "venue": "International Conference on Learning Representations",
                            "n_citations": 12
                        },
                        "score": 0.99267578125
                    },
                    {
                        "id": "(Shi et al., 2024)",
                        "snippets": [
                            "These models utilize extensive training data that may include copyrighted works, which they are sometimes capable of memorizing (Carlini et al. 2023). This ability can result in the pro- duction of images that closely resemble protected content (See in Figure 1), posing significant challenges to copyright protection (Dogoulis et al., 2023). Recent legal cases, such as those involving Stable Diffusion (Rombach et al., 2021) and Midjourney(Mansour 2023), highlight concerns over the use of copyrighted data in AI training, where the models potentially infringe on the rights of numerous artists. These cases highlight a growing concern: Could the high-quality content synthesized by these generative AIs be excessively similar to copyrighted training data, potentially violating the rights of copyright holders?"
                        ],
                        "paper": {
                            "corpus_id": 272146279,
                            "title": "RLCP: A Reinforcement Learning-based Copyright Protection Method for Text-to-Image Diffusion Model",
                            "authors": [
                                {
                                    "authorId": "2319419519",
                                    "name": "Zhuan Shi"
                                },
                                {
                                    "authorId": "2318391138",
                                    "name": "Jing Yan"
                                },
                                {
                                    "authorId": "2318236128",
                                    "name": "Xiaoli Tang"
                                },
                                {
                                    "authorId": "2287820224",
                                    "name": "Lingjuan Lyu"
                                },
                                {
                                    "authorId": "2054858128",
                                    "name": "Boi Faltings"
                                }
                            ],
                            "year": 2024,
                            "venue": "arXiv.org",
                            "n_citations": 1
                        },
                        "score": 0.98779296875
                    },
                    {
                        "id": "(Rombach et al., 2021)",
                        "snippets": [
                            "By decomposing the image formation process into a sequential application of denoising autoencoders, diffusion models (DMs) achieve state-of-the-art synthesis results on image data and beyond. Additionally, their formulation allows for a guiding mechanism to control the image generation process without retraining. However, since these models typically operate directly in pixel space, optimization of powerful DMs often consumes hundreds of GPU days and inference is expensive due to sequential evaluations. To enable DM training on limited computational resources while retaining their quality and flexibility, we apply them in the latent space of powerful pretrained autoencoders. In contrast to previous work, training diffusion models on such a representation allows for the first time to reach a near-optimal point between complexity reduction and detail preservation, greatly boosting visual fidelity. By introducing cross-attention layers into the model architecture, we turn diffusion models into powerful and flexible generators for general conditioning inputs such as text or bounding boxes and high-resolution synthesis becomes possible in a convolutional manner. Our latent diffusion models (LDMs) achieve new state of the art scores for image inpainting and class-conditional image synthesis and highly competitive performance on various tasks, including unconditional image generation, text-to-image synthesis, and super-resolution, while significantly reducing computational requirements compared to pixel-based DMs."
                        ],
                        "paper": {
                            "corpus_id": 245335280,
                            "title": "High-Resolution Image Synthesis with Latent Diffusion Models",
                            "authors": [
                                {
                                    "authorId": "1660819540",
                                    "name": "Robin Rombach"
                                },
                                {
                                    "authorId": "119843260",
                                    "name": "A. Blattmann"
                                },
                                {
                                    "authorId": "2053482699",
                                    "name": "Dominik Lorenz"
                                },
                                {
                                    "authorId": "35175531",
                                    "name": "Patrick Esser"
                                },
                                {
                                    "authorId": "1796707",
                                    "name": "B. Ommer"
                                }
                            ],
                            "year": 2021,
                            "venue": "Computer Vision and Pattern Recognition",
                            "n_citations": 15768
                        },
                        "score": 0
                    },
                    {
                        "id": "(Leotta et al., 2023)",
                        "snippets": [
                            "The rapid advancement of generative models, particularly Diffusion Models [7], has led to a surge in high-quality, realistic image generation. These models have demonstrated immense potential for creative applications across various domains, including art, design, and advertising. However, their ability to replicate the styles of specific artists raises concerns about Intellectual Property (IP) rights and potential copyright infringements [20,25,19]. \n\nAs the boundary between human creativity and machine-generated content becomes increasingly blurred, it is crucial to address the legal and ethical implications of using generative models to produce art, as well as to develop methods that evaluate the extent to which generated images are influenced by the works of real artists and ensure the protection of their intellectual property. \n\nSeveral studies and articles have explored the legal ramifications of generative models and their potential to infringe on copyrights. In an article dated 2021 the challenges in determining copyright ownership for AI-generated works were discussed and authors emphasized the need for legal frameworks that could adequately address the unique nature of generative models. MLQ.AI also reported on a copyright infringement case involving generative AI, which sparked debates on the responsibilities of AI developers and users in protecting original creators' rights. In response to these concerns, legal scholars have delved into the complexities of copyright law as it pertains to AI-generated artworks. Gillotte [8] examined the challenges in assigning liability and protecting IP rights, arguing that existing copyright laws may not be sufficient to address the unique characteristics of AI-generated contents. Indeed, it is became a copyright dilemma and the need for a balance between innovation and IP protection is arising to ensure that creative works are safeguarded without stifling technological advancements [16]."
                        ],
                        "paper": {
                            "corpus_id": 260155285,
                            "title": "Not with my name! Inferring artists' names of input strings employed by Diffusion Models",
                            "authors": [
                                {
                                    "authorId": "47253043",
                                    "name": "R. Leotta"
                                },
                                {
                                    "authorId": "1797543",
                                    "name": "Oliver Giudice"
                                },
                                {
                                    "authorId": "40010524",
                                    "name": "Luca Guarnera"
                                },
                                {
                                    "authorId": "1742452",
                                    "name": "S. Battiato"
                                }
                            ],
                            "year": 2023,
                            "venue": "International Conference on Image Analysis and Processing",
                            "n_citations": 7
                        },
                        "score": 0.982421875
                    }
                ],
                "format": "list",
                "table": null,
                "model": "claude-3-7-sonnet-20250219"
            },
            {
                "title": "Proposed Solutions and Defenses",
                "tldr": "Multiple technical approaches have been developed to address copyright infringement in generative AI, including watermarking techniques, content filtering systems, and constraint-based generation methods that prevent models from reproducing protected content. (11 sources)",
                "text": "\nAs concerns about copyright infringement in generative AI grow, researchers and developers have proposed several technical solutions to mitigate these risks. One prominent approach involves digital watermarking techniques for neural networks. These methods aim to protect the intellectual property rights of model owners by enabling verification of model ownership and tracking potential misuse <Paper corpusId=\"265066820\" paperTitle=\"(Fei et al., 2023)\" isShortName></Paper>. This is particularly important in commercial settings where generative models are licensed to users who might violate license agreements or engage in malicious activities.\n\nContent filtering and concept removal represent another significant defense strategy. Researchers have developed methods to continuously remove improper concepts from generative models to prevent copyright infringement and other misuse, such as mimicking specific artistic styles or generating inappropriate content <Paper corpusId=\"274436153\" paperTitle=\"(Han et al., 2024)\" isShortName></Paper> <Paper corpusId=\"245335280\" paperTitle=\"(Rombach et al., 2021)\" isShortName></Paper> <Paper corpusId=\"248986576\" paperTitle=\"(Saharia et al., 2022)\" isShortName></Paper> <Paper corpusId=\"253420366\" paperTitle=\"(Schramowski et al., 2022)\" isShortName></Paper>. These approaches are particularly important for preventing text-to-image models from generating content that infringes on artistic styles or intellectual properties.\n\nSome developers have focused on removing copyrighted images from training datasets entirely as a preventive measure <Paper corpusId=\"274436785\" paperTitle=\"(Guo et al., 2024)\" isShortName></Paper> <Paper corpusId=\"257050406\" paperTitle=\"(Vyas et al., 2023)\" isShortName></Paper>. This approach aims to avoid potential copyright issues by ensuring models never learn from protected content. However, this can be challenging to implement comprehensively given the scale of training datasets and the difficulty in identifying all copyrighted materials.\n\nMore sophisticated approaches include constraint-based generation techniques that guide diffusion models away from producing content that resembles copyrighted material. For example, researchers have implemented latent constrained models that selectively modify generated content during the initial stages of denoising to prevent copyright infringement while maintaining overall image quality <Paper corpusId=\"276250558\" paperTitle=\"(Zampini et al., 2025)\" isShortName></Paper>. In one implementation, this method reduced the generation of protected cartoon characters from 33% to just 10% of outputs, with minimal impact on image quality as measured by FID scores.\n\nThe development of formal frameworks for copyright-safe generation is also emerging. Some researchers have proposed the concept of \"near access-freeness\" (NAF), which provides theoretical bounds on the probability that a model outputs samples similar to copyrighted data, even when such data was included in training <Paper corpusId=\"274436785\" paperTitle=\"(Guo et al., 2024)\" isShortName></Paper> <Paper corpusId=\"257050406\" paperTitle=\"(Vyas et al., 2023)\" isShortName></Paper>. These frameworks offer mathematical guarantees about a model's behavior regarding protected content.\n\nDespite these advances, traditional copyright protection measures like watermarks and metadata have proven largely ineffective in the context of generative AI <Paper corpusId=\"277955485\" paperTitle=\"(Rakib et al., 2025)\" isShortName></Paper>. The challenge of balancing copyright protection with the creative capabilities of generative models remains significant, particularly as advanced few-shot generation techniques like DreamBooth enable efficient art imitation and style transfer with minimal training data <Paper corpusId=\"268513090\" paperTitle=\"(Wu et al., 2024)\" isShortName></Paper> <Paper corpusId=\"251800180\" paperTitle=\"(Ruiz et al., 2022)\" isShortName></Paper>.\n\nThe quest for effective defense mechanisms must continue as models become more sophisticated and backdoor attacks against diffusion models become more prevalent <Paper corpusId=\"274436785\" paperTitle=\"(Guo et al., 2024)\" isShortName></Paper>. These ongoing research efforts highlight the importance of developing solutions that preserve the generative capabilities of AI models while respecting copyright protections and intellectual property rights.",
                "citations": [
                    {
                        "id": "(Fei et al., 2023)",
                        "snippets": [
                            "In recent years, there has been significant growth in the commercial applications of generative models, licensed and distributed by model developers to users, who in turn use them to offer services. In this scenario, there is a need to track and identify the responsible user in the presence of a violation of the license agreement or any kind of malicious usage.\n\nProtecting the Intellectual Property Rights (IPR) of model owners has become a pressing issue to avoid potential copyright infringements, such as unauthorized duplication or model theft, when these models are delivered to malicious users. Deep Neural Network (DNN) watermarking has been proposed as a solution to protect the IPR associated with DNN models [1].\n\nWith the exception of a few scattered works [3], the existing approaches for the watermarking of generative models, notably GANs, are designed for ownership verification, aiming at making it possible to retrieve the model authorship information from the generated images."
                        ],
                        "paper": {
                            "corpus_id": 265066820,
                            "title": "Robust Retraining-free GAN Fingerprinting via Personalized Normalization",
                            "authors": [
                                {
                                    "authorId": "1638005137",
                                    "name": "Jianwei Fei"
                                },
                                {
                                    "authorId": "2072878384",
                                    "name": "Zhihua Xia"
                                },
                                {
                                    "authorId": "2488892",
                                    "name": "B. Tondi"
                                },
                                {
                                    "authorId": "2259940667",
                                    "name": "Mauro Barni"
                                }
                            ],
                            "year": 2023,
                            "venue": "International Workshop on Information Forensics and Security",
                            "n_citations": 6
                        },
                        "score": 0.98193359375
                    },
                    {
                        "id": "(Han et al., 2024)",
                        "snippets": [
                            "Advancements in Artificial Intelligence Generated Contents (AIGCs) [47] have revolutionized the field of image synthesis (Rombach et al., 2021)38,(Xue et al., 2023), among which text-to-image diffusion models enable the creation of high-quality images from textual descriptions (Saharia et al., 2022)(Zhang et al., 2023). However, this progress has also raised significant concerns regarding the potential misuse of these models (Carlini et al., 2023)13,(Sha et al., 2022)[43]. Such misuse includes generating content that infringes on copyrights, such as mimicking specific artistic styles [30], intellectual properties [45]46], or creating disturbing and improper subject matter, including eroticism and violence (Schramowski et al., 2022). Addressing these issues necessitates continuously removing those improper concepts from these models to prevent misuse and protect copyright from infringement."
                        ],
                        "paper": {
                            "corpus_id": 274436153,
                            "title": "Continuous Concepts Removal in Text-to-image Diffusion Models",
                            "authors": [
                                {
                                    "authorId": "2170360833",
                                    "name": "Tingxu Han"
                                },
                                {
                                    "authorId": "3433022",
                                    "name": "Weisong Sun"
                                },
                                {
                                    "authorId": "2333453981",
                                    "name": "Yanrong Hu"
                                },
                                {
                                    "authorId": "2239197945",
                                    "name": "Chunrong Fang"
                                },
                                {
                                    "authorId": "2333368518",
                                    "name": "Yonglong Zhang"
                                },
                                {
                                    "authorId": "2333472479",
                                    "name": "Shiqing Ma"
                                },
                                {
                                    "authorId": "2322486553",
                                    "name": "Tao Zheng"
                                },
                                {
                                    "authorId": "2238950128",
                                    "name": "Zhenyu Chen"
                                },
                                {
                                    "authorId": "2154723145",
                                    "name": "Zhenting Wang"
                                }
                            ],
                            "year": 2024,
                            "venue": "arXiv.org",
                            "n_citations": 3
                        },
                        "score": 0.98095703125
                    },
                    {
                        "id": "(Rombach et al., 2021)",
                        "snippets": [
                            "By decomposing the image formation process into a sequential application of denoising autoencoders, diffusion models (DMs) achieve state-of-the-art synthesis results on image data and beyond. Additionally, their formulation allows for a guiding mechanism to control the image generation process without retraining. However, since these models typically operate directly in pixel space, optimization of powerful DMs often consumes hundreds of GPU days and inference is expensive due to sequential evaluations. To enable DM training on limited computational resources while retaining their quality and flexibility, we apply them in the latent space of powerful pretrained autoencoders. In contrast to previous work, training diffusion models on such a representation allows for the first time to reach a near-optimal point between complexity reduction and detail preservation, greatly boosting visual fidelity. By introducing cross-attention layers into the model architecture, we turn diffusion models into powerful and flexible generators for general conditioning inputs such as text or bounding boxes and high-resolution synthesis becomes possible in a convolutional manner. Our latent diffusion models (LDMs) achieve new state of the art scores for image inpainting and class-conditional image synthesis and highly competitive performance on various tasks, including unconditional image generation, text-to-image synthesis, and super-resolution, while significantly reducing computational requirements compared to pixel-based DMs."
                        ],
                        "paper": {
                            "corpus_id": 245335280,
                            "title": "High-Resolution Image Synthesis with Latent Diffusion Models",
                            "authors": [
                                {
                                    "authorId": "1660819540",
                                    "name": "Robin Rombach"
                                },
                                {
                                    "authorId": "119843260",
                                    "name": "A. Blattmann"
                                },
                                {
                                    "authorId": "2053482699",
                                    "name": "Dominik Lorenz"
                                },
                                {
                                    "authorId": "35175531",
                                    "name": "Patrick Esser"
                                },
                                {
                                    "authorId": "1796707",
                                    "name": "B. Ommer"
                                }
                            ],
                            "year": 2021,
                            "venue": "Computer Vision and Pattern Recognition",
                            "n_citations": 15768
                        },
                        "score": 0
                    },
                    {
                        "id": "(Saharia et al., 2022)",
                        "snippets": [
                            "We present Imagen, a text-to-image diffusion model with an unprecedented degree of photorealism and a deep level of language understanding. Imagen builds on the power of large transformer language models in understanding text and hinges on the strength of diffusion models in high-fidelity image generation. Our key discovery is that generic large language models (e.g. T5), pretrained on text-only corpora, are surprisingly effective at encoding text for image synthesis: increasing the size of the language model in Imagen boosts both sample fidelity and image-text alignment much more than increasing the size of the image diffusion model. Imagen achieves a new state-of-the-art FID score of 7.27 on the COCO dataset, without ever training on COCO, and human raters find Imagen samples to be on par with the COCO data itself in image-text alignment. To assess text-to-image models in greater depth, we introduce DrawBench, a comprehensive and challenging benchmark for text-to-image models. With DrawBench, we compare Imagen with recent methods including VQ-GAN+CLIP, Latent Diffusion Models, and DALL-E 2, and find that human raters prefer Imagen over other models in side-by-side comparisons, both in terms of sample quality and image-text alignment. See https://imagen.research.google/ for an overview of the results."
                        ],
                        "paper": {
                            "corpus_id": 248986576,
                            "title": "Photorealistic Text-to-Image Diffusion Models with Deep Language Understanding",
                            "authors": [
                                {
                                    "authorId": "51497543",
                                    "name": "Chitwan Saharia"
                                },
                                {
                                    "authorId": "144333684",
                                    "name": "William Chan"
                                },
                                {
                                    "authorId": "2054003577",
                                    "name": "Saurabh Saxena"
                                },
                                {
                                    "authorId": "2111917831",
                                    "name": "Lala Li"
                                },
                                {
                                    "authorId": "21040156",
                                    "name": "Jay Whang"
                                },
                                {
                                    "authorId": "40081727",
                                    "name": "Emily L. Denton"
                                },
                                {
                                    "authorId": "81419386",
                                    "name": "Seyed Kamyar Seyed Ghasemipour"
                                },
                                {
                                    "authorId": "143990191",
                                    "name": "Burcu Karagol Ayan"
                                },
                                {
                                    "authorId": "1982213",
                                    "name": "S. S. Mahdavi"
                                },
                                {
                                    "authorId": "143826364",
                                    "name": "Raphael Gontijo Lopes"
                                },
                                {
                                    "authorId": "2887364",
                                    "name": "Tim Salimans"
                                },
                                {
                                    "authorId": "2126278",
                                    "name": "Jonathan Ho"
                                },
                                {
                                    "authorId": "1793739",
                                    "name": "David J. Fleet"
                                },
                                {
                                    "authorId": "144739074",
                                    "name": "Mohammad Norouzi"
                                }
                            ],
                            "year": 2022,
                            "venue": "Neural Information Processing Systems",
                            "n_citations": 6075
                        },
                        "score": 0
                    },
                    {
                        "id": "(Schramowski et al., 2022)",
                        "snippets": [
                            "Text-conditioned image generation models have recently achieved astonishing results in image quality and text alignment and are consequently employed in a fast-growing number of applications. Since they are highly data-driven, relying on billion-sized datasets randomly scraped from the internet, they also suffer, as we demonstrate, from degenerated and biased human behavior. In turn, they may even reinforce such biases. To help combat these undesired side effects, we present safe latent diffusion (SLD). Specifically, to measure the inappropriate degeneration due to unfiltered and imbalanced training sets, we establish a novel image generation test bed-inappropriate image prompts (I2P)-containing dedicated, real-world image-to-text prompts covering concepts such as nudity and violence. As our exhaustive empirical evaluation demonstrates, the introduced SLD removes and suppresses inappropriate image parts during the diffusion process, with no additional training required and no adverse effect on overall image quality or text alignment.11Code available at https://huggingface.co/docs/diffusers/api/pipelines/stable.diffusion.safe"
                        ],
                        "paper": {
                            "corpus_id": 253420366,
                            "title": "Safe Latent Diffusion: Mitigating Inappropriate Degeneration in Diffusion Models",
                            "authors": [
                                {
                                    "authorId": "40896023",
                                    "name": "P. Schramowski"
                                },
                                {
                                    "authorId": "2166299958",
                                    "name": "Manuel Brack"
                                },
                                {
                                    "authorId": "2905059",
                                    "name": "Bjorn Deiseroth"
                                },
                                {
                                    "authorId": "2066493115",
                                    "name": "K. Kersting"
                                }
                            ],
                            "year": 2022,
                            "venue": "Computer Vision and Pattern Recognition",
                            "n_citations": 309
                        },
                        "score": 0
                    },
                    {
                        "id": "(Guo et al., 2024)",
                        "snippets": [
                            "However, as commercial text-to-image diffusion models become increasingly prevalent [57,61], copyright issues have emerged as a significant concern. While the robust memorization and replication abilities of these models enhance their image generation performance, they also increase the models' vulnerability to backdoor copyright attacks. By injecting concealed poisoned data into the training set, diffusion models can be compromised without the need for fine-tuning [62]. Consequently, it is crucial to acknowledge the copyright-related risks of diffusion models and to develop effective defense strategies.\n\nCurrent solutions to copyright issues primarily involve the removal of copyrighted images from training datasets to prevent diffusion models from inadvertently learning these images, thus avoiding potential copyright infringement [7,61,76]. However, effective defense mechanisms against existing backdoor injection attacks [21,28,36,74]77], which can result in copyright violations, have yet to be developed."
                        ],
                        "paper": {
                            "corpus_id": 274436785,
                            "title": "CopyrightShield: Spatial Similarity Guided Backdoor Defense against Copyright Infringement in Diffusion Models",
                            "authors": [
                                {
                                    "authorId": "2333463963",
                                    "name": "Zhixiang Guo"
                                },
                                {
                                    "authorId": "2325884825",
                                    "name": "Siyuan Liang"
                                },
                                {
                                    "authorId": "2257572247",
                                    "name": "Aishan Liu"
                                },
                                {
                                    "authorId": "2237906923",
                                    "name": "Dacheng Tao"
                                }
                            ],
                            "year": 2024,
                            "venue": "arXiv.org",
                            "n_citations": 3
                        },
                        "score": 0.98828125
                    },
                    {
                        "id": "(Vyas et al., 2023)",
                        "snippets": [
                            "Generative models for images, text, code, and other domains pose new challenges for ensuring their outputs are protected from copyright infringement. Such models are trained on a large corpus of data, where it is often impractical to ensure the training set is 100% free of copyrighted material. Furthermore, removing copyrighted material from training may also be undesirable. For example, a human author is free to read and use copyrighted material as inspiration for their work, as long as they do not copy it. Similarly, it may be beneficial to use copyrighted material when training in order to have more effective generative models.\n\nCopyright infringement by generative models can potentially arise in (at least) two manners. First, in the training phase, the algorithm could directly access copyrighted material, and the learned model itself could implicitly contain (e.g. coded in its weights) verbatim copies of some of this material. The copyright issues arising during training share many similarities with other settings in which algorithms scrape significant amounts of data, including search-engine indexing and digitizing books. Here, the question of what constitutes a copyright infringement is largely a question of \"fair use.\" This work does not examine these fair use issues that arise in the training phase, and we refer the reader to the several legal precedents in this area [Samuelson, 2021].\n\nThe second notable source of potential infringement is in the deployment phase, where a user provides a prompt x to the model to obtain some output y. Apriori, we cannot rule out the possibility that y is either a verbatim copy or substantially similar to some copyrighted training data. Moreover, unlike search engines, generative models do not keep track of the provenance of their outputs. Hence, a user of such an output y (e.g., a software company using generated code, or a designer using a generated image) has no easy way to verify that it does not infringe upon any copyrighted material."
                        ],
                        "paper": {
                            "corpus_id": 257050406,
                            "title": "On Provable Copyright Protection for Generative Models",
                            "authors": [
                                {
                                    "authorId": "145603901",
                                    "name": "Nikhil Vyas"
                                },
                                {
                                    "authorId": "144695232",
                                    "name": "S. Kakade"
                                },
                                {
                                    "authorId": "1697211",
                                    "name": "B. Barak"
                                }
                            ],
                            "year": 2023,
                            "venue": "International Conference on Machine Learning",
                            "n_citations": 94
                        },
                        "score": 0.9892578125
                    },
                    {
                        "id": "(Zampini et al., 2025)",
                        "snippets": [
                            "This method ensures that the generated images are guided away from resembling copyrighted material while still allowing the model to produce high-quality outputs. By selectively modifying the generated content during the initial stages of denoising, we can effectively prevent the model from producing images that infringe on copyrights without significantly affecting the overall image quality",
                            "We implement a Conditional Diffusion Model baselines using and unconstrained stable diffusion model identical to the one used for our method. The conditional baseline generates the protected cartoon character (Mickey Mouse) 33% of the time, despite conditioning it against these generations. \n\nConversely, our Latent Constrained Model only generates the protected cartoon character 10% of the time, aligning with the expected bounds of the classifier's predictive accuracy. Our method has proven to be highly effective because it preserves the generative capabilities of the model while imposing the defined constraints. Notably, the difference between the original image and the corrected one primarily affects the areas near the figure that violate the constraint, while the rest of the image remains largely unchanged. The FID scores of the generated images, increasing only slightly from 61.2 to 65.1, remain largely unaltered by the gradientbased correction. This demonstrates that our approach can selectively modify generated content to avoid copyrighted material without compromising overall image quality."
                        ],
                        "paper": {
                            "corpus_id": 276250558,
                            "title": "Training-Free Constrained Generation With Stable Diffusion Models",
                            "authors": [
                                {
                                    "authorId": "2277539130",
                                    "name": "S. Zampini"
                                },
                                {
                                    "authorId": "2267727785",
                                    "name": "Jacob K. Christopher"
                                },
                                {
                                    "authorId": "2255314313",
                                    "name": "Luca Oneto"
                                },
                                {
                                    "authorId": "2300205982",
                                    "name": "Davide Anguita"
                                },
                                {
                                    "authorId": "2141569789",
                                    "name": "Ferdinando Fioretto"
                                }
                            ],
                            "year": 2025,
                            "venue": "arXiv.org",
                            "n_citations": 2
                        },
                        "score": 0.98583984375
                    },
                    {
                        "id": "(Rakib et al., 2025)",
                        "snippets": [
                            "In today's age of social media and marketing, copyright issues can be a major roadblock to the free sharing of images. Generative AI models have made it possible to create high-quality images, but concerns about copyright infringement are a hindrance to their abundant use. As these models use data from training images to generate new ones, it is often a daunting task to ensure they do not violate intellectual property rights. Some AI models have even been noted to directly copy copyrighted images, a problem often referred to as source copying. Traditional copyright protection measures such as watermarks and metadata have also proven to be futile in this regard",
                            ".Diffusion models have revolutionized image generation, surpassing predecessors like GANs and VAEs in both fidelity and performance. However, this progress has been shadowed by a critical challenge: source copying. This issue raises serious concerns regarding privacy and intellectual property rights, demanding innovative solutions that preserve content quality while enhancing copyright protection."
                        ],
                        "paper": {
                            "corpus_id": 277955485,
                            "title": "TWIG: Two-Step Image Generation using Segmentation Masks in Diffusion Models",
                            "authors": [
                                {
                                    "authorId": "2313107575",
                                    "name": "Mazharul Islam Rakib"
                                },
                                {
                                    "authorId": "2357491645",
                                    "name": "Showrin Rahman"
                                },
                                {
                                    "authorId": "2113939573",
                                    "name": "J. Mondal"
                                },
                                {
                                    "authorId": "2349232014",
                                    "name": "Xi Xiao"
                                },
                                {
                                    "authorId": "2357414290",
                                    "name": "David Lewis"
                                },
                                {
                                    "authorId": "2356548661",
                                    "name": "Alessandra Mileo"
                                },
                                {
                                    "authorId": "2210516789",
                                    "name": "Meem Arafat Manab"
                                }
                            ],
                            "year": 2025,
                            "venue": "arXiv.org",
                            "n_citations": 1
                        },
                        "score": 0.994140625
                    },
                    {
                        "id": "(Wu et al., 2024)",
                        "snippets": [
                            "Recent years have witnessed the advancement of Diffusion Models (DMs) in computer vision.These models demonstrate exceptional capabilities across a diverse array of tasks, including image editing [14], and video editing [35], among others.Particularly, the emergence of few-shot generation techniques, exemplified by Dreambooth (Ruiz et al., 2022) and swiftly capturing the style or primary objects by fine-tuning a pretrained model on a small set of images.This process enables efficient and high-quality art imitation and art style transfer by utilizing the fine-tuned model.\n\nHowever, these advanced few-shot generation techniques also spark widespread concerns regarding the protection of copyright for human artworks and individual photographs.There is a growing fear that parties may exploit the generative capabilities of DMs to create and profit from derivative artworks based on existing artists' works, without obtaining proper authorization [6,19].Concurrently, concerns arise regarding the creation of fabricated images of individuals without their consent [32].All of these collectively pose a serious threat to the security of personal data and intellectual property rights."
                        ],
                        "paper": {
                            "corpus_id": 268513090,
                            "title": "CGI-DM: Digital Copyright Authentication for Diffusion Models via Contrasting Gradient Inversion",
                            "authors": [
                                {
                                    "authorId": "2108069960",
                                    "name": "Xiaoyu Wu"
                                },
                                {
                                    "authorId": "2147311278",
                                    "name": "Yang Hua"
                                },
                                {
                                    "authorId": "2186858424",
                                    "name": "Chumeng Liang"
                                },
                                {
                                    "authorId": "2118001291",
                                    "name": "Jiaru Zhang"
                                },
                                {
                                    "authorId": "2144220882",
                                    "name": "Hao Wang"
                                },
                                {
                                    "authorId": "2055312951",
                                    "name": "Tao Song"
                                },
                                {
                                    "authorId": "2292035375",
                                    "name": "Haibing Guan"
                                }
                            ],
                            "year": 2024,
                            "venue": "Computer Vision and Pattern Recognition",
                            "n_citations": 6
                        },
                        "score": 0.984375
                    },
                    {
                        "id": "(Ruiz et al., 2022)",
                        "snippets": [
                            "Large text-to-image models achieved a remarkable leap in the evolution of AI, enabling high-quality and diverse synthesis of images from a given text prompt. However, these models lack the ability to mimic the appearance of subjects in a given reference set and synthesize novel renditions of them in different contexts. In this work, we present a new approach for \"personalization\" of text-to-image diffusion models. Given as input just a few images of a subject, we fine-tune a pretrained text-to-image model such that it learns to bind a unique identifier with that specific subject. Once the subject is embedded in the output domain of the model, the unique identifier can be used to synthesize novel photorealistic images of the subject contextualized in different scenes. By leveraging the semantic prior embedded in the model with a new autogenous class-specific prior preservation loss, our technique enables synthesizing the subject in diverse scenes, poses, views and lighting conditions that do not appear in the reference images. We apply our technique to several previously-unassailable tasks, including subject recontextualization, text-guided view synthesis, and artistic rendering, all while preserving the subject's key features. We also provide a new dataset and evaluation protocol for this new task of subject-driven generation. Project page: https://dreambooth.github.io/"
                        ],
                        "paper": {
                            "corpus_id": 251800180,
                            "title": "DreamBooth: Fine Tuning Text-to-Image Diffusion Models for Subject-Driven Generation",
                            "authors": [
                                {
                                    "authorId": "31601235",
                                    "name": "Nataniel Ruiz"
                                },
                                {
                                    "authorId": "2167749913",
                                    "name": "Yuanzhen Li"
                                },
                                {
                                    "authorId": "2131639924",
                                    "name": "Varun Jampani"
                                },
                                {
                                    "authorId": "1782328",
                                    "name": "Y. Pritch"
                                },
                                {
                                    "authorId": "144544291",
                                    "name": "Michael Rubinstein"
                                },
                                {
                                    "authorId": "3451442",
                                    "name": "Kfir Aberman"
                                }
                            ],
                            "year": 2022,
                            "venue": "Computer Vision and Pattern Recognition",
                            "n_citations": 2891
                        },
                        "score": 0
                    }
                ],
                "format": "synthesis",
                "table": null,
                "model": "claude-3-7-sonnet-20250219"
            },
            {
                "title": "Implications for Stakeholders",
                "tldr": "Copyright infringement in generative AI affects multiple groups, including original content creators who face unauthorized use of their work, AI developers who must manage legal risks, and users who may unknowingly generate infringing content. (8 sources)",
                "text": "\nThe rise of generative AI models for images has created a complex web of implications for various stakeholders in the creative ecosystem. For original content creators\u2014artists, photographers, and other visual content owners\u2014these models present significant challenges to their intellectual property rights. When their works are included in training datasets without permission or compensation, they face not only potential economic losses but also the unauthorized reproduction or adaptation of their distinctive styles and content <Paper corpusId=\"267400526\" paperTitle=\"(Zhuang, 2024)\" isShortName></Paper>. This problem is compounded by the fact that AI-generated outputs typically lack watermarks or source attribution, making it even more difficult for copyright owners to detect infringement or receive appropriate credit <Paper corpusId=\"267400526\" paperTitle=\"(Zhuang, 2024)\" isShortName></Paper>.\n\nFor AI model developers and companies, the stakes are equally high. They face mounting legal risks through multiple lawsuits, as evidenced by cases against Stability AI and other companies that have been accused of reproducing copyrighted works without permission <Paper corpusId=\"267400526\" paperTitle=\"(Zhuang, 2024)\" isShortName></Paper>. These legal challenges necessitate the development of protective measures such as DNN watermarking to verify model ownership and track potential misuse <Paper corpusId=\"265066820\" paperTitle=\"(Fei et al., 2023)\" isShortName></Paper>. Additionally, developers must navigate the complex landscape of licensing and distribution, particularly when their models are provided as services to users who might violate license agreements or engage in malicious usage <Paper corpusId=\"265066820\" paperTitle=\"(Fei et al., 2023)\" isShortName></Paper>.\n\nEnd users of generative AI services face their own set of challenges and responsibilities. They may potentially be considered direct or indirect infringers in copyright disputes, even without intending to replicate copyrighted material <Paper corpusId=\"273023255\" paperTitle=\"(Chiba-Okabe, 2024)\" isShortName></Paper>. This creates significant uncertainty for individuals and businesses using these technologies, as they must navigate copyright risks without clear guidelines on what constitutes infringement in AI-generated content. The uninterpretable nature of generative models further complicates matters, as it becomes difficult to provide direct evidence of copying when infringement is alleged <Paper corpusId=\"273023255\" paperTitle=\"(Chiba-Okabe, 2024)\" isShortName></Paper>.\n\nThe advancement of few-shot generation techniques such as DreamBooth has intensified these concerns. These methods enable efficient art imitation and style transfer with minimal training data <Paper corpusId=\"268513090\" paperTitle=\"(Wu et al., 2024)\" isShortName></Paper> <Paper corpusId=\"251800180\" paperTitle=\"(Ruiz et al., 2022)\" isShortName></Paper>, making it easier for unauthorized parties to directly edit or modify source data to create derivative works <Paper corpusId=\"267412857\" paperTitle=\"(Ren et al., 2024)\" isShortName></Paper>. This capability poses a serious threat to the security of personal data and intellectual property rights, as parties may exploit generative capabilities to create and profit from derivative artworks without proper authorization <Paper corpusId=\"268513090\" paperTitle=\"(Wu et al., 2024)\" isShortName></Paper>.\n\nAs generative models continue to evolve, the tension between technological innovation and copyright protection will likely intensify. Studies have shown that exposure to certain types of content during training can lead to a high likelihood of models reproducing or closely imitating that content <Paper corpusId=\"273023255\" paperTitle=\"(Chiba-Okabe, 2024)\" isShortName></Paper> <Paper corpusId=\"229156229\" paperTitle=\"(Carlini et al., 2020)\" isShortName></Paper> <Paper corpusId=\"256389993\" paperTitle=\"(Carlini et al., 2023)\" isShortName></Paper>, suggesting that the inclusion of specific data correlates with a greater probability of producing similar outputs through copying. This presents an ongoing challenge for all stakeholders involved in the generative AI ecosystem as they navigate the complex intersection of technological capabilities, creative expression, and legal frameworks.",
                "citations": [
                    {
                        "id": "(Zhuang, 2024)",
                        "snippets": [
                            "Stable Diffusion can quickly generate high-quality images based on simple text descriptions, thanks to its training on billions of sample images collected online. However, not all of these sample images fall within the public domain; some are even protected by copyright. As one can imagine, the authors and photographers of these copyrighted images are not pleased with the actions of Stable Diffusion.\n\nStable Diffusion has faced multiple lawsuits due to copyright issues. Artists and photographers have initiated collective lawsuits against this technology, and one of the world's leading image suppliers, Getty Images Holdings Inc., has also filed a similar case. According to Getty's (2023) complaint, \"Stability AI has reproduced over 12 million Getty Images, along with their related captions and metadata, without permission or compensation.\"",
                            "After exploring how artificial intelligence (AI)-generated platforms work, learn what processes may involve human artist infringement in the course of their work. So let's talk about where these workflows infringe on human artists' copyrighted works -Artificial intelligence (AI)-generated content raises concerns about copyright infringement of human artist works in several ways.\n\nThe researchers summarized three aspects of human artist infringement: model training using only copyrighted works without permission. Take Stable Diffusion, which has been involved in many copyright disputes. It uses the image database containing hundreds of millions-LAION-5B (Kaixin Zhu, & Yiqun Zhang. ( 2023)) as the source of training data, and it does not need the consent of the copyright owner [5].\n\nSecond, there is a lot of plagiarism and adaptation of copyrighted works in AI-generated images. In a case in which an artist sued an AI-generated platform, the painter Erin Hanson provided strong evidence that the AI-generated images plagiarized his work. And there are many such lawsuits now, not a few. Third, the images generated by the AI generation platform, such as Stable Diffusion, if works infringe copyright works, do not have any watermarks or source marks, making it even more difficult for copyright owners to accept."
                        ],
                        "paper": {
                            "corpus_id": 267400526,
                            "title": "AIGC (Artificial Intelligence Generated Content) infringes the copyright of human artists",
                            "authors": [
                                {
                                    "authorId": "2282411198",
                                    "name": "Lyulin Zhuang"
                                }
                            ],
                            "year": 2024,
                            "venue": "Applied and Computational Engineering",
                            "n_citations": 2
                        },
                        "score": 0.9912109375
                    },
                    {
                        "id": "(Fei et al., 2023)",
                        "snippets": [
                            "In recent years, there has been significant growth in the commercial applications of generative models, licensed and distributed by model developers to users, who in turn use them to offer services. In this scenario, there is a need to track and identify the responsible user in the presence of a violation of the license agreement or any kind of malicious usage.\n\nProtecting the Intellectual Property Rights (IPR) of model owners has become a pressing issue to avoid potential copyright infringements, such as unauthorized duplication or model theft, when these models are delivered to malicious users. Deep Neural Network (DNN) watermarking has been proposed as a solution to protect the IPR associated with DNN models [1].\n\nWith the exception of a few scattered works [3], the existing approaches for the watermarking of generative models, notably GANs, are designed for ownership verification, aiming at making it possible to retrieve the model authorship information from the generated images."
                        ],
                        "paper": {
                            "corpus_id": 265066820,
                            "title": "Robust Retraining-free GAN Fingerprinting via Personalized Normalization",
                            "authors": [
                                {
                                    "authorId": "1638005137",
                                    "name": "Jianwei Fei"
                                },
                                {
                                    "authorId": "2072878384",
                                    "name": "Zhihua Xia"
                                },
                                {
                                    "authorId": "2488892",
                                    "name": "B. Tondi"
                                },
                                {
                                    "authorId": "2259940667",
                                    "name": "Mauro Barni"
                                }
                            ],
                            "year": 2023,
                            "venue": "International Workshop on Information Forensics and Security",
                            "n_citations": 6
                        },
                        "score": 0.98193359375
                    },
                    {
                        "id": "(Chiba-Okabe, 2024)",
                        "snippets": [
                            "In cases where generative AI is used to create content, it is typically provided as a service by the AI developer and utilized by a user, and in such situations, both the user and the AI developer may potentially be considered direct or indirect infringers, depending on the specific circumstances [28]. Copyright infringement may occur even without the user or developer intending to replicate the copyrighted material or to create something similar [28]. \n\nDue to the uninterpretability of generative models, it can be challenging to provide direct evidence of factual copying for AI-generated content. In most cases, access and similarity will likely serve as the primary evidence of infringement, much like in traditional copyright disputes [28]. While the creative processes of generative models differ markedly from human creativity, it appears that similar evidentiary principles are likely to apply even when generative AI is involved in the creative processes. At a minimum, it is generally recognized that some form of access to the original work, which may be demonstrated by its inclusion in the training data, is necessary to establish copying, even for generative AI outputs [28,29,36]. Moreover, studies have shown that exposure to certain types of content during training can lead to a high likelihood of these models reproducing or closely imitating that content (Carlini et al., 2020)(Carlini et al., 2023)[39], suggesting that inclusion of specific data correlates with a greater probability of producing similar outputs through copying. This echoes the evidentiary framework applicable to traditional copyright cases, where greater similarity between works strengthens the inference of access and copying. \n\nA distinctive feature of generative models, setting them apart from human artists or traditional creative tools, is their capacity to be trained on immense and diverse datasets, which are often created by scraping content from the Internet. Furthermore, direct inclusion of copyrighted material in the training data is not even necessary for latent diffusion models to produce images similar to copyrighted works, as these models can reproduce content from other images that retain similar latent information [40], which exacerbates the problem. These characteristics suggest a high likelihood that generative models have what can be construed as \"access\" to copyrighted works."
                        ],
                        "paper": {
                            "corpus_id": 273023255,
                            "title": "Probabilistic Analysis of Copyright Disputes and Generative AI Safety",
                            "authors": [
                                {
                                    "authorId": "2297800322",
                                    "name": "Hiroaki Chiba-Okabe"
                                }
                            ],
                            "year": 2024,
                            "venue": "arXiv.org",
                            "n_citations": 2
                        },
                        "score": 0.98876953125
                    },
                    {
                        "id": "(Wu et al., 2024)",
                        "snippets": [
                            "Recent years have witnessed the advancement of Diffusion Models (DMs) in computer vision.These models demonstrate exceptional capabilities across a diverse array of tasks, including image editing [14], and video editing [35], among others.Particularly, the emergence of few-shot generation techniques, exemplified by Dreambooth (Ruiz et al., 2022) and swiftly capturing the style or primary objects by fine-tuning a pretrained model on a small set of images.This process enables efficient and high-quality art imitation and art style transfer by utilizing the fine-tuned model.\n\nHowever, these advanced few-shot generation techniques also spark widespread concerns regarding the protection of copyright for human artworks and individual photographs.There is a growing fear that parties may exploit the generative capabilities of DMs to create and profit from derivative artworks based on existing artists' works, without obtaining proper authorization [6,19].Concurrently, concerns arise regarding the creation of fabricated images of individuals without their consent [32].All of these collectively pose a serious threat to the security of personal data and intellectual property rights."
                        ],
                        "paper": {
                            "corpus_id": 268513090,
                            "title": "CGI-DM: Digital Copyright Authentication for Diffusion Models via Contrasting Gradient Inversion",
                            "authors": [
                                {
                                    "authorId": "2108069960",
                                    "name": "Xiaoyu Wu"
                                },
                                {
                                    "authorId": "2147311278",
                                    "name": "Yang Hua"
                                },
                                {
                                    "authorId": "2186858424",
                                    "name": "Chumeng Liang"
                                },
                                {
                                    "authorId": "2118001291",
                                    "name": "Jiaru Zhang"
                                },
                                {
                                    "authorId": "2144220882",
                                    "name": "Hao Wang"
                                },
                                {
                                    "authorId": "2055312951",
                                    "name": "Tao Song"
                                },
                                {
                                    "authorId": "2292035375",
                                    "name": "Haibing Guan"
                                }
                            ],
                            "year": 2024,
                            "venue": "Computer Vision and Pattern Recognition",
                            "n_citations": 6
                        },
                        "score": 0.984375
                    },
                    {
                        "id": "(Ruiz et al., 2022)",
                        "snippets": [
                            "Large text-to-image models achieved a remarkable leap in the evolution of AI, enabling high-quality and diverse synthesis of images from a given text prompt. However, these models lack the ability to mimic the appearance of subjects in a given reference set and synthesize novel renditions of them in different contexts. In this work, we present a new approach for \"personalization\" of text-to-image diffusion models. Given as input just a few images of a subject, we fine-tune a pretrained text-to-image model such that it learns to bind a unique identifier with that specific subject. Once the subject is embedded in the output domain of the model, the unique identifier can be used to synthesize novel photorealistic images of the subject contextualized in different scenes. By leveraging the semantic prior embedded in the model with a new autogenous class-specific prior preservation loss, our technique enables synthesizing the subject in diverse scenes, poses, views and lighting conditions that do not appear in the reference images. We apply our technique to several previously-unassailable tasks, including subject recontextualization, text-guided view synthesis, and artistic rendering, all while preserving the subject's key features. We also provide a new dataset and evaluation protocol for this new task of subject-driven generation. Project page: https://dreambooth.github.io/"
                        ],
                        "paper": {
                            "corpus_id": 251800180,
                            "title": "DreamBooth: Fine Tuning Text-to-Image Diffusion Models for Subject-Driven Generation",
                            "authors": [
                                {
                                    "authorId": "31601235",
                                    "name": "Nataniel Ruiz"
                                },
                                {
                                    "authorId": "2167749913",
                                    "name": "Yuanzhen Li"
                                },
                                {
                                    "authorId": "2131639924",
                                    "name": "Varun Jampani"
                                },
                                {
                                    "authorId": "1782328",
                                    "name": "Y. Pritch"
                                },
                                {
                                    "authorId": "144544291",
                                    "name": "Michael Rubinstein"
                                },
                                {
                                    "authorId": "3451442",
                                    "name": "Kfir Aberman"
                                }
                            ],
                            "year": 2022,
                            "venue": "Computer Vision and Pattern Recognition",
                            "n_citations": 2891
                        },
                        "score": 0
                    },
                    {
                        "id": "(Ren et al., 2024)",
                        "snippets": [
                            "For the source data owner, which refers to the party or individual who owns the originality of image works, their data can be intentionally or unintentionally collected by model trainers as training samples to construct DGMs as introduced above. For example, recent studies [16]134] have demonstrated that popular DGMs are highly possible to completely replicate their training data samples, which is called memorization. \n\nThe possibility of data replication may severely offend the ownership of the original data samples. Moreover, the development of fine-tuning strategies such as DreamBooth can greatly improve the efficiency for unauthorized parties to directly edit or modify the source data to obtain new samples, which also severely infringes the copyright of the original works."
                        ],
                        "paper": {
                            "corpus_id": 267412857,
                            "title": "Copyright Protection in Generative AI: A Technical Perspective",
                            "authors": [
                                {
                                    "authorId": "2256589810",
                                    "name": "Jie Ren"
                                },
                                {
                                    "authorId": "2253881697",
                                    "name": "Han Xu"
                                },
                                {
                                    "authorId": "2185740224",
                                    "name": "Pengfei He"
                                },
                                {
                                    "authorId": "2218740984",
                                    "name": "Yingqian Cui"
                                },
                                {
                                    "authorId": "2253682835",
                                    "name": "Shenglai Zeng"
                                },
                                {
                                    "authorId": "2282560420",
                                    "name": "Jiankun Zhang"
                                },
                                {
                                    "authorId": "2256788829",
                                    "name": "Hongzhi Wen"
                                },
                                {
                                    "authorId": "46496977",
                                    "name": "Jiayuan Ding"
                                },
                                {
                                    "authorId": "2253533415",
                                    "name": "Hui Liu"
                                },
                                {
                                    "authorId": "2267019992",
                                    "name": "Yi Chang"
                                },
                                {
                                    "authorId": "2115879611",
                                    "name": "Jiliang Tang"
                                }
                            ],
                            "year": 2024,
                            "venue": "arXiv.org",
                            "n_citations": 42
                        },
                        "score": 0.99169921875
                    },
                    {
                        "id": "(Carlini et al., 2020)",
                        "snippets": [
                            "It has become common to publish large (billion parameter) language models that have been trained on private datasets. This paper demonstrates that in such settings, an adversary can perform a training data extraction attack to recover individual training examples by querying the language model. \nWe demonstrate our attack on GPT-2, a language model trained on scrapes of the public Internet, and are able to extract hundreds of verbatim text sequences from the model's training data. These extracted examples include (public) personally identifiable information (names, phone numbers, and email addresses), IRC conversations, code, and 128-bit UUIDs. Our attack is possible even though each of the above sequences are included in just one document in the training data. \nWe comprehensively evaluate our extraction attack to understand the factors that contribute to its success. For example, we find that larger models are more vulnerable than smaller models. We conclude by drawing lessons and discussing possible safeguards for training large language models."
                        ],
                        "paper": {
                            "corpus_id": 229156229,
                            "title": "Extracting Training Data from Large Language Models",
                            "authors": [
                                {
                                    "authorId": "2483738",
                                    "name": "Nicholas Carlini"
                                },
                                {
                                    "authorId": "2444919",
                                    "name": "Florian Tram\u00e8r"
                                },
                                {
                                    "authorId": "145217343",
                                    "name": "Eric Wallace"
                                },
                                {
                                    "authorId": "40844378",
                                    "name": "Matthew Jagielski"
                                },
                                {
                                    "authorId": "1404060687",
                                    "name": "Ariel Herbert-Voss"
                                },
                                {
                                    "authorId": "3844009",
                                    "name": "Katherine Lee"
                                },
                                {
                                    "authorId": "145625142",
                                    "name": "Adam Roberts"
                                },
                                {
                                    "authorId": "31035595",
                                    "name": "Tom B. Brown"
                                },
                                {
                                    "authorId": "143711382",
                                    "name": "D. Song"
                                },
                                {
                                    "authorId": "1758110",
                                    "name": "\u00da. Erlingsson"
                                },
                                {
                                    "authorId": "3046437",
                                    "name": "Alina Oprea"
                                },
                                {
                                    "authorId": "2402716",
                                    "name": "Colin Raffel"
                                }
                            ],
                            "year": 2020,
                            "venue": "USENIX Security Symposium",
                            "n_citations": 1950
                        },
                        "score": 0
                    },
                    {
                        "id": "(Carlini et al., 2023)",
                        "snippets": [
                            "Image diffusion models such as DALL-E 2, Imagen, and Stable Diffusion have attracted significant attention due to their ability to generate high-quality synthetic images. In this work, we show that diffusion models memorize individual images from their training data and emit them at generation time. With a generate-and-filter pipeline, we extract over a thousand training examples from state-of-the-art models, ranging from photographs of individual people to trademarked company logos. We also train hundreds of diffusion models in various settings to analyze how different modeling and data decisions affect privacy. Overall, our results show that diffusion models are much less private than prior generative models such as GANs, and that mitigating these vulnerabilities may require new advances in privacy-preserving training."
                        ],
                        "paper": {
                            "corpus_id": 256389993,
                            "title": "Extracting Training Data from Diffusion Models",
                            "authors": [
                                {
                                    "authorId": "2483738",
                                    "name": "Nicholas Carlini"
                                },
                                {
                                    "authorId": "9200194",
                                    "name": "Jamie Hayes"
                                },
                                {
                                    "authorId": "3490923",
                                    "name": "Milad Nasr"
                                },
                                {
                                    "authorId": "40844378",
                                    "name": "Matthew Jagielski"
                                },
                                {
                                    "authorId": "3482535",
                                    "name": "Vikash Sehwag"
                                },
                                {
                                    "authorId": "2444919",
                                    "name": "Florian Tram\u00e8r"
                                },
                                {
                                    "authorId": "1718064",
                                    "name": "Borja Balle"
                                },
                                {
                                    "authorId": "7975935",
                                    "name": "Daphne Ippolito"
                                },
                                {
                                    "authorId": "145217343",
                                    "name": "Eric Wallace"
                                }
                            ],
                            "year": 2023,
                            "venue": "USENIX Security Symposium",
                            "n_citations": 617
                        },
                        "score": 0
                    }
                ],
                "format": "synthesis",
                "table": null,
                "model": "claude-3-7-sonnet-20250219"
            }
        ],
        "cost": 0.169722
    }
}