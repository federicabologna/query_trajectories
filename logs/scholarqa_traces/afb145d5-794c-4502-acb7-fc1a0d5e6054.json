{
    "query": "What are the most recent task-agnostic pruning methods for large language models (LLMs) that require no or minimal retraining, such as SparseGPT or Wanda, and how do they achieve efficiency gains while maintaining performance?",
    "user_id": "lib_user",
    "task_id": "afb145d5-794c-4502-acb7-fc1a0d5e6054",
    "timestamp": "2025-06-24T01:14:13.691383",
    "n_retrieval": 256,
    "n_retrieved": 256,
    "n_candidates": 38,
    "n_rerank": 50,
    "opt_in": true,
    "total_cost": 0.49391700000000005,
    "decomposed_query": {
        "rewritten_query": "Task-agnostic pruning methods for large language models (LLMs) that require no or minimal retraining, such as SparseGPT or Wanda, and how they achieve efficiency gains while maintaining performance.",
        "keyword_query": "task-agnostic pruning large language models LLMs minimal retraining SparseGPT Wanda efficiency gains performance",
        "search_filters": {
            "year": "2022-2025",
            "fieldsOfStudy": "Computer Science"
        },
        "cost": 0.010566,
        "model": "claude-3-7-sonnet-20250219"
    },
    "candidates": [
        {
            "title": "Revisiting Large Language Model Pruning using Neuron Semantic Attribution",
            "venue": "arXiv.org",
            "year": 2025,
            "reference_count": 33,
            "citation_count": 2,
            "influential_citation_count": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2503.01542, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2237598833",
                    "name": "Yizhuo Ding"
                },
                {
                    "authorId": "2244778967",
                    "name": "Xinwei Sun"
                },
                {
                    "authorId": "2244698019",
                    "name": "Yanwei Fu"
                },
                {
                    "authorId": "2349205822",
                    "name": "Guosheng Hu"
                }
            ],
            "abstract": "Model pruning technique is vital for accelerating large language models by reducing their size and computational requirements. However, the generalizability of existing pruning methods across diverse datasets and tasks remains unclear. Thus, we conduct extensive evaluations on 24 datasets and 4 tasks using popular pruning methods. Based on these evaluations, we find and then investigate that calibration set greatly affect the performance of pruning methods. In addition, we surprisingly find a significant performance drop of existing pruning methods in sentiment classification tasks. To understand the link between performance drop and pruned neurons, we propose Neuron Semantic Attribution, which learns to associate each neuron with specific semantics. This method first makes the unpruned neurons of LLMs explainable.",
            "corpus_id": 276774084,
            "sentences": [
                {
                    "corpus_id": "276774084",
                    "title": "Revisiting Large Language Model Pruning using Neuron Semantic Attribution",
                    "text": "In this study, we employed three state-of-the-art posttraining pruning methods: SparseGPT (Frantar & Alistarh, 2023), Wanda (Sun et al., 2023), and RIA (Zhang et al., 2024). These methods are designed to reduce the size of large language models (LLMs) while maintaining their performance across various tasks. Even though all three methods prune the model using a combination of weights W and activations X, they each use different metrics M and strategies to decide which parameters to prune. \n\n\u2022 SparseGPT: This method selects parameters based on the metric: \n\nAfter pruning, SparseGPT also applies an OBS (Hassibi et al., 1993) update to compensate for the pruned parameters. \n\n\u2022 Wanda: This method uses a simple yet effective metric for pruning: \n\nAfter pruning, there is no need for any parameter update step. \n\n\u2022 RIA: This method calculates the pruning metric using relative importance: \n\na . After calculating the metric, channel permutation is used to adjust the model for N:M sparsity. \n\nThese methods were evaluated on a range of tasks to assess how each performs across different calibration data. Each pruning method was applied iteratively, with models being pruned and evaluated to ensure that performance was maintained across tasks.",
                    "score": 0.834088992306186,
                    "section_title": "Pruning Methods",
                    "char_start_offset": 10014,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 173
                        },
                        {
                            "start": 174,
                            "end": 309
                        },
                        {
                            "start": 310,
                            "end": 493
                        },
                        {
                            "start": 496,
                            "end": 560
                        },
                        {
                            "start": 563,
                            "end": 678
                        },
                        {
                            "start": 681,
                            "end": 749
                        },
                        {
                            "start": 752,
                            "end": 814
                        },
                        {
                            "start": 817,
                            "end": 892
                        },
                        {
                            "start": 895,
                            "end": 898
                        },
                        {
                            "start": 899,
                            "end": 994
                        },
                        {
                            "start": 997,
                            "end": 1108
                        },
                        {
                            "start": 1109,
                            "end": 1248
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 90,
                            "end": 116,
                            "matchedPaperCorpusId": "255372747"
                        },
                        {
                            "start": 152,
                            "end": 172,
                            "matchedPaperCorpusId": "271745835"
                        },
                        {
                            "start": 608,
                            "end": 630,
                            "matchedPaperCorpusId": "61815367"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.9404296875
                }
            ],
            "relevance_judgement": 0.9404296875,
            "relevance_judgment_input_expanded": "# Title: Revisiting Large Language Model Pruning using Neuron Semantic Attribution\n# Venue: arXiv.org\n# Authors: Yizhuo Ding, Xinwei Sun, Yanwei Fu, Guosheng Hu\n## Abstract\nModel pruning technique is vital for accelerating large language models by reducing their size and computational requirements. However, the generalizability of existing pruning methods across diverse datasets and tasks remains unclear. Thus, we conduct extensive evaluations on 24 datasets and 4 tasks using popular pruning methods. Based on these evaluations, we find and then investigate that calibration set greatly affect the performance of pruning methods. In addition, we surprisingly find a significant performance drop of existing pruning methods in sentiment classification tasks. To understand the link between performance drop and pruned neurons, we propose Neuron Semantic Attribution, which learns to associate each neuron with specific semantics. This method first makes the unpruned neurons of LLMs explainable.\n## Pruning Methods\nIn this study, we employed three state-of-the-art posttraining pruning methods: SparseGPT (Frantar & Alistarh, 2023), Wanda (Sun et al., 2023), and RIA (Zhang et al., 2024). These methods are designed to reduce the size of large language models (LLMs) while maintaining their performance across various tasks. Even though all three methods prune the model using a combination of weights W and activations X, they each use different metrics M and strategies to decide which parameters to prune. \n\n\u2022 SparseGPT: This method selects parameters based on the metric: \n\nAfter pruning, SparseGPT also applies an OBS (Hassibi et al., 1993) update to compensate for the pruned parameters. \n\n\u2022 Wanda: This method uses a simple yet effective metric for pruning: \n\nAfter pruning, there is no need for any parameter update step. \n\n\u2022 RIA: This method calculates the pruning metric using relative importance: \n\na . After calculating the metric, channel permutation is used to adjust the model for N:M sparsity. \n\nThese methods were evaluated on a range of tasks to assess how each performs across different calibration data. Each pruning method was applied iteratively, with models being pruned and evaluated to ensure that performance was maintained across tasks.",
            "reference_string": "[276774084 | Ding et al. | 2025 | Citations: 2]"
        },
        {
            "title": "STADE: Standard Deviation as a Pruning Metric",
            "venue": "arXiv.org",
            "year": 2025,
            "reference_count": 37,
            "citation_count": 0,
            "influential_citation_count": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2503.22451, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2352793764",
                    "name": "Diego Coello de Portugal Mecke"
                },
                {
                    "authorId": "2352792956",
                    "name": "Haya Alyoussef"
                },
                {
                    "authorId": "2352793615",
                    "name": "Ilia Koloiarov"
                },
                {
                    "authorId": "2290075048",
                    "name": "Maximilian Stubbemann"
                },
                {
                    "authorId": "2280660731",
                    "name": "Lars Schmidt-Thieme"
                }
            ],
            "abstract": "Recently, Large Language Models (LLMs) have become very widespread and are used to solve a wide variety of tasks. To successfully handle these tasks, LLMs require longer training times and larger model sizes. This makes LLMs ideal candidates for pruning methods that reduce computational demands while maintaining performance. Previous methods require a retraining phase after pruning to maintain the original model's performance. However, state-of-the-art pruning methods, such as Wanda, prune the model without retraining, making the pruning process faster and more efficient. Building upon Wanda's work, this study provides a theoretical explanation of why the method is effective and leverages these insights to enhance the pruning process. Specifically, a theoretical analysis of the pruning problem reveals a common scenario in Machine Learning where Wanda is the optimal pruning method. Furthermore, this analysis is extended to cases where Wanda is no longer optimal, leading to the development of a new method, STADE, based on the standard deviation of the input. From a theoretical standpoint, STADE demonstrates better generality across different scenarios. Finally, extensive experiments on Llama and Open Pre-trained Transformers (OPT) models validate these theoretical findings, showing that depending on the training conditions, Wanda's optimal performance varies as predicted by the theoretical framework. These insights contribute to a more robust understanding of pruning strategies and their practical implications. Code is available at: https://github.com/Coello-dev/STADE/",
            "corpus_id": 277435040,
            "sentences": [
                {
                    "corpus_id": "277435040",
                    "title": "STADE: Standard Deviation as a Pruning Metric",
                    "text": "Recently, Large Language Models (LLMs) have become very widespread and are used to solve a wide variety of tasks. To successfully handle these tasks, LLMs require longer training times and larger model sizes. This makes LLMs ideal candidates for pruning methods that reduce computational demands while maintaining performance. Previous methods require a retraining phase after pruning to maintain the original model's performance. However, state-of-the-art pruning methods, such as Wanda, prune the model without retraining, making the pruning process faster and more efficient. Building upon Wanda's work, this study provides a theoretical explanation of why the method is effective and leverages these insights to enhance the pruning process. Specifically, a theoretical analysis of the pruning problem reveals a common scenario in Machine Learning where Wanda is the optimal pruning method. Furthermore, this analysis is extended to cases where Wanda is no longer optimal, leading to the development of a new method, STADE, based on the standard deviation of the input. From a theoretical standpoint, STADE demonstrates better generality across different scenarios. Finally, extensive experiments on Llama and Open Pre-trained Transformers (OPT) models validate these theoretical findings, showing that depending on the training conditions, Wanda's optimal performance varies as predicted by the theoretical framework. These insights contribute to a more robust understanding of pruning strategies and their practical implications. Code is available at: https://github.com/Coello-dev/STADE/",
                    "score": 0.9166916469172282,
                    "section_title": "abstract",
                    "char_start_offset": 0,
                    "sentence_offsets": [],
                    "ref_mentions": [],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.93017578125
                }
            ],
            "relevance_judgement": 0.93017578125,
            "relevance_judgment_input_expanded": "# Title: STADE: Standard Deviation as a Pruning Metric\n# Venue: arXiv.org\n# Authors: Diego Coello de Portugal Mecke, Haya Alyoussef, Ilia Koloiarov, Maximilian Stubbemann, Lars Schmidt-Thieme\n## Abstract\nRecently, Large Language Models (LLMs) have become very widespread and are used to solve a wide variety of tasks. To successfully handle these tasks, LLMs require longer training times and larger model sizes. This makes LLMs ideal candidates for pruning methods that reduce computational demands while maintaining performance. Previous methods require a retraining phase after pruning to maintain the original model's performance. However, state-of-the-art pruning methods, such as Wanda, prune the model without retraining, making the pruning process faster and more efficient. Building upon Wanda's work, this study provides a theoretical explanation of why the method is effective and leverages these insights to enhance the pruning process. Specifically, a theoretical analysis of the pruning problem reveals a common scenario in Machine Learning where Wanda is the optimal pruning method. Furthermore, this analysis is extended to cases where Wanda is no longer optimal, leading to the development of a new method, STADE, based on the standard deviation of the input. From a theoretical standpoint, STADE demonstrates better generality across different scenarios. Finally, extensive experiments on Llama and Open Pre-trained Transformers (OPT) models validate these theoretical findings, showing that depending on the training conditions, Wanda's optimal performance varies as predicted by the theoretical framework. These insights contribute to a more robust understanding of pruning strategies and their practical implications. Code is available at: https://github.com/Coello-dev/STADE/\n",
            "reference_string": "[277435040 | Mecke et al. | 2025 | Citations: 0]"
        },
        {
            "title": "Fine-Tuning and Deploying Large Language Models Over Edges: Issues and Approaches",
            "venue": "arXiv.org",
            "year": 2024,
            "reference_count": 16,
            "citation_count": 5,
            "influential_citation_count": 2,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2408.10691, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2278396109",
                    "name": "Yanjie Dong"
                },
                {
                    "authorId": "2316525665",
                    "name": "Xiaoyi Fan"
                },
                {
                    "authorId": "2266804662",
                    "name": "Fangxin Wang"
                },
                {
                    "authorId": "2278907861",
                    "name": "Chengming Li"
                },
                {
                    "authorId": "2264958744",
                    "name": "Victor C. M. Leung"
                },
                {
                    "authorId": "1718919",
                    "name": "Xiping Hu"
                }
            ],
            "abstract": "Since the invention of GPT2--1.5B in 2019, large language models (LLMs) have transitioned from specialized models to versatile foundation models. The LLMs exhibit impressive zero-shot ability, however, require fine-tuning on local datasets and significant resources for deployment. Traditional fine-tuning techniques with the first-order optimizers require substantial GPU memory that exceeds mainstream hardware capability. Therefore, memory-efficient methods are motivated to be investigated. Model compression techniques can reduce energy consumption, operational costs, and environmental impact so that to support sustainable artificial intelligence advancements. Additionally, large-scale foundation models have expanded to create images, audio, videos, and multi-modal contents, further emphasizing the need for efficient deployment. Therefore, we are motivated to present a comprehensive overview of the prevalent memory-efficient fine-tuning methods over the network edge. We also review the state-of-the-art literatures on model compression to provide a vision on deploying LLMs over the network edge.",
            "corpus_id": 271909626,
            "sentences": [
                {
                    "corpus_id": "271909626",
                    "title": "Fine-Tuning and Deploying Large Language Models Over Edges: Issues and Approaches",
                    "text": "Notwithstanding the compression capability, the knowledge distillation usually consumes a long duration to obtain a student model (e.g., 14 GPU days for distilling TinyBERT [13]) that works for specific tasks. In order to reduce the training duration, the model pruning technology can be used to remove unnecessary neurons in the LLMs while retaining the versatility of the pruned LLMs. In this vein, X. Ma et al. propose a task-agnostic pruner (named as, the LLM-Pruner) to preserve the capability to handle various task without requiring original training dataset and long duration  of retraining. Since the LLMs contain redundant parameters that have little/no effects on the performance of models, the LLM-Pruner reduces the scale of LLMs based on the gradient information. More specifically, the LLM-Pruner incorporates three main stages: discovery, estimation, and recovery. The detailed functionalities of the LLM-Pruner are as follows. \n\n\u2022 Discovery stage involves finding dependencies within the model such that the pruned neurons do not disproportionately affect others. \u2022 Estimation stage evaluates the importance of each identified group of neurons based on their contribution to the overall performance that can be estimated via the firstorder and the approximated second-order derivatives. \u2022 Recovery stage leverages a fast low-rank approximation method to reduce the required duration and data for performance recovery. After the above three stages, the performance pruned models can be quickly retained using the LoRA technique with a minimal dataset. Their numerical results also demonstrate that the LLM-Pruner can effectively reduce the model size and computational demands without significantly sacrificing performance. Even after pruning 20% of the parameters, the pruned models retain over 94% of the original performance.",
                    "score": 0.6114761562979609,
                    "section_title": "B. Compression-Then-Train",
                    "char_start_offset": 22469,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 209
                        },
                        {
                            "start": 210,
                            "end": 386
                        },
                        {
                            "start": 387,
                            "end": 599
                        },
                        {
                            "start": 600,
                            "end": 777
                        },
                        {
                            "start": 778,
                            "end": 880
                        },
                        {
                            "start": 881,
                            "end": 943
                        },
                        {
                            "start": 946,
                            "end": 1080
                        },
                        {
                            "start": 1081,
                            "end": 1303
                        },
                        {
                            "start": 1304,
                            "end": 1434
                        },
                        {
                            "start": 1435,
                            "end": 1567
                        },
                        {
                            "start": 1568,
                            "end": 1739
                        },
                        {
                            "start": 1740,
                            "end": 1844
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 173,
                            "end": 177,
                            "matchedPaperCorpusId": "258823276"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.9091796875
                }
            ],
            "relevance_judgement": 0.9091796875,
            "relevance_judgment_input_expanded": "# Title: Fine-Tuning and Deploying Large Language Models Over Edges: Issues and Approaches\n# Venue: arXiv.org\n# Authors: Yanjie Dong, Xiaoyi Fan, Fangxin Wang, Chengming Li, Victor C. M. Leung, Xiping Hu\n## Abstract\nSince the invention of GPT2--1.5B in 2019, large language models (LLMs) have transitioned from specialized models to versatile foundation models. The LLMs exhibit impressive zero-shot ability, however, require fine-tuning on local datasets and significant resources for deployment. Traditional fine-tuning techniques with the first-order optimizers require substantial GPU memory that exceeds mainstream hardware capability. Therefore, memory-efficient methods are motivated to be investigated. Model compression techniques can reduce energy consumption, operational costs, and environmental impact so that to support sustainable artificial intelligence advancements. Additionally, large-scale foundation models have expanded to create images, audio, videos, and multi-modal contents, further emphasizing the need for efficient deployment. Therefore, we are motivated to present a comprehensive overview of the prevalent memory-efficient fine-tuning methods over the network edge. We also review the state-of-the-art literatures on model compression to provide a vision on deploying LLMs over the network edge.\n## B. Compression-Then-Train\nNotwithstanding the compression capability, the knowledge distillation usually consumes a long duration to obtain a student model (e.g., 14 GPU days for distilling TinyBERT [13]) that works for specific tasks. In order to reduce the training duration, the model pruning technology can be used to remove unnecessary neurons in the LLMs while retaining the versatility of the pruned LLMs. In this vein, X. Ma et al. propose a task-agnostic pruner (named as, the LLM-Pruner) to preserve the capability to handle various task without requiring original training dataset and long duration  of retraining. Since the LLMs contain redundant parameters that have little/no effects on the performance of models, the LLM-Pruner reduces the scale of LLMs based on the gradient information. More specifically, the LLM-Pruner incorporates three main stages: discovery, estimation, and recovery. The detailed functionalities of the LLM-Pruner are as follows. \n\n\u2022 Discovery stage involves finding dependencies within the model such that the pruned neurons do not disproportionately affect others. \u2022 Estimation stage evaluates the importance of each identified group of neurons based on their contribution to the overall performance that can be estimated via the firstorder and the approximated second-order derivatives. \u2022 Recovery stage leverages a fast low-rank approximation method to reduce the required duration and data for performance recovery. After the above three stages, the performance pruned models can be quickly retained using the LoRA technique with a minimal dataset. Their numerical results also demonstrate that the LLM-Pruner can effectively reduce the model size and computational demands without significantly sacrificing performance. Even after pruning 20% of the parameters, the pruned models retain over 94% of the original performance.",
            "reference_string": "[271909626 | Dong et al. | 2024 | Citations: 5]"
        },
        {
            "title": "Structured Optimal Brain Pruning for Large Language Models",
            "venue": "Conference on Empirical Methods in Natural Language Processing",
            "year": 2024,
            "reference_count": 0,
            "citation_count": 4,
            "influential_citation_count": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": "CLOSED",
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://aclanthology.org/2024.emnlp-main.775, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2330242586",
                    "name": "Jiateng Wei"
                },
                {
                    "authorId": "2329898504",
                    "name": "Quan Lu"
                },
                {
                    "authorId": "2329738680",
                    "name": "Ning Jiang"
                },
                {
                    "authorId": "2258340244",
                    "name": "Siqi Li"
                },
                {
                    "authorId": "2256984205",
                    "name": "Jingyang Xiang"
                },
                {
                    "authorId": "2257200295",
                    "name": "Jun Chen"
                },
                {
                    "authorId": "2257376000",
                    "name": "Yong Liu"
                }
            ],
            "abstract": "The massive parameters and computational demands hinder the widespread application of Large Language Models (LLMs). Network pruning provides a practical solution to this problem. However, existing pruning works for LLMs mainly focus on unstructured pruning or necessitate post-pruning fine-tuning. The former relies on special hardware to accelerate computation, while the latter may need substantial computational resources. In this paper, we introduce a retraining-free structured pruning method called SoBP (Structured Optimal Brain Pruning). It leverages global first-order information to select pruning structures, then refines them with a local greedy approach, and finally adopts module-wise reconstruction to mitigate information loss. We assess the effectiveness of SoBP across 14 models from 3 LLM families on 8 distinct datasets. Experimental results demonstrate that SoBP outperforms current state-of-the-art methods.",
            "corpus_id": 273901152,
            "sentences": [
                {
                    "corpus_id": "273901152",
                    "title": "Structured Optimal Brain Pruning for Large Language Models",
                    "text": "The massive parameters and computational demands hinder the widespread application of Large Language Models (LLMs). Network pruning provides a practical solution to this problem. However, existing pruning works for LLMs mainly focus on unstructured pruning or necessitate post-pruning fine-tuning. The former relies on special hardware to accelerate computation, while the latter may need substantial computational resources. In this paper, we introduce a retraining-free structured pruning method called SoBP (Structured Optimal Brain Pruning). It leverages global first-order information to select pruning structures, then refines them with a local greedy approach, and finally adopts module-wise reconstruction to mitigate information loss. We assess the effectiveness of SoBP across 14 models from 3 LLM families on 8 distinct datasets. Experimental results demonstrate that SoBP outperforms current state-of-the-art methods.",
                    "score": 0.6499004488730877,
                    "section_title": "abstract",
                    "char_start_offset": 0,
                    "sentence_offsets": [],
                    "ref_mentions": [],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.89794921875
                }
            ],
            "relevance_judgement": 0.89794921875,
            "relevance_judgment_input_expanded": "# Title: Structured Optimal Brain Pruning for Large Language Models\n# Venue: Conference on Empirical Methods in Natural Language Processing\n# Authors: Jiateng Wei, Quan Lu, Ning Jiang, Siqi Li, Jingyang Xiang, Jun Chen, Yong Liu\n## Abstract\nThe massive parameters and computational demands hinder the widespread application of Large Language Models (LLMs). Network pruning provides a practical solution to this problem. However, existing pruning works for LLMs mainly focus on unstructured pruning or necessitate post-pruning fine-tuning. The former relies on special hardware to accelerate computation, while the latter may need substantial computational resources. In this paper, we introduce a retraining-free structured pruning method called SoBP (Structured Optimal Brain Pruning). It leverages global first-order information to select pruning structures, then refines them with a local greedy approach, and finally adopts module-wise reconstruction to mitigate information loss. We assess the effectiveness of SoBP across 14 models from 3 LLM families on 8 distinct datasets. Experimental results demonstrate that SoBP outperforms current state-of-the-art methods.\n",
            "reference_string": "[273901152 | Wei et al. | 2024 | Citations: 4]"
        },
        {
            "title": "A Simple and Effective Pruning Approach for Large Language Models",
            "venue": "International Conference on Learning Representations",
            "year": 2023,
            "reference_count": 107,
            "citation_count": 439,
            "influential_citation_count": 99,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "http://arxiv.org/pdf/2306.11695",
                "status": "CLOSED",
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2306.11695, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2984183",
                    "name": "Mingjie Sun"
                },
                {
                    "authorId": "2109168016",
                    "name": "Zhuang Liu"
                },
                {
                    "authorId": "25901845",
                    "name": "Anna Bair"
                },
                {
                    "authorId": "145116464",
                    "name": "J. Z. Kolter"
                }
            ],
            "abstract": "As their size increases, Large Languages Models (LLMs) are natural candidates for network pruning methods: approaches that drop a subset of network weights while striving to preserve performance. Existing methods, however, require either retraining, which is rarely affordable for billion-scale LLMs, or solving a weight reconstruction problem reliant on second-order information, which may also be computationally expensive. In this paper, we introduce a novel, straightforward yet effective pruning method, termed Wanda (Pruning by Weights and activations), designed to induce sparsity in pretrained LLMs. Motivated by the recent observation of emergent large magnitude features in LLMs, our approach prunes weights with the smallest magnitudes multiplied by the corresponding input activations, on a per-output basis. Notably, Wanda requires no retraining or weight update, and the pruned LLM can be used as is. We conduct a thorough evaluation of our method Wanda on LLaMA and LLaMA-2 across various language benchmarks. Wanda significantly outperforms the established baseline of magnitude pruning and performs competitively against recent method involving intensive weight update. Code is available at https://github.com/locuslab/wanda.",
            "corpus_id": 259203115,
            "sentences": [
                {
                    "corpus_id": "259203115",
                    "title": "A Simple and Effective Pruning Approach for Large Language Models",
                    "text": "As their size increases, Large Languages Models (LLMs) are natural candidates for network pruning methods: approaches that drop a subset of network weights while striving to preserve performance. Existing methods, however, require either retraining, which is rarely affordable for billion-scale LLMs, or solving a weight reconstruction problem reliant on second-order information, which may also be computationally expensive. In this paper, we introduce a novel, straightforward yet effective pruning method, termed Wanda (Pruning by Weights and activations), designed to induce sparsity in pretrained LLMs. Motivated by the recent observation of emergent large magnitude features in LLMs, our approach prunes weights with the smallest magnitudes multiplied by the corresponding input activations, on a per-output basis. Notably, Wanda requires no retraining or weight update, and the pruned LLM can be used as is. We conduct a thorough evaluation of our method Wanda on LLaMA and LLaMA-2 across various language benchmarks. Wanda significantly outperforms the established baseline of magnitude pruning and performs competitively against recent method involving intensive weight update. Code is available at https://github.com/locuslab/wanda.",
                    "score": 0.8037036472019136,
                    "section_title": "abstract",
                    "char_start_offset": 0,
                    "sentence_offsets": [],
                    "ref_mentions": [],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.88818359375
                },
                {
                    "corpus_id": "259203115",
                    "title": "A Simple and Effective Pruning Approach for Large Language Models",
                    "text": "We empirically evaluate Wanda on the widely adopted LLaMA (Touvron et al., 2023a) and LLaMA-2 (Touvron et al., 2023b) model families. Our results demonstrate Wanda can find efficient sparse networks from pretrained LLMs, without any retraining or weight update. Our approach Wanda outperforms the standard magnitude pruning by a large margin and also competes favorably with the prior best LLM pruning method (Frantar & Alistarh, 2023), while requiring a lower computational cost. We hope our work serves as a baseline for future work in this area, and encourages further exploration in understanding sparsity in LLMs.",
                    "score": 0.61281907437708,
                    "section_title": "Weights and activations",
                    "char_start_offset": 4400,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 133
                        },
                        {
                            "start": 134,
                            "end": 261
                        },
                        {
                            "start": 262,
                            "end": 480
                        },
                        {
                            "start": 481,
                            "end": 618
                        }
                    ],
                    "ref_mentions": [],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.873046875
                }
            ],
            "relevance_judgement": 0.88818359375,
            "relevance_judgment_input_expanded": "# Title: A Simple and Effective Pruning Approach for Large Language Models\n# Venue: International Conference on Learning Representations\n# Authors: Mingjie Sun, Zhuang Liu, Anna Bair, J. Z. Kolter\n## Abstract\nAs their size increases, Large Languages Models (LLMs) are natural candidates for network pruning methods: approaches that drop a subset of network weights while striving to preserve performance. Existing methods, however, require either retraining, which is rarely affordable for billion-scale LLMs, or solving a weight reconstruction problem reliant on second-order information, which may also be computationally expensive. In this paper, we introduce a novel, straightforward yet effective pruning method, termed Wanda (Pruning by Weights and activations), designed to induce sparsity in pretrained LLMs. Motivated by the recent observation of emergent large magnitude features in LLMs, our approach prunes weights with the smallest magnitudes multiplied by the corresponding input activations, on a per-output basis. Notably, Wanda requires no retraining or weight update, and the pruned LLM can be used as is. We conduct a thorough evaluation of our method Wanda on LLaMA and LLaMA-2 across various language benchmarks. Wanda significantly outperforms the established baseline of magnitude pruning and performs competitively against recent method involving intensive weight update. Code is available at https://github.com/locuslab/wanda.\n## Weights and activations\nWe empirically evaluate Wanda on the widely adopted LLaMA (Touvron et al., 2023a) and LLaMA-2 (Touvron et al., 2023b) model families. Our results demonstrate Wanda can find efficient sparse networks from pretrained LLMs, without any retraining or weight update. Our approach Wanda outperforms the standard magnitude pruning by a large margin and also competes favorably with the prior best LLM pruning method (Frantar & Alistarh, 2023), while requiring a lower computational cost. We hope our work serves as a baseline for future work in this area, and encourages further exploration in understanding sparsity in LLMs.",
            "reference_string": "[259203115 | Sun et al. | 2023 | Citations: 439]"
        },
        {
            "title": "Fluctuation-based Adaptive Structured Pruning for Large Language Models",
            "venue": "AAAI Conference on Artificial Intelligence",
            "year": 2023,
            "reference_count": 33,
            "citation_count": 61,
            "influential_citation_count": 14,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2312.11983, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2167834971",
                    "name": "Yongqi An"
                },
                {
                    "authorId": "2118489444",
                    "name": "Xu Zhao"
                },
                {
                    "authorId": "40418746",
                    "name": "Tao Yu"
                },
                {
                    "authorId": "2113727378",
                    "name": "Ming Tang"
                },
                {
                    "authorId": "2241943585",
                    "name": "Jinqiao Wang"
                }
            ],
            "abstract": "Network Pruning is a promising way to address the huge computing resource demands of the deployment and inference of Large Language Models (LLMs). Retraining-free is important for LLMs' pruning methods. However, almost all of the existing retraining-free pruning approaches for LLMs focus on unstructured pruning, which requires specific hardware support for acceleration. In this paper, we propose a novel retraining-free structured pruning framework for LLMs, named FLAP (FLuctuation-based Adaptive\nStructured Pruning). It is hardware-friendly by effectively reducing storage and enhancing inference speed. For effective structured pruning of LLMs, we highlight three critical elements that demand the utmost attention: formulating structured importance metrics, adaptively searching the global compressed model, and implementing compensation mechanisms to mitigate performance loss. First, FLAP determines whether the output feature map is easily recoverable when a column of weight is removed, based on the fluctuation pruning metric. Then it standardizes the importance scores to adaptively determine the global compressed model structure. At last, FLAP adds additional bias terms to recover the output feature maps using the baseline values. We thoroughly evaluate our approach on a variety of language benchmarks. Without any retraining, our method significantly outperforms the state-of-the-art methods, including LLM-Pruner and the extension of Wanda in structured pruning. The code is released at https://github.com/CASIA-IVA-Lab/FLAP.",
            "corpus_id": 266362404,
            "sentences": [
                {
                    "corpus_id": "266362404",
                    "title": "Fluctuation-based Adaptive Structured Pruning for Large Language Models",
                    "text": "We assessed the zero-shot capability of the pruned model across seven downstream tasks. As illustrated in Table 2, our method consistently outperforms LLM-Pruner with LoRA Fine-Tuning, achieving superior performance across varying pruning ratios, all without the need for retraining. At a 20% pruning ratio, Wanda-sp exhibits remarkable zero-shot capabilities, even surpassing the performance of the original, unpruned model. This suggests the presence of structured redundancy within LLMs that can be pruned away without necessitating retraining, thereby potentially enhancing model efficiency. However, when the pruning ratio is increased to 50%, the performance of Wanda-sp suffers a significant degradation. In stark contrast, our method continues to excel, maintaining a clear advantage over other approaches. This finding demonstrates the efficacy of our structured pruning method in preserving the generalization capabilities of large language models (LLMs), even under stringent pruning conditions.",
                    "score": 0.8191317890556897,
                    "section_title": "Zero-shot Tasks Performance",
                    "char_start_offset": 17417,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 87
                        },
                        {
                            "start": 88,
                            "end": 283
                        },
                        {
                            "start": 284,
                            "end": 425
                        },
                        {
                            "start": 426,
                            "end": 595
                        },
                        {
                            "start": 596,
                            "end": 711
                        },
                        {
                            "start": 712,
                            "end": 814
                        },
                        {
                            "start": 815,
                            "end": 1006
                        }
                    ],
                    "ref_mentions": [],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.88671875
                },
                {
                    "corpus_id": "266362404",
                    "title": "Fluctuation-based Adaptive Structured Pruning for Large Language Models",
                    "text": "In this work, we propose FLAP (FLuctuation-based Adaptive Structured Pruning), a retraining-free structured pruning framework explicitly designed for Large Language Models (LLMs). To address the challenges posed by structured pruning, we introduce a novel structured pruning metric, employ adaptive global model compression strategies, and implement robust compensation mechanisms designed to mitigate potential performance losses. Our empirical results affirm that the structured compression model crafted by FLAP can maintain perplexity and zero-shot performance without any retraining. Especially worth noting is the efficacy of FLAP in upholding model performance at both low and medium compression rates. Our work demonstrates that bias compensation can largely replace retraining or parameter efficient fine-tuning (PEFT). We hope that our work contributes to a better understanding of structured pruning and performance recovery of LLMs.",
                    "score": 0.6198534918835052,
                    "section_title": "Conclusion",
                    "char_start_offset": 23781,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 179
                        },
                        {
                            "start": 180,
                            "end": 431
                        },
                        {
                            "start": 432,
                            "end": 588
                        },
                        {
                            "start": 589,
                            "end": 709
                        },
                        {
                            "start": 710,
                            "end": 828
                        },
                        {
                            "start": 829,
                            "end": 944
                        }
                    ],
                    "ref_mentions": [],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.87158203125
                },
                {
                    "corpus_id": "266362404",
                    "title": "Fluctuation-based Adaptive Structured Pruning for Large Language Models",
                    "text": "Network Pruning is a promising way to address the huge computing resource demands of the deployment and inference of Large Language Models (LLMs). Retraining-free is important for LLMs' pruning methods. However, almost all of the existing retraining-free pruning approaches for LLMs focus on unstructured pruning, which requires specific hardware support for acceleration. In this paper, we propose a novel retraining-free structured pruning framework for LLMs, named FLAP (FLuctuation-based Adaptive\nStructured Pruning). It is hardware-friendly by effectively reducing storage and enhancing inference speed. For effective structured pruning of LLMs, we highlight three critical elements that demand the utmost attention: formulating structured importance metrics, adaptively searching the global compressed model, and implementing compensation mechanisms to mitigate performance loss. First, FLAP determines whether the output feature map is easily recoverable when a column of weight is removed, based on the fluctuation pruning metric. Then it standardizes the importance scores to adaptively determine the global compressed model structure. At last, FLAP adds additional bias terms to recover the output feature maps using the baseline values. We thoroughly evaluate our approach on a variety of language benchmarks. Without any retraining, our method significantly outperforms the state-of-the-art methods, including LLM-Pruner and the extension of Wanda in structured pruning. The code is released at https://github.com/CASIA-IVA-Lab/FLAP.",
                    "score": 0.7063340103127045,
                    "section_title": "abstract",
                    "char_start_offset": 0,
                    "sentence_offsets": [],
                    "ref_mentions": [],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.83154296875
                }
            ],
            "relevance_judgement": 0.88671875,
            "relevance_judgment_input_expanded": "# Title: Fluctuation-based Adaptive Structured Pruning for Large Language Models\n# Venue: AAAI Conference on Artificial Intelligence\n# Authors: Yongqi An, Xu Zhao, Tao Yu, Ming Tang, Jinqiao Wang\n## Abstract\nNetwork Pruning is a promising way to address the huge computing resource demands of the deployment and inference of Large Language Models (LLMs). Retraining-free is important for LLMs' pruning methods. However, almost all of the existing retraining-free pruning approaches for LLMs focus on unstructured pruning, which requires specific hardware support for acceleration. In this paper, we propose a novel retraining-free structured pruning framework for LLMs, named FLAP (FLuctuation-based Adaptive\nStructured Pruning). It is hardware-friendly by effectively reducing storage and enhancing inference speed. For effective structured pruning of LLMs, we highlight three critical elements that demand the utmost attention: formulating structured importance metrics, adaptively searching the global compressed model, and implementing compensation mechanisms to mitigate performance loss. First, FLAP determines whether the output feature map is easily recoverable when a column of weight is removed, based on the fluctuation pruning metric. Then it standardizes the importance scores to adaptively determine the global compressed model structure. At last, FLAP adds additional bias terms to recover the output feature maps using the baseline values. We thoroughly evaluate our approach on a variety of language benchmarks. Without any retraining, our method significantly outperforms the state-of-the-art methods, including LLM-Pruner and the extension of Wanda in structured pruning. The code is released at https://github.com/CASIA-IVA-Lab/FLAP.\n## Zero-shot Tasks Performance\nWe assessed the zero-shot capability of the pruned model across seven downstream tasks. As illustrated in Table 2, our method consistently outperforms LLM-Pruner with LoRA Fine-Tuning, achieving superior performance across varying pruning ratios, all without the need for retraining. At a 20% pruning ratio, Wanda-sp exhibits remarkable zero-shot capabilities, even surpassing the performance of the original, unpruned model. This suggests the presence of structured redundancy within LLMs that can be pruned away without necessitating retraining, thereby potentially enhancing model efficiency. However, when the pruning ratio is increased to 50%, the performance of Wanda-sp suffers a significant degradation. In stark contrast, our method continues to excel, maintaining a clear advantage over other approaches. This finding demonstrates the efficacy of our structured pruning method in preserving the generalization capabilities of large language models (LLMs), even under stringent pruning conditions.\n\n## Conclusion\nIn this work, we propose FLAP (FLuctuation-based Adaptive Structured Pruning), a retraining-free structured pruning framework explicitly designed for Large Language Models (LLMs). To address the challenges posed by structured pruning, we introduce a novel structured pruning metric, employ adaptive global model compression strategies, and implement robust compensation mechanisms designed to mitigate potential performance losses. Our empirical results affirm that the structured compression model crafted by FLAP can maintain perplexity and zero-shot performance without any retraining. Especially worth noting is the efficacy of FLAP in upholding model performance at both low and medium compression rates. Our work demonstrates that bias compensation can largely replace retraining or parameter efficient fine-tuning (PEFT). We hope that our work contributes to a better understanding of structured pruning and performance recovery of LLMs.",
            "reference_string": "[266362404 | An et al. | 2023 | Citations: 61]"
        },
        {
            "title": "LoRAShear: Efficient Large Language Model Structured Pruning and Knowledge Recovery",
            "venue": "arXiv.org",
            "year": 2023,
            "reference_count": 28,
            "citation_count": 32,
            "influential_citation_count": 2,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2310.18356, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2257252148",
                    "name": "Tianyi Chen"
                },
                {
                    "authorId": "2257191230",
                    "name": "Tianyu Ding"
                },
                {
                    "authorId": "2262446441",
                    "name": "Badal Yadav"
                },
                {
                    "authorId": "15623770",
                    "name": "Ilya Zharkov"
                },
                {
                    "authorId": "46225943",
                    "name": "Luming Liang"
                }
            ],
            "abstract": "Large Language Models (LLMs) have transformed the landscape of artificial intelligence, while their enormous size presents significant challenges in terms of computational costs. We introduce LoRAShear, a novel efficient approach to structurally prune LLMs and recover knowledge. Given general LLMs, LoRAShear at first creates the dependency graphs over LoRA modules to discover minimally removal structures and analyze the knowledge distribution. It then proceeds progressive structured pruning on LoRA adaptors and enables inherent knowledge transfer to better preserve the information in the redundant structures. To recover the lost knowledge during pruning, LoRAShear meticulously studies and proposes a dynamic fine-tuning schemes with dynamic data adaptors to effectively narrow down the performance gap to the full models. Numerical results demonstrate that by only using one GPU within a couple of GPU days, LoRAShear effectively reduced footprint of LLMs by 20% with only 1.0% performance degradation and significantly outperforms state-of-the-arts. The source code will be available at https://github.com/microsoft/lorashear.",
            "corpus_id": 264590698,
            "sentences": [
                {
                    "corpus_id": "264590698",
                    "title": "LoRAShear: Efficient Large Language Model Structured Pruning and Knowledge Recovery",
                    "text": "While pruning (Han et al., 2015) is well-established in traditional Deep Neural Networks (DNNs), its application to LLMs presents unique challenges. Unlike the smaller, taskspecific DNNs (Ding et al., 2021;2022), LLMs have a large number of parameters and require significant computational resources (Brown et al., 2020). Moreover, it's crucial for them to generalize well across multiple tasks (Xia et al., 2023). Recently, various pruning methods have been developed specifically for LLMs, generally falling into two main categories: unstructured and structured. \n\nUnstructured Pruning. Unstructured pruning methods (Dong et al., 2017;Chen et al., 2020;2021a) focus on setting unimportant individual weights in the model to zero. This fine-grained approach is straightforward and often maintains good performance, even with high compression rates. However, it results in sparse weight matrices that aren't well-suited for hardware accelerators, making them less efficient in real-world deployment. In the realm of LLMs, several new techniques have emerged. \n\nSparseGPT (Frantar & Alistarh, 2023) uses a sophisticated weight update process involving synchronized secondorder Hessian updates, bypassing traditional retraining. In contrast, Wanda (Sun et al., 2023) achieves high sparsity without any retraining, simply by pruning weights with the smallest magnitudes multiplied by their corresponding input activations. PST (Li et al., 2022), however, combines unstructured pruning with efficient fine-tuning, pruning both LoRA and pre-trained model weights. A drawback of this method is the need for a memory-intensive mask that matches the shape of the pre-trained weights. \n\nStructured Pruning. Structured pruning methods (Chen et al., 2021b;2023a;b) focus on removing entire groups of parameters, such as neurons or layers, rather than in-dividual weights. This group-level approach is hardwarefriendly as it maintains dense weight matrices. The main challenge is selecting which structures to remove without compromising model performance.",
                    "score": 0.5981957451985652,
                    "section_title": "Related Work",
                    "char_start_offset": 4789,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 148
                        },
                        {
                            "start": 149,
                            "end": 321
                        },
                        {
                            "start": 322,
                            "end": 414
                        },
                        {
                            "start": 415,
                            "end": 564
                        },
                        {
                            "start": 567,
                            "end": 588
                        },
                        {
                            "start": 589,
                            "end": 731
                        },
                        {
                            "start": 732,
                            "end": 849
                        },
                        {
                            "start": 850,
                            "end": 999
                        },
                        {
                            "start": 1000,
                            "end": 1058
                        },
                        {
                            "start": 1061,
                            "end": 1226
                        },
                        {
                            "start": 1227,
                            "end": 1419
                        },
                        {
                            "start": 1420,
                            "end": 1558
                        },
                        {
                            "start": 1559,
                            "end": 1675
                        },
                        {
                            "start": 1678,
                            "end": 1697
                        },
                        {
                            "start": 1698,
                            "end": 1860
                        },
                        {
                            "start": 1861,
                            "end": 1945
                        },
                        {
                            "start": 1946,
                            "end": 2044
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 300,
                            "end": 320,
                            "matchedPaperCorpusId": "218971783"
                        },
                        {
                            "start": 655,
                            "end": 661,
                            "matchedPaperCorpusId": "215416095"
                        },
                        {
                            "start": 1071,
                            "end": 1096,
                            "matchedPaperCorpusId": "255372747"
                        },
                        {
                            "start": 1725,
                            "end": 1745,
                            "matchedPaperCorpusId": "235899080"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.8828125
                }
            ],
            "relevance_judgement": 0.8828125,
            "relevance_judgment_input_expanded": "# Title: LoRAShear: Efficient Large Language Model Structured Pruning and Knowledge Recovery\n# Venue: arXiv.org\n# Authors: Tianyi Chen, Tianyu Ding, Badal Yadav, Ilya Zharkov, Luming Liang\n## Abstract\nLarge Language Models (LLMs) have transformed the landscape of artificial intelligence, while their enormous size presents significant challenges in terms of computational costs. We introduce LoRAShear, a novel efficient approach to structurally prune LLMs and recover knowledge. Given general LLMs, LoRAShear at first creates the dependency graphs over LoRA modules to discover minimally removal structures and analyze the knowledge distribution. It then proceeds progressive structured pruning on LoRA adaptors and enables inherent knowledge transfer to better preserve the information in the redundant structures. To recover the lost knowledge during pruning, LoRAShear meticulously studies and proposes a dynamic fine-tuning schemes with dynamic data adaptors to effectively narrow down the performance gap to the full models. Numerical results demonstrate that by only using one GPU within a couple of GPU days, LoRAShear effectively reduced footprint of LLMs by 20% with only 1.0% performance degradation and significantly outperforms state-of-the-arts. The source code will be available at https://github.com/microsoft/lorashear.\n## Related Work\nWhile pruning (Han et al., 2015) is well-established in traditional Deep Neural Networks (DNNs), its application to LLMs presents unique challenges. Unlike the smaller, taskspecific DNNs (Ding et al., 2021;2022), LLMs have a large number of parameters and require significant computational resources (Brown et al., 2020). Moreover, it's crucial for them to generalize well across multiple tasks (Xia et al., 2023). Recently, various pruning methods have been developed specifically for LLMs, generally falling into two main categories: unstructured and structured. \n\nUnstructured Pruning. Unstructured pruning methods (Dong et al., 2017;Chen et al., 2020;2021a) focus on setting unimportant individual weights in the model to zero. This fine-grained approach is straightforward and often maintains good performance, even with high compression rates. However, it results in sparse weight matrices that aren't well-suited for hardware accelerators, making them less efficient in real-world deployment. In the realm of LLMs, several new techniques have emerged. \n\nSparseGPT (Frantar & Alistarh, 2023) uses a sophisticated weight update process involving synchronized secondorder Hessian updates, bypassing traditional retraining. In contrast, Wanda (Sun et al., 2023) achieves high sparsity without any retraining, simply by pruning weights with the smallest magnitudes multiplied by their corresponding input activations. PST (Li et al., 2022), however, combines unstructured pruning with efficient fine-tuning, pruning both LoRA and pre-trained model weights. A drawback of this method is the need for a memory-intensive mask that matches the shape of the pre-trained weights. \n\nStructured Pruning. Structured pruning methods (Chen et al., 2021b;2023a;b) focus on removing entire groups of parameters, such as neurons or layers, rather than in-dividual weights. This group-level approach is hardwarefriendly as it maintains dense weight matrices. The main challenge is selecting which structures to remove without compromising model performance.",
            "reference_string": "[264590698 | Chen et al. | 2023 | Citations: 32]"
        },
        {
            "title": "LLM-Barber: Block-Aware Rebuilder for Sparsity Mask in One-Shot for Large Language Models",
            "venue": "arXiv.org",
            "year": 2024,
            "reference_count": 24,
            "citation_count": 2,
            "influential_citation_count": 1,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2408.10631, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2286850679",
                    "name": "Yupeng Su"
                },
                {
                    "authorId": "2120170158",
                    "name": "Ziyi Guan"
                },
                {
                    "authorId": "2316519699",
                    "name": "Xiaoqun Liu"
                },
                {
                    "authorId": "2316487762",
                    "name": "Tianlai Jin"
                },
                {
                    "authorId": "2316516436",
                    "name": "Dongkuan Wu"
                },
                {
                    "authorId": "1698669",
                    "name": "G. Chesi"
                },
                {
                    "authorId": "2287187433",
                    "name": "Ngai Wong"
                },
                {
                    "authorId": "2316516782",
                    "name": "Hao Yu"
                }
            ],
            "abstract": "Large language models (LLMs) have grown significantly in scale, leading to a critical need for efficient model pruning techniques. Existing post-training pruning techniques primarily focus on measuring weight importance on converged dense models to determine salient weights to retain. However, they often overlook the changes in weight importance during the pruning process, which can lead to performance degradation in the pruned models. To address this issue, we present LLM-Barber (Block-Aware Rebuilder for Sparsity Mask in One-Shot), a novel one-shot pruning framework that rebuilds the sparsity mask of pruned models without any retraining or weight reconstruction. LLM-Barber incorporates block-aware error optimization across Self-Attention and MLP blocks, ensuring global performance optimization. Inspired by the recent discovery of prominent outliers in LLMs, LLM-Barber introduces an innovative pruning metric that identifies weight importance using weights multiplied by gradients. Our experiments show that LLM-Barber can efficiently prune models like LLaMA and OPT families with 7B to 13B parameters on a single A100 GPU in just 30 minutes, achieving state-of-the-art results in both perplexity and zero-shot performance across various language benchmarks. Code is available at https://github.com/YupengSu/LLM-Barber.",
            "corpus_id": 271909582,
            "sentences": [
                {
                    "corpus_id": "271909582",
                    "title": "LLM-Barber: Block-Aware Rebuilder for Sparsity Mask in One-Shot for Large Language Models",
                    "text": "Large language models (LLMs) have become a cornerstone in natural language processing (NLP) due to their impressive performance on various tasks. However, as these models increase in size and complexity, their deployment poses significant challenges due to extensive computational and storage demands. For instance, models such as GPT-175B (Brown et al. 2020), with 175 billion parameters, require vast resources, making it impractical for many applications. Therefore, efficient model compression strategies are crucial for deploying these powerful models in practical applications. \n\nModel compression techniques commonly employ quantization and pruning to enhance model efficiency. Quantization reduces the precision of model parameters, while pruning removes less critical parameters. Traditional pruning methods, such as magnitude-based pruning (Han et al. 2015), directly trim weights based on their absolute values. While effective for smaller models, these methods often struggle with large-scale LLMs, resulting in suboptimal sparsity due to their inability to capture the complex interactions and importance of weights in such massive scalability. To address these limitations, recent post-training pruning techniques such as SparseGPT and Wanda have emerged. \n\nCurrent pruning methods face two major challenges. First, as depicted in Figure 1(a), traditional layer-aware pruning methods focus on individual layers and neglect inter-layer dependencies within, leading to higher error accumulation (represented by blue arrows). In contrast, block-aware pruning, by considering groups of layers, captures inter-layer interactions to reduce error accumulation (represented by orange arrows). Second, as shown in Figure 1(b), conventional methods typically build the pruning mask once, ignoring the changes of weight significance in post-pruning stage. This oversight can lead to improper identification of salient weights, resulting in performance degradation. \n\nTo address these limitations, we propose LLM-Barber, a novel and straightforward approach designed to rebuild sparsity mask of pruned networks without requiring for retraining or weight reconstruction. Firstly, unlike layer-aware methods that are confined to local optimization and thus arXiv:2408.10631v1",
                    "score": 0.7722434355896809,
                    "section_title": "Introduction",
                    "char_start_offset": 15,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 145
                        },
                        {
                            "start": 146,
                            "end": 301
                        },
                        {
                            "start": 302,
                            "end": 458
                        },
                        {
                            "start": 459,
                            "end": 583
                        },
                        {
                            "start": 586,
                            "end": 684
                        },
                        {
                            "start": 685,
                            "end": 788
                        },
                        {
                            "start": 789,
                            "end": 922
                        },
                        {
                            "start": 923,
                            "end": 1157
                        },
                        {
                            "start": 1158,
                            "end": 1269
                        },
                        {
                            "start": 1272,
                            "end": 1322
                        },
                        {
                            "start": 1323,
                            "end": 1536
                        },
                        {
                            "start": 1537,
                            "end": 1698
                        },
                        {
                            "start": 1699,
                            "end": 1858
                        },
                        {
                            "start": 1859,
                            "end": 1967
                        },
                        {
                            "start": 1970,
                            "end": 2171
                        },
                        {
                            "start": 2172,
                            "end": 2275
                        }
                    ],
                    "ref_mentions": [],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.8818359375
                },
                {
                    "corpus_id": "271909582",
                    "title": "LLM-Barber: Block-Aware Rebuilder for Sparsity Mask in One-Shot for Large Language Models",
                    "text": "Large language models (LLMs) have grown significantly in scale, leading to a critical need for efficient model pruning techniques. Existing post-training pruning techniques primarily focus on measuring weight importance on converged dense models to determine salient weights to retain. However, they often overlook the changes in weight importance during the pruning process, which can lead to performance degradation in the pruned models. To address this issue, we present LLM-Barber (Block-Aware Rebuilder for Sparsity Mask in One-Shot), a novel one-shot pruning framework that rebuilds the sparsity mask of pruned models without any retraining or weight reconstruction. LLM-Barber incorporates block-aware error optimization across Self-Attention and MLP blocks, ensuring global performance optimization. Inspired by the recent discovery of prominent outliers in LLMs, LLM-Barber introduces an innovative pruning metric that identifies weight importance using weights multiplied by gradients. Our experiments show that LLM-Barber can efficiently prune models like LLaMA and OPT families with 7B to 13B parameters on a single A100 GPU in just 30 minutes, achieving state-of-the-art results in both perplexity and zero-shot performance across various language benchmarks. Code is available at https://github.com/YupengSu/LLM-Barber.",
                    "score": 0.6291434891459331,
                    "section_title": "abstract",
                    "char_start_offset": 0,
                    "sentence_offsets": [],
                    "ref_mentions": [],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.83203125
                }
            ],
            "relevance_judgement": 0.8818359375,
            "relevance_judgment_input_expanded": "# Title: LLM-Barber: Block-Aware Rebuilder for Sparsity Mask in One-Shot for Large Language Models\n# Venue: arXiv.org\n# Authors: Yupeng Su, Ziyi Guan, Xiaoqun Liu, Tianlai Jin, Dongkuan Wu, G. Chesi, Ngai Wong, Hao Yu\n## Abstract\nLarge language models (LLMs) have grown significantly in scale, leading to a critical need for efficient model pruning techniques. Existing post-training pruning techniques primarily focus on measuring weight importance on converged dense models to determine salient weights to retain. However, they often overlook the changes in weight importance during the pruning process, which can lead to performance degradation in the pruned models. To address this issue, we present LLM-Barber (Block-Aware Rebuilder for Sparsity Mask in One-Shot), a novel one-shot pruning framework that rebuilds the sparsity mask of pruned models without any retraining or weight reconstruction. LLM-Barber incorporates block-aware error optimization across Self-Attention and MLP blocks, ensuring global performance optimization. Inspired by the recent discovery of prominent outliers in LLMs, LLM-Barber introduces an innovative pruning metric that identifies weight importance using weights multiplied by gradients. Our experiments show that LLM-Barber can efficiently prune models like LLaMA and OPT families with 7B to 13B parameters on a single A100 GPU in just 30 minutes, achieving state-of-the-art results in both perplexity and zero-shot performance across various language benchmarks. Code is available at https://github.com/YupengSu/LLM-Barber.\n## Introduction\nLarge language models (LLMs) have become a cornerstone in natural language processing (NLP) due to their impressive performance on various tasks. However, as these models increase in size and complexity, their deployment poses significant challenges due to extensive computational and storage demands. For instance, models such as GPT-175B (Brown et al. 2020), with 175 billion parameters, require vast resources, making it impractical for many applications. Therefore, efficient model compression strategies are crucial for deploying these powerful models in practical applications. \n\nModel compression techniques commonly employ quantization and pruning to enhance model efficiency. Quantization reduces the precision of model parameters, while pruning removes less critical parameters. Traditional pruning methods, such as magnitude-based pruning (Han et al. 2015), directly trim weights based on their absolute values. While effective for smaller models, these methods often struggle with large-scale LLMs, resulting in suboptimal sparsity due to their inability to capture the complex interactions and importance of weights in such massive scalability. To address these limitations, recent post-training pruning techniques such as SparseGPT and Wanda have emerged. \n\nCurrent pruning methods face two major challenges. First, as depicted in Figure 1(a), traditional layer-aware pruning methods focus on individual layers and neglect inter-layer dependencies within, leading to higher error accumulation (represented by blue arrows). In contrast, block-aware pruning, by considering groups of layers, captures inter-layer interactions to reduce error accumulation (represented by orange arrows). Second, as shown in Figure 1(b), conventional methods typically build the pruning mask once, ignoring the changes of weight significance in post-pruning stage. This oversight can lead to improper identification of salient weights, resulting in performance degradation. \n\nTo address these limitations, we propose LLM-Barber, a novel and straightforward approach designed to rebuild sparsity mask of pruned networks without requiring for retraining or weight reconstruction. Firstly, unlike layer-aware methods that are confined to local optimization and thus arXiv:2408.10631v1",
            "reference_string": "[271909582 | Su et al. | 2024 | Citations: 2]"
        },
        {
            "title": "AlphaPruning: Using Heavy-Tailed Self Regularization Theory for Improved Layer-wise Pruning of Large Language Models",
            "venue": "Neural Information Processing Systems",
            "year": 2024,
            "reference_count": 79,
            "citation_count": 10,
            "influential_citation_count": 4,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2410.10912, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2311995497",
                    "name": "Haiquan Lu"
                },
                {
                    "authorId": "2111405998",
                    "name": "Yefan Zhou"
                },
                {
                    "authorId": "2255081092",
                    "name": "Shiwei Liu"
                },
                {
                    "authorId": "2284563898",
                    "name": "Zhangyang Wang"
                },
                {
                    "authorId": "2249392052",
                    "name": "Michael W. Mahoney"
                },
                {
                    "authorId": "2249529142",
                    "name": "Yaoqing Yang"
                }
            ],
            "abstract": "Recent work on pruning large language models (LLMs) has shown that one can eliminate a large number of parameters without compromising performance, making pruning a promising strategy to reduce LLM model size. Existing LLM pruning strategies typically assign uniform pruning ratios across layers, limiting overall pruning ability; and recent work on layerwise pruning of LLMs is often based on heuristics that can easily lead to suboptimal performance. In this paper, we leverage Heavy-Tailed Self-Regularization (HT-SR) Theory, in particular the shape of empirical spectral densities (ESDs) of weight matrices, to design improved layerwise pruning ratios for LLMs. Our analysis reveals a wide variability in how well-trained, and thus relatedly how prunable, different layers of an LLM are. Based on this, we propose AlphaPruning, which uses shape metrics to allocate layerwise sparsity ratios in a more theoretically principled manner. AlphaPruning can be used in conjunction with multiple existing LLM pruning methods. Our empirical results show that AlphaPruning prunes LLaMA-7B to 80% sparsity while maintaining reasonable perplexity, marking a first in the literature on LLMs. We have open-sourced our code at https://github.com/haiquanlu/AlphaPruning.",
            "corpus_id": 273350592,
            "sentences": [
                {
                    "corpus_id": "273350592",
                    "title": "AlphaPruning: Using Heavy-Tailed Self Regularization Theory for Improved Layer-wise Pruning of Large Language Models",
                    "text": "Language Modeling. In Table 2, we report the perplexity of the pruned LLaMA and LLaMA-2 models at 70% sparsity. We provide results for more sparsity levels in Figure 2 and Appendix G. AlphaPruning, as a general layerwise sparsity method, consistently demonstrates performance improvements when used in conjunction with various pruning methods. For example, in the case of LLaMA-7B with a sparsity of 70%, AlphaPruning produces sparse networks with a perplexity of 231.01, significantly outperforming the Magnitude-based pruning baseline of 48419.13. Notably, when applied to Wanda and SparseGPT, two robust LLM pruning methods, AlphaPruning still achieves substantial perplexity reductions, evidenced by a decrease of 61.91 for Wanda and 7.76 for SparseGPT, in the case of LLaMA-7B with a sparsity of 70%. \n\nZero-shot tasks. We conducted empirical evaluations to determine the zero-shot ability of pruned LLMs on diverse zero-shot downstream tasks with prompting. The results are shown in Table 3, where we show the mean zero-shot accuracy on 7 zero-shot tasks of pruned LLaMA and LLaMA-2 models at sparsity of 70%. \n\nAlphaPruning consistently improves accuracy across all settings. For example, AlphaPruning achieves an average accuracy gain of 8.79, 6.05, and 2.61 over 7 tasks and 7 models compared to Magnitude, Wanda and SparseGPT alone, respectively. These results highlight the promise of AlphaPruning for more challenging zero-shot downstream tasks.",
                    "score": 0.6798304548736869,
                    "section_title": "Main results",
                    "char_start_offset": 23282,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 18
                        },
                        {
                            "start": 19,
                            "end": 111
                        },
                        {
                            "start": 112,
                            "end": 343
                        },
                        {
                            "start": 344,
                            "end": 549
                        },
                        {
                            "start": 550,
                            "end": 805
                        },
                        {
                            "start": 808,
                            "end": 824
                        },
                        {
                            "start": 825,
                            "end": 963
                        },
                        {
                            "start": 964,
                            "end": 1115
                        },
                        {
                            "start": 1118,
                            "end": 1182
                        },
                        {
                            "start": 1183,
                            "end": 1356
                        },
                        {
                            "start": 1357,
                            "end": 1457
                        }
                    ],
                    "ref_mentions": [],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.88037109375
                }
            ],
            "relevance_judgement": 0.88037109375,
            "relevance_judgment_input_expanded": "# Title: AlphaPruning: Using Heavy-Tailed Self Regularization Theory for Improved Layer-wise Pruning of Large Language Models\n# Venue: Neural Information Processing Systems\n# Authors: Haiquan Lu, Yefan Zhou, Shiwei Liu, Zhangyang Wang, Michael W. Mahoney, Yaoqing Yang\n## Abstract\nRecent work on pruning large language models (LLMs) has shown that one can eliminate a large number of parameters without compromising performance, making pruning a promising strategy to reduce LLM model size. Existing LLM pruning strategies typically assign uniform pruning ratios across layers, limiting overall pruning ability; and recent work on layerwise pruning of LLMs is often based on heuristics that can easily lead to suboptimal performance. In this paper, we leverage Heavy-Tailed Self-Regularization (HT-SR) Theory, in particular the shape of empirical spectral densities (ESDs) of weight matrices, to design improved layerwise pruning ratios for LLMs. Our analysis reveals a wide variability in how well-trained, and thus relatedly how prunable, different layers of an LLM are. Based on this, we propose AlphaPruning, which uses shape metrics to allocate layerwise sparsity ratios in a more theoretically principled manner. AlphaPruning can be used in conjunction with multiple existing LLM pruning methods. Our empirical results show that AlphaPruning prunes LLaMA-7B to 80% sparsity while maintaining reasonable perplexity, marking a first in the literature on LLMs. We have open-sourced our code at https://github.com/haiquanlu/AlphaPruning.\n## Main results\nLanguage Modeling. In Table 2, we report the perplexity of the pruned LLaMA and LLaMA-2 models at 70% sparsity. We provide results for more sparsity levels in Figure 2 and Appendix G. AlphaPruning, as a general layerwise sparsity method, consistently demonstrates performance improvements when used in conjunction with various pruning methods. For example, in the case of LLaMA-7B with a sparsity of 70%, AlphaPruning produces sparse networks with a perplexity of 231.01, significantly outperforming the Magnitude-based pruning baseline of 48419.13. Notably, when applied to Wanda and SparseGPT, two robust LLM pruning methods, AlphaPruning still achieves substantial perplexity reductions, evidenced by a decrease of 61.91 for Wanda and 7.76 for SparseGPT, in the case of LLaMA-7B with a sparsity of 70%. \n\nZero-shot tasks. We conducted empirical evaluations to determine the zero-shot ability of pruned LLMs on diverse zero-shot downstream tasks with prompting. The results are shown in Table 3, where we show the mean zero-shot accuracy on 7 zero-shot tasks of pruned LLaMA and LLaMA-2 models at sparsity of 70%. \n\nAlphaPruning consistently improves accuracy across all settings. For example, AlphaPruning achieves an average accuracy gain of 8.79, 6.05, and 2.61 over 7 tasks and 7 models compared to Magnitude, Wanda and SparseGPT alone, respectively. These results highlight the promise of AlphaPruning for more challenging zero-shot downstream tasks.",
            "reference_string": "[273350592 | Lu et al. | 2024 | Citations: 10]"
        },
        {
            "title": "LLM-Pruner: On the Structural Pruning of Large Language Models",
            "venue": "Neural Information Processing Systems",
            "year": 2023,
            "reference_count": 75,
            "citation_count": 440,
            "influential_citation_count": 57,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://arxiv.org/pdf/2305.11627",
                "status": "GREEN",
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2305.11627, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "15532066",
                    "name": "Xinyin Ma"
                },
                {
                    "authorId": "150110431",
                    "name": "Gongfan Fang"
                },
                {
                    "authorId": "48631088",
                    "name": "Xinchao Wang"
                }
            ],
            "abstract": "Large language models (LLMs) have shown remarkable capabilities in language understanding and generation. However, such impressive capability typically comes with a substantial model size, which presents significant challenges in both the deployment, inference, and training stages. With LLM being a general-purpose task solver, we explore its compression in a task-agnostic manner, which aims to preserve the multi-task solving and language generation ability of the original LLM. One challenge to achieving this is the enormous size of the training corpus of LLM, which makes both data transfer and model post-training over-burdensome. Thus, we tackle the compression of LLMs within the bound of two constraints: being task-agnostic and minimizing the reliance on the original training dataset. Our method, named LLM-Pruner, adopts structural pruning that selectively removes non-critical coupled structures based on gradient information, maximally preserving the majority of the LLM's functionality. To this end, the performance of pruned models can be efficiently recovered through tuning techniques, LoRA, in merely 3 hours, requiring only 50K data. We validate the LLM-Pruner on three LLMs, including LLaMA, Vicuna, and ChatGLM, and demonstrate that the compressed models still exhibit satisfactory capabilities in zero-shot classification and generation. The code is available at: https://github.com/horseee/LLM-Pruner",
            "corpus_id": 258823276,
            "sentences": [
                {
                    "corpus_id": "258823276",
                    "title": "LLM-Pruner: On the Structural Pruning of Large Language Models",
                    "text": "We conclude the advantages of the LLM-Pruner as (i) Task-agnostic compression, where the compressed language model retains its ability to serve as a multi-task solver. (ii) Reduced demand for the original training corpus, where only 50k publicly available samples are needed for compression, significantly reducing the budget for acquiring the training data (iii) Quick compression, where the compression process ends up in three hours. \n\n(iv) An automatic structural pruning framework, where all the dependent structures are grouped without the need for any manual design. To evaluate the effectiveness of LLM-Pruner, we conduct extensive experiments on three large language models: LLaMA-7B, Vicuna-7B, and ChatGLM-6B. The compressed models are evaluated using nine datasets to assess both the generation quality and the zero-shot classification performance of the pruned models. The experimental results demonstrate that even with the removal of 20% of the parameters, the pruned model maintains 94.97% of the performance of the original model.",
                    "score": 0.653389471688538,
                    "section_title": "Introduction",
                    "char_start_offset": 4184,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 167
                        },
                        {
                            "start": 168,
                            "end": 436
                        },
                        {
                            "start": 439,
                            "end": 573
                        },
                        {
                            "start": 574,
                            "end": 720
                        },
                        {
                            "start": 721,
                            "end": 881
                        },
                        {
                            "start": 882,
                            "end": 1047
                        }
                    ],
                    "ref_mentions": [],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.88037109375
                },
                {
                    "corpus_id": "258823276",
                    "title": "LLM-Pruner: On the Structural Pruning of Large Language Models",
                    "text": "Large language models (LLMs) have shown remarkable capabilities in language understanding and generation. However, such impressive capability typically comes with a substantial model size, which presents significant challenges in both the deployment, inference, and training stages. With LLM being a general-purpose task solver, we explore its compression in a task-agnostic manner, which aims to preserve the multi-task solving and language generation ability of the original LLM. One challenge to achieving this is the enormous size of the training corpus of LLM, which makes both data transfer and model post-training over-burdensome. Thus, we tackle the compression of LLMs within the bound of two constraints: being task-agnostic and minimizing the reliance on the original training dataset. Our method, named LLM-Pruner, adopts structural pruning that selectively removes non-critical coupled structures based on gradient information, maximally preserving the majority of the LLM's functionality. To this end, the performance of pruned models can be efficiently recovered through tuning techniques, LoRA, in merely 3 hours, requiring only 50K data. We validate the LLM-Pruner on three LLMs, including LLaMA, Vicuna, and ChatGLM, and demonstrate that the compressed models still exhibit satisfactory capabilities in zero-shot classification and generation. The code is available at: https://github.com/horseee/LLM-Pruner",
                    "score": 0.6663354806066972,
                    "section_title": "abstract",
                    "char_start_offset": 0,
                    "sentence_offsets": [],
                    "ref_mentions": [],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.87939453125
                },
                {
                    "corpus_id": "258823276",
                    "title": "LLM-Pruner: On the Structural Pruning of Large Language Models",
                    "text": "This introduces the task-agnostic compression of LLMs, which presents two key challenges: \n\n\u2022 The size of the training corpus of the LLM is enormous. Previous compression methods heavily depend on the training corpus. The LLM has escalated the corpus scale to 1 trillion tokens or more [17,49]. The extensive storage needs and protracted transmission times make the dataset difficult to acquire. Furthermore, if the dataset is proprietary, acquisition of the training corpus verges on impossibility, a situation encountered in [69,37]. \u2022 The unacceptably long duration for the post-training of the pruned LLM. Existing methods require a substantial amount of time for post-training the smaller model [53,28]. For instance, the general distillation in TinyBERT takes around 14 GPU days [20]. Even post-training a task-specific compressed model of BERT demands around 33 hours [59,22]. As the size of both the model and corpus for LLMs increases rapidly, this step will invariably consume an even more extensive time. \n\nTo tackle the aforementioned challenges associated with the task-agnostic compression of LLMs, we introduce a novel approach called LLM-Pruner. Since our goal is to compress LLMs with reduced data dependency and expedited post-training, how to prune model with the minimal disruption to the origin is crucial. To accomplish this, we propose a dependency detection algorithm that identifies all the dependent structures within the model. Once the coupled structure is identified, we employ an efficient importance estimation strategy to select the optimal group for pruning under the task-agnostic setting, where the first-order information and an approximated hessian information is taken into account. Finally, a rapid recovery stage is executed to post-train the pruned model with limited data. \n\nContribution. In this paper, we propose a novel framework, LLM-Pruner, for the task-agnostic compression of the large language model. To the best of our knowledge, LLM-Pruner is the first framework designed for structured pruning of LLMs. We conclude the advantages of the LLM-Pruner as (i) Task-agnostic compression, where the compressed language model retains its ability to serve as a multi-task solver.",
                    "score": 0.6036018221163127,
                    "section_title": "Introduction",
                    "char_start_offset": 2128,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 89
                        },
                        {
                            "start": 92,
                            "end": 149
                        },
                        {
                            "start": 150,
                            "end": 217
                        },
                        {
                            "start": 218,
                            "end": 294
                        },
                        {
                            "start": 295,
                            "end": 395
                        },
                        {
                            "start": 396,
                            "end": 535
                        },
                        {
                            "start": 536,
                            "end": 609
                        },
                        {
                            "start": 610,
                            "end": 708
                        },
                        {
                            "start": 709,
                            "end": 790
                        },
                        {
                            "start": 791,
                            "end": 883
                        },
                        {
                            "start": 884,
                            "end": 1015
                        },
                        {
                            "start": 1018,
                            "end": 1161
                        },
                        {
                            "start": 1162,
                            "end": 1327
                        },
                        {
                            "start": 1328,
                            "end": 1454
                        },
                        {
                            "start": 1455,
                            "end": 1720
                        },
                        {
                            "start": 1721,
                            "end": 1814
                        },
                        {
                            "start": 1817,
                            "end": 1830
                        },
                        {
                            "start": 1831,
                            "end": 1950
                        },
                        {
                            "start": 1951,
                            "end": 2055
                        },
                        {
                            "start": 2056,
                            "end": 2223
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 700,
                            "end": 704,
                            "matchedPaperCorpusId": "211296536"
                        },
                        {
                            "start": 785,
                            "end": 789,
                            "matchedPaperCorpusId": "202719327"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.85595703125
                }
            ],
            "relevance_judgement": 0.88037109375,
            "relevance_judgment_input_expanded": "# Title: LLM-Pruner: On the Structural Pruning of Large Language Models\n# Venue: Neural Information Processing Systems\n# Authors: Xinyin Ma, Gongfan Fang, Xinchao Wang\n## Abstract\nLarge language models (LLMs) have shown remarkable capabilities in language understanding and generation. However, such impressive capability typically comes with a substantial model size, which presents significant challenges in both the deployment, inference, and training stages. With LLM being a general-purpose task solver, we explore its compression in a task-agnostic manner, which aims to preserve the multi-task solving and language generation ability of the original LLM. One challenge to achieving this is the enormous size of the training corpus of LLM, which makes both data transfer and model post-training over-burdensome. Thus, we tackle the compression of LLMs within the bound of two constraints: being task-agnostic and minimizing the reliance on the original training dataset. Our method, named LLM-Pruner, adopts structural pruning that selectively removes non-critical coupled structures based on gradient information, maximally preserving the majority of the LLM's functionality. To this end, the performance of pruned models can be efficiently recovered through tuning techniques, LoRA, in merely 3 hours, requiring only 50K data. We validate the LLM-Pruner on three LLMs, including LLaMA, Vicuna, and ChatGLM, and demonstrate that the compressed models still exhibit satisfactory capabilities in zero-shot classification and generation. The code is available at: https://github.com/horseee/LLM-Pruner\n## Introduction\nThis introduces the task-agnostic compression of LLMs, which presents two key challenges: \n\n\u2022 The size of the training corpus of the LLM is enormous. Previous compression methods heavily depend on the training corpus. The LLM has escalated the corpus scale to 1 trillion tokens or more [17,49]. The extensive storage needs and protracted transmission times make the dataset difficult to acquire. Furthermore, if the dataset is proprietary, acquisition of the training corpus verges on impossibility, a situation encountered in [69,37]. \u2022 The unacceptably long duration for the post-training of the pruned LLM. Existing methods require a substantial amount of time for post-training the smaller model [53,28]. For instance, the general distillation in TinyBERT takes around 14 GPU days [20]. Even post-training a task-specific compressed model of BERT demands around 33 hours [59,22]. As the size of both the model and corpus for LLMs increases rapidly, this step will invariably consume an even more extensive time. \n\nTo tackle the aforementioned challenges associated with the task-agnostic compression of LLMs, we introduce a novel approach called LLM-Pruner. Since our goal is to compress LLMs with reduced data dependency and expedited post-training, how to prune model with the minimal disruption to the origin is crucial. To accomplish this, we propose a dependency detection algorithm that identifies all the dependent structures within the model. Once the coupled structure is identified, we employ an efficient importance estimation strategy to select the optimal group for pruning under the task-agnostic setting, where the first-order information and an approximated hessian information is taken into account. Finally, a rapid recovery stage is executed to post-train the pruned model with limited data. \n\nContribution. In this paper, we propose a novel framework, LLM-Pruner, for the task-agnostic compression of the large language model. To the best of our knowledge, LLM-Pruner is the first framework designed for structured pruning of LLMs. We conclude the advantages of the LLM-Pruner as (i) Task-agnostic compression, where the compressed language model retains its ability to serve as a multi-task solver.\n...\nWe conclude the advantages of the LLM-Pruner as (i) Task-agnostic compression, where the compressed language model retains its ability to serve as a multi-task solver. (ii) Reduced demand for the original training corpus, where only 50k publicly available samples are needed for compression, significantly reducing the budget for acquiring the training data (iii) Quick compression, where the compression process ends up in three hours. \n\n(iv) An automatic structural pruning framework, where all the dependent structures are grouped without the need for any manual design. To evaluate the effectiveness of LLM-Pruner, we conduct extensive experiments on three large language models: LLaMA-7B, Vicuna-7B, and ChatGLM-6B. The compressed models are evaluated using nine datasets to assess both the generation quality and the zero-shot classification performance of the pruned models. The experimental results demonstrate that even with the removal of 20% of the parameters, the pruned model maintains 94.97% of the performance of the original model.",
            "reference_string": "[258823276 | Ma et al. | 2023 | Citations: 440]"
        },
        {
            "title": "Model Hemorrhage and the Robustness Limits of Large Language Models",
            "venue": "arXiv.org",
            "year": 2025,
            "reference_count": 137,
            "citation_count": 1,
            "influential_citation_count": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2503.23924, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2352948034",
                    "name": "Ziyang Ma"
                },
                {
                    "authorId": "2274202084",
                    "name": "Zuchao Li"
                },
                {
                    "authorId": "2269488794",
                    "name": "Lefei Zhang"
                },
                {
                    "authorId": "2343636012",
                    "name": "Gui-Song Xia"
                },
                {
                    "authorId": "2306994733",
                    "name": "Bo Du"
                },
                {
                    "authorId": "2268745050",
                    "name": "Liangpei Zhang"
                },
                {
                    "authorId": "2275194788",
                    "name": "Dacheng Tao"
                }
            ],
            "abstract": "Large language models (LLMs) demonstrate strong performance across natural language processing tasks, yet undergo significant performance degradation when modified for deployment through quantization, pruning, or decoding strategy adjustments. We define this phenomenon as model hemorrhage - performance decline caused by parameter alterations and architectural changes. Through systematic analysis of various LLM frameworks, we identify key vulnerability patterns: layer expansion frequently disrupts attention mechanisms, compression techniques induce information loss cascades, and decoding adjustments amplify prediction divergences. Our investigation reveals transformer architectures exhibit inherent robustness thresholds that determine hemorrhage severity across modification types. We propose three mitigation strategies: gradient-aware pruning preserves critical weight pathways, dynamic quantization scaling maintains activation integrity, and decoding calibration aligns generation trajectories with original model distributions. This work establishes foundational metrics for evaluating model stability during adaptation, providing practical guidelines for maintaining performance while enabling efficient LLM deployment. Our findings advance understanding of neural network resilience under architectural transformations, particularly for large-scale language models.",
            "corpus_id": 277452419,
            "sentences": [
                {
                    "corpus_id": "277452419",
                    "title": "Model Hemorrhage and the Robustness Limits of Large Language Models",
                    "text": "Unstructured pruning is an optimization technique that achieves model sparsity by evaluating the importance of individual weights. Its flexibility and high compression rates make it a key method for optimizing large language models (LLMs). Unstructured pruning can achieve extremely high compression rates; for instance, Wanda achieves a 60% sparsity rate on LLaMA-7B with minimal performance degradation across multiple downstream tasks [27], while Flash-LLM achieves a 70% sparsity rate on OPT-175B, significantly reducing storage requirements with less than 2% performance degradation during inference [28]. However, unstructured pruning often results in irregular sparse patterns in the weight matrix, necessitating specialized hardware accelerators (e.g., sparse matrix multiplication units) to efficiently handle sparse matrix computations and fully exploit the benefits of sparsity in terms of storage and computation. Among various unstructured pruning methods, Magnitude Pruning is the most basic, directly removing weights with small magnitudes. While simple to implement, it does not account for the contextual importance of weights. SparseGPT [29], on the other hand, introduces a diagonal Hessian approximation to assess the impact of weights on errors, enabling more precise pruning at the cost of high computational complexity and hardware resource requirements. Wanda [27] simplifies the SparseGPT algorithm by eliminating the need for Hessian approximations and instead computes pruning metrics by multiplying weights with input activations. This simplification significantly reduces computational complexity while achieving a balance between high accuracy and efficiency. Following this approach, many subsequent methods use SparseGPT and Wanda as baselines or build upon their foundations. RIA [30] introduces a post-training pruning method that re-evaluates the importance of each weight element based on all input and output connections. ADMM [31] builds on SparseGPT by incorporating the Alternating Direction Method of Multipliers (ADMM) to restore model performance after pruning, using a simple iterative mask selection process for pruning. OWL [32] integrates both Wanda and SparseGPT, proposing the OWL metric to allocate varying pruning rates across different layers.",
                    "score": 0.8261701818539282,
                    "section_title": "Unstructured Pruning",
                    "char_start_offset": 21938,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 130
                        },
                        {
                            "start": 131,
                            "end": 239
                        },
                        {
                            "start": 240,
                            "end": 610
                        },
                        {
                            "start": 611,
                            "end": 925
                        },
                        {
                            "start": 926,
                            "end": 1055
                        },
                        {
                            "start": 1056,
                            "end": 1144
                        },
                        {
                            "start": 1145,
                            "end": 1377
                        },
                        {
                            "start": 1378,
                            "end": 1558
                        },
                        {
                            "start": 1559,
                            "end": 1689
                        },
                        {
                            "start": 1690,
                            "end": 1808
                        },
                        {
                            "start": 1809,
                            "end": 1958
                        },
                        {
                            "start": 1959,
                            "end": 2165
                        },
                        {
                            "start": 2166,
                            "end": 2295
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 438,
                            "end": 442,
                            "matchedPaperCorpusId": "259203115"
                        },
                        {
                            "start": 1155,
                            "end": 1159,
                            "matchedPaperCorpusId": "255372747"
                        },
                        {
                            "start": 1384,
                            "end": 1388,
                            "matchedPaperCorpusId": "259203115"
                        },
                        {
                            "start": 1813,
                            "end": 1817,
                            "matchedPaperCorpusId": "271745835"
                        },
                        {
                            "start": 1964,
                            "end": 1968,
                            "matchedPaperCorpusId": "266818263"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.87548828125
                }
            ],
            "relevance_judgement": 0.87548828125,
            "relevance_judgment_input_expanded": "# Title: Model Hemorrhage and the Robustness Limits of Large Language Models\n# Venue: arXiv.org\n# Authors: Ziyang Ma, Zuchao Li, Lefei Zhang, Gui-Song Xia, Bo Du, Liangpei Zhang, Dacheng Tao\n## Abstract\nLarge language models (LLMs) demonstrate strong performance across natural language processing tasks, yet undergo significant performance degradation when modified for deployment through quantization, pruning, or decoding strategy adjustments. We define this phenomenon as model hemorrhage - performance decline caused by parameter alterations and architectural changes. Through systematic analysis of various LLM frameworks, we identify key vulnerability patterns: layer expansion frequently disrupts attention mechanisms, compression techniques induce information loss cascades, and decoding adjustments amplify prediction divergences. Our investigation reveals transformer architectures exhibit inherent robustness thresholds that determine hemorrhage severity across modification types. We propose three mitigation strategies: gradient-aware pruning preserves critical weight pathways, dynamic quantization scaling maintains activation integrity, and decoding calibration aligns generation trajectories with original model distributions. This work establishes foundational metrics for evaluating model stability during adaptation, providing practical guidelines for maintaining performance while enabling efficient LLM deployment. Our findings advance understanding of neural network resilience under architectural transformations, particularly for large-scale language models.\n## Unstructured Pruning\nUnstructured pruning is an optimization technique that achieves model sparsity by evaluating the importance of individual weights. Its flexibility and high compression rates make it a key method for optimizing large language models (LLMs). Unstructured pruning can achieve extremely high compression rates; for instance, Wanda achieves a 60% sparsity rate on LLaMA-7B with minimal performance degradation across multiple downstream tasks [27], while Flash-LLM achieves a 70% sparsity rate on OPT-175B, significantly reducing storage requirements with less than 2% performance degradation during inference [28]. However, unstructured pruning often results in irregular sparse patterns in the weight matrix, necessitating specialized hardware accelerators (e.g., sparse matrix multiplication units) to efficiently handle sparse matrix computations and fully exploit the benefits of sparsity in terms of storage and computation. Among various unstructured pruning methods, Magnitude Pruning is the most basic, directly removing weights with small magnitudes. While simple to implement, it does not account for the contextual importance of weights. SparseGPT [29], on the other hand, introduces a diagonal Hessian approximation to assess the impact of weights on errors, enabling more precise pruning at the cost of high computational complexity and hardware resource requirements. Wanda [27] simplifies the SparseGPT algorithm by eliminating the need for Hessian approximations and instead computes pruning metrics by multiplying weights with input activations. This simplification significantly reduces computational complexity while achieving a balance between high accuracy and efficiency. Following this approach, many subsequent methods use SparseGPT and Wanda as baselines or build upon their foundations. RIA [30] introduces a post-training pruning method that re-evaluates the importance of each weight element based on all input and output connections. ADMM [31] builds on SparseGPT by incorporating the Alternating Direction Method of Multipliers (ADMM) to restore model performance after pruning, using a simple iterative mask selection process for pruning. OWL [32] integrates both Wanda and SparseGPT, proposing the OWL metric to allocate varying pruning rates across different layers.",
            "reference_string": "[277452419 | Ma et al. | 2025 | Citations: 1]"
        },
        {
            "title": "Composable Interventions for Language Models",
            "venue": "International Conference on Learning Representations",
            "year": 2024,
            "reference_count": 106,
            "citation_count": 4,
            "influential_citation_count": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2407.06483, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "68978244",
                    "name": "Arinbj\u00f6rn Kolbeinsson"
                },
                {
                    "authorId": "2212970046",
                    "name": "Kyle O'Brien"
                },
                {
                    "authorId": "2310637497",
                    "name": "Tianjin Huang"
                },
                {
                    "authorId": "2269765109",
                    "name": "Shanghua Gao"
                },
                {
                    "authorId": "2310512874",
                    "name": "Shiwei Liu"
                },
                {
                    "authorId": "2290185444",
                    "name": "Jonathan Richard Schwarz"
                },
                {
                    "authorId": "2187496179",
                    "name": "Anurag Vaidya"
                },
                {
                    "authorId": "2310436669",
                    "name": "Faisal Mahmood"
                },
                {
                    "authorId": "2095762",
                    "name": "M. Zitnik"
                },
                {
                    "authorId": "2295593785",
                    "name": "Tianlong Chen"
                },
                {
                    "authorId": "32452740",
                    "name": "Thomas Hartvigsen"
                }
            ],
            "abstract": "Test-time interventions for language models can enhance factual accuracy, mitigate harmful outputs, and improve model efficiency without costly retraining. But despite a flood of new methods, different types of interventions are largely developing independently. In practice, multiple interventions must be applied sequentially to the same model, yet we lack standardized ways to study how interventions interact. We fill this gap by introducing composable interventions, a framework to study the effects of using multiple interventions on the same language models, featuring new metrics and a unified codebase. Using our framework, we conduct extensive experiments and compose popular methods from three emerging intervention categories -- Knowledge Editing, Model Compression, and Machine Unlearning. Our results from 310 different compositions uncover meaningful interactions: compression hinders editing and unlearning, composing interventions hinges on their order of application, and popular general-purpose metrics are inadequate for assessing composability. Taken together, our findings showcase clear gaps in composability, suggesting a need for new multi-objective interventions. All of our code is public: https://github.com/hartvigsen-group/composable-interventions.",
            "corpus_id": 271064490,
            "sentences": [
                {
                    "corpus_id": "271064490",
                    "title": "Composable Interventions for Language Models",
                    "text": "We use four state-of-the-art compression methods including two pruning methods: \n\n\u2022 SparseGPT (Frantar & Alistarh, 2023): an efficient one-shot pruning method tailored for large models. It converts the pruning process into solving large-scale sparse regression problems using an approximate solver. This approach enables rapid pruning on a single GPU with minimal accuracy loss, achieving 50-60% on large models. \n\n\u2022 Wanda (Sun et al., 2023): is another popular method for pruning large language models that relies on a pruning metric that combines a weight's magnitude with the norm of its corresponding input activations, determined from a small calibration dataset. The method focuses on selectively pruning weights within individual outputs of linear layers, aiming for high sparsity levels without modifying unpruned weights. Wanda is computationally efficient, executable in a single forward pass. \n\nand two quantization methods: \n\n\u2022 GPTQ (Frantar et al., 2023): an algorithm designed for efficient weight quantization in large-scale models. It revises the weight quantization approach by quantizing weights in a fixed order rather than a greedy order, which shows minimal performance difference, especially in larger models. GPTQ introduced a novel method where each weight is quantized column-by-column, reducing computational complexity. \n\n\u2022 AWQ (Lin et al., 2023): is based on the premise that not all weights are equally critical for model performance, and it identifies a small fraction of salient weights whose quantization significantly impacts model accuracy. This identification is done by analyzing activation distributions rather than weight distributions, under the rationale that weights linked to larger activation magnitudes are more crucial.",
                    "score": 0.63235239778715,
                    "section_title": "C.3.2 COMPRESSOR DETAILS",
                    "char_start_offset": 35862,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 79
                        },
                        {
                            "start": 82,
                            "end": 185
                        },
                        {
                            "start": 186,
                            "end": 298
                        },
                        {
                            "start": 299,
                            "end": 412
                        },
                        {
                            "start": 415,
                            "end": 668
                        },
                        {
                            "start": 669,
                            "end": 830
                        },
                        {
                            "start": 831,
                            "end": 903
                        },
                        {
                            "start": 906,
                            "end": 935
                        },
                        {
                            "start": 938,
                            "end": 1047
                        },
                        {
                            "start": 1048,
                            "end": 1231
                        },
                        {
                            "start": 1232,
                            "end": 1346
                        },
                        {
                            "start": 1349,
                            "end": 1574
                        },
                        {
                            "start": 1575,
                            "end": 1764
                        }
                    ],
                    "ref_mentions": [],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.87060546875
                }
            ],
            "relevance_judgement": 0.87060546875,
            "relevance_judgment_input_expanded": "# Title: Composable Interventions for Language Models\n# Venue: International Conference on Learning Representations\n# Authors: Arinbj\u00f6rn Kolbeinsson, Kyle O'Brien, Tianjin Huang, Shanghua Gao, Shiwei Liu, Jonathan Richard Schwarz, Anurag Vaidya, Faisal Mahmood, M. Zitnik, Tianlong Chen, Thomas Hartvigsen\n## Abstract\nTest-time interventions for language models can enhance factual accuracy, mitigate harmful outputs, and improve model efficiency without costly retraining. But despite a flood of new methods, different types of interventions are largely developing independently. In practice, multiple interventions must be applied sequentially to the same model, yet we lack standardized ways to study how interventions interact. We fill this gap by introducing composable interventions, a framework to study the effects of using multiple interventions on the same language models, featuring new metrics and a unified codebase. Using our framework, we conduct extensive experiments and compose popular methods from three emerging intervention categories -- Knowledge Editing, Model Compression, and Machine Unlearning. Our results from 310 different compositions uncover meaningful interactions: compression hinders editing and unlearning, composing interventions hinges on their order of application, and popular general-purpose metrics are inadequate for assessing composability. Taken together, our findings showcase clear gaps in composability, suggesting a need for new multi-objective interventions. All of our code is public: https://github.com/hartvigsen-group/composable-interventions.\n## C.3.2 COMPRESSOR DETAILS\nWe use four state-of-the-art compression methods including two pruning methods: \n\n\u2022 SparseGPT (Frantar & Alistarh, 2023): an efficient one-shot pruning method tailored for large models. It converts the pruning process into solving large-scale sparse regression problems using an approximate solver. This approach enables rapid pruning on a single GPU with minimal accuracy loss, achieving 50-60% on large models. \n\n\u2022 Wanda (Sun et al., 2023): is another popular method for pruning large language models that relies on a pruning metric that combines a weight's magnitude with the norm of its corresponding input activations, determined from a small calibration dataset. The method focuses on selectively pruning weights within individual outputs of linear layers, aiming for high sparsity levels without modifying unpruned weights. Wanda is computationally efficient, executable in a single forward pass. \n\nand two quantization methods: \n\n\u2022 GPTQ (Frantar et al., 2023): an algorithm designed for efficient weight quantization in large-scale models. It revises the weight quantization approach by quantizing weights in a fixed order rather than a greedy order, which shows minimal performance difference, especially in larger models. GPTQ introduced a novel method where each weight is quantized column-by-column, reducing computational complexity. \n\n\u2022 AWQ (Lin et al., 2023): is based on the premise that not all weights are equally critical for model performance, and it identifies a small fraction of salient weights whose quantization significantly impacts model accuracy. This identification is done by analyzing activation distributions rather than weight distributions, under the rationale that weights linked to larger activation magnitudes are more crucial.",
            "reference_string": "[271064490 | Kolbeinsson et al. | 2024 | Citations: 4]"
        },
        {
            "title": "LEGO: Language Model Building Blocks",
            "venue": "arXiv.org",
            "year": 2024,
            "reference_count": 28,
            "citation_count": 0,
            "influential_citation_count": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2410.18287, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2164257071",
                    "name": "Shrenik Bhansali"
                },
                {
                    "authorId": "2327337534",
                    "name": "Alwin Jin"
                },
                {
                    "authorId": "2315304043",
                    "name": "Tyler Lizzo"
                },
                {
                    "authorId": "2315302093",
                    "name": "Larry Heck"
                }
            ],
            "abstract": "Large language models (LLMs) are essential in natural language processing (NLP) but are costly in data collection, pre-training, fine-tuning, and inference. Task-specific small language models (SLMs) offer a cheaper alternative but lack robustness and generalization. This paper proposes LEGO, a novel technique to extract SLMs from an LLM and recombine them. Using state-of-the-art LLM pruning strategies, we can create task- and user-specific SLM building blocks that are efficient for fine-tuning and inference while also preserving user data privacy. LEGO utilizes Federated Learning and a novel aggregation scheme for the LLM reconstruction, maintaining robustness without high costs and preserving user data privacy. We experimentally demonstrate the versatility of LEGO, showing its ability to enable model heterogeneity and mitigate the effects of data heterogeneity while maintaining LLM robustness.",
            "corpus_id": 273549773,
            "sentences": [
                {
                    "corpus_id": "273549773",
                    "title": "LEGO: Language Model Building Blocks",
                    "text": "In recent years, pruning has become widely used in NLP to compress LLMs (LeCun et al., 1989). Pruning involves the selective omission of model parameters with minimal contributions to the learning process. Pruning techniques have proven successful, enhancing the cost-effectiveness of large pre-trained models (Xia et al., 2023). \n\nRecently, more nuanced pruning approaches have been discussed in the literature, improving over more traditional methods like magnitude pruning. Specifically, two state-of-the-art pruning methods are widely discussed in the literature-SparseGPT (Frantar and Alistarh, 2023) and Wanda (Sun et al., 2023). Whereas traditional magnitude pruning operates by pruning weights with the largest magnitude, these pruning techniques instead track weight activations, and prune weights with the lowest amount of activation. \n\nSparseGPT creates and solves a layer-wise reconstruction problem to determine the weight activations, whereas Wanda takes the product of a weight's magnitude and the norm of its associated input activations.",
                    "score": 0.9164910835250937,
                    "section_title": "Model Compression",
                    "char_start_offset": 4276,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 93
                        },
                        {
                            "start": 94,
                            "end": 205
                        },
                        {
                            "start": 206,
                            "end": 329
                        },
                        {
                            "start": 332,
                            "end": 476
                        },
                        {
                            "start": 477,
                            "end": 635
                        },
                        {
                            "start": 636,
                            "end": 844
                        },
                        {
                            "start": 847,
                            "end": 1054
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 577,
                            "end": 605,
                            "matchedPaperCorpusId": "255372747"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.86962890625
                }
            ],
            "relevance_judgement": 0.86962890625,
            "relevance_judgment_input_expanded": "# Title: LEGO: Language Model Building Blocks\n# Venue: arXiv.org\n# Authors: Shrenik Bhansali, Alwin Jin, Tyler Lizzo, Larry Heck\n## Abstract\nLarge language models (LLMs) are essential in natural language processing (NLP) but are costly in data collection, pre-training, fine-tuning, and inference. Task-specific small language models (SLMs) offer a cheaper alternative but lack robustness and generalization. This paper proposes LEGO, a novel technique to extract SLMs from an LLM and recombine them. Using state-of-the-art LLM pruning strategies, we can create task- and user-specific SLM building blocks that are efficient for fine-tuning and inference while also preserving user data privacy. LEGO utilizes Federated Learning and a novel aggregation scheme for the LLM reconstruction, maintaining robustness without high costs and preserving user data privacy. We experimentally demonstrate the versatility of LEGO, showing its ability to enable model heterogeneity and mitigate the effects of data heterogeneity while maintaining LLM robustness.\n## Model Compression\nIn recent years, pruning has become widely used in NLP to compress LLMs (LeCun et al., 1989). Pruning involves the selective omission of model parameters with minimal contributions to the learning process. Pruning techniques have proven successful, enhancing the cost-effectiveness of large pre-trained models (Xia et al., 2023). \n\nRecently, more nuanced pruning approaches have been discussed in the literature, improving over more traditional methods like magnitude pruning. Specifically, two state-of-the-art pruning methods are widely discussed in the literature-SparseGPT (Frantar and Alistarh, 2023) and Wanda (Sun et al., 2023). Whereas traditional magnitude pruning operates by pruning weights with the largest magnitude, these pruning techniques instead track weight activations, and prune weights with the lowest amount of activation. \n\nSparseGPT creates and solves a layer-wise reconstruction problem to determine the weight activations, whereas Wanda takes the product of a weight's magnitude and the norm of its associated input activations.",
            "reference_string": "[273549773 | Bhansali et al. | 2024 | Citations: 0]"
        },
        {
            "title": "Beyond Size: How Gradients Shape Pruning Decisions in Large Language Models",
            "venue": "arXiv.org",
            "year": 2023,
            "reference_count": 46,
            "citation_count": 19,
            "influential_citation_count": 7,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2311.04902, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2211732585",
                    "name": "Rocktim Jyoti Das"
                },
                {
                    "authorId": "2243392466",
                    "name": "Liqun Ma"
                },
                {
                    "authorId": "2243374493",
                    "name": "Zhiqiang Shen"
                }
            ],
            "abstract": "Large Language Models (LLMs) with billions of parameters are prime targets for network pruning, removing some model weights without hurting performance. Prior approaches such as magnitude pruning, SparseGPT, and Wanda, either concentrated solely on weights or integrated weights with activations for sparsity. However, they overlooked the informative gradients derived from pretrained LLMs. In this paper, we present a novel sparsity-centric pruning method for pretrained LLMs, termed Gradient-based Language Model Pruner (GBLM-Pruner). GBLM-Pruner leverages the first-order term of the Taylor expansion, operating in a training-free manner by harnessing properly normalized gradients from a few calibration samples to determine the pruning metric, and substantially outperforms competitive counterparts like SparseGPT and Wanda in multiple benchmarks. Intriguingly, by incorporating gradients, unstructured pruning with our method tends to reveal some structural patterns, which mirrors the geometric interdependence inherent in the LLMs' parameter structure. Additionally, GBLM-Pruner functions without any subsequent retraining or weight updates to maintain its simplicity as other counterparts. Extensive evaluations on LLaMA-1 and LLaMA-2 across various benchmarks show that GBLM-Pruner surpasses magnitude pruning, Wanda and SparseGPT by significant margins. We further extend our approach on Vision Transformer. Our code and models are available at https://github.com/VILA-Lab/GBLM-Pruner.",
            "corpus_id": 265050936,
            "sentences": [
                {
                    "corpus_id": "265050936",
                    "title": "Beyond Size: How Gradients Shape Pruning Decisions in Large Language Models",
                    "text": "Large Language Models (LLMs) with billions of parameters are prime targets for network pruning, removing some model weights without hurting performance. Prior approaches such as magnitude pruning, SparseGPT, and Wanda, either concentrated solely on weights or integrated weights with activations for sparsity. However, they overlooked the informative gradients derived from pretrained LLMs. In this paper, we present a novel sparsity-centric pruning method for pretrained LLMs, termed Gradient-based Language Model Pruner (GBLM-Pruner). GBLM-Pruner leverages the first-order term of the Taylor expansion, operating in a training-free manner by harnessing properly normalized gradients from a few calibration samples to determine the pruning metric, and substantially outperforms competitive counterparts like SparseGPT and Wanda in multiple benchmarks. Intriguingly, by incorporating gradients, unstructured pruning with our method tends to reveal some structural patterns, which mirrors the geometric interdependence inherent in the LLMs' parameter structure. Additionally, GBLM-Pruner functions without any subsequent retraining or weight updates to maintain its simplicity as other counterparts. Extensive evaluations on LLaMA-1 and LLaMA-2 across various benchmarks show that GBLM-Pruner surpasses magnitude pruning, Wanda and SparseGPT by significant margins. We further extend our approach on Vision Transformer. Our code and models are available at https://github.com/VILA-Lab/GBLM-Pruner.",
                    "score": 0.8321014709920973,
                    "section_title": "abstract",
                    "char_start_offset": 0,
                    "sentence_offsets": [],
                    "ref_mentions": [],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.86962890625
                }
            ],
            "relevance_judgement": 0.86962890625,
            "relevance_judgment_input_expanded": "# Title: Beyond Size: How Gradients Shape Pruning Decisions in Large Language Models\n# Venue: arXiv.org\n# Authors: Rocktim Jyoti Das, Liqun Ma, Zhiqiang Shen\n## Abstract\nLarge Language Models (LLMs) with billions of parameters are prime targets for network pruning, removing some model weights without hurting performance. Prior approaches such as magnitude pruning, SparseGPT, and Wanda, either concentrated solely on weights or integrated weights with activations for sparsity. However, they overlooked the informative gradients derived from pretrained LLMs. In this paper, we present a novel sparsity-centric pruning method for pretrained LLMs, termed Gradient-based Language Model Pruner (GBLM-Pruner). GBLM-Pruner leverages the first-order term of the Taylor expansion, operating in a training-free manner by harnessing properly normalized gradients from a few calibration samples to determine the pruning metric, and substantially outperforms competitive counterparts like SparseGPT and Wanda in multiple benchmarks. Intriguingly, by incorporating gradients, unstructured pruning with our method tends to reveal some structural patterns, which mirrors the geometric interdependence inherent in the LLMs' parameter structure. Additionally, GBLM-Pruner functions without any subsequent retraining or weight updates to maintain its simplicity as other counterparts. Extensive evaluations on LLaMA-1 and LLaMA-2 across various benchmarks show that GBLM-Pruner surpasses magnitude pruning, Wanda and SparseGPT by significant margins. We further extend our approach on Vision Transformer. Our code and models are available at https://github.com/VILA-Lab/GBLM-Pruner.\n",
            "reference_string": "[265050936 | Das et al. | 2023 | Citations: 19]"
        },
        {
            "title": "Applications of Pruning Methods in Natural Language Processing",
            "venue": "IEEE Access",
            "year": 2024,
            "reference_count": 87,
            "citation_count": 1,
            "influential_citation_count": 0,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://doi.org/10.1109/access.2024.3411776",
                "status": "GOLD",
                "license": "CCBYNCND",
                "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.1109/ACCESS.2024.3411776?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1109/ACCESS.2024.3411776, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2305959319",
                    "name": "Marva Touheed"
                },
                {
                    "authorId": "2305868456",
                    "name": "Urooj Zubair"
                },
                {
                    "authorId": "17492832",
                    "name": "Dilshad Sabir"
                },
                {
                    "authorId": "2293111925",
                    "name": "Ali Hassan"
                },
                {
                    "authorId": "2305969817",
                    "name": "Muhammad Fasih Uddin Butt"
                },
                {
                    "authorId": "1713703",
                    "name": "Farhan Riaz"
                },
                {
                    "authorId": "2305963536",
                    "name": "Wadood Abdul"
                },
                {
                    "authorId": "119778535",
                    "name": "R. Ayub"
                }
            ],
            "abstract": "Deep neural networks (DNN) are in high demand because of their widespread applications in natural language processing, image processing, and a lot of other domains. However, due to their computational expense, over-parameterization, and large memory requirements, DNN applications often require the use of substantial model resources. This strict requirement of latency and limited memory availability are hurdles in the device deployment of these technologies. Therefore, a common idea could be to mitigate the DNN-based models\u2019 size without any performance degradation using different compression techniques. During the last few years, a great deal of progress has been made in the field of Natural Language Processing (NLP) using deep learning approaches. The objective of this research is to offer a thorough overview of the various pruning methods applied in the context of NLP. In this paper, we review several recent pruning-based schemes used for converting standard networks into their compact and accelerated versions. Traditionally, pruning is a technique for improving latency, reducing model size, and computational complexity which is a viable approach to deal with the above-mentioned challenges. In general, these techniques are divided into two main categories: structural and unstructured pruning methods. Structural pruning methods are further classified into filter, channel, layer, block, and movement pruning. Whereas, neuron, magnitude-based, and iterative pruning lie in the category of unstructured pruning. For each method, we discuss the related metrics and benchmarks. Then recent work on each method is discussed in detail, which provides insightful analysis of the performance, related applications, and pros and cons. Then, a comparative analysis is provided to analyze the differences among approaches. Finally, the paper concludes with possible future directions and some technical challenges.",
            "corpus_id": 270411995,
            "sentences": [
                {
                    "corpus_id": "270411995",
                    "title": "Applications of Pruning Methods in Natural Language Processing",
                    "text": "Their objective was to reduce model size by removing redundant parameters while preserving speech recognition performance.The authors propose a structured pruning method to lessen the size and complexity of self-supervised pre-trained models while maintaining their performance on speech-related tasks.The key contributions and findings of the paper include the following:\n\n1) Pruning Strategy: The authors present a specific pruning strategy tailored for self-supervised pre-trained models in the speech domain.This strategy identifies and removes less important structures or components, such as layers or neurons, based on their significance to the model's overall performance.2) Performance Analysis: The paper evaluates the impact of structured pruning on speech recognition and understanding tasks.It assesses the model's accuracy and efficiency before and after pruning, demonstrating the potential benefits of structured pruning in reducing model size and computational requirements while preserving task performance.3) Comparison with Baselines: The authors compare their proposed structured pruning approach with other baseline methods commonly used for model compressions, such as unstructured pruning or weight quantization.The comparison highlights the advantages and effectiveness of structured pruning specifically for selfsupervised pre-trained models in the speech domain.The results show significant compression and improved accuracy compared to the original model, as validated by experiments on LibriSpeech and SLURP datasets.\n\nMa et al. [29] proposes Large Language Model-Pruner (LLM-Pruner), a task-agnostic compression method, aiming to maintain the multi-task solving and language generation abilities of the original LLM while minimizing reliance on the extensive training dataset.LLM-Pruner adopts structural pruning, removing non-critical coupled structures based on gradient information to preserve most of the LLM's functionality.The pruned models can be efficiently recovered through tuning techniques in a short time and with minimal data.Experimental results on three LLMs, Large Language Model Meta AI (LLaMA), Vicuna, and General Language Model (ChatGLM), demonstrate that the compressed models perform well in zero-shot classification and generation tasks.",
                    "score": 0.6572475585317388,
                    "section_title": "IV. METHODS",
                    "char_start_offset": 18689,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 122
                        },
                        {
                            "start": 122,
                            "end": 302
                        },
                        {
                            "start": 302,
                            "end": 372
                        },
                        {
                            "start": 374,
                            "end": 512
                        },
                        {
                            "start": 512,
                            "end": 680
                        },
                        {
                            "start": 680,
                            "end": 804
                        },
                        {
                            "start": 804,
                            "end": 1025
                        },
                        {
                            "start": 1025,
                            "end": 1236
                        },
                        {
                            "start": 1236,
                            "end": 1389
                        },
                        {
                            "start": 1389,
                            "end": 1546
                        },
                        {
                            "start": 1548,
                            "end": 1806
                        },
                        {
                            "start": 1806,
                            "end": 1959
                        },
                        {
                            "start": 1959,
                            "end": 2070
                        },
                        {
                            "start": 2070,
                            "end": 2291
                        }
                    ],
                    "ref_mentions": [],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.8623046875
                }
            ],
            "relevance_judgement": 0.8623046875,
            "relevance_judgment_input_expanded": "# Title: Applications of Pruning Methods in Natural Language Processing\n# Venue: IEEE Access\n# Authors: Marva Touheed, Urooj Zubair, Dilshad Sabir, Ali Hassan, Muhammad Fasih Uddin Butt, Farhan Riaz, Wadood Abdul, R. Ayub\n## Abstract\nDeep neural networks (DNN) are in high demand because of their widespread applications in natural language processing, image processing, and a lot of other domains. However, due to their computational expense, over-parameterization, and large memory requirements, DNN applications often require the use of substantial model resources. This strict requirement of latency and limited memory availability are hurdles in the device deployment of these technologies. Therefore, a common idea could be to mitigate the DNN-based models\u2019 size without any performance degradation using different compression techniques. During the last few years, a great deal of progress has been made in the field of Natural Language Processing (NLP) using deep learning approaches. The objective of this research is to offer a thorough overview of the various pruning methods applied in the context of NLP. In this paper, we review several recent pruning-based schemes used for converting standard networks into their compact and accelerated versions. Traditionally, pruning is a technique for improving latency, reducing model size, and computational complexity which is a viable approach to deal with the above-mentioned challenges. In general, these techniques are divided into two main categories: structural and unstructured pruning methods. Structural pruning methods are further classified into filter, channel, layer, block, and movement pruning. Whereas, neuron, magnitude-based, and iterative pruning lie in the category of unstructured pruning. For each method, we discuss the related metrics and benchmarks. Then recent work on each method is discussed in detail, which provides insightful analysis of the performance, related applications, and pros and cons. Then, a comparative analysis is provided to analyze the differences among approaches. Finally, the paper concludes with possible future directions and some technical challenges.\n## IV. METHODS\nTheir objective was to reduce model size by removing redundant parameters while preserving speech recognition performance.The authors propose a structured pruning method to lessen the size and complexity of self-supervised pre-trained models while maintaining their performance on speech-related tasks.The key contributions and findings of the paper include the following:\n\n1) Pruning Strategy: The authors present a specific pruning strategy tailored for self-supervised pre-trained models in the speech domain.This strategy identifies and removes less important structures or components, such as layers or neurons, based on their significance to the model's overall performance.2) Performance Analysis: The paper evaluates the impact of structured pruning on speech recognition and understanding tasks.It assesses the model's accuracy and efficiency before and after pruning, demonstrating the potential benefits of structured pruning in reducing model size and computational requirements while preserving task performance.3) Comparison with Baselines: The authors compare their proposed structured pruning approach with other baseline methods commonly used for model compressions, such as unstructured pruning or weight quantization.The comparison highlights the advantages and effectiveness of structured pruning specifically for selfsupervised pre-trained models in the speech domain.The results show significant compression and improved accuracy compared to the original model, as validated by experiments on LibriSpeech and SLURP datasets.\n\nMa et al. [29] proposes Large Language Model-Pruner (LLM-Pruner), a task-agnostic compression method, aiming to maintain the multi-task solving and language generation abilities of the original LLM while minimizing reliance on the extensive training dataset.LLM-Pruner adopts structural pruning, removing non-critical coupled structures based on gradient information to preserve most of the LLM's functionality.The pruned models can be efficiently recovered through tuning techniques in a short time and with minimal data.Experimental results on three LLMs, Large Language Model Meta AI (LLaMA), Vicuna, and General Language Model (ChatGLM), demonstrate that the compressed models perform well in zero-shot classification and generation tasks.",
            "reference_string": "[270411995 | Touheed et al. | 2024 | Citations: 1]"
        },
        {
            "title": "ALPS: Improved Optimization for Highly Sparse One-Shot Pruning for Large Language Models",
            "venue": "Neural Information Processing Systems",
            "year": 2024,
            "reference_count": 72,
            "citation_count": 6,
            "influential_citation_count": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2406.07831, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2291083204",
                    "name": "Xiang Meng"
                },
                {
                    "authorId": "10706882",
                    "name": "Kayhan Behdin"
                },
                {
                    "authorId": "46506087",
                    "name": "Haoyue Wang"
                },
                {
                    "authorId": "2253495328",
                    "name": "Rahul Mazumder"
                }
            ],
            "abstract": "The impressive performance of Large Language Models (LLMs) across various natural language processing tasks comes at the cost of vast computational resources and storage requirements. One-shot pruning techniques offer a way to alleviate these burdens by removing redundant weights without the need for retraining. Yet, the massive scale of LLMs often forces current pruning approaches to rely on heuristics instead of optimization-based techniques, potentially resulting in suboptimal compression. In this paper, we introduce ALPS, an optimization-based framework that tackles the pruning problem using the operator splitting technique and a preconditioned conjugate gradient-based post-processing step. Our approach incorporates novel techniques to accelerate and theoretically guarantee convergence while leveraging vectorization and GPU parallelism for efficiency. ALPS substantially outperforms state-of-the-art methods in terms of the pruning objective and perplexity reduction, particularly for highly sparse models. On the OPT-30B model with 70% sparsity, ALPS achieves a 13% reduction in test perplexity on the WikiText dataset and a 19% improvement in zero-shot benchmark performance compared to existing methods.",
            "corpus_id": 270391791,
            "sentences": [
                {
                    "corpus_id": "270391791",
                    "title": "ALPS: Improved Optimization for Highly Sparse One-Shot Pruning for Large Language Models",
                    "text": "The impressive performance of Large Language Models (LLMs) across various natural language processing tasks comes at the cost of vast computational resources and storage requirements. One-shot pruning techniques offer a way to alleviate these burdens by removing redundant weights without the need for retraining. Yet, the massive scale of LLMs often forces current pruning approaches to rely on heuristics instead of optimization-based techniques, potentially resulting in suboptimal compression. In this paper, we introduce ALPS, an optimization-based framework that tackles the pruning problem using the operator splitting technique and a preconditioned conjugate gradient-based post-processing step. Our approach incorporates novel techniques to accelerate and theoretically guarantee convergence while leveraging vectorization and GPU parallelism for efficiency. ALPS substantially outperforms state-of-the-art methods in terms of the pruning objective and perplexity reduction, particularly for highly sparse models. On the OPT-30B model with 70% sparsity, ALPS achieves a 13% reduction in test perplexity on the WikiText dataset and a 19% improvement in zero-shot benchmark performance compared to existing methods.",
                    "score": 0.6385400904267634,
                    "section_title": "abstract",
                    "char_start_offset": 0,
                    "sentence_offsets": [],
                    "ref_mentions": [],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.8603515625
                }
            ],
            "relevance_judgement": 0.8603515625,
            "relevance_judgment_input_expanded": "# Title: ALPS: Improved Optimization for Highly Sparse One-Shot Pruning for Large Language Models\n# Venue: Neural Information Processing Systems\n# Authors: Xiang Meng, Kayhan Behdin, Haoyue Wang, Rahul Mazumder\n## Abstract\nThe impressive performance of Large Language Models (LLMs) across various natural language processing tasks comes at the cost of vast computational resources and storage requirements. One-shot pruning techniques offer a way to alleviate these burdens by removing redundant weights without the need for retraining. Yet, the massive scale of LLMs often forces current pruning approaches to rely on heuristics instead of optimization-based techniques, potentially resulting in suboptimal compression. In this paper, we introduce ALPS, an optimization-based framework that tackles the pruning problem using the operator splitting technique and a preconditioned conjugate gradient-based post-processing step. Our approach incorporates novel techniques to accelerate and theoretically guarantee convergence while leveraging vectorization and GPU parallelism for efficiency. ALPS substantially outperforms state-of-the-art methods in terms of the pruning objective and perplexity reduction, particularly for highly sparse models. On the OPT-30B model with 70% sparsity, ALPS achieves a 13% reduction in test perplexity on the WikiText dataset and a 19% improvement in zero-shot benchmark performance compared to existing methods.\n",
            "reference_string": "[270391791 | Meng et al. | 2024 | Citations: 6]"
        },
        {
            "title": "When Large Language Model Meets Optimization",
            "venue": "Swarm and Evolutionary Computation",
            "year": 2024,
            "reference_count": 151,
            "citation_count": 12,
            "influential_citation_count": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2405.10098, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2301544444",
                    "name": "Sen Huang"
                },
                {
                    "authorId": "2301490921",
                    "name": "Kaixiang Yang"
                },
                {
                    "authorId": "2301455526",
                    "name": "Sheng Qi"
                },
                {
                    "authorId": "2268020758",
                    "name": "Rui Wang"
                }
            ],
            "abstract": "Optimization algorithms and large language models (LLMs) enhance decision-making in dynamic environments by integrating artificial intelligence with traditional techniques. LLMs, with extensive domain knowledge, facilitate intelligent modeling and strategic decision-making in optimization, while optimization algorithms refine LLM architectures and output quality. This synergy offers novel approaches for advancing general AI, addressing both the computational challenges of complex problems and the application of LLMs in practical scenarios. This review outlines the progress and potential of combining LLMs with optimization algorithms, providing insights for future research directions.",
            "corpus_id": 269791108,
            "sentences": [
                {
                    "corpus_id": "269791108",
                    "title": "When Large Language Model Meets Optimization",
                    "text": "Structural pruning optimizes large language models (LLMs) by selectively removing non-critical coupled structures based on gradient information, effectively reducing model size while preserving functionality and ensuring taskagnosticism.Structural pruning is an essential optimization technique used to enhance pre-trained LLMs for subsequent tasks, such as text categorization and sentiment analysis.Structural pruning, as suggested by Klein [59], seeks to uncover numerous subnetworks of LLMs that achieve a compromise between performance and size, making them easier to use in different real-world applications.This approach employs a multi-objective local search algorithm to identify numerous Pareto-optimal subnetworks effectively.It does this by minimizing evaluation costs via weight sharing.Ma et al. [87] propose the LLM-Pruner approach to optimize large language models by selectively removing non-critical coupling structures based on gradient information to achieve efficient compression while preserving functionality and task agnosticism.Gholami et al. [44] demonstrate that weight pruning can be used as an optimisation strategy for the Transfer architecture, proving that judicious pruning can significantly reduce model size without sacrificing performance, thus contributing to bridging the gap between model efficiency and performance.",
                    "score": 0.6109725805490331,
                    "section_title": "Based on structural pruning",
                    "char_start_offset": 51118,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 237
                        },
                        {
                            "start": 237,
                            "end": 401
                        },
                        {
                            "start": 401,
                            "end": 614
                        },
                        {
                            "start": 614,
                            "end": 737
                        },
                        {
                            "start": 737,
                            "end": 800
                        },
                        {
                            "start": 800,
                            "end": 1053
                        },
                        {
                            "start": 1053,
                            "end": 1355
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 810,
                            "end": 814,
                            "matchedPaperCorpusId": "258823276"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.8603515625
                }
            ],
            "relevance_judgement": 0.8603515625,
            "relevance_judgment_input_expanded": "# Title: When Large Language Model Meets Optimization\n# Venue: Swarm and Evolutionary Computation\n# Authors: Sen Huang, Kaixiang Yang, Sheng Qi, Rui Wang\n## Abstract\nOptimization algorithms and large language models (LLMs) enhance decision-making in dynamic environments by integrating artificial intelligence with traditional techniques. LLMs, with extensive domain knowledge, facilitate intelligent modeling and strategic decision-making in optimization, while optimization algorithms refine LLM architectures and output quality. This synergy offers novel approaches for advancing general AI, addressing both the computational challenges of complex problems and the application of LLMs in practical scenarios. This review outlines the progress and potential of combining LLMs with optimization algorithms, providing insights for future research directions.\n## Based on structural pruning\nStructural pruning optimizes large language models (LLMs) by selectively removing non-critical coupled structures based on gradient information, effectively reducing model size while preserving functionality and ensuring taskagnosticism.Structural pruning is an essential optimization technique used to enhance pre-trained LLMs for subsequent tasks, such as text categorization and sentiment analysis.Structural pruning, as suggested by Klein [59], seeks to uncover numerous subnetworks of LLMs that achieve a compromise between performance and size, making them easier to use in different real-world applications.This approach employs a multi-objective local search algorithm to identify numerous Pareto-optimal subnetworks effectively.It does this by minimizing evaluation costs via weight sharing.Ma et al. [87] propose the LLM-Pruner approach to optimize large language models by selectively removing non-critical coupling structures based on gradient information to achieve efficient compression while preserving functionality and task agnosticism.Gholami et al. [44] demonstrate that weight pruning can be used as an optimisation strategy for the Transfer architecture, proving that judicious pruning can significantly reduce model size without sacrificing performance, thus contributing to bridging the gap between model efficiency and performance.",
            "reference_string": "[269791108 | Huang et al. | 2024 | Citations: 12]"
        },
        {
            "title": "Achieving Peak Performance for Large Language Models: A Systematic Review",
            "venue": "IEEE Access",
            "year": 2024,
            "reference_count": 98,
            "citation_count": 4,
            "influential_citation_count": 0,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://doi.org/10.1109/access.2024.3424945",
                "status": "GOLD",
                "license": "CCBYNCND",
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2409.04833, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "1470666105",
                    "name": "Z. R. K. Rostam"
                },
                {
                    "authorId": "3208184",
                    "name": "S. Sz\u00e9n\u00e1si"
                },
                {
                    "authorId": "9717627",
                    "name": "G\u00e1bor Kert\u00e9sz"
                }
            ],
            "abstract": "In recent years, large language models (LLMs) have achieved remarkable success in natural language processing (NLP). LLMs require an extreme amount of parameters to attain high performance. As models grow into the trillion-parameter range, computational and memory costs increase significantly. This makes it difficult for many researchers to access the resources needed to train or apply these models. Optimizing LLM performance involves two main approaches: fine-tuning pre-trained models for specific tasks to achieve state-of-the-art performance, and reducing costs or improving training time while maintaining similar performance. This paper presents a systematic literature review (SLR) following the Preferred Reporting Items for Systematic Reviews and Meta-Analyses (PRISMA) statement. We reviewed 65 publications out of 983 from 2017 to December 2023, retrieved from 5 databases. The study presents methods to optimize and accelerate LLMs while achieving cutting-edge results without sacrificing accuracy. We begin with an overview of the development of language modeling, followed by a detailed explanation of commonly used frameworks and libraries, and a taxonomy for improving and speeding up LLMs based on three classes: LLM training, LLM inference, and system serving. We then delve into recent optimization and acceleration strategies such as training optimization, hardware optimization, scalability and reliability, accompanied by the taxonomy and categorization of these strategies. Finally, we provide an in-depth comparison of each class and strategy, with two case studies on optimizing model training and enhancing inference efficiency. These case studies showcase practical approaches to address LLM resource limitations while maintaining performance.",
            "corpus_id": 271083368,
            "sentences": [
                {
                    "corpus_id": "271083368",
                    "title": "Achieving Peak Performance for Large Language Models: A Systematic Review",
                    "text": "Background: LLMs like GPT-3 have billions of parameters, which pose significant challenges in terms of storage, computational requirements, and energy consumption. Pruning, or removing less important parameters, can help mitigate these issues, but traditional pruning methods often require multiple iterations of fine-tuning, which is computationally expensive. This approach (SparseGPT [72]) proposes a one-shot pruning method that significantly reduces the number of parameters without the need for extensive retraining. \n\nContext and Problem: In this case study, the focus is on training a LLM with billions of parameters on limited hardware. The initial challenge was the high computational and memory requirements that exceeded the capabilities of available resources, making it difficult to efficiently train the model within a reasonable timeframe and budget. \n\nOptimization Strategy: The primary optimization strategies involved in SparseGPT are: \n\nOne-Shot Pruning: To achieve significant sparsity in the LLM in a single pruning step, eliminating the need for iterative pruning and retraining. One-Shot Pruning: SparseGPT implements its pruning strategy through a streamlined process. First, a thorough model analysis is conducted to pinpoint parameters that can be removed without significant impact. This analysis leverages pruning criteria that assess parameter importance without requiring gradient calculations, saving on computational resources. Finally, SparseGPT employs a single step pruning approach, achieving substantial sparsity (at least 50% for massive models) in a single step. This oneshot approach significantly reduces the time and complexity compared to iterative pruning methods. \n\nUnstructured Sparsity: To reduce the number of parameters while maintaining model accuracy through unstructured pruning, where individual weights are removed based on their importance. This approach focuses on eliminating individual weights within the model that are deemed less important. By analyzing the model's internal structure, SparseGPT achieves impressive sparsity levels of 50-60%, significantly reducing model size. This aggressive pruning strategy is remarkable because it achieves this with minimal impact on the model's ability to perform language modeling tasks accurately. For instance, SparseGPT can remove over 100 billion weights from massive models like OPT-175B and BLOOM-176B without compromising their performance on language modeling tasks.",
                    "score": 0.750640398579751,
                    "section_title": "A. OPTIMIZING MODEL TRAINING WITH SPARSEGPT",
                    "char_start_offset": 113010,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 163
                        },
                        {
                            "start": 164,
                            "end": 361
                        },
                        {
                            "start": 362,
                            "end": 522
                        },
                        {
                            "start": 525,
                            "end": 645
                        },
                        {
                            "start": 646,
                            "end": 866
                        },
                        {
                            "start": 869,
                            "end": 954
                        },
                        {
                            "start": 957,
                            "end": 1102
                        },
                        {
                            "start": 1103,
                            "end": 1193
                        },
                        {
                            "start": 1194,
                            "end": 1310
                        },
                        {
                            "start": 1311,
                            "end": 1460
                        },
                        {
                            "start": 1461,
                            "end": 1602
                        },
                        {
                            "start": 1603,
                            "end": 1709
                        },
                        {
                            "start": 1712,
                            "end": 1896
                        },
                        {
                            "start": 1897,
                            "end": 2001
                        },
                        {
                            "start": 2002,
                            "end": 2138
                        },
                        {
                            "start": 2139,
                            "end": 2300
                        },
                        {
                            "start": 2301,
                            "end": 2476
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 387,
                            "end": 391,
                            "matchedPaperCorpusId": "255372747"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.8583984375
                },
                {
                    "corpus_id": "271083368",
                    "title": "Achieving Peak Performance for Large Language Models: A Systematic Review",
                    "text": "For instance, SparseGPT can remove over 100 billion weights from massive models like OPT-175B and BLOOM-176B without compromising their performance on language modeling tasks. \n\nParametrization without Gradient Dependence: To leverage the parametrization of massive GPT models to enable pruning without relying on gradient information. This method allows the identification of sparse counterparts within a close range of the original dense model, ensuring these sparse models maintain similar performance. Interestingly, the strategy highlights that larger models are even easier to prune using this approach. They experience minimal accuracy drops even at significant sparsity levels (e.g., 50%). This observation underscores the effectiveness of the parametrization technique in enabling aggressive pruning while preserving model performance. \n\nOutcomes: The application of SparseGPT led to remarkable results: \n\n\u2022 Model size reduction: SparseGPT achieved 50-60% sparsity, significantly reducing the model size by removing more than 100 billion weights in models like OPT-175B and BLOOM-176B. \u2022 Processing time: The pruning process was completed in less than 4.5 hours for the largest open-source models, demonstrating high efficiency. \n\n\u2022 Accuracy maintenance: The pruned models exhibited negligible increases in perplexity and retained performance levels very similar to their dense counterparts. \n\n\u2022 Scalability: The study revealed that larger models are easier to prune, with practically no accuracy decrease observed at 50% sparsity. \n\nThis case study demonstrates the efficacy of SparseGPT's one-shot pruning approach for reducing the size of massive language models. By leveraging unstructured sparsity and parametrization strategies without gradient dependence, SparseGPT achieves substantial reductions in model size and resource requirements while maintaining high levels of performance. This approach enables more efficient and accessible deployment of large language models in various applications, making them more practical for real-world use.",
                    "score": 0.6221481433352118,
                    "section_title": "A. OPTIMIZING MODEL TRAINING WITH SPARSEGPT",
                    "char_start_offset": 115311,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 175
                        },
                        {
                            "start": 178,
                            "end": 335
                        },
                        {
                            "start": 336,
                            "end": 505
                        },
                        {
                            "start": 506,
                            "end": 609
                        },
                        {
                            "start": 610,
                            "end": 697
                        },
                        {
                            "start": 698,
                            "end": 844
                        },
                        {
                            "start": 847,
                            "end": 912
                        },
                        {
                            "start": 915,
                            "end": 1094
                        },
                        {
                            "start": 1095,
                            "end": 1237
                        },
                        {
                            "start": 1240,
                            "end": 1400
                        },
                        {
                            "start": 1403,
                            "end": 1540
                        },
                        {
                            "start": 1543,
                            "end": 1675
                        },
                        {
                            "start": 1676,
                            "end": 1899
                        },
                        {
                            "start": 1900,
                            "end": 2059
                        }
                    ],
                    "ref_mentions": [],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.8037109375
                }
            ],
            "relevance_judgement": 0.8583984375,
            "relevance_judgment_input_expanded": "# Title: Achieving Peak Performance for Large Language Models: A Systematic Review\n# Venue: IEEE Access\n# Authors: Z. R. K. Rostam, S. Sz\u00e9n\u00e1si, G\u00e1bor Kert\u00e9sz\n## Abstract\nIn recent years, large language models (LLMs) have achieved remarkable success in natural language processing (NLP). LLMs require an extreme amount of parameters to attain high performance. As models grow into the trillion-parameter range, computational and memory costs increase significantly. This makes it difficult for many researchers to access the resources needed to train or apply these models. Optimizing LLM performance involves two main approaches: fine-tuning pre-trained models for specific tasks to achieve state-of-the-art performance, and reducing costs or improving training time while maintaining similar performance. This paper presents a systematic literature review (SLR) following the Preferred Reporting Items for Systematic Reviews and Meta-Analyses (PRISMA) statement. We reviewed 65 publications out of 983 from 2017 to December 2023, retrieved from 5 databases. The study presents methods to optimize and accelerate LLMs while achieving cutting-edge results without sacrificing accuracy. We begin with an overview of the development of language modeling, followed by a detailed explanation of commonly used frameworks and libraries, and a taxonomy for improving and speeding up LLMs based on three classes: LLM training, LLM inference, and system serving. We then delve into recent optimization and acceleration strategies such as training optimization, hardware optimization, scalability and reliability, accompanied by the taxonomy and categorization of these strategies. Finally, we provide an in-depth comparison of each class and strategy, with two case studies on optimizing model training and enhancing inference efficiency. These case studies showcase practical approaches to address LLM resource limitations while maintaining performance.\n## A. OPTIMIZING MODEL TRAINING WITH SPARSEGPT\nBackground: LLMs like GPT-3 have billions of parameters, which pose significant challenges in terms of storage, computational requirements, and energy consumption. Pruning, or removing less important parameters, can help mitigate these issues, but traditional pruning methods often require multiple iterations of fine-tuning, which is computationally expensive. This approach (SparseGPT [72]) proposes a one-shot pruning method that significantly reduces the number of parameters without the need for extensive retraining. \n\nContext and Problem: In this case study, the focus is on training a LLM with billions of parameters on limited hardware. The initial challenge was the high computational and memory requirements that exceeded the capabilities of available resources, making it difficult to efficiently train the model within a reasonable timeframe and budget. \n\nOptimization Strategy: The primary optimization strategies involved in SparseGPT are: \n\nOne-Shot Pruning: To achieve significant sparsity in the LLM in a single pruning step, eliminating the need for iterative pruning and retraining. One-Shot Pruning: SparseGPT implements its pruning strategy through a streamlined process. First, a thorough model analysis is conducted to pinpoint parameters that can be removed without significant impact. This analysis leverages pruning criteria that assess parameter importance without requiring gradient calculations, saving on computational resources. Finally, SparseGPT employs a single step pruning approach, achieving substantial sparsity (at least 50% for massive models) in a single step. This oneshot approach significantly reduces the time and complexity compared to iterative pruning methods. \n\nUnstructured Sparsity: To reduce the number of parameters while maintaining model accuracy through unstructured pruning, where individual weights are removed based on their importance. This approach focuses on eliminating individual weights within the model that are deemed less important. By analyzing the model's internal structure, SparseGPT achieves impressive sparsity levels of 50-60%, significantly reducing model size. This aggressive pruning strategy is remarkable because it achieves this with minimal impact on the model's ability to perform language modeling tasks accurately. For instance, SparseGPT can remove over 100 billion weights from massive models like OPT-175B and BLOOM-176B without compromising their performance on language modeling tasks.\n...\nFor instance, SparseGPT can remove over 100 billion weights from massive models like OPT-175B and BLOOM-176B without compromising their performance on language modeling tasks. \n\nParametrization without Gradient Dependence: To leverage the parametrization of massive GPT models to enable pruning without relying on gradient information. This method allows the identification of sparse counterparts within a close range of the original dense model, ensuring these sparse models maintain similar performance. Interestingly, the strategy highlights that larger models are even easier to prune using this approach. They experience minimal accuracy drops even at significant sparsity levels (e.g., 50%). This observation underscores the effectiveness of the parametrization technique in enabling aggressive pruning while preserving model performance. \n\nOutcomes: The application of SparseGPT led to remarkable results: \n\n\u2022 Model size reduction: SparseGPT achieved 50-60% sparsity, significantly reducing the model size by removing more than 100 billion weights in models like OPT-175B and BLOOM-176B. \u2022 Processing time: The pruning process was completed in less than 4.5 hours for the largest open-source models, demonstrating high efficiency. \n\n\u2022 Accuracy maintenance: The pruned models exhibited negligible increases in perplexity and retained performance levels very similar to their dense counterparts. \n\n\u2022 Scalability: The study revealed that larger models are easier to prune, with practically no accuracy decrease observed at 50% sparsity. \n\nThis case study demonstrates the efficacy of SparseGPT's one-shot pruning approach for reducing the size of massive language models. By leveraging unstructured sparsity and parametrization strategies without gradient dependence, SparseGPT achieves substantial reductions in model size and resource requirements while maintaining high levels of performance. This approach enables more efficient and accessible deployment of large language models in various applications, making them more practical for real-world use.",
            "reference_string": "[271083368 | Rostam et al. | 2024 | Citations: 4]"
        },
        {
            "title": "Not All Experts are Equal: Efficient Expert Pruning and Skipping for Mixture-of-Experts Large Language Models",
            "venue": "Annual Meeting of the Association for Computational Linguistics",
            "year": 2024,
            "reference_count": 37,
            "citation_count": 33,
            "influential_citation_count": 6,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2402.14800, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2238395310",
                    "name": "Xudong Lu"
                },
                {
                    "authorId": "2284732510",
                    "name": "Qi Liu"
                },
                {
                    "authorId": "48615640",
                    "name": "Yuhui Xu"
                },
                {
                    "authorId": "2276425017",
                    "name": "Aojun Zhou"
                },
                {
                    "authorId": "2243292807",
                    "name": "Siyuan Huang"
                },
                {
                    "authorId": "2141897317",
                    "name": "Bo Zhang"
                },
                {
                    "authorId": "2281908785",
                    "name": "Junchi Yan"
                },
                {
                    "authorId": "2285017444",
                    "name": "Hongsheng Li"
                }
            ],
            "abstract": "A pivotal advancement in the progress of large language models (LLMs) is the emergence of the Mixture-of-Experts (MoE) LLMs. Compared to traditional LLMs, MoE LLMs can achieve higher performance with fewer parameters, but it is still hard to deploy them due to their immense parameter sizes. Different from previous weight pruning methods that rely on specifically designed hardware, this paper mainly aims to enhance the deployment efficiency of MoE LLMs by introducing plug-and-play expert-level sparsification techniques. Specifically, we propose, for the first time to our best knowledge, post-training approaches for task-agnostic and task-specific expert pruning and skipping of MoE LLMs, tailored to improve deployment efficiency while maintaining model performance across a wide range of tasks. Extensive experiments show that our proposed methods can simultaneously reduce model sizes and increase the inference speed, while maintaining satisfactory performance. Data and code will be available at https://github.com/Lucky-Lance/Expert_Sparsity.",
            "corpus_id": 267782440,
            "sentences": [
                {
                    "corpus_id": "267782440",
                    "title": "Not All Experts are Equal: Efficient Expert Pruning and Skipping for Mixture-of-Experts Large Language Models",
                    "text": "A pivotal advancement in the progress of large language models (LLMs) is the emergence of the Mixture-of-Experts (MoE) LLMs. Compared to traditional LLMs, MoE LLMs can achieve higher performance with fewer parameters, but it is still hard to deploy them due to their immense parameter sizes. Different from previous weight pruning methods that rely on specifically designed hardware, this paper mainly aims to enhance the deployment efficiency of MoE LLMs by introducing plug-and-play expert-level sparsification techniques. Specifically, we propose, for the first time to our best knowledge, post-training approaches for task-agnostic and task-specific expert pruning and skipping of MoE LLMs, tailored to improve deployment efficiency while maintaining model performance across a wide range of tasks. Extensive experiments show that our proposed methods can simultaneously reduce model sizes and increase the inference speed, while maintaining satisfactory performance. Data and code will be available at https://github.com/Lucky-Lance/Expert_Sparsity.",
                    "score": 0.588277789773586,
                    "section_title": "abstract",
                    "char_start_offset": 0,
                    "sentence_offsets": [],
                    "ref_mentions": [],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.85546875
                }
            ],
            "relevance_judgement": 0.85546875,
            "relevance_judgment_input_expanded": "# Title: Not All Experts are Equal: Efficient Expert Pruning and Skipping for Mixture-of-Experts Large Language Models\n# Venue: Annual Meeting of the Association for Computational Linguistics\n# Authors: Xudong Lu, Qi Liu, Yuhui Xu, Aojun Zhou, Siyuan Huang, Bo Zhang, Junchi Yan, Hongsheng Li\n## Abstract\nA pivotal advancement in the progress of large language models (LLMs) is the emergence of the Mixture-of-Experts (MoE) LLMs. Compared to traditional LLMs, MoE LLMs can achieve higher performance with fewer parameters, but it is still hard to deploy them due to their immense parameter sizes. Different from previous weight pruning methods that rely on specifically designed hardware, this paper mainly aims to enhance the deployment efficiency of MoE LLMs by introducing plug-and-play expert-level sparsification techniques. Specifically, we propose, for the first time to our best knowledge, post-training approaches for task-agnostic and task-specific expert pruning and skipping of MoE LLMs, tailored to improve deployment efficiency while maintaining model performance across a wide range of tasks. Extensive experiments show that our proposed methods can simultaneously reduce model sizes and increase the inference speed, while maintaining satisfactory performance. Data and code will be available at https://github.com/Lucky-Lance/Expert_Sparsity.\n",
            "reference_string": "[267782440 | Lu et al. | 2024 | Citations: 33]"
        },
        {
            "title": "Symmetric Pruning of Large Language Models",
            "venue": "arXiv.org",
            "year": 2025,
            "reference_count": 45,
            "citation_count": 0,
            "influential_citation_count": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2501.18980, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2292143960",
                    "name": "Kai Yi"
                },
                {
                    "authorId": "2342412598",
                    "name": "Peter Richt'arik"
                }
            ],
            "abstract": "Popular post-training pruning methods such as Wanda and RIA are known for their simple, yet effective, designs that have shown exceptional empirical performance. Wanda optimizes performance through calibrated activations during pruning, while RIA emphasizes the relative, rather than absolute, importance of weight elements. Despite their practical success, a thorough theoretical foundation explaining these outcomes has been lacking. This paper introduces new theoretical insights that redefine the standard minimization objective for pruning, offering a deeper understanding of the factors contributing to their success. Our study extends beyond these insights by proposing complementary strategies that consider both input activations and weight significance. We validate these approaches through rigorous experiments, demonstrating substantial enhancements over existing methods. Furthermore, we introduce a novel training-free fine-tuning approach $R^2$-DSnoT that incorporates relative weight importance and a regularized decision boundary within a dynamic pruning-and-growing framework, significantly outperforming strong baselines and establishing a new state of the art.",
            "corpus_id": 276079889,
            "sentences": [
                {
                    "corpus_id": "276079889",
                    "title": "Symmetric Pruning of Large Language Models",
                    "text": "Wanda (Sun et al., 2023) introduces a pruning metric that incorporates both weight magnitudes and corresponding input activations, achieving perplexity performance comparable to SparseGPT while surpassing simple magnitude-based pruning. The RIA method (Zhang et al., 2024b) builds on Wanda by considering relative weight importance, offering performance improvements at minimal additional cost. Moreover, DSnoT (Zhang et al., 2023) proposes pruning and regrowing weights based on statistical properties (e.g., mean and variance) in each pruning row, obviating the need for retraining.",
                    "score": 0.6294570091373282,
                    "section_title": "Related Work",
                    "char_start_offset": 5961,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 236
                        },
                        {
                            "start": 237,
                            "end": 394
                        },
                        {
                            "start": 395,
                            "end": 584
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 6,
                            "end": 24,
                            "matchedPaperCorpusId": "259203115"
                        },
                        {
                            "start": 252,
                            "end": 273,
                            "matchedPaperCorpusId": "271745835"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.85400390625
                }
            ],
            "relevance_judgement": 0.85400390625,
            "relevance_judgment_input_expanded": "# Title: Symmetric Pruning of Large Language Models\n# Venue: arXiv.org\n# Authors: Kai Yi, Peter Richt'arik\n## Abstract\nPopular post-training pruning methods such as Wanda and RIA are known for their simple, yet effective, designs that have shown exceptional empirical performance. Wanda optimizes performance through calibrated activations during pruning, while RIA emphasizes the relative, rather than absolute, importance of weight elements. Despite their practical success, a thorough theoretical foundation explaining these outcomes has been lacking. This paper introduces new theoretical insights that redefine the standard minimization objective for pruning, offering a deeper understanding of the factors contributing to their success. Our study extends beyond these insights by proposing complementary strategies that consider both input activations and weight significance. We validate these approaches through rigorous experiments, demonstrating substantial enhancements over existing methods. Furthermore, we introduce a novel training-free fine-tuning approach $R^2$-DSnoT that incorporates relative weight importance and a regularized decision boundary within a dynamic pruning-and-growing framework, significantly outperforming strong baselines and establishing a new state of the art.\n## Related Work\nWanda (Sun et al., 2023) introduces a pruning metric that incorporates both weight magnitudes and corresponding input activations, achieving perplexity performance comparable to SparseGPT while surpassing simple magnitude-based pruning. The RIA method (Zhang et al., 2024b) builds on Wanda by considering relative weight importance, offering performance improvements at minimal additional cost. Moreover, DSnoT (Zhang et al., 2023) proposes pruning and regrowing weights based on statistical properties (e.g., mean and variance) in each pruning row, obviating the need for retraining.",
            "reference_string": "[276079889 | Yi et al. | 2025 | Citations: 0]"
        },
        {
            "title": "SEAP: Training-free Sparse Expert Activation Pruning Unlock the Brainpower of Large Language Models",
            "venue": "arXiv.org",
            "year": 2025,
            "reference_count": 37,
            "citation_count": 0,
            "influential_citation_count": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2503.07605, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2268495952",
                    "name": "Xun Liang"
                },
                {
                    "authorId": "2284861141",
                    "name": "Hanyu Wang"
                },
                {
                    "authorId": "2349573796",
                    "name": "Huayi Lai"
                },
                {
                    "authorId": "2268393907",
                    "name": "Simin Niu"
                },
                {
                    "authorId": "2268434524",
                    "name": "Shichao Song"
                },
                {
                    "authorId": "2303425635",
                    "name": "Jiawei Yang"
                },
                {
                    "authorId": "2326243408",
                    "name": "Jihao Zhao"
                },
                {
                    "authorId": "2268399953",
                    "name": "Feiyu Xiong"
                },
                {
                    "authorId": "2268400606",
                    "name": "Bo Tang"
                },
                {
                    "authorId": "2268429641",
                    "name": "Zhiyu Li"
                }
            ],
            "abstract": "Large Language Models have achieved remarkable success across various natural language processing tasks, yet their high computational cost during inference remains a major bottleneck. This paper introduces Sparse Expert Activation Pruning (SEAP), a training-free pruning method that selectively retains task-relevant parameters to reduce inference overhead. Inspired by the clustering patterns of hidden states and activations in LLMs, SEAP identifies task-specific expert activation patterns and prunes the model while preserving task performance and enhancing computational efficiency. Experimental results demonstrate that SEAP significantly reduces computational overhead while maintaining competitive accuracy. Notably, at 50% pruning, SEAP surpasses both WandA and FLAP by over 20%, and at 20% pruning, it incurs only a 2.2% performance drop compared to the dense model. These findings highlight SEAP's scalability and effectiveness, making it a promising approach for optimizing large-scale LLMs.",
            "corpus_id": 276928323,
            "sentences": [
                {
                    "corpus_id": "276928323",
                    "title": "SEAP: Training-free Sparse Expert Activation Pruning Unlock the Brainpower of Large Language Models",
                    "text": "Large Language Models have achieved remarkable success across various natural language processing tasks, yet their high computational cost during inference remains a major bottleneck. This paper introduces Sparse Expert Activation Pruning (SEAP), a training-free pruning method that selectively retains task-relevant parameters to reduce inference overhead. Inspired by the clustering patterns of hidden states and activations in LLMs, SEAP identifies task-specific expert activation patterns and prunes the model while preserving task performance and enhancing computational efficiency. Experimental results demonstrate that SEAP significantly reduces computational overhead while maintaining competitive accuracy. Notably, at 50% pruning, SEAP surpasses both WandA and FLAP by over 20%, and at 20% pruning, it incurs only a 2.2% performance drop compared to the dense model. These findings highlight SEAP's scalability and effectiveness, making it a promising approach for optimizing large-scale LLMs.",
                    "score": 0.6314403861584473,
                    "section_title": "abstract",
                    "char_start_offset": 0,
                    "sentence_offsets": [],
                    "ref_mentions": [],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.853515625
                }
            ],
            "relevance_judgement": 0.853515625,
            "relevance_judgment_input_expanded": "# Title: SEAP: Training-free Sparse Expert Activation Pruning Unlock the Brainpower of Large Language Models\n# Venue: arXiv.org\n# Authors: Xun Liang, Hanyu Wang, Huayi Lai, Simin Niu, Shichao Song, Jiawei Yang, Jihao Zhao, Feiyu Xiong, Bo Tang, Zhiyu Li\n## Abstract\nLarge Language Models have achieved remarkable success across various natural language processing tasks, yet their high computational cost during inference remains a major bottleneck. This paper introduces Sparse Expert Activation Pruning (SEAP), a training-free pruning method that selectively retains task-relevant parameters to reduce inference overhead. Inspired by the clustering patterns of hidden states and activations in LLMs, SEAP identifies task-specific expert activation patterns and prunes the model while preserving task performance and enhancing computational efficiency. Experimental results demonstrate that SEAP significantly reduces computational overhead while maintaining competitive accuracy. Notably, at 50% pruning, SEAP surpasses both WandA and FLAP by over 20%, and at 20% pruning, it incurs only a 2.2% performance drop compared to the dense model. These findings highlight SEAP's scalability and effectiveness, making it a promising approach for optimizing large-scale LLMs.\n",
            "reference_string": "[276928323 | Liang et al. | 2025 | Citations: 0]"
        },
        {
            "title": "VTrans: Accelerating Transformer Compression with Variational Information Bottleneck based Pruning",
            "venue": "arXiv.org",
            "year": 2024,
            "reference_count": 68,
            "citation_count": 2,
            "influential_citation_count": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2406.05276, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "83923404",
                    "name": "Oshin Dutta"
                },
                {
                    "authorId": "2305637685",
                    "name": "Ritvik Gupta"
                },
                {
                    "authorId": "2305618337",
                    "name": "Sumeet Agarwal"
                }
            ],
            "abstract": "In recent years, there has been a growing emphasis on compressing large pre-trained transformer models for resource-constrained devices. However, traditional pruning methods often leave the embedding layer untouched, leading to model over-parameterization. Additionally, they require extensive compression time with large datasets to maintain performance in pruned models. To address these challenges, we propose VTrans, an iterative pruning framework guided by the Variational Information Bottleneck (VIB) principle. Our method compresses all structural components, including embeddings, attention heads, and layers using VIB-trained masks. This approach retains only essential weights in each layer, ensuring compliance with specified model size or computational constraints. Notably, our method achieves upto 70% more compression than prior state-of-the-art approaches, both task-agnostic and task-specific. We further propose faster variants of our method: Fast-VTrans utilizing only 3% of the data and Faster-VTrans, a time efficient alternative that involves exclusive finetuning of VIB masks, accelerating compression by upto 25 times with minimal performance loss compared to previous methods. Extensive experiments on BERT, ROBERTa, and GPT-2 models substantiate the efficacy of our method. Moreover, our method demonstrates scalability in compressing large models such as LLaMA-2-7B, achieving superior performance compared to previous pruning methods. Additionally, we use attention-based probing to qualitatively assess model redundancy and interpret the efficiency of our approach. Notably, our method considers heads with high attention to special and current tokens in un-pruned model as foremost candidates for pruning while retained heads are observed to attend more to task-critical keywords.",
            "corpus_id": 270370902,
            "sentences": [
                {
                    "corpus_id": "270370902",
                    "title": "VTrans: Accelerating Transformer Compression with Variational Information Bottleneck based Pruning",
                    "text": "Post-Prune uses post-training pruning framework (Aguilar et al., 2020) that proposes a Fisher-based technique to train masks for identification of redundant neurons and selectively fine-tunes only the masks to specific tasks.\n\nWe show the scaling ability of our pruning technique to prune large language models like LLaMA-2 (Touvron et al., 2023).Comparison with previous techniques includes SparseGPT (Frantar & Alistarh, 2023) which proposes a second-order layer-wise pruning method that approximates closed form equations thus being able to scale up pruning LLMs.Wanda (Sun et al., 2023) takes into account the norm of weights and input activations for pruning weights in an unstructured/structured manner.Bonsai (Dery et al., 2024) is a gradient-free structured pruning method that estimates module importance perturbatively by generating sub-models and evaluating their performances.LLM-pruner (Ma et al., 2023) is a structured pruning method that uses gradient information to prune large language models in a task-agnostic manner.",
                    "score": 0.7969145063855094,
                    "section_title": "A.2 Further details about the baseline models",
                    "char_start_offset": 25157,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 225
                        },
                        {
                            "start": 227,
                            "end": 347
                        },
                        {
                            "start": 347,
                            "end": 566
                        },
                        {
                            "start": 566,
                            "end": 709
                        },
                        {
                            "start": 709,
                            "end": 888
                        },
                        {
                            "start": 888,
                            "end": 1036
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 48,
                            "end": 70,
                            "matchedPaperCorpusId": "203953149"
                        },
                        {
                            "start": 402,
                            "end": 428,
                            "matchedPaperCorpusId": "255372747"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.85302734375
                }
            ],
            "relevance_judgement": 0.85302734375,
            "relevance_judgment_input_expanded": "# Title: VTrans: Accelerating Transformer Compression with Variational Information Bottleneck based Pruning\n# Venue: arXiv.org\n# Authors: Oshin Dutta, Ritvik Gupta, Sumeet Agarwal\n## Abstract\nIn recent years, there has been a growing emphasis on compressing large pre-trained transformer models for resource-constrained devices. However, traditional pruning methods often leave the embedding layer untouched, leading to model over-parameterization. Additionally, they require extensive compression time with large datasets to maintain performance in pruned models. To address these challenges, we propose VTrans, an iterative pruning framework guided by the Variational Information Bottleneck (VIB) principle. Our method compresses all structural components, including embeddings, attention heads, and layers using VIB-trained masks. This approach retains only essential weights in each layer, ensuring compliance with specified model size or computational constraints. Notably, our method achieves upto 70% more compression than prior state-of-the-art approaches, both task-agnostic and task-specific. We further propose faster variants of our method: Fast-VTrans utilizing only 3% of the data and Faster-VTrans, a time efficient alternative that involves exclusive finetuning of VIB masks, accelerating compression by upto 25 times with minimal performance loss compared to previous methods. Extensive experiments on BERT, ROBERTa, and GPT-2 models substantiate the efficacy of our method. Moreover, our method demonstrates scalability in compressing large models such as LLaMA-2-7B, achieving superior performance compared to previous pruning methods. Additionally, we use attention-based probing to qualitatively assess model redundancy and interpret the efficiency of our approach. Notably, our method considers heads with high attention to special and current tokens in un-pruned model as foremost candidates for pruning while retained heads are observed to attend more to task-critical keywords.\n## A.2 Further details about the baseline models\nPost-Prune uses post-training pruning framework (Aguilar et al., 2020) that proposes a Fisher-based technique to train masks for identification of redundant neurons and selectively fine-tunes only the masks to specific tasks.\n\nWe show the scaling ability of our pruning technique to prune large language models like LLaMA-2 (Touvron et al., 2023).Comparison with previous techniques includes SparseGPT (Frantar & Alistarh, 2023) which proposes a second-order layer-wise pruning method that approximates closed form equations thus being able to scale up pruning LLMs.Wanda (Sun et al., 2023) takes into account the norm of weights and input activations for pruning weights in an unstructured/structured manner.Bonsai (Dery et al., 2024) is a gradient-free structured pruning method that estimates module importance perturbatively by generating sub-models and evaluating their performances.LLM-pruner (Ma et al., 2023) is a structured pruning method that uses gradient information to prune large language models in a task-agnostic manner.",
            "reference_string": "[270370902 | Dutta et al. | 2024 | Citations: 2]"
        },
        {
            "title": "A Review on Edge Large Language Models: Design, Execution, and Applications",
            "venue": "ACM Computing Surveys",
            "year": 2024,
            "reference_count": 212,
            "citation_count": 22,
            "influential_citation_count": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2410.11845, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2326271980",
                    "name": "Yue Zheng"
                },
                {
                    "authorId": "2115868988",
                    "name": "Yuhao Chen"
                },
                {
                    "authorId": "101038657",
                    "name": "Bin Qian"
                },
                {
                    "authorId": "2324311927",
                    "name": "Xiufang Shi"
                },
                {
                    "authorId": "2294175636",
                    "name": "Yuanchao Shu"
                },
                {
                    "authorId": "2326126174",
                    "name": "Jiming Chen"
                }
            ],
            "abstract": "Large language models (LLMs) have revolutionized natural language processing with their exceptional understanding, synthesizing, and reasoning capabilities. However, deploying LLMs on resource-constrained edge devices presents significant challenges due to computational limitations, memory constraints, and edge hardware heterogeneity. This survey provides a comprehensive overview of recent advancements in edge LLMs, covering the entire lifecycle\u2014from resource-efficient model design and pre-deployment strategies to runtime inference optimizations. It also explores on-device applications across various domains. By synthesizing state-of-the-art techniques and identifying future research directions, this survey bridges the gap between the immense potential of LLMs and the constraints of edge computing.",
            "corpus_id": 273375608,
            "sentences": [
                {
                    "corpus_id": "273375608",
                    "title": "A Review on Edge Large Language Models: Design, Execution, and Applications",
                    "text": "Pruning. Unstructured pruning removes individual weights or neurons, resulting in sparse models that are harder to optimize. Movement Pruning [150] adapts pruning decisions based on weight dynamics during fine-tuning, preserving important weights that exhibit significant movement. oBERT [94] introduces a second-order pruning method supporting both unstructured and block pruning. SparseGPT [44] treats pruning as a sparse regression problem, allowing for one-shot pruning without retraining. Plugand-Play [224] integrates activation-based importance to prune weights selectively, further improving pruning robustness in large-scale models. Wanda [160] prunes weights with the smallest magnitudes multiplied by the corresponding input activations, on a per-output basis. BESA [204] targets the overall pruning error with respect to individual transformer blocks and allocates layer-specific sparsity in a differentiable manner. \n\nOverall, pruning techniques for LLMs offer various strategies for balancing size reduction and performance retention. Structured pruning methods like CoFi [197] and LLM-Pruner [124] provide controlled reductions, preserving architecture integrity, while unstructured methods such as Movement Pruning [150] and oBERT [94] offer greater flexibility but may result in irregular, sparse models. In practice, the choice of method depends on the specific deployment scenario, considering the trade-offs between model size, computational efficiency, and task performance.",
                    "score": 0.5944875800372429,
                    "section_title": "Unstructured",
                    "char_start_offset": 22245,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 8
                        },
                        {
                            "start": 9,
                            "end": 124
                        },
                        {
                            "start": 125,
                            "end": 281
                        },
                        {
                            "start": 282,
                            "end": 381
                        },
                        {
                            "start": 382,
                            "end": 493
                        },
                        {
                            "start": 494,
                            "end": 641
                        },
                        {
                            "start": 642,
                            "end": 771
                        },
                        {
                            "start": 772,
                            "end": 928
                        },
                        {
                            "start": 931,
                            "end": 1048
                        },
                        {
                            "start": 1049,
                            "end": 1321
                        },
                        {
                            "start": 1322,
                            "end": 1495
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 288,
                            "end": 292,
                            "matchedPaperCorpusId": "247446572"
                        },
                        {
                            "start": 392,
                            "end": 396,
                            "matchedPaperCorpusId": "255372747"
                        },
                        {
                            "start": 648,
                            "end": 653,
                            "matchedPaperCorpusId": "259203115"
                        },
                        {
                            "start": 1086,
                            "end": 1091,
                            "matchedPaperCorpusId": "247922354"
                        },
                        {
                            "start": 1107,
                            "end": 1112,
                            "matchedPaperCorpusId": "258823276"
                        },
                        {
                            "start": 1247,
                            "end": 1251,
                            "matchedPaperCorpusId": "247446572"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.8525390625
                }
            ],
            "relevance_judgement": 0.8525390625,
            "relevance_judgment_input_expanded": "# Title: A Review on Edge Large Language Models: Design, Execution, and Applications\n# Venue: ACM Computing Surveys\n# Authors: Yue Zheng, Yuhao Chen, Bin Qian, Xiufang Shi, Yuanchao Shu, Jiming Chen\n## Abstract\nLarge language models (LLMs) have revolutionized natural language processing with their exceptional understanding, synthesizing, and reasoning capabilities. However, deploying LLMs on resource-constrained edge devices presents significant challenges due to computational limitations, memory constraints, and edge hardware heterogeneity. This survey provides a comprehensive overview of recent advancements in edge LLMs, covering the entire lifecycle\u2014from resource-efficient model design and pre-deployment strategies to runtime inference optimizations. It also explores on-device applications across various domains. By synthesizing state-of-the-art techniques and identifying future research directions, this survey bridges the gap between the immense potential of LLMs and the constraints of edge computing.\n## Unstructured\nPruning. Unstructured pruning removes individual weights or neurons, resulting in sparse models that are harder to optimize. Movement Pruning [150] adapts pruning decisions based on weight dynamics during fine-tuning, preserving important weights that exhibit significant movement. oBERT [94] introduces a second-order pruning method supporting both unstructured and block pruning. SparseGPT [44] treats pruning as a sparse regression problem, allowing for one-shot pruning without retraining. Plugand-Play [224] integrates activation-based importance to prune weights selectively, further improving pruning robustness in large-scale models. Wanda [160] prunes weights with the smallest magnitudes multiplied by the corresponding input activations, on a per-output basis. BESA [204] targets the overall pruning error with respect to individual transformer blocks and allocates layer-specific sparsity in a differentiable manner. \n\nOverall, pruning techniques for LLMs offer various strategies for balancing size reduction and performance retention. Structured pruning methods like CoFi [197] and LLM-Pruner [124] provide controlled reductions, preserving architecture integrity, while unstructured methods such as Movement Pruning [150] and oBERT [94] offer greater flexibility but may result in irregular, sparse models. In practice, the choice of method depends on the specific deployment scenario, considering the trade-offs between model size, computational efficiency, and task performance.",
            "reference_string": "[273375608 | Zheng et al. | 2024 | Citations: 22]"
        },
        {
            "title": "COPAL: Continual Pruning in Large Language Generative Models",
            "venue": "International Conference on Machine Learning",
            "year": 2024,
            "reference_count": 35,
            "citation_count": 2,
            "influential_citation_count": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2405.02347, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "51128743",
                    "name": "Srikanth Malla"
                },
                {
                    "authorId": "2300246153",
                    "name": "Joon Hee Choi"
                },
                {
                    "authorId": "2301128189",
                    "name": "Chiho Choi"
                }
            ],
            "abstract": "Adapting pre-trained large language models to different domains in natural language processing requires two key considerations: high computational demands and model's inability to continual adaptation. To simultaneously address both issues, this paper presents COPAL (COntinual Pruning in Adaptive Language settings), an algorithm developed for pruning large language generative models under a continual model adaptation setting. While avoiding resource-heavy finetuning or retraining, our pruning process is guided by the proposed sensitivity analysis. The sensitivity effectively measures model's ability to withstand perturbations introduced by the new dataset and finds model's weights that are relevant for all encountered datasets. As a result, COPAL allows seamless model adaptation to new domains while enhancing the resource efficiency. Our empirical evaluation on a various size of LLMs show that COPAL outperforms baseline models, demonstrating its efficacy in efficiency and adaptability.",
            "corpus_id": 269605957,
            "sentences": [
                {
                    "corpus_id": "269605957",
                    "title": "COPAL: Continual Pruning in Large Language Generative Models",
                    "text": "Upon transitioning to the new dataset, COPAL employs a small set of calibration data to guide the pruning process based on the sensitivity analysis we propose.Since the sensitivity measures model's robustness to perturbation caused by the new dataset, this strategy enables the model to seamlessly adapt to new information while preserving previous knowledge.To the best of our knowledge, we are the first to introduce the concept of continual pruning that addresses pruning under a continual model adaptation setting, bypassing the requirement for model re-training.This marks a substantial advancement in the field of LLM optimization.\n\nTo illustrate the effectiveness of our approach, Figure 1 demonstrates the impact of COPAL in both average and worst-case scenarios, showcasing how continual pruning can maintain performance even with increased sparsity.This highlights COPAL's ability to adeptly navigate the balance between model complexity and performance, a crucial factor in real-world applications.\n\nOur contributions are threefold:\n\n\u2022 We explore the inherent challenges in finetuning pretrained LLMs and provide a strategic solution to address both computational inefficiency and limited model adaptability.\n\n\u2022 We propose a mathematical formulation for the concept of continual pruning, utilizing sensitivity analysis to enable the pruning process under a continual model adaptation setting.\n\n\u2022 Through empirical evaluations on large-scale language models, including LLaMA-7B, 13B, 30B, 65B, we show that COPAL outperforms baseline methods, setting a new standard in LLM optimization for both efficiency and adaptability.",
                    "score": 0.5891311612075506,
                    "section_title": "Introduction",
                    "char_start_offset": 3539,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 159
                        },
                        {
                            "start": 159,
                            "end": 359
                        },
                        {
                            "start": 359,
                            "end": 567
                        },
                        {
                            "start": 567,
                            "end": 637
                        },
                        {
                            "start": 639,
                            "end": 859
                        },
                        {
                            "start": 859,
                            "end": 1009
                        },
                        {
                            "start": 1011,
                            "end": 1043
                        },
                        {
                            "start": 1045,
                            "end": 1219
                        },
                        {
                            "start": 1221,
                            "end": 1403
                        },
                        {
                            "start": 1405,
                            "end": 1633
                        }
                    ],
                    "ref_mentions": [],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.849609375
                }
            ],
            "relevance_judgement": 0.849609375,
            "relevance_judgment_input_expanded": "# Title: COPAL: Continual Pruning in Large Language Generative Models\n# Venue: International Conference on Machine Learning\n# Authors: Srikanth Malla, Joon Hee Choi, Chiho Choi\n## Abstract\nAdapting pre-trained large language models to different domains in natural language processing requires two key considerations: high computational demands and model's inability to continual adaptation. To simultaneously address both issues, this paper presents COPAL (COntinual Pruning in Adaptive Language settings), an algorithm developed for pruning large language generative models under a continual model adaptation setting. While avoiding resource-heavy finetuning or retraining, our pruning process is guided by the proposed sensitivity analysis. The sensitivity effectively measures model's ability to withstand perturbations introduced by the new dataset and finds model's weights that are relevant for all encountered datasets. As a result, COPAL allows seamless model adaptation to new domains while enhancing the resource efficiency. Our empirical evaluation on a various size of LLMs show that COPAL outperforms baseline models, demonstrating its efficacy in efficiency and adaptability.\n## Introduction\nUpon transitioning to the new dataset, COPAL employs a small set of calibration data to guide the pruning process based on the sensitivity analysis we propose.Since the sensitivity measures model's robustness to perturbation caused by the new dataset, this strategy enables the model to seamlessly adapt to new information while preserving previous knowledge.To the best of our knowledge, we are the first to introduce the concept of continual pruning that addresses pruning under a continual model adaptation setting, bypassing the requirement for model re-training.This marks a substantial advancement in the field of LLM optimization.\n\nTo illustrate the effectiveness of our approach, Figure 1 demonstrates the impact of COPAL in both average and worst-case scenarios, showcasing how continual pruning can maintain performance even with increased sparsity.This highlights COPAL's ability to adeptly navigate the balance between model complexity and performance, a crucial factor in real-world applications.\n\nOur contributions are threefold:\n\n\u2022 We explore the inherent challenges in finetuning pretrained LLMs and provide a strategic solution to address both computational inefficiency and limited model adaptability.\n\n\u2022 We propose a mathematical formulation for the concept of continual pruning, utilizing sensitivity analysis to enable the pruning process under a continual model adaptation setting.\n\n\u2022 Through empirical evaluations on large-scale language models, including LLaMA-7B, 13B, 30B, 65B, we show that COPAL outperforms baseline methods, setting a new standard in LLM optimization for both efficiency and adaptability.",
            "reference_string": "[269605957 | Malla et al. | 2024 | Citations: 2]"
        },
        {
            "title": "Zeroth-Order Adaptive Neuron Alignment Based Pruning without Re-Training",
            "venue": "arXiv.org",
            "year": 2024,
            "reference_count": 49,
            "citation_count": 0,
            "influential_citation_count": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2411.07066, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2162450849",
                    "name": "Elia Cunegatti"
                },
                {
                    "authorId": "2037391480",
                    "name": "Leonardo Lucio Custode"
                },
                {
                    "authorId": "2295670461",
                    "name": "Giovanni Iacca"
                }
            ],
            "abstract": "Network pruning focuses on computational techniques that aim to reduce a given model's computational cost by removing a subset of its parameters while having minimal impact on performance. Throughout the last decade, the most widely used pruning paradigm has been pruning and re-training, which nowadays is inconvenient due to the vast amount of pre-trained models, which are in any case too expensive to re-train. In this paper, we exploit functional information from dense pre-trained models, i.e., their activations, to obtain sparse models that maximize the activations' alignment w.r.t. their corresponding dense models. Hence, we propose \\textsc{NeuroAL}, a \\emph{top-up} algorithm that can be used on top of any given pruning algorithm for LLMs, which modifies the block-wise and row-wise sparsity exploiting information from both the dense model and its sparse version to maximize the \\emph{neuron alignment} among activations. Differently from existing methods, our approach adaptively selects the best hyperparameters for the block-wise and row-wise sparsity ratios w.r.t. the model and the desired sparsity, and requires \\emph{no re-training}. We test our method over 276 cases combining four LLM families, three sparsity ratios, and ten language tasks (three language modeling and seven zero-shot datasets), showing how it consistently outperforms the latest state-of-the-art methods in terms of performance-runtime trade-off. The code is available at \\href{https://github.com/eliacunegatti/NeuroAL}{https://github.com/eliacunegatti/NeuroAL}.",
            "corpus_id": 273962638,
            "sentences": [
                {
                    "corpus_id": "273962638",
                    "title": "Zeroth-Order Adaptive Neuron Alignment Based Pruning without Re-Training",
                    "text": "In recent times, Large Language Models (LLMs) have shown incredible performance over almost any language task [44,33,4]. However, their performance tends to grow with their sizes (i.e., the number of trainable parameters), which in turn is proportional to the computational burden required to train and then use such models. One way to reduce the computational cost of LLMs is through network pruning, i.e., algorithms that remove parameters while minimizing performance degradation. This approach has been extensively studied on Convolutional Neural Networks (CNNs) [13,25,43,11], but nowadays the focus has shifted towards pre-trained models [41,42,22]. This shift has required a change of paradigm in pruning techniques: in fact, while in CNNs the main paradigm is iterative pruning (with re-training) [13], with pre-trained models (such as LLMs) in most cases it is not possible to fully re-train such models, because (1) training data are often not accessible, and (2) full re-training would be anyway too expensive. This calls for \"exploiting\" as much as possible the information contained in a pre-trained model to obtain a performant sparse version of it, using weight's information [21], activations [40,39], or reconstruction error [15], without the need for re-training. More recently, a new category of pruning algorithms, which we may call top-up algorithms (i.e., methods that can be applied on top of a given pruning algorithm for LLMs), has emerged, aiming at further improving pruning performance. Such approaches can be divided into two categories: those that minimize the reconstruction error [18,45,49], and those that impose non-uniform sparsity distribution modifying the block-wise sparsity [46]. The latter category is extremely effective for improving performance in CNNs [14,38], while its application to LLMs is still limited [46].",
                    "score": 0.6269273828942117,
                    "section_title": "Introduction",
                    "char_start_offset": 15,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 120
                        },
                        {
                            "start": 121,
                            "end": 324
                        },
                        {
                            "start": 325,
                            "end": 483
                        },
                        {
                            "start": 484,
                            "end": 655
                        },
                        {
                            "start": 656,
                            "end": 1021
                        },
                        {
                            "start": 1022,
                            "end": 1281
                        },
                        {
                            "start": 1282,
                            "end": 1514
                        },
                        {
                            "start": 1515,
                            "end": 1719
                        },
                        {
                            "start": 1720,
                            "end": 1858
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 110,
                            "end": 114,
                            "matchedPaperCorpusId": "246411621"
                        },
                        {
                            "start": 114,
                            "end": 117,
                            "matchedPaperCorpusId": "240420063"
                        },
                        {
                            "start": 117,
                            "end": 119,
                            "matchedPaperCorpusId": "259360395"
                        },
                        {
                            "start": 577,
                            "end": 580,
                            "matchedPaperCorpusId": "208267757"
                        },
                        {
                            "start": 922,
                            "end": 925,
                            "matchedPaperCorpusId": "267301573"
                        },
                        {
                            "start": 1191,
                            "end": 1195,
                            "matchedPaperCorpusId": "259088941"
                        },
                        {
                            "start": 1209,
                            "end": 1213,
                            "matchedPaperCorpusId": "259203115"
                        },
                        {
                            "start": 1242,
                            "end": 1246,
                            "matchedPaperCorpusId": "255372747"
                        },
                        {
                            "start": 1616,
                            "end": 1619,
                            "matchedPaperCorpusId": "268032346"
                        },
                        {
                            "start": 1619,
                            "end": 1622,
                            "matchedPaperCorpusId": "264128029"
                        },
                        {
                            "start": 1714,
                            "end": 1718,
                            "matchedPaperCorpusId": "263829692"
                        },
                        {
                            "start": 1797,
                            "end": 1801,
                            "matchedPaperCorpusId": "221802286"
                        },
                        {
                            "start": 1801,
                            "end": 1804,
                            "matchedPaperCorpusId": "221857593"
                        },
                        {
                            "start": 1853,
                            "end": 1857,
                            "matchedPaperCorpusId": "263829692"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.84375
                }
            ],
            "relevance_judgement": 0.84375,
            "relevance_judgment_input_expanded": "# Title: Zeroth-Order Adaptive Neuron Alignment Based Pruning without Re-Training\n# Venue: arXiv.org\n# Authors: Elia Cunegatti, Leonardo Lucio Custode, Giovanni Iacca\n## Abstract\nNetwork pruning focuses on computational techniques that aim to reduce a given model's computational cost by removing a subset of its parameters while having minimal impact on performance. Throughout the last decade, the most widely used pruning paradigm has been pruning and re-training, which nowadays is inconvenient due to the vast amount of pre-trained models, which are in any case too expensive to re-train. In this paper, we exploit functional information from dense pre-trained models, i.e., their activations, to obtain sparse models that maximize the activations' alignment w.r.t. their corresponding dense models. Hence, we propose \\textsc{NeuroAL}, a \\emph{top-up} algorithm that can be used on top of any given pruning algorithm for LLMs, which modifies the block-wise and row-wise sparsity exploiting information from both the dense model and its sparse version to maximize the \\emph{neuron alignment} among activations. Differently from existing methods, our approach adaptively selects the best hyperparameters for the block-wise and row-wise sparsity ratios w.r.t. the model and the desired sparsity, and requires \\emph{no re-training}. We test our method over 276 cases combining four LLM families, three sparsity ratios, and ten language tasks (three language modeling and seven zero-shot datasets), showing how it consistently outperforms the latest state-of-the-art methods in terms of performance-runtime trade-off. The code is available at \\href{https://github.com/eliacunegatti/NeuroAL}{https://github.com/eliacunegatti/NeuroAL}.\n## Introduction\nIn recent times, Large Language Models (LLMs) have shown incredible performance over almost any language task [44,33,4]. However, their performance tends to grow with their sizes (i.e., the number of trainable parameters), which in turn is proportional to the computational burden required to train and then use such models. One way to reduce the computational cost of LLMs is through network pruning, i.e., algorithms that remove parameters while minimizing performance degradation. This approach has been extensively studied on Convolutional Neural Networks (CNNs) [13,25,43,11], but nowadays the focus has shifted towards pre-trained models [41,42,22]. This shift has required a change of paradigm in pruning techniques: in fact, while in CNNs the main paradigm is iterative pruning (with re-training) [13], with pre-trained models (such as LLMs) in most cases it is not possible to fully re-train such models, because (1) training data are often not accessible, and (2) full re-training would be anyway too expensive. This calls for \"exploiting\" as much as possible the information contained in a pre-trained model to obtain a performant sparse version of it, using weight's information [21], activations [40,39], or reconstruction error [15], without the need for re-training. More recently, a new category of pruning algorithms, which we may call top-up algorithms (i.e., methods that can be applied on top of a given pruning algorithm for LLMs), has emerged, aiming at further improving pruning performance. Such approaches can be divided into two categories: those that minimize the reconstruction error [18,45,49], and those that impose non-uniform sparsity distribution modifying the block-wise sparsity [46]. The latter category is extremely effective for improving performance in CNNs [14,38], while its application to LLMs is still limited [46].",
            "reference_string": "[273962638 | Cunegatti et al. | 2024 | Citations: 0]"
        },
        {
            "title": "BESA: Pruning Large Language Models with Blockwise Parameter-Efficient Sparsity Allocation",
            "venue": "International Conference on Learning Representations",
            "year": 2024,
            "reference_count": 37,
            "citation_count": 32,
            "influential_citation_count": 6,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2402.16880, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2153917002",
                    "name": "Peng Xu"
                },
                {
                    "authorId": "2283133523",
                    "name": "Wenqi Shao"
                },
                {
                    "authorId": "2287768783",
                    "name": "Mengzhao Chen"
                },
                {
                    "authorId": "2287949030",
                    "name": "Shitao Tang"
                },
                {
                    "authorId": "2273778831",
                    "name": "Kai-Chuang Zhang"
                },
                {
                    "authorId": "2269823523",
                    "name": "Peng Gao"
                },
                {
                    "authorId": "2287838451",
                    "name": "Fengwei An"
                },
                {
                    "authorId": "2256992387",
                    "name": "Yu Qiao"
                },
                {
                    "authorId": "2253674868",
                    "name": "Ping Luo"
                }
            ],
            "abstract": "Large language models (LLMs) have demonstrated outstanding performance in various tasks, such as text summarization, text question-answering, and etc. While their performance is impressive, the computational footprint due to their vast number of parameters can be prohibitive. Existing solutions such as SparseGPT and Wanda attempt to alleviate this issue through weight pruning. However, their layer-wise approach results in significant perturbation to the model's output and requires meticulous hyperparameter tuning, such as the pruning rate, which can adversely affect overall model performance. To address this, this paper introduces a novel LLM pruning technique dubbed blockwise parameter-efficient sparsity allocation (BESA) by applying a blockwise reconstruction loss. In contrast to the typical layer-wise pruning techniques, BESA is characterized by two distinctive attributes: i) it targets the overall pruning error with respect to individual transformer blocks, and ii) it allocates layer-specific sparsity in a differentiable manner, both of which ensure reduced performance degradation after pruning. Our experiments show that BESA achieves state-of-the-art performance, efficiently pruning LLMs like LLaMA1, and LLaMA2 with 7B to 70B parameters on a single A100 GPU in just five hours. Code is available at https://github.com/OpenGVLab/LLMPrune-BESA.",
            "corpus_id": 268032346,
            "sentences": [
                {
                    "corpus_id": "268032346",
                    "title": "BESA: Pruning Large Language Models with Blockwise Parameter-Efficient Sparsity Allocation",
                    "text": "Large language models (LLMs) have demonstrated outstanding performance in various tasks, such as text summarization, text question-answering, and etc. While their performance is impressive, the computational footprint due to their vast number of parameters can be prohibitive. Existing solutions such as SparseGPT and Wanda attempt to alleviate this issue through weight pruning. However, their layer-wise approach results in significant perturbation to the model's output and requires meticulous hyperparameter tuning, such as the pruning rate, which can adversely affect overall model performance. To address this, this paper introduces a novel LLM pruning technique dubbed blockwise parameter-efficient sparsity allocation (BESA) by applying a blockwise reconstruction loss. In contrast to the typical layer-wise pruning techniques, BESA is characterized by two distinctive attributes: i) it targets the overall pruning error with respect to individual transformer blocks, and ii) it allocates layer-specific sparsity in a differentiable manner, both of which ensure reduced performance degradation after pruning. Our experiments show that BESA achieves state-of-the-art performance, efficiently pruning LLMs like LLaMA1, and LLaMA2 with 7B to 70B parameters on a single A100 GPU in just five hours. Code is available at https://github.com/OpenGVLab/LLMPrune-BESA.",
                    "score": 0.6117203553908974,
                    "section_title": "abstract",
                    "char_start_offset": 0,
                    "sentence_offsets": [],
                    "ref_mentions": [],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.84326171875
                }
            ],
            "relevance_judgement": 0.84326171875,
            "relevance_judgment_input_expanded": "# Title: BESA: Pruning Large Language Models with Blockwise Parameter-Efficient Sparsity Allocation\n# Venue: International Conference on Learning Representations\n# Authors: Peng Xu, Wenqi Shao, Mengzhao Chen, Shitao Tang, Kai-Chuang Zhang, Peng Gao, Fengwei An, Yu Qiao, Ping Luo\n## Abstract\nLarge language models (LLMs) have demonstrated outstanding performance in various tasks, such as text summarization, text question-answering, and etc. While their performance is impressive, the computational footprint due to their vast number of parameters can be prohibitive. Existing solutions such as SparseGPT and Wanda attempt to alleviate this issue through weight pruning. However, their layer-wise approach results in significant perturbation to the model's output and requires meticulous hyperparameter tuning, such as the pruning rate, which can adversely affect overall model performance. To address this, this paper introduces a novel LLM pruning technique dubbed blockwise parameter-efficient sparsity allocation (BESA) by applying a blockwise reconstruction loss. In contrast to the typical layer-wise pruning techniques, BESA is characterized by two distinctive attributes: i) it targets the overall pruning error with respect to individual transformer blocks, and ii) it allocates layer-specific sparsity in a differentiable manner, both of which ensure reduced performance degradation after pruning. Our experiments show that BESA achieves state-of-the-art performance, efficiently pruning LLMs like LLaMA1, and LLaMA2 with 7B to 70B parameters on a single A100 GPU in just five hours. Code is available at https://github.com/OpenGVLab/LLMPrune-BESA.\n",
            "reference_string": "[268032346 | Xu et al. | 2024 | Citations: 32]"
        },
        {
            "title": "Greedy Output Approximation: Towards Efficient Structured Pruning for LLMs Without Retraining",
            "venue": "arXiv.org",
            "year": 2024,
            "reference_count": 61,
            "citation_count": 6,
            "influential_citation_count": 1,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2407.19126, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2261150750",
                    "name": "Jianwei Li"
                },
                {
                    "authorId": "2310390224",
                    "name": "Yijun Dong"
                },
                {
                    "authorId": "2261081394",
                    "name": "Qi Lei"
                }
            ],
            "abstract": "To remove redundant components of large language models (LLMs) without incurring significant computational costs, this work focuses on single-shot pruning without a retraining phase. We simplify the pruning process for Transformer-based LLMs by identifying a depth-2 pruning structure that functions independently. Additionally, we propose two inference-aware pruning criteria derived from the optimization perspective of output approximation, which outperforms traditional training-aware metrics such as gradient and Hessian. We also introduce a two-step reconstruction technique to mitigate pruning errors without model retraining. Experimental results demonstrate that our approach significantly reduces computational costs and hardware requirements while maintaining superior performance across various datasets and models.",
            "corpus_id": 271533761,
            "sentences": [
                {
                    "corpus_id": "271533761",
                    "title": "Greedy Output Approximation: Towards Efficient Structured Pruning for LLMs Without Retraining",
                    "text": "To remove redundant components of large language models (LLMs) without incurring significant computational costs, this work focuses on single-shot pruning without a retraining phase. We simplify the pruning process for Transformer-based LLMs by identifying a depth-2 pruning structure that functions independently. Additionally, we propose two inference-aware pruning criteria derived from the optimization perspective of output approximation, which outperforms traditional training-aware metrics such as gradient and Hessian. We also introduce a two-step reconstruction technique to mitigate pruning errors without model retraining. Experimental results demonstrate that our approach significantly reduces computational costs and hardware requirements while maintaining superior performance across various datasets and models.",
                    "score": 0.7049720452485538,
                    "section_title": "abstract",
                    "char_start_offset": 0,
                    "sentence_offsets": [],
                    "ref_mentions": [],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.8359375
                },
                {
                    "corpus_id": "271533761",
                    "title": "Greedy Output Approximation: Towards Efficient Structured Pruning for LLMs Without Retraining",
                    "text": "This paper introduces a novel approach to pruning large language models (LLMs) by identifying a depth-2 pruning structure and developing two inference-aware pruning criteria. These methods surpass traditional metrics and eliminate the need for computationally expensive retraining. Our two-step reconstruction technique further mitigates pruning errors, ensuring superior performance across various datasets and models. Overall, our approach significantly reduces computational costs and hardware requirements, offering an efficient solution for pruning colossal LLMs.",
                    "score": 0.6663325115563953,
                    "section_title": "Conclusion:",
                    "char_start_offset": 29816,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 174
                        },
                        {
                            "start": 175,
                            "end": 281
                        },
                        {
                            "start": 282,
                            "end": 419
                        },
                        {
                            "start": 420,
                            "end": 568
                        }
                    ],
                    "ref_mentions": [],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.8173828125
                },
                {
                    "corpus_id": "271533761",
                    "title": "Greedy Output Approximation: Towards Efficient Structured Pruning for LLMs Without Retraining",
                    "text": "We apply this metric for both attention and feed-forward modules to remove the elements with minimal impact on the model's performance. Finally, we develop an optimization technique that eliminates the need for higher-order information by greedily reducing pruning error through weight reconstruction of the subsequent dense module. Our structured pruning experiments on pre-trained LLMs ranging from millions to billions of parameters demonstrate that our method ensures generalizability, hardware compatibility, and minimal pruning cost. Moreover, it outperforms or achieves comparative performance to other non-retraining methods and even some methods that require retraining.",
                    "score": 0.585804039501977,
                    "section_title": "Introduction",
                    "char_start_offset": 4189,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 135
                        },
                        {
                            "start": 136,
                            "end": 332
                        },
                        {
                            "start": 333,
                            "end": 539
                        },
                        {
                            "start": 540,
                            "end": 679
                        }
                    ],
                    "ref_mentions": [],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.80078125
                }
            ],
            "relevance_judgement": 0.8359375,
            "relevance_judgment_input_expanded": "# Title: Greedy Output Approximation: Towards Efficient Structured Pruning for LLMs Without Retraining\n# Venue: arXiv.org\n# Authors: Jianwei Li, Yijun Dong, Qi Lei\n## Abstract\nTo remove redundant components of large language models (LLMs) without incurring significant computational costs, this work focuses on single-shot pruning without a retraining phase. We simplify the pruning process for Transformer-based LLMs by identifying a depth-2 pruning structure that functions independently. Additionally, we propose two inference-aware pruning criteria derived from the optimization perspective of output approximation, which outperforms traditional training-aware metrics such as gradient and Hessian. We also introduce a two-step reconstruction technique to mitigate pruning errors without model retraining. Experimental results demonstrate that our approach significantly reduces computational costs and hardware requirements while maintaining superior performance across various datasets and models.\n## Introduction\nWe apply this metric for both attention and feed-forward modules to remove the elements with minimal impact on the model's performance. Finally, we develop an optimization technique that eliminates the need for higher-order information by greedily reducing pruning error through weight reconstruction of the subsequent dense module. Our structured pruning experiments on pre-trained LLMs ranging from millions to billions of parameters demonstrate that our method ensures generalizability, hardware compatibility, and minimal pruning cost. Moreover, it outperforms or achieves comparative performance to other non-retraining methods and even some methods that require retraining.\n\n## Conclusion:\nThis paper introduces a novel approach to pruning large language models (LLMs) by identifying a depth-2 pruning structure and developing two inference-aware pruning criteria. These methods surpass traditional metrics and eliminate the need for computationally expensive retraining. Our two-step reconstruction technique further mitigates pruning errors, ensuring superior performance across various datasets and models. Overall, our approach significantly reduces computational costs and hardware requirements, offering an efficient solution for pruning colossal LLMs.",
            "reference_string": "[271533761 | Li et al. | 2024 | Citations: 6]"
        },
        {
            "title": "BlockPruner: Fine-grained Pruning for Large Language Models",
            "venue": "arXiv.org",
            "year": 2024,
            "reference_count": 43,
            "citation_count": 10,
            "influential_citation_count": 3,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2406.10594, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2286975236",
                    "name": "Longguang Zhong"
                },
                {
                    "authorId": "2217614543",
                    "name": "Fanqi Wan"
                },
                {
                    "authorId": "2307325800",
                    "name": "Ruijun Chen"
                },
                {
                    "authorId": "2258552983",
                    "name": "Xiaojun Quan"
                },
                {
                    "authorId": "2257243199",
                    "name": "Liangzhi Li"
                }
            ],
            "abstract": "With the rapid growth in the size and complexity of large language models (LLMs), the costs associated with their training and inference have escalated significantly. Research indicates that certain layers in LLMs harbor substantial redundancy, and pruning these layers has minimal impact on the overall performance. While various layer pruning methods have been developed based on this insight, they generally overlook the finer-grained redundancies within the layers themselves. In this paper, we delve deeper into the architecture of LLMs and demonstrate that finer-grained pruning can be achieved by targeting redundancies in multi-head attention (MHA) and multi-layer perceptron (MLP) blocks. We propose a novel, training-free structured pruning approach called BlockPruner. Unlike existing layer pruning methods, BlockPruner segments each Transformer layer into MHA and MLP blocks. It then assesses the importance of these blocks using perplexity measures and applies a heuristic search for iterative pruning. We applied BlockPruner to LLMs of various sizes and architectures and validated its performance across a wide range of downstream tasks. Experimental results show that BlockPruner achieves more granular and effective pruning compared to state-of-the-art baselines.",
            "corpus_id": 270560879,
            "sentences": [
                {
                    "corpus_id": "270560879",
                    "title": "BlockPruner: Fine-grained Pruning for Large Language Models",
                    "text": "With the rapid growth in the size and complexity of large language models (LLMs), the costs associated with their training and inference have escalated significantly. Research indicates that certain layers in LLMs harbor substantial redundancy, and pruning these layers has minimal impact on the overall performance. While various layer pruning methods have been developed based on this insight, they generally overlook the finer-grained redundancies within the layers themselves. In this paper, we delve deeper into the architecture of LLMs and demonstrate that finer-grained pruning can be achieved by targeting redundancies in multi-head attention (MHA) and multi-layer perceptron (MLP) blocks. We propose a novel, training-free structured pruning approach called BlockPruner. Unlike existing layer pruning methods, BlockPruner segments each Transformer layer into MHA and MLP blocks. It then assesses the importance of these blocks using perplexity measures and applies a heuristic search for iterative pruning. We applied BlockPruner to LLMs of various sizes and architectures and validated its performance across a wide range of downstream tasks. Experimental results show that BlockPruner achieves more granular and effective pruning compared to state-of-the-art baselines.",
                    "score": 0.594842117166541,
                    "section_title": "abstract",
                    "char_start_offset": 0,
                    "sentence_offsets": [],
                    "ref_mentions": [],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.833984375
                }
            ],
            "relevance_judgement": 0.833984375,
            "relevance_judgment_input_expanded": "# Title: BlockPruner: Fine-grained Pruning for Large Language Models\n# Venue: arXiv.org\n# Authors: Longguang Zhong, Fanqi Wan, Ruijun Chen, Xiaojun Quan, Liangzhi Li\n## Abstract\nWith the rapid growth in the size and complexity of large language models (LLMs), the costs associated with their training and inference have escalated significantly. Research indicates that certain layers in LLMs harbor substantial redundancy, and pruning these layers has minimal impact on the overall performance. While various layer pruning methods have been developed based on this insight, they generally overlook the finer-grained redundancies within the layers themselves. In this paper, we delve deeper into the architecture of LLMs and demonstrate that finer-grained pruning can be achieved by targeting redundancies in multi-head attention (MHA) and multi-layer perceptron (MLP) blocks. We propose a novel, training-free structured pruning approach called BlockPruner. Unlike existing layer pruning methods, BlockPruner segments each Transformer layer into MHA and MLP blocks. It then assesses the importance of these blocks using perplexity measures and applies a heuristic search for iterative pruning. We applied BlockPruner to LLMs of various sizes and architectures and validated its performance across a wide range of downstream tasks. Experimental results show that BlockPruner achieves more granular and effective pruning compared to state-of-the-art baselines.\n",
            "reference_string": "[270560879 | Zhong et al. | 2024 | Citations: 10]"
        },
        {
            "title": "Efficient Shapley Value-based Non-Uniform Pruning of Large Language Models",
            "venue": "arXiv.org",
            "year": 2025,
            "reference_count": 44,
            "citation_count": 3,
            "influential_citation_count": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2505.01731, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2359207803",
                    "name": "Chuan Sun"
                },
                {
                    "authorId": "2148706587",
                    "name": "Han Yu"
                },
                {
                    "authorId": "2313694394",
                    "name": "Li-zhen Cui"
                },
                {
                    "authorId": "2283747425",
                    "name": "Xiaoxiao Li"
                }
            ],
            "abstract": "Pruning large language models (LLMs) is a promising solution for reducing model sizes and computational complexity while preserving performance. Traditional layer-wise pruning methods often adopt a uniform sparsity approach across all layers, which leads to suboptimal performance due to the varying significance of individual transformer layers within the model not being accounted for. To this end, we propose the Shapley Value-based Non-Uniform Pruning (SV-NUP) method for LLMs. This approach quantifies the contribution of each transformer layer to the overall model performance, enabling the assignment of tailored pruning budgets to different layers to retain critical parameters. To further improve efficiency, we design the Sliding Window-based Shapley Value approximation method. It substantially reduces computational overhead compared to exact SV calculation methods. Extensive experiments on various LLMs including LLaMA-v1, LLaMA-v2 and OPT demonstrate the effectiveness of the proposed approach. The results reveal that non-uniform pruning significantly enhances the performance of pruned models. Notably, SV-NUP achieves a reduction in perplexity (PPL) of 18.01% and 19.55% on LLaMA-7B and LLaMA-13B, respectively, compared to SparseGPT at 70% sparsity.",
            "corpus_id": 278327238,
            "sentences": [
                {
                    "corpus_id": "278327238",
                    "title": "Efficient Shapley Value-based Non-Uniform Pruning of Large Language Models",
                    "text": "Uniform Pruning. Traditional pruning requires a round of re-training to restore performance, which poses significant challenges for LLMs. Researchers have developed pruning algorithms specifically tailored for LLM compression. For instance, [9] investigated structured sparse LLMs by applying Taylor pruning to remove entire weight rows, followed by LoRA fine-tuning [13]. In recent years, the focus has shifted toward unstructured pruning which eliminates the need for fine-tuning. SparseGPT [4] employs the Hessian inverse for pruning, followed by weight updates to reduce reconstruction errors between dense and sparse weights. Wanda [14] introduced a criterion that incorporates weight magnitude and input activations to preserve outlier features. \n\nNon-uniform Pruning. Uniform layerwise sparsity is commonly used for pruning language models [36; 42], with several studies demonstrating its effectiveness in LLM pruning [43; 44]. However, there is a growing body of work exploring non-uniform layerwise sparsity, primarily in the context of vision models. For example, [15] proposed a non-uniform, scale-free topology inspired by graph theory, which outperforms dense counterparts when applied to restricted Boltzmann machines. Subsequent work has improved the scalability of this approach by leveraging Erd\u0151s-R\u00e9nyi graphs [16], extending the method to fully connected layers [11] and convolutional layers [17; 18] to achieve data-free and feedforward-free layerwise sparsity. Another approach to non-uniform sparsity involves applying a global threshold across all layers [19; 20; 21; 22]. However, global pruning has been found to be computationally expensive and ineffective when applied to LLMs. \n\nAnalyzing LLMs. The authors in [23] analyzed the contributions of various components in LLMs and their impact on overall performance. [24] explored the role of deep layers in LLMs through layer pruning, providing insights into how different layers in relation to model performance. [25] examined the redundancy of attention heads in transformer-based models, demonstrating that many attention heads can be pruned without significant performance degradation.",
                    "score": 0.7347879033213737,
                    "section_title": "A Extende Related Work",
                    "char_start_offset": 23991,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 16
                        },
                        {
                            "start": 17,
                            "end": 137
                        },
                        {
                            "start": 138,
                            "end": 226
                        },
                        {
                            "start": 227,
                            "end": 372
                        },
                        {
                            "start": 373,
                            "end": 482
                        },
                        {
                            "start": 483,
                            "end": 630
                        },
                        {
                            "start": 631,
                            "end": 751
                        },
                        {
                            "start": 754,
                            "end": 774
                        },
                        {
                            "start": 775,
                            "end": 934
                        },
                        {
                            "start": 935,
                            "end": 1060
                        },
                        {
                            "start": 1061,
                            "end": 1232
                        },
                        {
                            "start": 1233,
                            "end": 1481
                        },
                        {
                            "start": 1482,
                            "end": 1595
                        },
                        {
                            "start": 1596,
                            "end": 1704
                        },
                        {
                            "start": 1707,
                            "end": 1722
                        },
                        {
                            "start": 1723,
                            "end": 1840
                        },
                        {
                            "start": 1841,
                            "end": 1988
                        },
                        {
                            "start": 1989,
                            "end": 2164
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 241,
                            "end": 244,
                            "matchedPaperCorpusId": "258823276"
                        },
                        {
                            "start": 493,
                            "end": 496,
                            "matchedPaperCorpusId": "255372747"
                        },
                        {
                            "start": 637,
                            "end": 641,
                            "matchedPaperCorpusId": "259203115"
                        },
                        {
                            "start": 1074,
                            "end": 1078,
                            "matchedPaperCorpusId": "2090605"
                        },
                        {
                            "start": 1328,
                            "end": 1332,
                            "matchedPaperCorpusId": "4506156"
                        },
                        {
                            "start": 1381,
                            "end": 1385,
                            "matchedPaperCorpusId": "49310977"
                        },
                        {
                            "start": 1738,
                            "end": 1742,
                            "matchedPaperCorpusId": "251648872"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.82666015625
                }
            ],
            "relevance_judgement": 0.82666015625,
            "relevance_judgment_input_expanded": "# Title: Efficient Shapley Value-based Non-Uniform Pruning of Large Language Models\n# Venue: arXiv.org\n# Authors: Chuan Sun, Han Yu, Li-zhen Cui, Xiaoxiao Li\n## Abstract\nPruning large language models (LLMs) is a promising solution for reducing model sizes and computational complexity while preserving performance. Traditional layer-wise pruning methods often adopt a uniform sparsity approach across all layers, which leads to suboptimal performance due to the varying significance of individual transformer layers within the model not being accounted for. To this end, we propose the Shapley Value-based Non-Uniform Pruning (SV-NUP) method for LLMs. This approach quantifies the contribution of each transformer layer to the overall model performance, enabling the assignment of tailored pruning budgets to different layers to retain critical parameters. To further improve efficiency, we design the Sliding Window-based Shapley Value approximation method. It substantially reduces computational overhead compared to exact SV calculation methods. Extensive experiments on various LLMs including LLaMA-v1, LLaMA-v2 and OPT demonstrate the effectiveness of the proposed approach. The results reveal that non-uniform pruning significantly enhances the performance of pruned models. Notably, SV-NUP achieves a reduction in perplexity (PPL) of 18.01% and 19.55% on LLaMA-7B and LLaMA-13B, respectively, compared to SparseGPT at 70% sparsity.\n## A Extende Related Work\nUniform Pruning. Traditional pruning requires a round of re-training to restore performance, which poses significant challenges for LLMs. Researchers have developed pruning algorithms specifically tailored for LLM compression. For instance, [9] investigated structured sparse LLMs by applying Taylor pruning to remove entire weight rows, followed by LoRA fine-tuning [13]. In recent years, the focus has shifted toward unstructured pruning which eliminates the need for fine-tuning. SparseGPT [4] employs the Hessian inverse for pruning, followed by weight updates to reduce reconstruction errors between dense and sparse weights. Wanda [14] introduced a criterion that incorporates weight magnitude and input activations to preserve outlier features. \n\nNon-uniform Pruning. Uniform layerwise sparsity is commonly used for pruning language models [36; 42], with several studies demonstrating its effectiveness in LLM pruning [43; 44]. However, there is a growing body of work exploring non-uniform layerwise sparsity, primarily in the context of vision models. For example, [15] proposed a non-uniform, scale-free topology inspired by graph theory, which outperforms dense counterparts when applied to restricted Boltzmann machines. Subsequent work has improved the scalability of this approach by leveraging Erd\u0151s-R\u00e9nyi graphs [16], extending the method to fully connected layers [11] and convolutional layers [17; 18] to achieve data-free and feedforward-free layerwise sparsity. Another approach to non-uniform sparsity involves applying a global threshold across all layers [19; 20; 21; 22]. However, global pruning has been found to be computationally expensive and ineffective when applied to LLMs. \n\nAnalyzing LLMs. The authors in [23] analyzed the contributions of various components in LLMs and their impact on overall performance. [24] explored the role of deep layers in LLMs through layer pruning, providing insights into how different layers in relation to model performance. [25] examined the redundancy of attention heads in transformer-based models, demonstrating that many attention heads can be pruned without significant performance degradation.",
            "reference_string": "[278327238 | Sun et al. | 2025 | Citations: 3]"
        },
        {
            "title": "SparseLLM: Towards Global Pruning of Pre-trained Language Models",
            "venue": "Neural Information Processing Systems",
            "year": 2024,
            "reference_count": 40,
            "citation_count": 11,
            "influential_citation_count": 2,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2402.17946, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "7583867",
                    "name": "Guangji Bai"
                },
                {
                    "authorId": "2288037157",
                    "name": "Yijiang Li"
                },
                {
                    "authorId": "2284591355",
                    "name": "Chen Ling"
                },
                {
                    "authorId": "2288023827",
                    "name": "Kibaek Kim"
                },
                {
                    "authorId": "2284637383",
                    "name": "Liang Zhao"
                }
            ],
            "abstract": "The transformative impact of large language models (LLMs) like LLaMA and GPT on natural language processing is countered by their prohibitive computational demands. Pruning has emerged as a pivotal compression strategy, introducing sparsity to enhance both memory and computational efficiency. Yet, traditional global pruning is impractical for LLMs due to scalability issues, while local pruning, despite its efficiency, leads to suboptimal solutions. Addressing these challenges, we propose SparseLLM, a novel framework that redefines the global pruning process into manageable, coordinated subproblems, allowing for resource-efficient optimization with global optimality. SparseLLM's approach, which conceptualizes LLMs as a chain of modular functions and leverages auxiliary variables for problem decomposition, not only facilitates a pragmatic application on LLMs but also demonstrates significant performance improvements, particularly in high-sparsity regimes where it surpasses current state-of-the-art methods.",
            "corpus_id": 268041812,
            "sentences": [
                {
                    "corpus_id": "268041812",
                    "title": "SparseLLM: Towards Global Pruning of Pre-trained Language Models",
                    "text": "Li et al. Li et al. [2023] proposed LoSparse, a novel approach combining low-rank and sparse matrix approximations to balance pruning and expressive power. Tao et al. Tao et al. [2023] extended this concept to pruning hidden dimensions in LLMs, including embedding layers and attention heads. \n\nZipLM Kurtic et al. [2023], a structured pruning method for LLMs, is proposed to optimize for compression and accuracy while considering specific hardware constraints. More recently, Xia et al introduced LLM-shearing Xia et al. [2023], a structured pruning method that scales down LLaMA models by selectively pruning layers, heads, and dimensions. This approach, combined with dynamic data batching, reduces pre-training compute costs while maintaining competitive performance, outperforming similar open-source models on key tasks. \n\nOur work falls in the category of unstructured pruning of LLMs, where existing methods such as SparseGPT and Wanda only consider an entirely local pruning algorithm and suffer from suboptimal performance. We discuss the limitations and challenges of entirely local pruning in Sec. 3.",
                    "score": 0.7161138588476132,
                    "section_title": "Related work",
                    "char_start_offset": 5848,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 155
                        },
                        {
                            "start": 156,
                            "end": 292
                        },
                        {
                            "start": 295,
                            "end": 462
                        },
                        {
                            "start": 463,
                            "end": 642
                        },
                        {
                            "start": 643,
                            "end": 827
                        },
                        {
                            "start": 830,
                            "end": 1034
                        },
                        {
                            "start": 1035,
                            "end": 1113
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 156,
                            "end": 184,
                            "matchedPaperCorpusId": "259858812"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.826171875
                },
                {
                    "corpus_id": "268041812",
                    "title": "SparseLLM: Towards Global Pruning of Pre-trained Language Models",
                    "text": "The transformative impact of large language models (LLMs) like LLaMA and GPT on natural language processing is countered by their prohibitive computational demands. Pruning has emerged as a pivotal compression strategy, introducing sparsity to enhance both memory and computational efficiency. Yet, traditional global pruning is impractical for LLMs due to scalability issues, while local pruning, despite its efficiency, leads to suboptimal solutions. Addressing these challenges, we propose SparseLLM, a novel framework that redefines the global pruning process into manageable, coordinated subproblems, allowing for resource-efficient optimization with global optimality. SparseLLM's approach, which conceptualizes LLMs as a chain of modular functions and leverages auxiliary variables for problem decomposition, not only facilitates a pragmatic application on LLMs but also demonstrates significant performance improvements, particularly in high-sparsity regimes where it surpasses current state-of-the-art methods.",
                    "score": 0.6236483555778671,
                    "section_title": "abstract",
                    "char_start_offset": 0,
                    "sentence_offsets": [],
                    "ref_mentions": [],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.81591796875
                },
                {
                    "corpus_id": "268041812",
                    "title": "SparseLLM: Towards Global Pruning of Pre-trained Language Models",
                    "text": "Large language models (LLMs) Touvron et al. [2023], OpenAI [2023] have recently transformed the field of natural language processing (NLP) by delivering exceptional results across a variety of intricate language benchmarks Wei et al. [2022], Bommarito II and Katz [2022], Bubeck et al. [2023]. Nonetheless, these models, with billions of parameters, generally necessitate significant computational resources. To make LLMs more accessible, extensive efforts have been devoted to model compression of LLMs Xu and McAuley [2023], Bai et al. [2024a], including pruning, quantization, knowledge distillation, and low-rank factorization. Pruning, by introducing sparsity, jointly enhances memory and computational efficiency and offers unparalleled flexibility, seamlessly integrating with any LLMs, thus standing out as a highly effective and widely adopted compression strategy. \n\nModel pruning has a long history LeCun et al. [1989] and has proven effective in applications related to vision and smaller language models Hoefler et al. [2021]. However, conventional pruning techniques, which rely on global pruning and require loading the entire model into the same GPU Mallya and Lazebnik [2018], Singh and Alistarh [2020], become impractical for today's LLMs due to their vast size. Recently, several local pruning methods have been proposed for billion-scale LLMs. These methods compress each layer separately, and the overall compressed model is then obtained by \"stitching together\" the individually compressed layers. SparseGPT Frantar and Alistarh [2023], an efficient unstructured pruning method for LLMs with hundreds of billions of parameters, achieves parameter reduction of up to 60% with minimal performance loss. Another approach, Wanda Sun et al. [2023], introduces a novel pruning criterion that evaluates weights by considering both magnitude 38th Conference on Neural Information Processing Systems (NeurIPS 2024). arXiv:2402.17946v4 [cs.CL] 31 Oct 2024 and related input activations.",
                    "score": 0.8067699911616415,
                    "section_title": "Introduction",
                    "char_start_offset": 15,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 293
                        },
                        {
                            "start": 294,
                            "end": 408
                        },
                        {
                            "start": 409,
                            "end": 631
                        },
                        {
                            "start": 632,
                            "end": 874
                        },
                        {
                            "start": 877,
                            "end": 1039
                        },
                        {
                            "start": 1040,
                            "end": 1280
                        },
                        {
                            "start": 1281,
                            "end": 1363
                        },
                        {
                            "start": 1364,
                            "end": 1519
                        },
                        {
                            "start": 1520,
                            "end": 1722
                        },
                        {
                            "start": 1723,
                            "end": 1928
                        },
                        {
                            "start": 1929,
                            "end": 1947
                        },
                        {
                            "start": 1948,
                            "end": 1998
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 504,
                            "end": 525,
                            "matchedPaperCorpusId": "254069544"
                        },
                        {
                            "start": 910,
                            "end": 929,
                            "matchedPaperCorpusId": "7785881"
                        },
                        {
                            "start": 1017,
                            "end": 1038,
                            "matchedPaperCorpusId": "231740691"
                        },
                        {
                            "start": 1166,
                            "end": 1192,
                            "matchedPaperCorpusId": "35249701"
                        },
                        {
                            "start": 1194,
                            "end": 1219,
                            "matchedPaperCorpusId": "220364055"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.80859375
                },
                {
                    "corpus_id": "268041812",
                    "title": "SparseLLM: Towards Global Pruning of Pre-trained Language Models",
                    "text": "Local Pruning SparseLLM (Ours) To address these challenges and achieve global pruning with low memory consumption, we propose SparseLLM that decomposes the global pruning objective into multiple subproblems, each of which can be solved with low resources and coordinate to achieve the global pruning objective. More specifically, we first formulate LLMs as a composite function where the output of one module is the input of the next. Based on this formulation, we reformulate the global pruning goal into an equivalent form with auxiliary variables that facilitate its decomposition and coordination of the subproblems. Then we propose an alternating optimization algorithm to efficiently solve the subproblems, achieving computational resource efficiency and global optimality, due to the close-form solution of each subproblem. Empirically, we find that SparseLLM can consistently improve the performance of local pruning methods, particularly in high sparsity regimes (> 60%), where the perplexity can be significantly decreased by up to around 80% as compared to the state-of-the-art methods. \n\nFurthermore, our SparseLLM framework can be readily applicable to enhance the performance of most existing local pruning solvers, such as SparseGPT and Wanda, with marginal additional computational overhead. This adaptability ensures that our framework can be seamlessly integrated into a wide range of LLMs and pruning methods, making it a versatile tool and useful baseline for future research exploiting the sparsity of LLMs.",
                    "score": 0.6163416240993027,
                    "section_title": "Global Pruning",
                    "char_start_offset": 2422,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 310
                        },
                        {
                            "start": 311,
                            "end": 434
                        },
                        {
                            "start": 435,
                            "end": 620
                        },
                        {
                            "start": 621,
                            "end": 830
                        },
                        {
                            "start": 831,
                            "end": 1097
                        },
                        {
                            "start": 1100,
                            "end": 1307
                        },
                        {
                            "start": 1308,
                            "end": 1528
                        }
                    ],
                    "ref_mentions": [],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.8017578125
                }
            ],
            "relevance_judgement": 0.826171875,
            "relevance_judgment_input_expanded": "# Title: SparseLLM: Towards Global Pruning of Pre-trained Language Models\n# Venue: Neural Information Processing Systems\n# Authors: Guangji Bai, Yijiang Li, Chen Ling, Kibaek Kim, Liang Zhao\n## Abstract\nThe transformative impact of large language models (LLMs) like LLaMA and GPT on natural language processing is countered by their prohibitive computational demands. Pruning has emerged as a pivotal compression strategy, introducing sparsity to enhance both memory and computational efficiency. Yet, traditional global pruning is impractical for LLMs due to scalability issues, while local pruning, despite its efficiency, leads to suboptimal solutions. Addressing these challenges, we propose SparseLLM, a novel framework that redefines the global pruning process into manageable, coordinated subproblems, allowing for resource-efficient optimization with global optimality. SparseLLM's approach, which conceptualizes LLMs as a chain of modular functions and leverages auxiliary variables for problem decomposition, not only facilitates a pragmatic application on LLMs but also demonstrates significant performance improvements, particularly in high-sparsity regimes where it surpasses current state-of-the-art methods.\n## Introduction\nLarge language models (LLMs) Touvron et al. [2023], OpenAI [2023] have recently transformed the field of natural language processing (NLP) by delivering exceptional results across a variety of intricate language benchmarks Wei et al. [2022], Bommarito II and Katz [2022], Bubeck et al. [2023]. Nonetheless, these models, with billions of parameters, generally necessitate significant computational resources. To make LLMs more accessible, extensive efforts have been devoted to model compression of LLMs Xu and McAuley [2023], Bai et al. [2024a], including pruning, quantization, knowledge distillation, and low-rank factorization. Pruning, by introducing sparsity, jointly enhances memory and computational efficiency and offers unparalleled flexibility, seamlessly integrating with any LLMs, thus standing out as a highly effective and widely adopted compression strategy. \n\nModel pruning has a long history LeCun et al. [1989] and has proven effective in applications related to vision and smaller language models Hoefler et al. [2021]. However, conventional pruning techniques, which rely on global pruning and require loading the entire model into the same GPU Mallya and Lazebnik [2018], Singh and Alistarh [2020], become impractical for today's LLMs due to their vast size. Recently, several local pruning methods have been proposed for billion-scale LLMs. These methods compress each layer separately, and the overall compressed model is then obtained by \"stitching together\" the individually compressed layers. SparseGPT Frantar and Alistarh [2023], an efficient unstructured pruning method for LLMs with hundreds of billions of parameters, achieves parameter reduction of up to 60% with minimal performance loss. Another approach, Wanda Sun et al. [2023], introduces a novel pruning criterion that evaluates weights by considering both magnitude 38th Conference on Neural Information Processing Systems (NeurIPS 2024). arXiv:2402.17946v4 [cs.CL] 31 Oct 2024 and related input activations.\n\n## Global Pruning\nLocal Pruning SparseLLM (Ours) To address these challenges and achieve global pruning with low memory consumption, we propose SparseLLM that decomposes the global pruning objective into multiple subproblems, each of which can be solved with low resources and coordinate to achieve the global pruning objective. More specifically, we first formulate LLMs as a composite function where the output of one module is the input of the next. Based on this formulation, we reformulate the global pruning goal into an equivalent form with auxiliary variables that facilitate its decomposition and coordination of the subproblems. Then we propose an alternating optimization algorithm to efficiently solve the subproblems, achieving computational resource efficiency and global optimality, due to the close-form solution of each subproblem. Empirically, we find that SparseLLM can consistently improve the performance of local pruning methods, particularly in high sparsity regimes (> 60%), where the perplexity can be significantly decreased by up to around 80% as compared to the state-of-the-art methods. \n\nFurthermore, our SparseLLM framework can be readily applicable to enhance the performance of most existing local pruning solvers, such as SparseGPT and Wanda, with marginal additional computational overhead. This adaptability ensures that our framework can be seamlessly integrated into a wide range of LLMs and pruning methods, making it a versatile tool and useful baseline for future research exploiting the sparsity of LLMs.\n\n## Related work\nLi et al. Li et al. [2023] proposed LoSparse, a novel approach combining low-rank and sparse matrix approximations to balance pruning and expressive power. Tao et al. Tao et al. [2023] extended this concept to pruning hidden dimensions in LLMs, including embedding layers and attention heads. \n\nZipLM Kurtic et al. [2023], a structured pruning method for LLMs, is proposed to optimize for compression and accuracy while considering specific hardware constraints. More recently, Xia et al introduced LLM-shearing Xia et al. [2023], a structured pruning method that scales down LLaMA models by selectively pruning layers, heads, and dimensions. This approach, combined with dynamic data batching, reduces pre-training compute costs while maintaining competitive performance, outperforming similar open-source models on key tasks. \n\nOur work falls in the category of unstructured pruning of LLMs, where existing methods such as SparseGPT and Wanda only consider an entirely local pruning algorithm and suffer from suboptimal performance. We discuss the limitations and challenges of entirely local pruning in Sec. 3.",
            "reference_string": "[268041812 | Bai et al. | 2024 | Citations: 11]"
        },
        {
            "title": "Pruning Foundation Models for High Accuracy without Retraining",
            "venue": "Conference on Empirical Methods in Natural Language Processing",
            "year": 2024,
            "reference_count": 35,
            "citation_count": 13,
            "influential_citation_count": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2410.15567, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2241612245",
                    "name": "Pu Zhao"
                },
                {
                    "authorId": "2327046378",
                    "name": "Fei Sun"
                },
                {
                    "authorId": "2007668856",
                    "name": "Xuan Shen"
                },
                {
                    "authorId": "2241698013",
                    "name": "Pinrui Yu"
                },
                {
                    "authorId": "32409528",
                    "name": "Zhenglun Kong"
                },
                {
                    "authorId": "2290628977",
                    "name": "Yanzhi Wang"
                },
                {
                    "authorId": "2322988586",
                    "name": "Xue Lin"
                }
            ],
            "abstract": "Despite the superior performance, it is challenging to deploy foundation models or large language models (LLMs) due to their massive parameters and computations. While pruning is a promising technique to reduce model size and accelerate the inference, the traditional pruning techniques can hardly be applied for LLMs as they need to finetune the model on the full dataset with multiple epochs consuming massive data and hardware resources. To deal with this problem, post-training pruning methods are proposed to prune LLMs in one-shot without retraining. However, their accuracy after pruning may suffer from certain performance degradation due to the lack of retraining with massive data. To address this issue, in this paper, we first formulate the post-training problem for layer-wise LLM compression to simultaneously prune multiple weights in LLMs. Next, we provide an optimal solution for this problem and design our post-training pruning algorithm for both unstructured and semi-structured sparsity. Our extensive experiments demonstrate the superior performance of the proposed methods in comparison to SOTA baselines across various LLM families including transformer-based LLMs and Mamba-based LLMs. Code link: https://github.com/piuzha/APT",
            "corpus_id": 273501976,
            "sentences": [
                {
                    "corpus_id": "273501976",
                    "title": "Pruning Foundation Models for High Accuracy without Retraining",
                    "text": "Foundation models or large language models (LLMs) have achieved remarkable performance on a variety of tasks. However, it is challenging to deploy LLMs in practical applications due to their massive parameters and computations. To facilitate LLM deployment in practice, various model compression techniques targeting LLMs including pruning (Hubara et al., 2021b;Frantar and Alistarh, 2023) and quantization (Dettmers et al., 2022;Frantar et al., 2022;Yao et al., 2022;Xiao et al., 2023) have been proposed to reduce memory and computation costs. \n\nThe traditional pruning techniques, which finetune or retrain models (Li et al., 2020) on full datasets for many epochs (i.e., pruning-aware training), are too expensive for LLMs in terms of data and GPU resources. Thus, post-training pruning based on well-pre-trained models with reduced resource requirements represents a more reasonable approach for LLMs. Notably, SparseGPT (Frantar and Alistarh, 2023) is the representative post-training pruning work with outstanding performance. It reduces memory cost by sequentially loading transformer blocks, one at a time, instead of loading the whole model. Moreover, it reduces the data cost by using only a small amount of calibration data, eliminating the retraining process on massive data. Besides the optimization based SparseGPT, there are some other heuristic posttraining pruning methods such as (Sun et al., 2023;Zhang et al., 2024), achieving accuracy close to SparseGPT. \n\nHowever, the performance of SparseGPT is still sub-optimal as it relies on the solution of Single Removal Problem (SRP) (Singh and Alistarh, 2020;Frantar et al., 2021) to address the pruning of multiple weights, which is essentially a Multiple Removal Problem (MRP). In particular, the SRP provides the optimal solution to prune one weight at a time and modify all other weights to compensate the pruned single weight and minimize the loss.",
                    "score": 0.7015086770999334,
                    "section_title": "Introduction",
                    "char_start_offset": 15,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 109
                        },
                        {
                            "start": 110,
                            "end": 227
                        },
                        {
                            "start": 228,
                            "end": 545
                        },
                        {
                            "start": 548,
                            "end": 762
                        },
                        {
                            "start": 763,
                            "end": 906
                        },
                        {
                            "start": 907,
                            "end": 1033
                        },
                        {
                            "start": 1034,
                            "end": 1151
                        },
                        {
                            "start": 1152,
                            "end": 1288
                        },
                        {
                            "start": 1289,
                            "end": 1476
                        },
                        {
                            "start": 1479,
                            "end": 1745
                        },
                        {
                            "start": 1746,
                            "end": 1919
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 340,
                            "end": 362,
                            "matchedPaperCorpusId": "235825979"
                        },
                        {
                            "start": 362,
                            "end": 389,
                            "matchedPaperCorpusId": "255372747"
                        },
                        {
                            "start": 430,
                            "end": 451,
                            "matchedPaperCorpusId": "251765570"
                        },
                        {
                            "start": 451,
                            "end": 468,
                            "matchedPaperCorpusId": "249395624"
                        },
                        {
                            "start": 468,
                            "end": 486,
                            "matchedPaperCorpusId": "253708271"
                        },
                        {
                            "start": 617,
                            "end": 634,
                            "matchedPaperCorpusId": "221761597"
                        },
                        {
                            "start": 926,
                            "end": 954,
                            "matchedPaperCorpusId": "255372747"
                        },
                        {
                            "start": 1599,
                            "end": 1625,
                            "matchedPaperCorpusId": "220364055"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.826171875
                }
            ],
            "relevance_judgement": 0.826171875,
            "relevance_judgment_input_expanded": "# Title: Pruning Foundation Models for High Accuracy without Retraining\n# Venue: Conference on Empirical Methods in Natural Language Processing\n# Authors: Pu Zhao, Fei Sun, Xuan Shen, Pinrui Yu, Zhenglun Kong, Yanzhi Wang, Xue Lin\n## Abstract\nDespite the superior performance, it is challenging to deploy foundation models or large language models (LLMs) due to their massive parameters and computations. While pruning is a promising technique to reduce model size and accelerate the inference, the traditional pruning techniques can hardly be applied for LLMs as they need to finetune the model on the full dataset with multiple epochs consuming massive data and hardware resources. To deal with this problem, post-training pruning methods are proposed to prune LLMs in one-shot without retraining. However, their accuracy after pruning may suffer from certain performance degradation due to the lack of retraining with massive data. To address this issue, in this paper, we first formulate the post-training problem for layer-wise LLM compression to simultaneously prune multiple weights in LLMs. Next, we provide an optimal solution for this problem and design our post-training pruning algorithm for both unstructured and semi-structured sparsity. Our extensive experiments demonstrate the superior performance of the proposed methods in comparison to SOTA baselines across various LLM families including transformer-based LLMs and Mamba-based LLMs. Code link: https://github.com/piuzha/APT\n## Introduction\nFoundation models or large language models (LLMs) have achieved remarkable performance on a variety of tasks. However, it is challenging to deploy LLMs in practical applications due to their massive parameters and computations. To facilitate LLM deployment in practice, various model compression techniques targeting LLMs including pruning (Hubara et al., 2021b;Frantar and Alistarh, 2023) and quantization (Dettmers et al., 2022;Frantar et al., 2022;Yao et al., 2022;Xiao et al., 2023) have been proposed to reduce memory and computation costs. \n\nThe traditional pruning techniques, which finetune or retrain models (Li et al., 2020) on full datasets for many epochs (i.e., pruning-aware training), are too expensive for LLMs in terms of data and GPU resources. Thus, post-training pruning based on well-pre-trained models with reduced resource requirements represents a more reasonable approach for LLMs. Notably, SparseGPT (Frantar and Alistarh, 2023) is the representative post-training pruning work with outstanding performance. It reduces memory cost by sequentially loading transformer blocks, one at a time, instead of loading the whole model. Moreover, it reduces the data cost by using only a small amount of calibration data, eliminating the retraining process on massive data. Besides the optimization based SparseGPT, there are some other heuristic posttraining pruning methods such as (Sun et al., 2023;Zhang et al., 2024), achieving accuracy close to SparseGPT. \n\nHowever, the performance of SparseGPT is still sub-optimal as it relies on the solution of Single Removal Problem (SRP) (Singh and Alistarh, 2020;Frantar et al., 2021) to address the pruning of multiple weights, which is essentially a Multiple Removal Problem (MRP). In particular, the SRP provides the optimal solution to prune one weight at a time and modify all other weights to compensate the pruned single weight and minimize the loss.",
            "reference_string": "[273501976 | Zhao et al. | 2024 | Citations: 13]"
        },
        {
            "title": "MoE-I\u00b2: Compressing Mixture of Experts Models through Inter-Expert Pruning and Intra-Expert Low-Rank Decomposition",
            "venue": "Conference on Empirical Methods in Natural Language Processing",
            "year": 2024,
            "reference_count": 38,
            "citation_count": 7,
            "influential_citation_count": 0,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "http://arxiv.org/pdf/2411.01016",
                "status": "GREEN",
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2411.01016, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2329224758",
                    "name": "Cheng Yang"
                },
                {
                    "authorId": "2117517225",
                    "name": "Yang Sui"
                },
                {
                    "authorId": "2196307128",
                    "name": "Jinqi Xiao"
                },
                {
                    "authorId": "2152279863",
                    "name": "Lingyi Huang"
                },
                {
                    "authorId": "2168502148",
                    "name": "Yu Gong"
                },
                {
                    "authorId": "2329727093",
                    "name": "Yuanlin Duan"
                },
                {
                    "authorId": "2297818320",
                    "name": "Wenqi Jia"
                },
                {
                    "authorId": "1471722186",
                    "name": "Miao Yin"
                },
                {
                    "authorId": "2329746797",
                    "name": "Yu Cheng"
                },
                {
                    "authorId": "2241581494",
                    "name": "Bo Yuan"
                }
            ],
            "abstract": "The emergence of Mixture of Experts (MoE) LLMs has significantly advanced the development of language models. Compared to traditional LLMs, MoE LLMs outperform traditional LLMs by achieving higher performance with considerably fewer activated parameters. Despite this efficiency, their enormous parameter size still leads to high deployment costs. In this paper, we introduce a two-stage compression method tailored for MoE to reduce the model size and decrease the computational cost. First, in the inter-expert pruning stage, we analyze the importance of each layer and propose the Layer-wise Genetic Search and Block-wise KT-Reception Field with the non-uniform pruning ratio to prune the individual expert. Second, in the intra-expert decomposition stage, we apply the low-rank decomposition to further compress the parameters within the remaining experts. Extensive experiments on Qwen1.5-MoE-A2.7B, DeepSeek-V2-Lite, and Mixtral-8$\\times$7B demonstrate that our proposed methods can both reduce the model size and enhance inference efficiency while maintaining performance in various zero-shot tasks. The code will be available at \\url{https://github.com/xiaochengsky/MoEI-2.git}",
            "corpus_id": 273811289,
            "sentences": [
                {
                    "corpus_id": "273811289",
                    "title": "MoE-I\u00b2: Compressing Mixture of Experts Models through Inter-Expert Pruning and Intra-Expert Low-Rank Decomposition",
                    "text": "Recent advancements in large language models have underscored the need to reduce parameter sizes and latency (Ma et al., 2023). Compression techniques for language models include network pruning (Xu et al., 2021), knowledge distillation (Sun et al., 2019(Sun et al., , 2020)), quantization (Yao et al., 2022), decomposition (Hsu et al., 2022;Yuan et al., 2023;Wang et al., 2024), and methods like early exit (Xin et al., 2020). Building on these techniques, pruning, and sparsity is crucial for MoE models, which often have up to 95% of parameters dedicated to experts. Pruning MoE models involves removing less important experts or neurons to reduce the number of active parameters during inference. For example, (Kim et al., 2021) retains the most activated experts to enhance machine translation MoE models, while (Koishekenov et al., 2022) introduces gate statistics-based pruning during decoding. Although effective, these methods are mostly confined to linguistic models in machine translation. (Chen et al., 2022) dropping-while-training approach progressively removes non-essential experts for specific tasks, tested on Switch Transformers (Fedus et al., 2022). The merge-compression (Li et al., 2024) method and EPP (Lu et al., 2024) approach, which is similar to ours, consider pruning and skipping in MoE models but face challenges in reducing computational costs. Given a pruned or sparse model, finetuning aims to restore performance on original tasks. Recent studies on LLMs (Sun et al., 2023;Ma et al., 2023) focus on pruning linear layers, but these methods often fail to reduce computing costs without specialized hardware or libraries. Efficient post-finetuning expert pruning and sparsity methods for task-agnostic MoE LLMs remain underexplored. This gap highlights the need for advanced techniques to effectively balance pruning and sparsity while maintaining or enhancing performance across various tasks.",
                    "score": 0.6702382104817418,
                    "section_title": "Compression on MoE LLMs",
                    "char_start_offset": 6022,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 127
                        },
                        {
                            "start": 128,
                            "end": 427
                        },
                        {
                            "start": 428,
                            "end": 569
                        },
                        {
                            "start": 570,
                            "end": 700
                        },
                        {
                            "start": 701,
                            "end": 901
                        },
                        {
                            "start": 902,
                            "end": 1000
                        },
                        {
                            "start": 1001,
                            "end": 1169
                        },
                        {
                            "start": 1170,
                            "end": 1375
                        },
                        {
                            "start": 1376,
                            "end": 1465
                        },
                        {
                            "start": 1466,
                            "end": 1653
                        },
                        {
                            "start": 1654,
                            "end": 1764
                        },
                        {
                            "start": 1765,
                            "end": 1926
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 109,
                            "end": 126,
                            "matchedPaperCorpusId": "258823276"
                        },
                        {
                            "start": 290,
                            "end": 308,
                            "matchedPaperCorpusId": "249395624"
                        },
                        {
                            "start": 1148,
                            "end": 1168,
                            "matchedPaperCorpusId": "231573431"
                        },
                        {
                            "start": 1192,
                            "end": 1209,
                            "matchedPaperCorpusId": "263605809"
                        },
                        {
                            "start": 1507,
                            "end": 1523,
                            "matchedPaperCorpusId": "258823276"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.82470703125
                }
            ],
            "relevance_judgement": 0.82470703125,
            "relevance_judgment_input_expanded": "# Title: MoE-I\u00b2: Compressing Mixture of Experts Models through Inter-Expert Pruning and Intra-Expert Low-Rank Decomposition\n# Venue: Conference on Empirical Methods in Natural Language Processing\n# Authors: Cheng Yang, Yang Sui, Jinqi Xiao, Lingyi Huang, Yu Gong, Yuanlin Duan, Wenqi Jia, Miao Yin, Yu Cheng, Bo Yuan\n## Abstract\nThe emergence of Mixture of Experts (MoE) LLMs has significantly advanced the development of language models. Compared to traditional LLMs, MoE LLMs outperform traditional LLMs by achieving higher performance with considerably fewer activated parameters. Despite this efficiency, their enormous parameter size still leads to high deployment costs. In this paper, we introduce a two-stage compression method tailored for MoE to reduce the model size and decrease the computational cost. First, in the inter-expert pruning stage, we analyze the importance of each layer and propose the Layer-wise Genetic Search and Block-wise KT-Reception Field with the non-uniform pruning ratio to prune the individual expert. Second, in the intra-expert decomposition stage, we apply the low-rank decomposition to further compress the parameters within the remaining experts. Extensive experiments on Qwen1.5-MoE-A2.7B, DeepSeek-V2-Lite, and Mixtral-8$\\times$7B demonstrate that our proposed methods can both reduce the model size and enhance inference efficiency while maintaining performance in various zero-shot tasks. The code will be available at \\url{https://github.com/xiaochengsky/MoEI-2.git}\n## Compression on MoE LLMs\nRecent advancements in large language models have underscored the need to reduce parameter sizes and latency (Ma et al., 2023). Compression techniques for language models include network pruning (Xu et al., 2021), knowledge distillation (Sun et al., 2019(Sun et al., , 2020)), quantization (Yao et al., 2022), decomposition (Hsu et al., 2022;Yuan et al., 2023;Wang et al., 2024), and methods like early exit (Xin et al., 2020). Building on these techniques, pruning, and sparsity is crucial for MoE models, which often have up to 95% of parameters dedicated to experts. Pruning MoE models involves removing less important experts or neurons to reduce the number of active parameters during inference. For example, (Kim et al., 2021) retains the most activated experts to enhance machine translation MoE models, while (Koishekenov et al., 2022) introduces gate statistics-based pruning during decoding. Although effective, these methods are mostly confined to linguistic models in machine translation. (Chen et al., 2022) dropping-while-training approach progressively removes non-essential experts for specific tasks, tested on Switch Transformers (Fedus et al., 2022). The merge-compression (Li et al., 2024) method and EPP (Lu et al., 2024) approach, which is similar to ours, consider pruning and skipping in MoE models but face challenges in reducing computational costs. Given a pruned or sparse model, finetuning aims to restore performance on original tasks. Recent studies on LLMs (Sun et al., 2023;Ma et al., 2023) focus on pruning linear layers, but these methods often fail to reduce computing costs without specialized hardware or libraries. Efficient post-finetuning expert pruning and sparsity methods for task-agnostic MoE LLMs remain underexplored. This gap highlights the need for advanced techniques to effectively balance pruning and sparsity while maintaining or enhancing performance across various tasks.",
            "reference_string": "[273811289 | Yang et al. | 2024 | Citations: 7]"
        },
        {
            "title": "A Survey on Model Compression for Large Language Models",
            "venue": "Transactions of the Association for Computational Linguistics",
            "year": 2023,
            "reference_count": 201,
            "citation_count": 229,
            "influential_citation_count": 12,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2308.07633, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2205622982",
                    "name": "Xunyu Zhu"
                },
                {
                    "authorId": "153154515",
                    "name": "Jian Li"
                },
                {
                    "authorId": "2144384857",
                    "name": "Yong Liu"
                },
                {
                    "authorId": "2112563365",
                    "name": "Can Ma"
                },
                {
                    "authorId": "2154491572",
                    "name": "Weiping Wang"
                }
            ],
            "abstract": "Abstract Large Language Models (LLMs) have transformed natural language processing tasks successfully. Yet, their large size and high computational needs pose challenges for practical use, especially in resource-limited settings. Model compression has emerged as a key research area to address these challenges. This paper presents a survey of model compression techniques for LLMs. We cover methods like quantization, pruning, and knowledge distillation, highlighting recent advancements. We also discuss benchmarking strategies and evaluation metrics crucial for assessing compressed LLMs. This survey offers valuable insights for researchers and practitioners, aiming to enhance efficiency and real-world applicability of LLMs while laying a foundation for future advancements.",
            "corpus_id": 260900101,
            "sentences": [
                {
                    "corpus_id": "260900101",
                    "title": "A Survey on Model Compression for Large Language Models",
                    "text": "Unstructured pruning preserves the pruned model's performance, hence, works related to unstructured pruning of LLMs often dispense with retraining to restore performance. Nevertheless, unstructured pruning renders the pruned model irregular, necessitating specialized handling or software optimizations for inference acceleration. An innovative approach in this domain is SparseGPT (Frantar and Alistarh, 2023), which introduces a one-shot pruning strategy without retraining. SparseGPT frames pruning as an extensive sparse regression problem and solves it using an approximate sparse regression solver. SparseGPT achieves significant unstructured sparsity, even up to over 50% on the largest GPT models like OPT-175B and BLOOM-176B, with minimal increase in perplexity. To reduce the cost about the weight update process required by SparseGPT, Wanda (Sun et al., 2024) achieves model sparsity by pruning weights with the smallest magnitudes multiplied by the norm of the corresponding input activations, without the need for retraining or weight updates. To further minimize pruning-induced errors while upholding the desired overall sparsity level, SAMSP (Shao et al., 2024a) utilizes the Hessian matrix as a metric for weight matrix sensitivity evaluation, and dynamically adjusts sparsity allocation based on sensitivity. Furthermore, DSnoT (Zhang et al., 2024) minimizes the reconstruction error between dense and sparse models through iterative weight pruning-and-growing on top of sparse LLMs to enhance LLM performance across various sparsity rates, especially at high sparsity levels. To provide hardware support for handling unstructured pruning on the GPU Tensor Core hardware, Flash-LLM (Xia et al., 2023) introduces an unstructured sparse matrix multiplication method, which loads weight matrices in a sparse format from global memory and reconstructs them in a dense format within high-speed on-chip buffers for computation using tensor cores.",
                    "score": 0.6518651719547628,
                    "section_title": "Unstructured Pruning",
                    "char_start_offset": 17929,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 170
                        },
                        {
                            "start": 171,
                            "end": 330
                        },
                        {
                            "start": 331,
                            "end": 476
                        },
                        {
                            "start": 477,
                            "end": 604
                        },
                        {
                            "start": 605,
                            "end": 771
                        },
                        {
                            "start": 772,
                            "end": 1056
                        },
                        {
                            "start": 1057,
                            "end": 1326
                        },
                        {
                            "start": 1327,
                            "end": 1594
                        },
                        {
                            "start": 1595,
                            "end": 1958
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 852,
                            "end": 870,
                            "matchedPaperCorpusId": "259203115"
                        },
                        {
                            "start": 1158,
                            "end": 1178,
                            "matchedPaperCorpusId": "264146174"
                        },
                        {
                            "start": 1346,
                            "end": 1366,
                            "matchedPaperCorpusId": "264128029"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.8232421875
                }
            ],
            "relevance_judgement": 0.8232421875,
            "relevance_judgment_input_expanded": "# Title: A Survey on Model Compression for Large Language Models\n# Venue: Transactions of the Association for Computational Linguistics\n# Authors: Xunyu Zhu, Jian Li, Yong Liu, Can Ma, Weiping Wang\n## Abstract\nAbstract Large Language Models (LLMs) have transformed natural language processing tasks successfully. Yet, their large size and high computational needs pose challenges for practical use, especially in resource-limited settings. Model compression has emerged as a key research area to address these challenges. This paper presents a survey of model compression techniques for LLMs. We cover methods like quantization, pruning, and knowledge distillation, highlighting recent advancements. We also discuss benchmarking strategies and evaluation metrics crucial for assessing compressed LLMs. This survey offers valuable insights for researchers and practitioners, aiming to enhance efficiency and real-world applicability of LLMs while laying a foundation for future advancements.\n## Unstructured Pruning\nUnstructured pruning preserves the pruned model's performance, hence, works related to unstructured pruning of LLMs often dispense with retraining to restore performance. Nevertheless, unstructured pruning renders the pruned model irregular, necessitating specialized handling or software optimizations for inference acceleration. An innovative approach in this domain is SparseGPT (Frantar and Alistarh, 2023), which introduces a one-shot pruning strategy without retraining. SparseGPT frames pruning as an extensive sparse regression problem and solves it using an approximate sparse regression solver. SparseGPT achieves significant unstructured sparsity, even up to over 50% on the largest GPT models like OPT-175B and BLOOM-176B, with minimal increase in perplexity. To reduce the cost about the weight update process required by SparseGPT, Wanda (Sun et al., 2024) achieves model sparsity by pruning weights with the smallest magnitudes multiplied by the norm of the corresponding input activations, without the need for retraining or weight updates. To further minimize pruning-induced errors while upholding the desired overall sparsity level, SAMSP (Shao et al., 2024a) utilizes the Hessian matrix as a metric for weight matrix sensitivity evaluation, and dynamically adjusts sparsity allocation based on sensitivity. Furthermore, DSnoT (Zhang et al., 2024) minimizes the reconstruction error between dense and sparse models through iterative weight pruning-and-growing on top of sparse LLMs to enhance LLM performance across various sparsity rates, especially at high sparsity levels. To provide hardware support for handling unstructured pruning on the GPU Tensor Core hardware, Flash-LLM (Xia et al., 2023) introduces an unstructured sparse matrix multiplication method, which loads weight matrices in a sparse format from global memory and reconstructs them in a dense format within high-speed on-chip buffers for computation using tensor cores.",
            "reference_string": "[260900101 | Zhu et al. | 2023 | Citations: 229]"
        },
        {
            "title": "Towards Extreme Pruning of LLMs with Plug-and-Play Mixed Sparsity",
            "venue": "arXiv.org",
            "year": 2025,
            "reference_count": 35,
            "citation_count": 1,
            "influential_citation_count": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2503.11164, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2350425334",
                    "name": "Chi Xu"
                },
                {
                    "authorId": "2350430287",
                    "name": "Gefei Zhang"
                },
                {
                    "authorId": "2350478667",
                    "name": "Yantong Zhu"
                },
                {
                    "authorId": "2323368873",
                    "name": "Luca Benini"
                },
                {
                    "authorId": "2282196819",
                    "name": "Guosheng Hu"
                },
                {
                    "authorId": "2323432053",
                    "name": "Yawei Li"
                },
                {
                    "authorId": "2350803789",
                    "name": "Zhihong Zhang"
                }
            ],
            "abstract": "N:M structured pruning is essential for large language models (LLMs) because it can remove less important network weights and reduce the memory and computation requirements. Existing pruning methods mainly focus on designing metrics to measure the importance of network components to guide pruning. Apart from the impact of these metrics, we observe that different layers have different sensitivities over the network performance. Thus, we propose an efficient method based on the trace of Fisher Information Matrix (FIM) to quantitatively measure and verify the different sensitivities across layers. Based on this, we propose Mixed Sparsity Pruning (MSP) which uses a pruning-oriented evolutionary algorithm (EA) to determine the optimal sparsity levels for different layers. To guarantee fast convergence and achieve promising performance, we utilize efficient FIM-inspired layer-wise sensitivity to initialize the population of EA. In addition, our MSP can work as a plug-and-play module, ready to be integrated into existing pruning methods. Extensive experiments on LLaMA and LLaMA-2 on language modeling and zero-shot tasks demonstrate our superior performance. In particular, in extreme pruning ratio (e.g. 75%), our method significantly outperforms existing methods in terms of perplexity (PPL) by orders of magnitude (Figure 1).",
            "corpus_id": 277043299,
            "sentences": [
                {
                    "corpus_id": "277043299",
                    "title": "Towards Extreme Pruning of LLMs with Plug-and-Play Mixed Sparsity",
                    "text": "Language model pruning aims to reduce their computational and memory demands while preserving performance. Recent works have introduced innovative approaches in this domain. SparseGPT [6] formalizes the problem of pruning LLMs by solving a local layer-wise reconstruction problem, where their pruning metric and weight update procedure is inspired from Optimal Brain Surgeon (OBS) [4]. Wanda [13] streamlines the process by simplifying SparseGPT's [6] methodology, and explores the weight magnitude and activations as a criterion for pruning, offering a simple yet effective strategy to achieve high sparsity ratios. RIA [8] also focuses on metrics related to weights and activations but introduces channel permutation [14] to maximize the retention of important weights under N:M sparsity. Pruner Zero [9] employs evolutionary algorithms to discover optimal pruning metrics, providing a comprehensive framework for metric exploration. Despite the great success of these methods, their performance drops greatly at extreme sparsity ratios, e.g. 75%.",
                    "score": 0.7179205746207113,
                    "section_title": "Language Model Pruning",
                    "char_start_offset": 4824,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 106
                        },
                        {
                            "start": 107,
                            "end": 173
                        },
                        {
                            "start": 174,
                            "end": 385
                        },
                        {
                            "start": 386,
                            "end": 616
                        },
                        {
                            "start": 617,
                            "end": 790
                        },
                        {
                            "start": 791,
                            "end": 935
                        },
                        {
                            "start": 936,
                            "end": 1044
                        },
                        {
                            "start": 1045,
                            "end": 1049
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 184,
                            "end": 187,
                            "matchedPaperCorpusId": "255372747"
                        },
                        {
                            "start": 381,
                            "end": 384,
                            "matchedPaperCorpusId": "61815367"
                        },
                        {
                            "start": 392,
                            "end": 396,
                            "matchedPaperCorpusId": "248266822"
                        },
                        {
                            "start": 448,
                            "end": 451,
                            "matchedPaperCorpusId": "255372747"
                        },
                        {
                            "start": 719,
                            "end": 723,
                            "matchedPaperCorpusId": "245002847"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.81787109375
                }
            ],
            "relevance_judgement": 0.81787109375,
            "relevance_judgment_input_expanded": "# Title: Towards Extreme Pruning of LLMs with Plug-and-Play Mixed Sparsity\n# Venue: arXiv.org\n# Authors: Chi Xu, Gefei Zhang, Yantong Zhu, Luca Benini, Guosheng Hu, Yawei Li, Zhihong Zhang\n## Abstract\nN:M structured pruning is essential for large language models (LLMs) because it can remove less important network weights and reduce the memory and computation requirements. Existing pruning methods mainly focus on designing metrics to measure the importance of network components to guide pruning. Apart from the impact of these metrics, we observe that different layers have different sensitivities over the network performance. Thus, we propose an efficient method based on the trace of Fisher Information Matrix (FIM) to quantitatively measure and verify the different sensitivities across layers. Based on this, we propose Mixed Sparsity Pruning (MSP) which uses a pruning-oriented evolutionary algorithm (EA) to determine the optimal sparsity levels for different layers. To guarantee fast convergence and achieve promising performance, we utilize efficient FIM-inspired layer-wise sensitivity to initialize the population of EA. In addition, our MSP can work as a plug-and-play module, ready to be integrated into existing pruning methods. Extensive experiments on LLaMA and LLaMA-2 on language modeling and zero-shot tasks demonstrate our superior performance. In particular, in extreme pruning ratio (e.g. 75%), our method significantly outperforms existing methods in terms of perplexity (PPL) by orders of magnitude (Figure 1).\n## Language Model Pruning\nLanguage model pruning aims to reduce their computational and memory demands while preserving performance. Recent works have introduced innovative approaches in this domain. SparseGPT [6] formalizes the problem of pruning LLMs by solving a local layer-wise reconstruction problem, where their pruning metric and weight update procedure is inspired from Optimal Brain Surgeon (OBS) [4]. Wanda [13] streamlines the process by simplifying SparseGPT's [6] methodology, and explores the weight magnitude and activations as a criterion for pruning, offering a simple yet effective strategy to achieve high sparsity ratios. RIA [8] also focuses on metrics related to weights and activations but introduces channel permutation [14] to maximize the retention of important weights under N:M sparsity. Pruner Zero [9] employs evolutionary algorithms to discover optimal pruning metrics, providing a comprehensive framework for metric exploration. Despite the great success of these methods, their performance drops greatly at extreme sparsity ratios, e.g. 75%.",
            "reference_string": "[277043299 | Xu et al. | 2025 | Citations: 1]"
        },
        {
            "title": "Entropy-Based Block Pruning for Efficient Large Language Models",
            "venue": "arXiv.org",
            "year": 2025,
            "reference_count": 37,
            "citation_count": 0,
            "influential_citation_count": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2504.03794, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2354128342",
                    "name": "Liangwei Yang"
                },
                {
                    "authorId": "48615640",
                    "name": "Yuhui Xu"
                },
                {
                    "authorId": "2286700513",
                    "name": "Juntao Tan"
                },
                {
                    "authorId": "36187119",
                    "name": "Doyen Sahoo"
                },
                {
                    "authorId": "2238207181",
                    "name": "Silvio Savarese"
                },
                {
                    "authorId": "2256976968",
                    "name": "Caiming Xiong"
                },
                {
                    "authorId": "2258793468",
                    "name": "Huan Wang"
                },
                {
                    "authorId": "71926704",
                    "name": "Shelby Heinecke"
                }
            ],
            "abstract": "As large language models continue to scale, their growing computational and storage demands pose significant challenges for real-world deployment. In this work, we investigate redundancy within Transformer-based models and propose an entropy-based pruning strategy to enhance efficiency while maintaining performance. Empirical analysis reveals that the entropy of hidden representations decreases in the early blocks but progressively increases across most subsequent blocks. This trend suggests that entropy serves as a more effective measure of information richness within computation blocks. Unlike cosine similarity, which primarily captures geometric relationships, entropy directly quantifies uncertainty and information content, making it a more reliable criterion for pruning. Extensive experiments demonstrate that our entropy-based pruning approach surpasses cosine similarity-based methods in reducing model size while preserving accuracy, offering a promising direction for efficient model deployment.",
            "corpus_id": 277622258,
            "sentences": [
                {
                    "corpus_id": "277622258",
                    "title": "Entropy-Based Block Pruning for Efficient Large Language Models",
                    "text": "E-Sparse (Li et al., 2023) introduces an entropy-based pruning method that enhances inference speed and reduces memory usage in large language models by leveraging information rich- ness to guide N:M sparsity. SPP (Lu et al., 2024b) designs an efficient fine-tuning method to recover model performance post-pruning while maintaining sparsity. Beyond parameter pruning, structural pruning of LLMs has also gained popularity. LLM-Pruner (Ma et al., 2023) and ShearedLLaMA (Xia et al., 2023) remove unimportant structures such as layers and attention heads. Additionally, (Lu et al., 2024a) finds that certain experts in mixture-ofexperts (MoE) LLMs can also be pruned. Among structural pruning methods, layer pruning is particularly relevant. Laco (Yang et al., 2024) reduces model depth by merging adjacent layers from the topmost layer downward. ShortGPT (Men et al., 2024) prunes unimportant layers based on a cosine similarity criterion. LLMDrop (He et al., 2024) finds that attention layers are more redundant than MLP layers but also relies on cosine similarity for pruning. Different from these approaches, in this paper, we propose a more effective criterion i.e. En-tropy Increase to identify and remove unimportant layers.",
                    "score": 0.6348737527819102,
                    "section_title": "Speedup Test",
                    "char_start_offset": 20929,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 209
                        },
                        {
                            "start": 210,
                            "end": 342
                        },
                        {
                            "start": 343,
                            "end": 554
                        },
                        {
                            "start": 555,
                            "end": 666
                        },
                        {
                            "start": 667,
                            "end": 740
                        },
                        {
                            "start": 741,
                            "end": 845
                        },
                        {
                            "start": 846,
                            "end": 939
                        },
                        {
                            "start": 940,
                            "end": 1078
                        },
                        {
                            "start": 1079,
                            "end": 1169
                        },
                        {
                            "start": 1170,
                            "end": 1230
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 746,
                            "end": 765,
                            "matchedPaperCorpusId": "267751181"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.810546875
                }
            ],
            "relevance_judgement": 0.810546875,
            "relevance_judgment_input_expanded": "# Title: Entropy-Based Block Pruning for Efficient Large Language Models\n# Venue: arXiv.org\n# Authors: Liangwei Yang, Yuhui Xu, Juntao Tan, Doyen Sahoo, Silvio Savarese, Caiming Xiong, Huan Wang, Shelby Heinecke\n## Abstract\nAs large language models continue to scale, their growing computational and storage demands pose significant challenges for real-world deployment. In this work, we investigate redundancy within Transformer-based models and propose an entropy-based pruning strategy to enhance efficiency while maintaining performance. Empirical analysis reveals that the entropy of hidden representations decreases in the early blocks but progressively increases across most subsequent blocks. This trend suggests that entropy serves as a more effective measure of information richness within computation blocks. Unlike cosine similarity, which primarily captures geometric relationships, entropy directly quantifies uncertainty and information content, making it a more reliable criterion for pruning. Extensive experiments demonstrate that our entropy-based pruning approach surpasses cosine similarity-based methods in reducing model size while preserving accuracy, offering a promising direction for efficient model deployment.\n## Speedup Test\nE-Sparse (Li et al., 2023) introduces an entropy-based pruning method that enhances inference speed and reduces memory usage in large language models by leveraging information rich- ness to guide N:M sparsity. SPP (Lu et al., 2024b) designs an efficient fine-tuning method to recover model performance post-pruning while maintaining sparsity. Beyond parameter pruning, structural pruning of LLMs has also gained popularity. LLM-Pruner (Ma et al., 2023) and ShearedLLaMA (Xia et al., 2023) remove unimportant structures such as layers and attention heads. Additionally, (Lu et al., 2024a) finds that certain experts in mixture-ofexperts (MoE) LLMs can also be pruned. Among structural pruning methods, layer pruning is particularly relevant. Laco (Yang et al., 2024) reduces model depth by merging adjacent layers from the topmost layer downward. ShortGPT (Men et al., 2024) prunes unimportant layers based on a cosine similarity criterion. LLMDrop (He et al., 2024) finds that attention layers are more redundant than MLP layers but also relies on cosine similarity for pruning. Different from these approaches, in this paper, we propose a more effective criterion i.e. En-tropy Increase to identify and remove unimportant layers.",
            "reference_string": "[277622258 | Yang et al. | 2025 | Citations: 0]"
        },
        {
            "title": "Dependency-Aware Semi-Structured Sparsity of GLU Variants in Large Language Models",
            "venue": "Trans. Mach. Learn. Res.",
            "year": 2024,
            "reference_count": 59,
            "citation_count": 1,
            "influential_citation_count": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2405.01943, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2300138950",
                    "name": "Zhiyu Guo"
                },
                {
                    "authorId": "2300756",
                    "name": "Hidetaka Kamigaito"
                },
                {
                    "authorId": "2299941873",
                    "name": "Taro Wanatnabe"
                }
            ],
            "abstract": "The rapid advancement in Large Language Models (LLMs) has markedly enhanced the capabilities of language understanding and generation. However, the substantial model size poses hardware challenges, affecting both memory size for serving and inference latency for token generation. To address those challenges, we propose Dependency-aware Semi-structured Sparsity (DaSS), a novel method for the recent prevalent GLU-based LLMs pruning, which incorporates structural dependency into the weight magnitude-based unstructured pruning. We introduce an MLP-specific pruning metric that evaluates the importance of each weight by jointly considering its magnitude and its corresponding MLP intermediate activation norms. DaSS facilitates a balance between the adaptability offered by unstructured pruning and the structural consistency inherent in dependency-based structured pruning. Empirical evaluations on LLaMA2, Mistral, and Gemma model families demonstrate that DaSS not only outperforms both SparseGPT and Wanda in achieving hardware-friendly N:M sparsity patterns but also maintains the computational efficiency of Wanda.",
            "corpus_id": 270621063,
            "sentences": [
                {
                    "corpus_id": "270621063",
                    "title": "Dependency-Aware Semi-Structured Sparsity of GLU Variants in Large Language Models",
                    "text": "The rapid advancement in Large Language Models (LLMs) has markedly enhanced the capabilities of language understanding and generation. However, the substantial model size poses hardware challenges, affecting both memory size for serving and inference latency for token generation. To address those challenges, we propose Dependency-aware Semi-structured Sparsity (DaSS), a novel method for the recent prevalent GLU-based LLMs pruning, which incorporates structural dependency into the weight magnitude-based unstructured pruning. We introduce an MLP-specific pruning metric that evaluates the importance of each weight by jointly considering its magnitude and its corresponding MLP intermediate activation norms. DaSS facilitates a balance between the adaptability offered by unstructured pruning and the structural consistency inherent in dependency-based structured pruning. Empirical evaluations on LLaMA2, Mistral, and Gemma model families demonstrate that DaSS not only outperforms both SparseGPT and Wanda in achieving hardware-friendly N:M sparsity patterns but also maintains the computational efficiency of Wanda.",
                    "score": 0.6570433209026793,
                    "section_title": "abstract",
                    "char_start_offset": 0,
                    "sentence_offsets": [],
                    "ref_mentions": [],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.81005859375
                }
            ],
            "relevance_judgement": 0.81005859375,
            "relevance_judgment_input_expanded": "# Title: Dependency-Aware Semi-Structured Sparsity of GLU Variants in Large Language Models\n# Venue: Trans. Mach. Learn. Res.\n# Authors: Zhiyu Guo, Hidetaka Kamigaito, Taro Wanatnabe\n## Abstract\nThe rapid advancement in Large Language Models (LLMs) has markedly enhanced the capabilities of language understanding and generation. However, the substantial model size poses hardware challenges, affecting both memory size for serving and inference latency for token generation. To address those challenges, we propose Dependency-aware Semi-structured Sparsity (DaSS), a novel method for the recent prevalent GLU-based LLMs pruning, which incorporates structural dependency into the weight magnitude-based unstructured pruning. We introduce an MLP-specific pruning metric that evaluates the importance of each weight by jointly considering its magnitude and its corresponding MLP intermediate activation norms. DaSS facilitates a balance between the adaptability offered by unstructured pruning and the structural consistency inherent in dependency-based structured pruning. Empirical evaluations on LLaMA2, Mistral, and Gemma model families demonstrate that DaSS not only outperforms both SparseGPT and Wanda in achieving hardware-friendly N:M sparsity patterns but also maintains the computational efficiency of Wanda.\n",
            "reference_string": "[270621063 | Guo et al. | 2024 | Citations: 1]"
        },
        {
            "title": "Systematic Weight Evaluation for Pruning Large Language Models: Enhancing Performance and Sustainability",
            "venue": "arXiv.org",
            "year": 2025,
            "reference_count": 26,
            "citation_count": 0,
            "influential_citation_count": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2502.17071, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2003814068",
                    "name": "Ashhadul Islam"
                },
                {
                    "authorId": "102804035",
                    "name": "S. Belhaouari"
                },
                {
                    "authorId": "2256848295",
                    "name": "Amine Bermak"
                }
            ],
            "abstract": "The exponential growth of large language models (LLMs) like ChatGPT has revolutionized artificial intelligence, offering unprecedented capabilities in natural language processing. However, the extensive computational resources required for training these models have significant environmental implications, including high carbon emissions, energy consumption, and water usage. This research presents a novel approach to LLM pruning, focusing on the systematic evaluation of individual weight importance throughout the training process. By monitoring parameter evolution over time, we propose a method that effectively reduces model size without compromising performance. Extensive experiments with both a scaled-down LLM and a large multimodal model reveal that moderate pruning enhances efficiency and reduces loss, while excessive pruning drastically deteriorates model performance. These findings highlight the critical need for optimized AI models to ensure sustainable development, balancing technological advancement with environmental responsibility.",
            "corpus_id": 276576138,
            "sentences": [
                {
                    "corpus_id": "276576138",
                    "title": "Systematic Weight Evaluation for Pruning Large Language Models: Enhancing Performance and Sustainability",
                    "text": "Methods like SparseGPT (Frantar and Alistarh, 2023) and Wanda (Sun et al., 2023) use sophisticated weight updates and pruning without retraining, respectively, while PST (Li et al., 2022) combines unstructured pruning with efficient fine-tuning. Structured pruning methods (Chen et al., 2021b(Chen et al., , 2023) ) remove entire groups of parameters, maintaining dense weight matrices and improving hardware efficiency. Techniques such as LLM-Pruner (Ma et al., 2023) and LoRAPrune (Zhang et al., 2023) focus on efficient deployment and inference acceleration, with Sheared-LLaMA (Xia et al., 2023) aiming to prune models to a target architecture and train them dynamically. Furthermore, the compression of language models has garnered significant attention, leading to various methods like network pruning, knowledge distillation, and quantization (Bai et al., 2020;Brown et al., 2020;Devlin, 2018). Pruning, especially structural pruning, remains a crucial focus due to its hardware-friendly nature, with methods varying from l1-dependent pruning (Zafrir et al., 2021) to more advanced techniques like the optimal brain surgeon (LeCun et al., 1989). Efficient compression and low-resource strategies are increasingly essential, with advancements in layer-wise optimal brain surgeon and data-free pruning approaches aiming to optimize the balance between compression efficiency and training data availability (Kurti\u0107 et al., 2024;Srinivas and Babu, 2015).",
                    "score": 0.7191510010515965,
                    "section_title": "Related Work",
                    "char_start_offset": 5555,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 245
                        },
                        {
                            "start": 246,
                            "end": 420
                        },
                        {
                            "start": 421,
                            "end": 675
                        },
                        {
                            "start": 676,
                            "end": 901
                        },
                        {
                            "start": 902,
                            "end": 1152
                        },
                        {
                            "start": 1153,
                            "end": 1457
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 23,
                            "end": 51,
                            "matchedPaperCorpusId": "255372747"
                        },
                        {
                            "start": 273,
                            "end": 292,
                            "matchedPaperCorpusId": "235899080"
                        },
                        {
                            "start": 451,
                            "end": 468,
                            "matchedPaperCorpusId": "258823276"
                        },
                        {
                            "start": 868,
                            "end": 887,
                            "matchedPaperCorpusId": "218971783"
                        },
                        {
                            "start": 1131,
                            "end": 1151,
                            "matchedPaperCorpusId": "7785881"
                        },
                        {
                            "start": 1411,
                            "end": 1432,
                            "matchedPaperCorpusId": "256662263"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.80859375
                }
            ],
            "relevance_judgement": 0.80859375,
            "relevance_judgment_input_expanded": "# Title: Systematic Weight Evaluation for Pruning Large Language Models: Enhancing Performance and Sustainability\n# Venue: arXiv.org\n# Authors: Ashhadul Islam, S. Belhaouari, Amine Bermak\n## Abstract\nThe exponential growth of large language models (LLMs) like ChatGPT has revolutionized artificial intelligence, offering unprecedented capabilities in natural language processing. However, the extensive computational resources required for training these models have significant environmental implications, including high carbon emissions, energy consumption, and water usage. This research presents a novel approach to LLM pruning, focusing on the systematic evaluation of individual weight importance throughout the training process. By monitoring parameter evolution over time, we propose a method that effectively reduces model size without compromising performance. Extensive experiments with both a scaled-down LLM and a large multimodal model reveal that moderate pruning enhances efficiency and reduces loss, while excessive pruning drastically deteriorates model performance. These findings highlight the critical need for optimized AI models to ensure sustainable development, balancing technological advancement with environmental responsibility.\n## Related Work\nMethods like SparseGPT (Frantar and Alistarh, 2023) and Wanda (Sun et al., 2023) use sophisticated weight updates and pruning without retraining, respectively, while PST (Li et al., 2022) combines unstructured pruning with efficient fine-tuning. Structured pruning methods (Chen et al., 2021b(Chen et al., , 2023) ) remove entire groups of parameters, maintaining dense weight matrices and improving hardware efficiency. Techniques such as LLM-Pruner (Ma et al., 2023) and LoRAPrune (Zhang et al., 2023) focus on efficient deployment and inference acceleration, with Sheared-LLaMA (Xia et al., 2023) aiming to prune models to a target architecture and train them dynamically. Furthermore, the compression of language models has garnered significant attention, leading to various methods like network pruning, knowledge distillation, and quantization (Bai et al., 2020;Brown et al., 2020;Devlin, 2018). Pruning, especially structural pruning, remains a crucial focus due to its hardware-friendly nature, with methods varying from l1-dependent pruning (Zafrir et al., 2021) to more advanced techniques like the optimal brain surgeon (LeCun et al., 1989). Efficient compression and low-resource strategies are increasingly essential, with advancements in layer-wise optimal brain surgeon and data-free pruning approaches aiming to optimize the balance between compression efficiency and training data availability (Kurti\u0107 et al., 2024;Srinivas and Babu, 2015).",
            "reference_string": "[276576138 | Islam et al. | 2025 | Citations: 0]"
        },
        {
            "title": "Beyond the Limits: A Survey of Techniques to Extend the Context Length in Large Language Models",
            "venue": "International Joint Conference on Artificial Intelligence",
            "year": 2024,
            "reference_count": 79,
            "citation_count": 45,
            "influential_citation_count": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2402.02244, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2108048327",
                    "name": "Xindi Wang"
                },
                {
                    "authorId": "1904419",
                    "name": "Mahsa Salmani"
                },
                {
                    "authorId": "2282534833",
                    "name": "Parsa Omidi"
                },
                {
                    "authorId": "2283447900",
                    "name": "Xiangyu Ren"
                },
                {
                    "authorId": "2066076226",
                    "name": "Mehdi Rezagholizadeh"
                },
                {
                    "authorId": "50782111",
                    "name": "A. Eshaghi"
                }
            ],
            "abstract": "Recently, large language models (LLMs) have shown remarkable capabilities including understanding context, engaging in logical reasoning, and generating responses. However, this is achieved at the expense of stringent computational and memory requirements, hindering their ability to effectively support long input sequences. This survey provides an inclusive review of the recent techniques and methods devised to extend the sequence length in LLMs, thereby enhancing their capacity for long-context understanding. In particular, we review and categorize a wide range of techniques including architectural modifications, such as modified positional encoding and altered attention mechanisms, which are designed to enhance the processing of longer sequences while avoiding a proportional increase in computational cost. The diverse methodologies investigated in this study can be leveraged across different phases of LLMs, i.e., training, fine-tuning and inference. This enables LLMs to efficiently process extended sequences. The limitations of the current methodologies is discussed in the last section along with the suggestions for future research directions, underscoring the importance of sequence length in the continued advancement of LLMs.",
            "corpus_id": 267412232,
            "sentences": [
                {
                    "corpus_id": "267412232",
                    "title": "Beyond the Limits: A Survey of Techniques to Extend the Context Length in Large Language Models",
                    "text": "Pruning can help optimize the model for deployment and make the model more efficient in terms of computation complexity and memory usage. Accordingly, pruning can be considered as an approach to enable a language model to support longer sequence length, while maintaining the desirable complexity and performance. In general, pruning a model can be categorized into structured and unstructured pruning. \n\nStructured pruning aims at removing higher-granularity structures, such as entire neurons, layers, or rows/columns of weight matrices, which can result in a model that retains its original structure but with fewer number of parameters. LLM-Pruner [Ma et al., 2023a] is a structural task-agnostic pruning approach that selectively removes non-critical connection structures considering both first-order information and an approximated Hessian information gradient information. As an alternative, Sheared LLaMA [Xia et al., 2024] utilizes a two-stage approach for pruning an LLM. In the first stage, it exploits targeted structured pruning to prune a large model to a targeted shape by pruning layers, heads, and intermediate connections. In the second stage, the batches of data are loaded dynamically and the model structure is modified in each training iteration based on losses in various domains. As a result, Sheared LLaMA achieves a compressed model that can outperform the LLMs, with the same size but trained from scratch. \n\nUnstructured pruning involves with pruning individual parameters of a model independently based on their magnitudes or importance, resulting in an irregular sparse structure. Due to the irregularity in the structure and in the memory access patterns, unstructured pruning hinders the efficiency gain that might be achieved through structured pruning, and it requires specialized software and/or hardware for efficient deployment. SparseGPT [Frantar and Alistarh, 2023] compresses LLMs with billions of parameter by as much as 60%, almost without affecting the performance of the models. However, SparseGPT heavily relies on weight updates.",
                    "score": 0.7687203874595878,
                    "section_title": "Model Compression",
                    "char_start_offset": 28041,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 137
                        },
                        {
                            "start": 138,
                            "end": 313
                        },
                        {
                            "start": 314,
                            "end": 402
                        },
                        {
                            "start": 405,
                            "end": 640
                        },
                        {
                            "start": 641,
                            "end": 880
                        },
                        {
                            "start": 881,
                            "end": 982
                        },
                        {
                            "start": 983,
                            "end": 1141
                        },
                        {
                            "start": 1142,
                            "end": 1304
                        },
                        {
                            "start": 1305,
                            "end": 1434
                        },
                        {
                            "start": 1437,
                            "end": 1611
                        },
                        {
                            "start": 1612,
                            "end": 1866
                        },
                        {
                            "start": 1867,
                            "end": 2023
                        },
                        {
                            "start": 2024,
                            "end": 2076
                        }
                    ],
                    "ref_mentions": [],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.80078125
                }
            ],
            "relevance_judgement": 0.80078125,
            "relevance_judgment_input_expanded": "# Title: Beyond the Limits: A Survey of Techniques to Extend the Context Length in Large Language Models\n# Venue: International Joint Conference on Artificial Intelligence\n# Authors: Xindi Wang, Mahsa Salmani, Parsa Omidi, Xiangyu Ren, Mehdi Rezagholizadeh, A. Eshaghi\n## Abstract\nRecently, large language models (LLMs) have shown remarkable capabilities including understanding context, engaging in logical reasoning, and generating responses. However, this is achieved at the expense of stringent computational and memory requirements, hindering their ability to effectively support long input sequences. This survey provides an inclusive review of the recent techniques and methods devised to extend the sequence length in LLMs, thereby enhancing their capacity for long-context understanding. In particular, we review and categorize a wide range of techniques including architectural modifications, such as modified positional encoding and altered attention mechanisms, which are designed to enhance the processing of longer sequences while avoiding a proportional increase in computational cost. The diverse methodologies investigated in this study can be leveraged across different phases of LLMs, i.e., training, fine-tuning and inference. This enables LLMs to efficiently process extended sequences. The limitations of the current methodologies is discussed in the last section along with the suggestions for future research directions, underscoring the importance of sequence length in the continued advancement of LLMs.\n## Model Compression\nPruning can help optimize the model for deployment and make the model more efficient in terms of computation complexity and memory usage. Accordingly, pruning can be considered as an approach to enable a language model to support longer sequence length, while maintaining the desirable complexity and performance. In general, pruning a model can be categorized into structured and unstructured pruning. \n\nStructured pruning aims at removing higher-granularity structures, such as entire neurons, layers, or rows/columns of weight matrices, which can result in a model that retains its original structure but with fewer number of parameters. LLM-Pruner [Ma et al., 2023a] is a structural task-agnostic pruning approach that selectively removes non-critical connection structures considering both first-order information and an approximated Hessian information gradient information. As an alternative, Sheared LLaMA [Xia et al., 2024] utilizes a two-stage approach for pruning an LLM. In the first stage, it exploits targeted structured pruning to prune a large model to a targeted shape by pruning layers, heads, and intermediate connections. In the second stage, the batches of data are loaded dynamically and the model structure is modified in each training iteration based on losses in various domains. As a result, Sheared LLaMA achieves a compressed model that can outperform the LLMs, with the same size but trained from scratch. \n\nUnstructured pruning involves with pruning individual parameters of a model independently based on their magnitudes or importance, resulting in an irregular sparse structure. Due to the irregularity in the structure and in the memory access patterns, unstructured pruning hinders the efficiency gain that might be achieved through structured pruning, and it requires specialized software and/or hardware for efficient deployment. SparseGPT [Frantar and Alistarh, 2023] compresses LLMs with billions of parameter by as much as 60%, almost without affecting the performance of the models. However, SparseGPT heavily relies on weight updates.",
            "reference_string": "[267412232 | Wang et al. | 2024 | Citations: 45]"
        }
    ],
    "retrieved": [
        {
            "corpus_id": "270063400",
            "title": "SPP: Sparsity-Preserved Parameter-Efficient Fine-Tuning for Large Language Models",
            "text": "Large language models (LLMs) (Brown et al., 2020;Ope-nAI, 2023;Anil et al., 2023) have recently shown impressive success in various complex tasks (Wei et al., 2022;Zhou et al., 2024).However, these models are usually characterized by an extensive number of learnable param- * Equal contribution \u2020 Corresponding author Unstructured 75% (7B) 2:8 (7B) Unstructured 75% (30B) 2:8 (30B) We use LLaMA 7B/30B models at 75% sparsity and test zero-shot accuracies on 7 benchmarks of LM-eval (Gao et al., 2021) to compare different pruning methods.The results of Wanda, Wanda+DS\u2298T, Wanda+LoRA*, and Wanda+SPP are visualized.The first two approaches are post-training pruning schemes, LoRA* denotes applying the original Wanda pruning masks to sparse the dense model after LoRA training.Our method achieves overall best results.More experiment details are illustrated in Sec. 4. eters, ranging from several billions to around a hundred billions (Touvron et al., 2023a;b), as exemplified by GPT-4.This enormity makes LLMs cumbersome to be fine-tuned for different scenarios and challenging to deploy on various edge devices.\n\nTo reduce the parameters of pre-trained large language models without the demanding retraining phase, different posttraining pruning (Frantar & Alistarh, 2023) approaches have been presented.SparseGPT, as outlined in (Frantar & Alistarh, 2023), focuses on minimizing the squared errors between pruned and original dense models in a layer-by-layer manner.Wanda (Sun et al., 2023) incorporates both weight magnitude and input activation as metrics for identifying unimportant parameters.Despite their success in language model pruning, these methods often fail to maintain the performance of pre-trained models at even moderate sparsity levels (e.g., 50%) (Jaiswal et al., 2023).",
            "score": 0.9992584975889718,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 183
                },
                {
                    "start": 183,
                    "end": 538
                },
                {
                    "start": 538,
                    "end": 614
                },
                {
                    "start": 614,
                    "end": 776
                },
                {
                    "start": 776,
                    "end": 817
                },
                {
                    "start": 817,
                    "end": 985
                },
                {
                    "start": 985,
                    "end": 1112
                },
                {
                    "start": 1114,
                    "end": 1305
                },
                {
                    "start": 1305,
                    "end": 1468
                },
                {
                    "start": 1468,
                    "end": 1599
                },
                {
                    "start": 1599,
                    "end": 1791
                }
            ],
            "ref_mentions": [
                {
                    "start": 29,
                    "end": 49,
                    "matchedPaperCorpusId": "218971783"
                },
                {
                    "start": 146,
                    "end": 164,
                    "matchedPaperCorpusId": "249674500"
                },
                {
                    "start": 164,
                    "end": 182,
                    "matchedPaperCorpusId": "260900008"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.67724609375
        },
        {
            "corpus_id": "277435040",
            "title": "STADE: Standard Deviation as a Pruning Metric",
            "text": "Recently, Large Language Models (LLMs) have become very widespread and are used to solve a wide variety of tasks. To successfully handle these tasks, LLMs require longer training times and larger model sizes. This makes LLMs ideal candidates for pruning methods that reduce computational demands while maintaining performance. Previous methods require a retraining phase after pruning to maintain the original model's performance. However, state-of-the-art pruning methods, such as Wanda, prune the model without retraining, making the pruning process faster and more efficient. Building upon Wanda's work, this study provides a theoretical explanation of why the method is effective and leverages these insights to enhance the pruning process. Specifically, a theoretical analysis of the pruning problem reveals a common scenario in Machine Learning where Wanda is the optimal pruning method. Furthermore, this analysis is extended to cases where Wanda is no longer optimal, leading to the development of a new method, STADE, based on the standard deviation of the input. From a theoretical standpoint, STADE demonstrates better generality across different scenarios. Finally, extensive experiments on Llama and Open Pre-trained Transformers (OPT) models validate these theoretical findings, showing that depending on the training conditions, Wanda's optimal performance varies as predicted by the theoretical framework. These insights contribute to a more robust understanding of pruning strategies and their practical implications. Code is available at: https://github.com/Coello-dev/STADE/",
            "score": 0.9166916469172282,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.93017578125
        },
        {
            "corpus_id": "273549773",
            "title": "LEGO: Language Model Building Blocks",
            "text": "In recent years, pruning has become widely used in NLP to compress LLMs (LeCun et al., 1989). Pruning involves the selective omission of model parameters with minimal contributions to the learning process. Pruning techniques have proven successful, enhancing the cost-effectiveness of large pre-trained models (Xia et al., 2023). \n\nRecently, more nuanced pruning approaches have been discussed in the literature, improving over more traditional methods like magnitude pruning. Specifically, two state-of-the-art pruning methods are widely discussed in the literature-SparseGPT (Frantar and Alistarh, 2023) and Wanda (Sun et al., 2023). Whereas traditional magnitude pruning operates by pruning weights with the largest magnitude, these pruning techniques instead track weight activations, and prune weights with the lowest amount of activation. \n\nSparseGPT creates and solves a layer-wise reconstruction problem to determine the weight activations, whereas Wanda takes the product of a weight's magnitude and the norm of its associated input activations.",
            "score": 0.9164910835250937,
            "section_title": "Model Compression",
            "char_start_offset": 4276,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 93
                },
                {
                    "start": 94,
                    "end": 205
                },
                {
                    "start": 206,
                    "end": 329
                },
                {
                    "start": 332,
                    "end": 476
                },
                {
                    "start": 477,
                    "end": 635
                },
                {
                    "start": 636,
                    "end": 844
                },
                {
                    "start": 847,
                    "end": 1054
                }
            ],
            "ref_mentions": [
                {
                    "start": 577,
                    "end": 605,
                    "matchedPaperCorpusId": "255372747"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.86962890625
        },
        {
            "corpus_id": "276884735",
            "title": "Wanda++: Pruning Large Language Models via Regional Gradients",
            "text": "We further discuss the memory and time efficiency of our proposed method. The result is summarized in Table . 6. As mentioned earlier, integrating gradient information into the pruning score poses a significant computational challenge. Wanda avoids any gradient approximation and backpropagation, and therefore achieves simple and efficient LLM pruning compared with other post-training pruning methods like SparseGPT. \n\nHere, we evaluate the time and memory costs during the pruning process to demonstrate that our proposed method maintains similar computational advantages, especially when compared with previous LLMs pruning methods that utilize full model gradient information. It is important to note that the memory and pruning results were obtained by running the source code of each method, under the assumption that the released code has been fully optimized for that particular method. \n\nFor GBLM, we combined the time for both gradient computation and pruning, as they are handled separately in the code. \n\nWithout model weight updates, Wanda has the shortest pruning time. Wanda++ RGS (without RO) comes in second. When RO is added, as shown in the Wanda++ (M) row, the pruning time approaches that of SparseGPT. We also report metrics for Wanda++ (L) as a reference, though in practice, Wanda++ (M) is sufficient to achieve the performance shown in",
            "score": 0.8590058703420286,
            "section_title": "C PRUNING TIME AND MEMORY CONSUMPTION",
            "char_start_offset": 12114,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 73
                },
                {
                    "start": 74,
                    "end": 109
                },
                {
                    "start": 110,
                    "end": 112
                },
                {
                    "start": 113,
                    "end": 235
                },
                {
                    "start": 236,
                    "end": 418
                },
                {
                    "start": 421,
                    "end": 681
                },
                {
                    "start": 682,
                    "end": 895
                },
                {
                    "start": 898,
                    "end": 1015
                },
                {
                    "start": 1018,
                    "end": 1084
                },
                {
                    "start": 1085,
                    "end": 1126
                },
                {
                    "start": 1127,
                    "end": 1224
                },
                {
                    "start": 1225,
                    "end": 1361
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.71337890625
        },
        {
            "corpus_id": "271744772",
            "title": "A Convex-optimization-based Layer-wise Post-training Pruner for Large Language Models",
            "text": "In recent years, large language models (LLMs) have revolutionized natural language processing fields, achieving impressive results in tasks such as machine translation, sentiment analysis, question answering, and text generation (Lyu et al., 2023;Yao et al., 2023;Zhang et al., 2023a,b;Wang et al., 2023;Arefeen et al., 2024;Li et al., 2024). Advanced LLMs such as OpenAI's GPT-4 (OpenAI, 2023), Meta's LLaMA-3 (AI, 2023), and Google's Gemini (Team et al., 2023) excel in generating coherent text with extensive parameters. However, the growth in model sizes outpaces hardware improvements, posing significant deployment and inference challenges (Steiner et al., 2023). For example, operating OPT-175B (Zhang et al., 2022) requires over 320GB of memory and at least five 80GB A100 GPUs for loading its parameters in FP16 precision. This challenge becomes more pronounced in environments with limited resources, such as mobile devices, edge computing systems, and real-time applications. Consequently, there has been considerable interest in compressing LLMs to enhance their efficiency and practicality for deployment across various applications. \n\nPruning is a key method for compressing LLMs, aiming to eliminate redundant weights to reduce model size and computational demands while striving to maintain performance. Methods such as those in (Huang et al., 2020;Ma et al., 2023;Zhang et al., 2023c) require a retraining phase post-pruning, which is inefficient for billion-scale LLMs. Recent developments, including SparseGPT (Frantar and Alistarh, 2023) and Wanda (Sun et al., 2023), employ one-shot post-training pruning techniques for LLMs. These methods, however, rely on the heuristic-based optimal brain surgeon (OBS) framework (Hassibi and Stork, 1992) or utilize heuristic-based pruning metric to determine which weights to prune, potentially compromising performance.",
            "score": 0.843244690838836,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 342
                },
                {
                    "start": 343,
                    "end": 523
                },
                {
                    "start": 524,
                    "end": 669
                },
                {
                    "start": 670,
                    "end": 831
                },
                {
                    "start": 832,
                    "end": 986
                },
                {
                    "start": 987,
                    "end": 1146
                },
                {
                    "start": 1149,
                    "end": 1319
                },
                {
                    "start": 1320,
                    "end": 1487
                },
                {
                    "start": 1488,
                    "end": 1646
                },
                {
                    "start": 1647,
                    "end": 1879
                }
            ],
            "ref_mentions": [
                {
                    "start": 304,
                    "end": 325,
                    "matchedPaperCorpusId": "261531532"
                },
                {
                    "start": 325,
                    "end": 341,
                    "matchedPaperCorpusId": "260435365"
                },
                {
                    "start": 646,
                    "end": 668,
                    "matchedPaperCorpusId": "260814000"
                },
                {
                    "start": 1381,
                    "end": 1401,
                    "matchedPaperCorpusId": "233297054"
                },
                {
                    "start": 1529,
                    "end": 1557,
                    "matchedPaperCorpusId": "255372747"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.65576171875
        },
        {
            "corpus_id": "259203115",
            "title": "A Simple and Effective Pruning Approach for Large Language Models",
            "text": "Models and Evaluation. We evaluate Wanda on the two most widely adopted LLM model families: LLaMA 7B/13B/30B/65B (Touvron et al., 2023a) and LLaMA-2 7B/13B/70B (Touvron et al., 2023b) (LLaMA-2 34B is not released). Results for prior LLM families can be found in Appendix B. We measure the performance of pruned models on zero-shot tasks and language modeling. For zero-shot evaluation, we use seven tasks from EleutherAI LM Harness (Gao et al., 2021). Following previous works on LLM compression (Xiao et al., 2023;Frantar & Alistarh, 2023), we evaluate the perplexity on the held-out WikiText (Merity et al., 2016) validation set. \n\nBaselines. We compare Wanda with two prior pruning approaches. Magnitude pruning (Han et al., 2015) is a simple and strong baseline in which weights are discarded based on their magnitudes. SparseGPT (Frantar & Alistarh, 2023) is a second-order pruning method for LLMs, based on solving a layer-wise reconstruction problem. In Appendix C, we compare with additional pruning methods. \n\nBoth Wanda and SparseGPT require calibration data to estimate input statistics (see Table 1). To control this variable factor, we use the exact same set of calibration data as SparseGPT, which consists of 128 sequences with context length size sampled from C4 training set (Raffel et al., 2020). In Appendix D.1, we provide additional analysis on the number of calibration samples. \n\nSparsity. For all pruning methods, we focus on pruning the linear layers (skipping the first embedding layer and the final classification head), which account for around 99% of the total LLM parameters. We impose a uniform sparsity for all linear layers. We evaluate three types of sparsity: unstructured sparsity, structured 4:8 and 2:4 sparsities.",
            "score": 0.8371592778213608,
            "section_title": "EXPERIMENTS",
            "char_start_offset": 16183,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 22
                },
                {
                    "start": 23,
                    "end": 214
                },
                {
                    "start": 215,
                    "end": 359
                },
                {
                    "start": 360,
                    "end": 451
                },
                {
                    "start": 452,
                    "end": 631
                },
                {
                    "start": 634,
                    "end": 644
                },
                {
                    "start": 645,
                    "end": 696
                },
                {
                    "start": 697,
                    "end": 823
                },
                {
                    "start": 824,
                    "end": 957
                },
                {
                    "start": 958,
                    "end": 1016
                },
                {
                    "start": 1019,
                    "end": 1112
                },
                {
                    "start": 1113,
                    "end": 1314
                },
                {
                    "start": 1315,
                    "end": 1400
                },
                {
                    "start": 1403,
                    "end": 1412
                },
                {
                    "start": 1413,
                    "end": 1605
                },
                {
                    "start": 1606,
                    "end": 1657
                },
                {
                    "start": 1658,
                    "end": 1752
                }
            ],
            "ref_mentions": [
                {
                    "start": 496,
                    "end": 515,
                    "matchedPaperCorpusId": "253708271"
                },
                {
                    "start": 715,
                    "end": 733,
                    "matchedPaperCorpusId": "2238772"
                },
                {
                    "start": 1292,
                    "end": 1313,
                    "matchedPaperCorpusId": "204838007"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.755859375
        },
        {
            "corpus_id": "276884735",
            "title": "Wanda++: Pruning Large Language Models via Regional Gradients",
            "text": "The growing size of Large Language Models (LLMs) improves performance (Devlin et al., 2018;Touvron et al., 2023) at the cost of memory consumption and inference latency. For example, hosting an LLaMA-2-70B model needs at least four A100-40GB GPUs with the time to first token (TTFT) exceeding 100 milliseconds (Agarwal, 2023). To address these challenges, various model compression approaches, including weight decomposition (Hsu et al., 2022;Yang et al., 2024), quantization (Lin et al., 2024;Tian et al., 2023), and pruning (Sun et al., 2023;Xu et al., 2024), have been explored. Among pruning methods, post-training LLM pruning approaches, such as SparseGPT (Sun et al., 2023) and Wanda (Sun et al., 2023), have gained attention as they circumvent the prohibitive memory overhead associated with traditional in-training pruning techniques (Han et al., 2015;Frankle & Carbin, 2018). However, post-training pruning often leads to substantial performance degradation, limiting its effectiveness for efficient LLM deployment. \n\nTo mitigate the performance degradation, GBLM and Pruner-Zero (Das et al., 2023;Dong et al., 2024) propose improved pruning criteria that enhance the layer-wise Wanda score by incorporating gradient information obtained through full-model backpropagation. Meanwhile, other approaches focus on recovering model performance through sparsity-aware tuning (Sun et al., 2023) or distillation (Liang et al., 2023). Although these methods effectively reduce pruning-induced degradation, they suffer from impractical memory requirements and excessive pruning time due to the high computational cost of full-model backpropagation. This raises an important question: \n\nIs there a way to effectively involve gradient information while still in a lightweight manner?",
            "score": 0.8343675406145464,
            "section_title": "body",
            "char_start_offset": 1,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 169
                },
                {
                    "start": 170,
                    "end": 326
                },
                {
                    "start": 327,
                    "end": 581
                },
                {
                    "start": 582,
                    "end": 884
                },
                {
                    "start": 885,
                    "end": 1024
                },
                {
                    "start": 1027,
                    "end": 1282
                },
                {
                    "start": 1283,
                    "end": 1435
                },
                {
                    "start": 1436,
                    "end": 1648
                },
                {
                    "start": 1649,
                    "end": 1683
                },
                {
                    "start": 1686,
                    "end": 1781
                }
            ],
            "ref_mentions": [
                {
                    "start": 443,
                    "end": 461,
                    "matchedPaperCorpusId": "267750451"
                },
                {
                    "start": 476,
                    "end": 494,
                    "matchedPaperCorpusId": "258999941"
                },
                {
                    "start": 494,
                    "end": 512,
                    "matchedPaperCorpusId": "253224094"
                },
                {
                    "start": 544,
                    "end": 560,
                    "matchedPaperCorpusId": "273499891"
                },
                {
                    "start": 842,
                    "end": 860,
                    "matchedPaperCorpusId": "2238772"
                },
                {
                    "start": 1107,
                    "end": 1125,
                    "matchedPaperCorpusId": "270257857"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.77734375
        },
        {
            "corpus_id": "276774084",
            "title": "Revisiting Large Language Model Pruning using Neuron Semantic Attribution",
            "text": "In this study, we employed three state-of-the-art posttraining pruning methods: SparseGPT (Frantar & Alistarh, 2023), Wanda (Sun et al., 2023), and RIA (Zhang et al., 2024). These methods are designed to reduce the size of large language models (LLMs) while maintaining their performance across various tasks. Even though all three methods prune the model using a combination of weights W and activations X, they each use different metrics M and strategies to decide which parameters to prune. \n\n\u2022 SparseGPT: This method selects parameters based on the metric: \n\nAfter pruning, SparseGPT also applies an OBS (Hassibi et al., 1993) update to compensate for the pruned parameters. \n\n\u2022 Wanda: This method uses a simple yet effective metric for pruning: \n\nAfter pruning, there is no need for any parameter update step. \n\n\u2022 RIA: This method calculates the pruning metric using relative importance: \n\na . After calculating the metric, channel permutation is used to adjust the model for N:M sparsity. \n\nThese methods were evaluated on a range of tasks to assess how each performs across different calibration data. Each pruning method was applied iteratively, with models being pruned and evaluated to ensure that performance was maintained across tasks.",
            "score": 0.834088992306186,
            "section_title": "Pruning Methods",
            "char_start_offset": 10014,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 173
                },
                {
                    "start": 174,
                    "end": 309
                },
                {
                    "start": 310,
                    "end": 493
                },
                {
                    "start": 496,
                    "end": 560
                },
                {
                    "start": 563,
                    "end": 678
                },
                {
                    "start": 681,
                    "end": 749
                },
                {
                    "start": 752,
                    "end": 814
                },
                {
                    "start": 817,
                    "end": 892
                },
                {
                    "start": 895,
                    "end": 898
                },
                {
                    "start": 899,
                    "end": 994
                },
                {
                    "start": 997,
                    "end": 1108
                },
                {
                    "start": 1109,
                    "end": 1248
                }
            ],
            "ref_mentions": [
                {
                    "start": 90,
                    "end": 116,
                    "matchedPaperCorpusId": "255372747"
                },
                {
                    "start": 152,
                    "end": 172,
                    "matchedPaperCorpusId": "271745835"
                },
                {
                    "start": 608,
                    "end": 630,
                    "matchedPaperCorpusId": "61815367"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.9404296875
        },
        {
            "corpus_id": "273234021",
            "title": "Is C4 Dataset Optimal for Pruning? An Investigation of Calibration Data for LLM Pruning",
            "text": "Network pruning is a widely utilized technique to reduce model size with negligible performance loss (Mozer and Smolensky, 1989;Han et al., 2015;Molchanov et al., 2017). While numerous pruning approaches have been proposed, the success of pruning is inextricably linked to sufficient retraining (Liu et al., 2022;Wang et al., 2023). However, training large language models is prohibitively expensive and not feasible for most practitioners. \n\nFortunately, recent research efforts have proposed effective methods that enable accurate pruning of LLMs without the need for extensive fine-tuning. \n\nSparseGPT (Frantar and Alistarh, 2023a) employs second-order pruning followed by column-wise weight updates, allowing the removal of 50% of weights while maintaining the original perplexity. Wanda (Sun et al., 2023), motivated by the goal of preserving crucial outliers in LLMs, proposes pruning weights based on the multiplication of weight  magnitude with their input activation, demonstrating strong performance. OWL (Yin et al., 2023b) introduces a novel non-uniform layerwise sparsity approach for LLM pruning, showing promising results at high levels of sparsity. In addition to exploring accurate pruning methods, other studies focus on efficiently fine-tuning sparse LLMs to further enhance their performance (Zhang et al., 2023;Zimmer et al., 2023). In contrast to these previous works, our paper investigates the efficacy of input data for LLM pruning. This novel perspective is crucial for understanding and improving LLM pruning methodologies, as LLMs are sensitive to their input (Zhao et al., 2021).",
            "score": 0.8329804257407545,
            "section_title": "Large Language Model Pruning",
            "char_start_offset": 5410,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 169
                },
                {
                    "start": 170,
                    "end": 332
                },
                {
                    "start": 333,
                    "end": 440
                },
                {
                    "start": 443,
                    "end": 592
                },
                {
                    "start": 595,
                    "end": 785
                },
                {
                    "start": 786,
                    "end": 1010
                },
                {
                    "start": 1011,
                    "end": 1164
                },
                {
                    "start": 1165,
                    "end": 1353
                },
                {
                    "start": 1354,
                    "end": 1457
                },
                {
                    "start": 1458,
                    "end": 1608
                }
            ],
            "ref_mentions": [
                {
                    "start": 101,
                    "end": 128,
                    "matchedPaperCorpusId": "17651092"
                },
                {
                    "start": 128,
                    "end": 145,
                    "matchedPaperCorpusId": "2238772"
                },
                {
                    "start": 145,
                    "end": 168,
                    "matchedPaperCorpusId": "17240902"
                },
                {
                    "start": 605,
                    "end": 634,
                    "matchedPaperCorpusId": "255372747"
                },
                {
                    "start": 1588,
                    "end": 1607,
                    "matchedPaperCorpusId": "231979430"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.66064453125
        },
        {
            "corpus_id": "265050936",
            "title": "Beyond Size: How Gradients Shape Pruning Decisions in Large Language Models",
            "text": "Large Language Models (LLMs) with billions of parameters are prime targets for network pruning, removing some model weights without hurting performance. Prior approaches such as magnitude pruning, SparseGPT, and Wanda, either concentrated solely on weights or integrated weights with activations for sparsity. However, they overlooked the informative gradients derived from pretrained LLMs. In this paper, we present a novel sparsity-centric pruning method for pretrained LLMs, termed Gradient-based Language Model Pruner (GBLM-Pruner). GBLM-Pruner leverages the first-order term of the Taylor expansion, operating in a training-free manner by harnessing properly normalized gradients from a few calibration samples to determine the pruning metric, and substantially outperforms competitive counterparts like SparseGPT and Wanda in multiple benchmarks. Intriguingly, by incorporating gradients, unstructured pruning with our method tends to reveal some structural patterns, which mirrors the geometric interdependence inherent in the LLMs' parameter structure. Additionally, GBLM-Pruner functions without any subsequent retraining or weight updates to maintain its simplicity as other counterparts. Extensive evaluations on LLaMA-1 and LLaMA-2 across various benchmarks show that GBLM-Pruner surpasses magnitude pruning, Wanda and SparseGPT by significant margins. We further extend our approach on Vision Transformer. Our code and models are available at https://github.com/VILA-Lab/GBLM-Pruner.",
            "score": 0.8321014709920973,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.86962890625
        },
        {
            "corpus_id": "263830468",
            "title": "Compresso: Structured Pruning with Collaborative Prompting Learns Compact Large Language Models",
            "text": "SparseGPT is the first unstructured pruning approach specifically developed to be fast enough for pruning LLMs within a few hours. Wanda applies magnitude pruning by weights and activations, which further improves the pruning speed than SparseGPT. Both can be extended for semi-structured pruning (i.e., the N:M sparsity (Pool & Yu, 2021;Hubara et al., 2021)). However, in practice, it is more challenging to translate the theoretically achieved sparsity in unstructured or semistructured pruning to practical computation and storage savings on current GPU hardware (Frantar & Alistarh, 2023). LLM-Pruner (Ma et al., 2023) is the first attempt to structurally prune LLMs, offering the benefit of reducing both model computation and memory usage while keeping the overall LLM structure intact. It uses one-shot pruning based on first-order and approximated Hessian information and requires fine-tuning using LoRA to recover pruned model weights. \n\nDespite its fast speed, one-shot pruning has limitations. First, it depends heavily on pre-defined weight importance metrics for pruning decisions, and thus adopts a uniform-sparsity ratio across all layers without considering the different redundancy at each layer. Second, error recovery for remaining model parameters is limited compared to training-based pruning, potentially affecting the final performance. Our Compresso addresses all these limitations. \n\nPrompting. Prompting has emerged as a new paradigm for adapting pre-trained LLMs to new tasks by augmenting the model input with task-specific hints. Notable methods include template-based prompting (Schick & Sch\u00fctze, 2021), instruction-based prompting (Wei et al., 2021;Sanh et al., 2022) , and Chain-of-Thought prompting (Wei et al., 2022). Despite its demonstrated success across a spectrum of NLP tasks (Chung et al., 2022;Goyal et al., 2022;Wei et al., 2022;Chowdhery et al., 2022a), the application of prompting for pruning LLMs remains unexplored in the literature.",
            "score": 0.8266313489329903,
            "section_title": "RELATED WORKS",
            "char_start_offset": 5425,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 130
                },
                {
                    "start": 131,
                    "end": 247
                },
                {
                    "start": 248,
                    "end": 360
                },
                {
                    "start": 361,
                    "end": 593
                },
                {
                    "start": 594,
                    "end": 792
                },
                {
                    "start": 793,
                    "end": 944
                },
                {
                    "start": 947,
                    "end": 1004
                },
                {
                    "start": 1005,
                    "end": 1213
                },
                {
                    "start": 1214,
                    "end": 1359
                },
                {
                    "start": 1360,
                    "end": 1406
                },
                {
                    "start": 1409,
                    "end": 1419
                },
                {
                    "start": 1420,
                    "end": 1558
                },
                {
                    "start": 1559,
                    "end": 1751
                },
                {
                    "start": 1752,
                    "end": 1981
                }
            ],
            "ref_mentions": [
                {
                    "start": 321,
                    "end": 338,
                    "matchedPaperCorpusId": "245002847"
                },
                {
                    "start": 338,
                    "end": 358,
                    "matchedPaperCorpusId": "231934142"
                },
                {
                    "start": 1608,
                    "end": 1632,
                    "matchedPaperCorpusId": "210838924"
                },
                {
                    "start": 1680,
                    "end": 1698,
                    "matchedPaperCorpusId": "239009562"
                },
                {
                    "start": 1732,
                    "end": 1750,
                    "matchedPaperCorpusId": "246411621"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.75927734375
        },
        {
            "corpus_id": "277452419",
            "title": "Model Hemorrhage and the Robustness Limits of Large Language Models",
            "text": "Unstructured pruning is an optimization technique that achieves model sparsity by evaluating the importance of individual weights. Its flexibility and high compression rates make it a key method for optimizing large language models (LLMs). Unstructured pruning can achieve extremely high compression rates; for instance, Wanda achieves a 60% sparsity rate on LLaMA-7B with minimal performance degradation across multiple downstream tasks [27], while Flash-LLM achieves a 70% sparsity rate on OPT-175B, significantly reducing storage requirements with less than 2% performance degradation during inference [28]. However, unstructured pruning often results in irregular sparse patterns in the weight matrix, necessitating specialized hardware accelerators (e.g., sparse matrix multiplication units) to efficiently handle sparse matrix computations and fully exploit the benefits of sparsity in terms of storage and computation. Among various unstructured pruning methods, Magnitude Pruning is the most basic, directly removing weights with small magnitudes. While simple to implement, it does not account for the contextual importance of weights. SparseGPT [29], on the other hand, introduces a diagonal Hessian approximation to assess the impact of weights on errors, enabling more precise pruning at the cost of high computational complexity and hardware resource requirements. Wanda [27] simplifies the SparseGPT algorithm by eliminating the need for Hessian approximations and instead computes pruning metrics by multiplying weights with input activations. This simplification significantly reduces computational complexity while achieving a balance between high accuracy and efficiency. Following this approach, many subsequent methods use SparseGPT and Wanda as baselines or build upon their foundations. RIA [30] introduces a post-training pruning method that re-evaluates the importance of each weight element based on all input and output connections. ADMM [31] builds on SparseGPT by incorporating the Alternating Direction Method of Multipliers (ADMM) to restore model performance after pruning, using a simple iterative mask selection process for pruning. OWL [32] integrates both Wanda and SparseGPT, proposing the OWL metric to allocate varying pruning rates across different layers.",
            "score": 0.8261701818539282,
            "section_title": "Unstructured Pruning",
            "char_start_offset": 21938,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 130
                },
                {
                    "start": 131,
                    "end": 239
                },
                {
                    "start": 240,
                    "end": 610
                },
                {
                    "start": 611,
                    "end": 925
                },
                {
                    "start": 926,
                    "end": 1055
                },
                {
                    "start": 1056,
                    "end": 1144
                },
                {
                    "start": 1145,
                    "end": 1377
                },
                {
                    "start": 1378,
                    "end": 1558
                },
                {
                    "start": 1559,
                    "end": 1689
                },
                {
                    "start": 1690,
                    "end": 1808
                },
                {
                    "start": 1809,
                    "end": 1958
                },
                {
                    "start": 1959,
                    "end": 2165
                },
                {
                    "start": 2166,
                    "end": 2295
                }
            ],
            "ref_mentions": [
                {
                    "start": 438,
                    "end": 442,
                    "matchedPaperCorpusId": "259203115"
                },
                {
                    "start": 1155,
                    "end": 1159,
                    "matchedPaperCorpusId": "255372747"
                },
                {
                    "start": 1384,
                    "end": 1388,
                    "matchedPaperCorpusId": "259203115"
                },
                {
                    "start": 1813,
                    "end": 1817,
                    "matchedPaperCorpusId": "271745835"
                },
                {
                    "start": 1964,
                    "end": 1968,
                    "matchedPaperCorpusId": "266818263"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.87548828125
        },
        {
            "corpus_id": "263828939",
            "title": "The Cost of Down-Scaling Language Models: Fact Recall Deteriorates before In-Context Learning",
            "text": "Pruning algorithms. We investigate pruning as one possible technique to (down-)scale LLMs. Few pruning algorithms currently scale to LLMs. We use SparseGPT (Frantar & Alistarh, 2023) in the main text and Wanda (Sun et al., 2023) in Appendix F. Both are one-shot pruning algorithms that scale to LLMs and outperform magnitude pruning (i.e., pruning the smallest magnitude weights), without computationally intensive re-training (Frantar & Alistarh, 2023). SparseGPT/Wanda prune each layer of the language model by minimizing the \u2113 2 -distance between the outputs of the original dense layer and the pruned layer. SparseGPT/Wanda computes these outputs based on a small training dataset. See Frantar & Alistarh (2023, Sec. 3) for more details. While SparseGPT update the remaining weights after weights removal, Wanda does not. \n\nFollowing standard practice (Frantar & Alistarh, 2023;Frankle & Carbin, 2019), we only prune fully-connected layers. Since attention and feed forward modules consist of mostly fully-connected layers, parameters in fully-connected layers account for > 97.5% parameters for all models we examine. We do not prune embedding layers, language modeling heads and normalization layers. \n\nModels. We evaluate 6 models from 3 families: OPT (Zhang et al., 2022), LLaMA (Touvron et al., 2023) and Pythia (Biderman et al., 2023b). We focus on OPT and LLaMA in our main text and present Pythia results in Appendix G. Pythia family models show consistent results as LLaMA and OPT family models. From the OPT family, we evaluate the two largest models that fit in our hardware setup -OPT-13B and OPT-30B, with 13 and 30 billion parameters, respectively. From the LLaMA family, we evaluate LLaMA-13B and LLaMA-33B, with 13 and 33 billion parameters, respectively. \n\nA notable difference between OPT and LLaMA families is the ratio of training data to model parameters.",
            "score": 0.8199802723167496,
            "section_title": "Preliminaries",
            "char_start_offset": 10510,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 19
                },
                {
                    "start": 20,
                    "end": 90
                },
                {
                    "start": 91,
                    "end": 138
                },
                {
                    "start": 139,
                    "end": 454
                },
                {
                    "start": 455,
                    "end": 611
                },
                {
                    "start": 612,
                    "end": 685
                },
                {
                    "start": 686,
                    "end": 741
                },
                {
                    "start": 742,
                    "end": 825
                },
                {
                    "start": 828,
                    "end": 944
                },
                {
                    "start": 945,
                    "end": 1122
                },
                {
                    "start": 1123,
                    "end": 1206
                },
                {
                    "start": 1209,
                    "end": 1216
                },
                {
                    "start": 1217,
                    "end": 1346
                },
                {
                    "start": 1347,
                    "end": 1508
                },
                {
                    "start": 1509,
                    "end": 1666
                },
                {
                    "start": 1667,
                    "end": 1775
                },
                {
                    "start": 1778,
                    "end": 1880
                }
            ],
            "ref_mentions": [
                {
                    "start": 882,
                    "end": 905,
                    "matchedPaperCorpusId": "53388625"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.72412109375
        },
        {
            "corpus_id": "266362404",
            "title": "Fluctuation-based Adaptive Structured Pruning for Large Language Models",
            "text": "We assessed the zero-shot capability of the pruned model across seven downstream tasks. As illustrated in Table 2, our method consistently outperforms LLM-Pruner with LoRA Fine-Tuning, achieving superior performance across varying pruning ratios, all without the need for retraining. At a 20% pruning ratio, Wanda-sp exhibits remarkable zero-shot capabilities, even surpassing the performance of the original, unpruned model. This suggests the presence of structured redundancy within LLMs that can be pruned away without necessitating retraining, thereby potentially enhancing model efficiency. However, when the pruning ratio is increased to 50%, the performance of Wanda-sp suffers a significant degradation. In stark contrast, our method continues to excel, maintaining a clear advantage over other approaches. This finding demonstrates the efficacy of our structured pruning method in preserving the generalization capabilities of large language models (LLMs), even under stringent pruning conditions.",
            "score": 0.8191317890556897,
            "section_title": "Zero-shot Tasks Performance",
            "char_start_offset": 17417,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 87
                },
                {
                    "start": 88,
                    "end": 283
                },
                {
                    "start": 284,
                    "end": 425
                },
                {
                    "start": 426,
                    "end": 595
                },
                {
                    "start": 596,
                    "end": 711
                },
                {
                    "start": 712,
                    "end": 814
                },
                {
                    "start": 815,
                    "end": 1006
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.88671875
        },
        {
            "corpus_id": "269043120",
            "title": "Lightweight Deep Learning for Resource-Constrained Environments: A Survey",
            "text": "Some approaches [219,236] focus on implementing quantization, where numerical precision is greatly reduced.SparseGPT [55] demonstrates, for the first time, that large-scale Generative Pre-trained Transformer (GPT) models can be pruned to at least 50% sparsity in a single step, without any subsequent retraining, with minimal loss of accuracy.Following this, Wanda (Pruning by Weights and Activations) [181], specifically designed to induce sparsity in pre-trained LLMs, is introduced.Wanda prunes weights with the smallest magnitudes and does not require retraining or weight updates.The pruned LLM can be directly utilized, increasing its practicality.Notably, Wanda surpasses the established baseline of magnitude pruning and competes effectively with recent methods that involve extensive weight updates.These works set a significant milestone for future work in designing LLM pruning methods that do not require retraining.",
            "score": 0.8166067726401818,
            "section_title": "Building lightweight Large Language Models",
            "char_start_offset": 97708,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 107
                },
                {
                    "start": 107,
                    "end": 343
                },
                {
                    "start": 343,
                    "end": 485
                },
                {
                    "start": 485,
                    "end": 585
                },
                {
                    "start": 585,
                    "end": 654
                },
                {
                    "start": 654,
                    "end": 808
                },
                {
                    "start": 808,
                    "end": 928
                }
            ],
            "ref_mentions": [
                {
                    "start": 21,
                    "end": 25,
                    "matchedPaperCorpusId": "259858768"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.77197265625
        },
        {
            "corpus_id": "265050936",
            "title": "Beyond Size: How Gradients Shape Pruning Decisions in Large Language Models",
            "text": "Pruning methods stand out as notably simple and efficient mechanisms for model compression, serving to eliminate weights contingent on their significance. Reduced models can be conveniently dispatched to edge devices, and also exhibit substantially Preprint lower energy consumption, a sizable portion of energy is expended in transferring model parameters from a device's long-term storage to its memory (Dao et al., 2022). \n\nHowever, given the constraints of training-free conditions, existing solutions for pruning LLMs primarily employ either weight magnitude (Han et al., 2015a;2016) or a combination of magnitude and activation (Frantar & Alistarh, 2023;Sun et al., 2023). While these methods are substantiated with empirical ablations and experiments, they are, to a degree, either too complex to use like SparseGPT by computing matrix inverses and updating weights, or heuristic and lack profound theoretical justification like Wanda, especially concerning the application to the recently developed, highly advanced large language models. \n\nIn this study, we tackle the aforementioned complexity and interpretability challenges in LLM pruning methods by presenting a simple yet effective approach named GBLM-Pruner (Gradient-Based Language Model Pruner) that can be well explained in theory using the adapted optimal brain surgeon (OBS) (Hassibi et al., 1993b). This method proficiently prunes LLMs to significant levels of sparsity, eliminating the necessity to alter the residual weights. Specifically, we employ normalization of gradients across various samples to formulate an indicator matrix. This matrix can serve as activations and can either replace or supplement them. This method maintains simplicity over SparseGPT (Frantar & Alistarh, 2023) while showcasing enhanced robustness and improved interpretation than Wanda (Sun et al., 2023) on large language models. Furthermore, it is notable that although we employ gradients in our approach, there is no necessity for retraining or any updates to parameters. \n\nDifference to Previous Gradient-based Methods.",
            "score": 0.8147035048850163,
            "section_title": "Introduction",
            "char_start_offset": 1606,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 154
                },
                {
                    "start": 155,
                    "end": 424
                },
                {
                    "start": 427,
                    "end": 678
                },
                {
                    "start": 679,
                    "end": 1046
                },
                {
                    "start": 1049,
                    "end": 1369
                },
                {
                    "start": 1370,
                    "end": 1498
                },
                {
                    "start": 1499,
                    "end": 1606
                },
                {
                    "start": 1607,
                    "end": 1686
                },
                {
                    "start": 1687,
                    "end": 1882
                },
                {
                    "start": 1883,
                    "end": 2027
                },
                {
                    "start": 2030,
                    "end": 2076
                }
            ],
            "ref_mentions": [
                {
                    "start": 405,
                    "end": 423,
                    "matchedPaperCorpusId": "249151871"
                },
                {
                    "start": 564,
                    "end": 583,
                    "matchedPaperCorpusId": "2238772"
                },
                {
                    "start": 1345,
                    "end": 1368,
                    "matchedPaperCorpusId": "61815367"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.70263671875
        },
        {
            "corpus_id": "268041812",
            "title": "SparseLLM: Towards Global Pruning of Pre-trained Language Models",
            "text": "Large language models (LLMs) Touvron et al. [2023], OpenAI [2023] have recently transformed the field of natural language processing (NLP) by delivering exceptional results across a variety of intricate language benchmarks Wei et al. [2022], Bommarito II and Katz [2022], Bubeck et al. [2023]. Nonetheless, these models, with billions of parameters, generally necessitate significant computational resources. To make LLMs more accessible, extensive efforts have been devoted to model compression of LLMs Xu and McAuley [2023], Bai et al. [2024a], including pruning, quantization, knowledge distillation, and low-rank factorization. Pruning, by introducing sparsity, jointly enhances memory and computational efficiency and offers unparalleled flexibility, seamlessly integrating with any LLMs, thus standing out as a highly effective and widely adopted compression strategy. \n\nModel pruning has a long history LeCun et al. [1989] and has proven effective in applications related to vision and smaller language models Hoefler et al. [2021]. However, conventional pruning techniques, which rely on global pruning and require loading the entire model into the same GPU Mallya and Lazebnik [2018], Singh and Alistarh [2020], become impractical for today's LLMs due to their vast size. Recently, several local pruning methods have been proposed for billion-scale LLMs. These methods compress each layer separately, and the overall compressed model is then obtained by \"stitching together\" the individually compressed layers. SparseGPT Frantar and Alistarh [2023], an efficient unstructured pruning method for LLMs with hundreds of billions of parameters, achieves parameter reduction of up to 60% with minimal performance loss. Another approach, Wanda Sun et al. [2023], introduces a novel pruning criterion that evaluates weights by considering both magnitude 38th Conference on Neural Information Processing Systems (NeurIPS 2024). arXiv:2402.17946v4 [cs.CL] 31 Oct 2024 and related input activations.",
            "score": 0.8067699911616415,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 293
                },
                {
                    "start": 294,
                    "end": 408
                },
                {
                    "start": 409,
                    "end": 631
                },
                {
                    "start": 632,
                    "end": 874
                },
                {
                    "start": 877,
                    "end": 1039
                },
                {
                    "start": 1040,
                    "end": 1280
                },
                {
                    "start": 1281,
                    "end": 1363
                },
                {
                    "start": 1364,
                    "end": 1519
                },
                {
                    "start": 1520,
                    "end": 1722
                },
                {
                    "start": 1723,
                    "end": 1928
                },
                {
                    "start": 1929,
                    "end": 1947
                },
                {
                    "start": 1948,
                    "end": 1998
                }
            ],
            "ref_mentions": [
                {
                    "start": 504,
                    "end": 525,
                    "matchedPaperCorpusId": "254069544"
                },
                {
                    "start": 910,
                    "end": 929,
                    "matchedPaperCorpusId": "7785881"
                },
                {
                    "start": 1017,
                    "end": 1038,
                    "matchedPaperCorpusId": "231740691"
                },
                {
                    "start": 1166,
                    "end": 1192,
                    "matchedPaperCorpusId": "35249701"
                },
                {
                    "start": 1194,
                    "end": 1219,
                    "matchedPaperCorpusId": "220364055"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.80859375
        },
        {
            "corpus_id": "270257857",
            "title": "Pruner-Zero: Evolving Symbolic Pruning Metric from scratch for Large Language Models",
            "text": "The development of Large Language Models (LLMs) has witnessed a surge in model and dataset sizes, necessitating distributed training across numerous devices (Tang et al., 2020;2023;2024).This distributed approach, while effective, demands substantial computational and storage resources, with LLMs incurring higher energy costs compared to their smaller counterparts (Luccioni et al., 2023;Schwartz et al., 2020;Tang et al., 2019).Consequently, energy-efficient LLM training and inference are crucial for green computing, with LLM pruning emerging as a key technique for achieving this goal.Post-training pruning, in particular, has gained prominence due to its minimal resource requirements, making it a cost-effective approach for democratizing access to LLMs (Lu et al., 2022;Frantar & Alistarh, 2023;Sun et al., 2024).This method's efficiency and accessibility contribute significantly to the broader impact and applicability of LLMs.\n\nNetwork Pruning Network pruning is an effective technique for reducing model complexity while preserving performance, although it often requires extensive retraining.However, traditional pruning methods (Hoang et al., 2023;Sreenivasan et al., 2022;Liu et al., 2019;Chen et al., 2023;Chijiwa et al., 2021) become impractical when dealing with the substantial parameter sizes and vast datasets of Large Language Models (LLMs).Deep Compression (Han et al., 2016a) popularized magnitude-based pruning for deep neural networks, which removes the weights with the smallest absolute values, assuming that they have the least impact on the network's output.Network pruning techniques can be broadly categorized into two main approaches: unstructured pruning and structured pruning.\n\n(1) Unstructured Pruning involves removing individual weights or connections based on certain criteria.SparseGPT (Frantar & Alistarh, 2023) is the first post-training quantization method that performs unstructured pruning using an approximated Hessian matrix.",
            "score": 0.8055739914830715,
            "section_title": "A. Related Work",
            "char_start_offset": 33826,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 187
                },
                {
                    "start": 187,
                    "end": 431
                },
                {
                    "start": 431,
                    "end": 591
                },
                {
                    "start": 591,
                    "end": 822
                },
                {
                    "start": 822,
                    "end": 938
                },
                {
                    "start": 940,
                    "end": 1106
                },
                {
                    "start": 1106,
                    "end": 1364
                },
                {
                    "start": 1364,
                    "end": 1589
                },
                {
                    "start": 1589,
                    "end": 1713
                },
                {
                    "start": 1715,
                    "end": 1818
                },
                {
                    "start": 1818,
                    "end": 1974
                }
            ],
            "ref_mentions": [
                {
                    "start": 367,
                    "end": 390,
                    "matchedPaperCorpusId": "253265387"
                },
                {
                    "start": 412,
                    "end": 430,
                    "matchedPaperCorpusId": "166228547"
                },
                {
                    "start": 762,
                    "end": 779,
                    "matchedPaperCorpusId": "251648051"
                },
                {
                    "start": 1163,
                    "end": 1188,
                    "matchedPaperCorpusId": "247084008"
                },
                {
                    "start": 1205,
                    "end": 1223,
                    "matchedPaperCorpusId": "257495957"
                },
                {
                    "start": 1223,
                    "end": 1244,
                    "matchedPaperCorpusId": "235458499"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.61962890625
        },
        {
            "corpus_id": "259203115",
            "title": "A Simple and Effective Pruning Approach for Large Language Models",
            "text": "As their size increases, Large Languages Models (LLMs) are natural candidates for network pruning methods: approaches that drop a subset of network weights while striving to preserve performance. Existing methods, however, require either retraining, which is rarely affordable for billion-scale LLMs, or solving a weight reconstruction problem reliant on second-order information, which may also be computationally expensive. In this paper, we introduce a novel, straightforward yet effective pruning method, termed Wanda (Pruning by Weights and activations), designed to induce sparsity in pretrained LLMs. Motivated by the recent observation of emergent large magnitude features in LLMs, our approach prunes weights with the smallest magnitudes multiplied by the corresponding input activations, on a per-output basis. Notably, Wanda requires no retraining or weight update, and the pruned LLM can be used as is. We conduct a thorough evaluation of our method Wanda on LLaMA and LLaMA-2 across various language benchmarks. Wanda significantly outperforms the established baseline of magnitude pruning and performs competitively against recent method involving intensive weight update. Code is available at https://github.com/locuslab/wanda.",
            "score": 0.8037036472019136,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.88818359375
        },
        {
            "corpus_id": "270370902",
            "title": "VTrans: Accelerating Transformer Compression with Variational Information Bottleneck based Pruning",
            "text": "Post-Prune uses post-training pruning framework (Aguilar et al., 2020) that proposes a Fisher-based technique to train masks for identification of redundant neurons and selectively fine-tunes only the masks to specific tasks.\n\nWe show the scaling ability of our pruning technique to prune large language models like LLaMA-2 (Touvron et al., 2023).Comparison with previous techniques includes SparseGPT (Frantar & Alistarh, 2023) which proposes a second-order layer-wise pruning method that approximates closed form equations thus being able to scale up pruning LLMs.Wanda (Sun et al., 2023) takes into account the norm of weights and input activations for pruning weights in an unstructured/structured manner.Bonsai (Dery et al., 2024) is a gradient-free structured pruning method that estimates module importance perturbatively by generating sub-models and evaluating their performances.LLM-pruner (Ma et al., 2023) is a structured pruning method that uses gradient information to prune large language models in a task-agnostic manner.",
            "score": 0.7969145063855094,
            "section_title": "A.2 Further details about the baseline models",
            "char_start_offset": 25157,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 225
                },
                {
                    "start": 227,
                    "end": 347
                },
                {
                    "start": 347,
                    "end": 566
                },
                {
                    "start": 566,
                    "end": 709
                },
                {
                    "start": 709,
                    "end": 888
                },
                {
                    "start": 888,
                    "end": 1036
                }
            ],
            "ref_mentions": [
                {
                    "start": 48,
                    "end": 70,
                    "matchedPaperCorpusId": "203953149"
                },
                {
                    "start": 402,
                    "end": 428,
                    "matchedPaperCorpusId": "255372747"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.85302734375
        },
        {
            "corpus_id": "271909582",
            "title": "LLM-Barber: Block-Aware Rebuilder for Sparsity Mask in One-Shot for Large Language Models",
            "text": "Large language models (LLMs) have become a cornerstone in natural language processing (NLP) due to their impressive performance on various tasks. However, as these models increase in size and complexity, their deployment poses significant challenges due to extensive computational and storage demands. For instance, models such as GPT-175B (Brown et al. 2020), with 175 billion parameters, require vast resources, making it impractical for many applications. Therefore, efficient model compression strategies are crucial for deploying these powerful models in practical applications. \n\nModel compression techniques commonly employ quantization and pruning to enhance model efficiency. Quantization reduces the precision of model parameters, while pruning removes less critical parameters. Traditional pruning methods, such as magnitude-based pruning (Han et al. 2015), directly trim weights based on their absolute values. While effective for smaller models, these methods often struggle with large-scale LLMs, resulting in suboptimal sparsity due to their inability to capture the complex interactions and importance of weights in such massive scalability. To address these limitations, recent post-training pruning techniques such as SparseGPT and Wanda have emerged. \n\nCurrent pruning methods face two major challenges. First, as depicted in Figure 1(a), traditional layer-aware pruning methods focus on individual layers and neglect inter-layer dependencies within, leading to higher error accumulation (represented by blue arrows). In contrast, block-aware pruning, by considering groups of layers, captures inter-layer interactions to reduce error accumulation (represented by orange arrows). Second, as shown in Figure 1(b), conventional methods typically build the pruning mask once, ignoring the changes of weight significance in post-pruning stage. This oversight can lead to improper identification of salient weights, resulting in performance degradation. \n\nTo address these limitations, we propose LLM-Barber, a novel and straightforward approach designed to rebuild sparsity mask of pruned networks without requiring for retraining or weight reconstruction. Firstly, unlike layer-aware methods that are confined to local optimization and thus arXiv:2408.10631v1",
            "score": 0.7722434355896809,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 145
                },
                {
                    "start": 146,
                    "end": 301
                },
                {
                    "start": 302,
                    "end": 458
                },
                {
                    "start": 459,
                    "end": 583
                },
                {
                    "start": 586,
                    "end": 684
                },
                {
                    "start": 685,
                    "end": 788
                },
                {
                    "start": 789,
                    "end": 922
                },
                {
                    "start": 923,
                    "end": 1157
                },
                {
                    "start": 1158,
                    "end": 1269
                },
                {
                    "start": 1272,
                    "end": 1322
                },
                {
                    "start": 1323,
                    "end": 1536
                },
                {
                    "start": 1537,
                    "end": 1698
                },
                {
                    "start": 1699,
                    "end": 1858
                },
                {
                    "start": 1859,
                    "end": 1967
                },
                {
                    "start": 1970,
                    "end": 2171
                },
                {
                    "start": 2172,
                    "end": 2275
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.8818359375
        },
        {
            "corpus_id": "270621063",
            "title": "Dependency-Aware Semi-Structured Sparsity of GLU Variants in Large Language Models",
            "text": "Recent years have witnessed the great success of Large Language Models (LLMs) across various challenging tasks, such as mathematical reasoning, code generation. However, the practical use of these models for inference has faced a major obstacle due to the substantial computational resources they consume. To tackle this, many of the key developments up to now have revolved around weight quantization. It is possible to quantize LLMs down to 4 bits per weight with little impact on accuracy, which aids in memory reduction and speeds up inference (Lin et al., 2024). Nonetheless, maintaining accuracy becomes problematic when quantizing to around 3 bits per weight with existing methods (Dettmers et al., 2024;Egiazarian et al., 2024). \n\nA complementary method is neural network pruning (Han et al., 2015b), which can be combined with quantization to further improve the inference efficiency of LLMs (Kurtic et al., 2023;Frantar & Alistarh, 2023). Pruning can be categorized into two main approaches: unstructured pruning (Sun et al., 2024;Frantar & Alistarh, 2023), which involves the removal of specific weights, and structured pruning (Ma et al., 2023), which entails the removal of complete rows or columns of weights. In contrast to structured pruning, which struggles with performance in LLMs even at low sparsity levels, unstructured pruning methods like SparseGPT (Frantar & Alistarh, 2023) and Wanda (Sun et al., 2024) exhibit promising results without additional retraining, and achieves practical speedup in both CPU and GPU through the recent engineering advancements (Agarwalla et al., 2024). They also have the benefit in reducing hallucinations of LLMs (Chrysostomou et al., 2024).",
            "score": 0.7711575102976529,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 160
                },
                {
                    "start": 161,
                    "end": 305
                },
                {
                    "start": 306,
                    "end": 402
                },
                {
                    "start": 403,
                    "end": 567
                },
                {
                    "start": 568,
                    "end": 736
                },
                {
                    "start": 739,
                    "end": 948
                },
                {
                    "start": 949,
                    "end": 1223
                },
                {
                    "start": 1224,
                    "end": 1606
                },
                {
                    "start": 1607,
                    "end": 1697
                }
            ],
            "ref_mentions": [
                {
                    "start": 548,
                    "end": 566,
                    "matchedPaperCorpusId": "271271084"
                },
                {
                    "start": 688,
                    "end": 711,
                    "matchedPaperCorpusId": "259076379"
                },
                {
                    "start": 788,
                    "end": 807,
                    "matchedPaperCorpusId": "2238772"
                },
                {
                    "start": 1023,
                    "end": 1041,
                    "matchedPaperCorpusId": "259203115"
                },
                {
                    "start": 1139,
                    "end": 1156,
                    "matchedPaperCorpusId": "258823276"
                },
                {
                    "start": 1410,
                    "end": 1428,
                    "matchedPaperCorpusId": "259203115"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.7197265625
        },
        {
            "corpus_id": "267412232",
            "title": "Beyond the Limits: A Survey of Techniques to Extend the Context Length in Large Language Models",
            "text": "Pruning can help optimize the model for deployment and make the model more efficient in terms of computation complexity and memory usage. Accordingly, pruning can be considered as an approach to enable a language model to support longer sequence length, while maintaining the desirable complexity and performance. In general, pruning a model can be categorized into structured and unstructured pruning. \n\nStructured pruning aims at removing higher-granularity structures, such as entire neurons, layers, or rows/columns of weight matrices, which can result in a model that retains its original structure but with fewer number of parameters. LLM-Pruner [Ma et al., 2023a] is a structural task-agnostic pruning approach that selectively removes non-critical connection structures considering both first-order information and an approximated Hessian information gradient information. As an alternative, Sheared LLaMA [Xia et al., 2024] utilizes a two-stage approach for pruning an LLM. In the first stage, it exploits targeted structured pruning to prune a large model to a targeted shape by pruning layers, heads, and intermediate connections. In the second stage, the batches of data are loaded dynamically and the model structure is modified in each training iteration based on losses in various domains. As a result, Sheared LLaMA achieves a compressed model that can outperform the LLMs, with the same size but trained from scratch. \n\nUnstructured pruning involves with pruning individual parameters of a model independently based on their magnitudes or importance, resulting in an irregular sparse structure. Due to the irregularity in the structure and in the memory access patterns, unstructured pruning hinders the efficiency gain that might be achieved through structured pruning, and it requires specialized software and/or hardware for efficient deployment. SparseGPT [Frantar and Alistarh, 2023] compresses LLMs with billions of parameter by as much as 60%, almost without affecting the performance of the models. However, SparseGPT heavily relies on weight updates.",
            "score": 0.7687203874595878,
            "section_title": "Model Compression",
            "char_start_offset": 28041,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 137
                },
                {
                    "start": 138,
                    "end": 313
                },
                {
                    "start": 314,
                    "end": 402
                },
                {
                    "start": 405,
                    "end": 640
                },
                {
                    "start": 641,
                    "end": 880
                },
                {
                    "start": 881,
                    "end": 982
                },
                {
                    "start": 983,
                    "end": 1141
                },
                {
                    "start": 1142,
                    "end": 1304
                },
                {
                    "start": 1305,
                    "end": 1434
                },
                {
                    "start": 1437,
                    "end": 1611
                },
                {
                    "start": 1612,
                    "end": 1866
                },
                {
                    "start": 1867,
                    "end": 2023
                },
                {
                    "start": 2024,
                    "end": 2076
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.80078125
        },
        {
            "corpus_id": "270257857",
            "title": "Pruner-Zero: Evolving Symbolic Pruning Metric from scratch for Large Language Models",
            "text": "Complementing this, Wanda (Sun et al., 2024) streamlines the process by simplifying SparseGPT's methodology.Additionally, GBLM-Pruner (Das et al., 2023) employs the first-order term of the Taylor expansion, emphasizing the significance of gradients.Structured pruning methods, such as activation pruning and neuron/filter output statistics, are used for GPU acceleration.LLM-Pruner (Ma et al., 2023) examines model dependencies, incorporating both first-order and approximated Hessian information.LLM Surgeon (van der Ouderaa et al., 2024) adapts Kronecker-factored curvature approximations to LLMs, targeting 20%-25% low sparsity.In this paper, our focus is on the post-training pruning of language models without retraining or weight updates.\n\nEfficient and Low Resource Compression.Due to the large size of language models, there is an increasing demand for efficient LLM compression without using the original training data.As for efficient compression, (Kwon et al., 2022) accelerates the post-training by defining the reconstruction error as a linear least squares problem.SparseGPT (Frantar & Alistarh, 2023) and GPTQ (Frantar et al., 2023) propose the layer-wise optimal brain surgeon.Due to the constraint of availability of the training corpus, data-free methods (Srinivas & Babu, 2015;Yvinec et al., 2021) prune the neural network by measuring the similarity of neurons.Most related to our approach is pruning with limited data, which requires no modification to the original training procedure and no retraining of the pruned network on the full training datasets.To mitigate the accuracy drop, a layer-wise reconstruction problem is involved to minimize the change of output evaluated on the calibration data.",
            "score": 0.7672646634909062,
            "section_title": "Language Model Pruning",
            "char_start_offset": 9579,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 108
                },
                {
                    "start": 108,
                    "end": 249
                },
                {
                    "start": 249,
                    "end": 371
                },
                {
                    "start": 371,
                    "end": 497
                },
                {
                    "start": 497,
                    "end": 631
                },
                {
                    "start": 631,
                    "end": 744
                },
                {
                    "start": 746,
                    "end": 785
                },
                {
                    "start": 785,
                    "end": 928
                },
                {
                    "start": 928,
                    "end": 1079
                },
                {
                    "start": 1079,
                    "end": 1193
                },
                {
                    "start": 1193,
                    "end": 1381
                },
                {
                    "start": 1381,
                    "end": 1576
                },
                {
                    "start": 1576,
                    "end": 1722
                }
            ],
            "ref_mentions": [
                {
                    "start": 958,
                    "end": 977,
                    "matchedPaperCorpusId": "248266822"
                },
                {
                    "start": 1273,
                    "end": 1296,
                    "matchedPaperCorpusId": "15647317"
                },
                {
                    "start": 1296,
                    "end": 1316,
                    "matchedPaperCorpusId": "238259421"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.71728515625
        },
        {
            "corpus_id": "277275922",
            "title": "Efficient self-attention with smart pruning for sustainable large language models",
            "text": "The compression of language models has gained significant attention, leading to the development of various methods, including network pruning 12 , knowledge distillation, and quantization 13 . LLM-Pruner 14 and FLAP 15 aim to reduce network width by pruning coupled structures. Sheared-LLaMA 16 takes a more comprehensive approach, reducing network width and depth by removing entire layers. While methods that address both width and depth aspects exist 17,18 , there is still a need for in-depth analysis comparing the impact of these factors on LLM inference efficiency. Applying traditional pruning techniques to LLMs presents unique challenges due to their vast number of parameters and substantial computational requirements 19 . Pruning methods for LLMs can be broadly categorized into unstructured and structured approaches. \n\nStructured pruning methods 20 focus on removing entire groups of parameters, maintaining dense weight matrices, and improving hardware efficiency. Techniques such as LLM-Pruner 14 and LoRAPrune 11 emphasize efficient deployment and inference acceleration. Sheared-LLaMA 16 aims to prune models to a target architecture and train them dynamically. The work of Tao introduces a novel compression technique called QuantGPT. This method focused on quantization and module adaptation, allowing for systematic compression while maintaining the integrity of the model's architecture and performance 25 . LightPAFF focuses on transferring knowledge from a more prominent (teacher) model to a smaller (student) model. This process inherently involves structuring the model to retain important features learned by the teacher rather than simply removing individual weights, as seen in unstructured pruning 21 . Unstructured pruning methods 22 target individual weights, maintaining performance. Notable examples include SparseGPT 23 , which employs sophisticated weight updates and pruning without retraining. Edalati et al. developed KnGPT2 for compressing the linear mappings of the GPT-2 model, focusing on reducing the number of parameters flexibly without drastically altering the overall architecture. This technique allows for representing weight matrices in a more compact form while maintaining performance, which aligns with the characteristics of unstructured pruning 24 . \n\nStructure pruning typically removes entire groups of parameters, such as whole neurons, channels, or even layers, which can limit its flexibility 25 .",
            "score": 0.7655658228038681,
            "section_title": "Related work",
            "char_start_offset": 4650,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 192
                },
                {
                    "start": 193,
                    "end": 277
                },
                {
                    "start": 278,
                    "end": 391
                },
                {
                    "start": 392,
                    "end": 572
                },
                {
                    "start": 573,
                    "end": 734
                },
                {
                    "start": 735,
                    "end": 831
                },
                {
                    "start": 834,
                    "end": 980
                },
                {
                    "start": 981,
                    "end": 1089
                },
                {
                    "start": 1090,
                    "end": 1180
                },
                {
                    "start": 1181,
                    "end": 1254
                },
                {
                    "start": 1255,
                    "end": 1430
                },
                {
                    "start": 1431,
                    "end": 1542
                },
                {
                    "start": 1543,
                    "end": 1734
                },
                {
                    "start": 1735,
                    "end": 1818
                },
                {
                    "start": 1819,
                    "end": 1933
                },
                {
                    "start": 1934,
                    "end": 2131
                },
                {
                    "start": 2132,
                    "end": 2307
                },
                {
                    "start": 2310,
                    "end": 2460
                }
            ],
            "ref_mentions": [
                {
                    "start": 216,
                    "end": 218,
                    "matchedPaperCorpusId": "266362404"
                },
                {
                    "start": 457,
                    "end": 459,
                    "matchedPaperCorpusId": "256662263"
                },
                {
                    "start": 730,
                    "end": 732,
                    "matchedPaperCorpusId": "160025533"
                },
                {
                    "start": 861,
                    "end": 863,
                    "matchedPaperCorpusId": "235899080"
                },
                {
                    "start": 1426,
                    "end": 1428,
                    "matchedPaperCorpusId": "204009154"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.74169921875
        },
        {
            "corpus_id": "270257857",
            "title": "Pruner-Zero: Evolving Symbolic Pruning Metric from scratch for Large Language Models",
            "text": "Implementation Details.\n\nIn this section, we primarily assess the effectiveness of our Pruner-Zero on two of the most extensively used LLM families: LLaMA 7B/13B/30B/65B (Touvron et al., 2023a) and LLaMA-2-7B/13B/70B (Touvron et al., 2023b).To further explore the generalizability of Pruner-Zero, we apply the developed symbolic pruning metric to earlier LLM families, such as OPT (Zhang et al., 2022b) and Tiny-LLaMA (Zhang et al., 2024a), as detailed in Appendix E.4.The performance of the pruned models is evaluated in terms of language modeling and zero-shot tasks.For language modeling, we follow the established protocols in LLM compression research (Sun et al., 2024;Frantar & Alistarh, 2023) to assess the perplexity on the WikiText2 (Merity et al., 2017) validation set.For zero-shot tasks, we utilize seven tasks from the EleutherAI LM Harness (Gao et al., 2021).\n\nCounterparts.In comparing Pruner-Zero with established Post-training Pruning methods, magnitude pruning (Han et al., 2015) serves as a baseline, removing weights based on the absolute value of magnitude.SparseGPT (Frantar & Alistarh, 2023) introduces second-order pruning for LLMs, incorporating layer-wise reconstruction, while Wanda (Sun et al., 2024) simplifies this by using a diagonal approximation, avoiding matrix inverse calculations.Our approach avoids the computationally expensive Hessian matrix and aligns with magnitude pruning and Wanda in forgoing retraining or weight updates during pruning.Unlike SparseGPT and Wanda, which require calibration data, we use WikiText2 to estimate input statistics like Gradients (G) and Activations (X).\n\nSparsity.We adopt a uniform sparsity across all linear layers in unstructured pruning settings, maintaining consistency with the approach used by Wanda.",
            "score": 0.7626994409473062,
            "section_title": "Models and Implementation Details",
            "char_start_offset": 20549,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 23
                },
                {
                    "start": 25,
                    "end": 241
                },
                {
                    "start": 241,
                    "end": 469
                },
                {
                    "start": 469,
                    "end": 569
                },
                {
                    "start": 569,
                    "end": 779
                },
                {
                    "start": 779,
                    "end": 873
                },
                {
                    "start": 875,
                    "end": 888
                },
                {
                    "start": 888,
                    "end": 1078
                },
                {
                    "start": 1078,
                    "end": 1317
                },
                {
                    "start": 1317,
                    "end": 1482
                },
                {
                    "start": 1482,
                    "end": 1627
                },
                {
                    "start": 1629,
                    "end": 1638
                },
                {
                    "start": 1638,
                    "end": 1781
                }
            ],
            "ref_mentions": [
                {
                    "start": 742,
                    "end": 763,
                    "matchedPaperCorpusId": "16299141"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.791015625
        },
        {
            "corpus_id": "273532238",
            "title": "Beware of Calibration Data for Pruning Large Language Models",
            "text": "Recently, Large Language Models (LLMs) have exhibited remarkable performance and enormous potential in Natural Language Processing (NLP) and Artificial Intelligence (AI) (OpenAI, 2022;2023;Bubeck et al., 2023;Yang et al., 2023). The success of LLMs is closely tied to scaling laws (Kaplan et al., 2020;Hoffmann et al., 2022): training language models with more parameters, using more data and greater computational resources leads to more powerful capabilities. However, LLMs with more parameters increase the difficulty and cost of deployment and inference. Therefore, much work has been devoted to compressing LLMs to achieve a trade-off between efficiency and performance, such as pruning (Frantar & Alistarh, 2023;Ma et al., 2023;Xia et al., 2024) and quantization (Frantar et al., 2023;Huang et al., 2024;Shao et al., 2024). \n\nPruning is a model compression technique that has evolved over many years (LeCun et al., 1989) and remains full of potential and challenges. Based on the over-parameterization of neural networks, it aims to remove redundant parameters while minimizing the degradation of model performance. Pruning has been successfully applied to compress small to medium-sized neural networks. Through sparse training (Lee et al., 2019;Frankle & Carbin, 2019;Yuan et al., 2021;Lasby et al., 2024) or pruning-aware training (Sanh et al., 2020;Lagunas et al., 2021;Jiang et al., 2023) methods, it can achieve performance comparable to dense models with a high sparsity ratio (\u226570%). However, these methods require iterative training, which is costly and time-consuming for LLMs with billions of parameters. As a result, post-training pruning that does not require iterative training has become the preferred approach for pruning LLMs. \n\nThe challenge of post-training pruning is how to perform training-free parameter importance estimation.",
            "score": 0.7609987749984238,
            "section_title": "INTRODUCTION",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 228
                },
                {
                    "start": 229,
                    "end": 461
                },
                {
                    "start": 462,
                    "end": 558
                },
                {
                    "start": 559,
                    "end": 829
                },
                {
                    "start": 832,
                    "end": 972
                },
                {
                    "start": 973,
                    "end": 1121
                },
                {
                    "start": 1122,
                    "end": 1210
                },
                {
                    "start": 1211,
                    "end": 1497
                },
                {
                    "start": 1498,
                    "end": 1621
                },
                {
                    "start": 1622,
                    "end": 1749
                },
                {
                    "start": 1752,
                    "end": 1855
                }
            ],
            "ref_mentions": [
                {
                    "start": 718,
                    "end": 734,
                    "matchedPaperCorpusId": "258823276"
                },
                {
                    "start": 734,
                    "end": 751,
                    "matchedPaperCorpusId": "263830786"
                },
                {
                    "start": 769,
                    "end": 791,
                    "matchedPaperCorpusId": "255372747"
                },
                {
                    "start": 791,
                    "end": 810,
                    "matchedPaperCorpusId": "267523201"
                },
                {
                    "start": 810,
                    "end": 828,
                    "matchedPaperCorpusId": "261214575"
                },
                {
                    "start": 906,
                    "end": 925,
                    "matchedPaperCorpusId": "7785881"
                },
                {
                    "start": 1235,
                    "end": 1253,
                    "matchedPaperCorpusId": "52920837"
                },
                {
                    "start": 1253,
                    "end": 1276,
                    "matchedPaperCorpusId": "53388625"
                },
                {
                    "start": 1276,
                    "end": 1294,
                    "matchedPaperCorpusId": "239998338"
                },
                {
                    "start": 1294,
                    "end": 1313,
                    "matchedPaperCorpusId": "258461498"
                },
                {
                    "start": 1340,
                    "end": 1359,
                    "matchedPaperCorpusId": "218665313"
                },
                {
                    "start": 1359,
                    "end": 1380,
                    "matchedPaperCorpusId": "237485472"
                },
                {
                    "start": 1380,
                    "end": 1399,
                    "matchedPaperCorpusId": "252846209"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.587890625
        },
        {
            "corpus_id": "271957102",
            "title": "Investigating Language-Specific Calibration For Pruning Multilingual Large Language Models",
            "text": "Overall, these results suggest that vocabulary size and pre-training data impact baseline performance and accuracy after pruning in a multilingual context. \n\nBetween Wanda and SparseGPT, Llama-3 8B's performance degrades less after pruning with SparseGPT pruning, while the performance of the pruned Aya-23 8B model varies, that is, excelling with Wanda or SparseGPT depends on the task. Specifically, SparseGPT produces a distinct diagonal pattern for Aya-23 8B on ARC and HellaSwag, while Wanda often yields superior performance for calibration with non-target languages on Belebele and MMLU. No pruning technique consistently performs best in all tasks.",
            "score": 0.7566365798441912,
            "section_title": "DOWNSTREAM TASK PERFORMANCE",
            "char_start_offset": 17666,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 155
                },
                {
                    "start": 158,
                    "end": 387
                },
                {
                    "start": 388,
                    "end": 594
                },
                {
                    "start": 595,
                    "end": 656
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.410888671875
        },
        {
            "corpus_id": "271083368",
            "title": "Achieving Peak Performance for Large Language Models: A Systematic Review",
            "text": "Background: LLMs like GPT-3 have billions of parameters, which pose significant challenges in terms of storage, computational requirements, and energy consumption. Pruning, or removing less important parameters, can help mitigate these issues, but traditional pruning methods often require multiple iterations of fine-tuning, which is computationally expensive. This approach (SparseGPT [72]) proposes a one-shot pruning method that significantly reduces the number of parameters without the need for extensive retraining. \n\nContext and Problem: In this case study, the focus is on training a LLM with billions of parameters on limited hardware. The initial challenge was the high computational and memory requirements that exceeded the capabilities of available resources, making it difficult to efficiently train the model within a reasonable timeframe and budget. \n\nOptimization Strategy: The primary optimization strategies involved in SparseGPT are: \n\nOne-Shot Pruning: To achieve significant sparsity in the LLM in a single pruning step, eliminating the need for iterative pruning and retraining. One-Shot Pruning: SparseGPT implements its pruning strategy through a streamlined process. First, a thorough model analysis is conducted to pinpoint parameters that can be removed without significant impact. This analysis leverages pruning criteria that assess parameter importance without requiring gradient calculations, saving on computational resources. Finally, SparseGPT employs a single step pruning approach, achieving substantial sparsity (at least 50% for massive models) in a single step. This oneshot approach significantly reduces the time and complexity compared to iterative pruning methods. \n\nUnstructured Sparsity: To reduce the number of parameters while maintaining model accuracy through unstructured pruning, where individual weights are removed based on their importance. This approach focuses on eliminating individual weights within the model that are deemed less important. By analyzing the model's internal structure, SparseGPT achieves impressive sparsity levels of 50-60%, significantly reducing model size. This aggressive pruning strategy is remarkable because it achieves this with minimal impact on the model's ability to perform language modeling tasks accurately. For instance, SparseGPT can remove over 100 billion weights from massive models like OPT-175B and BLOOM-176B without compromising their performance on language modeling tasks.",
            "score": 0.750640398579751,
            "section_title": "A. OPTIMIZING MODEL TRAINING WITH SPARSEGPT",
            "char_start_offset": 113010,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 163
                },
                {
                    "start": 164,
                    "end": 361
                },
                {
                    "start": 362,
                    "end": 522
                },
                {
                    "start": 525,
                    "end": 645
                },
                {
                    "start": 646,
                    "end": 866
                },
                {
                    "start": 869,
                    "end": 954
                },
                {
                    "start": 957,
                    "end": 1102
                },
                {
                    "start": 1103,
                    "end": 1193
                },
                {
                    "start": 1194,
                    "end": 1310
                },
                {
                    "start": 1311,
                    "end": 1460
                },
                {
                    "start": 1461,
                    "end": 1602
                },
                {
                    "start": 1603,
                    "end": 1709
                },
                {
                    "start": 1712,
                    "end": 1896
                },
                {
                    "start": 1897,
                    "end": 2001
                },
                {
                    "start": 2002,
                    "end": 2138
                },
                {
                    "start": 2139,
                    "end": 2300
                },
                {
                    "start": 2301,
                    "end": 2476
                }
            ],
            "ref_mentions": [
                {
                    "start": 387,
                    "end": 391,
                    "matchedPaperCorpusId": "255372747"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.8583984375
        },
        {
            "corpus_id": "268032346",
            "title": "BESA: Pruning Large Language Models with Blockwise Parameter-Efficient Sparsity Allocation",
            "text": "Large language models (LLMs) have demonstrated remarkable performance in a wide range of NLP tasks, including language modeling, code generation, machine translation, sentiment analysis, and question answering (Zhang et al., 2022a;Touvron et al., 2023a;b;Xu et al., 2023;Team, 2023;Zeng et al., 2022). However, LLMs have a vast number of parameters, resulting in high memory consumption and slow inference speed (Dettmers et al., 2022). For example, it requires 335GB GPU memory (i.e. five A100 GPU with 80G memory) to load its parameters in FP16 of Falcon-180B (Penedo et al., 2023), which corresponds to the inference speed of merely 4 tokens per second. Thus, there has been considerable interest in compressing LLMs to make LLMs more efficient and practical for deployment in various applications. \n\nOne of the approaches to compress a network is weight pruning. Although it has a long history in model compression (Hassibi et al., 1993;Hassibi & Stork, 1992), few pieces of work can be used to prune LLMs due to the requirement of extensive retraining. Recent studies, such as SparseGPT (Frantar & Alistarh, 2023) and Wanda (Sun et al., 2023) aim to tackle this challenge by reconstructing the layer-wise output of LLMs, as illustrated in Fig. 1(c). Specifically, SparseGPT proposes to prune unimportant with an importance metric derived from the hessian matrix. and then reconstruct layer-wise output. Moreover, Wanda removes intricate computation in SparseGPT by only leveraging the product of weight and activation magnitudes. \n\nWhile these approaches can eliminate considerable unnecessary weights, they typically operate within each weight by minimizing each layer's pruning error, which has two drawbacks.",
            "score": 0.7388924224958191,
            "section_title": "INTRODUCTION",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 301
                },
                {
                    "start": 302,
                    "end": 436
                },
                {
                    "start": 437,
                    "end": 484
                },
                {
                    "start": 485,
                    "end": 656
                },
                {
                    "start": 657,
                    "end": 801
                },
                {
                    "start": 804,
                    "end": 866
                },
                {
                    "start": 867,
                    "end": 1057
                },
                {
                    "start": 1058,
                    "end": 1254
                },
                {
                    "start": 1255,
                    "end": 1367
                },
                {
                    "start": 1368,
                    "end": 1407
                },
                {
                    "start": 1408,
                    "end": 1534
                },
                {
                    "start": 1537,
                    "end": 1716
                }
            ],
            "ref_mentions": [
                {
                    "start": 919,
                    "end": 941,
                    "matchedPaperCorpusId": "61815367"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.6533203125
        },
        {
            "corpus_id": "278327238",
            "title": "Efficient Shapley Value-based Non-Uniform Pruning of Large Language Models",
            "text": "Uniform Pruning. Traditional pruning requires a round of re-training to restore performance, which poses significant challenges for LLMs. Researchers have developed pruning algorithms specifically tailored for LLM compression. For instance, [9] investigated structured sparse LLMs by applying Taylor pruning to remove entire weight rows, followed by LoRA fine-tuning [13]. In recent years, the focus has shifted toward unstructured pruning which eliminates the need for fine-tuning. SparseGPT [4] employs the Hessian inverse for pruning, followed by weight updates to reduce reconstruction errors between dense and sparse weights. Wanda [14] introduced a criterion that incorporates weight magnitude and input activations to preserve outlier features. \n\nNon-uniform Pruning. Uniform layerwise sparsity is commonly used for pruning language models [36; 42], with several studies demonstrating its effectiveness in LLM pruning [43; 44]. However, there is a growing body of work exploring non-uniform layerwise sparsity, primarily in the context of vision models. For example, [15] proposed a non-uniform, scale-free topology inspired by graph theory, which outperforms dense counterparts when applied to restricted Boltzmann machines. Subsequent work has improved the scalability of this approach by leveraging Erd\u0151s-R\u00e9nyi graphs [16], extending the method to fully connected layers [11] and convolutional layers [17; 18] to achieve data-free and feedforward-free layerwise sparsity. Another approach to non-uniform sparsity involves applying a global threshold across all layers [19; 20; 21; 22]. However, global pruning has been found to be computationally expensive and ineffective when applied to LLMs. \n\nAnalyzing LLMs. The authors in [23] analyzed the contributions of various components in LLMs and their impact on overall performance. [24] explored the role of deep layers in LLMs through layer pruning, providing insights into how different layers in relation to model performance. [25] examined the redundancy of attention heads in transformer-based models, demonstrating that many attention heads can be pruned without significant performance degradation.",
            "score": 0.7347879033213737,
            "section_title": "A Extende Related Work",
            "char_start_offset": 23991,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 16
                },
                {
                    "start": 17,
                    "end": 137
                },
                {
                    "start": 138,
                    "end": 226
                },
                {
                    "start": 227,
                    "end": 372
                },
                {
                    "start": 373,
                    "end": 482
                },
                {
                    "start": 483,
                    "end": 630
                },
                {
                    "start": 631,
                    "end": 751
                },
                {
                    "start": 754,
                    "end": 774
                },
                {
                    "start": 775,
                    "end": 934
                },
                {
                    "start": 935,
                    "end": 1060
                },
                {
                    "start": 1061,
                    "end": 1232
                },
                {
                    "start": 1233,
                    "end": 1481
                },
                {
                    "start": 1482,
                    "end": 1595
                },
                {
                    "start": 1596,
                    "end": 1704
                },
                {
                    "start": 1707,
                    "end": 1722
                },
                {
                    "start": 1723,
                    "end": 1840
                },
                {
                    "start": 1841,
                    "end": 1988
                },
                {
                    "start": 1989,
                    "end": 2164
                }
            ],
            "ref_mentions": [
                {
                    "start": 241,
                    "end": 244,
                    "matchedPaperCorpusId": "258823276"
                },
                {
                    "start": 493,
                    "end": 496,
                    "matchedPaperCorpusId": "255372747"
                },
                {
                    "start": 637,
                    "end": 641,
                    "matchedPaperCorpusId": "259203115"
                },
                {
                    "start": 1074,
                    "end": 1078,
                    "matchedPaperCorpusId": "2090605"
                },
                {
                    "start": 1328,
                    "end": 1332,
                    "matchedPaperCorpusId": "4506156"
                },
                {
                    "start": 1381,
                    "end": 1385,
                    "matchedPaperCorpusId": "49310977"
                },
                {
                    "start": 1738,
                    "end": 1742,
                    "matchedPaperCorpusId": "251648872"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.82666015625
        },
        {
            "corpus_id": "271909421",
            "title": "Enhancing One-shot Pruned Pre-trained Language Models through Sparse-Dense-Sparse Mechanism",
            "text": "To illustrate the enhanced efficiency of pruned models, we present the inference speed of dense and sparse models on AMD CPU. We use the DeepSparse library [12] and apply 50% unstructured pruning on OPTs and LLaMAs in this experiment. Table 7 indicates that the pruned models can achieve 1.19x \u223c 1.87x speedup compared to their dense counterparts. This significant boost in inference speed underscores the critical importance of model pruning in practical applications. Pruning for Language Model Compression. The surging complexity of Transformer-based language models, which now feature hundreds of billions of parameters, has accentuated the urgent need for effective and efficient model pruning methods [6,8,9]. These pruning methods can be broadly classified into structured and unstructured approaches. Structured pruning is more hardware-friendly, as it directly prunes entire segments of weights, thereby removing consecutive computations [17,14]. Unstructured pruning (sparsification) is also receiving interest, particularly as hardware advancements increasingly support the acceleration of sparse patterns such as 2:4 or 4:8 sparse [20]. Techniques such as SparseGPT [3] extend the OBS [9] methodology to column-wise prune weights, allowing the modification of values in the unpruned columns to compensate for the pruning errors. Aaquib et al. [28] enhance SparseGPT by incorporating minimal iterative task fine-tuning during the pruning process, demonstrating performance improvements at high sparsity levels. Wanda [27] introduces a simple yet effective no-retraining-needed pruning strategy that prunes weights based on their magnitudes and corresponding activations. OWL [39] considers the inhomogeneous distribution of interlayer outliers and further extends Wanda to a non-uniform sparsity distribution, achieving better performance. DS\u2205T [42] and SPP [16], as fine-tuning methods designed for sparse models, can improve the performance of pruned PLMs within limited complexity. \n\nWeight Distribution Optimization. Various techniques have been employed to understand and optimize weight distributions in the quest for more efficient neural networks.",
            "score": 0.7339763874579239,
            "section_title": "Efficiency Analysis",
            "char_start_offset": 19063,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 125
                },
                {
                    "start": 126,
                    "end": 234
                },
                {
                    "start": 235,
                    "end": 347
                },
                {
                    "start": 348,
                    "end": 469
                },
                {
                    "start": 470,
                    "end": 509
                },
                {
                    "start": 510,
                    "end": 715
                },
                {
                    "start": 716,
                    "end": 808
                },
                {
                    "start": 809,
                    "end": 955
                },
                {
                    "start": 956,
                    "end": 1148
                },
                {
                    "start": 1149,
                    "end": 1340
                },
                {
                    "start": 1341,
                    "end": 1521
                },
                {
                    "start": 1522,
                    "end": 1681
                },
                {
                    "start": 1682,
                    "end": 1850
                },
                {
                    "start": 1851,
                    "end": 1995
                },
                {
                    "start": 1998,
                    "end": 2031
                },
                {
                    "start": 2032,
                    "end": 2166
                }
            ],
            "ref_mentions": [
                {
                    "start": 707,
                    "end": 710,
                    "matchedPaperCorpusId": "2134321"
                },
                {
                    "start": 710,
                    "end": 712,
                    "matchedPaperCorpusId": "2238772"
                },
                {
                    "start": 712,
                    "end": 714,
                    "matchedPaperCorpusId": "61815367"
                },
                {
                    "start": 951,
                    "end": 954,
                    "matchedPaperCorpusId": "260815690"
                },
                {
                    "start": 1178,
                    "end": 1181,
                    "matchedPaperCorpusId": "255372747"
                },
                {
                    "start": 1197,
                    "end": 1200,
                    "matchedPaperCorpusId": "61815367"
                },
                {
                    "start": 1355,
                    "end": 1359,
                    "matchedPaperCorpusId": "259950394"
                },
                {
                    "start": 1686,
                    "end": 1690,
                    "matchedPaperCorpusId": "263829692"
                },
                {
                    "start": 1856,
                    "end": 1860,
                    "matchedPaperCorpusId": "264128029"
                },
                {
                    "start": 1869,
                    "end": 1873,
                    "matchedPaperCorpusId": "270063400"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.7431640625
        },
        {
            "corpus_id": "270391791",
            "title": "ALPS: Improved Optimization for Highly Sparse One-Shot Pruning for Large Language Models",
            "text": "Large Language Models (LLMs) have revolutionized the field of natural language processing, demonstrating remarkable performance across a wide spectrum of tasks, from question answering and text generation to sentiment analysis and named entity recognition [Wei et al., 2022, Bubeck et al., 2023, Achiam et al., 2023]. The success of LLMs can in part be attributed to their massive scale-state-ofthe-art models like GPT-3 [Brown et al., 2020] and OPT-175B [Zhang et al., 2022a] have hundreds of billions of parameters. However, this enormous size comes at a steep cost in terms of storage and computational resources. For instance, the OPT-175B model requires at least 320 GB of memory to store its parameters in half-precision (FP16) format, necessitating the use of multiple high-end GPUs for inference [Frantar and Alistarh, 2023]. To make LLMs more accessible and efficient, considerable efforts have been made to compress these models, with a particular emphasis on model quantization techniques [Lin et al., 2023, Behdin et al., 2023, Dettmers et al., 2023]. \n\nNetwork pruning [LeCun et al., 1989, Hassibi and Stork, 1992, Han et al., 2015], a complementary approach to quantization, has received comparatively less attention in the realm of LLMs. Pruning aims to reduce the model size by identifying and removing redundant or less important weights, resulting in a sparser and more efficient network. Traditional pruning methods rely on iterative retraining to recover accuracy after each pruning stage [Han et al., 2015, Luo et al., 2017, Molchanov et al., 2016, Liu et al., 2018], which can be computationally expensive and time-consuming.",
            "score": 0.7329327493494142,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 317
                },
                {
                    "start": 318,
                    "end": 517
                },
                {
                    "start": 518,
                    "end": 616
                },
                {
                    "start": 617,
                    "end": 833
                },
                {
                    "start": 834,
                    "end": 1063
                },
                {
                    "start": 1066,
                    "end": 1252
                },
                {
                    "start": 1253,
                    "end": 1406
                },
                {
                    "start": 1407,
                    "end": 1647
                }
            ],
            "ref_mentions": [
                {
                    "start": 421,
                    "end": 441,
                    "matchedPaperCorpusId": "218971783"
                },
                {
                    "start": 804,
                    "end": 832,
                    "matchedPaperCorpusId": "251765570"
                },
                {
                    "start": 1082,
                    "end": 1101,
                    "matchedPaperCorpusId": "7785881"
                },
                {
                    "start": 1101,
                    "end": 1126,
                    "matchedPaperCorpusId": "7057040"
                },
                {
                    "start": 1126,
                    "end": 1145,
                    "matchedPaperCorpusId": "2238772"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.51611328125
        },
        {
            "corpus_id": "276408266",
            "title": "MaZO: Masked Zeroth-Order Optimization for Multi-Task Fine-Tuning of Large Language Models",
            "text": "F.1 Wanda: Pruning by Weights and Activations \n\nIn this section, we introduce Wanda (Pruning by Weights and Activations), a simple yet effective method for pruning large language models (LLMs). Wanda can induce high sparsity in pretrained LLMs without requiring retraining or weight updates, making it computationally efficient and easy to implement. \n\nThe key idea of Wanda is to evaluate the importance of each weight based on both its magnitude and the corresponding input activation. Specifically, for a linear layer with weight matrix W \u2208 R Cout\u00d7C in and input activations X \u2208 R (N \u2022L)\u00d7C in , the importance score S ij of weight W ij is defined as: \n\nwhere |W ij | is the absolute value of the weight, and \u2225X j \u2225 2 is the L 2 norm of the j-th column of X, aggregated across all tokens in the batch and sequence dimensions. This metric effectively com-bines weight magnitude and input activation information to determine the importance of each weight. Unlike traditional pruning methods that compare weights globally or layer-wise, Wanda adopts a peroutput comparison strategy (the same as our rowwise comparison). For a weight W ij connecting input j to output i, its comparison group is defined as all weights connected to the same output i: \n\nWithin each comparison group, weights are ranked by their importance scores S ij , and a predefined sparsity ratio s% is applied to prune the lowestranked weights.",
            "score": 0.731333890291126,
            "section_title": "F Details of Different Weight Score Metrics",
            "char_start_offset": 34708,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 45
                },
                {
                    "start": 48,
                    "end": 193
                },
                {
                    "start": 194,
                    "end": 350
                },
                {
                    "start": 353,
                    "end": 487
                },
                {
                    "start": 488,
                    "end": 653
                },
                {
                    "start": 656,
                    "end": 827
                },
                {
                    "start": 828,
                    "end": 955
                },
                {
                    "start": 956,
                    "end": 1118
                },
                {
                    "start": 1119,
                    "end": 1247
                },
                {
                    "start": 1250,
                    "end": 1413
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.77783203125
        },
        {
            "corpus_id": "268032346",
            "title": "BESA: Pruning Large Language Models with Blockwise Parameter-Efficient Sparsity Allocation",
            "text": "Compression of Large Language Models. Numerous technologies aim to mitigate the memory and computation demands of Large Language Models (LLMs). These techniques can be broadly categorized into two primary types: quantization (Frantar et al., 2022;Lin et al., 2023;Shao et al., 2023) and pruning (Sun et al., 2023;Frantar & Alistarh, 2023;Ma et al., 2023). Quantization converts full-precision values to low-bit representations, while pruning selectively eliminates insignificant weights. These two compression strategies are distinct but can be synergistically combined to enhance the compression ratio (Frantar et al., 2022;Kim et al., 2023). In this paper, we focus on impelling the performance of LLM pruning. \n\nPruning of Large Language Models. Pruning methods for neural networks can be broadly classified into structured pruning (Ma et al., 2023;Huang et al., 2020) and unstructured pruning (Frantar et al., 2022;Sun et al., 2023;Zhang et al., 2023;2022b). Conventional techniques such as those in (Huang et al., 2020;Zhang et al., 2023) are ill-suited for LLMs due to their reliance on extensive retraining, a challenge amplified within the context of LLMs. In contrast, LLM-specific pruning methods emphasize data and time efficiency. Regarding structured pruning, LLMpruner (Ma et al., 2023) delves into the structured pruning of LLMs and employs LoRA to recuperate the performance of pruned models. In the realm of unstructured pruning, SparseGPT (Frantar & Alistarh, 2023) introduces an efficient technique for estimating the Hessian matrix, thereby adapting the traditional OBS approach (Hassibi et al., 1993) to large-scale models. Furthermore, Wanda (Sun et al., 2023) adopts a straightforward strategy, eliminating weights based on the product of weight and activation values.",
            "score": 0.7291689466892779,
            "section_title": "RELATED WORK",
            "char_start_offset": 5765,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 37
                },
                {
                    "start": 38,
                    "end": 143
                },
                {
                    "start": 144,
                    "end": 355
                },
                {
                    "start": 356,
                    "end": 487
                },
                {
                    "start": 488,
                    "end": 643
                },
                {
                    "start": 644,
                    "end": 712
                },
                {
                    "start": 715,
                    "end": 748
                },
                {
                    "start": 749,
                    "end": 962
                },
                {
                    "start": 963,
                    "end": 1164
                },
                {
                    "start": 1165,
                    "end": 1242
                },
                {
                    "start": 1243,
                    "end": 1408
                },
                {
                    "start": 1409,
                    "end": 1644
                },
                {
                    "start": 1645,
                    "end": 1791
                }
            ],
            "ref_mentions": [
                {
                    "start": 936,
                    "end": 955,
                    "matchedPaperCorpusId": "233297054"
                },
                {
                    "start": 1024,
                    "end": 1043,
                    "matchedPaperCorpusId": "233297054"
                },
                {
                    "start": 1599,
                    "end": 1621,
                    "matchedPaperCorpusId": "61815367"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.7744140625
        },
        {
            "corpus_id": "273350958",
            "title": "Beyond Linear Approximations: A Novel Pruning Approach for Attention Matrix",
            "text": "Large Language Models (LLMs) have shown immense potential in enhancing various aspects of our daily lives, from conversational AI to search and AI assistants. However, their growing capabilities come at the cost of extremely large model sizes, making deployment on edge devices challenging due to memory and computational constraints. This paper introduces a novel approach to LLM weight pruning that directly optimizes for approximating the attention matrix, a core component of transformer architectures. Unlike existing methods that focus on linear approximations, our approach accounts for the non-linear nature of the Softmax attention mechanism. We provide theoretical guarantees for the convergence of our Gradient Descent-based optimization method to a near-optimal pruning mask solution. Our empirical results demonstrate the effectiveness of our non-linear pruning approach in maintaining model performance while significantly reducing computational costs, which is beyond the current state-of-the-art methods, i.e., SparseGPT and Wanda, by a large margin. This work establishes a new theoretical foundation for pruning algorithm design in LLMs, potentially paving the way for more efficient LLM inference on resource-constrained devices.",
            "score": 0.7278518971213453,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.7255859375
        },
        {
            "corpus_id": "266374655",
            "title": "A Performance Evaluation of a Quantized Large Language Model on Various Smartphones",
            "text": "Pruning can be defined as the reduction in size and processing requirements of an LLM by removing redundant or ineffective elements (weights, neurons, and/or layers) that vary according to the method. Not only can pruning notably reduce LLM size, but the removal of redundant/inefficient elements also results in an acceleration of the inference mechanism. A critical point to consider regarding this method is whether training is required either during or after pruning [8]. Cutting-edge pruning methodologies, such as SparseGTP [9] and Wanda [10], can achieve success even at high sparsity without the need for re-training. Due to integration challenges, similar to KD, pruning techniques were not employed in our current study.",
            "score": 0.7246527980563533,
            "section_title": "Pruning",
            "char_start_offset": 3995,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 200
                },
                {
                    "start": 201,
                    "end": 356
                },
                {
                    "start": 357,
                    "end": 475
                },
                {
                    "start": 476,
                    "end": 625
                },
                {
                    "start": 626,
                    "end": 730
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.7177734375
        },
        {
            "corpus_id": "277043299",
            "title": "Towards Extreme Pruning of LLMs with Plug-and-Play Mixed Sparsity",
            "text": "Despite the great success of large language models (LLMs) [1,2], the deployment of LLMs is very challenging because the immense parameter size requires significant memory and computational resources. One effective solution of accelerating LLMs is model pruning or model sparsity [3,4,5], which uses the inherent redundancy in LLMs to prune less important parameters without compromising performance. Among pruning techniques, post-training pruning [5,6,7], which removes redundant parameters after model training, is particularly critical. It allows for the retention of pre-trained knowledge while significantly reducing computational complexity. This approach offers a practical pathway for optimizing LLMs, striking a balance between efficiency and effectiveness. \n\nExisting LLM pruning methods usually focus on designing an effective metric that determines the importance of network components in guiding pruning. For example, Wanda [7] computes the weight importance as the element-wise product between the weight magnitude and the norm of input activations. RIA [8] proposes relative importance and combines with activations as a new metric based on the fact that welltrained LLMs contain unique information in the input and output channels. Pruner-zero [9] uses evolutionary algorithms to find the optimal metric for pruning. Though these methods with various importance metrics achieve great success on sparsity ratio 50% or smaller, their performance drops significantly on the higher ratio (e.g. \u2265 75%). For example, the perplexity of LLaMA-13B pruned with Wanda [7] is 9.59 at 50% sparsity, whereas it drastically increases to 13418.32 at 75% sparsity. \n\nThe limitation of existing methods on very high sparsity ratio inspires us to revisit pruning methods from a fundamentally different perspective. Specifically, we observe that existing methods make an implicit assumption that the whole neural network shares an identical sparsity ratio like 50% across different layers. Intuitively, the ratio of unimportant weights in different layers can be different. Actually, existing methods have verified the soundness of our intuition. For example, the work [10] shows that the first four and last two layers are much more important than intermediate layers of a LLM.",
            "score": 0.7203949994049741,
            "section_title": "Introduction",
            "char_start_offset": 82,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 199
                },
                {
                    "start": 200,
                    "end": 399
                },
                {
                    "start": 400,
                    "end": 539
                },
                {
                    "start": 540,
                    "end": 647
                },
                {
                    "start": 648,
                    "end": 766
                },
                {
                    "start": 769,
                    "end": 917
                },
                {
                    "start": 918,
                    "end": 1063
                },
                {
                    "start": 1064,
                    "end": 1247
                },
                {
                    "start": 1248,
                    "end": 1332
                },
                {
                    "start": 1333,
                    "end": 1505
                },
                {
                    "start": 1506,
                    "end": 1513
                },
                {
                    "start": 1514,
                    "end": 1663
                },
                {
                    "start": 1666,
                    "end": 1811
                },
                {
                    "start": 1812,
                    "end": 1985
                },
                {
                    "start": 1986,
                    "end": 2069
                },
                {
                    "start": 2070,
                    "end": 2142
                },
                {
                    "start": 2143,
                    "end": 2274
                }
            ],
            "ref_mentions": [
                {
                    "start": 58,
                    "end": 61,
                    "matchedPaperCorpusId": "218971783"
                },
                {
                    "start": 282,
                    "end": 284,
                    "matchedPaperCorpusId": "61815367"
                },
                {
                    "start": 451,
                    "end": 453,
                    "matchedPaperCorpusId": "255372747"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.7568359375
        },
        {
            "corpus_id": "276576138",
            "title": "Systematic Weight Evaluation for Pruning Large Language Models: Enhancing Performance and Sustainability",
            "text": "Methods like SparseGPT (Frantar and Alistarh, 2023) and Wanda (Sun et al., 2023) use sophisticated weight updates and pruning without retraining, respectively, while PST (Li et al., 2022) combines unstructured pruning with efficient fine-tuning. Structured pruning methods (Chen et al., 2021b(Chen et al., , 2023) ) remove entire groups of parameters, maintaining dense weight matrices and improving hardware efficiency. Techniques such as LLM-Pruner (Ma et al., 2023) and LoRAPrune (Zhang et al., 2023) focus on efficient deployment and inference acceleration, with Sheared-LLaMA (Xia et al., 2023) aiming to prune models to a target architecture and train them dynamically. Furthermore, the compression of language models has garnered significant attention, leading to various methods like network pruning, knowledge distillation, and quantization (Bai et al., 2020;Brown et al., 2020;Devlin, 2018). Pruning, especially structural pruning, remains a crucial focus due to its hardware-friendly nature, with methods varying from l1-dependent pruning (Zafrir et al., 2021) to more advanced techniques like the optimal brain surgeon (LeCun et al., 1989). Efficient compression and low-resource strategies are increasingly essential, with advancements in layer-wise optimal brain surgeon and data-free pruning approaches aiming to optimize the balance between compression efficiency and training data availability (Kurti\u0107 et al., 2024;Srinivas and Babu, 2015).",
            "score": 0.7191510010515965,
            "section_title": "Related Work",
            "char_start_offset": 5555,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 245
                },
                {
                    "start": 246,
                    "end": 420
                },
                {
                    "start": 421,
                    "end": 675
                },
                {
                    "start": 676,
                    "end": 901
                },
                {
                    "start": 902,
                    "end": 1152
                },
                {
                    "start": 1153,
                    "end": 1457
                }
            ],
            "ref_mentions": [
                {
                    "start": 23,
                    "end": 51,
                    "matchedPaperCorpusId": "255372747"
                },
                {
                    "start": 273,
                    "end": 292,
                    "matchedPaperCorpusId": "235899080"
                },
                {
                    "start": 451,
                    "end": 468,
                    "matchedPaperCorpusId": "258823276"
                },
                {
                    "start": 868,
                    "end": 887,
                    "matchedPaperCorpusId": "218971783"
                },
                {
                    "start": 1131,
                    "end": 1151,
                    "matchedPaperCorpusId": "7785881"
                },
                {
                    "start": 1411,
                    "end": 1432,
                    "matchedPaperCorpusId": "256662263"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.80859375
        },
        {
            "corpus_id": "277043299",
            "title": "Towards Extreme Pruning of LLMs with Plug-and-Play Mixed Sparsity",
            "text": "Language model pruning aims to reduce their computational and memory demands while preserving performance. Recent works have introduced innovative approaches in this domain. SparseGPT [6] formalizes the problem of pruning LLMs by solving a local layer-wise reconstruction problem, where their pruning metric and weight update procedure is inspired from Optimal Brain Surgeon (OBS) [4]. Wanda [13] streamlines the process by simplifying SparseGPT's [6] methodology, and explores the weight magnitude and activations as a criterion for pruning, offering a simple yet effective strategy to achieve high sparsity ratios. RIA [8] also focuses on metrics related to weights and activations but introduces channel permutation [14] to maximize the retention of important weights under N:M sparsity. Pruner Zero [9] employs evolutionary algorithms to discover optimal pruning metrics, providing a comprehensive framework for metric exploration. Despite the great success of these methods, their performance drops greatly at extreme sparsity ratios, e.g. 75%.",
            "score": 0.7179205746207113,
            "section_title": "Language Model Pruning",
            "char_start_offset": 4824,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 106
                },
                {
                    "start": 107,
                    "end": 173
                },
                {
                    "start": 174,
                    "end": 385
                },
                {
                    "start": 386,
                    "end": 616
                },
                {
                    "start": 617,
                    "end": 790
                },
                {
                    "start": 791,
                    "end": 935
                },
                {
                    "start": 936,
                    "end": 1044
                },
                {
                    "start": 1045,
                    "end": 1049
                }
            ],
            "ref_mentions": [
                {
                    "start": 184,
                    "end": 187,
                    "matchedPaperCorpusId": "255372747"
                },
                {
                    "start": 381,
                    "end": 384,
                    "matchedPaperCorpusId": "61815367"
                },
                {
                    "start": 392,
                    "end": 396,
                    "matchedPaperCorpusId": "248266822"
                },
                {
                    "start": 448,
                    "end": 451,
                    "matchedPaperCorpusId": "255372747"
                },
                {
                    "start": 719,
                    "end": 723,
                    "matchedPaperCorpusId": "245002847"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.81787109375
        },
        {
            "corpus_id": "263829692",
            "title": "Outlier Weighed Layerwise Sparsity (OWL): A Missing Secret Sauce for Pruning LLMs to High Sparsity",
            "text": "Pruning and LLM Pruning. Since the 1980s, network pruning has been a well-established technique for simplifying neural networks in various applications while maintaining accuracy (Mozer & Smolensky, 1989;Han et al., 2015;Mocanu et al., 2018;Wen et al., 2017;Lin et al., 2019). However, when it comes to pruning Large Language Models (LLMs), progress has been limited. Traditional pruning typically requires a round of re-training to restore performance, which can be challenging for LLMs. To address this challenge, researchers have developed pruning algorithms specifically tailored for LLM compression. For example, Ma et al. (2023) explored structured sparse LLMs using Taylor pruning to remove entire weight rows, followed by LoRA fine-tuning (Hu et al., 2021). Recent research has shifted toward unstructured pruning without the need for fine-tuning, showing substantial advancements. SparseGPT (Frantar & Alistarh, 2023) utilizes the Hessian inverse for pruning and with subsequent weight updates to reduce reconstruction error of dense and sparse weights, while Wanda (Sun et al., 2023) produces a criterion incorporating weight magnitude with their input activations, aiming to preserve outlier features. Zhang et al. (2023) extended dynamic sparsity (Mocanu et al., 2018;Evci et al., 2020;Liu et al., 2021b) to efficiently fine-tune sparse LLM without weight updating. Our work for the first time probes the crucial role of non-uniform layerwise sparsity for LLM pruning, making good progress in this field. \n\nLayerwise Sparsity for Pruning. While it is common to use uniform layerwise sparsity (Zhu & Gupta, 2017;Gale et al., 2019) to prune language models (Sanh et al., 2020;Kurtic et al., 2022), there is a well-established line of work that explore non-uniform layerwise sparsity in terms of pruning vision models.",
            "score": 0.7169535608128175,
            "section_title": "Related Work",
            "char_start_offset": 7414,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 24
                },
                {
                    "start": 25,
                    "end": 276
                },
                {
                    "start": 277,
                    "end": 367
                },
                {
                    "start": 368,
                    "end": 488
                },
                {
                    "start": 489,
                    "end": 604
                },
                {
                    "start": 605,
                    "end": 765
                },
                {
                    "start": 766,
                    "end": 889
                },
                {
                    "start": 890,
                    "end": 1212
                },
                {
                    "start": 1213,
                    "end": 1377
                },
                {
                    "start": 1378,
                    "end": 1516
                },
                {
                    "start": 1519,
                    "end": 1550
                },
                {
                    "start": 1551,
                    "end": 1827
                }
            ],
            "ref_mentions": [
                {
                    "start": 179,
                    "end": 204,
                    "matchedPaperCorpusId": "17651092"
                },
                {
                    "start": 204,
                    "end": 221,
                    "matchedPaperCorpusId": "2238772"
                },
                {
                    "start": 221,
                    "end": 241,
                    "matchedPaperCorpusId": "49310977"
                },
                {
                    "start": 258,
                    "end": 275,
                    "matchedPaperCorpusId": "85459412"
                },
                {
                    "start": 1259,
                    "end": 1280,
                    "matchedPaperCorpusId": "49310977"
                },
                {
                    "start": 1280,
                    "end": 1298,
                    "matchedPaperCorpusId": "208267757"
                },
                {
                    "start": 1298,
                    "end": 1316,
                    "matchedPaperCorpusId": "231839425"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.76025390625
        },
        {
            "corpus_id": "271903658",
            "title": "MoDeGPT: Modular Decomposition for Large Language Model Compression",
            "text": "Pruning In early pruning methods like magnitude-based tuning, scalability is achieved but often at the cost of reduced effectiveness in large language models (LLMs) (Hagiwara, 1994;Han et al., 2015;Li et al., 2017;Frantar & Alistarh, 2023;van der Ouderaa et al., 2023). To improve  performance while managing computational demands, frameworks such as Optimal Brain Damage (LeCun et al., 1989) and Surgeon (Hassibi et al., 1993;Yu et al., 2022;van der Ouderaa et al., 2023) incorporate second-order loss information, necessitating substantial resources for Hessian calculations. Recent adaptations like WoodFisher (Singh & Alistarh, 2020), Kronecker factorization (Wang et al., 2019a;van der Ouderaa et al., 2023), and layer-wise compression (Dong et al., 2017;Frantar & Alistarh, 2022) aim to streamline these intensive methods. Concurrently, learnable parameters for pruning in vision and language models have been investigated (Liu et al., 2017;Huang & Wang, 2018;Xia et al., 2022), although these techniques generally demand significant computational resources for intensive backward propagation. Other approaches, such as feature-mimic-based methods (An et al., 2024;Ji et al., 2024), have not matched the performance of gradient-based methods like LLM Surgeon (van der Ouderaa et al., 2023). Alternatives like SparseGPT (Frantar & Alistarh, 2023), Wanda (Sun et al., 2024), and ZeroPruner (Dong et al., 2024), exploring unstructured and semi-structured pruning, offer scalability but often compromise runtime speed. Additional research has utilized layer importance scores for layer pruning and sparsity distribution, as demonstrated by ShortGPT (Men et al., 2024), OWL (Yin et al., 2023), LaCo (Yang et al., 2024), and others (Chen et al., 2024).",
            "score": 0.7166072555435062,
            "section_title": "RELATED WORKS",
            "char_start_offset": 5358,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 269
                },
                {
                    "start": 270,
                    "end": 577
                },
                {
                    "start": 578,
                    "end": 828
                },
                {
                    "start": 829,
                    "end": 1099
                },
                {
                    "start": 1100,
                    "end": 1296
                },
                {
                    "start": 1297,
                    "end": 1520
                },
                {
                    "start": 1521,
                    "end": 1752
                }
            ],
            "ref_mentions": [
                {
                    "start": 165,
                    "end": 181,
                    "matchedPaperCorpusId": "25970113"
                },
                {
                    "start": 181,
                    "end": 198,
                    "matchedPaperCorpusId": "2238772"
                },
                {
                    "start": 372,
                    "end": 392,
                    "matchedPaperCorpusId": "7785881"
                },
                {
                    "start": 405,
                    "end": 427,
                    "matchedPaperCorpusId": "61815367"
                },
                {
                    "start": 427,
                    "end": 443,
                    "matchedPaperCorpusId": "247318543"
                },
                {
                    "start": 663,
                    "end": 683,
                    "matchedPaperCorpusId": "155089879"
                },
                {
                    "start": 741,
                    "end": 760,
                    "matchedPaperCorpusId": "5750817"
                },
                {
                    "start": 929,
                    "end": 947,
                    "matchedPaperCorpusId": "5993328"
                },
                {
                    "start": 947,
                    "end": 966,
                    "matchedPaperCorpusId": "575794"
                },
                {
                    "start": 1154,
                    "end": 1171,
                    "matchedPaperCorpusId": "266362404"
                },
                {
                    "start": 1359,
                    "end": 1377,
                    "matchedPaperCorpusId": "259203115"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.55029296875
        },
        {
            "corpus_id": "268041812",
            "title": "SparseLLM: Towards Global Pruning of Pre-trained Language Models",
            "text": "Li et al. Li et al. [2023] proposed LoSparse, a novel approach combining low-rank and sparse matrix approximations to balance pruning and expressive power. Tao et al. Tao et al. [2023] extended this concept to pruning hidden dimensions in LLMs, including embedding layers and attention heads. \n\nZipLM Kurtic et al. [2023], a structured pruning method for LLMs, is proposed to optimize for compression and accuracy while considering specific hardware constraints. More recently, Xia et al introduced LLM-shearing Xia et al. [2023], a structured pruning method that scales down LLaMA models by selectively pruning layers, heads, and dimensions. This approach, combined with dynamic data batching, reduces pre-training compute costs while maintaining competitive performance, outperforming similar open-source models on key tasks. \n\nOur work falls in the category of unstructured pruning of LLMs, where existing methods such as SparseGPT and Wanda only consider an entirely local pruning algorithm and suffer from suboptimal performance. We discuss the limitations and challenges of entirely local pruning in Sec. 3.",
            "score": 0.7161138588476132,
            "section_title": "Related work",
            "char_start_offset": 5848,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 155
                },
                {
                    "start": 156,
                    "end": 292
                },
                {
                    "start": 295,
                    "end": 462
                },
                {
                    "start": 463,
                    "end": 642
                },
                {
                    "start": 643,
                    "end": 827
                },
                {
                    "start": 830,
                    "end": 1034
                },
                {
                    "start": 1035,
                    "end": 1113
                }
            ],
            "ref_mentions": [
                {
                    "start": 156,
                    "end": 184,
                    "matchedPaperCorpusId": "259858812"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.826171875
        },
        {
            "corpus_id": "264405577",
            "title": "Breaking through Deterministic Barriers: Randomized Pruning Mask Generation and Selection",
            "text": "In the current age of large language models, achieving effective pruning is a formidable challenge, particularly when striving to preserve high sparsity without sacrificing performance. While initiatives like SparseGPT have ventured into pruning for these colossal models, they have only managed a 2x compression rate (Frantar and Alistarh, 2023). The computational complexity of our method is primarily determined by the number of parameters involved. Consequently, our random pruning technique is not yet adaptable to models with billions of parameters. Nevertheless, we are diligently working on refining methods that incorporate controllable randomness more efficiently.",
            "score": 0.7133211929298703,
            "section_title": "Extending to Billion Parameters",
            "char_start_offset": 24029,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 185
                },
                {
                    "start": 186,
                    "end": 347
                },
                {
                    "start": 348,
                    "end": 452
                },
                {
                    "start": 453,
                    "end": 555
                },
                {
                    "start": 556,
                    "end": 674
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.409423828125
        },
        {
            "corpus_id": "267312283",
            "title": "A Comprehensive Survey of Compression Algorithms for Language Models",
            "text": "Kwon et al. [81] is the first work that proposes a low-cost pruning framework for language models. Kwon et al. efficiently estimates the saliency of attention heads and neurons via the empirical Fisher Information Matrix (FIM) that reflects the amount of influence on the objective function (cross-entropy) following the derivations in OBD [84]. Kwon et al. formulate an efficient sublayer-wise weight-tuning process that takes shorter than a minute using linear solvers. KCM [110] addresses the inefficiencies of the method of Kwon et al. which requires expensive gradient computations for calculating FIM; KCM evaluates the importance of neurons using their representative powers without computing gradients. However, Kwon et al. and KCM show significant accuracy degradations at high compression rates due to their one-shot pruning process without iteration. Kprune [112] demonstrates the importance of an efficient iterative pruning process by achieving remarkable improvement up to 58% higher accuracy than previous works without losing the efficiency of low-cost pruning processes. \n\nDecoder-only models have not been extensively researched regarding low-cost pruning algorithms, especially for LLMs [136,137,173]. SparseGPT [40] succeeds in pruning massive-scale LLMs up to 175B on a single A100 GPU in three hours by maximizing the efficiency of the OBS [52] algorithm. We select SparseGPT as the representative pruning algorithm considering their contribution to LLM pruning and elaborate on details in Section 3.4. Wanda [163] shows comparable results with SparseGPT in a low compression rate of 50% using a significantly efficient unstructured pruning process without weight update, but it shows severe accuracy degradation in a high sparsity regime [129]. \n\nOWL [129] improves the accuracy of both Wanda and SparseGPT by replacing their sublayer-wise uniform pruning rates with the non-uniform pruning rates reflecting the outlier ratio in each sublayer. For structured pruning of LLMs, LLM-pruner [105] prunes attention heads and neurons using a Fisher Information matrix and uses LoRA [60] for error compensation.",
            "score": 0.7120484283129267,
            "section_title": "Pruning Strategies: High-cost vs. Low-cost",
            "char_start_offset": 30108,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 98
                },
                {
                    "start": 99,
                    "end": 345
                },
                {
                    "start": 346,
                    "end": 471
                },
                {
                    "start": 472,
                    "end": 710
                },
                {
                    "start": 711,
                    "end": 861
                },
                {
                    "start": 862,
                    "end": 1087
                },
                {
                    "start": 1090,
                    "end": 1220
                },
                {
                    "start": 1221,
                    "end": 1377
                },
                {
                    "start": 1378,
                    "end": 1524
                },
                {
                    "start": 1525,
                    "end": 1767
                },
                {
                    "start": 1770,
                    "end": 1966
                },
                {
                    "start": 1967,
                    "end": 2127
                }
            ],
            "ref_mentions": [
                {
                    "start": 12,
                    "end": 16,
                    "matchedPaperCorpusId": "248266822"
                },
                {
                    "start": 476,
                    "end": 481,
                    "matchedPaperCorpusId": "257404900"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.56982421875
        },
        {
            "corpus_id": "266362404",
            "title": "Fluctuation-based Adaptive Structured Pruning for Large Language Models",
            "text": "The recent LLM-Pruner (Ma, Fang, and Wang 2023) attempted structured pruning for LLMs, but its dependence on LoRA finetuning (Hu et al. 2021) creates a tough trade-off between high computation and effective pruning, limiting its use in larger models. \n\nPruning essentially involves two key aspects: discovering redundancy and recovering performance. For an effective structured pruning method tailored to LLMs, three fundamental criteria must be satisfied: a) a structured importance metric to discover structured redundancy; b) a mechanism for adaptively searching the optimal global compression model structure; and c) a compensation strategy to minimize performance degradation. \n\nIn response to these three essential criteria, we introduce FLAP (FLuctuation-based Adaptive Structured Pruning), a novel structured pruning framework. We find that certain channels of hidden state features exhibit structured sample stability. This observation enables us to compensate for bias within the model using baseline values. Specifically, we design a structured pruning metric that estimates the fluctuation of each input feature relative to the baseline value, utilizing a set of calibration samples. This metric assists in determining whether the output feature map can be recovered when a column of the weight matrix is removed. We then standardize these fluctuation metric scores across lay-ers and modules separately, allowing for the adaptive determination of the global compressed model structure. Finally, FLAP employs the baseline values to add additional biases, recovering the output feature maps for the corresponding layers. Remarkably, our method avoids the need for the retraining process and requires only a single forward pass for both pruning and bias compensation, thereby maintaining low memory overhead. \n\nWe evaluate the effectiveness of FLAP on the LLaMA model family, and FLAP achieves remarkable performance on a variety of language benchmarks. Impressively, without any retraining, our method significantly outperforms the state-of-the-art methods, including LLM-Pruner and the extension of Wanda in structured pruning. \n\nOur main contributions are listed as follows: \u2022 We propose a novel retraining-free structured pruning framework for LLMs named FLAP. To our best knowledge, this is the first work that identifies the characteristic of structured sample stability in LLMs.",
            "score": 0.7116801271107362,
            "section_title": "Introduction",
            "char_start_offset": 1708,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 250
                },
                {
                    "start": 253,
                    "end": 349
                },
                {
                    "start": 350,
                    "end": 681
                },
                {
                    "start": 684,
                    "end": 835
                },
                {
                    "start": 836,
                    "end": 927
                },
                {
                    "start": 928,
                    "end": 1018
                },
                {
                    "start": 1019,
                    "end": 1195
                },
                {
                    "start": 1196,
                    "end": 1325
                },
                {
                    "start": 1326,
                    "end": 1498
                },
                {
                    "start": 1499,
                    "end": 1631
                },
                {
                    "start": 1632,
                    "end": 1818
                },
                {
                    "start": 1821,
                    "end": 1963
                },
                {
                    "start": 1964,
                    "end": 2139
                },
                {
                    "start": 2142,
                    "end": 2274
                },
                {
                    "start": 2275,
                    "end": 2395
                }
            ],
            "ref_mentions": [
                {
                    "start": 22,
                    "end": 47,
                    "matchedPaperCorpusId": "17240902"
                },
                {
                    "start": 125,
                    "end": 140,
                    "matchedPaperCorpusId": "7785881"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.72119140625
        },
        {
            "corpus_id": "276250081",
            "title": "EfficientLLM: Scalable Pruning-Aware Pretraining for Architecture-Agnostic Edge Language Models",
            "text": "Scalability. According to Eq. 7, we set the ratio of pruning steps to gradient descent steps to 4:1, 2:1, 1:1, and 1:9 \n\nAccuracy (%) in a iteration, respectively. When the target model size is reached, the pruning-aware pretraining requires 2.5B, 4.5B, 8.4B, and 72.1B tokens of pretraining, respectively. Fig. 5 indicates that scaling up pruning-aware pretraining continuously improves pruning performance. Therefore, by scaling up LLM pruning during pretraining, the upper boundary of LLM compression can be extended. \n\nGeneralization. There is a large number of methods that perform post-training pruning based on second-order Taylor expansion, such as OBC and SparseGPT. As shown in Fig. 6, we generalize EfficientLLM to the second order updating case as EfficientLLM-B. Compared with post-training settings, EfficientLLM retains source model performance consistently in large pruning ratio. We observe similar performance of EfficientLLM-A/B in large scale pruning-aware pretrainning, but when the pruning data is small (< 1B), EfficientLLM-B significantly improves accuracy in Table 4. In this section, we conduct light-weight pruning-aware pretraining to compare with existing LLM pruning methods and ShearedLlama. Experiments reveal that only scaling up the pruning stage to 5B tokens can achieve much higher performance than what was possible previously (Table 3). \n\nTraditional LLM Pruning. We mainly focus on large pruning ratio because it is more practical to achieve highly efficiency based on heavy source LLMs. In Table 3, we scale up pruning-aware pretraining to only 5B tokens. We report both results with or without finetuning after pruning. \n\nBecause previous works finetune in different settings, we finetune additional 1B tokens if with it. Notice that, even without finetuning, EfficientLLM exceeds all the according baselines. It is shown that existing LLM pruning is impractical in large pruning ratio.",
            "score": 0.7113995714535464,
            "section_title": "Ablation Studies.",
            "char_start_offset": 22656,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 12
                },
                {
                    "start": 13,
                    "end": 118
                },
                {
                    "start": 121,
                    "end": 163
                },
                {
                    "start": 164,
                    "end": 306
                },
                {
                    "start": 307,
                    "end": 408
                },
                {
                    "start": 409,
                    "end": 520
                },
                {
                    "start": 523,
                    "end": 538
                },
                {
                    "start": 539,
                    "end": 675
                },
                {
                    "start": 676,
                    "end": 775
                },
                {
                    "start": 776,
                    "end": 896
                },
                {
                    "start": 897,
                    "end": 1092
                },
                {
                    "start": 1093,
                    "end": 1222
                },
                {
                    "start": 1223,
                    "end": 1374
                },
                {
                    "start": 1377,
                    "end": 1401
                },
                {
                    "start": 1402,
                    "end": 1526
                },
                {
                    "start": 1527,
                    "end": 1595
                },
                {
                    "start": 1596,
                    "end": 1660
                },
                {
                    "start": 1663,
                    "end": 1762
                },
                {
                    "start": 1763,
                    "end": 1850
                },
                {
                    "start": 1851,
                    "end": 1927
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.449951171875
        },
        {
            "corpus_id": "263828939",
            "title": "The Cost of Down-Scaling Language Models: Fact Recall Deteriorates before In-Context Learning",
            "text": "Overall, our study provides a valuable first step towards evaluating pruning algorithms for LLMs, as well as identifying limitations of a particular type of pruning approaches. We have validated our claims on a large number of benchmarks (a total of 8 tasks and 6 models). Since our work is empirical in nature, our observations may not generalize to the full spectrum of tasks and large language models (LLMs). In this study, we focused on evaluating pruning algorithms that scale to large-scale LLMs. This was a deliberate decision, as we believe that scalability is essential for any pruning algorithm to be widely adopted in the real world. More sophisticated one-shot/iterative pruning algorithms exist (Renda et al., 2020). They typically require re-training -redoing the training of these foundation models for every sparsity level we examine. The cost of such an experiment is at least in the millions of dollars therefore beyond our means.",
            "score": 0.7101833451792112,
            "section_title": "A Limitation.",
            "char_start_offset": 31449,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 176
                },
                {
                    "start": 177,
                    "end": 272
                },
                {
                    "start": 273,
                    "end": 411
                },
                {
                    "start": 412,
                    "end": 502
                },
                {
                    "start": 503,
                    "end": 644
                },
                {
                    "start": 645,
                    "end": 729
                },
                {
                    "start": 730,
                    "end": 850
                },
                {
                    "start": 851,
                    "end": 948
                }
            ],
            "ref_mentions": [
                {
                    "start": 708,
                    "end": 728,
                    "matchedPaperCorpusId": "212415013"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.339599609375
        },
        {
            "corpus_id": "259203115",
            "title": "A Simple and Effective Pruning Approach for Large Language Models",
            "text": "Weight Update Calibration Data Pruning Metric \n\nTable 1: Comparison of Wanda with existing pruning algorithms on LLMs. \n\nA comparison of LLM pruning methods can be found in Table 1. Computing the pruning metric of Wanda has a reduced time complexity compared to SparseGPT, because it does not involve inverse computation. Overall, our method Wanda (Pruning by Weights and activations) has several attractive properties as an approach for pruning LLMs: \n\n1. It maintains the simplicity of magnitude pruning in the pre-LLM era, requiring no gradient computation via back-propagation or any second-order Hessian inverses, but is also highly effective in discovering sparse networks in pretrained LLMs. 2. Wanda can be done with a single forward pass of the LLM. At each layer, the pruned weights can be decided in one shot without an iterative procedure. In practice, computing the pruning metric of Wanda can be 300 times faster in pruning LLMs compared with SparseGPT. 3. Unlike SparseGPT, our approach entails no weight update on pruned networks, suggesting that LLMs have effective sparse sub-networks that are exact, instead of them merely existing in the neighborhood of the original weights.",
            "score": 0.708398586592556,
            "section_title": "Method",
            "char_start_offset": 14972,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 45
                },
                {
                    "start": 48,
                    "end": 118
                },
                {
                    "start": 121,
                    "end": 181
                },
                {
                    "start": 182,
                    "end": 321
                },
                {
                    "start": 322,
                    "end": 451
                },
                {
                    "start": 454,
                    "end": 456
                },
                {
                    "start": 457,
                    "end": 758
                },
                {
                    "start": 759,
                    "end": 851
                },
                {
                    "start": 852,
                    "end": 967
                },
                {
                    "start": 968,
                    "end": 1195
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.7958984375
        },
        {
            "corpus_id": "266362404",
            "title": "Fluctuation-based Adaptive Structured Pruning for Large Language Models",
            "text": "Network Pruning is a promising way to address the huge computing resource demands of the deployment and inference of Large Language Models (LLMs). Retraining-free is important for LLMs' pruning methods. However, almost all of the existing retraining-free pruning approaches for LLMs focus on unstructured pruning, which requires specific hardware support for acceleration. In this paper, we propose a novel retraining-free structured pruning framework for LLMs, named FLAP (FLuctuation-based Adaptive\nStructured Pruning). It is hardware-friendly by effectively reducing storage and enhancing inference speed. For effective structured pruning of LLMs, we highlight three critical elements that demand the utmost attention: formulating structured importance metrics, adaptively searching the global compressed model, and implementing compensation mechanisms to mitigate performance loss. First, FLAP determines whether the output feature map is easily recoverable when a column of weight is removed, based on the fluctuation pruning metric. Then it standardizes the importance scores to adaptively determine the global compressed model structure. At last, FLAP adds additional bias terms to recover the output feature maps using the baseline values. We thoroughly evaluate our approach on a variety of language benchmarks. Without any retraining, our method significantly outperforms the state-of-the-art methods, including LLM-Pruner and the extension of Wanda in structured pruning. The code is released at https://github.com/CASIA-IVA-Lab/FLAP.",
            "score": 0.7063340103127045,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.83154296875
        },
        {
            "corpus_id": "267682299",
            "title": "NutePrune: Efficient Progressive Pruning with Numerous Teachers for Large Language Models",
            "text": "Pruning Type Speedup No Support No Index Pruning for LLMs For LLMs, SparseGPT (Frantar and Alistarh, 2023) and WANDA (Sun et al., 2023) employ unstructured pruning methods, while N:M sparsity (Zhou et al., 2021) is considered semi-structured. Despite the effectiveness of these methods, their intricate structures do not yield significant inference speedup on standard hardware (Frantar and Alistarh, 2023) and they need to store additional indexes. As compared in Table 2, structured pruning offers significant advantages, resulting in increased focus on this field in recent works. \n\nCoFi (Xia et al., 2022) and nn pruning (Lagunas et al., 2021) are proposed for smaller language models like BERT (Devlin et al., 2018), often designed for specific tasks. CoFi loads both the teacher and student models, which is impractical for LLMs. Sheared-LLaMA (Xia et al., 2023) proposes pruning LLMs using a dynamic pre-training method, enhancing performance through extensive data and training resources. However, concerns persist regarding limited memory and training resources for LLMs. In a pioneering effort, LLM-Pruner (Ma et al., 2023) prunes LLMs in one-shot and utilizes LoRA (Hu et al., 2021) for fine-tuning. LoRAPrune (Zhang et al., 2023) employs iterative pruning, replacing gradients on full weights with gradients on LoRA to calculate group importance. Compresso (Guo et al., 2023) leverages LoRA and elaborately designed prompts for training and inference. Meanwhile, LoRAShear (Chen et al., 2023) employs LoRA and a dynamic fine-tuning scheme to recover knowledge. \n\nKnowledge Distillation (KD) for LLMs KD (Hinton et al., 2015) has emerged as a vital technique to reduce inference costs while maintaining performance quality in the context of LLMs.",
            "score": 0.7057042080752625,
            "section_title": "Related Works",
            "char_start_offset": 4795,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 242
                },
                {
                    "start": 243,
                    "end": 449
                },
                {
                    "start": 450,
                    "end": 583
                },
                {
                    "start": 586,
                    "end": 756
                },
                {
                    "start": 757,
                    "end": 835
                },
                {
                    "start": 836,
                    "end": 996
                },
                {
                    "start": 997,
                    "end": 1080
                },
                {
                    "start": 1081,
                    "end": 1210
                },
                {
                    "start": 1211,
                    "end": 1358
                },
                {
                    "start": 1359,
                    "end": 1463
                },
                {
                    "start": 1464,
                    "end": 1572
                },
                {
                    "start": 1575,
                    "end": 1757
                }
            ],
            "ref_mentions": [
                {
                    "start": 78,
                    "end": 106,
                    "matchedPaperCorpusId": "255372747"
                },
                {
                    "start": 378,
                    "end": 406,
                    "matchedPaperCorpusId": "255372747"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.4609375
        },
        {
            "corpus_id": "271533761",
            "title": "Greedy Output Approximation: Towards Efficient Structured Pruning for LLMs Without Retraining",
            "text": "To remove redundant components of large language models (LLMs) without incurring significant computational costs, this work focuses on single-shot pruning without a retraining phase. We simplify the pruning process for Transformer-based LLMs by identifying a depth-2 pruning structure that functions independently. Additionally, we propose two inference-aware pruning criteria derived from the optimization perspective of output approximation, which outperforms traditional training-aware metrics such as gradient and Hessian. We also introduce a two-step reconstruction technique to mitigate pruning errors without model retraining. Experimental results demonstrate that our approach significantly reduces computational costs and hardware requirements while maintaining superior performance across various datasets and models.",
            "score": 0.7049720452485538,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.8359375
        },
        {
            "corpus_id": "259203115",
            "title": "A Simple and Effective Pruning Approach for Large Language Models",
            "text": "Large language models (Brown et al., 2020;OpenAI, 2023) have recently reshaped the field of NLP with their remarkable performance across a range of complex language benchmarks (Bommarito & Katz, 2022;Wei et al., 2022a;Bubeck et al., 2023). However, these models, with their billions of parameters, usually require significant computational resources. To democratize LLMs, considerable efforts have been taken to mitigate their high computational cost. Many of the notable advancements to date have centered on model quantization, a process where parameters are quantized into lower bit-level representations. The fast pace of LLM quantization research (Dettmers et al., 2022;Frantar et al., 2023a;Xiao et al., 2023;Ahmadian et al., 2023) has led to substantial resource savings for these models (Sheng et al., 2023;Lin et al., 2023). \n\nNetwork pruning (LeCun et al., 1989;Hassibi et al., 1993;Han et al., 2015), on the other hand, shrinks network sizes by removing specific weights from the model -essentially setting them to zero. Along with quantization, it is often considered another popular approach for compressing neural networks. However, it has received relatively little focus in compressing LLMs. This seems to contradict the trend of model compression in the pre-LLM era, where both approaches have received large amounts of research effort. A quick review of existing pruning methods reveals a possible reason: they typically require retraining (Liu et al., 2019;Blalock et al., 2020), training from random initializations (Zhu & Gupta, 2017;Louizos et al., 2018;Gale et al., 2019) or even an extensive iterative process (Frankle & Michael, 2019;Renda et al., 2020). The sheer amount of computational resources required by LLMs limits these methods. A recent LLM pruning approach, SparseGPT (Frantar & Alistarh, 2023), does not require traditional retraining, but still demands a computationally intensive weight update process.",
            "score": 0.7037695709966878,
            "section_title": "INTRODUCTION",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 239
                },
                {
                    "start": 240,
                    "end": 350
                },
                {
                    "start": 351,
                    "end": 451
                },
                {
                    "start": 452,
                    "end": 608
                },
                {
                    "start": 609,
                    "end": 833
                },
                {
                    "start": 836,
                    "end": 1031
                },
                {
                    "start": 1032,
                    "end": 1137
                },
                {
                    "start": 1138,
                    "end": 1207
                },
                {
                    "start": 1208,
                    "end": 1353
                },
                {
                    "start": 1354,
                    "end": 1679
                },
                {
                    "start": 1680,
                    "end": 1762
                },
                {
                    "start": 1763,
                    "end": 1941
                }
            ],
            "ref_mentions": [
                {
                    "start": 200,
                    "end": 218,
                    "matchedPaperCorpusId": "249674500"
                },
                {
                    "start": 652,
                    "end": 675,
                    "matchedPaperCorpusId": "251564521"
                },
                {
                    "start": 697,
                    "end": 715,
                    "matchedPaperCorpusId": "253708271"
                },
                {
                    "start": 715,
                    "end": 737,
                    "matchedPaperCorpusId": "258967189"
                },
                {
                    "start": 795,
                    "end": 815,
                    "matchedPaperCorpusId": "257495837"
                },
                {
                    "start": 852,
                    "end": 872,
                    "matchedPaperCorpusId": "7785881"
                },
                {
                    "start": 872,
                    "end": 893,
                    "matchedPaperCorpusId": "61815367"
                },
                {
                    "start": 893,
                    "end": 910,
                    "matchedPaperCorpusId": "2238772"
                },
                {
                    "start": 1458,
                    "end": 1476,
                    "matchedPaperCorpusId": "52978527"
                },
                {
                    "start": 1476,
                    "end": 1497,
                    "matchedPaperCorpusId": "212628335"
                },
                {
                    "start": 1555,
                    "end": 1576,
                    "matchedPaperCorpusId": "30535508"
                },
                {
                    "start": 1576,
                    "end": 1594,
                    "matchedPaperCorpusId": "67855585"
                },
                {
                    "start": 1634,
                    "end": 1659,
                    "matchedPaperCorpusId": "53388625"
                },
                {
                    "start": 1659,
                    "end": 1678,
                    "matchedPaperCorpusId": "212415013"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.59619140625
        },
        {
            "corpus_id": "271533761",
            "title": "Greedy Output Approximation: Towards Efficient Structured Pruning for LLMs Without Retraining",
            "text": "With the development of LLMs displaying emergent capabilities like sophisticated reasoning, the focus of the community has shifted to models with billions of parameters, for example, GPT-4 and Llama2 [1,56]. This transition introduces unprecedented computational costs both in the training and the inference phases [55,34,52,5]. To address this challenge, pruning plays a constructive role by removing redundant components from models, thereby reducing computational costs [22,46,59,36]. Notably, designing an optimal pruning strategy is an NP-hard problem (as it reduces to subset selection) and requires balancing accuracy, sparsity, generalizability, pruning costs, and hardware compatibility in practice [61,37,12]. Traditional pruning methods primarily focus on accuracy and sparsity, often neglecting other key factors. They typically involve model retraining and knowledge distillation to mitigate pruning errors. However, with current LLMs featuring billions of parameters, the training process is already a significant challenge, making the additional cost of model retraining even more unaffordable [18,19]. Given these challenges, there's a pressing need for more efficient pruning approaches. \n\nRecently, some works have focused on structured pruning on pre-trained LLMs, directly addressing hardware compatibility and generalizability. This approach allows them to concentrate on the remaining trade-off factors: sparsity, accuracy, and pruning cost. For example, methods like LLM-Pruner, Shortened LLaMA, and Sheared LLaMA use a single-shot pruning strategy that requires only one round of retraining [32,60,40]. On the other hand, strategies such as FLAP, OPTIN, Sliced GPT, LLM Surgeon, Wanda, ZipLM, and KRP seek to eliminate the need for model retraining entirely [2,4,57,51,33,31,37]. However, these approaches have respective limitations, such as high computational costs from the calculation of higher-order information, a lack of fully structured pruning patterns [45], or compromised performance in some cases.",
            "score": 0.7022133929722008,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 207
                },
                {
                    "start": 208,
                    "end": 328
                },
                {
                    "start": 329,
                    "end": 487
                },
                {
                    "start": 488,
                    "end": 719
                },
                {
                    "start": 720,
                    "end": 825
                },
                {
                    "start": 826,
                    "end": 920
                },
                {
                    "start": 921,
                    "end": 1117
                },
                {
                    "start": 1118,
                    "end": 1204
                },
                {
                    "start": 1207,
                    "end": 1348
                },
                {
                    "start": 1349,
                    "end": 1463
                },
                {
                    "start": 1464,
                    "end": 1626
                },
                {
                    "start": 1627,
                    "end": 1803
                },
                {
                    "start": 1804,
                    "end": 2033
                }
            ],
            "ref_mentions": [
                {
                    "start": 473,
                    "end": 477,
                    "matchedPaperCorpusId": "211171709"
                },
                {
                    "start": 477,
                    "end": 480,
                    "matchedPaperCorpusId": "218487454"
                },
                {
                    "start": 480,
                    "end": 483,
                    "matchedPaperCorpusId": "204009154"
                },
                {
                    "start": 708,
                    "end": 712,
                    "matchedPaperCorpusId": "237433629"
                },
                {
                    "start": 715,
                    "end": 718,
                    "matchedPaperCorpusId": "257219883"
                },
                {
                    "start": 1109,
                    "end": 1113,
                    "matchedPaperCorpusId": "251765570"
                },
                {
                    "start": 1113,
                    "end": 1116,
                    "matchedPaperCorpusId": "255372747"
                },
                {
                    "start": 1622,
                    "end": 1625,
                    "matchedPaperCorpusId": "258823276"
                },
                {
                    "start": 1782,
                    "end": 1785,
                    "matchedPaperCorpusId": "266362404"
                },
                {
                    "start": 1793,
                    "end": 1796,
                    "matchedPaperCorpusId": "256662263"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.69970703125
        },
        {
            "corpus_id": "277272730",
            "title": "Maximum Redundancy Pruning: A Principle-Driven Layerwise Sparsity Allocation for LLMs",
            "text": "Large language models (LLMs), which have demonstrated exceptional performance across a range of applications, have sparked an unprecedented race between tech giants and academic institutions to develop models with billions of parameters [Brown et al., 2020;Touvron et al., 2023]. However, their massive size and high computational demands raise concerns about financial costs and environmental impact. [Luccioni et al., 2023;Patterson et al., 2021]. Consequently, efficiently compressing LLMs has become a key research focus. \n\nNetwork pruning, a well-established model compression technique, holds promise as a solution for reducing the size of LLM. Conventional pruning methods [LeCun et al., 1989] propose extensive pruning metrics [Hu, 2016] and layerwise sparsity allocation strategies [Lin et al., 2020;Sui et al., 2021]. However, these methods are difficult to transfer to LLMs due to differences in architectures and high cost of the extra training. \n\nTo address this, some efforts such as SparseGPT [Frantar and Alistarh, 2023] and Wanda [Sun et al., 2024] have been developed specifically for LLMs pruning. SparseGPT prunes unimportant weights and reconstructs layerwise outputs. Wanda prunes weights based on the product of weights and activation magnitudes. These methods assign a uniform sparsity to each layer, which is often suboptimal given the varying importance of layers in LLMs. Recent methods employ search strategies (e.g., evolutionary algorithms [Li et al., 2024a] and linear programming [Li et al., 2024b]) or heuristic functions [Yin et al., 2024] to allocate layerwise sparsity. However, for high-dimensional search spaces, these approaches often yield suboptimal solutions. This dilemma highlights the challenge of finding an optimal layerwise sparsity in a high-dimensional space without principled constraints. Consequently, a pressing question arises: What principles should layerwise sparsity follow for LLMs? \n\nTo answer this question, we analyze the sensitivity of each layer in LLMs to pruning.",
            "score": 0.7021714835085251,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 279
                },
                {
                    "start": 280,
                    "end": 401
                },
                {
                    "start": 402,
                    "end": 449
                },
                {
                    "start": 450,
                    "end": 525
                },
                {
                    "start": 528,
                    "end": 650
                },
                {
                    "start": 651,
                    "end": 827
                },
                {
                    "start": 828,
                    "end": 957
                },
                {
                    "start": 960,
                    "end": 1116
                },
                {
                    "start": 1117,
                    "end": 1189
                },
                {
                    "start": 1190,
                    "end": 1269
                },
                {
                    "start": 1270,
                    "end": 1398
                },
                {
                    "start": 1399,
                    "end": 1605
                },
                {
                    "start": 1606,
                    "end": 1701
                },
                {
                    "start": 1702,
                    "end": 1840
                },
                {
                    "start": 1841,
                    "end": 1941
                },
                {
                    "start": 1944,
                    "end": 2029
                }
            ],
            "ref_mentions": [
                {
                    "start": 237,
                    "end": 257,
                    "matchedPaperCorpusId": "218971783"
                },
                {
                    "start": 402,
                    "end": 425,
                    "matchedPaperCorpusId": "49310977"
                },
                {
                    "start": 425,
                    "end": 448,
                    "matchedPaperCorpusId": "49310977"
                },
                {
                    "start": 680,
                    "end": 700,
                    "matchedPaperCorpusId": "276288500"
                },
                {
                    "start": 791,
                    "end": 809,
                    "matchedPaperCorpusId": "276288500"
                },
                {
                    "start": 809,
                    "end": 826,
                    "matchedPaperCorpusId": "239998772"
                },
                {
                    "start": 1008,
                    "end": 1036,
                    "matchedPaperCorpusId": "255372747"
                },
                {
                    "start": 1047,
                    "end": 1065,
                    "matchedPaperCorpusId": "259203115"
                },
                {
                    "start": 1470,
                    "end": 1488,
                    "matchedPaperCorpusId": "276288500"
                },
                {
                    "start": 1512,
                    "end": 1530,
                    "matchedPaperCorpusId": "276288500"
                },
                {
                    "start": 1555,
                    "end": 1573,
                    "matchedPaperCorpusId": "263829692"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.755859375
        },
        {
            "corpus_id": "265050936",
            "title": "Beyond Size: How Gradients Shape Pruning Decisions in Large Language Models",
            "text": "Large Language Models (LLMs) like OpenAI's GPT series (Radford et al., 2018;2019;Brown et al., 2020a;OpenAI, 2023), BERT (Devlin et al., 2018), LLaMA (Touvron et al., 2023a;b) and others have made significant strides in recent years, leading to a paradigm shift in natural language processing (OpenAI, 2023;Anil et al., 2023;Touvron et al., 2023b) and multimodal learning (Alayrac et al., 2022;Li et al., 2023). Many industries have integrated LLMs into their workflow, such as in chatbots (OpenAI, 2023), code completion tools (e.g., GitHub Copilot) (Chen et al., 2021), and assistive technologies (Zdravkova et al., 2022), etc. While enjoying impressive generalization capabilities, LLMs come with a set of challenges and disadvantages. The presence of abundant parameters, large memory consumption, and high computational cost during inference present several concerns in real-world applications. Previous literature proposed multiple solutions to address these disadvantages, such as distillation (Hinton et al., 2015), quantization (Jacob et al., 2018), pruning (Han et al., 2016), hardware acceleration (Chen et al., 2020), etc. \n\nAmong them, pruning refers to the removal of certain weights or entire neurons/layers based on some criteria, e.g., the smallest weights. A pruned model can maintain similar performance with fewer parameters, resulting in a reduction in storage and computational requirements. Inducing nonstructural sparsity in pruning is a widely embraced method aimed at minimizing the memory requirements of neural networks with only a minimal sacrifice in accuracy. Pruning methods stand out as notably simple and efficient mechanisms for model compression, serving to eliminate weights contingent on their significance.",
            "score": 0.701565022213776,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 411
                },
                {
                    "start": 412,
                    "end": 629
                },
                {
                    "start": 630,
                    "end": 738
                },
                {
                    "start": 739,
                    "end": 899
                },
                {
                    "start": 900,
                    "end": 1134
                },
                {
                    "start": 1137,
                    "end": 1274
                },
                {
                    "start": 1275,
                    "end": 1413
                },
                {
                    "start": 1414,
                    "end": 1590
                },
                {
                    "start": 1591,
                    "end": 1745
                }
            ],
            "ref_mentions": [
                {
                    "start": 81,
                    "end": 101,
                    "matchedPaperCorpusId": "218971783"
                },
                {
                    "start": 372,
                    "end": 394,
                    "matchedPaperCorpusId": "248476411"
                },
                {
                    "start": 599,
                    "end": 623,
                    "matchedPaperCorpusId": "253164819"
                },
                {
                    "start": 1109,
                    "end": 1128,
                    "matchedPaperCorpusId": "213852573"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.57763671875
        },
        {
            "corpus_id": "273501976",
            "title": "Pruning Foundation Models for High Accuracy without Retraining",
            "text": "Foundation models or large language models (LLMs) have achieved remarkable performance on a variety of tasks. However, it is challenging to deploy LLMs in practical applications due to their massive parameters and computations. To facilitate LLM deployment in practice, various model compression techniques targeting LLMs including pruning (Hubara et al., 2021b;Frantar and Alistarh, 2023) and quantization (Dettmers et al., 2022;Frantar et al., 2022;Yao et al., 2022;Xiao et al., 2023) have been proposed to reduce memory and computation costs. \n\nThe traditional pruning techniques, which finetune or retrain models (Li et al., 2020) on full datasets for many epochs (i.e., pruning-aware training), are too expensive for LLMs in terms of data and GPU resources. Thus, post-training pruning based on well-pre-trained models with reduced resource requirements represents a more reasonable approach for LLMs. Notably, SparseGPT (Frantar and Alistarh, 2023) is the representative post-training pruning work with outstanding performance. It reduces memory cost by sequentially loading transformer blocks, one at a time, instead of loading the whole model. Moreover, it reduces the data cost by using only a small amount of calibration data, eliminating the retraining process on massive data. Besides the optimization based SparseGPT, there are some other heuristic posttraining pruning methods such as (Sun et al., 2023;Zhang et al., 2024), achieving accuracy close to SparseGPT. \n\nHowever, the performance of SparseGPT is still sub-optimal as it relies on the solution of Single Removal Problem (SRP) (Singh and Alistarh, 2020;Frantar et al., 2021) to address the pruning of multiple weights, which is essentially a Multiple Removal Problem (MRP). In particular, the SRP provides the optimal solution to prune one weight at a time and modify all other weights to compensate the pruned single weight and minimize the loss.",
            "score": 0.7015086770999334,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 109
                },
                {
                    "start": 110,
                    "end": 227
                },
                {
                    "start": 228,
                    "end": 545
                },
                {
                    "start": 548,
                    "end": 762
                },
                {
                    "start": 763,
                    "end": 906
                },
                {
                    "start": 907,
                    "end": 1033
                },
                {
                    "start": 1034,
                    "end": 1151
                },
                {
                    "start": 1152,
                    "end": 1288
                },
                {
                    "start": 1289,
                    "end": 1476
                },
                {
                    "start": 1479,
                    "end": 1745
                },
                {
                    "start": 1746,
                    "end": 1919
                }
            ],
            "ref_mentions": [
                {
                    "start": 340,
                    "end": 362,
                    "matchedPaperCorpusId": "235825979"
                },
                {
                    "start": 362,
                    "end": 389,
                    "matchedPaperCorpusId": "255372747"
                },
                {
                    "start": 430,
                    "end": 451,
                    "matchedPaperCorpusId": "251765570"
                },
                {
                    "start": 451,
                    "end": 468,
                    "matchedPaperCorpusId": "249395624"
                },
                {
                    "start": 468,
                    "end": 486,
                    "matchedPaperCorpusId": "253708271"
                },
                {
                    "start": 617,
                    "end": 634,
                    "matchedPaperCorpusId": "221761597"
                },
                {
                    "start": 926,
                    "end": 954,
                    "matchedPaperCorpusId": "255372747"
                },
                {
                    "start": 1599,
                    "end": 1625,
                    "matchedPaperCorpusId": "220364055"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.826171875
        },
        {
            "corpus_id": "270214634",
            "title": "Large Language Model Pruning",
            "text": "Let us go through the past pruning methods that address large-scale models or LLMs and explain how they can achieve the goal.If we consider the structured pruning for LLMs, we separate the methods into the supervised and unsupervised types.In supervised methods, we focus on the relation between hidden neurons and the label, while the mutual relation between different groups of neurons receives more attention in the unsupervised methods.Which type of pruning should be used also depends on the applications.\n\nFirst, we start with the supervised pruning methods, which are also the majority in the LLM pruning up to now [36,37,38,39,40,41].Among them, Voita et al. [36] proposed a pruning method based on stochastic gates and a differentiable relaxation of L 0 penalty, which can remove the vast majority of heads without seriously affecting the model's performance.Liu et al. [37] proposed a structured pruning method for efficient BERT inference (EBERT), which can dynamically prune unimportant heads in Multi-Head Self-attention (MHA) and unimportant channels in FFN with the help of the predictor branch.That means that the labeled data are necessary for their operations.Kwon et al. [38] proposed a three-stages pruning framework, which used a Fisher-based mask search algorithm (labeled data are needed) to decide which heads/filters to prune, then rearranged the pruned heads/filters, and at last tuned the mask variables to recover the output signal for each layer.Yang et al. [39] proposed a model pruning toolkit called TextPruner for pre-trained language models.The toolkit includes two pruning methods: one is supervised method, which used the training loss to measure the importance score of neurons; and the other is the self-supervised method, which used the Kullback-Leibler divergence to measure the importance score of neurons.",
            "score": 0.6988785445503144,
            "section_title": "Structured Pruning for LLMs",
            "char_start_offset": 10968,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 125
                },
                {
                    "start": 125,
                    "end": 240
                },
                {
                    "start": 240,
                    "end": 440
                },
                {
                    "start": 440,
                    "end": 510
                },
                {
                    "start": 512,
                    "end": 642
                },
                {
                    "start": 642,
                    "end": 868
                },
                {
                    "start": 868,
                    "end": 1110
                },
                {
                    "start": 1110,
                    "end": 1178
                },
                {
                    "start": 1178,
                    "end": 1475
                },
                {
                    "start": 1475,
                    "end": 1575
                },
                {
                    "start": 1575,
                    "end": 1847
                }
            ],
            "ref_mentions": [
                {
                    "start": 626,
                    "end": 629,
                    "matchedPaperCorpusId": "236477807"
                },
                {
                    "start": 629,
                    "end": 632,
                    "matchedPaperCorpusId": "248266822"
                },
                {
                    "start": 632,
                    "end": 635,
                    "matchedPaperCorpusId": "247794014"
                },
                {
                    "start": 635,
                    "end": 638,
                    "matchedPaperCorpusId": "260682950"
                },
                {
                    "start": 638,
                    "end": 641,
                    "matchedPaperCorpusId": "258823276"
                },
                {
                    "start": 879,
                    "end": 883,
                    "matchedPaperCorpusId": "236477807"
                },
                {
                    "start": 1190,
                    "end": 1194,
                    "matchedPaperCorpusId": "248266822"
                },
                {
                    "start": 1487,
                    "end": 1491,
                    "matchedPaperCorpusId": "247794014"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.619140625
        },
        {
            "corpus_id": "267412953",
            "title": "Faster and Lighter LLMs: A Survey on Current Challenges and Way Forward",
            "text": "As discussed above, there exist several approaches for model compression, and there is no clear consensus on which method to use when or which method is superior over the others. Thus, we present here an experimental analysis of the different LLM compression methods and present important insights. For all the experiments, we provide practical inference metrics including model weight memory (WM), runtime memory consumption (RM), inference token rate and Wiki-Text2 perplexity computed on a Nvidia A100 40GB GPU. Pruning of LLaMA-7B. In this analysis, we examine the structured pruning of the LLaMA-7B model using three recent Large Language Model (LLM) pruning methods. Table 1 showcases the performance scores for these methods at sparsity levels of 20% and 50%. Notably, all compression methods exhibit effective performance in terms of perplexity at lower sparsity levels.Wanda-SP denotes Wanda adapted to structured pruning as reported in [An et al., 2023]. Noticeably, Wanda-SP and LLM-Pruner impacts the model's performance and have suboptimal results at 50% sparsity. On the other hand, both FLaP and the fine-tuned variant of LLMpruner perform well at this level. Comparing RM, WM, and Perplexity, these two methods demonstrate similar performance, with FLaP slightly outperforming the fine-tuningbased LLM-pruner. It is important to note that beyond superior performance, FLaP is also training-free, which makes it a preferred choice for LLM pruning. \n\nQuantized LLaMA2-7B. Table 2 presents a comparative study demonstrating the efficacy of different quantization methods for improving LLM inference. For each quantization method choice, we default to Pytorch as the default inference engine and use propriety engines when Pytorch support is not available. As can be seen, the perplexity of all the models is mostly intact with only marginal degradation. \n\nAs expected, lower precision leads to lower working and running memory consumption. Importantly, we see that at 4-bit, OmniQuant can maintain performance the most. However, GPTQ and AWQ have a wider support on different engines.",
            "score": 0.6975836912942434,
            "section_title": "Experimental Analysis",
            "char_start_offset": 23554,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 178
                },
                {
                    "start": 179,
                    "end": 298
                },
                {
                    "start": 299,
                    "end": 514
                },
                {
                    "start": 515,
                    "end": 535
                },
                {
                    "start": 536,
                    "end": 672
                },
                {
                    "start": 673,
                    "end": 766
                },
                {
                    "start": 767,
                    "end": 964
                },
                {
                    "start": 965,
                    "end": 1077
                },
                {
                    "start": 1078,
                    "end": 1174
                },
                {
                    "start": 1175,
                    "end": 1325
                },
                {
                    "start": 1326,
                    "end": 1462
                },
                {
                    "start": 1465,
                    "end": 1485
                },
                {
                    "start": 1486,
                    "end": 1612
                },
                {
                    "start": 1613,
                    "end": 1768
                },
                {
                    "start": 1769,
                    "end": 1866
                },
                {
                    "start": 1869,
                    "end": 1952
                },
                {
                    "start": 1953,
                    "end": 2032
                },
                {
                    "start": 2033,
                    "end": 2097
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.544921875
        },
        {
            "corpus_id": "259088941",
            "title": "The Emergence of Essential Sparsity in Large Pre-trained Models: The Weights that Matter",
            "text": "Large Language Models (LLMs) recently are shaping the field of NLP with remarkable performance benefits across a range of complex language benchmarks. However, due to their gigantic size and computational costs; they still remain out of reach for many researchers and small industries. \n\nFor instance, GPT-175B requires at least five A100 GPUs with 80GB of memory for every single inference. The sheer computational resources required by LLMs rule out most of the state-of-the-art available pruning algorithms due to the inability to perform any training or fine-tuning routine [86]. \n\nIn this section, we investigate the presence of essential sparsity in one popular LLM (Vicuna-7B) with one-shot magnitude pruning. Note that unlike our previous setting, where we perform sparse fine-tuning after the mask is identified; here we do not perform downstream task-specific fine-tuning and evaluate the performance directly \"zero-shot\" on the sparsified pre-trained checkpoint. \n\nFigure 11(a) illustrates the performance drop of Vicuna-7B on popular MMLU benchmark (Stem, Humanities, Social Science, Others) when x% of the lowest magnitude weights are removed from the dense pre-trained checkpoint. It is interesting to see that our essential sparsity observations hold true even for modern LLMs, sending a favorable signal about the hidden existence of high-quality sparse subnetwork which can be identified at free in dense pre-trained checkpoints. To further enrich our study, we replaced OMP with the recently proposed SparseGPT [85] and found it to have generally consistent trends with OMP (Figure 11(b)). In addition, it is interesting to observe that better-designed pruning strategies such as SparseGPT can further push the boundary of essential sparsity and identify better sparse subnetworks at comparatively higher sparsity ratios: yet at higher compute costs. We leave as future work on how to close the performance gap between OMP and SparseGPT.",
            "score": 0.6910055305322813,
            "section_title": "Scaling Essential Sparsity to Modern-Scale LLMs: A Case Study",
            "char_start_offset": 25451,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 150
                },
                {
                    "start": 151,
                    "end": 285
                },
                {
                    "start": 288,
                    "end": 391
                },
                {
                    "start": 392,
                    "end": 583
                },
                {
                    "start": 586,
                    "end": 716
                },
                {
                    "start": 717,
                    "end": 973
                },
                {
                    "start": 976,
                    "end": 1194
                },
                {
                    "start": 1195,
                    "end": 1446
                },
                {
                    "start": 1447,
                    "end": 1607
                },
                {
                    "start": 1608,
                    "end": 1868
                },
                {
                    "start": 1869,
                    "end": 1955
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.43408203125
        },
        {
            "corpus_id": "268512967",
            "title": "Toward Adaptive Large Language Models Structured Pruning via Hybrid-grained Weight Importance Assessment",
            "text": "Large Language Models (LLMs) have demonstrated unparalleled efficacy in various application domains (Li et al. 2023a;Touvron et al. 2023;Chowdhery et al. 2023). However, deploying LLMs at inference time incurs significant financial and energy costs, mainly due to their large model scale, which requires extensive computational resources and GPU memory (Zhao et al. 2023;Shen et al. 2024). In response, there has been marked increase in interest in compressing LLMs, which upholds the promise of LLMs while substantially reducing their memory requirements and computational costs. Prominent techniques include parameter Figure 1: Sparsity allocation across different layers of LLaMA-7B pruned by fine-grained (Xia, Zhong, and Chen 2022) and coarse-grained (Lee et al. 2020) weight importance criteria (50% global pruning rate). Fine-grained pruning tends to preserve more weight in the shallow layers, which is in stark contrast to coarse-grained pruning. The vertical axis represents the parameter quantity of each layer in terms of millions. The horizontal axis represents the layer number of LLaMA-7B. quantization (Xiao et al. 2023;Shao et al. 2023), network pruning (Frantar and Alistarh 2023; Yuan et al. 2021Yuan et al. , 2022;;Zhao, Sun et al. 2024), token reduction (Zhan et al. 2024a,b) and low-rank decomposition (Bach and Jordan 2005), etc. \n\nThis paper focuses on pruning LLMs by removing redundant parameters to produce a sparse, lightweight model. Pruning methods vary in granularity, ranging from fineto coarse-grained approaches. Fine-grained pruning evaluates the importance of individual weights, as seen in SparseGPT (Frantar and Alistarh 2023), which uses the Hessian matrix for layer-wise weight reconstruction, and Wanda (Sun et al. 2024), which combines weight magnitude with input activations to assess significance.",
            "score": 0.6900590335551762,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 160
                },
                {
                    "start": 161,
                    "end": 389
                },
                {
                    "start": 390,
                    "end": 580
                },
                {
                    "start": 581,
                    "end": 827
                },
                {
                    "start": 828,
                    "end": 955
                },
                {
                    "start": 956,
                    "end": 1043
                },
                {
                    "start": 1044,
                    "end": 1104
                },
                {
                    "start": 1105,
                    "end": 1352
                },
                {
                    "start": 1355,
                    "end": 1462
                },
                {
                    "start": 1463,
                    "end": 1546
                },
                {
                    "start": 1547,
                    "end": 1841
                }
            ],
            "ref_mentions": [
                {
                    "start": 100,
                    "end": 117,
                    "matchedPaperCorpusId": "259679498"
                },
                {
                    "start": 117,
                    "end": 137,
                    "matchedPaperCorpusId": "273551589"
                },
                {
                    "start": 137,
                    "end": 158,
                    "matchedPaperCorpusId": "247951931"
                },
                {
                    "start": 353,
                    "end": 371,
                    "matchedPaperCorpusId": "273501976"
                },
                {
                    "start": 371,
                    "end": 388,
                    "matchedPaperCorpusId": "273551589"
                },
                {
                    "start": 709,
                    "end": 736,
                    "matchedPaperCorpusId": "247922354"
                },
                {
                    "start": 1118,
                    "end": 1136,
                    "matchedPaperCorpusId": "253708271"
                },
                {
                    "start": 1199,
                    "end": 1215,
                    "matchedPaperCorpusId": "235458090"
                },
                {
                    "start": 1235,
                    "end": 1257,
                    "matchedPaperCorpusId": "273501976"
                },
                {
                    "start": 1324,
                    "end": 1346,
                    "matchedPaperCorpusId": "5344737"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.7021484375
        },
        {
            "corpus_id": "263828939",
            "title": "The Cost of Down-Scaling Language Models: Fact Recall Deteriorates before In-Context Learning",
            "text": "In our main paper, we focus on a single pruning algorithm SparseGPT (Frantar & Alistarh, 2023). In this section, we study whether our observation that scaling down LLM size affects fact recall more readily than ICL generalize to another pruning algorithm. \n\nMethod. We repeat key experiments with another pruning algorithm called Wanda (Sun et al., 2023). Notably, unlike SparseGPT, Wanda does not update the remaining weights after weights removal. The author of Wanda shows that at 50% sparsity, Wanda achieves accuracy that is competitive with SparseGPT. \n\nResults. We observe the same disparate effects from pruning on fact recall versus ICL: while moderate pruning hurts fact recall, ICL survives to higher sparsity. Specifically, accepting the same 5% relative drop in accuracy as we did for SparseGPT results, one may remove 30%, 40% and 50% weights on TriviaQA(Closebook), TriviaQA(Openbook) and Linear Classification tasks. Unsurprisingly, given that Wanda is computationally less expensive, it underperforms SparseGPT at high sparsities on ICL tasks.",
            "score": 0.6888230092006595,
            "section_title": "F Additional Pruning Algorithm Evaluation",
            "char_start_offset": 37381,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 95
                },
                {
                    "start": 96,
                    "end": 255
                },
                {
                    "start": 258,
                    "end": 265
                },
                {
                    "start": 266,
                    "end": 355
                },
                {
                    "start": 356,
                    "end": 449
                },
                {
                    "start": 450,
                    "end": 557
                },
                {
                    "start": 560,
                    "end": 568
                },
                {
                    "start": 569,
                    "end": 721
                },
                {
                    "start": 722,
                    "end": 932
                },
                {
                    "start": 933,
                    "end": 1060
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.60498046875
        },
        {
            "corpus_id": "263830468",
            "title": "Compresso: Structured Pruning with Collaborative Prompting Learns Compact Large Language Models",
            "text": "Compression of Small Language Models. In the era of small language models (Devlin et al., 2018;Liu et al., 2019;Lan et al., 2019;Raffel et al., 2020), various compression techniques have been proposed to reduce the model size and inference costs, including weight pruning (Sanh et al., 2020b;Gordon et al., 2020;Zhang et al., 2022a;Xia et al., 2022), input token pruning (Li et al., 2023;Kim et al., 2022;Guan et al., 2022), quantization (Shen et al., 2020;Kim et al., 2021) and distillation (Sanh et al., 2020a;Jiao et al., 2020). We focus on weight pruning, particularly structured pruning, as it can directly reduce inference costs without special hardware support. Most state-of-the-art pruning methods involve a training process to update gradients and utilize them to estimate weight importance. Notable examples include CoFi (Xia et al., 2022) and nn pruning (Lagunas et al., 2021). However, these approaches cannot be directly applied to LLMs for two primary reasons. First, they are taskspecific pruning methods requiring downstream training datasets. Therefore, the pruned models do not retain the generalization capabilities across different tasks. Second, the pruning process for LLMs demands substantial training resources (e.g., expensive GPU memory). \n\nPruning Large Language Model. Given the above challenges, training-based pruning for LLMs remains unexplored. Existing efforts, such as SparseGPT (Frantar & Alistarh, 2023), Wanda (Sun et al., 2023) and LLM-Pruner (Ma et al., 2023), all adopt low-resource, one-shot pruning methods without training. SparseGPT is the first unstructured pruning approach specifically developed to be fast enough for pruning LLMs within a few hours.",
            "score": 0.6884286286936138,
            "section_title": "RELATED WORKS",
            "char_start_offset": 3857,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 37
                },
                {
                    "start": 38,
                    "end": 531
                },
                {
                    "start": 532,
                    "end": 668
                },
                {
                    "start": 669,
                    "end": 801
                },
                {
                    "start": 802,
                    "end": 889
                },
                {
                    "start": 890,
                    "end": 975
                },
                {
                    "start": 976,
                    "end": 1060
                },
                {
                    "start": 1061,
                    "end": 1159
                },
                {
                    "start": 1160,
                    "end": 1265
                },
                {
                    "start": 1268,
                    "end": 1297
                },
                {
                    "start": 1298,
                    "end": 1377
                },
                {
                    "start": 1378,
                    "end": 1567
                },
                {
                    "start": 1568,
                    "end": 1698
                }
            ],
            "ref_mentions": [
                {
                    "start": 129,
                    "end": 149,
                    "matchedPaperCorpusId": "204838007"
                },
                {
                    "start": 312,
                    "end": 332,
                    "matchedPaperCorpusId": "251979775"
                },
                {
                    "start": 332,
                    "end": 349,
                    "matchedPaperCorpusId": "246411621"
                },
                {
                    "start": 371,
                    "end": 388,
                    "matchedPaperCorpusId": "259251699"
                },
                {
                    "start": 388,
                    "end": 405,
                    "matchedPaperCorpusId": "235727659"
                },
                {
                    "start": 405,
                    "end": 423,
                    "matchedPaperCorpusId": "248780407"
                },
                {
                    "start": 438,
                    "end": 457,
                    "matchedPaperCorpusId": "202565587"
                },
                {
                    "start": 832,
                    "end": 850,
                    "matchedPaperCorpusId": "246411621"
                },
                {
                    "start": 866,
                    "end": 888,
                    "matchedPaperCorpusId": "237485472"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.72802734375
        },
        {
            "corpus_id": "271217883",
            "title": "MINI-LLM: Memory-Efficient Structured Pruning for Large Language Models",
            "text": ", the learning rate is set to 1e-4 and a total of 2 epochs.Each pruned model is recovered by an Adam optimizer [Kingma and Ba, 2015] paired with a cosine decay schedule for the learning rate.We set LoRA r = 8, \u03b1 = 16, and attach LoRA modules on all linear layers of the base model.In the inference stage, all the evaluations are implemented with a context length of 128.\n\nBaselines.We compare MINI-LLM with four one-shot structured pruning methods for LLMs.Magnitude-l1/l2: pruning based on the absolute values or the l2-norm of weights, respectively.LLM-Pruner [Ma et al., 2023]: pruning using criterion Eq. ( 3) with backpropagation gradients.Wanda [Sun et al., 2024]: pruning based on the product of the magnitude of weights and their corresponding activations.Given that vanilla SparseGPT and Wanda are retraining-free unstructured methods, we adapt them for structured pruning with pruning and fine-tuning stages for a fair comparison while maintaining the same criterion.Except for LLM-pruner, which is a gradient-based method, the other methods are all gradient-free methods.",
            "score": 0.6874884197788059,
            "section_title": "al., 2023]]",
            "char_start_offset": 17544,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 59
                },
                {
                    "start": 59,
                    "end": 191
                },
                {
                    "start": 191,
                    "end": 281
                },
                {
                    "start": 281,
                    "end": 370
                },
                {
                    "start": 372,
                    "end": 382
                },
                {
                    "start": 382,
                    "end": 457
                },
                {
                    "start": 457,
                    "end": 551
                },
                {
                    "start": 551,
                    "end": 645
                },
                {
                    "start": 645,
                    "end": 764
                },
                {
                    "start": 764,
                    "end": 977
                },
                {
                    "start": 977,
                    "end": 1082
                }
            ],
            "ref_mentions": [
                {
                    "start": 562,
                    "end": 579,
                    "matchedPaperCorpusId": "17240902"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.67041015625
        },
        {
            "corpus_id": "276557981",
            "title": "EvoP: Robust LLM Inference via Evolutionary Pruning",
            "text": "Large Language Models (LLMs), such as Llama (Touvron et al., 2023b) and OPT (Zhang et al., 2022a), have revolutionized natural language processing (NLP) by achieving state-of-the-art performance across a wide range of tasks (Touvron et al., 2023a;Zhang et al., 2022a;Scao et al., 2022;Chowdhery et al., 2023;Thoppilan et al., 2022;Zeng et al., 2023;Chowdhery et al., 2023). However, the success of these models comes at a significant computational cost. LLM consists of billions of parameters, requiring substantial memory, storage, and energy resources. This computational burden limits their deployment in resource-constrained environments. \n\nTo address these challenges, network pruning (LeCun et al., 1989) has emerged as a critical technique for optimizing LLMs by reducing their size while preserving their performance. Pruning involves selectively removing redundant or less important parameters (e.g., elements (Sun et al., 2024), channels (Ashkboos et al., 2024), or layers (Song et al., 2024)) from a pre-trained model, which enables faster inference and lower energy consumption. Importantly, pruning is feasible in LLMs because these models are often overparameterized, containing more parameters than necessary to achieve their performance. \n\nUnfortunately, existing pruning techniques still face different challenges. Element-wise pruning achieves the finest-grained pruning to obtain a quite high sparsity, e.g., 50%, but requires specific computation kernels and hardware support to accelerate the inference. Channel-wise pruning trades off the hardware efficiency and sparsity. However, the complex channel dependency increases the difficulty of determining the pruning channel, thus resulting in performance drops. Layer-wise pruning adopts heuristic algorithms to search the pruning pattern and maintain the hardware friendliness for faster inference. But it often fails to find the optimal pruning patterns.",
            "score": 0.6868132956499943,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 373
                },
                {
                    "start": 374,
                    "end": 453
                },
                {
                    "start": 454,
                    "end": 554
                },
                {
                    "start": 555,
                    "end": 642
                },
                {
                    "start": 645,
                    "end": 825
                },
                {
                    "start": 826,
                    "end": 1090
                },
                {
                    "start": 1091,
                    "end": 1253
                },
                {
                    "start": 1256,
                    "end": 1331
                },
                {
                    "start": 1332,
                    "end": 1524
                },
                {
                    "start": 1525,
                    "end": 1594
                },
                {
                    "start": 1595,
                    "end": 1732
                },
                {
                    "start": 1733,
                    "end": 1870
                },
                {
                    "start": 1871,
                    "end": 1927
                }
            ],
            "ref_mentions": [
                {
                    "start": 285,
                    "end": 308,
                    "matchedPaperCorpusId": "247951931"
                },
                {
                    "start": 331,
                    "end": 349,
                    "matchedPaperCorpusId": "252715691"
                },
                {
                    "start": 349,
                    "end": 372,
                    "matchedPaperCorpusId": "247951931"
                },
                {
                    "start": 690,
                    "end": 710,
                    "matchedPaperCorpusId": "7785881"
                },
                {
                    "start": 919,
                    "end": 937,
                    "matchedPaperCorpusId": "259203115"
                },
                {
                    "start": 948,
                    "end": 971,
                    "matchedPaperCorpusId": "267301573"
                },
                {
                    "start": 983,
                    "end": 1002,
                    "matchedPaperCorpusId": "267657949"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.70751953125
        },
        {
            "corpus_id": "263829692",
            "title": "Outlier Weighed Layerwise Sparsity (OWL): A Missing Secret Sauce for Pruning LLMs to High Sparsity",
            "text": "We primarily focus on high sparsity levels, not falling below 50%, as regions with low sparsity pose challenges for existing sparse GPU kernels to outperform their dense counterparts (Gale et al., 2020). To ensure equitable comparisons, we have employed the identical set of calibration data as utilized by SparseGPT and Wanda for model pruning, i.e., comprising 128 sequences with 2048 tokens for each, randomly sampled from the first shard of the C4 (Raffel et al., 2020) dataset. We incorporate OWL directly into Wanda and SparseGPT, resulting in two variants: \"OWL w. \n\nWanda\" and \"OWL w. SparseGPT\". The only distinction between these variants lies in their layerwise sparsity ratios, with OWL providing a more tailored layerwise sparsity in this regard. Hyperparameters are shared in Appendix D. \n\nLanguage Modelling. We first report the performance of various LLM pruning methods on language modelling with WikiText. The results is presented in Table 3 and Figure 2. We summarize the key observation below: \n\n1 OWL serves as a general layerwise sparsity method suitable for various scenarios. As illustrated in Table 3, OWL exhibits effectiveness across different pruning methods (such as Wanda and SparseGPT), architectural variants (including LLaMA-V1 and OPT), and diverse model sizes (ranging from 7B, 13B, 30B, to 65B parameters), resulting in substantial reductions in perplexity scores. Notably, even when applied to SparseGPT, a strong pruning method incorporating second-order information, OWL still achieves significant perplexity reductions, exemplified by a reduction of 6.81 for LLaMA-7B. \n\n2 The benefits of OWL increase significantly as model size decreases. There is a clear trend that the performance gain of OWL monotonically increases as LLaMA-V1 scales down from 65B to 7B. While the performance improvement  Zero-Shot Tasks. While perplexity is a widely used metric for language modeling, it primarily serves as a statistical measure of how confidently a language model predicts a text sample and does not necessarily align with the quality of the generated text (Jaiswal et al., 2023a).",
            "score": 0.6860909374211933,
            "section_title": "Main Experiments",
            "char_start_offset": 23085,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 203
                },
                {
                    "start": 204,
                    "end": 482
                },
                {
                    "start": 483,
                    "end": 571
                },
                {
                    "start": 574,
                    "end": 592
                },
                {
                    "start": 593,
                    "end": 604
                },
                {
                    "start": 605,
                    "end": 759
                },
                {
                    "start": 760,
                    "end": 801
                },
                {
                    "start": 804,
                    "end": 823
                },
                {
                    "start": 824,
                    "end": 923
                },
                {
                    "start": 924,
                    "end": 1013
                },
                {
                    "start": 1016,
                    "end": 1099
                },
                {
                    "start": 1100,
                    "end": 1400
                },
                {
                    "start": 1401,
                    "end": 1608
                },
                {
                    "start": 1611,
                    "end": 1680
                },
                {
                    "start": 1681,
                    "end": 1800
                },
                {
                    "start": 1801,
                    "end": 1852
                },
                {
                    "start": 1853,
                    "end": 2115
                }
            ],
            "ref_mentions": [
                {
                    "start": 183,
                    "end": 202,
                    "matchedPaperCorpusId": "219955899"
                },
                {
                    "start": 452,
                    "end": 473,
                    "matchedPaperCorpusId": "204838007"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.6005859375
        },
        {
            "corpus_id": "276617893",
            "title": "On Pruning State-Space LLMs",
            "text": "Pruning is frequently used to compress LLMs. Below we describe common pruning methods. \n\nUnstructured pruning induces sparsity in the model weights. Such methods reduce model size, though it is hard to translate this sparsity to runtime gains. A common implementation of this approach is magnitude pruning, which prunes unimportant parameters based on their absolute values (Frankle and Carbin, 2019). A recent highly effective variant magnitude pruning is WANDA (Sun et al., 2024), which also takes activations into account. Importantly, WANDA does not require fine-tuning, making it highly efficient for compressing LLMs. \n\nStructured pruning removes entire sections of model weights rather than individual elements (Ma et al., 2023;Molchanov et al., 2019;Fan et al., 2019). FLAP (An et al., 2023) is a recent structured pruning framework, based on the fluctuation of each input channel. It determines whether the output feature map can be recovered after pruning a column from the weight matrix. FLAP avoids the need for retraining and requires only single forward pass for both pruning and bias compensation. Finally, group-query attention (GQA), which merges distinct attention KV heads using meanpooling, can also be thought of as a structured pruning method.",
            "score": 0.6853879019264408,
            "section_title": "Pruning Methods",
            "char_start_offset": 4054,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 44
                },
                {
                    "start": 45,
                    "end": 86
                },
                {
                    "start": 89,
                    "end": 148
                },
                {
                    "start": 149,
                    "end": 243
                },
                {
                    "start": 244,
                    "end": 401
                },
                {
                    "start": 402,
                    "end": 525
                },
                {
                    "start": 526,
                    "end": 623
                },
                {
                    "start": 626,
                    "end": 776
                },
                {
                    "start": 777,
                    "end": 889
                },
                {
                    "start": 890,
                    "end": 998
                },
                {
                    "start": 999,
                    "end": 1112
                },
                {
                    "start": 1113,
                    "end": 1265
                }
            ],
            "ref_mentions": [
                {
                    "start": 374,
                    "end": 400,
                    "matchedPaperCorpusId": "53388625"
                },
                {
                    "start": 718,
                    "end": 735,
                    "matchedPaperCorpusId": "258823276"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.7578125
        },
        {
            "corpus_id": "272693912",
            "title": "KVPruner: Structural Pruning for Faster and Memory-Efficient Large Language Models",
            "text": "Large language models (LLMs) refer to natural language processing (NLP) models with a massive number of parameters [1]- [5], commonly based on the Transformer architecture [5]. These models have also found widespread applications in fields such as speech processing [6]- [8] and computer vision [9]- [11]. In recent years, LLMs have demonstrated remarkable capabilities in handling complex tasks in applications like dialogue systems [12], [13] and knowledge-based question answering [14]- [16], significantly accelerating the development of downstream applications. However, as model sizes continue to grow, the challenges related to inference efficiency have become more pronounced. \n\nCurrently, optimization methods for large models include pruning (structured pruning [17]- [19] and unstructured pruning [20], [21]), quantization [22]- [24], and distillation [25], [26]. This work focuses on structured pruning, making it more deployment-friendly and hardware-friendly. In modern structured pruning algorithms for LLMs, LLM-Pruner [17] achieves model size reduction by removing inter-group dependencies in the network. Sheared-LLaMA [19] not only removes structures within groups but also prunes less important blocks to achieve compression. \n\nThese methods can employ LoRA [27] to recover performance, but the gains in runtime memory efficiency and inference performance are still minimal. Shortened-LLM [18] aggressively removes entire blocks to speed up inference, but The top left compares perplexity(PPL) across different strategies under the same pruning ratio and fine-tuning steps, where our method demonstrates superior performance. The bottom left shows the keyvalue (KV) cache usage, where our approach achieves more significant KV memory pruning at both strategy-level and model parameter-level pruning ratios. Right side: Under the same model parameter settings, KVPruner achieves faster inference speeds compared to Shortened-LLM [18] and LLM-Pruner [17] pruning method. \n\nit shows that directly removing blocks requires retraining with CPT [18], which can take several weeks to restore the pruned model.",
            "score": 0.6842977387524403,
            "section_title": "I. INTRODUCTION",
            "char_start_offset": 18,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 176
                },
                {
                    "start": 177,
                    "end": 305
                },
                {
                    "start": 306,
                    "end": 566
                },
                {
                    "start": 567,
                    "end": 684
                },
                {
                    "start": 687,
                    "end": 874
                },
                {
                    "start": 875,
                    "end": 973
                },
                {
                    "start": 974,
                    "end": 1122
                },
                {
                    "start": 1123,
                    "end": 1245
                },
                {
                    "start": 1248,
                    "end": 1394
                },
                {
                    "start": 1395,
                    "end": 1645
                },
                {
                    "start": 1646,
                    "end": 1826
                },
                {
                    "start": 1827,
                    "end": 1988
                },
                {
                    "start": 1991,
                    "end": 2122
                }
            ],
            "ref_mentions": [
                {
                    "start": 115,
                    "end": 118,
                    "matchedPaperCorpusId": "258546397"
                },
                {
                    "start": 120,
                    "end": 123,
                    "matchedPaperCorpusId": "13756489"
                },
                {
                    "start": 172,
                    "end": 175,
                    "matchedPaperCorpusId": "13756489"
                },
                {
                    "start": 271,
                    "end": 274,
                    "matchedPaperCorpusId": "239024736"
                },
                {
                    "start": 300,
                    "end": 304,
                    "matchedPaperCorpusId": "233444273"
                },
                {
                    "start": 434,
                    "end": 438,
                    "matchedPaperCorpusId": "259129398"
                },
                {
                    "start": 440,
                    "end": 444,
                    "matchedPaperCorpusId": "1820614"
                },
                {
                    "start": 484,
                    "end": 488,
                    "matchedPaperCorpusId": "261048772"
                },
                {
                    "start": 490,
                    "end": 494,
                    "matchedPaperCorpusId": "218869575"
                },
                {
                    "start": 772,
                    "end": 776,
                    "matchedPaperCorpusId": "258823276"
                },
                {
                    "start": 814,
                    "end": 818,
                    "matchedPaperCorpusId": "255372747"
                },
                {
                    "start": 1035,
                    "end": 1039,
                    "matchedPaperCorpusId": "258823276"
                },
                {
                    "start": 1968,
                    "end": 1972,
                    "matchedPaperCorpusId": "258823276"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.7548828125
        },
        {
            "corpus_id": "264146174",
            "title": "One-Shot Sensitivity-Aware Mixed Sparsity Pruning for Large Language Models",
            "text": "The study of model pruning emerged since the 1980s' [20][21][22], and it has demonstrated favorable outcomes across Computer Vision and NLP tasks. However, these methods require extensive retraining of pruned models to restore the performance level of the original models [13]. While this is feasible for smaller-scale models, it becomes prohibitive for LLMs due to the associated computational costs. While there are some one-shot pruning methods that don't require retraining [22,23], their effectiveness are limited. Therefore, extending the existing pruning methods to models with 10-100+ billion parameters is still an open and challenging problem. \n\nRecently, SparseGPT [14] further developed the Optimal Brain Surgeon (OBS) [21] approach. It achieved at least 50% pruning ratio on LLMs with over a billion parameters in one shot, minimizing performance loss without requiring any retraining. Based on previous studies in this area, we believe there is still considerable room for further improvements. Firstly, saliency criterion for weight element selection, such as OBS and OBD [21], each captured one aspect of the weight element's saliency. We show that fusing the two criteria of OBS and OBD yields a better option for selecting the weights to be pruned, as observed in our experiments. \n\nMoreover, SparseGPT assumes a uniform sparsity across all layers. Inspired by the analysis of weight sensitivity in mixed-precision quantization methods [24,25], we investigated the sensitivity properties of different LLM layers. The magnitude of pruning ratios (sparsity) for each layer are then determined based on their respective sensitivities. Specifically, we utilize second-order information (the Hessian matrix) to assess the sensitivity of each layer. \n\nFurthermore, we extend pruning to individual weight-level, an even finer granularity, for more precise trade-off. This means that each weight matrix is assigned a distinct sparsity level according to its sensitivity. In experiments with multiple cutting edge LLMs, our approach beats SparseGPT in terms of both perplexity and zero-shot downstream task performances.",
            "score": 0.6832919050608139,
            "section_title": "INTRODUCTION",
            "char_start_offset": 1910,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 146
                },
                {
                    "start": 147,
                    "end": 277
                },
                {
                    "start": 278,
                    "end": 401
                },
                {
                    "start": 402,
                    "end": 519
                },
                {
                    "start": 520,
                    "end": 653
                },
                {
                    "start": 656,
                    "end": 745
                },
                {
                    "start": 746,
                    "end": 898
                },
                {
                    "start": 899,
                    "end": 1008
                },
                {
                    "start": 1009,
                    "end": 1151
                },
                {
                    "start": 1152,
                    "end": 1298
                },
                {
                    "start": 1301,
                    "end": 1366
                },
                {
                    "start": 1367,
                    "end": 1530
                },
                {
                    "start": 1531,
                    "end": 1649
                },
                {
                    "start": 1650,
                    "end": 1761
                },
                {
                    "start": 1764,
                    "end": 1877
                },
                {
                    "start": 1878,
                    "end": 1980
                },
                {
                    "start": 1981,
                    "end": 2129
                }
            ],
            "ref_mentions": [
                {
                    "start": 52,
                    "end": 56,
                    "matchedPaperCorpusId": "7785881"
                },
                {
                    "start": 56,
                    "end": 60,
                    "matchedPaperCorpusId": "7057040"
                },
                {
                    "start": 482,
                    "end": 485,
                    "matchedPaperCorpusId": "231934142"
                },
                {
                    "start": 731,
                    "end": 735,
                    "matchedPaperCorpusId": "7057040"
                },
                {
                    "start": 1087,
                    "end": 1091,
                    "matchedPaperCorpusId": "7057040"
                },
                {
                    "start": 1454,
                    "end": 1458,
                    "matchedPaperCorpusId": "148571720"
                },
                {
                    "start": 1458,
                    "end": 1461,
                    "matchedPaperCorpusId": "207852310"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.79443359375
        },
        {
            "corpus_id": "275932211",
            "title": "SwiftPrune: Hessian-Free Weight Pruning for Large Language Models",
            "text": "In a notable simplification, Wanda (Sun et al., 2024) demonstrates that preserving only the diagonal elements of the Hessian matrix suffices for effective pruning, significantly reducing computational overhead while maintaining competitive performance. \n\nSimultaneously, achieve tangible speed improvements in practical applications, there has been a growing recognition of the need to apply pruning in a structured and hardware-compatible manner (Santacroce et al., 2023;Ma et al., 2023a;Li et al., 2023;Xia et al., 2024). This approach is typically followed by additional training (or finetuning) to restore any diminished performance. For example, the LLM-pruner (Ma et al., 2023c) eliminates specific connection structures within LLMs prior to further training. Similarly, the Large Language Model Surgeon (van der Ouderaa et al., 2024) interleaves recovery fine-tuning with pruning.",
            "score": 0.6826358028762115,
            "section_title": "Related Work",
            "char_start_offset": 23339,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 252
                },
                {
                    "start": 255,
                    "end": 523
                },
                {
                    "start": 524,
                    "end": 637
                },
                {
                    "start": 638,
                    "end": 765
                },
                {
                    "start": 766,
                    "end": 887
                }
            ],
            "ref_mentions": [
                {
                    "start": 35,
                    "end": 53,
                    "matchedPaperCorpusId": "259203115"
                },
                {
                    "start": 489,
                    "end": 505,
                    "matchedPaperCorpusId": "259203385"
                },
                {
                    "start": 505,
                    "end": 522,
                    "matchedPaperCorpusId": "263830786"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.69140625
        },
        {
            "corpus_id": "267413136",
            "title": "Shortened LLaMA: A Simple Depth Pruning for Large Language Models",
            "text": "The advancement of large language models (LLMs) (Touvron et al., 2023;OpenAI, 2023;Chowdhery et al., 2022;Zhang et al., 2022;Scao et al., 2022) has brought significant improvements in language-based tasks, enabling versatile applications such as powerful chatbots (Google, 2023;OpenAI, 2022). However, the deployment of LLMs is constrained by their intensive computational demands. To make LLMs more accessible and efficient for practical use, various optimization strategies have been actively studied over recent years (see Zhu et al. (2023); Wan et al. (2023) for survey). * Equal contribution. \u2020 Corresponding author. (An et al., 2024) and LLM-Pruner (Ma et al., 2023), our depth pruning (D\u2702) achieves faster inference. Right: Continued pretraining is crucial for restoring the quality of heavily pruned models with fewer than 3.7B parameters, enabling our method to surpass the baselines, including SLEB (Song et al., 2024). See Table 3 for details. \n\nThis work focuses on structured pruning (Fang et al., 2023;Li et al., 2017a), which removes groups of unnecessary weights and can facilitate hardwareagnostic acceleration. \n\nIn the context of compressing recent LLMs, LLM-Pruner (Ma et al., 2023) and FLAP (An et al., 2024) narrow the network width by pruning coupled structures (e.g., attention heads and their associated weight connections) while maintaining the number of layers. Sheared-LLaMA (Xia et al., 2024) reduces not only the network width but also its depth by entirely removing some layers. Despite the existence of pruning methods (Xia et al., 2022;Kurtic et al., 2023;Xia et al., 2024) that incorporate both width and depth aspects, there remains a gap in detailed analysis comparing these two factors (width vs. depth), specifically in relation to their impact on LLM inference efficiency.",
            "score": 0.6812108522923579,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 292
                },
                {
                    "start": 293,
                    "end": 381
                },
                {
                    "start": 382,
                    "end": 575
                },
                {
                    "start": 576,
                    "end": 597
                },
                {
                    "start": 598,
                    "end": 621
                },
                {
                    "start": 622,
                    "end": 723
                },
                {
                    "start": 724,
                    "end": 929
                },
                {
                    "start": 930,
                    "end": 954
                },
                {
                    "start": 957,
                    "end": 1128
                },
                {
                    "start": 1131,
                    "end": 1388
                },
                {
                    "start": 1389,
                    "end": 1509
                },
                {
                    "start": 1510,
                    "end": 1811
                }
            ],
            "ref_mentions": [
                {
                    "start": 622,
                    "end": 639,
                    "matchedPaperCorpusId": "266362404"
                },
                {
                    "start": 655,
                    "end": 672,
                    "matchedPaperCorpusId": "258823276"
                },
                {
                    "start": 909,
                    "end": 928,
                    "matchedPaperCorpusId": "267657949"
                },
                {
                    "start": 997,
                    "end": 1016,
                    "matchedPaperCorpusId": "256390345"
                },
                {
                    "start": 1016,
                    "end": 1033,
                    "matchedPaperCorpusId": "14089312"
                },
                {
                    "start": 1185,
                    "end": 1202,
                    "matchedPaperCorpusId": "258823276"
                },
                {
                    "start": 1212,
                    "end": 1229,
                    "matchedPaperCorpusId": "266362404"
                },
                {
                    "start": 1403,
                    "end": 1421,
                    "matchedPaperCorpusId": "263830786"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.360595703125
        },
        {
            "corpus_id": "273350592",
            "title": "AlphaPruning: Using Heavy-Tailed Self Regularization Theory for Improved Layer-wise Pruning of Large Language Models",
            "text": "Language Modeling. In Table 2, we report the perplexity of the pruned LLaMA and LLaMA-2 models at 70% sparsity. We provide results for more sparsity levels in Figure 2 and Appendix G. AlphaPruning, as a general layerwise sparsity method, consistently demonstrates performance improvements when used in conjunction with various pruning methods. For example, in the case of LLaMA-7B with a sparsity of 70%, AlphaPruning produces sparse networks with a perplexity of 231.01, significantly outperforming the Magnitude-based pruning baseline of 48419.13. Notably, when applied to Wanda and SparseGPT, two robust LLM pruning methods, AlphaPruning still achieves substantial perplexity reductions, evidenced by a decrease of 61.91 for Wanda and 7.76 for SparseGPT, in the case of LLaMA-7B with a sparsity of 70%. \n\nZero-shot tasks. We conducted empirical evaluations to determine the zero-shot ability of pruned LLMs on diverse zero-shot downstream tasks with prompting. The results are shown in Table 3, where we show the mean zero-shot accuracy on 7 zero-shot tasks of pruned LLaMA and LLaMA-2 models at sparsity of 70%. \n\nAlphaPruning consistently improves accuracy across all settings. For example, AlphaPruning achieves an average accuracy gain of 8.79, 6.05, and 2.61 over 7 tasks and 7 models compared to Magnitude, Wanda and SparseGPT alone, respectively. These results highlight the promise of AlphaPruning for more challenging zero-shot downstream tasks.",
            "score": 0.6798304548736869,
            "section_title": "Main results",
            "char_start_offset": 23282,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 18
                },
                {
                    "start": 19,
                    "end": 111
                },
                {
                    "start": 112,
                    "end": 343
                },
                {
                    "start": 344,
                    "end": 549
                },
                {
                    "start": 550,
                    "end": 805
                },
                {
                    "start": 808,
                    "end": 824
                },
                {
                    "start": 825,
                    "end": 963
                },
                {
                    "start": 964,
                    "end": 1115
                },
                {
                    "start": 1118,
                    "end": 1182
                },
                {
                    "start": 1183,
                    "end": 1356
                },
                {
                    "start": 1357,
                    "end": 1457
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.88037109375
        },
        {
            "corpus_id": "277452419",
            "title": "Model Hemorrhage and the Robustness Limits of Large Language Models",
            "text": "OWL [32] integrates both Wanda and SparseGPT, proposing the OWL metric to allocate varying pruning rates across different layers. Similarly, BESA [33] refines pruning by considering each transformer block's pruning error and allocating sparsity in a differentiable way, overcoming the perturbations associated with traditional layer-wise approaches. DsnoT [34] is also an extension of the SparseGPT and Wanda pruning strategies, introducing a training-free fine-tuning approach that iteratively refines sparse LLMs by adjusting sparse masks, minimizing the reconstruction error between sparse and dense models. Several pruning methods are developed independently of Wanda and SparseGPT. For example, Flash-LLM [28] introduces a \"Load-as-Sparse, Compute-as-Dense\" strategy, which optimizes memory bandwidth while allowing tensor cores to perform computations as if the model were dense. LoRAPrune [17] incorporates LoRA (Low-Rank Adaptation) modules to evaluate the importance of weights and activations, excelling in taskspecific pruning scenarios, albeit at the expense of additional computational overhead due to the extra modules. Table 2 summarizes the specific details of these methods. SparseGPT [29] Yes Yes \n\nhidden ) LoRAPrune [17] Yes Yes",
            "score": 0.6790193166624138,
            "section_title": "Unstructured Pruning",
            "char_start_offset": 24104,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 129
                },
                {
                    "start": 130,
                    "end": 349
                },
                {
                    "start": 350,
                    "end": 610
                },
                {
                    "start": 611,
                    "end": 686
                },
                {
                    "start": 687,
                    "end": 885
                },
                {
                    "start": 886,
                    "end": 1133
                },
                {
                    "start": 1134,
                    "end": 1191
                },
                {
                    "start": 1192,
                    "end": 1214
                },
                {
                    "start": 1217,
                    "end": 1248
                }
            ],
            "ref_mentions": [
                {
                    "start": 4,
                    "end": 8,
                    "matchedPaperCorpusId": "263829692"
                },
                {
                    "start": 146,
                    "end": 150,
                    "matchedPaperCorpusId": "268032346"
                },
                {
                    "start": 356,
                    "end": 360,
                    "matchedPaperCorpusId": "264128029"
                },
                {
                    "start": 1202,
                    "end": 1206,
                    "matchedPaperCorpusId": "255372747"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.75732421875
        },
        {
            "corpus_id": "278033481",
            "title": "The Rise of Small Language Models in Healthcare: A Comprehensive Survey",
            "text": "Pruning is a long-established technique for model compression that reduces the size of a model by removing unnecessary or less important parameters [61,90]. In language models, many weights can be zeroed out or eliminated with little to no effect on overall performance [18]. By pruning these redundant parameters, we can significantly shrink model size and accelerate inference, with only a minor drop in accuracy if done carefully. Pruning thus makes models more storage-friendly, memoryefficient, and computation-efficient. Based on the granularity of removal, pruning is categorized as unstructured pruning and structured pruning. \n\nStructured pruning streamlines LLM by eliminating entire architectural components-such as neurons, filter channels, attention heads, or even layers-instead of removing individual parameters [126,198]. By selectively pruning structurally coherent groups of parameters, this approach yields a smaller yet regularly structured model-maintaining dense connectivity while reducing inference costs, for instance, LLM-Pruner [126]. Unstructured pruning removes individual weights from LLM without considering any specific structure within the model [41]. Unstructured pruning removes less important weights based on a threshold, resulting in a sparse model where the architecture remains unchanged, as long as enough weights stay to maintain performance. Unstructured pruning can deliver higher accuracy for a given compression ratio by allowing more targeted weight removal. However, its irregular sparsity pattern leads to inefficient memory access and limited speed-ups on standard hardware. To fully leverage the compression benefits, specialized libraries or hardware are needed, and even then, acceleration gains are modest unless the model is highly sparse. Post unstructured pruning, significant retraining or fine-tuning is often required to recover lost accuracy that can be resource-intensive. \n\nDiscussion on pruning: Contextual pruning is a promising method for building domain-specific language models [177]. Recent research has focused on one-shot pruning methods and improved criteria to minimize the need for costly retraining [211], validating its performance for healthcare tasks.",
            "score": 0.678776771416332,
            "section_title": "Pruning",
            "char_start_offset": 44224,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 156
                },
                {
                    "start": 157,
                    "end": 275
                },
                {
                    "start": 276,
                    "end": 433
                },
                {
                    "start": 434,
                    "end": 526
                },
                {
                    "start": 527,
                    "end": 634
                },
                {
                    "start": 637,
                    "end": 837
                },
                {
                    "start": 838,
                    "end": 1061
                },
                {
                    "start": 1062,
                    "end": 1184
                },
                {
                    "start": 1185,
                    "end": 1384
                },
                {
                    "start": 1385,
                    "end": 1505
                },
                {
                    "start": 1506,
                    "end": 1624
                },
                {
                    "start": 1625,
                    "end": 1794
                },
                {
                    "start": 1795,
                    "end": 1934
                },
                {
                    "start": 1937,
                    "end": 2052
                },
                {
                    "start": 2053,
                    "end": 2229
                }
            ],
            "ref_mentions": [
                {
                    "start": 148,
                    "end": 152,
                    "matchedPaperCorpusId": "2238772"
                },
                {
                    "start": 152,
                    "end": 155,
                    "matchedPaperCorpusId": "7785881"
                },
                {
                    "start": 270,
                    "end": 274,
                    "matchedPaperCorpusId": "276575434"
                },
                {
                    "start": 827,
                    "end": 832,
                    "matchedPaperCorpusId": "258823276"
                },
                {
                    "start": 1055,
                    "end": 1060,
                    "matchedPaperCorpusId": "258823276"
                },
                {
                    "start": 1179,
                    "end": 1183,
                    "matchedPaperCorpusId": "255372747"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.470703125
        },
        {
            "corpus_id": "263829692",
            "title": "Outlier Weighed Layerwise Sparsity (OWL): A Missing Secret Sauce for Pruning LLMs to High Sparsity",
            "text": "The remarkable performance exhibited by Large Language Models (LLMs) across a diverse spectrum of applications has ignited an unparalleled race among tech giants and academic institutions to build LLMs at the billion-parameter scale (Brown et al., 2020;Touvron et al., 2023a;b;Brown et al., 2020). The compelling performance of Large Language Models (LLMs) demonstrated in various applications triggers an unprecedented competition of building billion-level LLMs among tech giants and academic institutions (Brown et al., 2020;Touvron et al., 2023a;b;Brown et al., 2020). While their exceptional capabilities are undeniable, the colossal size and computational demands of these models have also raised substantial concerns, particularly in terms of financial expenditure and environment (Luccioni et al., 2022;Patterson et al., 2021). \n\nNetwork pruning (Mozer & Smolensky, 1989;Janowsky, 1989;LeCun et al., 1989;Han et al., 2015), as a longestablished model compression method, is expected to serve as an effective solution for reducing the size of LLMs. However, network pruning usually favors a certain time of finetuning or re-training to reacquire the original optimal performance. Given the extensive text corpus and model size associated with LLMs, conventional fine-tuning becomes exceedingly challenging and less desirable. Fortunately, recent endeavors have explored the possibility of LLM pruning without the need for fine-tuning, showcasing that LLMs contain a substantial number of parameters that can be removed in a single step with minimal performance degra- dation (Frantar & Alistarh, 2023;Sun et al., 2023;Jaiswal et al., 2023b;Ma et al., 2023). SparseGPT (Frantar & Alistarh, 2023) addresses the challenge of LLM pruning from the perspective of layerwise reconstruction problem. In this context, the primary goal is to minimize the output discrepancy in terms of the reconstruction error between dense and sparse LLMs. It adopts an iterative strategy to handle the computational hurdle posed by the row-Hessian problem.",
            "score": 0.6777668066278061,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 297
                },
                {
                    "start": 298,
                    "end": 571
                },
                {
                    "start": 572,
                    "end": 834
                },
                {
                    "start": 837,
                    "end": 1054
                },
                {
                    "start": 1055,
                    "end": 1185
                },
                {
                    "start": 1186,
                    "end": 1331
                },
                {
                    "start": 1332,
                    "end": 1663
                },
                {
                    "start": 1664,
                    "end": 1797
                },
                {
                    "start": 1798,
                    "end": 1937
                },
                {
                    "start": 1938,
                    "end": 2038
                }
            ],
            "ref_mentions": [
                {
                    "start": 233,
                    "end": 253,
                    "matchedPaperCorpusId": "218971783"
                },
                {
                    "start": 277,
                    "end": 296,
                    "matchedPaperCorpusId": "218971783"
                },
                {
                    "start": 507,
                    "end": 527,
                    "matchedPaperCorpusId": "218971783"
                },
                {
                    "start": 551,
                    "end": 570,
                    "matchedPaperCorpusId": "218971783"
                },
                {
                    "start": 853,
                    "end": 878,
                    "matchedPaperCorpusId": "17651092"
                },
                {
                    "start": 878,
                    "end": 893,
                    "matchedPaperCorpusId": "31375995"
                },
                {
                    "start": 893,
                    "end": 912,
                    "matchedPaperCorpusId": "7785881"
                },
                {
                    "start": 912,
                    "end": 929,
                    "matchedPaperCorpusId": "2238772"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.529296875
        },
        {
            "corpus_id": "272987828",
            "title": "Aggressive Post-Training Compression on Extremely Large Language Models",
            "text": "To perform pruning on large language models (LLMs), we employ the open-source models BLOOM-176B Scao et al. [2022] and OPT Zhang et al. [2022] (OPT-125M, OPT-6.7B, OPT-30B, and OPT-66B). We conduct experiments on a single Nvidia A100 GPU, fixing the sparsity setting to 0.7 during the testing of SparseGPT. Our layerwise sparsity scheduler is restricted to output only final sparsities greater than 0.7. When applying quantizations, we quantize the weights to 4 bits. To calibrate our method, we select 128 sequences with 2048 tokens from C4 Raffel et al. [2020], while we use WikiText2 Merity et al. [2016] as the validation dataset and report perplexity as the metric. \n\nFor comparison with baselines, we compare our method against SparseGPT, which is currently the state-of-the-art extremely-large LLM pruning technique, and a naive layer-wise sparsity scheduler based on the sequential order of layers.",
            "score": 0.67705221624148,
            "section_title": "Experiments 4.1 Settings",
            "char_start_offset": 15760,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 163
                },
                {
                    "start": 164,
                    "end": 186
                },
                {
                    "start": 187,
                    "end": 306
                },
                {
                    "start": 307,
                    "end": 403
                },
                {
                    "start": 404,
                    "end": 467
                },
                {
                    "start": 468,
                    "end": 670
                },
                {
                    "start": 673,
                    "end": 906
                }
            ],
            "ref_mentions": [
                {
                    "start": 542,
                    "end": 562,
                    "matchedPaperCorpusId": "204838007"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.51171875
        },
        {
            "corpus_id": "270559363",
            "title": "Bypass Back-propagation: Optimization-based Structural Pruning for Large Language Models via Policy Gradient",
            "text": "In contrast to moderate-size neural network pruning, structural weight pruning on the Large-Language Models (LLMs) imposes a novel challenge on the efficiency of the pruning algorithms, due to the heavy computation/memory demands of the LLMs. Recent efficient LLM pruning methods typically operate at the post-training phase without the expensive weight finetuning, however, their pruning criteria often rely on heuristically hand-crafted metrics, potentially leading to suboptimal performance. We instead propose a novel optimization-based structural pruning that learns the pruning masks in a probabilistic space directly by optimizing the loss of the pruned model. To preserve the efficiency, our method eliminates the back-propagation through the LLM per se during the optimization, requiring only the forward pass of the LLM. We achieve this by learning an underlying Bernoulli distribution to sample binary pruning masks, where we decouple the Bernoulli parameters from the LLM loss, thus facilitating an efficient optimization via a policy gradient estimator without back-propagation. As a result, our method is able to 1) operate at structural granularities of channels, heads, and layers, 2) support global and heterogeneous pruning (i.e., our method automatically determines different redundancy for different layers), and 3) optionally initialize with a metric-based method (for our Bernoulli distributions). Extensive experiments on LLaMA, LLaMA-2, LLaMA-3, Vicuna, and Mistral using the C4 and WikiText2 datasets demonstrate that our method operates for 2.7 hours with around 35GB memory for the 13B models on a single A100 GPU, and our pruned models outperform the state-of-the-arts w.r.t. both perplexity and the majority of various zero-shot tasks. Codes will be released.",
            "score": 0.6757391747143706,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.72705078125
        },
        {
            "corpus_id": "278327238",
            "title": "Efficient Shapley Value-based Non-Uniform Pruning of Large Language Models",
            "text": "Pruning large language models (LLMs) is a promising solution for reducing model sizes and computational complexity while preserving performance. Traditional layer-wise pruning methods often adopt a uniform sparsity approach across all layers, which leads to suboptimal performance due to the varying significance of individual transformer layers within the model not being accounted for. To this end, we propose the Shapley Value-based Non-Uniform Pruning (SV-NUP) method for LLMs. This approach quantifies the contribution of each transformer layer to the overall model performance, enabling the assignment of tailored pruning budgets to different layers to retain critical parameters. To further improve efficiency, we design the Sliding Window-based Shapley Value approximation method. It substantially reduces computational overhead compared to exact SV calculation methods. Extensive experiments on various LLMs including LLaMA-v1, LLaMA-v2 and OPT demonstrate the effectiveness of the proposed approach. The results reveal that non-uniform pruning significantly enhances the performance of pruned models. Notably, SV-NUP achieves a reduction in perplexity (PPL) of 18.01% and 19.55% on LLaMA-7B and LLaMA-13B, respectively, compared to SparseGPT at 70% sparsity.",
            "score": 0.6753413517950991,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.767578125
        },
        {
            "corpus_id": "263830786",
            "title": "Sheared LLaMA: Accelerating Language Model Pre-training via Structured Pruning",
            "text": "Large language models (LLMs) are extremely performant on a wide range of natural language tasks, but they require enormous amounts of compute to train (OpenAI, 2023;Anthropic, 2023). As such, there is growing interest in building strong moderate-sized models, such as LLaMA (Touvron et al., 2023a;b), MPT (MosaicML, 2023), and Falcon (Almazrouei et al., 2023), that allow for efficient inference and fine-tuning. These LLMs are available in varied sizes suited for different use cases, but training each individual model from scratch-even the smallest billion-parameter models-requires substantial computational resources that are cost-prohibitive for most organizations. In this work, we seek to address the following question: \n\nCan we produce a smaller, general-purpose, and competitive LLM by leveraging existing pre-trained LLMs, while using much less compute than training one from scratch? \n\nWe explore structured pruning as a means to achieve this goal. Pruning is commonly viewed as a solution for compressing task-specific models (Han et al., 2016;Li et al., 2016;Lagunas et al., 2021;Xia et al., 2022;Kurtic et al., 2023), removing redundant parameters and accelerating inference without sacrificing task performance. However, for general-purpose LLMs, pruning inevitably results in performance degradation compared to original models (Frantar & Alistarh, 2023;Sun et al., 2023;Ma et al., 2023), especially when without significant compute invested post-pruning. In this work, we use pruning as an effective approach for developing smaller yet competitive LLMs that require only a fraction of the training compute compared to training them from scratch. \n\nWe identify two key technical challenges in this problem. First, how can we decide on final pruned architectures that are strong in performance and efficient for inference?",
            "score": 0.674918765468881,
            "section_title": "INTRODUCTION",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 182
                },
                {
                    "start": 183,
                    "end": 412
                },
                {
                    "start": 413,
                    "end": 671
                },
                {
                    "start": 672,
                    "end": 728
                },
                {
                    "start": 731,
                    "end": 896
                },
                {
                    "start": 899,
                    "end": 961
                },
                {
                    "start": 962,
                    "end": 1228
                },
                {
                    "start": 1229,
                    "end": 1473
                },
                {
                    "start": 1474,
                    "end": 1664
                },
                {
                    "start": 1667,
                    "end": 1724
                },
                {
                    "start": 1725,
                    "end": 1839
                }
            ],
            "ref_mentions": [
                {
                    "start": 1040,
                    "end": 1058,
                    "matchedPaperCorpusId": "2134321"
                },
                {
                    "start": 1058,
                    "end": 1074,
                    "matchedPaperCorpusId": "14089312"
                },
                {
                    "start": 1095,
                    "end": 1112,
                    "matchedPaperCorpusId": "247922354"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.5107421875
        },
        {
            "corpus_id": "260815690",
            "title": "Deja Vu: Contextual Sparsity for Efficient LLMs at Inference Time",
            "text": "Large language models (LLMs), such as GPT-3, PaLM, and OPT have demonstrated that an immense number of parameters unleashes impressive performance and emergent in-context-learning abilities-they can perform a task by conditioning on input-output examples, without updating their parameters (Bommasani et al., 2021;Liang et al., 2022;Brown et al., 2020;Min et al., 2022;Chan et al., 2022). However, they are very expensive at inference time, especially for latency-sensitive applications (Pope et al., 2022). An ideal inference-time model should use less computation and memory while maintaining the performance and special abilities of pre-trained LLMs. The simplest and most natural approach is sparsification or pruning, which has a long history before the LLM era (LeCun et al., 1989). Unfortunately, speeding up inference-time sparse LLMs in wall-clock time while maintaining quality and in-context learning abilities remains a challenging problem. \n\nWhile sparsity and pruning have been well-studied, they have not seen wide adoption on LLMs due to the poor quality and efficiency trade-offs on modern hardware such as GPUs. First, it is infeasible to retrain or iteratively prune models at the scale of hundreds of billions of parameters. Thus, methods in iterative pruning and lottery ticket hypothesis (Lee et al., 2018;Frankle & Carbin, 2018) can only be applied to smaller-scale models. Second, it is challenging to find sparsity that preserves the in-context learning ability of LLMs. Many works have shown the effectiveness of task-dependent pruning (Michel et al., 2019;Bansal et al., 2022), but maintaining different models for each task conflicts with the task independence goal of LLMs. Lastly, it is hard to achieve wall-clock time speed-up with unstructured sparsity due to its well-known difficulty with modern hardware (Hooker, 2021).",
            "score": 0.6726854786081429,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 388
                },
                {
                    "start": 389,
                    "end": 507
                },
                {
                    "start": 508,
                    "end": 653
                },
                {
                    "start": 654,
                    "end": 788
                },
                {
                    "start": 789,
                    "end": 952
                },
                {
                    "start": 955,
                    "end": 1129
                },
                {
                    "start": 1130,
                    "end": 1244
                },
                {
                    "start": 1245,
                    "end": 1396
                },
                {
                    "start": 1397,
                    "end": 1495
                },
                {
                    "start": 1496,
                    "end": 1702
                },
                {
                    "start": 1703,
                    "end": 1854
                }
            ],
            "ref_mentions": [
                {
                    "start": 333,
                    "end": 352,
                    "matchedPaperCorpusId": "218971783"
                },
                {
                    "start": 369,
                    "end": 387,
                    "matchedPaperCorpusId": "248665718"
                },
                {
                    "start": 767,
                    "end": 787,
                    "matchedPaperCorpusId": "7785881"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.47900390625
        },
        {
            "corpus_id": "272910976",
            "title": "MaskLLM: Learnable Semi-Structured Sparsity for Large Language Models",
            "text": "Large Language Models (LLMs) have demonstrated remarkable effectiveness across a diverse range of tasks [19,6,48,13]. However, the generality and robustness of LLMs are largely attributed to their vast scale, with parameter counts ranging from one billion to several hundred billion [39,45,5]. This substantial model size, in turn, makes it challenging and resource-intensive to deploy LLMs in real-world applications. One effective and practical approach to address this issue is semi-structured pruning [29,32,12,38], which introduces N:M sparsity into LLMs to improve both memory and computational efficiency. The N:M pattern, with N non-zero values among M consecutive parameters, is typically hardware-friendly to accelerators like GPUs and has thus garnered considerable attention [12,38,32]. \n\nDespite the simplicity of its core idea, semi-structured pruning still presents considerable challenges within the realm of LLMs. Sparsity aims to identify a subset of parameters that attain a comparable quality to the dense model. Nevertheless, the extensive parameter scale of large language models usually leads to a vast search space. In a fully sparsified LLaMA2-7B model with 2:4 sparsity, for instance, there are 1.6 billion 2:4 masks to be chosen for dense layers. This makes the combinatorial problem of finding the optimal mask set exceedingly challenging. In the literature, leading approaches such as SparseGPT [12] and Wanda [38], utilize a small calibration set and carefully designed importance criteria to identify redundant parameters. While these techniques have demonstrated remarkable results on several large language models, two substantial challenges remain: Firstly, the small calibration set is insufficient to represent the comprehensive knowledge embedded in LLMs, which are pre-trained on extensive and diverse data domains [39,7,31]. As demonstrated in our experiments, hand-crafted importance criteria are only applicable to a compact subset of data, and enlarging the calibration set beyond 256 entries does not improve the resulting quality. This limits the generalizability of pruned LLMs in different domains. Secondly, using handcrafted criteria as a proxy for the true discrepancy inevitably results in errors.",
            "score": 0.6718167854894711,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 117
                },
                {
                    "start": 118,
                    "end": 293
                },
                {
                    "start": 294,
                    "end": 418
                },
                {
                    "start": 419,
                    "end": 612
                },
                {
                    "start": 613,
                    "end": 798
                },
                {
                    "start": 801,
                    "end": 930
                },
                {
                    "start": 931,
                    "end": 1032
                },
                {
                    "start": 1033,
                    "end": 1139
                },
                {
                    "start": 1140,
                    "end": 1273
                },
                {
                    "start": 1274,
                    "end": 1367
                },
                {
                    "start": 1368,
                    "end": 1553
                },
                {
                    "start": 1554,
                    "end": 1863
                },
                {
                    "start": 1864,
                    "end": 2074
                },
                {
                    "start": 2075,
                    "end": 2144
                },
                {
                    "start": 2145,
                    "end": 2247
                }
            ],
            "ref_mentions": [
                {
                    "start": 110,
                    "end": 113,
                    "matchedPaperCorpusId": "259129398"
                },
                {
                    "start": 290,
                    "end": 292,
                    "matchedPaperCorpusId": "218971783"
                },
                {
                    "start": 512,
                    "end": 515,
                    "matchedPaperCorpusId": "255372747"
                },
                {
                    "start": 787,
                    "end": 791,
                    "matchedPaperCorpusId": "255372747"
                },
                {
                    "start": 1424,
                    "end": 1428,
                    "matchedPaperCorpusId": "255372747"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.71728515625
        },
        {
            "corpus_id": "269605957",
            "title": "COPAL: Continual Pruning in Large Language Generative Models",
            "text": "The advent of Large Language Models (LLMs) such as GPT-3 (Brown et al., 2020) and LLaMA (Touvron et al., 2023) has been a landmark in natural language processing (NLP).Adapting these pre-trained LLMs to diverse domains has offered unprecedented capabilities in various NLP tasks including language understanding and generation (Gururangan et al., 2020).However, there are two primary challenges associated with this approach: (i) extensive computational requirements; and (ii) limited model adaptability.Considering the large-scale models and datasets, the retraining procedure necessitates considerable computational effort, which often limits its feasibility in resource-constrained environments.Moreover, once the models are updated for certain tasks or domains, they may not exhibit the same 1 Samsung Semiconductor, San Jose, USA.Correspondence to: Joon Hee Choi <jh4.choi@samsung.com>,Chiho Choi <chiho1.choi@samsung.com>.Perplexity BWT Sparsity Ratio vs Perplexity BWT COPAL (mean) Wanda (mean) SparseGPT (mean) COPAL (max) Wanda (max) SparseGPT (max)\n\nFigure 1: Motivation: average (mean) and worst (max) case scenarios of backward transfer (BWT) in perplexity with increase in sparsity ratio in unstructured continual pruning of LLaMA-7B level of performance when confronted with data that deviate significantly from what they were trained on (known as \"catastrophic forgetting\").\n\nTraditional methods have tackled these challenges separately from different standpoints, either utilizing neural network pruning (Frantar & Alistarh, 2023) or adopting continual learning techniques (Kirkpatrick et al., 2017).The former (i.e., pruning) suggests to eliminate less critical connections (Frankle & Carbin, 2018) or structural elements (He et al., 2017) of the neural network, being beneficial in enhancing the efficiency of model inference.",
            "score": 0.6712740604563943,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 168
                },
                {
                    "start": 168,
                    "end": 353
                },
                {
                    "start": 353,
                    "end": 504
                },
                {
                    "start": 504,
                    "end": 698
                },
                {
                    "start": 698,
                    "end": 835
                },
                {
                    "start": 835,
                    "end": 891
                },
                {
                    "start": 891,
                    "end": 928
                },
                {
                    "start": 928,
                    "end": 1058
                },
                {
                    "start": 1060,
                    "end": 1389
                },
                {
                    "start": 1391,
                    "end": 1616
                },
                {
                    "start": 1616,
                    "end": 1844
                }
            ],
            "ref_mentions": [
                {
                    "start": 57,
                    "end": 77,
                    "matchedPaperCorpusId": "218971783"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.279296875
        },
        {
            "corpus_id": "273811289",
            "title": "MoE-I\u00b2: Compressing Mixture of Experts Models through Inter-Expert Pruning and Intra-Expert Low-Rank Decomposition",
            "text": "Recent advancements in large language models have underscored the need to reduce parameter sizes and latency (Ma et al., 2023). Compression techniques for language models include network pruning (Xu et al., 2021), knowledge distillation (Sun et al., 2019(Sun et al., , 2020)), quantization (Yao et al., 2022), decomposition (Hsu et al., 2022;Yuan et al., 2023;Wang et al., 2024), and methods like early exit (Xin et al., 2020). Building on these techniques, pruning, and sparsity is crucial for MoE models, which often have up to 95% of parameters dedicated to experts. Pruning MoE models involves removing less important experts or neurons to reduce the number of active parameters during inference. For example, (Kim et al., 2021) retains the most activated experts to enhance machine translation MoE models, while (Koishekenov et al., 2022) introduces gate statistics-based pruning during decoding. Although effective, these methods are mostly confined to linguistic models in machine translation. (Chen et al., 2022) dropping-while-training approach progressively removes non-essential experts for specific tasks, tested on Switch Transformers (Fedus et al., 2022). The merge-compression (Li et al., 2024) method and EPP (Lu et al., 2024) approach, which is similar to ours, consider pruning and skipping in MoE models but face challenges in reducing computational costs. Given a pruned or sparse model, finetuning aims to restore performance on original tasks. Recent studies on LLMs (Sun et al., 2023;Ma et al., 2023) focus on pruning linear layers, but these methods often fail to reduce computing costs without specialized hardware or libraries. Efficient post-finetuning expert pruning and sparsity methods for task-agnostic MoE LLMs remain underexplored. This gap highlights the need for advanced techniques to effectively balance pruning and sparsity while maintaining or enhancing performance across various tasks.",
            "score": 0.6702382104817418,
            "section_title": "Compression on MoE LLMs",
            "char_start_offset": 6022,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 127
                },
                {
                    "start": 128,
                    "end": 427
                },
                {
                    "start": 428,
                    "end": 569
                },
                {
                    "start": 570,
                    "end": 700
                },
                {
                    "start": 701,
                    "end": 901
                },
                {
                    "start": 902,
                    "end": 1000
                },
                {
                    "start": 1001,
                    "end": 1169
                },
                {
                    "start": 1170,
                    "end": 1375
                },
                {
                    "start": 1376,
                    "end": 1465
                },
                {
                    "start": 1466,
                    "end": 1653
                },
                {
                    "start": 1654,
                    "end": 1764
                },
                {
                    "start": 1765,
                    "end": 1926
                }
            ],
            "ref_mentions": [
                {
                    "start": 109,
                    "end": 126,
                    "matchedPaperCorpusId": "258823276"
                },
                {
                    "start": 290,
                    "end": 308,
                    "matchedPaperCorpusId": "249395624"
                },
                {
                    "start": 1148,
                    "end": 1168,
                    "matchedPaperCorpusId": "231573431"
                },
                {
                    "start": 1192,
                    "end": 1209,
                    "matchedPaperCorpusId": "263605809"
                },
                {
                    "start": 1507,
                    "end": 1523,
                    "matchedPaperCorpusId": "258823276"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.82470703125
        },
        {
            "corpus_id": "259203115",
            "title": "A Simple and Effective Pruning Approach for Large Language Models",
            "text": "In addition to LLaMA and LLaMA-2, we experiment with three previous LLM model families: namely OPT (Zhang et al., 2022), BLOOM (Scao et al., 2022) and Pythia (Biderman et al., 2023). \n\nComparison with Baselines. For OPT and Pythia, we experiment with varying sparsity levels (10% to 50%). We conduct additional evaluation on OPT and BLOOM models with various sizes. Results are shown in Table 9, Table 10 and Table 11 respectively. Our observations are as follows: \n\n\u2022 Unlike LLaMA and LLaMA-2, the well-established magnitude pruning approach fails catastropically on OPT-13B and Pythia-12B, even for low sparsity levels (e.g., 20%). This result further highlights the limitations of magnitude pruning for LLMs, as discussed in Section 3. \u2022 Unlike magnitude pruning, Wanda successfully prunes these LLMs to much higher sparsities across various LLM model families, without any weight update on the kept weights. This result shows that LLMs have effective sub-networks that are exact. We hope this observation could contribute to a better understanding of sparsity in LLMs. \u2022 There are cases where Wanda slightly underperforms SparseGPT, especially for OPT models (see Table 10), suggesting that for OPT, there may be a tradeoff between pruning speed and pruning accuracy. However, the gap between SparseGPT and Wanda tends to get smaller as model sizes increase. This can be seen in Table 10",
            "score": 0.6701020253472857,
            "section_title": "B WANDA ON PREVIOUS LLMS",
            "char_start_offset": 33593,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 182
                },
                {
                    "start": 185,
                    "end": 211
                },
                {
                    "start": 212,
                    "end": 288
                },
                {
                    "start": 289,
                    "end": 365
                },
                {
                    "start": 366,
                    "end": 431
                },
                {
                    "start": 432,
                    "end": 464
                },
                {
                    "start": 467,
                    "end": 633
                },
                {
                    "start": 634,
                    "end": 911
                },
                {
                    "start": 912,
                    "end": 983
                },
                {
                    "start": 984,
                    "end": 1072
                },
                {
                    "start": 1073,
                    "end": 1271
                },
                {
                    "start": 1272,
                    "end": 1362
                },
                {
                    "start": 1363,
                    "end": 1391
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.609375
        },
        {
            "corpus_id": "260887757",
            "title": "A Survey on Deep Neural Network Pruning: Taxonomy, Comparison, Analysis, and Recommendations",
            "text": "In contrast to the general PAT methods that follow the Pretrain-Prune-Retrain procedure, recently proposed posttraining pruning methods simplify the three-step process Pretrain-Prune. It involves pruning a pre-trained model f (x; W t ) without retraining, typically achieving negligible accuracy loss by using compensation mechanisms to mitigate performance degradation. This class of pruning methods is particularly attractive for billion-parameter models because retraining such pruned models is still very expensive. For example, Kwon et al. [145] propose a structured post-training pruning framework for Transformers that features Fisher-based mask search, rearrangement, and tuning, achieving pruning on a single GPU in three minutes without retraining. SparseGPT [17], a groundbreaking unstructured post-training pruning method for LLMs, tackles the pruning problem as an approximate sparsity reconstruction problem and prunes LLMs at least 50% sparsity with minor accuracy loss without retraining. To address SparseGPT's reconstruction cost, Wanda [49] uses weight magnitudes and input norms to facilitate unstructured post-training pruning on LLMs without updating weights. Unlike SparseGPT and Wanda, SliceGPT [19] implements structured post-training pruning using orthogonal matrix transformations and principal component analysis (PCA) to remove columns and rows of the weight matrices in LLMs. FLAP [146] introduces a fluctuation metric and uses a bias compensation mechanism for LLMs' performance recovery.",
            "score": 0.6699960038574198,
            "section_title": "Post-Training Pruning",
            "char_start_offset": 34370,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 183
                },
                {
                    "start": 184,
                    "end": 370
                },
                {
                    "start": 371,
                    "end": 519
                },
                {
                    "start": 520,
                    "end": 758
                },
                {
                    "start": 759,
                    "end": 1004
                },
                {
                    "start": 1005,
                    "end": 1181
                },
                {
                    "start": 1182,
                    "end": 1405
                },
                {
                    "start": 1406,
                    "end": 1519
                }
            ],
            "ref_mentions": [
                {
                    "start": 1055,
                    "end": 1059,
                    "matchedPaperCorpusId": "259203115"
                },
                {
                    "start": 1411,
                    "end": 1416,
                    "matchedPaperCorpusId": "266362404"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.75341796875
        },
        {
            "corpus_id": "278339531",
            "title": "ReplaceMe: Network Simplification via Layer Pruning and Linear Transformations",
            "text": "Model pruning [19,12] has been at the frontier of deep-learning research since the early developments in this field. It has found practical applications not only in reducing the model size but also in enhancing the interpretability of the models under study. The same holds true for pruning large language models (LLMs). \n\nA significant number of studies focuses on unstructured pruning, where individual weights within matrices throughout the model are zeroed out, resulting in sparse connections. SparseGPT [9] tackles the challenge of layer-wise reconstruction in pruning by leveraging approximations of the inverse Hessian matrix. Wanda [37] improves the SparseGPT idea of reducing computations via simplification of the Hessian approximation. The LLM Surgeon [40] uses Kronecker-factored curvature approximations to perform pruning of LLMs. Despite maintaining high model quality post-pruning, computational savings from unstructured pruning requires specialized hardware support for sparse computations, limiting its wide applicability. \n\nIn contrast, structured pruning involves the complete elimination of certain structures inside the network. In this context, removing entire attention heads or MLP channels is referred to as width pruning. LLM-Pruner [25] suggested to calculate an importance metric based on the difference in the loss when this is computed with and without a pruned group of weights, respectively. FLAP [1] proposed a training free approach that is based on a fluctuation pruning metric and an adaptive compression ratio. \n\nAnother typical strategy within structured pruning, which is also the focus of this work, is depth pruning. Methods that fall in this category, aim to remove entire transformer layers of the network. In Shortened llama [15], the authors suggested to identify the significance of each decoder layer using perplexity analysis and a Taylor metric. This Taylor metric is based on a similar idea with the LLM-Pruner importance metirc, that is it measures the difference of the model loss when it is computed with and without a pruned layer. After pruning, the authors [15] further propose to do healing through LoRA fine-tuning, continual pre-training, or their combination.",
            "score": 0.6682795637153139,
            "section_title": "Related Work",
            "char_start_offset": 24238,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 116
                },
                {
                    "start": 117,
                    "end": 258
                },
                {
                    "start": 259,
                    "end": 320
                },
                {
                    "start": 323,
                    "end": 498
                },
                {
                    "start": 499,
                    "end": 634
                },
                {
                    "start": 635,
                    "end": 747
                },
                {
                    "start": 748,
                    "end": 845
                },
                {
                    "start": 846,
                    "end": 1042
                },
                {
                    "start": 1045,
                    "end": 1152
                },
                {
                    "start": 1153,
                    "end": 1250
                },
                {
                    "start": 1251,
                    "end": 1426
                },
                {
                    "start": 1427,
                    "end": 1550
                },
                {
                    "start": 1553,
                    "end": 1660
                },
                {
                    "start": 1661,
                    "end": 1752
                },
                {
                    "start": 1753,
                    "end": 1897
                },
                {
                    "start": 1898,
                    "end": 2088
                },
                {
                    "start": 2089,
                    "end": 2222
                }
            ],
            "ref_mentions": [
                {
                    "start": 14,
                    "end": 18,
                    "matchedPaperCorpusId": "7785881"
                },
                {
                    "start": 18,
                    "end": 21,
                    "matchedPaperCorpusId": "2238772"
                },
                {
                    "start": 509,
                    "end": 512,
                    "matchedPaperCorpusId": "255372747"
                },
                {
                    "start": 641,
                    "end": 645,
                    "matchedPaperCorpusId": "259203115"
                },
                {
                    "start": 764,
                    "end": 768,
                    "matchedPaperCorpusId": "266573164"
                },
                {
                    "start": 1262,
                    "end": 1266,
                    "matchedPaperCorpusId": "258823276"
                },
                {
                    "start": 1432,
                    "end": 1435,
                    "matchedPaperCorpusId": "266362404"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.7080078125
        },
        {
            "corpus_id": "266818263",
            "title": "Fast and Effective Weight Update for Pruned Large Language Models",
            "text": "Large language models (LLMs) (Brown et al., 2020;Zhang et al., 2022;Touvron et al., 2023a;b) have displayed impressive performance in different tasks, but deploying them can be challenging due to their large size and high memory demands. In this work, we introduce a one-shot pruning and weight update algorithm for LLMs that is both fast and effective. Our algorithm produces state-of-the-art results for LLM pruning while imposing minimal computational overhead (Table 1). \n\nNeural networks are usually compressed by either quantization or weight pruning. LLM quantization (Dettmers et al., 2022;Dettmers & Zettlemoyer, 2023;Ahmadian et al., 2023;Xiao et al., 2023) compresses LLMs by storing weights using a small number of bits. On the other hand, pruning compresses models by dropping irrelevant weights (LeCun et al., 1989;Han et al., 2015;Zhu & Gupta, 2018). Pruning can be helpful for LLMs since, during inference, the main bottleneck is memory bandwidth for loading weights to processing unit (Xia et al., 2023). However, the main challenge in deploying LLM pruning is that the network needs to be fine-tuned (Blalock et al., 2020;Liu et al., 2018), which is not feasible with LLMs due to extensive computational and memory footprint. For example, Agarwalla et al. (2024) needed retraining on 45 -100 billion tokens to recover lost performance by pruning. Also, memory-efficient fine-tuning like LoRA (Hu et al., 2021) is not applicable for LLM weight pruning since we cannot easily merge the low-rank update with the sparsified matrix. \n\nA feasible alternative is one-shot pruning, where one is given a trained model with a small amount of calibration data, and has to compress the model in a single forward pass using limited computational resources.",
            "score": 0.6670919083429026,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 237
                },
                {
                    "start": 238,
                    "end": 353
                },
                {
                    "start": 354,
                    "end": 474
                },
                {
                    "start": 477,
                    "end": 557
                },
                {
                    "start": 558,
                    "end": 732
                },
                {
                    "start": 733,
                    "end": 865
                },
                {
                    "start": 866,
                    "end": 1021
                },
                {
                    "start": 1022,
                    "end": 1243
                },
                {
                    "start": 1244,
                    "end": 1364
                },
                {
                    "start": 1365,
                    "end": 1545
                },
                {
                    "start": 1548,
                    "end": 1761
                }
            ],
            "ref_mentions": [
                {
                    "start": 29,
                    "end": 49,
                    "matchedPaperCorpusId": "218971783"
                },
                {
                    "start": 598,
                    "end": 627,
                    "matchedPaperCorpusId": "254853733"
                },
                {
                    "start": 649,
                    "end": 667,
                    "matchedPaperCorpusId": "253708271"
                },
                {
                    "start": 809,
                    "end": 829,
                    "matchedPaperCorpusId": "7785881"
                },
                {
                    "start": 829,
                    "end": 846,
                    "matchedPaperCorpusId": "2238772"
                },
                {
                    "start": 846,
                    "end": 864,
                    "matchedPaperCorpusId": "27494814"
                },
                {
                    "start": 1410,
                    "end": 1427,
                    "matchedPaperCorpusId": "231934142"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.52685546875
        },
        {
            "corpus_id": "258823276",
            "title": "LLM-Pruner: On the Structural Pruning of Large Language Models",
            "text": "Large language models (LLMs) have shown remarkable capabilities in language understanding and generation. However, such impressive capability typically comes with a substantial model size, which presents significant challenges in both the deployment, inference, and training stages. With LLM being a general-purpose task solver, we explore its compression in a task-agnostic manner, which aims to preserve the multi-task solving and language generation ability of the original LLM. One challenge to achieving this is the enormous size of the training corpus of LLM, which makes both data transfer and model post-training over-burdensome. Thus, we tackle the compression of LLMs within the bound of two constraints: being task-agnostic and minimizing the reliance on the original training dataset. Our method, named LLM-Pruner, adopts structural pruning that selectively removes non-critical coupled structures based on gradient information, maximally preserving the majority of the LLM's functionality. To this end, the performance of pruned models can be efficiently recovered through tuning techniques, LoRA, in merely 3 hours, requiring only 50K data. We validate the LLM-Pruner on three LLMs, including LLaMA, Vicuna, and ChatGLM, and demonstrate that the compressed models still exhibit satisfactory capabilities in zero-shot classification and generation. The code is available at: https://github.com/horseee/LLM-Pruner",
            "score": 0.6663354806066972,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.87939453125
        },
        {
            "corpus_id": "271533761",
            "title": "Greedy Output Approximation: Towards Efficient Structured Pruning for LLMs Without Retraining",
            "text": "This paper introduces a novel approach to pruning large language models (LLMs) by identifying a depth-2 pruning structure and developing two inference-aware pruning criteria. These methods surpass traditional metrics and eliminate the need for computationally expensive retraining. Our two-step reconstruction technique further mitigates pruning errors, ensuring superior performance across various datasets and models. Overall, our approach significantly reduces computational costs and hardware requirements, offering an efficient solution for pruning colossal LLMs.",
            "score": 0.6663325115563953,
            "section_title": "Conclusion:",
            "char_start_offset": 29816,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 174
                },
                {
                    "start": 175,
                    "end": 281
                },
                {
                    "start": 282,
                    "end": 419
                },
                {
                    "start": 420,
                    "end": 568
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.8173828125
        },
        {
            "corpus_id": "276884735",
            "title": "Wanda++: Pruning Large Language Models via Regional Gradients",
            "text": "Large Language Models (LLMs) pruning seeks to remove unimportant weights for inference speedup with minimal performance impact. However, existing methods often suffer from performance loss without full-model sparsity-aware fine-tuning. This paper presents Wanda++, a novel pruning framework that outperforms the state-of-the-art methods by utilizing decoder-block-level \\textbf{regional} gradients. Specifically, Wanda++ improves the pruning score with regional gradients for the first time and proposes an efficient regional optimization method to minimize pruning-induced output discrepancies between the dense and sparse decoder output. Notably, Wanda++ improves perplexity by up to 32\\% over Wanda in the language modeling task and generalizes effectively to downstream tasks. Further experiments indicate our proposed method is orthogonal to sparsity-aware fine-tuning, where Wanda++ can be combined with LoRA fine-tuning to achieve a similar perplexity improvement as the Wanda method. The proposed method is lightweight, pruning a 7B LLaMA model in under 10 minutes on a single NVIDIA H100 GPU.",
            "score": 0.6647676437693106,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.791015625
        },
        {
            "corpus_id": "278208127",
            "title": "Efficient LLMs with AMP: Attention Heads and MLP Pruning",
            "text": "While the method achieves competitive results, it requires significant computational resources. \n\nIn contrast to the existing approaches we mentioned above, our AMP strategy prunes attention heads and MLP neurons by using activation magnitudes from a small number of forward passes, rather than relying on computationally expensive lossbased or projection-driven heuristics. Our effective design avoids rigid constraints, such as pruning only the Multilayer Perceptron. By applying uniform pruning across layers, AMP also prevents the creation of layers with varying widths, thereby avoiding additional overhead. Semi-Structured Pruning. COPAL [27] prunes weights in Large Language Models by analyzing directional derivatives of the loss function across datasets. Although effective, it requires numerous differential operations to determine importance, thereby increasing its computational cost. Wanda [10] identifies sparse sub-networks in LLMs by ranking each weight using the product of its absolute value and its squared input feature norm. However, as Gao et al. [11] argue, Wanda excels only as an unstructured pruning technique, a form of sparsity that does not lead to inference speedups in the resulting model. Unlike these methods, AMP does not require specialized hardware to achieve practical speedups. Furthermore, while existing techniques are constrained to fixed sparsity patterns such as 2:4 or 4:8, limiting their compression rates, AMP offers greater flexibility in achieving desired compression levels without being bound by predefined sparsity structures. Unstructured Pruning. SparseGPT [16] formulates pruning as a sparse regression solved via an efficient approximate mechanism, achieving up to 60% sparsity while preserving model performance without traditional retraining. However, it performs less accurately on small to medium-sized models compared to larger networks. Plug-and-Play [28] introduces a one-shot post-training pruning combining Relative Importance and Activation (RIA) with Channel Permutation. Despite avoiding retraining, it exhibits sensitivity to calibration data and increases complexity, particularly for larger models. Moreover, these methods often fail to provide practical inference speedups, as modern GPUs accelerate inference for semistructured patterns (e.g., 2:4) [17], [18].",
            "score": 0.6627524399543798,
            "section_title": "II. RELATED WORK",
            "char_start_offset": 7667,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 95
                },
                {
                    "start": 98,
                    "end": 374
                },
                {
                    "start": 375,
                    "end": 469
                },
                {
                    "start": 470,
                    "end": 612
                },
                {
                    "start": 613,
                    "end": 637
                },
                {
                    "start": 638,
                    "end": 763
                },
                {
                    "start": 764,
                    "end": 896
                },
                {
                    "start": 897,
                    "end": 1045
                },
                {
                    "start": 1046,
                    "end": 1220
                },
                {
                    "start": 1221,
                    "end": 1315
                },
                {
                    "start": 1316,
                    "end": 1577
                },
                {
                    "start": 1578,
                    "end": 1599
                },
                {
                    "start": 1600,
                    "end": 1799
                },
                {
                    "start": 1800,
                    "end": 1897
                },
                {
                    "start": 1898,
                    "end": 2037
                },
                {
                    "start": 2038,
                    "end": 2168
                },
                {
                    "start": 2169,
                    "end": 2332
                }
            ],
            "ref_mentions": [
                {
                    "start": 644,
                    "end": 648,
                    "matchedPaperCorpusId": "269605957"
                },
                {
                    "start": 903,
                    "end": 907,
                    "matchedPaperCorpusId": "259203115"
                },
                {
                    "start": 1069,
                    "end": 1073,
                    "matchedPaperCorpusId": "273374936"
                },
                {
                    "start": 1610,
                    "end": 1614,
                    "matchedPaperCorpusId": "255372747"
                },
                {
                    "start": 1912,
                    "end": 1916,
                    "matchedPaperCorpusId": "271745835"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.6796875
        },
        {
            "corpus_id": "264128029",
            "title": "Dynamic Sparse No Training: Training-Free Fine-tuning for Sparse LLMs",
            "text": "Recent research endeavours have evolved towards the direction of unstructured pruning in one-shot without fine-tuning, demonstrating significant progresses. SparseGPT (Frantar & Alistarh, 2023) incorporates the Hessian inverse for pruning and subsequent residual weight updates, whereas Wanda (Sun et al., 2023) directly arrives at a sparse LLM model by a criterion depicted by the multiplication of the absolute values of weights and their activations with the aim to preserve outliers (Dettmers et al., 2022) emerged in LLMs. DS\u25cbT serves as an orthogonal perspective and can be organically integrated on top of them.",
            "score": 0.6614405499286806,
            "section_title": "RELATED WORK",
            "char_start_offset": 11232,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 156
                },
                {
                    "start": 157,
                    "end": 527
                },
                {
                    "start": 528,
                    "end": 618
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.62548828125
        },
        {
            "corpus_id": "276902790",
            "title": "Sample-aware Adaptive Structured Pruning for Large Language Models",
            "text": "Large language models (LLMs), such as GPT-3 (Brown et al. 2020), OPT (Zhang et al. 2022), LLaMA (Touvron et al. 2023) and Vicuna (Chiang et al. 2023), have demonstrated significant accomplishments in the realm of natural language processing (Wei et al. 2022;Wu et al. 2020). Nevertheless, their exceptional capabilities are coupled with substantial model sizes and elevated computational expenses. Furthermore, owing to the scaling law (Hoffmann et al. 2022;Kaplan et al. 2020), LLMs tend to enhance model performance by progressively augmenting model parameters. Regrettably, larger model sizes entail heightened consumption of computational resources, presenting a notable obstacle to their practical deployment, particularly in settings with limited resources. \n\nStructured pruning (Xia et al. 2024) emerges as a pivotal technique for mitigating resource demands in the deployment of LLMs. In comparison to other strategies (Zhu et al. 2023) like unstructured pruning (Yin et al. 2023;Frantar and Alistarh 2023;Jaiswal et al. 2023), model quantization (Liu et al. 2023;Xiao et al. 2023), and knowledge distillation (Gu et al. 2024;Yuan et al. 2023;Hsieh et al. 2023), structured pruning not only provides a practical and hardwareindependent solution but also offers an effective approach to streamline LLMs implementation on devices with limited computational resources. Its unique ability to selectively remove redundant model parameters while maintaining model integrity positions structured pruning as a cornerstone in optimizing the efficiency of LLMs deployment. \n\nHowever, existing work on structured pruning commonly employs Taylor expansion as the metric for estimating the importance of structures. These methods hinge on a localized approximation of the loss function, necessitating additional calibration data for gradient information computation. As a result, the precision of the gradient is directly tied to the calibration data's quality, thereby influencing both the Taylor expansion approximation and the pruning decision. Subpar calibration data and unsuitable importance estimation metrics can lead to substantial performance degradation in pruned models.",
            "score": 0.6608254338247106,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 274
                },
                {
                    "start": 275,
                    "end": 397
                },
                {
                    "start": 398,
                    "end": 563
                },
                {
                    "start": 564,
                    "end": 763
                },
                {
                    "start": 766,
                    "end": 892
                },
                {
                    "start": 893,
                    "end": 1373
                },
                {
                    "start": 1374,
                    "end": 1570
                },
                {
                    "start": 1573,
                    "end": 1710
                },
                {
                    "start": 1711,
                    "end": 1861
                },
                {
                    "start": 1862,
                    "end": 2042
                },
                {
                    "start": 2043,
                    "end": 2177
                }
            ],
            "ref_mentions": [
                {
                    "start": 44,
                    "end": 63,
                    "matchedPaperCorpusId": "218971783"
                },
                {
                    "start": 69,
                    "end": 88,
                    "matchedPaperCorpusId": "248496292"
                },
                {
                    "start": 241,
                    "end": 258,
                    "matchedPaperCorpusId": "246411621"
                },
                {
                    "start": 258,
                    "end": 272,
                    "matchedPaperCorpusId": "226262321"
                },
                {
                    "start": 785,
                    "end": 801,
                    "matchedPaperCorpusId": "263830786"
                },
                {
                    "start": 927,
                    "end": 944,
                    "matchedPaperCorpusId": "248496292"
                },
                {
                    "start": 971,
                    "end": 988,
                    "matchedPaperCorpusId": "263829692"
                },
                {
                    "start": 988,
                    "end": 1014,
                    "matchedPaperCorpusId": "3922816"
                },
                {
                    "start": 1014,
                    "end": 1034,
                    "matchedPaperCorpusId": "259088941"
                },
                {
                    "start": 1072,
                    "end": 1089,
                    "matchedPaperCorpusId": "253708271"
                },
                {
                    "start": 1134,
                    "end": 1151,
                    "matchedPaperCorpusId": "263829692"
                },
                {
                    "start": 1151,
                    "end": 1169,
                    "matchedPaperCorpusId": "258461606"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.65673828125
        },
        {
            "corpus_id": "259203115",
            "title": "A Simple and Effective Pruning Approach for Large Language Models",
            "text": "Emergent Properties of LLMs. Our work is also related to recent studies on the existence of large magnitude outlier features in large language models (Kovaleva et al., 2021;Bondarenko et al., 2021;Timkey & Schijndel, 2021;Luo et al., 2021;Puccetti et al., 2022;Wei et al., 2022b). Dettmers et al. (2022) demonstrate that when LLMs exceed a certain parameter scale (e.g., 6B), large magnitude features start to emerge and strongly affect all layers, which can be seen as an emergent property of LLMs (Dettmers et al., 2022;Wei et al., 2022a;Schaeffer et al., 2023). They also pinpoint these emerging features as the reason why existing quantization methods fail. This observation has spurred the development of various quantization schemes (Dettmers et al., 2022;Xiao et al., 2023;Lin et al., 2023;Dettmers et al., 2023;Behdin et al., 2023) tailored specifically for LLMs to handle outlier features. Our work extends this understanding, demonstrating that outlier features should also serve as pivotal indicators of which weights to prune in LLMs. \n\nIn this work, we propose a simple and effective method for pruning Large Language Models (LLMs). Inspired by the recent discovery of emergent large magnitude features in LLMs, our approach, termed Wanda (Pruning by Weights and activations), removes weights with the smallest magnitudes multiplied by the corresponding input activation norms, on a per-output basis. Without the need for any retraining or weight update procedures, Wanda is able to identify effective sparse networks within pretrained LLMs. We hope our work contributes to a better understanding of sparsity in LLMs.",
            "score": 0.6596889071955212,
            "section_title": "RELATED WORK",
            "char_start_offset": 29683,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 28
                },
                {
                    "start": 29,
                    "end": 280
                },
                {
                    "start": 281,
                    "end": 564
                },
                {
                    "start": 565,
                    "end": 661
                },
                {
                    "start": 662,
                    "end": 898
                },
                {
                    "start": 899,
                    "end": 1046
                },
                {
                    "start": 1049,
                    "end": 1145
                },
                {
                    "start": 1146,
                    "end": 1413
                },
                {
                    "start": 1414,
                    "end": 1554
                },
                {
                    "start": 1555,
                    "end": 1630
                }
            ],
            "ref_mentions": [
                {
                    "start": 150,
                    "end": 173,
                    "matchedPaperCorpusId": "235313996"
                },
                {
                    "start": 222,
                    "end": 239,
                    "matchedPaperCorpusId": "235187469"
                },
                {
                    "start": 281,
                    "end": 303,
                    "matchedPaperCorpusId": "251564521"
                },
                {
                    "start": 499,
                    "end": 522,
                    "matchedPaperCorpusId": "251564521"
                },
                {
                    "start": 522,
                    "end": 540,
                    "matchedPaperCorpusId": "249674500"
                },
                {
                    "start": 739,
                    "end": 762,
                    "matchedPaperCorpusId": "251564521"
                },
                {
                    "start": 762,
                    "end": 780,
                    "matchedPaperCorpusId": "253708271"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.734375
        },
        {
            "corpus_id": "278208127",
            "title": "Efficient LLMs with AMP: Attention Heads and MLP Pruning",
            "text": "Within the evolving landscape of Artificial Intelligence, Large Language Models (LLMs) stand out as a pivotal force, propelling Natural Language Processing towards unprecedented stages -often matching or even surpassing human-level performance in many language benchmarks [1]- [3]. However, this performance often comes at the cost of larger model sizes, with models such as DeepSeek-V3 [4] reaching the mark of 671 billion parameters. \n\nThis substantial size introduces significant challenges for the deployment of LLMs in low-resource and time-critical applications due to high computational costs and slow inference [5]. To overcome these challenges, some techniques such as pruning [6] and quantization [7], [8] aim to reduce the model size, thereby lowering computational overhead. \n\nRecent studies confirm pruning as a promising solution to compress models as it maintains predictive ability and is often hardware-agnostic [9]- [13]. Within the field of LLMs, pruning \u00a7 Equal contribution. \n\ntechniques fall into three main categories: structured, semistructured and unstructured pruning. \n\nStructured pruning removes entire components -such as attention heads or layers -while preserving the overall network structure without introducing sparsity (i.e., without zeroing out a significant portion of the model's parameters) [6]. However, the removal of larger and potentially more critical components may result in performance degradation, typically requiring Parameter-Efficient Fine-Tuning (PEFT) techniques for performance recovery [14]. Due to the removal of complete components, structured pruning usually achieves inference acceleration and memory reduction without the need for specialized hardware or software [15]. \n\nSemi-structured (a.k.a. structured N : M ) pruning promotes model sparsity by removing groups of consecutive parameters following a pruning mask [16]. Specifically, structured N : M sparsity requires that at least N out of every M consecutive weights be non-zero [17], [18]. While this is a promising technique, it requires specialized hardware to achieve practical speedup, making it less suitable for deployment on consumergrade GPUs [17].",
            "score": 0.6591778637396062,
            "section_title": "I. INTRODUCTION",
            "char_start_offset": 18,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 281
                },
                {
                    "start": 282,
                    "end": 435
                },
                {
                    "start": 438,
                    "end": 623
                },
                {
                    "start": 624,
                    "end": 786
                },
                {
                    "start": 789,
                    "end": 939
                },
                {
                    "start": 940,
                    "end": 995
                },
                {
                    "start": 998,
                    "end": 1094
                },
                {
                    "start": 1097,
                    "end": 1334
                },
                {
                    "start": 1335,
                    "end": 1546
                },
                {
                    "start": 1547,
                    "end": 1729
                },
                {
                    "start": 1732,
                    "end": 1755
                },
                {
                    "start": 1756,
                    "end": 1882
                },
                {
                    "start": 1883,
                    "end": 2006
                },
                {
                    "start": 2007,
                    "end": 2173
                }
            ],
            "ref_mentions": [
                {
                    "start": 277,
                    "end": 280,
                    "matchedPaperCorpusId": "218971783"
                },
                {
                    "start": 619,
                    "end": 622,
                    "matchedPaperCorpusId": "257495837"
                },
                {
                    "start": 707,
                    "end": 710,
                    "matchedPaperCorpusId": "259298689"
                },
                {
                    "start": 929,
                    "end": 932,
                    "matchedPaperCorpusId": "258823276"
                },
                {
                    "start": 934,
                    "end": 938,
                    "matchedPaperCorpusId": "266573164"
                },
                {
                    "start": 1541,
                    "end": 1545,
                    "matchedPaperCorpusId": "268553763"
                },
                {
                    "start": 1724,
                    "end": 1728,
                    "matchedPaperCorpusId": "267413136"
                },
                {
                    "start": 1877,
                    "end": 1881,
                    "matchedPaperCorpusId": "255372747"
                },
                {
                    "start": 1995,
                    "end": 1999,
                    "matchedPaperCorpusId": "231847094"
                },
                {
                    "start": 2001,
                    "end": 2005,
                    "matchedPaperCorpusId": "233296249"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.59033203125
        },
        {
            "corpus_id": "273501976",
            "title": "Pruning Foundation Models for High Accuracy without Retraining",
            "text": "Despite the superior performance, it is challenging to deploy foundation models or large language models (LLMs) due to their massive parameters and computations. While pruning is a promising technique to reduce model size and accelerate the inference, the traditional pruning techniques can hardly be applied for LLMs as they need to finetune the model on the full dataset with multiple epochs consuming massive data and hardware resources. To deal with this problem, post-training pruning methods are proposed to prune LLMs in one-shot without retraining. However, their accuracy after pruning may suffer from certain performance degradation due to the lack of retraining with massive data. To address this issue, in this paper, we first formulate the post-training problem for layer-wise LLM compression to simultaneously prune multiple weights in LLMs. Next, we provide an optimal solution for this problem and design our post-training pruning algorithm for both unstructured and semi-structured sparsity. Our extensive experiments demonstrate the superior performance of the proposed methods in comparison to SOTA baselines across various LLM families including transformer-based LLMs and Mamba-based LLMs. Code link: https://github.com/piuzha/APT",
            "score": 0.6585602023926745,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.6904296875
        },
        {
            "corpus_id": "266573164",
            "title": "The LLM Surgeon",
            "text": "Recent advancements in language modeling (Vaswani et al., 2017) allow fitting large language models (LLMs) with millions or even billions of parameters (such as OPT (Zhang et al., 2022) and Llama 2 (Touvron et al., 2023)) on big text corpora achieving high performance. Unfortunately, the size of these LLMs often makes it hard to deploy them within practical constraints. Cloudbased deployment can get very expensive for larger models, and efficient devices such as phones are frequently limited in the memory size to host a model. \n\nA body of literature extending back to the late 1980s, e.g., Optimal Brain Damage (OBD, LeCun et al. (1989)) and Optimal Brain Surgeon (OBS, Hassibi & Stork (1992)), phrases pruning as a constraint optimization problem to reduce a model's footprint and runtime requirements. The Hessian required for this approach grows with the square of the number of parameters, and can only be computed in practice for unrealistically small networks. To overcome this issue, Eigendamage (Wang et al., 2019) introduces a Kronecker factorization of a blockwise-diagonal approximation of the Hessian. Recent works, like Optimal Brain Compression (Frantar & Alistarh, 2022), SparseGPT (Frantar & Alistarh, 2023), demonstrate practical post-training pruning of LLMs, but only consider a loss curvature of a pruned layer's squared output reconstruction error, ignoring gradients that relate local removal costs to the target loss. As a result, their approximation to the target loss landscape is inaccurate, leading to a significant performance degradation for pruned LLMs. Further, these methods do not readily extend to structured pruning. This work introduces LLM Surgeon, a general framework for unstructured, semi-structured and structured pruning of LLMs. At paper submission, we deemed this the first method to successfully perform structured pruning of LLMs. Concurrent work by Ashkboos et al. (2024) also considers structured pruning of LLMs but ignores gradient information, resulting in lower final performance.",
            "score": 0.6573036433555947,
            "section_title": "INTRODUCTION",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 269
                },
                {
                    "start": 270,
                    "end": 372
                },
                {
                    "start": 373,
                    "end": 532
                },
                {
                    "start": 535,
                    "end": 809
                },
                {
                    "start": 810,
                    "end": 972
                },
                {
                    "start": 973,
                    "end": 1119
                },
                {
                    "start": 1120,
                    "end": 1446
                },
                {
                    "start": 1447,
                    "end": 1589
                },
                {
                    "start": 1590,
                    "end": 1657
                },
                {
                    "start": 1658,
                    "end": 1777
                },
                {
                    "start": 1778,
                    "end": 1882
                },
                {
                    "start": 1883,
                    "end": 2038
                }
            ],
            "ref_mentions": [
                {
                    "start": 676,
                    "end": 698,
                    "matchedPaperCorpusId": "7057040"
                },
                {
                    "start": 1009,
                    "end": 1028,
                    "matchedPaperCorpusId": "155089879"
                },
                {
                    "start": 1203,
                    "end": 1229,
                    "matchedPaperCorpusId": "255372747"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.53759765625
        },
        {
            "corpus_id": "270411995",
            "title": "Applications of Pruning Methods in Natural Language Processing",
            "text": "Their objective was to reduce model size by removing redundant parameters while preserving speech recognition performance.The authors propose a structured pruning method to lessen the size and complexity of self-supervised pre-trained models while maintaining their performance on speech-related tasks.The key contributions and findings of the paper include the following:\n\n1) Pruning Strategy: The authors present a specific pruning strategy tailored for self-supervised pre-trained models in the speech domain.This strategy identifies and removes less important structures or components, such as layers or neurons, based on their significance to the model's overall performance.2) Performance Analysis: The paper evaluates the impact of structured pruning on speech recognition and understanding tasks.It assesses the model's accuracy and efficiency before and after pruning, demonstrating the potential benefits of structured pruning in reducing model size and computational requirements while preserving task performance.3) Comparison with Baselines: The authors compare their proposed structured pruning approach with other baseline methods commonly used for model compressions, such as unstructured pruning or weight quantization.The comparison highlights the advantages and effectiveness of structured pruning specifically for selfsupervised pre-trained models in the speech domain.The results show significant compression and improved accuracy compared to the original model, as validated by experiments on LibriSpeech and SLURP datasets.\n\nMa et al. [29] proposes Large Language Model-Pruner (LLM-Pruner), a task-agnostic compression method, aiming to maintain the multi-task solving and language generation abilities of the original LLM while minimizing reliance on the extensive training dataset.LLM-Pruner adopts structural pruning, removing non-critical coupled structures based on gradient information to preserve most of the LLM's functionality.The pruned models can be efficiently recovered through tuning techniques in a short time and with minimal data.Experimental results on three LLMs, Large Language Model Meta AI (LLaMA), Vicuna, and General Language Model (ChatGLM), demonstrate that the compressed models perform well in zero-shot classification and generation tasks.",
            "score": 0.6572475585317388,
            "section_title": "IV. METHODS",
            "char_start_offset": 18689,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 122
                },
                {
                    "start": 122,
                    "end": 302
                },
                {
                    "start": 302,
                    "end": 372
                },
                {
                    "start": 374,
                    "end": 512
                },
                {
                    "start": 512,
                    "end": 680
                },
                {
                    "start": 680,
                    "end": 804
                },
                {
                    "start": 804,
                    "end": 1025
                },
                {
                    "start": 1025,
                    "end": 1236
                },
                {
                    "start": 1236,
                    "end": 1389
                },
                {
                    "start": 1389,
                    "end": 1546
                },
                {
                    "start": 1548,
                    "end": 1806
                },
                {
                    "start": 1806,
                    "end": 1959
                },
                {
                    "start": 1959,
                    "end": 2070
                },
                {
                    "start": 2070,
                    "end": 2291
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.8623046875
        },
        {
            "corpus_id": "271533761",
            "title": "Greedy Output Approximation: Towards Efficient Structured Pruning for LLMs Without Retraining",
            "text": "Baselines: This paper presents a comprehensive comparison of state-of-the-art pruning methods across multiple dimensions, aiming for fair evaluations and in-depth analyses to uncover the reasons behind the observed results. First, we compare our approach with data-free pruning methods, including random pruning and magnitude-based pruning (L1 and L2 norms) [28]. Next, we evaluate our methods against data-dependent pruning techniques, encompassing training-aware, inferenceaware, and retraining-required methods. In the training-aware category, we compare with various configurations of LLM-Pruner [40], such as Element1, Element2, and Vector-wise magnitude pruning. Within the inference-aware category, we compare with the structured version of Wanda [51] and FLAP [2]. Additionally, we extend our comparisons to include the LLM-Pruner method augmented with retraining. Such comprehensive evaluations will demonstrate the effectiveness of our pruning approach. \n\nModels: Our primary experiments are categorized into two series based on the model scale: LLaMA-7B with 7 billion parameters and GPT-2 with 110 million parameters [47,55]. This aligns with our study's goal to assess pruning performance across different model sizes and ensure a thorough examination. Additionally, we extend our experiments to other models, including LLaMA-13B, Vicuna-7B [7]. This comprehensive selection allows us to explore a broader spectrum of capabilities and sizes, enhancing our understanding of how different architectures perform under various computational constraints. Additional experiment results can be found in the Appendix. \n\nEvaluation and Datasets: To evaluate performance, we adopt LLaMa's approach by conducting zero-shot task classification on a range of common sense reasoning datasets: BoolQ [8], PIQA [6], HellaSwag [63], WinoGrande [48], ARC-easy [9], ARC-challenge [9], and OpenbookQA [43]. Following the methodology in [21], the model either ranks the options in multiple-choice tasks or generates answers in open-ended formats.",
            "score": 0.657135460902591,
            "section_title": "Setup",
            "char_start_offset": 21718,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 223
                },
                {
                    "start": 224,
                    "end": 363
                },
                {
                    "start": 364,
                    "end": 514
                },
                {
                    "start": 515,
                    "end": 668
                },
                {
                    "start": 669,
                    "end": 772
                },
                {
                    "start": 773,
                    "end": 872
                },
                {
                    "start": 873,
                    "end": 963
                },
                {
                    "start": 966,
                    "end": 1137
                },
                {
                    "start": 1138,
                    "end": 1265
                },
                {
                    "start": 1266,
                    "end": 1358
                },
                {
                    "start": 1359,
                    "end": 1562
                },
                {
                    "start": 1563,
                    "end": 1622
                },
                {
                    "start": 1625,
                    "end": 1899
                },
                {
                    "start": 1900,
                    "end": 2038
                }
            ],
            "ref_mentions": [
                {
                    "start": 358,
                    "end": 362,
                    "matchedPaperCorpusId": "231740691"
                },
                {
                    "start": 600,
                    "end": 604,
                    "matchedPaperCorpusId": "258823276"
                },
                {
                    "start": 768,
                    "end": 771,
                    "matchedPaperCorpusId": "266362404"
                },
                {
                    "start": 1129,
                    "end": 1133,
                    "matchedPaperCorpusId": "160025533"
                },
                {
                    "start": 1808,
                    "end": 1811,
                    "matchedPaperCorpusId": "208290939"
                },
                {
                    "start": 1840,
                    "end": 1844,
                    "matchedPaperCorpusId": "199370376"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.6376953125
        },
        {
            "corpus_id": "270621063",
            "title": "Dependency-Aware Semi-Structured Sparsity of GLU Variants in Large Language Models",
            "text": "The rapid advancement in Large Language Models (LLMs) has markedly enhanced the capabilities of language understanding and generation. However, the substantial model size poses hardware challenges, affecting both memory size for serving and inference latency for token generation. To address those challenges, we propose Dependency-aware Semi-structured Sparsity (DaSS), a novel method for the recent prevalent GLU-based LLMs pruning, which incorporates structural dependency into the weight magnitude-based unstructured pruning. We introduce an MLP-specific pruning metric that evaluates the importance of each weight by jointly considering its magnitude and its corresponding MLP intermediate activation norms. DaSS facilitates a balance between the adaptability offered by unstructured pruning and the structural consistency inherent in dependency-based structured pruning. Empirical evaluations on LLaMA2, Mistral, and Gemma model families demonstrate that DaSS not only outperforms both SparseGPT and Wanda in achieving hardware-friendly N:M sparsity patterns but also maintains the computational efficiency of Wanda.",
            "score": 0.6570433209026793,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.81005859375
        },
        {
            "corpus_id": "267759551",
            "title": "EBFT: Effective and Block-Wise Fine-Tuning for Sparse LLMs",
            "text": "LLMs have demonstrated remarkable potential in various NLP tasks.However, the large sizes of these models pose challenges in terms of resource requirements for deployment.For instance, the inference of GPT-3 (Brown et al., 2020) in halfprecision floating-point format demands at least 5 80G A100 GPUs.To address this issue, several model compression methods, such as network quantization (Lin et al., 2023;Frantar et al., 2022), network pruning (Frantar and Alistarh, 2023), and knowledge distillation (Hsieh et al., 2023), have been proposed to compress and accelerate these Large Language Models.Among these methods, network pruning has gained increasing attention.However, pruning often leads to a decline in the performance of sparse models.To address this issue, recent works (Zhang et al., 2023d;Frantar and Alistarh, 2023;Zhang et al., 2023a) have emerged that can fine-tune the pruned models to recover their performance through regression reconstruction, costly retraining, or other heuristic methods.In this paper, we introduce EBFT, a framework designed to effectively fine-tune sparse LLMs, significantly enhancing the performance and generality of pruned models.\n\nDataset used for fine-tuning.Some existing pruning then fine-tuning approaches require significant retraining resources, partly due to the large size of the retraining dataset.For example, LLM-Pruner (Ma et al., 2023) employs Alpaca-cleaned (Taori et al., 2023) as its fine-tuning dataset to restore the performance of sparse LLMs.Alpacacleaned consists of 51.8K rows of data, resulting in substantial time costs for fine-tuning LLMs.Similarly, Sheared Llama (Xia et al., 2023) employs RedPajama (Computer, 2023), containing 2.17M rows of data, for LLM pruning and fine-tuning, which incurs huge resource costs.",
            "score": 0.6561363102981532,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 65
                },
                {
                    "start": 65,
                    "end": 171
                },
                {
                    "start": 171,
                    "end": 301
                },
                {
                    "start": 301,
                    "end": 598
                },
                {
                    "start": 598,
                    "end": 667
                },
                {
                    "start": 667,
                    "end": 745
                },
                {
                    "start": 745,
                    "end": 1010
                },
                {
                    "start": 1010,
                    "end": 1175
                },
                {
                    "start": 1177,
                    "end": 1206
                },
                {
                    "start": 1206,
                    "end": 1353
                },
                {
                    "start": 1353,
                    "end": 1508
                },
                {
                    "start": 1508,
                    "end": 1611
                },
                {
                    "start": 1611,
                    "end": 1788
                }
            ],
            "ref_mentions": [
                {
                    "start": 208,
                    "end": 228,
                    "matchedPaperCorpusId": "218971783"
                },
                {
                    "start": 406,
                    "end": 427,
                    "matchedPaperCorpusId": "253237200"
                },
                {
                    "start": 445,
                    "end": 473,
                    "matchedPaperCorpusId": "253237200"
                },
                {
                    "start": 802,
                    "end": 829,
                    "matchedPaperCorpusId": "253237200"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.4384765625
        },
        {
            "corpus_id": "276774084",
            "title": "Revisiting Large Language Model Pruning using Neuron Semantic Attribution",
            "text": "Pruning Strategy Model pruning has emerged as an effective and efficient method for compressing large neural networks, particularly in the context of LLMs. \n\nRecent advancements in pruning methods for LLMs have introduced several promising methods. For instance, SparseGPT (Frantar & Alistarh, 2023) introduces a pruning method that incorporates OBS updates (optimal block sparsity), which allows for efficient pruning while retaining high accuracy in large models. This method revolutionized pruning by addressing both sparsity and computational efficiency in one framework. Following this, Wanda (Sun et al., 2023) introduces a simpler, yet highly effective pruning technique that further reduces the computational burden by refining the selection of parameters to prune, while still maintaining the model's original functionality. RIA (Zhang et al., 2024) improves the pruning metric with relative importance and used channel permutation to adjust the pruned model. However, it is not clear the generaliaility of those methods for downstream tasks. \n\nCalibration Data Calibration data refers to a small set of unlabeled samples used to adjust model parameters during model compression including model pruning and posttraining quantization. In pruning process, calibration data helps guide the selection of parameters to prune, ensuring that critical model components are preserved. Studies (Frantar & Alistarh, 2023;Sun et al., 2023;Zhang et al., 2024) combine calibration data with model weight to calculate the pruning metrics. In quantization (Frantar & Alistarh, 2022), calibration data is utilized to set activation ranges, ensuring that the quantized model maintains performance close to the original. One work (Hubara et al., 2021) demonstrated that even with a minimal calibration set, it is possible to achieve accurate post-training quantization by optimizing layer parameters over this data. Recent studies show that for both pruning and quantization, proper calibration data selection helps maintain task performance, while poor selection may cause underperformance. \n\nGeneralizability It is interesting to explore the generalizability of pruned models.",
            "score": 0.6553130400509276,
            "section_title": "Related work",
            "char_start_offset": 6481,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 155
                },
                {
                    "start": 158,
                    "end": 248
                },
                {
                    "start": 249,
                    "end": 465
                },
                {
                    "start": 466,
                    "end": 575
                },
                {
                    "start": 576,
                    "end": 833
                },
                {
                    "start": 834,
                    "end": 968
                },
                {
                    "start": 969,
                    "end": 1051
                },
                {
                    "start": 1054,
                    "end": 1242
                },
                {
                    "start": 1243,
                    "end": 1384
                },
                {
                    "start": 1385,
                    "end": 1532
                },
                {
                    "start": 1533,
                    "end": 1710
                },
                {
                    "start": 1711,
                    "end": 1905
                },
                {
                    "start": 1906,
                    "end": 2081
                },
                {
                    "start": 2084,
                    "end": 2168
                }
            ],
            "ref_mentions": [
                {
                    "start": 273,
                    "end": 298,
                    "matchedPaperCorpusId": "255372747"
                },
                {
                    "start": 838,
                    "end": 858,
                    "matchedPaperCorpusId": "271745835"
                },
                {
                    "start": 1393,
                    "end": 1419,
                    "matchedPaperCorpusId": "255372747"
                },
                {
                    "start": 1436,
                    "end": 1455,
                    "matchedPaperCorpusId": "271745835"
                },
                {
                    "start": 1549,
                    "end": 1575,
                    "matchedPaperCorpusId": "251765570"
                },
                {
                    "start": 1720,
                    "end": 1741,
                    "matchedPaperCorpusId": "235825979"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.7978515625
        },
        {
            "corpus_id": "267751193",
            "title": "Why Lift so Heavy? Slimming Large Language Models by Cutting Off the Layers",
            "text": "Pruning, a prevalent technique for model compression method, seeks to discard non-essential parameters within language models, thereby reducing computational and storage costs while maintaining performance [13], [31], [35]. Pruning methods can be broadly categorized into unstructured pruning, which removes individual weights, and structured pruning, which removes higher-granularity structures such as neurons, attention heads, or entire layers. Layer-wise pruning, a specific form of structured pruning, has been explored in several studies. Fan et al. (2019) [5] introduced LayerDrop, a structured dropout method that randomly drops layers during training to improve robustness to layer reduction at inference time. Additionally, several studies have investigated the disparities in representations across different layers, highlighting the significance of updating the last few layers for language models [12], [14], [18]. Building on this foundation, Peer et al. (2022) [22] and Sajjad et al. (2023) [24] aimed to identify and remove an optimal subset of layers directly from the pretrained models for use in downstream tasks. These works focus on pruning layers to reduce model size while preserving taskspecific performance within the standard fine-tuning paradigm. However, they do not explore the interaction between layer pruning and prompt-based fine-tuning, which is the focus of our work. \n\nRecent studies have explored the application of pruning techniques to large language models, such as Sheared LLMs [34] and SparseGPT [6], which aims to sparsify or prune LLMs for efficient inference. These methods often involve complex optimization strategies or require additional training steps [1], [4], [11]. In contrast, our approach is a simple top-layer pruning strategy that directly removes layers from LLMs and evaluates their performance in prompt-based fine-tuning scenarios. Unlike prior work, we quantitatively investigate the impact of layer pruning on LLMs in few-shot learning settings, demonstrating that even drastic reductions in layers can maintain or improve performance.",
            "score": 0.6551739765425079,
            "section_title": "A. Model Pruning",
            "char_start_offset": 3976,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 223
                },
                {
                    "start": 224,
                    "end": 447
                },
                {
                    "start": 448,
                    "end": 544
                },
                {
                    "start": 545,
                    "end": 719
                },
                {
                    "start": 720,
                    "end": 927
                },
                {
                    "start": 928,
                    "end": 1132
                },
                {
                    "start": 1133,
                    "end": 1273
                },
                {
                    "start": 1274,
                    "end": 1402
                },
                {
                    "start": 1405,
                    "end": 1604
                },
                {
                    "start": 1605,
                    "end": 1717
                },
                {
                    "start": 1718,
                    "end": 1892
                },
                {
                    "start": 1893,
                    "end": 2098
                }
            ],
            "ref_mentions": [
                {
                    "start": 206,
                    "end": 210,
                    "matchedPaperCorpusId": "7785881"
                },
                {
                    "start": 212,
                    "end": 216,
                    "matchedPaperCorpusId": "258865530"
                },
                {
                    "start": 218,
                    "end": 222,
                    "matchedPaperCorpusId": "247922354"
                },
                {
                    "start": 545,
                    "end": 562,
                    "matchedPaperCorpusId": "202750230"
                },
                {
                    "start": 563,
                    "end": 566,
                    "matchedPaperCorpusId": "202750230"
                },
                {
                    "start": 910,
                    "end": 914,
                    "matchedPaperCorpusId": "201645145"
                },
                {
                    "start": 922,
                    "end": 926,
                    "matchedPaperCorpusId": "84841767"
                },
                {
                    "start": 976,
                    "end": 980,
                    "matchedPaperCorpusId": "247771234"
                },
                {
                    "start": 985,
                    "end": 1005,
                    "matchedPaperCorpusId": "251005814"
                },
                {
                    "start": 1006,
                    "end": 1010,
                    "matchedPaperCorpusId": "251005814"
                },
                {
                    "start": 1519,
                    "end": 1523,
                    "matchedPaperCorpusId": "263830786"
                },
                {
                    "start": 1538,
                    "end": 1541,
                    "matchedPaperCorpusId": "255372747"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.76220703125
        },
        {
            "corpus_id": "274166138",
            "title": "AutoMixQ: Self-Adjusting Quantization for High Performance Memory-Efficient Fine-Tuning",
            "text": "LLM-Pruner (Ma, Fang, and Wang 2023) uses structured pruning to eliminate non-essential interconnected structures by leveraging gradient information. This technique enables compressed models to maintain good performance across multiple tasks with basic fine-tuning. Santacroce et al. (2023) proposes Globally Unique Movement (GUM), a novel pruning technique focusing on the sensitivity and uniqueness of LLMs' network components. GUM prunes neurons that uniquely contribute to the model output and are sensitive to loss changes, thus preserving high accuracy. This method optimizes the trade-off between information retention and computational efficiency. SparseGPT (Frantar and Alistarh 2023) is a pruning method that transforms the process into a series of large-scale sparse regression problems, solvable through Hessian matrix inversion without retraining. It efficiently prunes large models to high sparsity in a single step while maintaining high accuracy. Wanda (Sun et al. 2023) prunes LLMs by selectively removing weights based on their sizes and input activations, adaptively adjusting sparsity levels to reduce more than half without sacrificing accuracy. Quantization-Aware Training (QAT) combines quantization with full model fine-tuning to adapt models for downstream tasks (Peri, Patel, and Park 2020;Liu et al. 2023). Although QAT is effective, it requires substantial computational resources, such as gradient calculations and optimization states, and it complicates the gradient computation for quantized weights. However, by leveraging LoRA, these challenges can be bypassed during task adaptation. Post-Training Quantization (PTQ) frameworks, such as GPTQ and SmoothQuant (Frantar et al. 2022;Xiao et al. 2023), use a small subset of training data to calibrate highprecision models, enabling the generation of task-specific quantized models without the need for gradient backpropagation. This makes PTQ more cost-efficient than QAT, although it generally results in lower accuracy.",
            "score": 0.65512559825238,
            "section_title": "Efficient Compression of LLMs",
            "char_start_offset": 22907,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 149
                },
                {
                    "start": 150,
                    "end": 265
                },
                {
                    "start": 266,
                    "end": 429
                },
                {
                    "start": 430,
                    "end": 559
                },
                {
                    "start": 560,
                    "end": 655
                },
                {
                    "start": 656,
                    "end": 860
                },
                {
                    "start": 861,
                    "end": 962
                },
                {
                    "start": 963,
                    "end": 1166
                },
                {
                    "start": 1167,
                    "end": 1333
                },
                {
                    "start": 1334,
                    "end": 1531
                },
                {
                    "start": 1532,
                    "end": 1617
                },
                {
                    "start": 1618,
                    "end": 1907
                },
                {
                    "start": 1908,
                    "end": 2001
                }
            ],
            "ref_mentions": [
                {
                    "start": 11,
                    "end": 36,
                    "matchedPaperCorpusId": "258823276"
                },
                {
                    "start": 969,
                    "end": 986,
                    "matchedPaperCorpusId": "259203115"
                },
                {
                    "start": 1713,
                    "end": 1730,
                    "matchedPaperCorpusId": "253708271"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.79833984375
        },
        {
            "corpus_id": "268032253",
            "title": "LLM Inference Unveiled: Survey and Roofline Model Insights",
            "text": "Unstructured pruning selectively eliminates individual weights or neurons from a model, leading to a sparser, yet more irregularly structured network. This form of pruning excels in ensuring model accuracy, however, the resultant irregularity in the weight distribution necessitates specialized handling or software optimizations. SparseGPT [Frantar and Alistarh, 2023] is a groundbreaking one-shot pruning method tailored for LLMs. It tackles the pruning challenge by reconceptualizing it into a series of extensive sparse regression problems, efficiently solved by a newly developed solver. Notably, SparseGPT can efficiently process a model with 175 billion parameters in just a few hours on a single GPU, and it can induce significant sparsity (50-60%) in LLMs without significantly sacrificing accuracy or necessitating fine-tuning. To tackle the challenge of reconstruction cost in SparseGPT, Sun et al. [2023a] propose Wanda, which assesses the significance of each weight by evaluating its magnitude and the norm of the corresponding input, significantly increasing the computational efficiency. Further, Yin et al. [2023a] design a set of non-uniform hierarchical sparsity ratios to pay more attention to the layers with higher outlier occurrences, thus boosting the pruning performance. Moreover, Considering the hardware support for unstructured pruning, Flash-LLM [Xia et al., 2023a] proposes an unstructured sparse matrix multiplication method, which is characterized by sparse loading and dense computation, to implement the GPU Tensor Core's sophisticated support for unstructured sparsity.",
            "score": 0.6541126940668074,
            "section_title": "Unstructured pruning",
            "char_start_offset": 36263,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 150
                },
                {
                    "start": 151,
                    "end": 330
                },
                {
                    "start": 331,
                    "end": 432
                },
                {
                    "start": 433,
                    "end": 592
                },
                {
                    "start": 593,
                    "end": 837
                },
                {
                    "start": 838,
                    "end": 1103
                },
                {
                    "start": 1104,
                    "end": 1296
                },
                {
                    "start": 1297,
                    "end": 1605
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.58544921875
        },
        {
            "corpus_id": "258823276",
            "title": "LLM-Pruner: On the Structural Pruning of Large Language Models",
            "text": "We conclude the advantages of the LLM-Pruner as (i) Task-agnostic compression, where the compressed language model retains its ability to serve as a multi-task solver. (ii) Reduced demand for the original training corpus, where only 50k publicly available samples are needed for compression, significantly reducing the budget for acquiring the training data (iii) Quick compression, where the compression process ends up in three hours. \n\n(iv) An automatic structural pruning framework, where all the dependent structures are grouped without the need for any manual design. To evaluate the effectiveness of LLM-Pruner, we conduct extensive experiments on three large language models: LLaMA-7B, Vicuna-7B, and ChatGLM-6B. The compressed models are evaluated using nine datasets to assess both the generation quality and the zero-shot classification performance of the pruned models. The experimental results demonstrate that even with the removal of 20% of the parameters, the pruned model maintains 94.97% of the performance of the original model.",
            "score": 0.653389471688538,
            "section_title": "Introduction",
            "char_start_offset": 4184,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 167
                },
                {
                    "start": 168,
                    "end": 436
                },
                {
                    "start": 439,
                    "end": 573
                },
                {
                    "start": 574,
                    "end": 720
                },
                {
                    "start": 721,
                    "end": 881
                },
                {
                    "start": 882,
                    "end": 1047
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.88037109375
        },
        {
            "corpus_id": "276961144",
            "title": "T\u00fdr-the-Pruner: Unlocking Accurate 50% Structural Pruning for LLMs via Global Sparsity Distribution Optimization",
            "text": "Language comprehension and downstream task performance of post-pruned LLMs. We applied structural pruning to various large language models using T\u00fdr-the-Pruner at overall sparsity levels of 12.5%, 25%, 37.5%, and 50%. \n\nThe performance was benchmarked against state-of-the-art methods, including ShortGPT (layer pruning) (Men et al., 2024), LaCO+ (ShortGPT with LaCO layer merging) (Yang et al., 2024), SliceGPT (embedding dimension pruning) (Ashkboos et al., 2024), Wanda-SP (Sun et al., 2024;An et al., 2024), LLM-Pruner (Ma et al., 2023), ZipLM (Kurtic et al., 2023), OSSCAR (Meng et al., 2024), and FLAP (An et al., 2024). Table 1 summarizes the comparative results, highlighting post-pruning performance in language comprehension and downstream tasks (c.f., Appendix A.10 for detailed results within each task). \n\nT\u00fdr-the-Pruner demonstrates competitive performance across various sparsity ratios and LLMs. It consistently achieves state-of-the-art results at low sparsity ratios (\u226425%). For instance, pruning 12.5% of Llama-3-8B's parameters yields the lowest perplexity (7.39) and the highest average downstream accuracy (62.37%), surpassing the previous SOTA methods, LLM-Pruner and LaCO+, by 8.0% and 2.6%. At higher sparsities (\u226537.5%), maintaining performance poses a significant challenge for existing methods, with advanced techniques like OSSCAR often exhibiting perplexities exceeding 100 and accuracies dropping below 40%. T\u00fdr-the-Pruner, by contrast, excels under these conditions.",
            "score": 0.6525835020572583,
            "section_title": "Performance",
            "char_start_offset": 17164,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 75
                },
                {
                    "start": 76,
                    "end": 217
                },
                {
                    "start": 220,
                    "end": 626
                },
                {
                    "start": 627,
                    "end": 816
                },
                {
                    "start": 819,
                    "end": 911
                },
                {
                    "start": 912,
                    "end": 992
                },
                {
                    "start": 993,
                    "end": 1215
                },
                {
                    "start": 1216,
                    "end": 1246
                },
                {
                    "start": 1247,
                    "end": 1438
                },
                {
                    "start": 1439,
                    "end": 1498
                }
            ],
            "ref_mentions": [
                {
                    "start": 382,
                    "end": 401,
                    "matchedPaperCorpusId": "267751181"
                },
                {
                    "start": 442,
                    "end": 465,
                    "matchedPaperCorpusId": "267301573"
                },
                {
                    "start": 476,
                    "end": 494,
                    "matchedPaperCorpusId": "259203115"
                },
                {
                    "start": 523,
                    "end": 540,
                    "matchedPaperCorpusId": "258823276"
                },
                {
                    "start": 578,
                    "end": 597,
                    "matchedPaperCorpusId": "268536948"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.74951171875
        },
        {
            "corpus_id": "267759551",
            "title": "EBFT: Effective and Block-Wise Fine-Tuning for Sparse LLMs",
            "text": "Unstructured Pruning.We perform comprehensive comparative experiments on the Wikitext2 dataset, and the results are presented in Table .1We compare the perplexity of pruned LlamaV1 and LlamaV2 models using our method, DsnoT, magnitude pruning, Wanda, and SparseGPT across a range of sparsity levels, from 50% to 90%.The experimental results show the strong effectiveness of our EBFT.We can observe that regardless of the magnitude pruning method used, be it SparseGPT or Wanda, our method enhances the performance of the sparse model.For instance, with magnitude pruning, our method achieves a perplexity of 7.11, surpassing the perplexity of 17.29 before fine- tuning, and even outperforming Wanda (7.26) and SparseGPT (7.20).\n\nWe also find that as the sparsity increases, two observations emerge: (1) The state-of-theart DsnoT loses its effectiveness as a fine-tuning method.For example, when using SparseGPT, DsnoT degrades the performance of the sparse model at sparsity levels of 70%, 80%, and 90%.This demonstrates the limitations of heuristic optimization strategies, which lack theoretical support.\n\n(2) The advantage of our method becomes more pronounced, indicating that our method enhances the ability of pruned models even at extremely high sparsity levels.\n\nIn Table 1, we further observe that SparseGPT, which updates the values of the remaining weights, outperforms Wanda, which leaves the remaining weights unchanged.As sparsity increases, the advantage of SparseGPT over Wanda becomes more evident, particularly at high sparsity levels.Additionally, the DsnoT approach, which reselects the masks after pruning and keep weights unchanged, also faces challenges.For example, when the sparsity exceeds 70%, regardless of LlamaV1 or Lla-maV2, DsnoT significantly decreases the performance of the sparse model pruned by SparseGPT.In contrast, our method effectively and efficiently fine-tunes the weights of the LLM block by block, surpassing other baselines overall.",
            "score": 0.6519081386440793,
            "section_title": "Language Modeling",
            "char_start_offset": 13224,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 21
                },
                {
                    "start": 21,
                    "end": 137
                },
                {
                    "start": 137,
                    "end": 316
                },
                {
                    "start": 316,
                    "end": 383
                },
                {
                    "start": 383,
                    "end": 534
                },
                {
                    "start": 534,
                    "end": 727
                },
                {
                    "start": 729,
                    "end": 877
                },
                {
                    "start": 877,
                    "end": 1003
                },
                {
                    "start": 1003,
                    "end": 1106
                },
                {
                    "start": 1108,
                    "end": 1269
                },
                {
                    "start": 1271,
                    "end": 1433
                },
                {
                    "start": 1433,
                    "end": 1553
                },
                {
                    "start": 1553,
                    "end": 1677
                },
                {
                    "start": 1677,
                    "end": 1842
                },
                {
                    "start": 1842,
                    "end": 1979
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.3720703125
        },
        {
            "corpus_id": "260900101",
            "title": "A Survey on Model Compression for Large Language Models",
            "text": "Unstructured pruning preserves the pruned model's performance, hence, works related to unstructured pruning of LLMs often dispense with retraining to restore performance. Nevertheless, unstructured pruning renders the pruned model irregular, necessitating specialized handling or software optimizations for inference acceleration. An innovative approach in this domain is SparseGPT (Frantar and Alistarh, 2023), which introduces a one-shot pruning strategy without retraining. SparseGPT frames pruning as an extensive sparse regression problem and solves it using an approximate sparse regression solver. SparseGPT achieves significant unstructured sparsity, even up to over 50% on the largest GPT models like OPT-175B and BLOOM-176B, with minimal increase in perplexity. To reduce the cost about the weight update process required by SparseGPT, Wanda (Sun et al., 2024) achieves model sparsity by pruning weights with the smallest magnitudes multiplied by the norm of the corresponding input activations, without the need for retraining or weight updates. To further minimize pruning-induced errors while upholding the desired overall sparsity level, SAMSP (Shao et al., 2024a) utilizes the Hessian matrix as a metric for weight matrix sensitivity evaluation, and dynamically adjusts sparsity allocation based on sensitivity. Furthermore, DSnoT (Zhang et al., 2024) minimizes the reconstruction error between dense and sparse models through iterative weight pruning-and-growing on top of sparse LLMs to enhance LLM performance across various sparsity rates, especially at high sparsity levels. To provide hardware support for handling unstructured pruning on the GPU Tensor Core hardware, Flash-LLM (Xia et al., 2023) introduces an unstructured sparse matrix multiplication method, which loads weight matrices in a sparse format from global memory and reconstructs them in a dense format within high-speed on-chip buffers for computation using tensor cores.",
            "score": 0.6518651719547628,
            "section_title": "Unstructured Pruning",
            "char_start_offset": 17929,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 170
                },
                {
                    "start": 171,
                    "end": 330
                },
                {
                    "start": 331,
                    "end": 476
                },
                {
                    "start": 477,
                    "end": 604
                },
                {
                    "start": 605,
                    "end": 771
                },
                {
                    "start": 772,
                    "end": 1056
                },
                {
                    "start": 1057,
                    "end": 1326
                },
                {
                    "start": 1327,
                    "end": 1594
                },
                {
                    "start": 1595,
                    "end": 1958
                }
            ],
            "ref_mentions": [
                {
                    "start": 852,
                    "end": 870,
                    "matchedPaperCorpusId": "259203115"
                },
                {
                    "start": 1158,
                    "end": 1178,
                    "matchedPaperCorpusId": "264146174"
                },
                {
                    "start": 1346,
                    "end": 1366,
                    "matchedPaperCorpusId": "264128029"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.8232421875
        },
        {
            "corpus_id": "255372747",
            "title": "SparseGPT: Massive Language Models Can Be Accurately Pruned in One-Shot",
            "text": "Pruning vs. Model Size. We first study how the difficulty of pruning LLMs changes with their size. We consider the entire OPT model family and uniformly prune all linear layers, excluding the embeddings and the head, as standard (Sanh et al., 2020;Kurtic et al., 2022), to 50% unstructured sparsity, full 4:8 or full 2:4 semi-structured sparsity (the 2:4 pattern is the most stringent). The raw-WikiText2 performance numbers are given in Table 1 and visualized in Figure 2. The corresponding results for PTB and C4 can be found in Appendix C and show very similar trends overall. One immediate finding is that the accuracy of magnitudepruned models collapses across all scales, with larger variants generally dropping faster than smaller ones. This is in stark contrast to smaller vision models which can usually be pruned via simple magnitude selection to 50% sparsity or more at very little loss of accuracy (Singh & Alistarh, 2020;Frantar et al., 2022b). It highlights the importance of accurate pruners for massive generative language models, but also the fact that perplexity is a very sensitive metric. \n\nFor SparseGPT, the trend is very different: already at 2.7B parameters, the perplexity loss is \u2248 1 point, at 66B, there is essentially zero loss and at the very largest scale there is even a slight accuracy improvement over the dense baseline, which however seems to be dataset specific (see also Appendix C). AdaPrune, as expected, also yields a big improvement over magnitude pruning, but is significantly less accurate than SparseGPT. Despite the efficiency of AdaPrune, running it takes approximately \u2248 1.3h on a 350M model and \u2248 4.3h on a 1.3B one, while SparseGPT can fully sparsify 66B and 175B models in roughly the same time, executing on the same A100 GPU. \n\nIn general, there is a clear trend of larger models being easier to sparsify, which we speculate is due to overparametrization. A detailed investigation of this phenomenon would be a good direction for future work.",
            "score": 0.65113845911549,
            "section_title": "Results",
            "char_start_offset": 23834,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 23
                },
                {
                    "start": 24,
                    "end": 98
                },
                {
                    "start": 99,
                    "end": 386
                },
                {
                    "start": 387,
                    "end": 579
                },
                {
                    "start": 580,
                    "end": 743
                },
                {
                    "start": 744,
                    "end": 957
                },
                {
                    "start": 958,
                    "end": 1108
                },
                {
                    "start": 1111,
                    "end": 1420
                },
                {
                    "start": 1421,
                    "end": 1548
                },
                {
                    "start": 1549,
                    "end": 1777
                },
                {
                    "start": 1780,
                    "end": 1907
                },
                {
                    "start": 1908,
                    "end": 1994
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.420654296875
        },
        {
            "corpus_id": "271051500",
            "title": "Pruning Large Language Models to Intra-module Low-rank Architecture with Transitional Activations",
            "text": "Deploying large language models (LLMs) locally on edge devices instead of relying on remote APIs has been a pressing initiative.Local deployment of LLMs ensures independence from network conditions and enhances privacy at an advanced level (Ma et al., 2023a).Nevertheless, deploying a scaled-up LLM onto a resource-constrained end device poses multifaceted challenges, encompassing inference speed, memory footprint, and power Sheared-Llama (Xia, et al., 2024) activation projection layernorm consumption.Therefore, comprehensive optimizations on the efficiency of LLMs are imperative, including architecture design (Gu and Dao, 2023), model compression (Zhu et al., 2023), inference schemes (Leviathan et al., 2023;Cai et al., 2024), compilation and runtime (Lai et al., 2023).Model compression emerges as the silver-bullet solution for reducing deployment costs given an accessible LLM.To essentially reduce model computation and memory overhead, pruning aims to discard weights with low salience to the LLM.Jaiswal et al. (2023) suggest that state-of-the-art (SOTA) unstructured pruning approaches i.e., SparseGPT (Frantar and Alistarh, 2023) and Wanda (Sun et al., 2023), along with their semi-structured variations, often underperform in downstream benchmarks.Zimmer et al. (2023) emphasize the significance of post-training after pruning to restore the capabilities of the LLM.However, the post-training and inference of a sparse model are notably inefficient.Also, an unstructured pruning with arbitrary sparsity pattern has no speedup or memory saving on the LLM, whereas a semi-structured sparse model heavily relies on specific hardware (Frantar and Alistarh, 2023).\n\nAn alternative pruning category, i.e., structured pruning, has shown feasibility for LLMs.LLM-Pruner (Ma et al., 2023b), the pioneering structured pruning of LLM, incorporates the approximated Taylor series as the pruning metric.",
            "score": 0.6509414173761385,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 128
                },
                {
                    "start": 128,
                    "end": 259
                },
                {
                    "start": 259,
                    "end": 505
                },
                {
                    "start": 505,
                    "end": 778
                },
                {
                    "start": 778,
                    "end": 888
                },
                {
                    "start": 888,
                    "end": 1010
                },
                {
                    "start": 1010,
                    "end": 1265
                },
                {
                    "start": 1265,
                    "end": 1383
                },
                {
                    "start": 1383,
                    "end": 1466
                },
                {
                    "start": 1466,
                    "end": 1676
                },
                {
                    "start": 1678,
                    "end": 1768
                },
                {
                    "start": 1768,
                    "end": 1907
                }
            ],
            "ref_mentions": [
                {
                    "start": 240,
                    "end": 258,
                    "matchedPaperCorpusId": "261431473"
                },
                {
                    "start": 441,
                    "end": 460,
                    "matchedPaperCorpusId": "263830786"
                },
                {
                    "start": 692,
                    "end": 716,
                    "matchedPaperCorpusId": "254096365"
                },
                {
                    "start": 716,
                    "end": 733,
                    "matchedPaperCorpusId": "267061277"
                },
                {
                    "start": 1010,
                    "end": 1031,
                    "matchedPaperCorpusId": "263605754"
                },
                {
                    "start": 1117,
                    "end": 1145,
                    "matchedPaperCorpusId": "255372747"
                },
                {
                    "start": 1647,
                    "end": 1675,
                    "matchedPaperCorpusId": "255372747"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.355224609375
        },
        {
            "corpus_id": "273901152",
            "title": "Structured Optimal Brain Pruning for Large Language Models",
            "text": "The massive parameters and computational demands hinder the widespread application of Large Language Models (LLMs). Network pruning provides a practical solution to this problem. However, existing pruning works for LLMs mainly focus on unstructured pruning or necessitate post-pruning fine-tuning. The former relies on special hardware to accelerate computation, while the latter may need substantial computational resources. In this paper, we introduce a retraining-free structured pruning method called SoBP (Structured Optimal Brain Pruning). It leverages global first-order information to select pruning structures, then refines them with a local greedy approach, and finally adopts module-wise reconstruction to mitigate information loss. We assess the effectiveness of SoBP across 14 models from 3 LLM families on 8 distinct datasets. Experimental results demonstrate that SoBP outperforms current state-of-the-art methods.",
            "score": 0.6499004488730877,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.89794921875
        },
        {
            "corpus_id": "273501861",
            "title": "FedSpaLLM: Federated Pruning of Large Language Models",
            "text": "Pruning of LLMs. Pruning regained prominence in the late 2010s for reducing inference costs (Han et al., 2015). LLM pruning can be categorized into structured and unstructured pruning. Unstructured pruning removes individual parameters without regard to model structure, often using thresholds to nullify smaller weights. SparseGPT (Frantar and Alistarh, 2023) achieves up to 60% parameter reduction in LLMs with minimal performance loss. Wanda (Sun et al., 2023) introduces a pruning criterion based on both weight magnitude and activations, particularly effective in linear layers. DynaTran (Tuli and Jha, 2023) dynamically prunes activations at runtime, enhanced by a custom ASIC architecture. \n\nStructured pruning removes groups of parameters such as filters or attention heads. LLM-Pruner (Ma et al., 2023) combines first-order data and Hessian information for structured pruning, while LoSparse (Li et al., 2023) uses low-rank and sparse approximations to balance pruning and model expressiveness. Structured pruning of hidden dimensions, as shown by (Tao et al., 2023), extends to embeddings and attention heads. Zi-pLM (Kurtic et al., 2023) optimizes structured compression for accuracy and hardware efficiency. Federated Learning with LLMs. FL on LLMs has primarily focused on LLM fine-tuning and has gained attention for enabling private and efficient model updates. FedPrompt (Zhao et al., 2023) introduces prompt-tuning in FL, reducing communication costs by updating only soft prompts. Fed-Pipe (Fang et al., 2024) leverages low-rank adapters for parameter-efficient fine-tuning, improving training speed and accuracy. FedNLP (Lin et al., 2022) provides a benchmarking framework for evaluating FL methods on NLP tasks. FedAdapter (Cai et al., 2023) uses adapters to accelerate model convergence in federated settings. FeDeRA (Yan et al., 2024) employs singular value decomposition to further improve LoRA-based fine-tuning efficiency.",
            "score": 0.6489243282418944,
            "section_title": "Related Work",
            "char_start_offset": 3244,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 16
                },
                {
                    "start": 17,
                    "end": 111
                },
                {
                    "start": 112,
                    "end": 184
                },
                {
                    "start": 185,
                    "end": 321
                },
                {
                    "start": 322,
                    "end": 438
                },
                {
                    "start": 439,
                    "end": 583
                },
                {
                    "start": 584,
                    "end": 696
                },
                {
                    "start": 699,
                    "end": 782
                },
                {
                    "start": 783,
                    "end": 1003
                },
                {
                    "start": 1004,
                    "end": 1119
                },
                {
                    "start": 1120,
                    "end": 1219
                },
                {
                    "start": 1220,
                    "end": 1249
                },
                {
                    "start": 1250,
                    "end": 1376
                },
                {
                    "start": 1377,
                    "end": 1498
                },
                {
                    "start": 1499,
                    "end": 1631
                },
                {
                    "start": 1632,
                    "end": 1731
                },
                {
                    "start": 1732,
                    "end": 1830
                },
                {
                    "start": 1831,
                    "end": 1947
                }
            ],
            "ref_mentions": [
                {
                    "start": 593,
                    "end": 613,
                    "matchedPaperCorpusId": "257232385"
                },
                {
                    "start": 1057,
                    "end": 1075,
                    "matchedPaperCorpusId": "259858812"
                },
                {
                    "start": 1387,
                    "end": 1406,
                    "matchedPaperCorpusId": "252762685"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.74755859375
        },
        {
            "corpus_id": "271957102",
            "title": "Investigating Language-Specific Calibration For Pruning Multilingual Large Language Models",
            "text": "State-of-the-art language models often rely on over-parameterization with millions or billions of parameters, resulting in significant memory and computational demands (Zhang et al., 2017;Allen-Zhu et al., 2019;Xu & Du, 2023). Pruning is a model compression technique that removes unimportant weights to reduce memory footprint and inference computation (Gholami et al., 2022;Hoefler et al., 2021;Kuzmin et al., 2023). Recent pruning methods for language models, such as SparseGPT (Frantar & Alistarh, 2023) and Wanda (Sun et al., 2024), demonstrated SotA performance in a post-training and retraining-free setting (Zhu et al., 2023). The pruning process involves passing a small number of examples, i.e. calibration data, to the model to determine the importance of weights for subsequent pruning (Kuzmin et al., 2022;Frantar & Alistarh, 2023;Kuzmin et al., 2023). \n\nNotably, existing studies typically use calibration data in English and evaluate the post-pruning performance in English. On the other hand, most SotA LLMs are multilingual and frequently employed for tasks in non-English languages (Touvron et al., 2023;Achiam et al., 2023;Jiang et al., 2023). It remains unclear how to calibrate pruning to optimize the post-pruning performance of multilingual LLMs on tasks in non-English languages. In other words, the current literature provides little insight into efficient calibration strategies for multilingual LLMs targeting specific non-English languages. For example, if we aim to prune a multilingual LLM and use the pruned LLM for tasks in German, should we calibrate the compression process with text in English, German, or both? This research, as the first, presents an extensive study investigating the impact of calibration data language on pruning LLMs for language-specific tasks.",
            "score": 0.6488464889710881,
            "section_title": "INTRODUCTION",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 226
                },
                {
                    "start": 227,
                    "end": 418
                },
                {
                    "start": 419,
                    "end": 634
                },
                {
                    "start": 635,
                    "end": 865
                },
                {
                    "start": 868,
                    "end": 989
                },
                {
                    "start": 990,
                    "end": 1162
                },
                {
                    "start": 1163,
                    "end": 1303
                },
                {
                    "start": 1304,
                    "end": 1468
                },
                {
                    "start": 1469,
                    "end": 1646
                },
                {
                    "start": 1647,
                    "end": 1802
                }
            ],
            "ref_mentions": [
                {
                    "start": 168,
                    "end": 188,
                    "matchedPaperCorpusId": "6212000"
                },
                {
                    "start": 188,
                    "end": 211,
                    "matchedPaperCorpusId": "53250107"
                },
                {
                    "start": 211,
                    "end": 225,
                    "matchedPaperCorpusId": "257039036"
                },
                {
                    "start": 354,
                    "end": 376,
                    "matchedPaperCorpusId": "232352683"
                },
                {
                    "start": 376,
                    "end": 397,
                    "matchedPaperCorpusId": "231740691"
                },
                {
                    "start": 397,
                    "end": 417,
                    "matchedPaperCorpusId": "259360935"
                },
                {
                    "start": 518,
                    "end": 536,
                    "matchedPaperCorpusId": "259203115"
                },
                {
                    "start": 798,
                    "end": 819,
                    "matchedPaperCorpusId": "251710272"
                },
                {
                    "start": 844,
                    "end": 864,
                    "matchedPaperCorpusId": "259360935"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.56640625
        },
        {
            "corpus_id": "277435040",
            "title": "STADE: Standard Deviation as a Pruning Metric",
            "text": "Large Language Models (LLMs) [7,34,35] have revolutionized not only the field of Natural Language Processing (NLP) but also numerous real-world applications that affect everyday life. Their ability to generate coherent text, perform complex reasoning, and support a variety of conversational and decision-making tasks has led to widespread adoption in both research and industry. With the advent of increasingly autonomous systems [16,24,42], these models now assist with tasks ranging from content creation and translation to automated customer support and strategic decision making. \n\nDespite these impressive capabilities, LLMs are notorious for their substantial computational requirements [25]. The high memory footprint, extensive processing power, and significant energy consumption often preclude their deployment on devices with limited resources, such as mobile phones or embedded edge devices. In addition, the large-scale training of these models contributes to increased operational costs and a non-negligible environmental impact. Consequently, the drive to reduce the computational and storage demands of LLMs has become a central focus in the field. \n\nTo mitigate these computational challenges, a variety of approaches have been explored. One prominent strategy involves reducing the storage requirements of model weights through quantization [29,41]. Quantization techniques lower the numerical precision of weights and activations, resulting in reduced memory usage and accelerated inference speeds, often with minimal degradation in performance. Another effective approach is to remove unimportant weight parameters through pruning [27]. Pruning methods seek to eliminate redundancies in the network by removing weights that contribute little to overall model performance, thereby decreasing both the computational load and the inference latency. \n\nPruning techniques can be applied during training [37] or after the model has been fully trained, in what is known as posttraining pruning [3]. The latter approach is particularly appealing when the goal is to adapt a pre-trained model for deployment on resource-constrained devices, as the main challenge is not the training process but rather fitting the model into a limited hardware environment. Although some post-training pruning strategies involve costly retraining steps [2,43], previous studies [19,38] have demonstrated that a model can maintain a large fraction of its original performance even when up to 50% of its weights are pruned without any retraining.",
            "score": 0.6480585579425295,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 183
                },
                {
                    "start": 184,
                    "end": 379
                },
                {
                    "start": 380,
                    "end": 584
                },
                {
                    "start": 587,
                    "end": 699
                },
                {
                    "start": 700,
                    "end": 904
                },
                {
                    "start": 905,
                    "end": 1044
                },
                {
                    "start": 1045,
                    "end": 1165
                },
                {
                    "start": 1168,
                    "end": 1255
                },
                {
                    "start": 1256,
                    "end": 1368
                },
                {
                    "start": 1369,
                    "end": 1565
                },
                {
                    "start": 1566,
                    "end": 1657
                },
                {
                    "start": 1658,
                    "end": 1866
                },
                {
                    "start": 1869,
                    "end": 2012
                },
                {
                    "start": 2013,
                    "end": 2268
                },
                {
                    "start": 2269,
                    "end": 2539
                }
            ],
            "ref_mentions": [
                {
                    "start": 29,
                    "end": 32,
                    "matchedPaperCorpusId": "218971783"
                },
                {
                    "start": 35,
                    "end": 38,
                    "matchedPaperCorpusId": "160025533"
                },
                {
                    "start": 435,
                    "end": 438,
                    "matchedPaperCorpusId": "267547997"
                },
                {
                    "start": 438,
                    "end": 441,
                    "matchedPaperCorpusId": "267626905"
                },
                {
                    "start": 1652,
                    "end": 1656,
                    "matchedPaperCorpusId": "7785881"
                },
                {
                    "start": 1919,
                    "end": 1923,
                    "matchedPaperCorpusId": "218665313"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.64892578125
        },
        {
            "corpus_id": "271544038",
            "title": "Pruning Large Language Models with Semi-Structural Adaptive Sparse Training",
            "text": "Transformer-based Large Language Models (LLMs) are equipped to handle complex tasks (Devlin et al. 2018;Brown et al. 2020;Achiam et al. 2023) and exhibit emergent abilities (Wei et al. 2022) due to their expanding parameter count. However, this continual growth in model size poses significant challenges for practical deployment. In particular, inference speed suffers due to the increasing computational and memory demands. This has spurred a series of efforts to develop effective model compression techniques aimed at reducing memory footprints and easing the constraints associated with deploying these large-scale models. \n\nModel pruning (Frantar and Alistarh 2023; Han, Mao, and Dally 2016; Sun et al. 2023) is an effective technique for compressing LLMs by setting a proportion of weights to zero, thereby adhering to a specific sparsity pattern. Recently, N:M sparsity has emerged as a type of semistructured sparsity pattern that offers an optimal balance between precision and hardware efficiency. Specifically, N:M sparsity retains only N nonzero elements out of every group of M elements. This sparsity pattern can accelerate both matrix multiplication and memory access, potentially enhancing the performance of both pre-filling and decoding processes on off-the-shelf GPUs. Despite the promise of N:M sparsity, current state-of-the-art methods for pruning LLMs, such as SparseGPT (Frantar and Alistarh 2023) and Wanda (Sun et al. 2023), employ a post-training approach that determines the sparsity pattern in a layer-by-layer fashion without back-propagation. Although these methods improve efficiency, they can lead to significant performance degradation, particularly in knowledge-intensive tasks (Nowak et al. 2024), raising concerns about the feasibility of pruning LLMs. Additionally, while retraining pruned models has been successful in the pre-LLM era (Wang, Wohlwend, and Lei 2019;Lee, Ajanthan, and Torr 2019;Evci et al. 2020), its application to models with billions of parameters remains under-explored.",
            "score": 0.6475098785821122,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 230
                },
                {
                    "start": 231,
                    "end": 330
                },
                {
                    "start": 331,
                    "end": 425
                },
                {
                    "start": 426,
                    "end": 627
                },
                {
                    "start": 630,
                    "end": 854
                },
                {
                    "start": 855,
                    "end": 1008
                },
                {
                    "start": 1009,
                    "end": 1101
                },
                {
                    "start": 1102,
                    "end": 1288
                },
                {
                    "start": 1289,
                    "end": 1574
                },
                {
                    "start": 1575,
                    "end": 1790
                },
                {
                    "start": 1791,
                    "end": 2030
                }
            ],
            "ref_mentions": [
                {
                    "start": 173,
                    "end": 189,
                    "matchedPaperCorpusId": "249674500"
                },
                {
                    "start": 698,
                    "end": 713,
                    "matchedPaperCorpusId": "259203115"
                },
                {
                    "start": 1433,
                    "end": 1450,
                    "matchedPaperCorpusId": "259203115"
                },
                {
                    "start": 1714,
                    "end": 1733,
                    "matchedPaperCorpusId": "263605754"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.7216796875
        },
        {
            "corpus_id": "263829692",
            "title": "Outlier Weighed Layerwise Sparsity (OWL): A Missing Secret Sauce for Pruning LLMs to High Sparsity",
            "text": "To provide more insights into these two seemingly contradictory arguments, we study the effect of various pruning granularities on LLMs. Specifically, we study two sets of pruning granularities: (1) Across different layers, we compare the performance of uniform pruning and global pruning; \n\n(2) Within the same layer, we study the output-imbalanced sparsity used by SparseGPT against the output-balanced sparsity adopted by Wanda. Output-balanced sparsity eliminates the same amount of weights for all outputs. We conduct experiments with magnitude pruning and Wanda using LLaMA-7B. While this part of the study is irrelevant to LOD, we place it here due to the crucial role of pruning granularity. \n\nResults: We present our findings from Study 1-3, in Figure 1, Table 1, and Table 2, respectively. These results provide positive support for our conjecture, and we summarize the key observations below: \n\n1 LOD of dense LLMs exhibits a highly non-uniform distribution across layers. In essence, the distribution of dense LLMs shown in Figure 1 loosely follows a \"U\" shape, with notable proportions at both ends, while the central region displays a monotonic descending trend. This finding validates our conjecture that individual layers need unique consideration during the pruning procedure. Employing uniform pruning across all layers would inevitably disrupt the outlier structure in layers characterized by a large outlier ratio, such as those layers at the beginning or end of models. \n\n2 The performance of sparse pruning methods on LLMs is closely correlated with their ability to retain outlier features. Leading pruning techniques like Wanda and SparseGPT all excel in outlier, resulting in an overall increase in LOD. In contrast, the naive baseline of magnitude pruning performs no better than random selection at 70% sparsity, as evidenced by a negative change of -0.110 in LOD, indicating the removal of important outliers. It is interesting to see that despite SparseGPT not being explicitly designed for outlier preservation, it achieves the highest LOD as well as performance, providing further insight into the underlying reason for its success.",
            "score": 0.6474908694496442,
            "section_title": "Empirical Study",
            "char_start_offset": 16570,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 136
                },
                {
                    "start": 137,
                    "end": 289
                },
                {
                    "start": 292,
                    "end": 431
                },
                {
                    "start": 432,
                    "end": 511
                },
                {
                    "start": 512,
                    "end": 583
                },
                {
                    "start": 584,
                    "end": 699
                },
                {
                    "start": 702,
                    "end": 799
                },
                {
                    "start": 800,
                    "end": 903
                },
                {
                    "start": 906,
                    "end": 983
                },
                {
                    "start": 984,
                    "end": 1176
                },
                {
                    "start": 1177,
                    "end": 1293
                },
                {
                    "start": 1294,
                    "end": 1490
                },
                {
                    "start": 1493,
                    "end": 1613
                },
                {
                    "start": 1614,
                    "end": 1728
                },
                {
                    "start": 1729,
                    "end": 1937
                },
                {
                    "start": 1938,
                    "end": 2163
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.470458984375
        },
        {
            "corpus_id": "270063400",
            "title": "SPP: Sparsity-Preserved Parameter-Efficient Fine-Tuning for Large Language Models",
            "text": "Despite their success in language model pruning, these methods often fail to maintain the performance of pre-trained models at even moderate sparsity levels (e.g., 50%) (Jaiswal et al., 2023).\n\nIn contrast, pruning methods for smaller-scale deep models -those with fewer than 200 million parameters, such as ResNet50 (He et al., 2016) and BERT-based models (Devlin et al., 2018) -have realized high sparsity ratios (e.g., >80%) only with negligible performance drop (Evci et al., 2020;Lin et al., 2020;Zhou et al., 2021).The success of these models is largely attributed to the role of retraining during their pruning process (Liu et al., 2018).However, compared with training dense neural networks, it will take much longer to train the sparse models to achieve the same performance.For example, 5\u00d7 more training time is needed in RigL (Evci et al., 2020).These methods heavily rely on back-propagation with full parameters, which is prohibitively expensive for LLMs.This observation raises a crucial question: Can we introduce an efficient retraining stage for the pruned LLMs?\n\nIn response to these challenges, we propose a novel, plugand-play method, SPP, a Sparsity-Preserved Paramaterefficient fine-tuning method designed to effectively retrain or fine-tune sparse LLMs after post-training pruning (e.g.SparseGPT (Frantar & Alistarh, 2023), Wanda (Sun et al., 2023), etc), thereby enhancing their performances.Inspired by the Low-Rank Adaptation (LoRA) method for dense large language models (Hu et al., 2021), SPP consists of two phases, training and weight-merging.More specifically, we introduce two sets of lightweight learnable parameters to the sparse matrices of each linear layer.During training and weight-merging phases, these learnable parameters are multiplied with the original frozen post-training pruned weights, achieving the effect of exactly maintaining the sparse pattern and ratio throughout all the processes.",
            "score": 0.6472024545662078,
            "section_title": "Introduction",
            "char_start_offset": 1614,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 192
                },
                {
                    "start": 194,
                    "end": 521
                },
                {
                    "start": 521,
                    "end": 645
                },
                {
                    "start": 645,
                    "end": 784
                },
                {
                    "start": 784,
                    "end": 857
                },
                {
                    "start": 857,
                    "end": 968
                },
                {
                    "start": 968,
                    "end": 1079
                },
                {
                    "start": 1081,
                    "end": 1309
                },
                {
                    "start": 1309,
                    "end": 1416
                },
                {
                    "start": 1416,
                    "end": 1573
                },
                {
                    "start": 1573,
                    "end": 1694
                },
                {
                    "start": 1694,
                    "end": 1936
                }
            ],
            "ref_mentions": [
                {
                    "start": 169,
                    "end": 191,
                    "matchedPaperCorpusId": "211069143"
                },
                {
                    "start": 317,
                    "end": 334,
                    "matchedPaperCorpusId": "206594692"
                },
                {
                    "start": 466,
                    "end": 485,
                    "matchedPaperCorpusId": "208267757"
                },
                {
                    "start": 837,
                    "end": 856,
                    "matchedPaperCorpusId": "208267757"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.478271484375
        },
        {
            "corpus_id": "265607979",
            "title": "GPTCache: An Open-Source Semantic Cache for LLM Applications Enabling Faster Answers and Cost Savings",
            "text": "Accelerating LLM Inference. Large language models (LLMs) typically take seconds to infer answers, prompting researchers to explore ways to reduce inference time and resource consumption. One approach is quantization (Dettmers and Zettlemoyer, 2023), which decreases the number of bits needed to represent each parameter and, therefore reduces the model size. However, this can result in a trade-off between accuracy and memory footprint. Another method is pruning, which can sparsify large-scale generative pre-trained transformer (GPT) models without retraining, as demonstrated by SparseGPT (Frantar and Alistarh, 2023). Additional methods include Compressing (Xu et al., 2020) and Inference with Reference (Yang et al., 2023).",
            "score": 0.6470847430801139,
            "section_title": "Accelerating LLM Inference",
            "char_start_offset": 2938,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 27
                },
                {
                    "start": 28,
                    "end": 186
                },
                {
                    "start": 187,
                    "end": 358
                },
                {
                    "start": 359,
                    "end": 437
                },
                {
                    "start": 438,
                    "end": 622
                },
                {
                    "start": 623,
                    "end": 729
                }
            ],
            "ref_mentions": [
                {
                    "start": 216,
                    "end": 248,
                    "matchedPaperCorpusId": "254853733"
                },
                {
                    "start": 662,
                    "end": 679,
                    "matchedPaperCorpusId": "211066200"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.69580078125
        },
        {
            "corpus_id": "263605754",
            "title": "Compressing LLMs: The Truth is Rarely Pure and Never Simple",
            "text": "The advent of large-scale pre-trained models has led to the development of advanced post-training pruning methods, aiming to enhance the cost-effectiveness of these expansive models (Sanh et al., 2020;Chen et al., 2020a;Jaiswal et al., 2023b;Zafrir et al., 2021;Kurtic et al., 2022;Xu et al., 2021;Lagunas et al., 2021;Zhang et al., 2022;Frantar et al., 2021;Jaiswal et al., 2023a;Ma et al., 2023;Ji et al., 2023). Among them, Frantar et al. (2021) extend second-order pruning to the BERTlevel scale, enabling the pruning of blocks of weights and achieving state-of-the-art results for sparse BERT. Frantar & Alistarh (2023) introduce SparseGPT for pruning large language models (LLMs) in a single shot without requiring re-training or fine-tuning. They leverage column-wise second-order pruning, and successfully remove 100B weights from OPT-175B without a significant increase in perplexity. More recently, Sun et al. (2023) propose a straightforward pruning method that takes both weights and activations into account, demonstrating comparable performance to Frantar & Alistarh (2023). Li et al. (2022) reveal that activation sparsity is a prevalent phenomenon in Transformers (90% of intermediate output), yielding another opportunity for acceleration. Liu et al. (2023b) introduce a large-scale SMC-Bench, indicating that state-of-the-art magnitude-and/or gradient-based sparse algorithms fall short when applied out-of-the-box to larger-scale models and a selected of complex downstream tasks.",
            "score": 0.6469370891317539,
            "section_title": "A.1.1 SPARSITY IN LARGE LANGUAGE MODELS",
            "char_start_offset": 26730,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 414
                },
                {
                    "start": 415,
                    "end": 598
                },
                {
                    "start": 599,
                    "end": 748
                },
                {
                    "start": 749,
                    "end": 893
                },
                {
                    "start": 894,
                    "end": 1088
                },
                {
                    "start": 1089,
                    "end": 1256
                },
                {
                    "start": 1257,
                    "end": 1499
                }
            ],
            "ref_mentions": [
                {
                    "start": 182,
                    "end": 201,
                    "matchedPaperCorpusId": "218665313"
                },
                {
                    "start": 201,
                    "end": 220,
                    "matchedPaperCorpusId": "220768628"
                },
                {
                    "start": 319,
                    "end": 338,
                    "matchedPaperCorpusId": "250072480"
                },
                {
                    "start": 338,
                    "end": 359,
                    "matchedPaperCorpusId": "243861553"
                },
                {
                    "start": 427,
                    "end": 448,
                    "matchedPaperCorpusId": "243861553"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.65966796875
        },
        {
            "corpus_id": "267617160",
            "title": "A Survey on Transformer Compression",
            "text": "In response to this motivation, recent works such as SIMPLE [158], LLM Pruner [29] and Sheared LLama [30] incorporate the causal language modeling loss in the pruning objective. However, this \"general pruning\" strategy raises another issue of data selection in the pruning process, as the domain gap between pre-training and pruning may hinder the importance of estimation/performance recovery. As detailed in [30], a dynamic batch loading strategy is applied to adaptively input the data batch from a proper domain to balance the model performance as a multi-task learner. \n\nComputation cost for parameter pruning. Considering the huge numbers of parameters, post-pruning retraining and parameter importance estimation may both generate a significant computational burden for large language models. In an early attempt, Frantar et al. [143]   another perspective, LoRAPrune [159] shows that introducing parameter-efficient tuning such as Low-Rank Adaption can also decrease the computation cost in retraining while compensating for the performance degradation. \n\nContext and token pruning. Different from the case in the computer vision domain, language sequence can be very long, and long-range reference or induction also calls for the expansion of input tokens (i.e., context) to prompt better language understanding. Also, since the inference cost of LLM is in quadratic complexity with respect to sequence length, this expansion greatly hinders the efficient inference of language models. To tackle these problems, methods have been proposed since BERT era to prune redundancy in context or context attention calculation. For context pruning, Kim et al. [160] propose to prune tokens across the Transformer layers progressively and improves the throughput by several folds on GLUE benchmark with less than 1% accuracy loss. Among the second line of research, various sparse attention techniques have been proposed, including local attention restricted to nearby tokens [161], [162], [163], introducing global tokens [164], [162], content-based token grouping [37], [163], and attention back tracking [165]. With the advance of the modeling capacity of LLM, the context input for LLM is also growing readily.",
            "score": 0.6466483299412157,
            "section_title": "Pruning for Transformer-Based Large Language Models",
            "char_start_offset": 46296,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 177
                },
                {
                    "start": 178,
                    "end": 394
                },
                {
                    "start": 395,
                    "end": 573
                },
                {
                    "start": 576,
                    "end": 615
                },
                {
                    "start": 616,
                    "end": 799
                },
                {
                    "start": 800,
                    "end": 1061
                },
                {
                    "start": 1064,
                    "end": 1090
                },
                {
                    "start": 1091,
                    "end": 1321
                },
                {
                    "start": 1322,
                    "end": 1494
                },
                {
                    "start": 1495,
                    "end": 1627
                },
                {
                    "start": 1628,
                    "end": 1829
                },
                {
                    "start": 1830,
                    "end": 2112
                },
                {
                    "start": 2113,
                    "end": 2213
                }
            ],
            "ref_mentions": [
                {
                    "start": 60,
                    "end": 65,
                    "matchedPaperCorpusId": "259858812"
                },
                {
                    "start": 1660,
                    "end": 1665,
                    "matchedPaperCorpusId": "235727659"
                },
                {
                    "start": 1989,
                    "end": 1994,
                    "matchedPaperCorpusId": "212718077"
                },
                {
                    "start": 2071,
                    "end": 2076,
                    "matchedPaperCorpusId": "212718077"
                },
                {
                    "start": 2106,
                    "end": 2111,
                    "matchedPaperCorpusId": "259298782"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.42626953125
        },
        {
            "corpus_id": "269605957",
            "title": "COPAL: Continual Pruning in Large Language Generative Models",
            "text": "The PTB dataset is a widely-used resource for syntactic analysis (Marcus et al., 1993).Lastly, the C4 dataset, part of the T5 model training corpus, provides a broad and diverse range of internet text, essential for evaluating model performance across various linguistic contexts (Raffel et al., 2020).\n\nBaselines.In our comparison, we evaluate the standard magnitude pruning approach, as established by Zhu & Gupta (2017), alongside the more recent developments in posttraining pruning works WANDA (Sun et al., 2023) and SparseGPT (Frantar & Alistarh, 2023).Each of these techniques is applied in a layer-wise manner, facilitating scalability even in the context of exceptionally large models.Magnitude pruning, a method detailed by Zhu & Gupta (2017), effectively compresses models by removing weights with the smallest absolute values, which are deemed least impactful on the network's output.In contrast, SparseGPT, introduced by Frantar & Alistarh (2023), integrates sparsity into the post-training process of transformer-based models.It often employs methods such as utilizing the Hessian matrix to identify weights that can be pruned with minimal loss in performance, effectively making the model sparse from the outset.Finally, WANDA, as proposed by Sun et al. (2023), does pruning by analyzing weight and activation distributions.This technique identifies and prunes network segments that minimally contribute to outputs.\n\nIn Table 2, baseline models employ global initialization due to weight stasis in sequential setups, as detailed in Section 3.2 and Table 1.Global initialization also leads to better average perplexity compared to sequential initialization.\n\nSparsity.In our evaluation, we specifically target the linear layers of large language models (LLMs), excluding the initial embedding layer and the final classification head.These linear layers constitute approximately 99% of the total parameters in LLMs (Sun et al., 2023), making them the primary focus for pruning.Uniform sparsity is maintained across all linear layers for consistency.",
            "score": 0.6458275221481065,
            "section_title": "Experimental Setting",
            "char_start_offset": 16775,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 87
                },
                {
                    "start": 87,
                    "end": 302
                },
                {
                    "start": 304,
                    "end": 314
                },
                {
                    "start": 314,
                    "end": 559
                },
                {
                    "start": 559,
                    "end": 694
                },
                {
                    "start": 694,
                    "end": 896
                },
                {
                    "start": 896,
                    "end": 1040
                },
                {
                    "start": 1040,
                    "end": 1227
                },
                {
                    "start": 1227,
                    "end": 1339
                },
                {
                    "start": 1339,
                    "end": 1430
                },
                {
                    "start": 1432,
                    "end": 1571
                },
                {
                    "start": 1571,
                    "end": 1671
                },
                {
                    "start": 1673,
                    "end": 1682
                },
                {
                    "start": 1682,
                    "end": 1847
                },
                {
                    "start": 1847,
                    "end": 1990
                },
                {
                    "start": 1990,
                    "end": 2062
                }
            ],
            "ref_mentions": [
                {
                    "start": 280,
                    "end": 301,
                    "matchedPaperCorpusId": "204838007"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.78076171875
        },
        {
            "corpus_id": "270257857",
            "title": "Pruner-Zero: Evolving Symbolic Pruning Metric from scratch for Large Language Models",
            "text": "The efficiency of pruning algorithms is critical in their application to large language models, such as the LLaMa 2-7b.Pruning time, an important metric for evaluating such algorithms, depends on various computational factors, including gradients, Hessian matrices, weight adjustments, and activation computations.The sequence of time consumption for these components generally follows:\n\nT ime(Hessian) > T ime(Gradient) > T ime(Activation) > T ime(W eight)\n\nThis hierarchy highlights the computational intensity of calculating the Hessian matrix, which requires the most time due to the complexity of computing second-order derivatives for each element in the weight matrix.To enhance the pruning process's efficiency, gradients are pre-computed offline.This preparatory step significantly speeds up the pruning phase by eliminating real-time gradient computation, thus streamlining the entire procedure.\n\nA comparative analysis of pruning time was conducted under fair conditions, where extraneous factors such as dataset loading and evaluation procedures were systematically excluded to ensure a level comparison field.The following table presents the pruning times and the associated perplexity results of various pruning methods, including our Pruner-Zero, SparseGPT (Frantar & Alistarh, 2023), and Wanda (Sun et al., 2024):\n\nThe data reveals that Pruner-Zero is twice as fast as SparseGPT (Frantar & Alistarh, 2023) and is slightly slower compared to Wanda (Sun et al., 2024), taking only 10% more time.However, this modest increase in time is offset by a significant improvement in model performance, as evidenced by Pruner-Zero's lower perplexity score (6.95) compared to Wanda's (7.26).This improvement underscores Pruner-Zero's ability to balance efficiency with effectiveness, optimizing not just for speed but also for enhancing the model's linguistic capabilities post-pruning.F. Searched Metrics with Expressions.\n\nTable 26 provides a symbolic string representation of the pruning metrics, offering an alternative perspective on the structural patterns that emerged from Pruner-Zero's search algorithm.",
            "score": 0.6447801342361795,
            "section_title": "E.5. Pruning Time Comparison.",
            "char_start_offset": 66830,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 119
                },
                {
                    "start": 119,
                    "end": 314
                },
                {
                    "start": 314,
                    "end": 386
                },
                {
                    "start": 388,
                    "end": 457
                },
                {
                    "start": 459,
                    "end": 675
                },
                {
                    "start": 675,
                    "end": 755
                },
                {
                    "start": 755,
                    "end": 905
                },
                {
                    "start": 907,
                    "end": 1122
                },
                {
                    "start": 1122,
                    "end": 1329
                },
                {
                    "start": 1331,
                    "end": 1509
                },
                {
                    "start": 1509,
                    "end": 1695
                },
                {
                    "start": 1695,
                    "end": 1890
                },
                {
                    "start": 1890,
                    "end": 1927
                },
                {
                    "start": 1929,
                    "end": 2116
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.50537109375
        },
        {
            "corpus_id": "277955463",
            "title": "NoWag: A Unified Framework for Shape Preserving Compression of Large Language Models",
            "text": "Large language models (LLMs) (Brown et al., 2020) have demonstrated remarkable capabilities across a wide range of natural language processing tasks, but their immense computational and memory requirements during inference pose significant challenges for deployment. Consequently, post-training compression techniques have emerged as a promising tool to reduce model size and computational overhead while maintaining accuracy. Two promising families of methods for post-training compression are Pruning (Lecun et al., 1989;Hassibi et al., 1993;Han et al., 2015) and Quantization (Yao et al., 2022;Dettmers et al., 2022;Ahmadian et al., 2023). \n\nPruning aims to remove redundant parameters from LLMs while preserving performance. We will focus on two forms of pruning, unstructured pruning (Liao et al., 2023), which removes zeroed out, and N:M semi-structured pruning (Huang et al., 2024b), where N of every M elements are zeroed out. SparseGPT (Frantar & Alistarh, 2023) introduced an efficient, unstructured and semi-structured pruning method that leverages Hessian-based weight updates to minimize performance loss. More recently, Wanda (Sun et al., 2024) demonstrated a simple yet effective unstructured and semi-structured pruning method that requires no weight updates or hessian computation, making it significantly faster and easier to apply than SparseGPT. However, current hardware only supports 2:4 semistructured sparsity, which results in significant post compression performance loss. \n\nA more effective compression method is quantization, which reduces the number of bits used to store each weight (Kuzmin et al., 2023). For the scope of this paper we focus on a common form of quantization, weight only Post Training Quantization (PTQ). Pioneering works (Frantar et al., 2023;Lin et al., 2024;Kim et al., 2024) focused on scalar quantization.",
            "score": 0.6436211985168043,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 266
                },
                {
                    "start": 267,
                    "end": 426
                },
                {
                    "start": 427,
                    "end": 642
                },
                {
                    "start": 645,
                    "end": 728
                },
                {
                    "start": 729,
                    "end": 934
                },
                {
                    "start": 935,
                    "end": 1118
                },
                {
                    "start": 1119,
                    "end": 1365
                },
                {
                    "start": 1366,
                    "end": 1498
                },
                {
                    "start": 1501,
                    "end": 1635
                },
                {
                    "start": 1636,
                    "end": 1752
                },
                {
                    "start": 1753,
                    "end": 1858
                }
            ],
            "ref_mentions": [
                {
                    "start": 523,
                    "end": 544,
                    "matchedPaperCorpusId": "61815367"
                },
                {
                    "start": 579,
                    "end": 597,
                    "matchedPaperCorpusId": "249395624"
                },
                {
                    "start": 597,
                    "end": 619,
                    "matchedPaperCorpusId": "251564521"
                },
                {
                    "start": 619,
                    "end": 641,
                    "matchedPaperCorpusId": "258967189"
                },
                {
                    "start": 789,
                    "end": 808,
                    "matchedPaperCorpusId": "260887085"
                },
                {
                    "start": 1613,
                    "end": 1634,
                    "matchedPaperCorpusId": "259360935"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.78076171875
        },
        {
            "corpus_id": "276250081",
            "title": "EfficientLLM: Scalable Pruning-Aware Pretraining for Architecture-Agnostic Edge Language Models",
            "text": "We mainly focus on structural pruning (Chen et al., 2023;Choukse et al., 2018) for hardware friendly edge models. The most widely used LLM pruning is based on the Taylor expansion (LeCun et al., 1989;Hassibi et al., 1993;van der Ouderaa et al., 2023), as shown in Table 1. By calibration, typical SparseGPT (Frantar & Alistarh, 2023) and Wanda (Sun et al., 2023) can only applied in simi-structured pruning; LLM-Pruner (Ma et al., 2023) only achieves 20% pruning ratio with reasonable accuracy. Even if pruning with finetuning, LoraPrune (Zhang et al., 2023a) can only prune in 50% ratio. So there is an urgent requirement to scale up LLM pruning in pretraining. \n\nAnother line of works learn to initialize from source model such as ShearedLlama (Xia et al., 2023) and NutePrune (Li et al., 2024) with less than 0.5B tokens. However the constraint optimization is biased in large-scale pretraining and this work scales up Taylor expansion metrics.",
            "score": 0.642950334950305,
            "section_title": "Preliminary and Related Works",
            "char_start_offset": 5888,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 113
                },
                {
                    "start": 114,
                    "end": 272
                },
                {
                    "start": 273,
                    "end": 494
                },
                {
                    "start": 495,
                    "end": 588
                },
                {
                    "start": 589,
                    "end": 662
                },
                {
                    "start": 665,
                    "end": 824
                },
                {
                    "start": 825,
                    "end": 947
                }
            ],
            "ref_mentions": [
                {
                    "start": 180,
                    "end": 200,
                    "matchedPaperCorpusId": "7785881"
                },
                {
                    "start": 200,
                    "end": 221,
                    "matchedPaperCorpusId": "61815367"
                },
                {
                    "start": 221,
                    "end": 250,
                    "matchedPaperCorpusId": "259950998"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.429931640625
        },
        {
            "corpus_id": "273345728",
            "title": "SLiM: One-shot Quantization and Sparsity with Low-rank Approximation for LLM Weight Compression",
            "text": "Large Language Models (LLMs) (Brown et al., 2020;Radford et al., 2019) are transformative for natural language understanding and generation (Suzgun et al., 2022;Zhou et al., 2023); however, their extensive parameter count leads to large memory footprints and longer inference times, making them expensive to execute. Model compression methods, such as sparsity and quantization, have shown promising results in reducing the inference costs of LLMs. However, these methods often require costly retraining on large amounts of data to restore the original model accuracy (Sanh et al., 2020;Park et al., 2018), while facing numerical and optimization stability challenges when dealing with quantized weights in quantization-aware-training (Gholami et al., 2022). \n\nTo address these issues, one-shot pruning methods have emerged, eliminating the need for the retraining. They achieve high accuracy using only a small set of calibration data. Optimal Brain Damage (OBD) (LeCun et al., 1989) pioneered the use of second-order information of the loss function for model compression (Singh & Alistarh, 2020;Mozaffari et al., 2023), though at a high computational cost. Subsequent methods like Optimal Brain Surgeon (OBS) (Hassibi et al., 1993) and modern approaches such as SparseGPT (Frantar & Alistarh, 2023) and WANDA (Sun et al., 2023) build on these ideas, and introduce computationally feasible alternatives for LLMs. While these methods perform well with unstructured sparsity, they struggle with semi-structured sparsity patterns like the NVIDIA 2:4 sparsity pattern (Mishra et al., 2021), which are necessary for hardware-accelerated inference. \n\nPost-training quantization (Rokh et al., 2023) methods are an effective compression strategy for reducing both memory usage and computation costs. SmoothQuant (Xiao et al., 2023) preserves accuracy by leaving the more salient weight channels unquantized, but this results in inefficient de-quantization during inference.",
            "score": 0.6426709938571955,
            "section_title": "INTRODUCTION",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 316
                },
                {
                    "start": 317,
                    "end": 448
                },
                {
                    "start": 449,
                    "end": 758
                },
                {
                    "start": 761,
                    "end": 865
                },
                {
                    "start": 866,
                    "end": 936
                },
                {
                    "start": 937,
                    "end": 1159
                },
                {
                    "start": 1160,
                    "end": 1414
                },
                {
                    "start": 1415,
                    "end": 1644
                },
                {
                    "start": 1647,
                    "end": 1793
                },
                {
                    "start": 1794,
                    "end": 1967
                }
            ],
            "ref_mentions": [
                {
                    "start": 29,
                    "end": 49,
                    "matchedPaperCorpusId": "218971783"
                },
                {
                    "start": 49,
                    "end": 70,
                    "matchedPaperCorpusId": "160025533"
                },
                {
                    "start": 568,
                    "end": 587,
                    "matchedPaperCorpusId": "218665313"
                },
                {
                    "start": 735,
                    "end": 757,
                    "matchedPaperCorpusId": "232352683"
                },
                {
                    "start": 964,
                    "end": 984,
                    "matchedPaperCorpusId": "7785881"
                },
                {
                    "start": 1074,
                    "end": 1098,
                    "matchedPaperCorpusId": "220364055"
                },
                {
                    "start": 1212,
                    "end": 1234,
                    "matchedPaperCorpusId": "7001469"
                },
                {
                    "start": 1275,
                    "end": 1301,
                    "matchedPaperCorpusId": "255372747"
                },
                {
                    "start": 1674,
                    "end": 1693,
                    "matchedPaperCorpusId": "261661742"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.62646484375
        },
        {
            "corpus_id": "270257857",
            "title": "Pruner-Zero: Evolving Symbolic Pruning Metric from scratch for Large Language Models",
            "text": "This section extends the discourse on pruning methodologies to the Tiny-LLaMA (Zhang et al., 2024a) and OPT (Zhang et al., 2022b), which is engineered for optimal performance with a minimal parameter set.Table 22 benchmarks Tiny-LLaMA against a suite of pruning approaches, which include unstructured pruning as well as structured pruning with specific ratios, namely 2:4 and 4:8.\n\nWe scrutinize a range of pruning methods: Magnitude-based pruning, the SparseGPT algorithm, the Wanda technique, and our proposed Pruner-Zero.The post-pruning performance metrics illuminate the interplay between model size and operational effectiveness.While Magnitude pruning serves as a ubiquitous benchmark, it demonstrates a variable efficacy across the pruning ratios, particularly underperforming at the 2:4 ratio.Contrastingly, SparseGPT and Wanda deliver enhanced capabilities over Magnitude pruning, attesting to their prowess in preserving model competence during downsizing.\n\nOur Pruner-Zero is noteworthy for its competitive performance, especially prominent at the unstructured and 4:8 pruning thresholds, which underscores its suitability as an adept pruning strategy for Tiny-LLaMA.The empirical evidence accentuates the proficiency of Pruner-Zero in closely approximating the performance of models with lower levels of pruning, a critical consideration for deployment in resource-limited settings where compact models are requisite without considerable compromise in functionality.\n\nSubsequently, Table 4 delineates the pruning performance across the spectrum of OpenAI's OPT models, with sizes ranging from 125 million to 13 billion parameters.The methodologies under scrutiny here comprise a baseline dense model, Magnitude pruning, SparseGPT, Wanda, and Pruner-Zero, each targeting a consistent sparsity quotient of 50%.\n\nThe results conspicuously reveal that Pruner-Zero outperforms other approaches that forego a weight update phase, specifically Magnitude pruning and Wanda.This is manifested in markedly lower perplexity scores for models pruned via Pruner-Zero, indicating substantial retention of predictive capacity post-pruning.",
            "score": 0.6425401713701795,
            "section_title": "E.4. Comparative Analysis of Pruning Techniques",
            "char_start_offset": 64189,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 204
                },
                {
                    "start": 204,
                    "end": 380
                },
                {
                    "start": 382,
                    "end": 524
                },
                {
                    "start": 524,
                    "end": 635
                },
                {
                    "start": 635,
                    "end": 802
                },
                {
                    "start": 802,
                    "end": 967
                },
                {
                    "start": 969,
                    "end": 1179
                },
                {
                    "start": 1179,
                    "end": 1479
                },
                {
                    "start": 1481,
                    "end": 1643
                },
                {
                    "start": 1643,
                    "end": 1821
                },
                {
                    "start": 1823,
                    "end": 1978
                },
                {
                    "start": 1978,
                    "end": 2137
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.7314453125
        },
        {
            "corpus_id": "273375561",
            "title": "MoE-Pruner: Pruning Mixture-of-Experts Large Language Model using the Hints from Its Router",
            "text": "Table 2 shows the one-shot pruning model perplexity on WikiText with 50% sparsity. There is a clear difference between MoE-Pruner and other pruning methods, including SparseGPT (Frantar & Alistarh, 2023b) and Wanda (Sun et al., 2024). For Mixtral-8x7B (Jiang et al., 2024) models, MoE-Pruner achieves 0.22-0.31 better perplexity over SparseGPT and Wanda. This improvement expands when the MoE model scales to the Mixtral-8x22B model. For the larger Mixtral-8x22B model, MoE-Pruner achieves 0.55 better perplexity over SparseGPT and 0.31-0.34 better perplexity over Wanda. MoE-Pruner further expands the improvement to 1.21 better perplexity over SparseGPT and 1.10 better perplexity over Wanda when we prune the MoE models with the 2:4 semi-structured sparsity. \n\nTable 3 shows the average zero-shot accuracies on nine zero-shot tasks of the pruned Mixtral-8x7B MoE models with 50% unstructured sparsity. The average performance of pretrained models, SparseGPT, Wanda, and our pruned models are 69.16, 66.27, 65.90, and 67.23, respectively. MoE-Pruner outperforms the state-of-the-art pruning approaches, SparseGPT and Wanda, by a large margin. Given that no fine-tuning takes place at this time, there is a noticeable gap between the sparse pruned MoE model and the original pretrained MoE model.",
            "score": 0.6422767647110554,
            "section_title": "ONE-SHOT PRUNING",
            "char_start_offset": 18691,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 82
                },
                {
                    "start": 83,
                    "end": 310
                },
                {
                    "start": 311,
                    "end": 354
                },
                {
                    "start": 355,
                    "end": 433
                },
                {
                    "start": 434,
                    "end": 541
                },
                {
                    "start": 542,
                    "end": 571
                },
                {
                    "start": 572,
                    "end": 761
                },
                {
                    "start": 764,
                    "end": 904
                },
                {
                    "start": 905,
                    "end": 1040
                },
                {
                    "start": 1041,
                    "end": 1144
                },
                {
                    "start": 1145,
                    "end": 1297
                }
            ],
            "ref_mentions": [
                {
                    "start": 177,
                    "end": 204,
                    "matchedPaperCorpusId": "255372747"
                },
                {
                    "start": 215,
                    "end": 232,
                    "matchedPaperCorpusId": "259203115"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.59130859375
        },
        {
            "corpus_id": "277634221",
            "title": "Mosaic: Composite Projection Pruning for Resource-efficient LLMs",
            "text": "Compressing LLMs: The three main LLM compression methods to improve resource efficiency are model quantization, pruning, and knowledge distillation. \n\nModel Quantization -State-of-the-art quantization methods, such as GPTQ [29] and AWQ [44] reduce FP16 weights of the LLM into lower precision, such as INT3, INT4, or INT8, which requires less memory. Although the model size is reduced by more than half with minimal accuracy loss, the overall memory required remains high because model activations are not quantized and still use the higher FP16 precision. As memory demands grow due to larger activations from longer input lengths (more tokens), the quantized weights account for only a small fraction of the total memory usage, which reduces the overall effectiveness of this approach. Alternative methods that quantize model activations have lower accuracies and rely on a decomposition scheme to keep outlier weights at FP16 [45]. Although all quantization methods reduce memory requirements, orthogonal approaches are required to lower inference latency. For example, GPTQ relies on custom CUDA kernels [29], and AWQ utilizes QKV kernel fusion [44]. They are, therefore, suited to GPU-based systems for performance gains, which may not be available in resource-constrained environments. \n\nModel Pruningtwo categories of LLM pruning remove redundant parameters: unstructured pruning, which preserves model accuracy, and structured pruning, which reduces memory and computational requirements. \n\nSparseGPT [12] and Wanda [21] are unstructured pruning methods. SparseGPT implements Optimal Brain Surgeon (OBS) [46] and prunes models with hundreds of billions of parameters in a few hours. In contrast, Wanda employs a simpler pruning metric based solely on model weights and activations, making it two orders of magnitude faster than SparseGPT. Unstructured pruning does not reduce model size since parameters are set to zero. These models require specialized sparse acceleration libraries, such as NVIDIA CUTLASS, which is limited to models that are 50% sparsity pruned using a specific semi-structured format [12], [21], [47].",
            "score": 0.642274488516227,
            "section_title": "VI. RELATED WORK",
            "char_start_offset": 43878,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 148
                },
                {
                    "start": 151,
                    "end": 350
                },
                {
                    "start": 351,
                    "end": 557
                },
                {
                    "start": 558,
                    "end": 788
                },
                {
                    "start": 789,
                    "end": 935
                },
                {
                    "start": 936,
                    "end": 1060
                },
                {
                    "start": 1061,
                    "end": 1155
                },
                {
                    "start": 1156,
                    "end": 1292
                },
                {
                    "start": 1295,
                    "end": 1497
                },
                {
                    "start": 1500,
                    "end": 1563
                },
                {
                    "start": 1564,
                    "end": 1691
                },
                {
                    "start": 1692,
                    "end": 1847
                },
                {
                    "start": 1848,
                    "end": 1929
                },
                {
                    "start": 1930,
                    "end": 2131
                }
            ],
            "ref_mentions": [
                {
                    "start": 223,
                    "end": 227,
                    "matchedPaperCorpusId": "259298689"
                },
                {
                    "start": 236,
                    "end": 240,
                    "matchedPaperCorpusId": "258999941"
                },
                {
                    "start": 930,
                    "end": 934,
                    "matchedPaperCorpusId": "251564521"
                },
                {
                    "start": 1109,
                    "end": 1113,
                    "matchedPaperCorpusId": "259298689"
                },
                {
                    "start": 1150,
                    "end": 1154,
                    "matchedPaperCorpusId": "258999941"
                },
                {
                    "start": 1510,
                    "end": 1514,
                    "matchedPaperCorpusId": "255372747"
                },
                {
                    "start": 1525,
                    "end": 1529,
                    "matchedPaperCorpusId": "259203115"
                },
                {
                    "start": 1613,
                    "end": 1617,
                    "matchedPaperCorpusId": "61815367"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.650390625
        },
        {
            "corpus_id": "277622174",
            "title": "Task-Aware Parameter-Efficient Fine-Tuning of Large Pre-Trained Models at the Edge",
            "text": "The demand for edge-deployed LLMs is driven by needs for faster response times, low latency, offline functionality, privacy, and personalized experiences, as seen in applications on smartphones, wearables, and smart home assistants [34,35,36,37,38,39,40]. The background work highlights the widening gap between LLM complexity and edge device capabilities, with much research focusing on efficient LLMs and edge computing optimizations including quantization [41], pruning [42], knowledge distillation [43], and low-rank approximations. For example, Dettmers et al. [41] introduced a novel Int8 quantization method for large language models, combining vector-wise quantization and mixed-precision decomposition to reduce memory usage by half while maintaining full precision performance, enabling inference of models up to 175 billion parameters on consumer GPUs without degradation. Franter et al. [42] proposed SparseGPT, a one-shot pruning method that efficiently compresses massive GPT-family language models like OPT-175B and BLOOM-176B to 50-60% sparsity without retraining, achieving minimal accuracy loss by solving large-scale sparse regression problems in under 4.5 hours on a single GPU. In addition, Wang et al. [43] proposed a task-agnostic distillation method that compresses large Transformer-based language models like BERT into smaller models by deeply mimicking the self-attention module of the teacher's last layer, introducing value-relation transfer, and optionally using a teacher assistant, achieving over 99% accuracy retention on SQuAD 2.0 and GLUE tasks with 50% fewer parameters and computations.",
            "score": 0.6419737974241659,
            "section_title": "C. Edge Computing for Large Pre-trained Models",
            "char_start_offset": 7954,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 255
                },
                {
                    "start": 256,
                    "end": 536
                },
                {
                    "start": 537,
                    "end": 883
                },
                {
                    "start": 884,
                    "end": 1198
                },
                {
                    "start": 1199,
                    "end": 1623
                }
            ],
            "ref_mentions": [
                {
                    "start": 232,
                    "end": 236,
                    "matchedPaperCorpusId": "271534421"
                },
                {
                    "start": 236,
                    "end": 239,
                    "matchedPaperCorpusId": "257771867"
                },
                {
                    "start": 239,
                    "end": 242,
                    "matchedPaperCorpusId": "265018995"
                },
                {
                    "start": 251,
                    "end": 254,
                    "matchedPaperCorpusId": "269614239"
                },
                {
                    "start": 459,
                    "end": 463,
                    "matchedPaperCorpusId": "251564521"
                },
                {
                    "start": 473,
                    "end": 477,
                    "matchedPaperCorpusId": "255372747"
                },
                {
                    "start": 502,
                    "end": 506,
                    "matchedPaperCorpusId": "211296536"
                },
                {
                    "start": 566,
                    "end": 570,
                    "matchedPaperCorpusId": "251564521"
                },
                {
                    "start": 899,
                    "end": 903,
                    "matchedPaperCorpusId": "255372747"
                },
                {
                    "start": 1224,
                    "end": 1228,
                    "matchedPaperCorpusId": "211296536"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.73876953125
        },
        {
            "corpus_id": "270621063",
            "title": "Dependency-Aware Semi-Structured Sparsity of GLU Variants in Large Language Models",
            "text": "We perform extensive experiments on LLaMA2 (Touvron et al., 2023), Gemma (Team et al., 2024), andMistral (Jiang et al., 2023) to evaluate DaSS across various tasks from language modeling, 5 commonsense reasoning tasks. In achieving hardware-friendly N:M sparsity patterns, DaSS consistently excels beyond the existing LLM pruning methods SparseGPT and Wanda, while maintaining the computational efficiency akin to Wanda. Moreover, DaSS demonstrates consistent effectiveness in all the prevalent GLU variants, including SwiGLU, GeGLU and ReGLU. Impressively, DaSS outperforms SparseGPT at high sparsity even without weight update. We hope our fresh insights can motivate more nuanced GLU-specific LLM compression strategies.",
            "score": 0.6419330366279864,
            "section_title": "Introduction",
            "char_start_offset": 5209,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 218
                },
                {
                    "start": 219,
                    "end": 420
                },
                {
                    "start": 421,
                    "end": 543
                },
                {
                    "start": 544,
                    "end": 629
                },
                {
                    "start": 630,
                    "end": 723
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.650390625
        },
        {
            "corpus_id": "275993741",
            "title": "DReSS: Data-driven Regularized Structured Streamlining for Large Language Models",
            "text": "Large language models (LLMs) have achieved significant advancements across a wide range of tasks and domains, Preprint demonstrating their robust capabilities (Zhang et al., 2022;Achiam et al., 2023;Touvron et al., 2023;Wu et al., 2024). However, as the model size increases, the growing number of parameters leads to significant computational and memory requirements, which significantly hinder the practical deployment of LLMs. Consequently, it is urgent to develop methods that can reduce model size while maintaining performance. \n\nTo address these challenges, several methods have been proposed, including pruning (Frantar & Alistarh, 2023;Sun et al., 2023;Ma et al., 2023;An et al., 2024), quantization (Frantar et al., 2022;Xiao et al., 2023), knowledge distillation (Shridhar et al., 2022;Hsieh et al., 2023), and low-rank decomposition (Saha et al., 2023). In this work, we mainly focus on pruning-an efficient and highly generalizable approach that can be seamlessly integrated with other model compression strategies. Pruning techniques are generally classified into two primary categories: unstructured pruning (Frantar & Alistarh, 2023;Sun et al., 2023) and structured pruning (Ma et al., 2023;An et al., 2024). Compared to unstructured pruning, structured pruning offers the flexibility to do recovery fine-tuning (RFT) for specific downstream tasks without relying on specialized hardware (Zhu et al., 2024). Moreover, the model obtained through structured pruning typically achieves much faster inference speed due to the regular data patterns. Despite these advancements, existing structured pruning methods still have some limitations. They all follow the paradigm of first selecting channels or layers to prune based on a designed metric, and then performing RFT (Chavan et al., 2024).",
            "score": 0.6409777505243527,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 237
                },
                {
                    "start": 238,
                    "end": 429
                },
                {
                    "start": 430,
                    "end": 533
                },
                {
                    "start": 536,
                    "end": 865
                },
                {
                    "start": 866,
                    "end": 1028
                },
                {
                    "start": 1029,
                    "end": 1224
                },
                {
                    "start": 1225,
                    "end": 1423
                },
                {
                    "start": 1424,
                    "end": 1560
                },
                {
                    "start": 1561,
                    "end": 1653
                },
                {
                    "start": 1654,
                    "end": 1804
                }
            ],
            "ref_mentions": [
                {
                    "start": 199,
                    "end": 220,
                    "matchedPaperCorpusId": "257219404"
                },
                {
                    "start": 619,
                    "end": 645,
                    "matchedPaperCorpusId": "255372747"
                },
                {
                    "start": 662,
                    "end": 678,
                    "matchedPaperCorpusId": "258823276"
                },
                {
                    "start": 845,
                    "end": 864,
                    "matchedPaperCorpusId": "262233736"
                },
                {
                    "start": 1123,
                    "end": 1149,
                    "matchedPaperCorpusId": "255372747"
                },
                {
                    "start": 1190,
                    "end": 1207,
                    "matchedPaperCorpusId": "258823276"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.71728515625
        },
        {
            "corpus_id": "276902790",
            "title": "Sample-aware Adaptive Structured Pruning for Large Language Models",
            "text": "For example, LLM-Pruner (Ma, Fang, and Wang 2023) is the first attempt at structured pruning of LLMs, reducing model computation and memory usage while keeping the overall structure of the LLMs. Specifically, it utilizes a dependency detection algorithm to identify coupled structures in the LLMs, scores the importance of the coupled structures, and selectively removes non-critical structures according to first-order and approximate Hessianbased information. Finally, LLM-Pruner uses Low-Rank-Adaptor (LoRA) for fine-tuning to recover the pruned model weights. Further, LoRAShear (Chen et al. 2023) utilizes the Lora Half-Space Projected Gradient (LH-SPG) technique for incremental structured pruning and knowledge recovery. LoRAShear can be applied to various LLMs through dependency graph analysis and sparsity optimization. Recently, Sheared-LLaMA (Xia et al. 2024) is a directed structured pruning that prunes a large model to a specified target structure. It then trains the pruned model using dynamically loaded data based on the loss reduction ratio in each domain, thus improving data usage efficiency and model performance. However, Sheared-LLaMA uses substantial computational resources for subsequent pre-training to performance recovery. Like LLM-Pruner, the number of layers is constant. Shortened LLaMA (Kim et al. 2024) proposes that a simple deep pruning method can achieve promising performance and improve inference speed in a zero-shot task. \u2022 Unstructured pruning. It performs pruning operations on weights or neurons instead of structured blocks. Although unstructured pruning typically results in higher compression ratios, the need for specific hardware support leads to the failure to achieve proper inference acceleration or storage reduction. SparseGPT (Frantar and Alistarh 2023) is the first approach to perform unstructured pruning on LLMs. It applies the Hessian inverse to pruning and uses a complex weight-updating process that includes synchronized second-order Hessian updates. The  strategy is computationally expensive to execute iteratively between weight pruning and weight updating at each layer.",
            "score": 0.6398123243258327,
            "section_title": "Model Quantization Model quantization compresses",
            "char_start_offset": 25661,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 194
                },
                {
                    "start": 195,
                    "end": 461
                },
                {
                    "start": 462,
                    "end": 563
                },
                {
                    "start": 564,
                    "end": 727
                },
                {
                    "start": 728,
                    "end": 829
                },
                {
                    "start": 830,
                    "end": 963
                },
                {
                    "start": 964,
                    "end": 1135
                },
                {
                    "start": 1136,
                    "end": 1252
                },
                {
                    "start": 1253,
                    "end": 1303
                },
                {
                    "start": 1304,
                    "end": 1463
                },
                {
                    "start": 1464,
                    "end": 1487
                },
                {
                    "start": 1488,
                    "end": 1570
                },
                {
                    "start": 1571,
                    "end": 1771
                },
                {
                    "start": 1772,
                    "end": 1872
                },
                {
                    "start": 1873,
                    "end": 2014
                },
                {
                    "start": 2015,
                    "end": 2138
                }
            ],
            "ref_mentions": [
                {
                    "start": 24,
                    "end": 49,
                    "matchedPaperCorpusId": "258823276"
                },
                {
                    "start": 854,
                    "end": 870,
                    "matchedPaperCorpusId": "263830786"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.7265625
        },
        {
            "corpus_id": "266375109",
            "title": "Mini-GPTs: Efficient Large Language Models through Contextual Pruning",
            "text": "The advent of Large Language Models (LLMs) like GPT-4 has marked a paradigm shift in artificial intelligence, offering unparalleled capabilities in natural language processing. However, their extensive computational demands pose significant challenges, particularly in terms of cost, latency, emission concerns, and cloud dependence. This has spurred interest in model optimization techniques, notably model pruning, to address these challenges. \n\nModel pruning, as explored by Han et al., 2015 in \"Deep Compression: Compressing Deep Neural Networks with Pruning, Trained Quantization and Huffman Coding\", has emerged as a promising avenue for reducing neural network sizes without substantially compromising their performance. This technique involves systematically removing non-critical weights from a network, thereby reducing its complexity, size, cost, and latency. Further advancements by Frankle et al., 2018 in \"The Lottery Ticket Hypothesis: Finding Sparse, Trainable Neural Networks\" introduced the concept of identifying and training sparse subnetworks within larger models, suggesting that these 'lottery tickets' can achieve similar accuracy to their dense counterparts. This paper examines the application of contextual pruning in creating Mini-GPTs, smaller yet efficient versions of existing LLMs. By analyzing and removing less critical weights specific to different domains, such as law, healthcare, and finance, we aim to maintain or enhance model performance while significantly reducing size and resource usage. This approach stacks with those designed by Han et al., 2015 as synapse pruning (or connection pruning), quantization, and neural architecture search may done separately to our approach. \n\nThe initial motivation for pruning on context came from the realization that modern open-source LLMs are trained on broad datasets (e.g. Wikipedia, commercial-free books, and Reddit) but B2B users are only leveraging a small fraction of the information latent in the network that's relevant to their use case. By analogy, an LLM used at a hospital doesn't need to know options trading and Shakespeare -it just needs common sense, logical reasoning skills, and healthcare domain knowledge.",
            "score": 0.6398041320912786,
            "section_title": "Introduction & Literature Review",
            "char_start_offset": 35,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 176
                },
                {
                    "start": 177,
                    "end": 333
                },
                {
                    "start": 334,
                    "end": 445
                },
                {
                    "start": 448,
                    "end": 727
                },
                {
                    "start": 728,
                    "end": 870
                },
                {
                    "start": 871,
                    "end": 1183
                },
                {
                    "start": 1184,
                    "end": 1313
                },
                {
                    "start": 1314,
                    "end": 1532
                },
                {
                    "start": 1533,
                    "end": 1719
                },
                {
                    "start": 1722,
                    "end": 1858
                },
                {
                    "start": 1859,
                    "end": 2031
                },
                {
                    "start": 2032,
                    "end": 2210
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.64208984375
        },
        {
            "corpus_id": "269605957",
            "title": "COPAL: Continual Pruning in Large Language Generative Models",
            "text": "Pre-trained LLMs have shown generizability across various NLP tasks.Despite their advancements, finetuning of LLMs face challenges in computational efficiency, requiring optimization strategies like pruning for practical deployment.\n\nPruning.Pruning plays an important role in optimizing neural network architectures, especially in large models.\n\nTypes of Pruning: Structured Pruning involves removing entire structural elements such as neurons, filters, or layers.Key contributions in this area include network slimming Liu et al. (2017), channel pruning (He et al., 2017), and optimizing network architectures with minimal performance trade-offs (Luo et al., 2017;Yu et al., 2018).Structured pruning is particularly relevant for simplifying large language models and enhancing their efficiency.Whereas, unstructured pruning focuses on the selective removal of individual weights.It aims to eliminate less critical connections within the network.Pioneering techniques like \"Optimal Brain Damage\" (LeCun et al., 1989) and \"Deep Compression\" (Han et al., 2015) have significantly contributed to reducing neural network size.The \"Lottery Ticket Hypothesis\" (Frankle & Carbin, 2018) suggests the existence of smaller, effective sub-networks, which is a critical concept for large model optimization.\n\nStages of Pruning: Pruning can be applied before training, during training, or post-training.Each stage offers unique advantages and challenges.For instance, SNIP (Lee et al., 2018) and GraSP (Wang et al., 2020) focus on identifying critical connections before training starts.On the other hand, dynamic sparse training (Liu et al., 2020), soft filter pruning (He et al., 2018)",
            "score": 0.6389114313582093,
            "section_title": "Prior Works",
            "char_start_offset": 5187,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 68
                },
                {
                    "start": 68,
                    "end": 232
                },
                {
                    "start": 234,
                    "end": 242
                },
                {
                    "start": 242,
                    "end": 345
                },
                {
                    "start": 347,
                    "end": 465
                },
                {
                    "start": 465,
                    "end": 683
                },
                {
                    "start": 683,
                    "end": 796
                },
                {
                    "start": 796,
                    "end": 881
                },
                {
                    "start": 881,
                    "end": 947
                },
                {
                    "start": 947,
                    "end": 1123
                },
                {
                    "start": 1123,
                    "end": 1296
                },
                {
                    "start": 1298,
                    "end": 1391
                },
                {
                    "start": 1391,
                    "end": 1442
                },
                {
                    "start": 1442,
                    "end": 1575
                },
                {
                    "start": 1575,
                    "end": 1675
                }
            ],
            "ref_mentions": [
                {
                    "start": 521,
                    "end": 538,
                    "matchedPaperCorpusId": "5993328"
                },
                {
                    "start": 556,
                    "end": 573,
                    "matchedPaperCorpusId": "20157893"
                },
                {
                    "start": 648,
                    "end": 666,
                    "matchedPaperCorpusId": "11169209"
                },
                {
                    "start": 666,
                    "end": 682,
                    "matchedPaperCorpusId": "4142619"
                },
                {
                    "start": 997,
                    "end": 1017,
                    "matchedPaperCorpusId": "7785881"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.54052734375
        },
        {
            "corpus_id": "270391791",
            "title": "ALPS: Improved Optimization for Highly Sparse One-Shot Pruning for Large Language Models",
            "text": "The impressive performance of Large Language Models (LLMs) across various natural language processing tasks comes at the cost of vast computational resources and storage requirements. One-shot pruning techniques offer a way to alleviate these burdens by removing redundant weights without the need for retraining. Yet, the massive scale of LLMs often forces current pruning approaches to rely on heuristics instead of optimization-based techniques, potentially resulting in suboptimal compression. In this paper, we introduce ALPS, an optimization-based framework that tackles the pruning problem using the operator splitting technique and a preconditioned conjugate gradient-based post-processing step. Our approach incorporates novel techniques to accelerate and theoretically guarantee convergence while leveraging vectorization and GPU parallelism for efficiency. ALPS substantially outperforms state-of-the-art methods in terms of the pruning objective and perplexity reduction, particularly for highly sparse models. On the OPT-30B model with 70% sparsity, ALPS achieves a 13% reduction in test perplexity on the WikiText dataset and a 19% improvement in zero-shot benchmark performance compared to existing methods.",
            "score": 0.6385400904267634,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.8603515625
        },
        {
            "corpus_id": "276961144",
            "title": "T\u00fdr-the-Pruner: Unlocking Accurate 50% Structural Pruning for LLMs via Global Sparsity Distribution Optimization",
            "text": "T\u00fdr-the-Pruner, by contrast, excels under these conditions. For example, at 37.5% sparsity, the pruned Mistral-Nemo model achieves a perplexity of 11.47 and an accuracy of 55.63%, substantially outperforming ZipLM and FLAP. \n\nScale up to massive language models. Structural pruning of massive language models challenges post-pruned performance and resource budgets. We incorporated a CPU offload policy into typical baseline methods to ensure a fair comparison on 70B-scale models. Table 2 compares the post-pruning performance of Llama-2-70B and Llama-3.1-70B at 50% sparsity. \n\nExperimental results demonstrate T\u00fdr-the-Pruner's strong scalability under high sparsity for massive models. LLM-Pruner shows clear scaling limitations, maintaining only 48% accuracy when pruning Llama-2-70B. In contrast, T\u00fdrthe-Pruner achieves 97% accuracy maintenance when prun-ing Llama-3.1-70B, outperforming alternative methods. \n\nInference efficiency of post-pruned LLMs. To evaluate the efficiency gains of post-pruned LLMs, we constructed inference benchmarks summarized in Table 3. For Llama-3.1-8B, 50% sparsity reduces time to first token (TTFT, in seconds) by 43% and boosts decode throughput (tokens/s) by 38%. These results highlight pruning as a key technique for inference optimization in large language models. More detailed efficiency analysis can be found in Appendix A.4.",
            "score": 0.6368463061318301,
            "section_title": "Performance",
            "char_start_offset": 18603,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 59
                },
                {
                    "start": 60,
                    "end": 223
                },
                {
                    "start": 226,
                    "end": 262
                },
                {
                    "start": 263,
                    "end": 365
                },
                {
                    "start": 366,
                    "end": 481
                },
                {
                    "start": 482,
                    "end": 577
                },
                {
                    "start": 580,
                    "end": 688
                },
                {
                    "start": 689,
                    "end": 788
                },
                {
                    "start": 789,
                    "end": 878
                },
                {
                    "start": 879,
                    "end": 913
                },
                {
                    "start": 916,
                    "end": 957
                },
                {
                    "start": 958,
                    "end": 1070
                },
                {
                    "start": 1071,
                    "end": 1203
                },
                {
                    "start": 1204,
                    "end": 1307
                },
                {
                    "start": 1308,
                    "end": 1371
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.47265625
        },
        {
            "corpus_id": "273350592",
            "title": "AlphaPruning: Using Heavy-Tailed Self Regularization Theory for Improved Layer-wise Pruning of Large Language Models",
            "text": "Models and Evaluation. We evaluate AlphaPruning on the three most widely adopted LLM model families: LLaMA 7B/13B/30B/65B (Touvron et al., 2023a), LLaMA-2 7B/13B/70B (Touvron et al., 2023b), OPT 125M/350M/2.7B/6.7B, and other advanced LLMs: LLaMA-3-8B, Vicuna-7B, Mistral-7B. Our evaluation protocol aligns with established methodologies for LLM pruning (Xiao et al., 2023a), including assessments of language modeling proficiency and zero-shot capabilities. Specifically, we evaluate the perplexity on the held-out WikiText (Merity et al., 2016) validation set, and use seven tasks, including BoolQ (Clark et al., 2019), RTE (Wang et al., 2018), HellaSwag (Zellers et al., 2019), WinoGrande (Sakaguchi et al., 2021), ARC Easy and Challenge (Clark et al., 2018) and OpenbookQA (Mihaylov et al., 2018) for downstream zero-shot evaluation (Gao et al., 2023). \n\nBaselines. We apply the layer-wise sparsities determined by AlphaPruning to three LLM pruning methods, including Magnitude (Han et al., 2015), SparseGPT (Frantar and Alistarh, 2023b) and Wanda (Sun et al., 2023). Magnitude-based pruning is a simple and strong baseline in which weights are discarded based on their magnitudes. Wanda and SparseGPT are two strong LLM pruning baselines due to their capability to sustain reasonable performance even at relatively high sparsity levels (around 50%). All these methods originally use uniform layerwise sparsity. We incorporate AlphaPruning directly into these baselines, and we demonstrate that this results in improved performance.",
            "score": 0.6367641646780169,
            "section_title": "Experimental setup",
            "char_start_offset": 21303,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 22
                },
                {
                    "start": 23,
                    "end": 215
                },
                {
                    "start": 216,
                    "end": 275
                },
                {
                    "start": 276,
                    "end": 458
                },
                {
                    "start": 459,
                    "end": 856
                },
                {
                    "start": 859,
                    "end": 869
                },
                {
                    "start": 870,
                    "end": 1071
                },
                {
                    "start": 1072,
                    "end": 1185
                },
                {
                    "start": 1186,
                    "end": 1354
                },
                {
                    "start": 1355,
                    "end": 1415
                },
                {
                    "start": 1416,
                    "end": 1536
                }
            ],
            "ref_mentions": [
                {
                    "start": 354,
                    "end": 374,
                    "matchedPaperCorpusId": "253708271"
                },
                {
                    "start": 692,
                    "end": 716,
                    "matchedPaperCorpusId": "199370376"
                },
                {
                    "start": 982,
                    "end": 1000,
                    "matchedPaperCorpusId": "2238772"
                },
                {
                    "start": 1012,
                    "end": 1041,
                    "matchedPaperCorpusId": "255372747"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.6953125
        },
        {
            "corpus_id": "272550518",
            "title": "STUN: Structured-Then-Unstructured Pruning for Scalable MoE Pruning",
            "text": "LLM pruning can be classified into unstructured and structured pruning (Behnke and Heafield, 2021). Unstructured pruning involves finding mask tensors to sparsify weight tensors. SparseGPT (Frantar and Alistarh, 2023) uses the Hessian matrix for second-order Taylor approximation, while GBLM-Pruner (Das et al., 2024) and Pruner-Zero (Dong et al., 2024) leverage gradients to identify mask tensors. However, as these methods demand substantial GPU memory for LLMs, we consider two recent works with higher memory efficiency and strong performance as our baselines: Wanda (Sun et al., 2024) evaluates the importance of neurons in each layer by its weight multiplied by the activation value, removing those with low scores. While Wanda assumes a uniform pruning ratio across layers, OWL (Yin et al., 2024) probes the optimal pruning ratio per layer, given the pruning budget. \n\nStructured pruning, on the other hand, imposes constraints on the sparsification pattern, such as removing rows, columns, or even entire weight tensors. Early methods that involve pruning attention heads (Voita et al., 2019;Shim et al., 2021;Zhang et al., 2021), rows (Gong et al., 2022), entire dense layers (Liang et al., 2021), or whole transformer blocks (Fan et al., 2019;Li et al., 2020) fall under this category. Recent works have applied structured pruning for LLMs (Ma et al., 2023;Cheng et al., 2024;Gao et al., 2024;Zhang et al., 2024;Dery et al., 2024), but without fine-tuning, these methods generally underperform compared to unstructured pruning. \n\nOur distinction is to introduce a new class of pruning-structured-then-unstructured pruning-and demonstrate its significant advantages for MoEs, surpassing the performance of unstructuredonly pruning.",
            "score": 0.6360428199822477,
            "section_title": "LLM Pruning",
            "char_start_offset": 5168,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 99
                },
                {
                    "start": 100,
                    "end": 178
                },
                {
                    "start": 179,
                    "end": 398
                },
                {
                    "start": 399,
                    "end": 721
                },
                {
                    "start": 722,
                    "end": 873
                },
                {
                    "start": 876,
                    "end": 1028
                },
                {
                    "start": 1029,
                    "end": 1295
                },
                {
                    "start": 1296,
                    "end": 1537
                },
                {
                    "start": 1540,
                    "end": 1740
                }
            ],
            "ref_mentions": [
                {
                    "start": 71,
                    "end": 98,
                    "matchedPaperCorpusId": "245855868"
                },
                {
                    "start": 189,
                    "end": 217,
                    "matchedPaperCorpusId": "255372747"
                },
                {
                    "start": 334,
                    "end": 353,
                    "matchedPaperCorpusId": "270257857"
                },
                {
                    "start": 785,
                    "end": 803,
                    "matchedPaperCorpusId": "263829692"
                },
                {
                    "start": 1080,
                    "end": 1100,
                    "matchedPaperCorpusId": "162183964"
                },
                {
                    "start": 1100,
                    "end": 1118,
                    "matchedPaperCorpusId": "238419078"
                },
                {
                    "start": 1118,
                    "end": 1137,
                    "matchedPaperCorpusId": "226282192"
                },
                {
                    "start": 1185,
                    "end": 1205,
                    "matchedPaperCorpusId": "235186841"
                },
                {
                    "start": 1235,
                    "end": 1253,
                    "matchedPaperCorpusId": "202750230"
                },
                {
                    "start": 1253,
                    "end": 1269,
                    "matchedPaperCorpusId": "221970592"
                },
                {
                    "start": 1350,
                    "end": 1367,
                    "matchedPaperCorpusId": "258823276"
                },
                {
                    "start": 1403,
                    "end": 1422,
                    "matchedPaperCorpusId": "269839498"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.50830078125
        },
        {
            "corpus_id": "267060803",
            "title": "Pruning for Protection: Increasing Jailbreak Resistance in Aligned LLMs Without Fine-Tuning",
            "text": "WANDA is a recently introduced pruning method that is computationally efficient, does not require any finetuning, and maintains good performance. Consider a linear layer W \u2208 R Cout\u00d7C in , and a batched input X \u2208 R T \u00d7C in . In LLMs, T = N \u2022 L represents the total token count, where N is the batch size and L is the sequence length. WANDA assigns an importance score for each weight \n\nwhere \u2225X j \u2225 2 is the l 2 norm of X[:, j]. They consider an output index i and construct the sets of all weights connecting into i: {W uv | u = i}. Finally, they remove all the lowest s% connections in each group where s% is the target sparsity. Consistent with the previous findings, our experiments with WANDA pruning revealed regularizing effects at sparsity levels up to 20-30%, while higher sparsity levels began to degrade performance. In this work, we focus on how compression affects a different-and currently under-explored-dimension of LLM performance: resilience to adversarial attacks on safety alignment. We demonstrate that, in certain cases, WANDA pruning appears to improve model performance, similar to how low-rank factorization benefits reasoning tasks, and contrary to some evaluations where WANDA pruning negatively impacts truthfulness metrics.",
            "score": 0.6359630889861857,
            "section_title": "WANDA Pruning",
            "char_start_offset": 9074,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 145
                },
                {
                    "start": 146,
                    "end": 223
                },
                {
                    "start": 224,
                    "end": 332
                },
                {
                    "start": 333,
                    "end": 382
                },
                {
                    "start": 385,
                    "end": 427
                },
                {
                    "start": 428,
                    "end": 532
                },
                {
                    "start": 533,
                    "end": 630
                },
                {
                    "start": 631,
                    "end": 826
                },
                {
                    "start": 827,
                    "end": 1002
                },
                {
                    "start": 1003,
                    "end": 1251
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.6884765625
        },
        {
            "corpus_id": "269188009",
            "title": "Shears: Unstructured Sparsity with Neural Low-rank Adapter Search",
            "text": "Pruning the weights of a neural network is a popular technique for model compression.The most common approach of element-wise pruning uses the magnitude of the weights and a thresholding function that zeroes out the weights below a threshold.Weight magnitude pruning is ineffective when applied to LLMs (Frantar and Alistarh, 2023).One possible reason is the existence of outlier features when models reach several billion parameters (Dettmers et al., 2022).Alternative approaches have been proposed to measure the importance of the weights.For instance, first-order approaches use several iterations to update the weights, e.g., Movement Pruning (Sanh et al., 2020) and SparseGPT (Frantar and Alistarh, 2023).These approaches have also improved weight-sharing NAS (Mu\u00f1oz et al., 2024b).Unfortunately, using weight updates for LLM pruning requires a significant computational cost.Recently, efficient approaches have been proposed to achieve high degrees of sparsity with a single forward pass of N samples.For example, Wanda (Sun et al., 2023) is a simple but effective sparsification method that determines which parameters to zero out by the importance of weights based on both the weights and the activations.LLM-Pruner (Ma et al., 2023) is proposed to compress LLMs in a task-agnostic manner (Ma et al., 2023).This approach produces good zero-shot results after applying structured pruning on the targe LLM.Unlike these approaches, Shears is designed for specific task fine-tuning scenarios, which can obtain higher levels of unstructured sparsity while improving or with minor drops in accuracy by combining unstructured sparsity with neural low-rank adapter search (NLS).\n\nSparsity and Fine-Tuning SparseFT (Kurtic et al., 2023) uses SparseGPT (Frantar and Alistarh, 2023) to sparsify the model and then fine-tunes all the weights of the model using a novel knowledge distillation technique (see section 4.3).",
            "score": 0.6353086314162134,
            "section_title": "Sparsity and Pruning",
            "char_start_offset": 19757,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 85
                },
                {
                    "start": 85,
                    "end": 242
                },
                {
                    "start": 242,
                    "end": 332
                },
                {
                    "start": 332,
                    "end": 458
                },
                {
                    "start": 458,
                    "end": 541
                },
                {
                    "start": 541,
                    "end": 710
                },
                {
                    "start": 710,
                    "end": 787
                },
                {
                    "start": 787,
                    "end": 881
                },
                {
                    "start": 881,
                    "end": 1007
                },
                {
                    "start": 1007,
                    "end": 1213
                },
                {
                    "start": 1213,
                    "end": 1315
                },
                {
                    "start": 1315,
                    "end": 1412
                },
                {
                    "start": 1412,
                    "end": 1678
                },
                {
                    "start": 1680,
                    "end": 1916
                }
            ],
            "ref_mentions": [
                {
                    "start": 303,
                    "end": 331,
                    "matchedPaperCorpusId": "59599816"
                },
                {
                    "start": 434,
                    "end": 457,
                    "matchedPaperCorpusId": "258509304"
                },
                {
                    "start": 681,
                    "end": 709,
                    "matchedPaperCorpusId": "59599816"
                },
                {
                    "start": 765,
                    "end": 786,
                    "matchedPaperCorpusId": "269804708"
                },
                {
                    "start": 1224,
                    "end": 1241,
                    "matchedPaperCorpusId": "258823276"
                },
                {
                    "start": 1297,
                    "end": 1314,
                    "matchedPaperCorpusId": "258823276"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.436767578125
        },
        {
            "corpus_id": "273345395",
            "title": "Self-Data Distillation for Recovering Quality in Pruned Large Language Models",
            "text": "The advent of large language models (LLMs) such as GPT-4 (OpenAI et al., 2024), Gemini (Gemini et al., 2024), and Llama 3 (Dubey et al., 2024) has revolutionized natural language processing (NLP), driving significant advancements across various tasks through extensive pre-training on textual data. These models, enhanced by supervised fine-tuning (SFT), demonstrate impressive instruction-following abilities (Ouyang et al., 2022;Touvron et al., 2023a), but come with high compute costs for both training and inference (Kaplan et al., 2020;Hoffmann et al., 2022). To address diverse deployment requirements across varying model scales, sizes, and compute budgets, compressing models for efficient inference is essential, particularly given the significant time, data, and resource constraints associated with training multiple multi-billion parameter models from scratch. \n\nMost model compression techniques can be grouped into four main categories: knowledge distillation (KD) (Hinton et al., 2015), factorization (Hu et al., 2022), pruning (Le-Cun et al., 1989), and quantization (Han et al., 2015). In our work, we focus on pruning, though we aim for our method to inspire further developments across these other compression methods. Structured pruning, which selectively removes less critical components of a neural network, has emerged as a promising method for improving LLM efficiency (Ma et al., 2023). This method has gained attention for its ability to reduce memory and compute requirements, making inference more efficient. Recent works have shown that LLMs exhibit significant redundancy, particularly in the middle layers, where removing these layers has a minimal impact on overall model quality (Men et al., 2024;Gromov et al., 2024). The residual stream of the Transformer (Vaswani et al., 2017) architecture is only slightly modified by the output of non-essential layers, enabling the removal of these layers without drastically harming model quality. \n\nDespite its potential advantages, depth-wise structured pruning presents inherent challenges.",
            "score": 0.6350654421571946,
            "section_title": "INTRODUCTION",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 298
                },
                {
                    "start": 299,
                    "end": 564
                },
                {
                    "start": 565,
                    "end": 872
                },
                {
                    "start": 875,
                    "end": 1102
                },
                {
                    "start": 1103,
                    "end": 1237
                },
                {
                    "start": 1238,
                    "end": 1411
                },
                {
                    "start": 1412,
                    "end": 1536
                },
                {
                    "start": 1537,
                    "end": 1751
                },
                {
                    "start": 1752,
                    "end": 1971
                },
                {
                    "start": 1974,
                    "end": 2067
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.68603515625
        },
        {
            "corpus_id": "277622258",
            "title": "Entropy-Based Block Pruning for Efficient Large Language Models",
            "text": "E-Sparse (Li et al., 2023) introduces an entropy-based pruning method that enhances inference speed and reduces memory usage in large language models by leveraging information rich- ness to guide N:M sparsity. SPP (Lu et al., 2024b) designs an efficient fine-tuning method to recover model performance post-pruning while maintaining sparsity. Beyond parameter pruning, structural pruning of LLMs has also gained popularity. LLM-Pruner (Ma et al., 2023) and ShearedLLaMA (Xia et al., 2023) remove unimportant structures such as layers and attention heads. Additionally, (Lu et al., 2024a) finds that certain experts in mixture-ofexperts (MoE) LLMs can also be pruned. Among structural pruning methods, layer pruning is particularly relevant. Laco (Yang et al., 2024) reduces model depth by merging adjacent layers from the topmost layer downward. ShortGPT (Men et al., 2024) prunes unimportant layers based on a cosine similarity criterion. LLMDrop (He et al., 2024) finds that attention layers are more redundant than MLP layers but also relies on cosine similarity for pruning. Different from these approaches, in this paper, we propose a more effective criterion i.e. En-tropy Increase to identify and remove unimportant layers.",
            "score": 0.6348737527819102,
            "section_title": "Speedup Test",
            "char_start_offset": 20929,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 209
                },
                {
                    "start": 210,
                    "end": 342
                },
                {
                    "start": 343,
                    "end": 554
                },
                {
                    "start": 555,
                    "end": 666
                },
                {
                    "start": 667,
                    "end": 740
                },
                {
                    "start": 741,
                    "end": 845
                },
                {
                    "start": 846,
                    "end": 939
                },
                {
                    "start": 940,
                    "end": 1078
                },
                {
                    "start": 1079,
                    "end": 1169
                },
                {
                    "start": 1170,
                    "end": 1230
                }
            ],
            "ref_mentions": [
                {
                    "start": 746,
                    "end": 765,
                    "matchedPaperCorpusId": "267751181"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.810546875
        },
        {
            "corpus_id": "276576138",
            "title": "Systematic Weight Evaluation for Pruning Large Language Models: Enhancing Performance and Sustainability",
            "text": "Magnitude pruning (Han et al., 2015) is a standard technique to induce sparsity in neural networks by removing individual weights based on their magnitudes, typically determined either locally within each layer or globally across the entire network. Despite its simplicity, it has been effective in finding extremely sparse networks (Frankle and Carbin, 2019) and is considered a strong baseline approach (Blalock et al., 2020) for neural network sparsification. Dettmers et al. (Dettmers et al., 2022) observed emergent large magnitude features in Transformer-based large language models (LLMs), noting that when LLMs reach around 6B parameters, a small set of hidden state features emerges with significantly larger magnitudes than others, which are crucial for predictive performance. In the context of compressing recent LLMs, methods like LLM-Pruner (Ma et al., 2023) and FLAP (An et al., 2024) narrow network width by pruning coupled structures, while Sheared-LLaMA (Xia et al., 2023) reduces both network width and depth by removing entire layers. Although pruning methods that incorporate both width and depth aspects exist (Xia et al., 2022;Kurti\u0107 et al., 2024), there remains a need for detailed analysis comparing these factors' impact on LLM inference efficiency. Traditional pruning in Deep Neural Networks (DNNs) faces unique challenges when applied to LLMs, which have a large number of parameters and require significant computational resources (Brown et al., 2020). Various pruning methods for LLMs fall into unstructured and structured categories. Unstructured pruning methods (Dong et al., 2017;Chen et al., 2020Chen et al., , 2021a) ) set unimportant individual weights to zero, maintaining performance but resulting in sparse weight matrices that are less hardwareefficient. Methods like SparseGPT (Frantar and Alistarh, 2023) and Wanda (Sun et al., 2023) use sophisticated weight updates and pruning without retraining, respectively, while PST (Li et al., 2022) combines unstructured pruning with efficient fine-tuning.",
            "score": 0.6346268747231268,
            "section_title": "Related Work",
            "char_start_offset": 3759,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 249
                },
                {
                    "start": 250,
                    "end": 462
                },
                {
                    "start": 463,
                    "end": 787
                },
                {
                    "start": 788,
                    "end": 1054
                },
                {
                    "start": 1055,
                    "end": 1275
                },
                {
                    "start": 1276,
                    "end": 1482
                },
                {
                    "start": 1483,
                    "end": 1565
                },
                {
                    "start": 1566,
                    "end": 1795
                },
                {
                    "start": 1796,
                    "end": 2041
                }
            ],
            "ref_mentions": [
                {
                    "start": 18,
                    "end": 36,
                    "matchedPaperCorpusId": "2238772"
                },
                {
                    "start": 333,
                    "end": 359,
                    "matchedPaperCorpusId": "53388625"
                },
                {
                    "start": 479,
                    "end": 502,
                    "matchedPaperCorpusId": "251564521"
                },
                {
                    "start": 855,
                    "end": 872,
                    "matchedPaperCorpusId": "258823276"
                },
                {
                    "start": 882,
                    "end": 899,
                    "matchedPaperCorpusId": "266362404"
                },
                {
                    "start": 1150,
                    "end": 1170,
                    "matchedPaperCorpusId": "256662263"
                },
                {
                    "start": 1461,
                    "end": 1481,
                    "matchedPaperCorpusId": "218971783"
                },
                {
                    "start": 1614,
                    "end": 1631,
                    "matchedPaperCorpusId": "222297215"
                },
                {
                    "start": 1631,
                    "end": 1654,
                    "matchedPaperCorpusId": "234292017"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.67626953125
        },
        {
            "corpus_id": "271710591",
            "title": "STBLLM: Breaking the 1-Bit Barrier with Structured Binary LLMs",
            "text": "Sparsity Methods for LLM. Pruning removes less important parameters from a neural network to reduce its size and improve efficiency. For LLMs, Pruning can be divided to structured pruning (Ma et al., 2023;Ashkboos et al., 2024;Xia et al., 2024;An et al., 2023), semi-structured pruning (Frantar & Alistarh, 2023;Sun et al., 2024;Zhang et al., 2024b) and unstructured pruning (Frantar & Alistarh, 2023;Sun et al., 2024;Dong et al., 2024). Structured pruning methods, including LLM-Pruner (Ma et al., 2023) and Sheared LLaMA (Xia et al., 2024), aim to simplify LLM by removing specific components such as heads, layers, and dimensions. Although these techniques enhance model efficiency, they often result in significant performance degradation and require extensive retraining to recover lost capabilities. In contrast, unstructured pruning methods (Frantar & Alistarh, 2023;Sun et al., 2024) remove individual weights based on their significance within the model. However, this approach leads to irregular sparsity patterns that do not effectively leverage hardware acceleration. Semi-structured pruning offers a more balanced approach to model optimization. Methods such as SparseGPT (Frantar & Alistarh, 2023) and Wanda (Sun et al., 2024) exemplify this strategy by maintaining regular, hardware-friendly sparsity patterns, such as N:M sparsity. This approach combines the fine-grained control characteristic of unstructured pruning with the operational efficiency associated with structured pruning. \n\nSynergy of Pruning and Quantization. The complementary nature of pruning and quantization has been extensively explored in the literature, where pruning reduces the number of parameters in a neural network, and quantization focuses on the precision of those parameters.",
            "score": 0.6330717548853655,
            "section_title": "RELATED WORK",
            "char_start_offset": 8460,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 25
                },
                {
                    "start": 26,
                    "end": 132
                },
                {
                    "start": 133,
                    "end": 437
                },
                {
                    "start": 438,
                    "end": 633
                },
                {
                    "start": 634,
                    "end": 805
                },
                {
                    "start": 806,
                    "end": 963
                },
                {
                    "start": 964,
                    "end": 1079
                },
                {
                    "start": 1080,
                    "end": 1158
                },
                {
                    "start": 1159,
                    "end": 1347
                },
                {
                    "start": 1348,
                    "end": 1502
                },
                {
                    "start": 1505,
                    "end": 1541
                },
                {
                    "start": 1542,
                    "end": 1774
                }
            ],
            "ref_mentions": [
                {
                    "start": 244,
                    "end": 260,
                    "matchedPaperCorpusId": "266362404"
                },
                {
                    "start": 418,
                    "end": 436,
                    "matchedPaperCorpusId": "270257857"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.49169921875
        },
        {
            "corpus_id": "263829692",
            "title": "Outlier Weighed Layerwise Sparsity (OWL): A Missing Secret Sauce for Pruning LLMs to High Sparsity",
            "text": "It adopts an iterative strategy to handle the computational hurdle posed by the row-Hessian problem. Specifically, it employs the Optimal Brain Surgeon (OBS) algorithm (Hassibi et al., 1993) to selectively prune and update weights in a column-wise manner. Wanda (Sun et al., 2023), on the other hand, introduces a novel pruning metric that takes into account both the weight magnitudes and their corresponding input activations. Remarkably, it achieves performance on par with SparseGPT without relying on computationally expensive second-order information. The effectiveness of Wanda stems from the emergence of the outlier features residing within LLMs. These outliers, which tend to be significantly larger than typical features, are nonetheless crucial for optimizing LLM performance (Kovaleva et al., 2021;Puccetti et al., 2022;Timkey & van Schijndel, 2021;Dettmers et al., 2022). Both SparseGPT and Wanda exhibit appealing performance, showcasing their ability to reduce model parameters by up to 50% while incurring only a modest increase of perplexity (Sun et al., 2023). \n\nIt is worth noting that SparseGPT and Wanda unanimously follow previous work on BERT pruning (Sanh et al., 2020;Kurtic et al., 2022) and choose to prune LLMs with a uniform sparsity ratio per layer, i.e., each layer will be pruned at the same sparsity. Such choice is reasonable for LLMs, as the pruning process typically involves sorting the importance scores of weights. Conducting such sorting globally across layers could become a computational bottleneck, especially for models at the billion-parameter scale. Nevertheless, before it has been taken root that uniform layerwise sparsity is the default choice for LLMs, we raise a timely inquiry: are there any pivotal aspects that have been inadvertently omitted in the context of favorable layerwise sparsity ratios for LLM pruning?",
            "score": 0.6328053532737273,
            "section_title": "Introduction",
            "char_start_offset": 1953,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 100
                },
                {
                    "start": 101,
                    "end": 255
                },
                {
                    "start": 256,
                    "end": 428
                },
                {
                    "start": 429,
                    "end": 557
                },
                {
                    "start": 558,
                    "end": 655
                },
                {
                    "start": 656,
                    "end": 885
                },
                {
                    "start": 886,
                    "end": 1079
                },
                {
                    "start": 1082,
                    "end": 1334
                },
                {
                    "start": 1335,
                    "end": 1454
                },
                {
                    "start": 1455,
                    "end": 1596
                },
                {
                    "start": 1597,
                    "end": 1869
                }
            ],
            "ref_mentions": [
                {
                    "start": 168,
                    "end": 190,
                    "matchedPaperCorpusId": "61815367"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.58740234375
        },
        {
            "corpus_id": "271064490",
            "title": "Composable Interventions for Language Models",
            "text": "We use four state-of-the-art compression methods including two pruning methods: \n\n\u2022 SparseGPT (Frantar & Alistarh, 2023): an efficient one-shot pruning method tailored for large models. It converts the pruning process into solving large-scale sparse regression problems using an approximate solver. This approach enables rapid pruning on a single GPU with minimal accuracy loss, achieving 50-60% on large models. \n\n\u2022 Wanda (Sun et al., 2023): is another popular method for pruning large language models that relies on a pruning metric that combines a weight's magnitude with the norm of its corresponding input activations, determined from a small calibration dataset. The method focuses on selectively pruning weights within individual outputs of linear layers, aiming for high sparsity levels without modifying unpruned weights. Wanda is computationally efficient, executable in a single forward pass. \n\nand two quantization methods: \n\n\u2022 GPTQ (Frantar et al., 2023): an algorithm designed for efficient weight quantization in large-scale models. It revises the weight quantization approach by quantizing weights in a fixed order rather than a greedy order, which shows minimal performance difference, especially in larger models. GPTQ introduced a novel method where each weight is quantized column-by-column, reducing computational complexity. \n\n\u2022 AWQ (Lin et al., 2023): is based on the premise that not all weights are equally critical for model performance, and it identifies a small fraction of salient weights whose quantization significantly impacts model accuracy. This identification is done by analyzing activation distributions rather than weight distributions, under the rationale that weights linked to larger activation magnitudes are more crucial.",
            "score": 0.63235239778715,
            "section_title": "C.3.2 COMPRESSOR DETAILS",
            "char_start_offset": 35862,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 79
                },
                {
                    "start": 82,
                    "end": 185
                },
                {
                    "start": 186,
                    "end": 298
                },
                {
                    "start": 299,
                    "end": 412
                },
                {
                    "start": 415,
                    "end": 668
                },
                {
                    "start": 669,
                    "end": 830
                },
                {
                    "start": 831,
                    "end": 903
                },
                {
                    "start": 906,
                    "end": 935
                },
                {
                    "start": 938,
                    "end": 1047
                },
                {
                    "start": 1048,
                    "end": 1231
                },
                {
                    "start": 1232,
                    "end": 1346
                },
                {
                    "start": 1349,
                    "end": 1574
                },
                {
                    "start": 1575,
                    "end": 1764
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.87060546875
        },
        {
            "corpus_id": "276928323",
            "title": "SEAP: Training-free Sparse Expert Activation Pruning Unlock the Brainpower of Large Language Models",
            "text": "Large Language Models have achieved remarkable success across various natural language processing tasks, yet their high computational cost during inference remains a major bottleneck. This paper introduces Sparse Expert Activation Pruning (SEAP), a training-free pruning method that selectively retains task-relevant parameters to reduce inference overhead. Inspired by the clustering patterns of hidden states and activations in LLMs, SEAP identifies task-specific expert activation patterns and prunes the model while preserving task performance and enhancing computational efficiency. Experimental results demonstrate that SEAP significantly reduces computational overhead while maintaining competitive accuracy. Notably, at 50% pruning, SEAP surpasses both WandA and FLAP by over 20%, and at 20% pruning, it incurs only a 2.2% performance drop compared to the dense model. These findings highlight SEAP's scalability and effectiveness, making it a promising approach for optimizing large-scale LLMs.",
            "score": 0.6314403861584473,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.853515625
        },
        {
            "corpus_id": "273186571",
            "title": "Large Language Model Inference Acceleration: A Comprehensive Hardware Perspective",
            "text": "Weight Sparsity. LLM-pruner [132], adopts structural pruning that selectively removes non-critical coupled structures based on gradient information, maximally preserving the majority of the LLM's functionality. To this end, the performance of pruned models can be efficiently recovered through tuning techniques, LoRA, in merely 3 hours, requiring only 50K data. We validate the LLM-Pruner on three LLMs, including Llama, Vicuna, and ChatGLM, and demonstrate that the compressed models still exhibit satisfactory capabilities in zero-shot classification and generation. \n\nLLMs can be pruned to at least 50% sparsity in one-shot, without any retraining, at minimal loss of accuracy. \n\nSparseGPT [133] requires a sophisticated weight update procedure in an iterative pruning process. Wanda [134] prunes weights with the smallest magnitudes multiplied by the corresponding input activations, on a per-output basis. Notably, Wanda requires no retraining or weight update, where pruning process is faster. Besides unstructured pattern, these two methods generalizes to semi-structured N:M (2:4 and 4:8) patterns. E-Sparse [135] introduces entropy to quantify the information richness within each channel (intra-channel) of the input features, and adopts it to enhance the feature norms (crosschannel) as a metric to evaluate parameter importance. Furthermore, it proposes Channel Shuffle to reorder the information distribution in LLMs to obtain N:M Sparsity with less information loss. 2:4 sparsity as supported by NVIDIA GPUs of generation Ampere and newer theoretically offers 2\u00d7 acceleration of matrix multiplications. In practical, 2:4 sparsity can achieve 1.54\u00d7-1.79\u00d7 speedup for MatMul, and end-to-end speedups are about 1.21\u00d7-1.25\u00d7 (due to some extra overheads from e.g. attention). \n\nBased on the key observation that the bottleneck of LLM inference is the skinny matrix multiplications, Flash-LLM [136] proposes a general Load-as-Sparse and Compute-as-Dense methodology for unstructured sparse matrix multiplication.",
            "score": 0.6305784659208755,
            "section_title": "GPU",
            "char_start_offset": 43297,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 16
                },
                {
                    "start": 17,
                    "end": 210
                },
                {
                    "start": 211,
                    "end": 362
                },
                {
                    "start": 363,
                    "end": 569
                },
                {
                    "start": 572,
                    "end": 681
                },
                {
                    "start": 684,
                    "end": 781
                },
                {
                    "start": 782,
                    "end": 911
                },
                {
                    "start": 912,
                    "end": 1000
                },
                {
                    "start": 1001,
                    "end": 1107
                },
                {
                    "start": 1108,
                    "end": 1341
                },
                {
                    "start": 1342,
                    "end": 1481
                },
                {
                    "start": 1482,
                    "end": 1617
                },
                {
                    "start": 1618,
                    "end": 1668
                },
                {
                    "start": 1669,
                    "end": 1734
                },
                {
                    "start": 1735,
                    "end": 1773
                },
                {
                    "start": 1774,
                    "end": 1785
                },
                {
                    "start": 1788,
                    "end": 2021
                }
            ],
            "ref_mentions": [
                {
                    "start": 28,
                    "end": 33,
                    "matchedPaperCorpusId": "258823276"
                },
                {
                    "start": 694,
                    "end": 699,
                    "matchedPaperCorpusId": "255372747"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.74951171875
        },
        {
            "corpus_id": "276774084",
            "title": "Revisiting Large Language Model Pruning using Neuron Semantic Attribution",
            "text": "In this paper, we explore the generalizability of pruning methods on large language models using three methods-SparseGPT, Wanda, and RIA-across various tasks. \n\nOur findings show that pruning methods reduce model size without significant performance loss, but the choice of calibration dataset, pruning sparsity, and task type influence outcomes. Sentiment classification is more sensitive to pruning methods and calibration datasets, while tasks like question answering and logical reasoning are more robust. Using Neuron Semantic Attribution (NSA), we demonstrate how pruning methods impact neuron activations and model decision-making. Our work provides insights into the taskspecific nature of pruning and lays the foundation for future research to develop more efficient and interpretable pruning strategies. \n\ninherent structure of reasoning tasks. In this dataset, pruning appears to have less impact on the model's ability to focus on critical decision-making tokens, which might explain why the performance drop was less significant compared to tasks like sentiment classification.",
            "score": 0.6301693919813329,
            "section_title": "Conclusion",
            "char_start_offset": 27366,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 158
                },
                {
                    "start": 161,
                    "end": 346
                },
                {
                    "start": 347,
                    "end": 509
                },
                {
                    "start": 510,
                    "end": 638
                },
                {
                    "start": 639,
                    "end": 813
                },
                {
                    "start": 816,
                    "end": 854
                },
                {
                    "start": 855,
                    "end": 1090
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.7578125
        },
        {
            "corpus_id": "267682299",
            "title": "NutePrune: Efficient Progressive Pruning with Numerous Teachers for Large Language Models",
            "text": "Large Language Models (LLMs) excel in language tasks (OpenAI, 2023;Touvron et al., 2023;Thoppilan et al., 2022;Scao et al., 2022), but their substantial size poses deployment and inference challenges (Frantar et al., 2022). Techniques like model pruning (Molchanov et al., 2016), knowledge distillation (Jiao et al., 2019), and quantization (Dettmers    \u2020    et al., 2023) have been proposed to address computational demands. The exploration of LLM pruning, especially structured pruning (Frantar and Alistarh, 2023), holds great significance. Structured pruning reduces model size by removing coherent parameter groups, cutting inference costs on standard hardware. But it is more challenging than unstructured pruning in retaining the capabilities of LLMs (Hoefler et al., 2021). Existing methods either adopt data-efficient approaches, causing a performance decline (Ma et al., 2023), or require extensive posttraining to recover model performance (Xia et al., 2023). In this work, we investigate efficient methods to prune the model to higher sparsity without significant performance decline. \n\nKnowledge distillation (KD) aims to train a more compact student model with supervision from a larger teacher model (Sanh et al., 2019;Gou et al., 2021). It's widely adopted and proven highly effective in the field of LLMs. Progressive learning, utilizing intermediate teachers with a reduced gap in capabilities, has been demonstrated to improve performance in KD (Xiang et al., 2020). Previous work has shown that pruning with a distillation objective can improve performance (Xia et al., 2022). Distillation is particularly suitable for pruning since the full original model inherently serves as an excellent teacher for the pruned model (Sanh et al., 2020), which can offer a more detailed supervisory signal than conventional supervised training, enhancing the effectiveness of pruning with limited data (Lagunas et al., 2021). \n\nHowever, applying this method in the realm of LLMs proves challenging.",
            "score": 0.6299699322752419,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 223
                },
                {
                    "start": 224,
                    "end": 425
                },
                {
                    "start": 426,
                    "end": 543
                },
                {
                    "start": 544,
                    "end": 666
                },
                {
                    "start": 667,
                    "end": 781
                },
                {
                    "start": 782,
                    "end": 970
                },
                {
                    "start": 971,
                    "end": 1096
                },
                {
                    "start": 1099,
                    "end": 1252
                },
                {
                    "start": 1253,
                    "end": 1322
                },
                {
                    "start": 1323,
                    "end": 1485
                },
                {
                    "start": 1486,
                    "end": 1596
                },
                {
                    "start": 1597,
                    "end": 1931
                },
                {
                    "start": 1934,
                    "end": 2004
                }
            ],
            "ref_mentions": [
                {
                    "start": 488,
                    "end": 516,
                    "matchedPaperCorpusId": "255372747"
                },
                {
                    "start": 1234,
                    "end": 1250,
                    "matchedPaperCorpusId": "219559263"
                },
                {
                    "start": 1464,
                    "end": 1484,
                    "matchedPaperCorpusId": "209862398"
                },
                {
                    "start": 1740,
                    "end": 1759,
                    "matchedPaperCorpusId": "218665313"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.63525390625
        },
        {
            "corpus_id": "273375561",
            "title": "MoE-Pruner: Pruning Mixture-of-Experts Large Language Model using the Hints from Its Router",
            "text": "Pruning and Sparsity. Pruning (LeCun et al., 1989;Hassibi et al., 1993;Han et al., 2015) is an important approach for compressing neural networks through eliminating weights (Han et al., 2016) or activations (Rao et al., 2021), yielding sparse networks. It can be mainly classified into two categories based on the granularity: unstructured and structured pruning. \n\nUnstructured pruning such as magnitude pruning (Han et al., 2015;2016) removes individual weights to introduce sparsity while preserving accuracy even at high sparsity. Existing methods either require retraining or fine-tuning the pruned models (Liu et al., 2019) or the whole iterative retraining process (Frankle & Carbin, 2019). However, in the era of LLMs, these methods fail as retraining LLMs demands substantial computational resources. SparseGPT (Frantar & Alistarh, 2023b) and Wanda (Sun et al., 2024) propose efficient post-training pruning method that prunes LLM weights in a layer-wise manner without retraining the model. \n\nStructured pruning eliminates weights as a group, such as channel pruning (He et al., 2017), kernel pruning (Zhong et al., 2022), attention head pruning (Wang et al., 2021), token pruning (Rao et al., 2021), and layer pruning (Elhoushi et al., 2024). Unlike unstructured pruning, it leads to more hardware-friendly, dense blocks of computation, which facilitates acceleration on modern hardware platforms. Some methods explore structured pruning based on sparsity on the structural components of LLMs, such as attention heads (Wang et al., 2021) and FFN channels (Ma et al., 2023). Muralidharan et al. (2024) uses both structured pruning and knowledge distillation to compress LLM models and shows improvement over models trained from scratch.",
            "score": 0.6298614736325834,
            "section_title": "RELATED WORKS",
            "char_start_offset": 21952,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 21
                },
                {
                    "start": 22,
                    "end": 253
                },
                {
                    "start": 254,
                    "end": 364
                },
                {
                    "start": 367,
                    "end": 535
                },
                {
                    "start": 536,
                    "end": 698
                },
                {
                    "start": 699,
                    "end": 810
                },
                {
                    "start": 811,
                    "end": 1001
                },
                {
                    "start": 1004,
                    "end": 1254
                },
                {
                    "start": 1255,
                    "end": 1409
                },
                {
                    "start": 1410,
                    "end": 1585
                },
                {
                    "start": 1586,
                    "end": 1747
                }
            ],
            "ref_mentions": [
                {
                    "start": 30,
                    "end": 50,
                    "matchedPaperCorpusId": "7785881"
                },
                {
                    "start": 50,
                    "end": 71,
                    "matchedPaperCorpusId": "61815367"
                },
                {
                    "start": 71,
                    "end": 88,
                    "matchedPaperCorpusId": "2238772"
                },
                {
                    "start": 174,
                    "end": 192,
                    "matchedPaperCorpusId": "2134321"
                },
                {
                    "start": 208,
                    "end": 226,
                    "matchedPaperCorpusId": "235313562"
                },
                {
                    "start": 414,
                    "end": 432,
                    "matchedPaperCorpusId": "2238772"
                },
                {
                    "start": 612,
                    "end": 630,
                    "matchedPaperCorpusId": "52978527"
                },
                {
                    "start": 821,
                    "end": 848,
                    "matchedPaperCorpusId": "255372747"
                },
                {
                    "start": 859,
                    "end": 877,
                    "matchedPaperCorpusId": "259203115"
                },
                {
                    "start": 1078,
                    "end": 1095,
                    "matchedPaperCorpusId": "20157893"
                },
                {
                    "start": 1112,
                    "end": 1132,
                    "matchedPaperCorpusId": "251647291"
                },
                {
                    "start": 1157,
                    "end": 1176,
                    "matchedPaperCorpusId": "229298088"
                },
                {
                    "start": 1192,
                    "end": 1210,
                    "matchedPaperCorpusId": "235313562"
                },
                {
                    "start": 1530,
                    "end": 1549,
                    "matchedPaperCorpusId": "229298088"
                },
                {
                    "start": 1567,
                    "end": 1584,
                    "matchedPaperCorpusId": "258823276"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.73974609375
        },
        {
            "corpus_id": "276079889",
            "title": "Symmetric Pruning of Large Language Models",
            "text": "Wanda (Sun et al., 2023) introduces a pruning metric that incorporates both weight magnitudes and corresponding input activations, achieving perplexity performance comparable to SparseGPT while surpassing simple magnitude-based pruning. The RIA method (Zhang et al., 2024b) builds on Wanda by considering relative weight importance, offering performance improvements at minimal additional cost. Moreover, DSnoT (Zhang et al., 2023) proposes pruning and regrowing weights based on statistical properties (e.g., mean and variance) in each pruning row, obviating the need for retraining.",
            "score": 0.6294570091373282,
            "section_title": "Related Work",
            "char_start_offset": 5961,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 236
                },
                {
                    "start": 237,
                    "end": 394
                },
                {
                    "start": 395,
                    "end": 584
                }
            ],
            "ref_mentions": [
                {
                    "start": 6,
                    "end": 24,
                    "matchedPaperCorpusId": "259203115"
                },
                {
                    "start": 252,
                    "end": 273,
                    "matchedPaperCorpusId": "271745835"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.85400390625
        },
        {
            "corpus_id": "271909582",
            "title": "LLM-Barber: Block-Aware Rebuilder for Sparsity Mask in One-Shot for Large Language Models",
            "text": "Large language models (LLMs) have grown significantly in scale, leading to a critical need for efficient model pruning techniques. Existing post-training pruning techniques primarily focus on measuring weight importance on converged dense models to determine salient weights to retain. However, they often overlook the changes in weight importance during the pruning process, which can lead to performance degradation in the pruned models. To address this issue, we present LLM-Barber (Block-Aware Rebuilder for Sparsity Mask in One-Shot), a novel one-shot pruning framework that rebuilds the sparsity mask of pruned models without any retraining or weight reconstruction. LLM-Barber incorporates block-aware error optimization across Self-Attention and MLP blocks, ensuring global performance optimization. Inspired by the recent discovery of prominent outliers in LLMs, LLM-Barber introduces an innovative pruning metric that identifies weight importance using weights multiplied by gradients. Our experiments show that LLM-Barber can efficiently prune models like LLaMA and OPT families with 7B to 13B parameters on a single A100 GPU in just 30 minutes, achieving state-of-the-art results in both perplexity and zero-shot performance across various language benchmarks. Code is available at https://github.com/YupengSu/LLM-Barber.",
            "score": 0.6291434891459331,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.83203125
        },
        {
            "corpus_id": "258823276",
            "title": "LLM-Pruner: On the Structural Pruning of Large Language Models",
            "text": "Foundation Large Language Model. To showcase the effectiveness and versatility of LLM-Pruner, we test it over three open-source large language models with two kinds of structure: LLaMA-7B [49], Vicuna-7B [4] 2 and ChatGLM-6B [69]. \n\nEvaluation and Datasets. To assess the performance of the model in the task-agnostic setting, we follow LLaMa's evaluation to perform zero-shot task classification on common sense reasoning datasets: BoolQ [6], PIQA [2], HellaSwag [68], WinoGrande [41], ARC-easy [7], ARC-challenge [7] and OpenbookQA [36]. Follow [14], the model ranks the choices in the multiple choice tasks or generates the answer in the open-ended generation 3 . Additionally, we complement our evaluation with a zero-shot perplexity (PPL) analysis on WikiText2 [35] and PTB [33]. \n\nTable 1: Zero-shot performance of the compressed LLaMA-7B. The average is calculated among seven classification datasets. 'Underline' indicates the best pruning-only performance, while 'bold' represents the overall best performance with the same pruning ratio, considering both pruning and post-training. The 'Channel' strategy only prunes the dependent group of Type C, while all other methods employ the 'Block' strategy to prune dependent groups in both Type A and Type B. Since [49] did not provide its prompt, the evaluation of the result with \u22c6 is performed under different prompts, which is lower than the official results. Implementation Details. In the model pruning process, we use 10 randomly selected samples from Bookcorpus [70], each truncated to a sequence length of 128, as the calibration samples for establishing dependency and calculating the gradient for both LLaMA and Vicuna. For ChatGLM, we select 10 random samples from DailyDialog [27]. During the recovery phase, we utilize the cleaned version of Alpaca [47], which comprises approximately 50k samples. Remarkably, tuning these samples requires merely 3 hours on a single GPU with only 2 epochs.",
            "score": 0.6290916938214176,
            "section_title": "Experimental Settings",
            "char_start_offset": 14180,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 32
                },
                {
                    "start": 33,
                    "end": 230
                },
                {
                    "start": 233,
                    "end": 257
                },
                {
                    "start": 258,
                    "end": 539
                },
                {
                    "start": 540,
                    "end": 666
                },
                {
                    "start": 667,
                    "end": 784
                },
                {
                    "start": 787,
                    "end": 845
                },
                {
                    "start": 846,
                    "end": 908
                },
                {
                    "start": 909,
                    "end": 1091
                },
                {
                    "start": 1092,
                    "end": 1417
                },
                {
                    "start": 1418,
                    "end": 1441
                },
                {
                    "start": 1442,
                    "end": 1684
                },
                {
                    "start": 1685,
                    "end": 1748
                },
                {
                    "start": 1749,
                    "end": 1865
                },
                {
                    "start": 1866,
                    "end": 1958
                }
            ],
            "ref_mentions": [
                {
                    "start": 439,
                    "end": 442,
                    "matchedPaperCorpusId": "165163607"
                },
                {
                    "start": 449,
                    "end": 452,
                    "matchedPaperCorpusId": "208290939"
                },
                {
                    "start": 464,
                    "end": 468,
                    "matchedPaperCorpusId": "159041722"
                },
                {
                    "start": 534,
                    "end": 538,
                    "matchedPaperCorpusId": "52183757"
                },
                {
                    "start": 779,
                    "end": 783,
                    "matchedPaperCorpusId": "252796"
                },
                {
                    "start": 1524,
                    "end": 1528,
                    "matchedPaperCorpusId": "6866988"
                },
                {
                    "start": 1743,
                    "end": 1747,
                    "matchedPaperCorpusId": "11267601"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.5751953125
        },
        {
            "corpus_id": "271909421",
            "title": "Enhancing One-shot Pruned Pre-trained Language Models through Sparse-Dense-Sparse Mechanism",
            "text": "Pre-trained language models (PLMs) are engineered to be robust in contextual understanding and exhibit outstanding performance in various natural language processing tasks. However, their considerable size incurs significant computational and storage costs. Modern pruning strategies employ one-shot techniques to compress PLMs without the need for retraining on task-specific or otherwise general data; however, these approaches often lead to an indispensable reduction in performance. In this paper, we propose SDS, a Sparse-Dense-Sparse pruning framework to enhance the performance of the pruned PLMs from a weight distribution optimization perspective. We outline the pruning process in three steps. Initially, we prune less critical connections in the model using conventional one-shot pruning methods. Next, we reconstruct a dense model featuring a pruning-friendly weight distribution by reactivating pruned connections with sparse regularization. Finally, we perform a second pruning round, yielding a superior pruned model compared to the initial pruning. Experimental results demonstrate that SDS outperforms the state-of-the-art pruning techniques SparseGPT and Wanda under an identical sparsity configuration. For instance, SDS reduces perplexity by 9.13 on Raw-Wikitext2 and improves accuracy by an average of 2.05% across multiple zero-shot benchmarks for OPT-125M with 2:4 sparsity.",
            "score": 0.6284655908451307,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.79443359375
        },
        {
            "corpus_id": "271217883",
            "title": "MINI-LLM: Memory-Efficient Structured Pruning for Large Language Models",
            "text": "In addition, at a 10% prune ratio on OPT-6.7B,MINI-LLM achieves an average classification accuracy of 67.81% across four datasets and obtains 98.60% of the accuracy achieved by the original model, which is even better than LLM-Pruner' s 67.50% and 98.15%.This demonstration validates the effectiveness of MINI-LLM in efficiently compressing models of various structures to a specified size, while optimizing memory usage.\n\nIn addition, we observe that the pruning outcomes achieved by gradient-free methods such as Wanda and magnitude l1/l2 shown in Table 2 significantly fell short in comparison to gradient-based pruning methods such as LLM-Pruner and MINI-LLM at a prune ratio of 30% on the WikiText2 and PTB datasets for BLOOM and OPT.Using LLM-Pruner as a high-quality benchmark, we compare Wanda, representing gradient-free approaches, by assessing the similarity of their retained channels per layer against LLM-Pruner on the Wiki-Text2 dataset.Similarly, we evaluate the similarity between LLM-Pruner and MINI-LLM.Specifically, the similarity is calculated by the formula: ||Intersection(A, B)|| 0 /||A|| 0 \u00d7 100%, where A and B denote the sets of the pruned channels obtained by LLM-Pruner and the examined method, respectively.The results are illustrated in Figure 2. We can see that LLM-Pruner and MINI-LLM have more similar pruned channels compared to LLM-Pruner and Wanda.As a result, compared to gradient-free methods, the perplexity of MINI-LLM in Table 2 is closer to the results of LLM-Pruner.\n\nZero-shot Performance on LLaMA-13B Due to the efficient approximation for the gradients of the pre-trained weights, MINI-LLM enables pruning on larger-scale LLMs, such as LLaMA-13B 3 .",
            "score": 0.6281496297790881,
            "section_title": "Main Results",
            "char_start_offset": 20267,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 46
                },
                {
                    "start": 46,
                    "end": 255
                },
                {
                    "start": 255,
                    "end": 421
                },
                {
                    "start": 423,
                    "end": 739
                },
                {
                    "start": 739,
                    "end": 952
                },
                {
                    "start": 952,
                    "end": 1022
                },
                {
                    "start": 1022,
                    "end": 1237
                },
                {
                    "start": 1237,
                    "end": 1385
                },
                {
                    "start": 1385,
                    "end": 1510
                },
                {
                    "start": 1512,
                    "end": 1696
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.447509765625
        },
        {
            "corpus_id": "270257857",
            "title": "Pruner-Zero: Evolving Symbolic Pruning Metric from scratch for Large Language Models",
            "text": "SparseGPT (Frantar & Alistarh, 2023) is the first post-training quantization method that performs unstructured pruning using an approximated Hessian matrix.Wanda (Sun et al., 2024) further simplifies the Hessian matrix by using just the weight and l 2 norm of activation.GBLM-Pruner (Das et al., 2023) further introduces the gradient to boost the performance.Plug-and-play (Zhang et al., 2024b) The paper presents a plug-and-play post-training pruning method for large language models (LLMs) that introduces two innovative components: Relative Importance and Activations (RIA), a new pruning metric, and Channel Permutation, a technique to maximize the preservation of important weights under N:M sparsity constraints.The proposed method, named plug-and-play, outperforms existing pruning techniques and achieves practical speed-up on specific hardware without the need for additional fine-tuning or retraining.PERP (Zimmer et al., 2023) uses Low-rank adaptation to mitigate the expense of the retraining process in the original prune-retrain paradigm.NutePrune (Li et al., 2024b) combines structure pruning with progressive knowledge distillation by utilizing the unpruned model as a teacher and the pruned model as a student.OWL (Yin et al., 2024) proposed outliers metric to re-assign the sparsity of different layers.BESA (Xu et al., 2024) proposes to use parameter-efficient sparsity learning to learn the sparsity ratio in a differentiable manner.GRAIN (Yang et al., 2022) utilizes gradient information to prune intra-attention structures, Find the optimal proxy that can measure the vit-based architectures\n\nFind the optimal metric that can better rank candidate bitwidth configurations\n\nFind the optimal symbolic pruning metric that can measure the importance of different weights Table 9. Impact of Pruning Metrics on GBLM-Pruner Performance.",
            "score": 0.6270684261578191,
            "section_title": "A. Related Work",
            "char_start_offset": 35644,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 156
                },
                {
                    "start": 156,
                    "end": 271
                },
                {
                    "start": 271,
                    "end": 359
                },
                {
                    "start": 359,
                    "end": 718
                },
                {
                    "start": 718,
                    "end": 911
                },
                {
                    "start": 911,
                    "end": 1052
                },
                {
                    "start": 1052,
                    "end": 1227
                },
                {
                    "start": 1227,
                    "end": 1321
                },
                {
                    "start": 1321,
                    "end": 1453
                },
                {
                    "start": 1453,
                    "end": 1613
                },
                {
                    "start": 1615,
                    "end": 1693
                },
                {
                    "start": 1695,
                    "end": 1851
                }
            ],
            "ref_mentions": [
                {
                    "start": 1231,
                    "end": 1249,
                    "matchedPaperCorpusId": "263829692"
                },
                {
                    "start": 1326,
                    "end": 1343,
                    "matchedPaperCorpusId": "268032346"
                },
                {
                    "start": 1459,
                    "end": 1478,
                    "matchedPaperCorpusId": "254685796"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.7607421875
        },
        {
            "corpus_id": "263830786",
            "title": "Sheared LLaMA: Accelerating Language Model Pre-training via Structured Pruning",
            "text": "Pruning. Structured pruning has been extensively studied as a model compression technique in computer vision and natural language processing, where task-specific models like classification ones are often overparameterized and can be pruned significantly with minimal impact on performance (Han et al., 2016;Wen et al., 2016;Liu et al., 2017;Luo et al., 2017;Cai et al., 2019;Deng et al., 2020;Hou et al., 2020;Wang et al., 2020;Lagunas et al., 2021;Xia et al., 2022;Kurtic et al., 2023). Unstructured pruning (Frankle & Carbin, 2018;Li et al., 2020;Chen et al., 2020;Sanh et al., 2020) prunes individual neurons instead of structured blocks. Though unstructured pruning usually achieve higher compression rates, they are not practical for model speedup. \n\nIn the era of LLMs, the prevalent NLP pipeline has shifted from task-specific models to generalpurpose LMs, which leaves little room for redundancy. Both unstructured pruning, semi-structured pruning (Frantar & Alistarh, 2023;Sun et al., 2023), and structured pruning (Ma et al., 2023) lead to significant performance drops on LLM even at a modest sparsity. Noticeably, all previous works fix the original models or tune them minimally. We see pruning as an initialization and consider it necessary to expend substantial compute to continually pre-training the model to recover performance. \n\nEfficient pre-training approaches. As orthogonal to our pruning approach, There is an extensive body of work on improving efficiency of training LLMs. For example, quantization reduces the numeric precision of model weights and activations and speeds up training and inference (Dettmers et al., 2022;2023;Xiao et al., 2023).",
            "score": 0.6270649553583441,
            "section_title": "RELATED WORK",
            "char_start_offset": 18874,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 8
                },
                {
                    "start": 9,
                    "end": 487
                },
                {
                    "start": 488,
                    "end": 641
                },
                {
                    "start": 642,
                    "end": 753
                },
                {
                    "start": 756,
                    "end": 904
                },
                {
                    "start": 905,
                    "end": 1113
                },
                {
                    "start": 1114,
                    "end": 1192
                },
                {
                    "start": 1193,
                    "end": 1346
                },
                {
                    "start": 1349,
                    "end": 1383
                },
                {
                    "start": 1384,
                    "end": 1499
                },
                {
                    "start": 1500,
                    "end": 1673
                }
            ],
            "ref_mentions": [
                {
                    "start": 289,
                    "end": 307,
                    "matchedPaperCorpusId": "2134321"
                },
                {
                    "start": 307,
                    "end": 324,
                    "matchedPaperCorpusId": "2056019"
                },
                {
                    "start": 324,
                    "end": 341,
                    "matchedPaperCorpusId": "5993328"
                },
                {
                    "start": 341,
                    "end": 358,
                    "matchedPaperCorpusId": "11169209"
                },
                {
                    "start": 358,
                    "end": 375,
                    "matchedPaperCorpusId": "201666112"
                },
                {
                    "start": 375,
                    "end": 393,
                    "matchedPaperCorpusId": "215799572"
                },
                {
                    "start": 393,
                    "end": 410,
                    "matchedPaperCorpusId": "215415863"
                },
                {
                    "start": 410,
                    "end": 428,
                    "matchedPaperCorpusId": "204009154"
                },
                {
                    "start": 449,
                    "end": 466,
                    "matchedPaperCorpusId": "247922354"
                },
                {
                    "start": 509,
                    "end": 533,
                    "matchedPaperCorpusId": "53388625"
                },
                {
                    "start": 533,
                    "end": 549,
                    "matchedPaperCorpusId": "263868979"
                },
                {
                    "start": 549,
                    "end": 567,
                    "matchedPaperCorpusId": "220768628"
                },
                {
                    "start": 567,
                    "end": 585,
                    "matchedPaperCorpusId": "218665313"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.45068359375
        },
        {
            "corpus_id": "273962638",
            "title": "Zeroth-Order Adaptive Neuron Alignment Based Pruning without Re-Training",
            "text": "In recent times, Large Language Models (LLMs) have shown incredible performance over almost any language task [44,33,4]. However, their performance tends to grow with their sizes (i.e., the number of trainable parameters), which in turn is proportional to the computational burden required to train and then use such models. One way to reduce the computational cost of LLMs is through network pruning, i.e., algorithms that remove parameters while minimizing performance degradation. This approach has been extensively studied on Convolutional Neural Networks (CNNs) [13,25,43,11], but nowadays the focus has shifted towards pre-trained models [41,42,22]. This shift has required a change of paradigm in pruning techniques: in fact, while in CNNs the main paradigm is iterative pruning (with re-training) [13], with pre-trained models (such as LLMs) in most cases it is not possible to fully re-train such models, because (1) training data are often not accessible, and (2) full re-training would be anyway too expensive. This calls for \"exploiting\" as much as possible the information contained in a pre-trained model to obtain a performant sparse version of it, using weight's information [21], activations [40,39], or reconstruction error [15], without the need for re-training. More recently, a new category of pruning algorithms, which we may call top-up algorithms (i.e., methods that can be applied on top of a given pruning algorithm for LLMs), has emerged, aiming at further improving pruning performance. Such approaches can be divided into two categories: those that minimize the reconstruction error [18,45,49], and those that impose non-uniform sparsity distribution modifying the block-wise sparsity [46]. The latter category is extremely effective for improving performance in CNNs [14,38], while its application to LLMs is still limited [46].",
            "score": 0.6269273828942117,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 120
                },
                {
                    "start": 121,
                    "end": 324
                },
                {
                    "start": 325,
                    "end": 483
                },
                {
                    "start": 484,
                    "end": 655
                },
                {
                    "start": 656,
                    "end": 1021
                },
                {
                    "start": 1022,
                    "end": 1281
                },
                {
                    "start": 1282,
                    "end": 1514
                },
                {
                    "start": 1515,
                    "end": 1719
                },
                {
                    "start": 1720,
                    "end": 1858
                }
            ],
            "ref_mentions": [
                {
                    "start": 110,
                    "end": 114,
                    "matchedPaperCorpusId": "246411621"
                },
                {
                    "start": 114,
                    "end": 117,
                    "matchedPaperCorpusId": "240420063"
                },
                {
                    "start": 117,
                    "end": 119,
                    "matchedPaperCorpusId": "259360395"
                },
                {
                    "start": 577,
                    "end": 580,
                    "matchedPaperCorpusId": "208267757"
                },
                {
                    "start": 922,
                    "end": 925,
                    "matchedPaperCorpusId": "267301573"
                },
                {
                    "start": 1191,
                    "end": 1195,
                    "matchedPaperCorpusId": "259088941"
                },
                {
                    "start": 1209,
                    "end": 1213,
                    "matchedPaperCorpusId": "259203115"
                },
                {
                    "start": 1242,
                    "end": 1246,
                    "matchedPaperCorpusId": "255372747"
                },
                {
                    "start": 1616,
                    "end": 1619,
                    "matchedPaperCorpusId": "268032346"
                },
                {
                    "start": 1619,
                    "end": 1622,
                    "matchedPaperCorpusId": "264128029"
                },
                {
                    "start": 1714,
                    "end": 1718,
                    "matchedPaperCorpusId": "263829692"
                },
                {
                    "start": 1797,
                    "end": 1801,
                    "matchedPaperCorpusId": "221802286"
                },
                {
                    "start": 1801,
                    "end": 1804,
                    "matchedPaperCorpusId": "221857593"
                },
                {
                    "start": 1853,
                    "end": 1857,
                    "matchedPaperCorpusId": "263829692"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.84375
        },
        {
            "corpus_id": "266362404",
            "title": "Fluctuation-based Adaptive Structured Pruning for Large Language Models",
            "text": "Large Language Models (LLMs) (Brown et al. 2020;Touvron et al. 2023;Zhang et al. 2022;Scao et al. 2022) have recently achieved outstanding performance across various language benchmarks in NLP (Bommarito and Katz 2022;Bubeck et al. 2023;Wei et al. 2022), spurring a large number of open-source applications (Taori et al. 2023;Anand et al. 2023;Richards 2023). These remarkable capabilities typically come with a huge-scale model size with high inference costs. This makes it harder for more people to benefit from LLMs. Due to the computational resource constraints, most of the model compression methods in the pre-LLM era are no longer feasible for LLMs. Model compression methods for LLMs to date focus on model quantization (Dettmers et al. 2022;Xiao et al. 2023;Frantar et al. 2023;Dettmers et al. 2023) and unstructured pruning (Sun et al. 2023;Frantar and Alistarh 2023). \n\nStructured pruning (He and Xiao 2023), which prunes entire rows or columns of weights, offers a promising solution to the deployment challenges of LLMs. Unlike unstructured pruning, structured pruning reduces both parameters and inference time without relying on specific hardware, making it more widely applicable (Anwar, Hwang, and Sung 2017). For effective structured pruning, it's crucial to have a metric that captures the collective significance of an entire row or column. However, current unstructured pruning techniques for LLMs, as seen in methods like (Sun et al. 2023;Frantar and Alistarh 2023), primarily focus on the importance of individual elements of each row in isolation. This absence of structured metrics that evaluate entire rows or columns makes them less suitable for structured pruning. The recent LLM-Pruner (Ma, Fang, and Wang 2023) attempted structured pruning for LLMs, but its dependence on LoRA finetuning (Hu et al. 2021) creates a tough trade-off between high computation and effective pruning, limiting its use in larger models.",
            "score": 0.6257666837902519,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 359
                },
                {
                    "start": 360,
                    "end": 460
                },
                {
                    "start": 461,
                    "end": 519
                },
                {
                    "start": 520,
                    "end": 656
                },
                {
                    "start": 657,
                    "end": 878
                },
                {
                    "start": 881,
                    "end": 1033
                },
                {
                    "start": 1034,
                    "end": 1226
                },
                {
                    "start": 1227,
                    "end": 1360
                },
                {
                    "start": 1361,
                    "end": 1571
                },
                {
                    "start": 1572,
                    "end": 1692
                },
                {
                    "start": 1693,
                    "end": 1943
                }
            ],
            "ref_mentions": [
                {
                    "start": 48,
                    "end": 68,
                    "matchedPaperCorpusId": "221082307"
                },
                {
                    "start": 237,
                    "end": 253,
                    "matchedPaperCorpusId": "249674500"
                },
                {
                    "start": 307,
                    "end": 326,
                    "matchedPaperCorpusId": "221082307"
                },
                {
                    "start": 750,
                    "end": 767,
                    "matchedPaperCorpusId": "253708271"
                },
                {
                    "start": 1196,
                    "end": 1225,
                    "matchedPaperCorpusId": "7333079"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.54638671875
        },
        {
            "corpus_id": "273532238",
            "title": "Beware of Calibration Data for Pruning Large Language Models",
            "text": "As large language models (LLMs) are widely applied across various fields, model compression has become increasingly crucial for reducing costs and improving inference efficiency. Post-training pruning is a promising method that does not require resource-intensive iterative training and only needs a small amount of calibration data to assess the importance of parameters. Previous research has primarily focused on designing advanced pruning methods, while different calibration data's impact on pruning performance still lacks systematical exploration. We fill this blank and surprisingly observe that the effects of calibration data even value more than designing advanced pruning strategies, especially for high sparsity. Our preliminary exploration also discloses that using calibration data similar to the training data can yield better performance. As pre-training data is usually inaccessible for advanced LLMs, we further provide a self-generating calibration data synthesis strategy to construct feasible calibration data. We conduct experiments on the recent strong open-source LLMs (e.g., DCLM, and LLaMA-3), and the results show that the proposed method outperforms commonly used calibration data and can effectively enhance strong pruning methods (e.g., Wanda, OWL).",
            "score": 0.6240012220354426,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.69921875
        },
        {
            "corpus_id": "270379842",
            "title": "MoreauPruner: Robust Pruning of Large Language Models against Weight Perturbations",
            "text": "In this paper, we discussed how minor changes in model weights can lead to unstable pruning results for large language models (LLMs).To address this instability, we introduced MoreauPruner, a weightperturbation structural pruning method.Our theoretical analysis demonstrates that MoreauPruner is robust to norm-bounded perturbations.Numerical experiments conducted on well-known LLMs, such as LLaMA-7B, LLaMA-13B, LLaMA-3-8B, and Vicuna-7B, suggest that MoreauPruner can efficiently compress LLMs while maintaining their performance.For future work, we propose combining structural pruning technology with other model compression methods to accelerate model inference and reduce computational costs.\n\nLimitations.The authors acknowledge that the number of parameters utilized in the models for this paper only reach 13B due to limited hardware budget.The performance of MoreauPruner on extremely large-scale models (e.g., 30B, 70B, etc.) will be further explored once enough hardware resources are available.",
            "score": 0.6239728701422889,
            "section_title": "Conclusion",
            "char_start_offset": 23984,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 133
                },
                {
                    "start": 133,
                    "end": 237
                },
                {
                    "start": 237,
                    "end": 333
                },
                {
                    "start": 333,
                    "end": 533
                },
                {
                    "start": 533,
                    "end": 699
                },
                {
                    "start": 701,
                    "end": 713
                },
                {
                    "start": 713,
                    "end": 851
                },
                {
                    "start": 851,
                    "end": 1008
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.53955078125
        },
        {
            "corpus_id": "268041812",
            "title": "SparseLLM: Towards Global Pruning of Pre-trained Language Models",
            "text": "The transformative impact of large language models (LLMs) like LLaMA and GPT on natural language processing is countered by their prohibitive computational demands. Pruning has emerged as a pivotal compression strategy, introducing sparsity to enhance both memory and computational efficiency. Yet, traditional global pruning is impractical for LLMs due to scalability issues, while local pruning, despite its efficiency, leads to suboptimal solutions. Addressing these challenges, we propose SparseLLM, a novel framework that redefines the global pruning process into manageable, coordinated subproblems, allowing for resource-efficient optimization with global optimality. SparseLLM's approach, which conceptualizes LLMs as a chain of modular functions and leverages auxiliary variables for problem decomposition, not only facilitates a pragmatic application on LLMs but also demonstrates significant performance improvements, particularly in high-sparsity regimes where it surpasses current state-of-the-art methods.",
            "score": 0.6236483555778671,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.81591796875
        },
        {
            "corpus_id": "256662734",
            "title": "What Matters In The Structured Pruning of Generative Language Models?",
            "text": "Large language models (LLMs), such as the state-of-the-art GPT-3 model (Brown et al., 2020) with up to 175 billion parameters, have achieved remarkable performance in natural language processing (NLP) tasks. However, training and deploying such massive models also poses significant challenges in terms of computational cost, energy consumption, and environmental impact. Therefore, it is crucial to develop effective methods to reduce the size of LLMs without compromising their quality. \n\nNeural network pruning is a long-standing model compression method (Janowsky, 1989;Mozer & Smolensky, 1988;Frankle & Carbin, 2018;Karnin, 1990;Blalock et al., 2020). It can be broadly classified into two types: unstructured and structured. Unstructured pruning removes individual weights from the network based on some criteria, resulting in sparse weight matrices that can be stored and processed more efficiently. Structured pruning, on the other hand, eliminates whole components, such as neurons, channels, or blocks, leading to smaller architectures to reduce end-to-end inference latency. While unstructured pruning has been extensively studied and applied to LLMs (Wang et al., 2020b;Xu et al., 2021;Zafrir et al., 2021;Li et al., 2022), structured pruning is more challenging and less explored. However, structured pruning is also more desirable in many practical scenarios, such as deploying these models on resource-constrained devices or providing fast services based on LLMs. \n\nExisting work on structured pruning for LMs focuses on BERT-like networks (Devlin et al., 2018) that consist of an encoder-decoder or an encoder-only architecture (Li et al., 2020;Xia et al., 2022;Zhang et al., 2022;Yao et al., 2021). These models are mainly used for natural language understanding (NLU) tasks, such as question answering, sentiment analysis, or natural language inference. Among the various methods, Block Movement Pruning (Lagunas et al., 2021) is a recent and popular technique that removes weight blocks based on movement.",
            "score": 0.6234560775987508,
            "section_title": "INTRODUCTION",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 207
                },
                {
                    "start": 208,
                    "end": 371
                },
                {
                    "start": 372,
                    "end": 488
                },
                {
                    "start": 491,
                    "end": 656
                },
                {
                    "start": 657,
                    "end": 730
                },
                {
                    "start": 731,
                    "end": 906
                },
                {
                    "start": 907,
                    "end": 1085
                },
                {
                    "start": 1086,
                    "end": 1293
                },
                {
                    "start": 1294,
                    "end": 1478
                },
                {
                    "start": 1481,
                    "end": 1715
                },
                {
                    "start": 1716,
                    "end": 1871
                },
                {
                    "start": 1872,
                    "end": 2024
                }
            ],
            "ref_mentions": [
                {
                    "start": 558,
                    "end": 574,
                    "matchedPaperCorpusId": "31375995"
                },
                {
                    "start": 574,
                    "end": 598,
                    "matchedPaperCorpusId": "17651092"
                },
                {
                    "start": 621,
                    "end": 634,
                    "matchedPaperCorpusId": "1101832"
                },
                {
                    "start": 1162,
                    "end": 1182,
                    "matchedPaperCorpusId": "204009154"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.6142578125
        },
        {
            "corpus_id": "272987828",
            "title": "Aggressive Post-Training Compression on Extremely Large Language Models",
            "text": "In this paper, we proposed a novel score-based sparsity scheduler for pruning large language models (LLMs) that outperform existing techniques. Our method leverages the information provided by previous weight updates to estimate the expectation of weight updating terms under all possible pruning masks, allowing us to choose an optimal sparsity level for each layer. We demonstrated the effectiveness of our approach by comparing it with SparseGPT, the current state-of-the-art LLM pruning technique, and a naive layer-wise scheduler based on sequential order. Our experiments revealed that our method consistently outperforms SparseGPT in terms of perplexity, except for the OPT-6.7B model. We also found that a narrower sparsity range is helpful for models like OPT-6.7B which have short-tailed score distributions. Additionally, our analysis showed that higher layers contribute more losses due to pruning, indicating the importance of selecting appropriate sparsity levels for different layers. Future work could explore quantitative metrics for determining optimal sparsity ranges and investigate the relationship between sparsity and other factors, such as speed and memory usage. Overall, our score-based sparsity scheduler provides an effective and efficient solution for pruning LLMs while maintaining their perplexity.",
            "score": 0.6225089195416462,
            "section_title": "Conclusion",
            "char_start_offset": 19708,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 143
                },
                {
                    "start": 144,
                    "end": 367
                },
                {
                    "start": 368,
                    "end": 561
                },
                {
                    "start": 562,
                    "end": 685
                },
                {
                    "start": 686,
                    "end": 692
                },
                {
                    "start": 693,
                    "end": 818
                },
                {
                    "start": 819,
                    "end": 999
                },
                {
                    "start": 1000,
                    "end": 1187
                },
                {
                    "start": 1188,
                    "end": 1329
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.73974609375
        },
        {
            "corpus_id": "271083368",
            "title": "Achieving Peak Performance for Large Language Models: A Systematic Review",
            "text": "For instance, SparseGPT can remove over 100 billion weights from massive models like OPT-175B and BLOOM-176B without compromising their performance on language modeling tasks. \n\nParametrization without Gradient Dependence: To leverage the parametrization of massive GPT models to enable pruning without relying on gradient information. This method allows the identification of sparse counterparts within a close range of the original dense model, ensuring these sparse models maintain similar performance. Interestingly, the strategy highlights that larger models are even easier to prune using this approach. They experience minimal accuracy drops even at significant sparsity levels (e.g., 50%). This observation underscores the effectiveness of the parametrization technique in enabling aggressive pruning while preserving model performance. \n\nOutcomes: The application of SparseGPT led to remarkable results: \n\n\u2022 Model size reduction: SparseGPT achieved 50-60% sparsity, significantly reducing the model size by removing more than 100 billion weights in models like OPT-175B and BLOOM-176B. \u2022 Processing time: The pruning process was completed in less than 4.5 hours for the largest open-source models, demonstrating high efficiency. \n\n\u2022 Accuracy maintenance: The pruned models exhibited negligible increases in perplexity and retained performance levels very similar to their dense counterparts. \n\n\u2022 Scalability: The study revealed that larger models are easier to prune, with practically no accuracy decrease observed at 50% sparsity. \n\nThis case study demonstrates the efficacy of SparseGPT's one-shot pruning approach for reducing the size of massive language models. By leveraging unstructured sparsity and parametrization strategies without gradient dependence, SparseGPT achieves substantial reductions in model size and resource requirements while maintaining high levels of performance. This approach enables more efficient and accessible deployment of large language models in various applications, making them more practical for real-world use.",
            "score": 0.6221481433352118,
            "section_title": "A. OPTIMIZING MODEL TRAINING WITH SPARSEGPT",
            "char_start_offset": 115311,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 175
                },
                {
                    "start": 178,
                    "end": 335
                },
                {
                    "start": 336,
                    "end": 505
                },
                {
                    "start": 506,
                    "end": 609
                },
                {
                    "start": 610,
                    "end": 697
                },
                {
                    "start": 698,
                    "end": 844
                },
                {
                    "start": 847,
                    "end": 912
                },
                {
                    "start": 915,
                    "end": 1094
                },
                {
                    "start": 1095,
                    "end": 1237
                },
                {
                    "start": 1240,
                    "end": 1400
                },
                {
                    "start": 1403,
                    "end": 1540
                },
                {
                    "start": 1543,
                    "end": 1675
                },
                {
                    "start": 1676,
                    "end": 1899
                },
                {
                    "start": 1900,
                    "end": 2059
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.8037109375
        },
        {
            "corpus_id": "270063400",
            "title": "SPP: Sparsity-Preserved Parameter-Efficient Fine-Tuning for Large Language Models",
            "text": "SparseGPT uses second-order information to address a layer-wise reconstruction problem and prunes large models with unstructured and N:M structured sparsity (Zhou et al., 2021) respectively.Wanda (Sun et al., 2023) proposes a new pruning metric that takes both weight magnitude and their corresponding input activations into consideration, achieving comparable perplexity with SparseGPT (Frantar & Alistarh, 2023).However, (Jaiswal et al., 2023) points out that perplexity is not necessarily an accurate metric for evaluating the effectiveness of model compression, with both SparseGPT and Wanda fail to achieve satisfactory performance even with low-level sparsity (25-30%).Based on this issue, we speculate that the lack of retraining after removing the unimportant weights will lead to an undesirable decline in performance, and put forward a novel training method with high training efficiency.\n\nParameter-efficient Fine-tuning: In different language and vision tasks, the pre-training and fine-tuning paradigms have been proven to be highly effective.Compared with full parameter fine-tuning, Parameter-Efficient Fine-Tuning (PEFT) (Mangrulkar et al., 2022;Xu et al., 2023) methods freeze most parameters of dense pre-trained models and aim to exhibit comparable capabilities on downstream tasks.LoRA (Hu et al., 2021) introduces trainable lowrank decomposition matrices into dense network weights.Adapters (Houlsby et al., 2019) insert lightweight adaption modules into each block of the language models.Different from previous efforts for dense pre-trained language models, we propose the SPP method with few learnable parameters, which is specially designed for sparse LLMs.",
            "score": 0.6218579774817223,
            "section_title": "Related Work",
            "char_start_offset": 6579,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 190
                },
                {
                    "start": 190,
                    "end": 414
                },
                {
                    "start": 414,
                    "end": 675
                },
                {
                    "start": 675,
                    "end": 898
                },
                {
                    "start": 900,
                    "end": 1056
                },
                {
                    "start": 1056,
                    "end": 1301
                },
                {
                    "start": 1301,
                    "end": 1403
                },
                {
                    "start": 1403,
                    "end": 1510
                },
                {
                    "start": 1510,
                    "end": 1682
                }
            ],
            "ref_mentions": [
                {
                    "start": 423,
                    "end": 445,
                    "matchedPaperCorpusId": "211069143"
                },
                {
                    "start": 1412,
                    "end": 1434,
                    "matchedPaperCorpusId": "59599816"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.6103515625
        },
        {
            "corpus_id": "270063400",
            "title": "SPP: Sparsity-Preserved Parameter-Efficient Fine-Tuning for Large Language Models",
            "text": "To further validate the effectiveness of our SPP method, we extend the existing post-training pruning method Wanda (Sun et al., 2023) to 75% sparsity ratio with unstructured 75% and 2:8 sparsity.The results are shown in Tab. 3. \"LM-eval\" stands for zero-shot evaluation results of 7 different tasks from EleutherAI LM Harness (Gao et al., 2021), \"PPL\" stands for Wikitext perplexity (Merity et al., 2016).We find that Wanda (Sun et al., 2023) with unstructured 75% and 2:8 sparsity has a significant performance drop, while the dynamic mask without retraining method DS\u2298T (Zhang et al., 2023b) has limited performance improvement.We then apply the SPP method to the pruned models by Wanda and obtain far better results.This further highlights the effectiveness of parameter retraining -just as SPP does -in boosting the performance of sparse LLMs.",
            "score": 0.6217884160033811,
            "section_title": "Extend to Higher Sparsity",
            "char_start_offset": 22260,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 195
                },
                {
                    "start": 195,
                    "end": 405
                },
                {
                    "start": 405,
                    "end": 630
                },
                {
                    "start": 630,
                    "end": 719
                },
                {
                    "start": 719,
                    "end": 847
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.5517578125
        },
        {
            "corpus_id": "271328221",
            "title": "Compact Language Models via Pruning and Knowledge Distillation",
            "text": "Large language models (LLMs) now dominate realworld natural language processing and have demonstrated excellent proficiency in understanding difficult contexts [7,40,52,49,48]. To aid users targeting different deployment sizes and scales, model providers often train an entire family of models from scratch, each with a different size (number of parameters). For instance, the LLaMa-2 model family [49] includes three different variants with 7, 13, and 70 billion parameters, while the Pythia family [6] offers  1: Demonstration of how various pruning strategies perform before and after lightweight retraining using \u223c1.8B tokens. We prune the Nemotron-4 15B model down to the size of Nemotron-3 8B and report the change in distillation loss (KL divergence [28] on logits) and the final LM validation loss with retraining. We see that width (attention, MLP, embedding) pruning outperforms depth, but only after retraining. The last row shows change in loss for the Nemotron-3 8B model. \n\na selection of eight models with sizes ranging from 80 million to 12 billion parameters. However, training multiple multi-billion parameter models from scratch is extremely time, data and resourceintensive. In this paper, we ask the following question: can we train one big model, and obtain smaller, more accurate (w.r.t. training from scratch) models from it through a combination of weight pruning and retraining, while only using a small fraction of the original training data? Achieving such a goal would make producing LLMs targeting different deployment scales significantly cheaper. Weight pruning is a powerful and well-known technique for reducing model size [51,21]. In this paper, we focus on structured pruning, where blocks of nonzero elements are removed at once from model weights; examples of structured pruning techniques include neuron, attention head, convolutional filter, and depth pruning [32,18,53,4,34,55,26].",
            "score": 0.6215557615640416,
            "section_title": "body",
            "char_start_offset": 1,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 176
                },
                {
                    "start": 177,
                    "end": 358
                },
                {
                    "start": 359,
                    "end": 630
                },
                {
                    "start": 631,
                    "end": 822
                },
                {
                    "start": 823,
                    "end": 922
                },
                {
                    "start": 923,
                    "end": 985
                },
                {
                    "start": 988,
                    "end": 1076
                },
                {
                    "start": 1077,
                    "end": 1194
                },
                {
                    "start": 1195,
                    "end": 1310
                },
                {
                    "start": 1311,
                    "end": 1469
                },
                {
                    "start": 1470,
                    "end": 1578
                },
                {
                    "start": 1579,
                    "end": 1665
                },
                {
                    "start": 1666,
                    "end": 1922
                }
            ],
            "ref_mentions": [
                {
                    "start": 160,
                    "end": 163,
                    "matchedPaperCorpusId": "218971783"
                },
                {
                    "start": 166,
                    "end": 169,
                    "matchedPaperCorpusId": "246411621"
                },
                {
                    "start": 500,
                    "end": 503,
                    "matchedPaperCorpusId": "257921893"
                },
                {
                    "start": 757,
                    "end": 761,
                    "matchedPaperCorpusId": "116908168"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.37548828125
        },
        {
            "corpus_id": "268041812",
            "title": "SparseLLM: Towards Global Pruning of Pre-trained Language Models",
            "text": "SparseLLM also prunes other linear layers besides those in FFNs. By following Eq. 4, for each linear layer out of FFN modules, the pruning objective simplifies to \u03b1\u2225z pre \u2113+1 \u2212 (M \u2113+1 \u2299 W \u2113+1 )a pre \u2113 \u2225 2 2 , which is equivalent (with some simple math) to that of completely local pruning as shown in Eq. 2. \n\nExisting LLM pruning solvers such as SparseGPT and Wanda are applicable here.",
            "score": 0.6204305655120201,
            "section_title": "Pruning of MHAs",
            "char_start_offset": 20784,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 64
                },
                {
                    "start": 65,
                    "end": 81
                },
                {
                    "start": 82,
                    "end": 307
                },
                {
                    "start": 310,
                    "end": 387
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.67138671875
        },
        {
            "corpus_id": "266362404",
            "title": "Fluctuation-based Adaptive Structured Pruning for Large Language Models",
            "text": "In this work, we propose FLAP (FLuctuation-based Adaptive Structured Pruning), a retraining-free structured pruning framework explicitly designed for Large Language Models (LLMs). To address the challenges posed by structured pruning, we introduce a novel structured pruning metric, employ adaptive global model compression strategies, and implement robust compensation mechanisms designed to mitigate potential performance losses. Our empirical results affirm that the structured compression model crafted by FLAP can maintain perplexity and zero-shot performance without any retraining. Especially worth noting is the efficacy of FLAP in upholding model performance at both low and medium compression rates. Our work demonstrates that bias compensation can largely replace retraining or parameter efficient fine-tuning (PEFT). We hope that our work contributes to a better understanding of structured pruning and performance recovery of LLMs.",
            "score": 0.6198534918835052,
            "section_title": "Conclusion",
            "char_start_offset": 23781,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 179
                },
                {
                    "start": 180,
                    "end": 431
                },
                {
                    "start": 432,
                    "end": 588
                },
                {
                    "start": 589,
                    "end": 709
                },
                {
                    "start": 710,
                    "end": 828
                },
                {
                    "start": 829,
                    "end": 944
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.87158203125
        },
        {
            "corpus_id": "271534421",
            "title": "Mobile Edge Intelligence for Large Language Models: A Contemporary Survey",
            "text": "Then, the pruned LLMs are fine-tuned with LoRA [112] to recover model performance. The compressed LLMs pruned via structural pruning can be directly deployed and executed on standard computational frameworks without additional adjustments since the structured pruning removes entire structures in LLMs. 2) Unstructured pruning aims to remove individual weights, realizing finer-grained pruning by setting unimportant weights to zero. This approach makes the weights sparse, allowing us to leverage the sparsity to accelerate the inference of LLMs. For example, SparseGPT [92] transforms the pruning problem into a series of largescale sparse regression problems and addresses them with an innovative approximate solver. The proposed SparseGPT enables up to 60% sparsity for OPT-175B [118] before significant accuracy loss occurs. Although this approach can better preserve the performance of pruned LLM for on-device inference, it results in sparse models that require specialized hardware or software platforms during deployment, which is the major limitation of LLM unstructured pruning. Knowledge distillation: Knowledge distillation (KD) [119] involves transferring knowledge from a large and complex teacher model to a smaller student model. This technique enables the small-size student model to learn the behavior of the teacher model, making it suitable for deploying on resourceconstrained edge devices while ensuring a competitive performance. 1) When the teacher model is fully accessible, the KD process is known as white-box KD, which allows the student model to learn the output distributions, intermediate features, and activations of the teacher model [11], [120]. For example, MiniLLM [93] addresses the limitations of traditional KD loss functions, which do not perform well for text-generation tasks, by minimizing the reversed Kullback-Leibler divergence between the student and teacher model output distributions, thereby enhancing the performance of the student model. Additionally, the authors derive an effective optimization strategy to update the student model. 2) When the internal structure of the models is inaccessible, the form of KD is referred to as black-box KD [120].",
            "score": 0.6195631932529636,
            "section_title": "Resource-efficient fine-tuning",
            "char_start_offset": 39300,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 82
                },
                {
                    "start": 83,
                    "end": 302
                },
                {
                    "start": 303,
                    "end": 433
                },
                {
                    "start": 434,
                    "end": 547
                },
                {
                    "start": 548,
                    "end": 719
                },
                {
                    "start": 720,
                    "end": 829
                },
                {
                    "start": 830,
                    "end": 1089
                },
                {
                    "start": 1090,
                    "end": 1246
                },
                {
                    "start": 1247,
                    "end": 1453
                },
                {
                    "start": 1454,
                    "end": 1680
                },
                {
                    "start": 1681,
                    "end": 1990
                },
                {
                    "start": 1991,
                    "end": 2087
                },
                {
                    "start": 2088,
                    "end": 2202
                }
            ],
            "ref_mentions": [
                {
                    "start": 47,
                    "end": 52,
                    "matchedPaperCorpusId": "235458009"
                },
                {
                    "start": 571,
                    "end": 575,
                    "matchedPaperCorpusId": "255372747"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.53076171875
        },
        {
            "corpus_id": "265220879",
            "title": "Investigating Hallucinations in Pruned Large Language Models for Abstractive Summarization",
            "text": "We first compare language modeling performance between the original and pruned models. Following Frantar and Alistarh (2023) and Sun et al. (2024), we compute perplexity on the WikiText test set (Merity et al., 2017), shown in Table 3. \n\nOverall, pruned models consistently generate text with higher perplexity than their original counterparts. Unsurprisingly, magnitude pruning routinely produces the highest perplexity. In many cases, the increase over the original model (denoted by '-') is substantial. For example, we observe more than a twentyfold increase for OPT-IML 30B, from 10.56 to 246.42. In contrast, SparseGPT and Wanda achieve perplexity close to the original for the majority of the models. Surprisingly, Falcon 7B records higher perplexity across all pruning methods, e.g. 85.68 when applying Wanda from 19.93 without pruning. \n\nDue to the substantial degradation in language modeling performance, we omit magnitude pruning from further analysis. For the same reason, we also exclude the Falcon 7B and OPT-IML 1.3B models.",
            "score": 0.6184831879705639,
            "section_title": "Language Modeling",
            "char_start_offset": 19054,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 86
                },
                {
                    "start": 87,
                    "end": 235
                },
                {
                    "start": 238,
                    "end": 344
                },
                {
                    "start": 345,
                    "end": 421
                },
                {
                    "start": 422,
                    "end": 506
                },
                {
                    "start": 507,
                    "end": 601
                },
                {
                    "start": 602,
                    "end": 707
                },
                {
                    "start": 708,
                    "end": 790
                },
                {
                    "start": 791,
                    "end": 844
                },
                {
                    "start": 847,
                    "end": 964
                },
                {
                    "start": 965,
                    "end": 1040
                }
            ],
            "ref_mentions": [
                {
                    "start": 129,
                    "end": 146,
                    "matchedPaperCorpusId": "259203115"
                },
                {
                    "start": 195,
                    "end": 216,
                    "matchedPaperCorpusId": "16299141"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.2293701171875
        },
        {
            "corpus_id": "276250081",
            "title": "EfficientLLM: Scalable Pruning-Aware Pretraining for Architecture-Agnostic Edge Language Models",
            "text": "Modern large language models (LLMs) driven by scaling laws, achieve intelligence emergency in large model sizes. Recently, the increasing concerns about cloud costs, latency, and privacy make it an urgent requirement to develop compact edge language models. Distinguished from direct pretraining that bounded by the scaling law, this work proposes the pruning-aware pretraining, focusing on retaining performance of much larger optimized models. It features following characteristics: 1) Data-scalable: we introduce minimal parameter groups in LLM and continuously optimize structural pruning, extending post-training pruning methods like LLM-Pruner and SparseGPT into the pretraining phase. 2) Architecture-agnostic: the LLM architecture is auto-designed using saliency-driven pruning, which is the first time to exceed SoTA human-designed LLMs in modern pretraining. We reveal that it achieves top-quality edge language models, termed EfficientLLM, by scaling up LLM compression and extending its boundary. EfficientLLM significantly outperforms SoTA baselines with $100M \\sim 1B$ parameters, such as MobileLLM, SmolLM, Qwen2.5-0.5B, OLMo-1B, Llama3.2-1B in common sense benchmarks. As the first attempt, EfficientLLM bridges the performance gap between traditional LLM compression and direct pretraining methods, and we will fully open source at https://github.com/Xingrun-Xing2/EfficientLLM.",
            "score": 0.6180212128775049,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.67822265625
        },
        {
            "corpus_id": "271909421",
            "title": "Enhancing One-shot Pruned Pre-trained Language Models through Sparse-Dense-Sparse Mechanism",
            "text": "These insights lay the foundation for the work presented in this paper. \n\nWe propose a three-step Sparse-Dense-Sparse (SDS) pruning framework to enhance the performance of pruned pre-trained language models. In the first step, we employ conventional one-shot pruning methods on a PLM to remove irrelevant connections. In the second step, we perform a dense reconstruction of the sparse model to reactivate the pruned connections, aiming to identify a dense model with enhanced pruning awareness. This process is aided by a multidimensional sparse regularization strategy, which optimally guides the weight distribution, rendering it more pruningfriendly for the subsequent step. In the third step, we further prune and adjust the weights of the second-pruned model. Importantly, SDS requires only a limited number of samples for calibration, identical to conventional one-shot methods. Experimental results demonstrate that SDS outperforms SparseGPT and Wanda under the same sparsity configuration. For example, SDS reduces perplexity by 9.13 on Raw-Wikitext2 and increases average accuracy by 2.05% across multiple downstream tasks for OPT-125M with 2:4 sparsity. The pruned PLMs achieve up to 1.87x acceleration on an AMD R7 Pro CPU. The main contributions of the paper are summarized as follows: \n\n\u2022 We introduce SDS, a three-step Sparse-Dense-Sparse framework. It involves weight redistribution and pruning, enhancing the performance of the one-shot pruned pre-trained language models. \u2022 We design sparse regularization strategies that improve the effectiveness of re-dense weight reconstruction and find a more pruning-friendly weight distribution. \u2022 Experimental results demonstrate that SDS outperforms existing pruning methods in language comprehension and downstream task performance.",
            "score": 0.6174898784974913,
            "section_title": "Introduction",
            "char_start_offset": 2070,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 71
                },
                {
                    "start": 74,
                    "end": 207
                },
                {
                    "start": 208,
                    "end": 317
                },
                {
                    "start": 318,
                    "end": 495
                },
                {
                    "start": 496,
                    "end": 678
                },
                {
                    "start": 679,
                    "end": 765
                },
                {
                    "start": 766,
                    "end": 885
                },
                {
                    "start": 886,
                    "end": 998
                },
                {
                    "start": 999,
                    "end": 1164
                },
                {
                    "start": 1165,
                    "end": 1235
                },
                {
                    "start": 1236,
                    "end": 1298
                },
                {
                    "start": 1301,
                    "end": 1364
                },
                {
                    "start": 1365,
                    "end": 1489
                },
                {
                    "start": 1490,
                    "end": 1653
                },
                {
                    "start": 1654,
                    "end": 1793
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.63671875
        },
        {
            "corpus_id": "276079889",
            "title": "Symmetric Pruning of Large Language Models",
            "text": "Traditional model pruning. Pruning has emerged as a powerful strategy to compress and accelerate deep neural networks by removing redundant connections while preserv-ing overall performance (Han et al., 2015;Frankle & Carbin, 2018;Hoefler et al., 2021). Early works introduced iterative pruning-and-retraining approaches, which iteratively identify unimportant weights, discard them, and retrain the resulting sparse network to recover accuracy (LeCun et al., 1989;Han et al., 2015). More recent dynamic sparse training techniques (Mocanu et al., 2018;Bellec et al., 2018;Lee et al., 2018;Mostafa & Wang, 2019) start from a sparse initialization and continuously prune and grow connections throughout training. These methods integrate sparsification into the training loop, yielding promising trade-offs between model size and performance. A prominent line of work has leveraged learnable thresholds to realize non-uniform sparsity (Kusupati et al., 2020) or combined magnitudebased pruning with periodic connectivity updates to regrow valuable weights (Evci et al., 2020;Lasby et al., 2023). However, most of these methods still rely on standard backpropagation over the full parameter set, which can be prohibitively expensive when scaling up to LLMs. \n\nLLM post-training pruning. The substantial computational demands of LLMs have raised the development of pruning methods tailored to reduce parameters counts without compromising performance (Li et al., 2023;Zhu et al., 2024). Among these methods, post-training pruning eliminates redundant parameters in a pre-training network without requiring resource-intensive fine-tuning. For instance, SparseGPT (Frantar & Alistarh, 2023) leverages secondorder information to solve layer-wise reconstruction problems, supporting both unstructured and N:M structured sparsity (Zhou et al., 2021). Wanda (Sun et al., 2023) introduces a pruning metric that incorporates both weight magnitudes and corresponding input activations, achieving perplexity performance comparable to SparseGPT while surpassing simple magnitude-based pruning.",
            "score": 0.6174533612043696,
            "section_title": "Related Work",
            "char_start_offset": 4120,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 26
                },
                {
                    "start": 27,
                    "end": 253
                },
                {
                    "start": 254,
                    "end": 483
                },
                {
                    "start": 484,
                    "end": 710
                },
                {
                    "start": 711,
                    "end": 839
                },
                {
                    "start": 840,
                    "end": 1092
                },
                {
                    "start": 1093,
                    "end": 1253
                },
                {
                    "start": 1256,
                    "end": 1282
                },
                {
                    "start": 1283,
                    "end": 1481
                },
                {
                    "start": 1482,
                    "end": 1632
                },
                {
                    "start": 1633,
                    "end": 1840
                },
                {
                    "start": 1841,
                    "end": 2077
                }
            ],
            "ref_mentions": [
                {
                    "start": 190,
                    "end": 208,
                    "matchedPaperCorpusId": "231740691"
                },
                {
                    "start": 208,
                    "end": 231,
                    "matchedPaperCorpusId": "53388625"
                },
                {
                    "start": 231,
                    "end": 252,
                    "matchedPaperCorpusId": "231740691"
                },
                {
                    "start": 445,
                    "end": 465,
                    "matchedPaperCorpusId": "7785881"
                },
                {
                    "start": 465,
                    "end": 482,
                    "matchedPaperCorpusId": "231740691"
                },
                {
                    "start": 531,
                    "end": 552,
                    "matchedPaperCorpusId": "49310977"
                },
                {
                    "start": 552,
                    "end": 572,
                    "matchedPaperCorpusId": "2835189"
                },
                {
                    "start": 589,
                    "end": 610,
                    "matchedPaperCorpusId": "53556443"
                },
                {
                    "start": 932,
                    "end": 955,
                    "matchedPaperCorpusId": "211069143"
                },
                {
                    "start": 1053,
                    "end": 1072,
                    "matchedPaperCorpusId": "208267757"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.7060546875
        },
        {
            "corpus_id": "277876026",
            "title": "Adaptive Depth-Wise Pruning for Efficient Environmental Sound Classification",
            "text": "Structured pruning techniques have advanced significantly in dependency-based methods for fine-grained Structured Pruning. Zhang et al. initially proposed DepGraph [31], which lists related units based on the interdependency between paired layers in neural networks. Based on DepGraph, Ma et al. proposed LLM-Pruner [32] for pruning large language models (LLMs), which discovers groups of interdependent heads and neurons, estimates the group importance by the gradient of loss change, and finally employs the low-rank approximation for recovery. Following that, Zhang et al. proposed LoRA prune [33], which builds a LoRA-guided criterion for efficient pruning with less memory. Xia et al. proposed Sheared LLaMa for targeted Structured Pruning towards different domains [34]. \n\nOn the other hand, unstructured pruning has also been prompted by enhanced criteria and pruning strategies on LLMs. Frantar and Alistarh proposed SparseGPT [35], a sparsification method tailored for large-scale language models. It achieves 60% sparsity on models like OPT-175B and BLOOM-176B, demonstrating its effectiveness without significant performance degradation. SparseGPT updates partial parameters of a subset based on loss change for pruning GPT models at one shot. Subsequently, Shao et al. use enhanced pruning criteria by sensitivity [36], which further reduces the pruning error. Sun et al. use both weights and activations to identify effective sparse networks and reach more robust results than SparseGPT [37]. \n\nSimilarly, early-exit deep neural networks (DNNs) [42] adopt a related idea of dynamically utilizing partial computations to accelerate inference. These methods expand the original network into a multi-branch structure by attaching intermediate exits, allowing different inputs to exit early based on their complexity. However, early-exit DNNs do not aim to simplify the network architecture permanently for the entire dataset; instead, they effectively enlarge the model. In contrast, our pruning-based approach compresses the network into a subnetwork, taking into account knowledge transfer from deeper to shallower layers, and thus differs in its optimization objective.",
            "score": 0.616890442598949,
            "section_title": "D. RECENT ADVANCES",
            "char_start_offset": 9028,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 122
                },
                {
                    "start": 123,
                    "end": 266
                },
                {
                    "start": 267,
                    "end": 546
                },
                {
                    "start": 547,
                    "end": 678
                },
                {
                    "start": 679,
                    "end": 776
                },
                {
                    "start": 779,
                    "end": 894
                },
                {
                    "start": 895,
                    "end": 1006
                },
                {
                    "start": 1007,
                    "end": 1148
                },
                {
                    "start": 1149,
                    "end": 1254
                },
                {
                    "start": 1255,
                    "end": 1372
                },
                {
                    "start": 1373,
                    "end": 1505
                },
                {
                    "start": 1508,
                    "end": 1654
                },
                {
                    "start": 1655,
                    "end": 1826
                },
                {
                    "start": 1827,
                    "end": 1980
                },
                {
                    "start": 1981,
                    "end": 2182
                }
            ],
            "ref_mentions": [
                {
                    "start": 164,
                    "end": 168,
                    "matchedPaperCorpusId": "233375914"
                },
                {
                    "start": 1326,
                    "end": 1330,
                    "matchedPaperCorpusId": "264146174"
                },
                {
                    "start": 1558,
                    "end": 1562,
                    "matchedPaperCorpusId": "273211111"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.6259765625
        },
        {
            "corpus_id": "276107452",
            "title": "Progressive Binarization with Semi-Structured Pruning for LLMs",
            "text": "Pruning is a widely used technique for compressing neural networks by removing less significant parameters, reducing the number of active weights. This results in sparse networks that are more efficient in memory, computation, and size. In LLMs, pruning methods are generally divided into structured, unstructured, and semi-structured approaches. Structured pruning (Ma et al., 2023;Ashkboos et al., 2024;Xia et al., 2024;An et al., 2024) eliminates entire structured model components to improve efficiency. However, this approach can lead to substantial performance degradation, often requiring retraining to restore lost functionality. Unstructured pruning (Dong et al., 2024), removes weight elements individually based on their importance, maintaining high performance even at higher sparsity levels, but the resulting sparsity patterns are not hardware-efficient. Semi-structured pruning strikes an optimal balance by keeping regular sparsity patterns, such as N :M sparsity, which is optimized for hardware, as seen in methods like SparseGPT (Frantar & Alistarh, 2023), Wanda (Sun et al., 2024), and STBLLM (Dong et al., 2025). Our approach leverages semi-structured pruning with N :M sparsity, aiming to minimize performance degradation while maintaining hardware efficiency.",
            "score": 0.6168595053748033,
            "section_title": "LLM Pruning",
            "char_start_offset": 6710,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 146
                },
                {
                    "start": 147,
                    "end": 236
                },
                {
                    "start": 237,
                    "end": 346
                },
                {
                    "start": 347,
                    "end": 507
                },
                {
                    "start": 508,
                    "end": 637
                },
                {
                    "start": 638,
                    "end": 868
                },
                {
                    "start": 869,
                    "end": 1133
                },
                {
                    "start": 1134,
                    "end": 1282
                }
            ],
            "ref_mentions": [
                {
                    "start": 659,
                    "end": 678,
                    "matchedPaperCorpusId": "270257857"
                },
                {
                    "start": 1113,
                    "end": 1132,
                    "matchedPaperCorpusId": "271710591"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.498046875
        },
        {
            "corpus_id": "272694272",
            "title": "Evaluating the Impact of Compression Techniques on Task-Specific Performance of Large Language Models",
            "text": "To evaluate the impact of compression techniques on task-specific performance in large language models, we selected the LLaMA-2-7B model [4] due to its balance between efficiency and complexity. Despite its lower compressibility compared to its predecessor Open Pre-trained Transformer (OPT) models [21], it provides a rigorous test for evaluating the effectiveness of different compression methods. \n\nWe employed three popular compression techniques: Magnitude Pruning [22], SparseGPT [9], and Wanda [10]. These methods were chosen based on their algorithmic nature and compatibility with fine-tuning for downstream tasks. Magnitude Pruning reduces model size by removing weights with the smallest absolute values, while SparseGPT and Wanda leverage calibration data during the pruning process to maintain model performance. \n\nOur methodology involved calibrating SparseGPT and Wanda with 128 random samples from the C4 dataset [23] to achieve 50% sparsity. We measured performance metrics, including Loss and Perplexity, on 5,000 random samples from the Unnatural dataset [24]. To ensure consistency, these samples were also used to evaluate Jensen-Shannon (JS) Divergence [8], providing a comprehensive assessment of model alterations post-compression. \n\nThe Jensen-Shannon (JS) Divergence is defined as: \n\nwhere M =1 2 (P + Q) and KL denotes the Kullback-Leibler Divergence. The KL Divergence is given by: \n\nHere, P and Q are the probability distributions being compared, and M is the average of these distributions. The terms P (i) and Q(i) represent the probability of the i-th event in distributions P and Q respectively. \n\nJensen-Shannon (JS) Divergence is introduced as a crucial evaluation metric for LLM compression, offering a more nuanced understanding of how compression techniques impact model behavior than traditional metrics like perplexity. While perplexity focuses on next-token prediction confidence, JS Divergence quantifies the overall similarity between the output distributions of the original and compressed models. This makes it particularly valuable for evaluating methods like SparseGPT and Wanda, which aim to induce sparsity while preserving model functionality.",
            "score": 0.616500918326474,
            "section_title": "JS Divergence as a Evaluation Metric",
            "char_start_offset": 5946,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 194
                },
                {
                    "start": 195,
                    "end": 399
                },
                {
                    "start": 402,
                    "end": 506
                },
                {
                    "start": 507,
                    "end": 623
                },
                {
                    "start": 624,
                    "end": 825
                },
                {
                    "start": 828,
                    "end": 958
                },
                {
                    "start": 959,
                    "end": 1079
                },
                {
                    "start": 1080,
                    "end": 1255
                },
                {
                    "start": 1258,
                    "end": 1307
                },
                {
                    "start": 1310,
                    "end": 1378
                },
                {
                    "start": 1379,
                    "end": 1409
                },
                {
                    "start": 1412,
                    "end": 1520
                },
                {
                    "start": 1521,
                    "end": 1628
                },
                {
                    "start": 1631,
                    "end": 1859
                },
                {
                    "start": 1860,
                    "end": 2041
                },
                {
                    "start": 2042,
                    "end": 2193
                }
            ],
            "ref_mentions": [
                {
                    "start": 470,
                    "end": 474,
                    "matchedPaperCorpusId": "2238772"
                },
                {
                    "start": 929,
                    "end": 933,
                    "matchedPaperCorpusId": "204838007"
                },
                {
                    "start": 1175,
                    "end": 1178,
                    "matchedPaperCorpusId": "12121632"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.68408203125
        },
        {
            "corpus_id": "268041812",
            "title": "SparseLLM: Towards Global Pruning of Pre-trained Language Models",
            "text": "Local Pruning SparseLLM (Ours) To address these challenges and achieve global pruning with low memory consumption, we propose SparseLLM that decomposes the global pruning objective into multiple subproblems, each of which can be solved with low resources and coordinate to achieve the global pruning objective. More specifically, we first formulate LLMs as a composite function where the output of one module is the input of the next. Based on this formulation, we reformulate the global pruning goal into an equivalent form with auxiliary variables that facilitate its decomposition and coordination of the subproblems. Then we propose an alternating optimization algorithm to efficiently solve the subproblems, achieving computational resource efficiency and global optimality, due to the close-form solution of each subproblem. Empirically, we find that SparseLLM can consistently improve the performance of local pruning methods, particularly in high sparsity regimes (> 60%), where the perplexity can be significantly decreased by up to around 80% as compared to the state-of-the-art methods. \n\nFurthermore, our SparseLLM framework can be readily applicable to enhance the performance of most existing local pruning solvers, such as SparseGPT and Wanda, with marginal additional computational overhead. This adaptability ensures that our framework can be seamlessly integrated into a wide range of LLMs and pruning methods, making it a versatile tool and useful baseline for future research exploiting the sparsity of LLMs.",
            "score": 0.6163416240993027,
            "section_title": "Global Pruning",
            "char_start_offset": 2422,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 310
                },
                {
                    "start": 311,
                    "end": 434
                },
                {
                    "start": 435,
                    "end": 620
                },
                {
                    "start": 621,
                    "end": 830
                },
                {
                    "start": 831,
                    "end": 1097
                },
                {
                    "start": 1100,
                    "end": 1307
                },
                {
                    "start": 1308,
                    "end": 1528
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.8017578125
        },
        {
            "corpus_id": "273850564",
            "title": "Change Is the Only Constant: Dynamic LLM Slicing based on Layer Redundancy",
            "text": "Recent advancements in model compression techniques (Hoefler et al., 2021;Zhu et al., 2023) have markedly improved the efficiency of deploying LLMs while striving to retain their performance. The field has seen a variety of approaches including knowledge distillation (Hinton et al., 2015;Hsieh et al., 2023), quantization (Ma et al., 2024), pruning (Ma et al., 2023;Yang et al., 2024), low-rank adaptation (Hu et al., 2021) or hybrid variants (Xu et al., 2023;Dettmers et al., 2024), each designed to address the growing computational and memory requirements of these models. \n\nInnovative approaches such as LLM-Pruner (Ma et al., 2023) and LaCo (Layer Collapse) (Yang et al., 2024) offer novel perspectives on model pruning. LLM-Pruner focuses on structured pruning by identifying and removing dependency groups within the model, aiming to minimize dependency on the original training corpus while preserving linguistic capabilities. Similarly, LaCo presents a layer-wise pruning strategy where subsequent layers collapse into preceding ones, achieving notable size reduction while maintaining good performance. A third approach (Gromov et al., 2024) explores the potential of simple layer-pruning strategies combined with parameter-efficient finetuning (PEFT), demonstrating minimal performance loss even when half of the model's layers are removed. \n\nAmong the innovative strategies in LLM optimization, SliceGPT (Ashkboos et al., 2024) emerges as a significant breakthrough in model compression. Developed to address the intensive computational and memory demands of deploying LLMs, SliceGPT employs a unique post-training sparsification technique. Although effective in practice, previous research has illustrated that the order in which layers are removed plays a critical role in model performance (Gromov et al., 2024;Men et al., 2024). This insight led us to explore variable slicing percentages across different layers, challenging the constant slice for all layers.",
            "score": 0.6148446482803794,
            "section_title": "Related Work",
            "char_start_offset": 3164,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 191
                },
                {
                    "start": 192,
                    "end": 576
                },
                {
                    "start": 579,
                    "end": 726
                },
                {
                    "start": 727,
                    "end": 935
                },
                {
                    "start": 936,
                    "end": 1113
                },
                {
                    "start": 1114,
                    "end": 1352
                },
                {
                    "start": 1355,
                    "end": 1500
                },
                {
                    "start": 1501,
                    "end": 1653
                },
                {
                    "start": 1654,
                    "end": 1845
                },
                {
                    "start": 1846,
                    "end": 1977
                }
            ],
            "ref_mentions": [
                {
                    "start": 52,
                    "end": 74,
                    "matchedPaperCorpusId": "231740691"
                },
                {
                    "start": 289,
                    "end": 308,
                    "matchedPaperCorpusId": "258461606"
                },
                {
                    "start": 461,
                    "end": 483,
                    "matchedPaperCorpusId": "258841328"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.75927734375
        },
        {
            "corpus_id": "273654967",
            "title": "A Survey of Small Language Models",
            "text": "Weight pruning is a model optimization technique that reduces the number of parameters to enhance computational efficiency and lower memory usage, all while maintaining performance levels. We differentiate between two major approaches for pruning: unstructured pruning and structured pruning. \n\nUnstructured pruning removes less significant individual weights, offering fine-grained control and flexibility in reducing model size. For example, to perform irregular pruning on large language models, SparseGPT (Frantar and Alistarh, 2023) reformulates the pruning task as a sparse regression problem, optimizing both the remaining and pruned weights using a layer-wise approximate regression solver. SparseGPT can efficiently handle large-scale models like OPT-175B and BLOOM-176B. Additionally, (Bo\u017ea, 2024) integrates the ADMM (Boyd et al., 2011) algorithm for weight updates to further mitigate pruning errors. Wanda (Sun et al., 2023) incorporates both weights and activations into consideration during pruning process, and eliminates the need of weight updates. The n:m pruning strategy (Zhou et al., 2021) brings unstructured pruning to model acceleration by pruning exactly n weights out of every m, balancing pruning flexibility and computational efficiency for significant speedups. NVIDIA's Ten-sorRT leverages such sparse patterns to optimize memory access and reduce computational loads, accelerating inference on GPUs, particularly hardware like the A100. Notably, unstructured pruning often results in sparse matrices requiring specialized hardware or algorithms to maximize computational benefits (Frantar and Alistarh, 2023). \n\nStructured pruning (Wang et al., 2020b;Santacroce et al., 2023;Ma et al., 2023;Tao et al., 2023;Xia et al., 2024;Kurti\u0107 et al., 2024) aims to compress LLMs while maintaining performance by removing groups of parameters in a structured manner, which enables more efficient hardware implementation. A major direction in this approach concerns the sparsity of neurons in the model. For instance, Li et al. (2023b) observes prevalent sparsity in feed-forward networks.",
            "score": 0.614777233012381,
            "section_title": "Pruning Techniques",
            "char_start_offset": 17759,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 188
                },
                {
                    "start": 189,
                    "end": 292
                },
                {
                    "start": 295,
                    "end": 430
                },
                {
                    "start": 431,
                    "end": 698
                },
                {
                    "start": 699,
                    "end": 780
                },
                {
                    "start": 781,
                    "end": 912
                },
                {
                    "start": 913,
                    "end": 1065
                },
                {
                    "start": 1066,
                    "end": 1290
                },
                {
                    "start": 1291,
                    "end": 1467
                },
                {
                    "start": 1468,
                    "end": 1640
                },
                {
                    "start": 1643,
                    "end": 1939
                },
                {
                    "start": 1940,
                    "end": 2021
                },
                {
                    "start": 2022,
                    "end": 2107
                }
            ],
            "ref_mentions": [
                {
                    "start": 509,
                    "end": 537,
                    "matchedPaperCorpusId": "253237200"
                },
                {
                    "start": 1611,
                    "end": 1639,
                    "matchedPaperCorpusId": "253237200"
                },
                {
                    "start": 1662,
                    "end": 1682,
                    "matchedPaperCorpusId": "204009154"
                },
                {
                    "start": 1682,
                    "end": 1706,
                    "matchedPaperCorpusId": "256662734"
                },
                {
                    "start": 1722,
                    "end": 1739,
                    "matchedPaperCorpusId": "259858812"
                },
                {
                    "start": 1739,
                    "end": 1756,
                    "matchedPaperCorpusId": "263830786"
                },
                {
                    "start": 1756,
                    "end": 1776,
                    "matchedPaperCorpusId": "256662263"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.61865234375
        },
        {
            "corpus_id": "273549773",
            "title": "LEGO: Language Model Building Blocks",
            "text": "As discussed in the Background section, there are two pruning techniques that dominate the literature. We test both SparseGPT and Wanda and analyze the best pruning technique to use. The results in table 3 show that SparseGPT produces more robust models on average, with a significant advantage at higher levels of sparsity. However, SparseGPT is more computationally expensive when pruning, while Wanda is computationally inexpensive. \n\nThis provides us a few insights. The first is that regardless of pruning strategy, performance degrades significantly beyond 50% sparsity. The second is that while more computationally expensive, SparseGPT may be necessary at high sparsity levels or more resource constrained client devices, as it not only produced a more robust model, but the increase in performance due to fine-tuning was almost double that of Wanda. \n\nGiven these insights, the superior pruning method depends on the use case scenario. If we are defining rigid model sizes and assert that client devices will be initialized with one of these 'default' model sizes, then SparseGPT would be superior. This is especially true given our compute budget is capable of fine-tuning LLMs and performing inference, since SparseGPT is relatively cheap compared to those tasks if not being performed for ever device initialization. Thus, we can use SparseGPT to generate various model sizes/sparsity's before the FL process begins, and assign models accordingly. \n\nHowever, in practice, creating a methodology to calculate the ideal model size given the device's compute budget would return more robust client models for users in the FL system. In this scenario, when a client is initialized, a model would be pruned according to their compute budget, meaning a lightweight process like Wanda would be superior. \n\nHowever it is worth noting that, with the exception of high sparsity scenarios, the difference between the two pruning method's performances is negligible. Therefore, our results should be generalizable to both pruning methods. \n\nAdditionally, as pruning methods continue to evolve, the performance of pruned models will improve. Therefore its important evaluate model performance in our experiments with the limitations of current pruning techniques, but as pruning techniques improve, our methodologies and results would generalize to them and should scale accordingly.",
            "score": 0.6140004755789958,
            "section_title": "A.1 Comparison of Pruning Methods",
            "char_start_offset": 26753,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 102
                },
                {
                    "start": 103,
                    "end": 182
                },
                {
                    "start": 183,
                    "end": 324
                },
                {
                    "start": 325,
                    "end": 435
                },
                {
                    "start": 438,
                    "end": 470
                },
                {
                    "start": 471,
                    "end": 576
                },
                {
                    "start": 577,
                    "end": 858
                },
                {
                    "start": 861,
                    "end": 944
                },
                {
                    "start": 945,
                    "end": 1107
                },
                {
                    "start": 1108,
                    "end": 1328
                },
                {
                    "start": 1329,
                    "end": 1459
                },
                {
                    "start": 1462,
                    "end": 1641
                },
                {
                    "start": 1642,
                    "end": 1808
                },
                {
                    "start": 1811,
                    "end": 1966
                },
                {
                    "start": 1967,
                    "end": 2038
                },
                {
                    "start": 2041,
                    "end": 2140
                },
                {
                    "start": 2141,
                    "end": 2382
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.60009765625
        },
        {
            "corpus_id": "270257857",
            "title": "Pruner-Zero: Evolving Symbolic Pruning Metric from scratch for Large Language Models",
            "text": "Model sparsing is one of the most promising techniques for model deployment due to its flexibility.Previous research works on model sparsing require training from random initialization (Hoang et al., 2023;Sreenivasan et al., 2022;Louizos et al., 2018), retraining process (Liu et al., 2019;Chen et al., 2023) or extensive iterative pruning (Chijiwa et al., 2021;Tanaka et al., 2020).However, the inherent complexities, along with the substantial computational and data requirements of LLMs, present significant challenges that render these conventional sparse strategies less feasible.\n\nGiven the extensive data corpus and substantial model dimensions required by LLMs, post-training pruning has become an increasingly crucial methodology.Due to its minimal resource demands, this approach is highly advantageous, offering a cost-efficient alternative for optimizing LLMs.The development of the post-training pruning method, as highlighted in recent studies (Lu et al., 2022;Frantar & Alistarh, 2023;Sun et al., 2024), marks a significant advancement in this field.These methods streamline the pruning process, further reducing the resource requirements and making LLMs more democratized and accessible.SparseGPT (Frantar & Alistarh, 2023) is proposed to con-Table 1.The existing pruning metrics tailored for LLMs.\"W\" denotes weight update, and \"C\" denotes the calibration data.\"UOP\" denotes unary operations, and \"BOP\" denotes binary operations.\u03c3 denotes the min-max scaling operation.",
            "score": 0.6135858948197795,
            "section_title": "Introduction",
            "char_start_offset": 1721,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 99
                },
                {
                    "start": 99,
                    "end": 383
                },
                {
                    "start": 383,
                    "end": 585
                },
                {
                    "start": 587,
                    "end": 739
                },
                {
                    "start": 739,
                    "end": 872
                },
                {
                    "start": 872,
                    "end": 1065
                },
                {
                    "start": 1065,
                    "end": 1203
                },
                {
                    "start": 1203,
                    "end": 1267
                },
                {
                    "start": 1267,
                    "end": 1314
                },
                {
                    "start": 1314,
                    "end": 1378
                },
                {
                    "start": 1378,
                    "end": 1446
                },
                {
                    "start": 1446,
                    "end": 1486
                }
            ],
            "ref_mentions": [
                {
                    "start": 205,
                    "end": 230,
                    "matchedPaperCorpusId": "247084008"
                },
                {
                    "start": 290,
                    "end": 308,
                    "matchedPaperCorpusId": "257495957"
                },
                {
                    "start": 340,
                    "end": 362,
                    "matchedPaperCorpusId": "235458499"
                },
                {
                    "start": 362,
                    "end": 382,
                    "matchedPaperCorpusId": "219558821"
                },
                {
                    "start": 958,
                    "end": 975,
                    "matchedPaperCorpusId": "251648051"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.473876953125
        },
        {
            "corpus_id": "259203115",
            "title": "A Simple and Effective Pruning Approach for Large Language Models",
            "text": "We empirically evaluate Wanda on the widely adopted LLaMA (Touvron et al., 2023a) and LLaMA-2 (Touvron et al., 2023b) model families. Our results demonstrate Wanda can find efficient sparse networks from pretrained LLMs, without any retraining or weight update. Our approach Wanda outperforms the standard magnitude pruning by a large margin and also competes favorably with the prior best LLM pruning method (Frantar & Alistarh, 2023), while requiring a lower computational cost. We hope our work serves as a baseline for future work in this area, and encourages further exploration in understanding sparsity in LLMs.",
            "score": 0.61281907437708,
            "section_title": "Weights and activations",
            "char_start_offset": 4400,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 133
                },
                {
                    "start": 134,
                    "end": 261
                },
                {
                    "start": 262,
                    "end": 480
                },
                {
                    "start": 481,
                    "end": 618
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.873046875
        },
        {
            "corpus_id": "255372747",
            "title": "SparseGPT: Massive Language Models Can Be Accurately Pruned in One-Shot",
            "text": "Large Language Models (LLMs) from the Generative Pretrained Transformer (GPT) family have shown remarkable performance on a wide range of tasks, but are difficult to deploy because of their massive size and computational costs. For illustration, the top-performing GPT-175B models have 175 billion parameters, which total at least 320GB (counting multiples of 1024) of storage in half-precision (FP16) format, leading it to require at least five A100 GPUs with 80GB of memory each for inference. It is therefore natural that there has been significant interest in reducing these costs via model compression. To date, virtually all existing GPT compression approaches have focused on quantization (Dettmers et al., 2022;Yao et al., 2022;Xiao et al., 2022;Frantar et al., 2022a), that is, reducing the precision of the model's numerical representation. \n\nA complementary approach for compression is pruning, which removes network elements, from individual weights (unstructured pruning) to higher-granularity structures such as rows/columns of the weight matrices (structured pruning). 1 Institute of Science and Technology Austria (ISTA) 2 Neural Magic Inc. Corresponding author: elias.frantar@ist.ac.at Pruning has a long history (LeCun et al., 1989;Hassibi et al., 1993), and has been applied successfully in the case of vision and smaller-scale language models (Hoefler et al., 2021). Yet, the best-performing pruning methods require extensive retraining of the model to recover accuracy. In turn, this is extremely expensive for GPT-scale models. While some accurate one-shot pruning methods exist (Hubara et al., 2021a;Frantar et al., 2022b), compressing the model without retraining, unfortunately even they become very expensive when applied to models with billions of parameters. Thus, to date, there is essentially no work on accurate pruning of billion-parameter models. \n\nOverview.",
            "score": 0.6123199503285701,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 227
                },
                {
                    "start": 228,
                    "end": 495
                },
                {
                    "start": 496,
                    "end": 607
                },
                {
                    "start": 608,
                    "end": 850
                },
                {
                    "start": 853,
                    "end": 1085
                },
                {
                    "start": 1086,
                    "end": 1202
                },
                {
                    "start": 1203,
                    "end": 1386
                },
                {
                    "start": 1387,
                    "end": 1490
                },
                {
                    "start": 1491,
                    "end": 1549
                },
                {
                    "start": 1550,
                    "end": 1786
                },
                {
                    "start": 1787,
                    "end": 1879
                },
                {
                    "start": 1882,
                    "end": 1891
                }
            ],
            "ref_mentions": [
                {
                    "start": 1230,
                    "end": 1250,
                    "matchedPaperCorpusId": "7785881"
                },
                {
                    "start": 1250,
                    "end": 1271,
                    "matchedPaperCorpusId": "61815367"
                },
                {
                    "start": 1601,
                    "end": 1623,
                    "matchedPaperCorpusId": "231934142"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.59423828125
        },
        {
            "corpus_id": "270391791",
            "title": "ALPS: Improved Optimization for Highly Sparse One-Shot Pruning for Large Language Models",
            "text": "This section compares our proposed framework, ALPS, with state-of-the-art unstructured pruning methods for LLMs. Detailed information on the experimental setup and reproducibility is provided in Appendix B.1, while additional results are presented in Appendix B.2. Models and datasets. We evaluate the performance of ALPS on the OPT model family [Zhang et al., 2022b] with sizes ranging from 1.3 billion to 30 billion parameters and the LLaMA2 model family [Touvron et al., 2023] with 7 billion and 13 billion parameters. Following the approach of Frantar and Alistarh, 2023, we use 128 segments of 2048 tokens each, randomly selected from the first shard of the C4 dataset [Raffel et al., 2020], as calibration data. We assess the performance using perplexity and zero-shot evaluation benchmarks, with perplexity calculated according to the procedure described by HuggingFace [Per, 2022], using full stride. The test sets of raw-WikiText2 [Merity et al., 2017], PTB [Marcus et al., 1994], and a subset of the C4 validation data, which are popular benchmarks in LLM pruning literature [Yao et al., 2022, Xiao et al., 2023, Meng et al., 2024b], are used for evaluation. Additionally, we consider four zero-shot tasks: PIQA [Bisk et al., 2020], LAMBADA [Paperno et al., 2016], ARC-Easy, and ARC-Challenge [Clark et al., 2018]. \n\nCompeting methods. We compare ALPS with several one-shot pruning methods for LLMs, including (i) Magnitude Pruning (MP, [Han et al., 2015]), (ii) SparseGPT [Frantar and Alistarh, 2023], (iii) Wanda Sun et al. [2023], and (iv) DSnoT [Zhang et al., 2023]. We first evaluate the performance of our proposed ALPS framework on a single layer.",
            "score": 0.6119419655206891,
            "section_title": "Experimental Results",
            "char_start_offset": 19666,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 112
                },
                {
                    "start": 113,
                    "end": 285
                },
                {
                    "start": 286,
                    "end": 521
                },
                {
                    "start": 522,
                    "end": 717
                },
                {
                    "start": 718,
                    "end": 908
                },
                {
                    "start": 909,
                    "end": 1168
                },
                {
                    "start": 1169,
                    "end": 1324
                },
                {
                    "start": 1327,
                    "end": 1345
                },
                {
                    "start": 1346,
                    "end": 1580
                },
                {
                    "start": 1581,
                    "end": 1664
                }
            ],
            "ref_mentions": [
                {
                    "start": 674,
                    "end": 695,
                    "matchedPaperCorpusId": "204838007"
                },
                {
                    "start": 940,
                    "end": 961,
                    "matchedPaperCorpusId": "16299141"
                },
                {
                    "start": 967,
                    "end": 988,
                    "matchedPaperCorpusId": "5151364"
                },
                {
                    "start": 1085,
                    "end": 1102,
                    "matchedPaperCorpusId": "249395624"
                },
                {
                    "start": 1102,
                    "end": 1121,
                    "matchedPaperCorpusId": "253708271"
                },
                {
                    "start": 1222,
                    "end": 1241,
                    "matchedPaperCorpusId": "208290939"
                },
                {
                    "start": 1447,
                    "end": 1465,
                    "matchedPaperCorpusId": "2238772"
                },
                {
                    "start": 1483,
                    "end": 1511,
                    "matchedPaperCorpusId": "251765570"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.374267578125
        },
        {
            "corpus_id": "268032346",
            "title": "BESA: Pruning Large Language Models with Blockwise Parameter-Efficient Sparsity Allocation",
            "text": "Large language models (LLMs) have demonstrated outstanding performance in various tasks, such as text summarization, text question-answering, and etc. While their performance is impressive, the computational footprint due to their vast number of parameters can be prohibitive. Existing solutions such as SparseGPT and Wanda attempt to alleviate this issue through weight pruning. However, their layer-wise approach results in significant perturbation to the model's output and requires meticulous hyperparameter tuning, such as the pruning rate, which can adversely affect overall model performance. To address this, this paper introduces a novel LLM pruning technique dubbed blockwise parameter-efficient sparsity allocation (BESA) by applying a blockwise reconstruction loss. In contrast to the typical layer-wise pruning techniques, BESA is characterized by two distinctive attributes: i) it targets the overall pruning error with respect to individual transformer blocks, and ii) it allocates layer-specific sparsity in a differentiable manner, both of which ensure reduced performance degradation after pruning. Our experiments show that BESA achieves state-of-the-art performance, efficiently pruning LLMs like LLaMA1, and LLaMA2 with 7B to 70B parameters on a single A100 GPU in just five hours. Code is available at https://github.com/OpenGVLab/LLMPrune-BESA.",
            "score": 0.6117203553908974,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.84326171875
        },
        {
            "corpus_id": "278327238",
            "title": "Efficient Shapley Value-based Non-Uniform Pruning of Large Language Models",
            "text": "We evaluate PPL using the WikiText-2 dataset [39], selecting two randomly downloaded subsets of WikiText-2 for testing. For zero-shot evaluation, we choose 4 tasks-BoolQ, RTE, WinoGrande (WG), and OpenBookQA (OBQA)-from the EleutherAI LM Harness [40], where higher scores indicate better performance. Detailed settings are shown in Appendix B. \n\nPPL Improvement by SV-NUP. We first evaluate the PPL performance gains achieved by integrating SV-NUP into LLM pruning methods, as shown in Table 1. SV-NUP significantly enhances existing pruning approaches, particularly Magnitude and Wanda, while SparseGPT demonstrates minimal improvements due to its already strong baseline performance. SV-NUP has 61.28 % improvement on LLaMA-13B by Wanda. Wanda consistently benefits from SV-NUP across all LLaMA models, with the largest impact observed in larger models like LLaMA-13B, where SV-NUP effectively mitigates performance degradation. SparseGPT, despite its high baseline effectiveness, achieves further refinement with SV-NUP, highlighting the compatibility and synergy between the two. Overall, SV-NUP emerges as a crucial enhancement, stabilizing pruning outcomes and delivering superior results, especially when combined with advanced methods like SparseGPT. \n\nPPL and Zero-shot Performance. The results in Table 2 underscore the effectiveness of integrating SV-NUP, OWL, and ALS into existing pruning methods for the LLaMA and OPT model families at 50% sparsity. Particularly, as ALS does not release its source code, the results of ALS are not fair and are for reference only. In most scenarios, SV-NUP consistently demonstrates superior performance. Specifically, Magnitude combined with SV-NUP consistently outperforms both its standalone version and Magnitude with OWL. Notably, while SV-NUP slightly improves the performance of Wanda and SparseGPT, the gains are less pronounced, as the pruned LLMs at 50% sparsity already exhibit robust performance, leaving limited room for further enhancements.",
            "score": 0.6116504266894384,
            "section_title": "Experimental Evalution",
            "char_start_offset": 18360,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 119
                },
                {
                    "start": 120,
                    "end": 300
                },
                {
                    "start": 301,
                    "end": 343
                },
                {
                    "start": 346,
                    "end": 372
                },
                {
                    "start": 373,
                    "end": 494
                },
                {
                    "start": 495,
                    "end": 685
                },
                {
                    "start": 686,
                    "end": 739
                },
                {
                    "start": 740,
                    "end": 930
                },
                {
                    "start": 931,
                    "end": 1083
                },
                {
                    "start": 1084,
                    "end": 1258
                },
                {
                    "start": 1261,
                    "end": 1291
                },
                {
                    "start": 1292,
                    "end": 1463
                },
                {
                    "start": 1464,
                    "end": 1578
                },
                {
                    "start": 1579,
                    "end": 1652
                },
                {
                    "start": 1653,
                    "end": 1774
                },
                {
                    "start": 1775,
                    "end": 2003
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.58203125
        },
        {
            "corpus_id": "271909626",
            "title": "Fine-Tuning and Deploying Large Language Models Over Edges: Issues and Approaches",
            "text": "Notwithstanding the compression capability, the knowledge distillation usually consumes a long duration to obtain a student model (e.g., 14 GPU days for distilling TinyBERT [13]) that works for specific tasks. In order to reduce the training duration, the model pruning technology can be used to remove unnecessary neurons in the LLMs while retaining the versatility of the pruned LLMs. In this vein, X. Ma et al. propose a task-agnostic pruner (named as, the LLM-Pruner) to preserve the capability to handle various task without requiring original training dataset and long duration  of retraining. Since the LLMs contain redundant parameters that have little/no effects on the performance of models, the LLM-Pruner reduces the scale of LLMs based on the gradient information. More specifically, the LLM-Pruner incorporates three main stages: discovery, estimation, and recovery. The detailed functionalities of the LLM-Pruner are as follows. \n\n\u2022 Discovery stage involves finding dependencies within the model such that the pruned neurons do not disproportionately affect others. \u2022 Estimation stage evaluates the importance of each identified group of neurons based on their contribution to the overall performance that can be estimated via the firstorder and the approximated second-order derivatives. \u2022 Recovery stage leverages a fast low-rank approximation method to reduce the required duration and data for performance recovery. After the above three stages, the performance pruned models can be quickly retained using the LoRA technique with a minimal dataset. Their numerical results also demonstrate that the LLM-Pruner can effectively reduce the model size and computational demands without significantly sacrificing performance. Even after pruning 20% of the parameters, the pruned models retain over 94% of the original performance.",
            "score": 0.6114761562979609,
            "section_title": "B. Compression-Then-Train",
            "char_start_offset": 22469,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 209
                },
                {
                    "start": 210,
                    "end": 386
                },
                {
                    "start": 387,
                    "end": 599
                },
                {
                    "start": 600,
                    "end": 777
                },
                {
                    "start": 778,
                    "end": 880
                },
                {
                    "start": 881,
                    "end": 943
                },
                {
                    "start": 946,
                    "end": 1080
                },
                {
                    "start": 1081,
                    "end": 1303
                },
                {
                    "start": 1304,
                    "end": 1434
                },
                {
                    "start": 1435,
                    "end": 1567
                },
                {
                    "start": 1568,
                    "end": 1739
                },
                {
                    "start": 1740,
                    "end": 1844
                }
            ],
            "ref_mentions": [
                {
                    "start": 173,
                    "end": 177,
                    "matchedPaperCorpusId": "258823276"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.9091796875
        },
        {
            "corpus_id": "269791108",
            "title": "When Large Language Model Meets Optimization",
            "text": "Structural pruning optimizes large language models (LLMs) by selectively removing non-critical coupled structures based on gradient information, effectively reducing model size while preserving functionality and ensuring taskagnosticism.Structural pruning is an essential optimization technique used to enhance pre-trained LLMs for subsequent tasks, such as text categorization and sentiment analysis.Structural pruning, as suggested by Klein [59], seeks to uncover numerous subnetworks of LLMs that achieve a compromise between performance and size, making them easier to use in different real-world applications.This approach employs a multi-objective local search algorithm to identify numerous Pareto-optimal subnetworks effectively.It does this by minimizing evaluation costs via weight sharing.Ma et al. [87] propose the LLM-Pruner approach to optimize large language models by selectively removing non-critical coupling structures based on gradient information to achieve efficient compression while preserving functionality and task agnosticism.Gholami et al. [44] demonstrate that weight pruning can be used as an optimisation strategy for the Transfer architecture, proving that judicious pruning can significantly reduce model size without sacrificing performance, thus contributing to bridging the gap between model efficiency and performance.",
            "score": 0.6109725805490331,
            "section_title": "Based on structural pruning",
            "char_start_offset": 51118,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 237
                },
                {
                    "start": 237,
                    "end": 401
                },
                {
                    "start": 401,
                    "end": 614
                },
                {
                    "start": 614,
                    "end": 737
                },
                {
                    "start": 737,
                    "end": 800
                },
                {
                    "start": 800,
                    "end": 1053
                },
                {
                    "start": 1053,
                    "end": 1355
                }
            ],
            "ref_mentions": [
                {
                    "start": 810,
                    "end": 814,
                    "matchedPaperCorpusId": "258823276"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.8603515625
        },
        {
            "corpus_id": "277622258",
            "title": "Entropy-Based Block Pruning for Efficient Large Language Models",
            "text": "To evaluate the efficiency gains from pruning attention blocks, we conduct inference speed tests on Llama3.1-8B and Mistral-7B-v0.3. We prune attention layers progressively and measure both model performance and inference time. The speed test is performed by fixing the input sequence length to 1024 tokens and generating an output of 1024 tokens. Each experiment is repeated 10 times, and the average inference time is reported. \n\nFigure 8 shows the relationship between the number of dropped attention layers, model performance, and inference time. The results indicate that inference time decreases linearly as more attention layers are pruned. Notably, the first 12 layers provide the most significant speedup while maintaining model performance. Beyond this point, additional pruning begins to negatively impact accuracy. EntroDrop based on Bucket/KNN estimation outperforms currently widely used cosine similarity. Overall, these results highlight that our method can achieve substantial computational savings while preserving accuracy, making it an effective strategy for accelerating large language models in realworld deployment scenarios. In the era of large language models (LLMs), various methods have been proposed to reduce model size and accelerate inference (Frantar et al., 2022;Lin et al., 2024;Xiao et al., 2023;Shao et al., 2023;Zhu et al., 2023;Xu et al., 2023;Dettmers et al., 2023;Liu et al., 2023). Recent advances focus on post-training pruning techniques that eliminate redundant parameters or structures. \n\nSparseGPT (Frantar and Alistarh, 2023) leverages second-order information to identify unimportant parameters in LLMs. Wanda (Sun et al., 2023) introduces a pruning matrix that considers both weight magnitude and corresponding input activations. NEPENTHE (Liao et al., 2024) introduces a method that utilizes entropy to identify and remove low-entropy layers in deep neural networks, effectively reducing model depth while maintaining performance. E-Sparse (Li et al., 2023) introduces an entropy-based pruning method that enhances inference speed and reduces memory usage in large language models by leveraging information rich- ness to guide N:M sparsity.",
            "score": 0.6108304900282542,
            "section_title": "Speedup Test",
            "char_start_offset": 18948,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 132
                },
                {
                    "start": 133,
                    "end": 227
                },
                {
                    "start": 228,
                    "end": 347
                },
                {
                    "start": 348,
                    "end": 429
                },
                {
                    "start": 432,
                    "end": 550
                },
                {
                    "start": 551,
                    "end": 647
                },
                {
                    "start": 648,
                    "end": 750
                },
                {
                    "start": 751,
                    "end": 826
                },
                {
                    "start": 827,
                    "end": 920
                },
                {
                    "start": 921,
                    "end": 1148
                },
                {
                    "start": 1149,
                    "end": 1422
                },
                {
                    "start": 1423,
                    "end": 1531
                },
                {
                    "start": 1534,
                    "end": 1651
                },
                {
                    "start": 1652,
                    "end": 1778
                },
                {
                    "start": 1779,
                    "end": 1980
                },
                {
                    "start": 1981,
                    "end": 2190
                }
            ],
            "ref_mentions": [
                {
                    "start": 1296,
                    "end": 1313,
                    "matchedPaperCorpusId": "258999941"
                },
                {
                    "start": 1313,
                    "end": 1331,
                    "matchedPaperCorpusId": "253708271"
                },
                {
                    "start": 1544,
                    "end": 1571,
                    "matchedPaperCorpusId": "255372747"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.78857421875
        },
        {
            "corpus_id": "277272730",
            "title": "Maximum Redundancy Pruning: A Principle-Driven Layerwise Sparsity Allocation for LLMs",
            "text": "To comprehensively investigate whether pruning LLMs requires differentiated treatment across individual layers, we analyzed the LPS of the model under various settings, including architecture (LLaMA2-(7B, 13B), OPT-6.7B, VICUNA-7B [Zheng et al., 2023], Mixtral-56B [Jiang et al., 2024], and LLaVA-7B [Liu et al., 2024]), pruning metrics (Magnitude and Wanda), pruning granularity (unstructured, semistructured, and structured), and tasks (language model and zero-shot tasks). If the LPS of the model exhibits a nonuniform pattern, it implies that pruning LLMs should adopt non-uniform layerwise sparsity ratios, and vice versa. Results: The LPS of LLMs exhibits a highly nonuniform pattern across layers. The complete experimental results are provided in Appendix A, with a summary of the results on the language modeling task (WikiText2) presented in Table 1. In Table 1, LPS is considered non-uniform when the maximum sensitivity exceeds twice the minimum sensitivity. Table 1 illustrates that LPS is non-uniform in all settings, with the maximum sensitivity being thousands of times larger than the minimum in some cases. Additionally, this disparity grows progressively as the sparsity level increases. \n\nAdditionally, we present results on zero-shot tasks in Table 2, where the LPS similarly demonstrates non-uniform behavior. The observed non-uniformity reflects the varying importance of layers in LLMs. Therefore, uniform pruning may degrade performance, especially in the case of high sparsity. Differentiated pruning strategies, which assign different sparsity ratios to layers, are crucial for preserving accuracy while increasing sparsity. \n\nIn addition to the standard architectures discussed above, we also conducted experiments on specialized architectures, including Mixtral-56B (an MoE model) and LLaVA-7B (a multimodal model). For Mixtral-56B, the LPS is nonuniform, and the gap between the maximum and minimum sensitivity increases as the sparsity rises.",
            "score": 0.6106252140702115,
            "section_title": "Empirical Study I: LLMs vs. LPS",
            "char_start_offset": 8484,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 220
                },
                {
                    "start": 221,
                    "end": 475
                },
                {
                    "start": 476,
                    "end": 627
                },
                {
                    "start": 628,
                    "end": 704
                },
                {
                    "start": 705,
                    "end": 860
                },
                {
                    "start": 861,
                    "end": 970
                },
                {
                    "start": 971,
                    "end": 1124
                },
                {
                    "start": 1125,
                    "end": 1206
                },
                {
                    "start": 1209,
                    "end": 1331
                },
                {
                    "start": 1332,
                    "end": 1410
                },
                {
                    "start": 1411,
                    "end": 1503
                },
                {
                    "start": 1504,
                    "end": 1651
                },
                {
                    "start": 1654,
                    "end": 1844
                },
                {
                    "start": 1845,
                    "end": 1973
                }
            ],
            "ref_mentions": [
                {
                    "start": 231,
                    "end": 251,
                    "matchedPaperCorpusId": "259129398"
                },
                {
                    "start": 265,
                    "end": 285,
                    "matchedPaperCorpusId": "276288500"
                },
                {
                    "start": 300,
                    "end": 318,
                    "matchedPaperCorpusId": "49310977"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.354736328125
        },
        {
            "corpus_id": "273374936",
            "title": "DISP-LLM: Dimension-Independent Structural Pruning for Large Language Models",
            "text": "In Table 1, we report the perplexity of pruned OPT and LLaMA-2 models. Our DISP-LLM, which does not update weights, consistently outperforms more complex pruning methods such as K-OBD and LLM Surgeon, which involve weight updates, across all pruning ratios and models. The   performance gap is even larger when compared to SliceGPT. The advantage is particularly clear in better-trained models like LLaMA-2 7B and 13B. For instance, our method surpasses LLM Surgeon by margins of 5.54 and 2.22 when pruning 50% of parameters of LLaMA-2 7B and 13B, respectively. Against K-OBD, our performance advantage extends to 36.80 and 9.49 under the same setting. For consistency, we let the pruning ratio of SliceGPT equal the slicing ratio. However, the actual pruning ratio for SliceGPT is much lower than the slicing ratio. More details are given in Appendix A.5. \n\nIn Table 2, we report the perplexity of pruned LLaMA and LLaMA-2 models and we compare our method with semi-structure pruning methods. From the table, we can see that our method outperforms both SparseGPT and Wanda on LLaMA 13B and LLaMA-2 7B/13B models. Our method performs on par with SparseGPT and Wanda with the LLaMA 7B model, and our DISP-LLM is a little bit worse than SparseGPT and is similar to Wanda. We are the first to show that structural pruning methods can have a better or similar performance than semi-structural pruning methods.",
            "score": 0.6105595522679665,
            "section_title": "Language Modeling",
            "char_start_offset": 20799,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 70
                },
                {
                    "start": 71,
                    "end": 268
                },
                {
                    "start": 269,
                    "end": 332
                },
                {
                    "start": 333,
                    "end": 418
                },
                {
                    "start": 419,
                    "end": 561
                },
                {
                    "start": 562,
                    "end": 652
                },
                {
                    "start": 653,
                    "end": 731
                },
                {
                    "start": 732,
                    "end": 816
                },
                {
                    "start": 817,
                    "end": 856
                },
                {
                    "start": 859,
                    "end": 993
                },
                {
                    "start": 994,
                    "end": 1113
                },
                {
                    "start": 1114,
                    "end": 1269
                },
                {
                    "start": 1270,
                    "end": 1405
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.66455078125
        },
        {
            "corpus_id": "259203115",
            "title": "A Simple and Effective Pruning Approach for Large Language Models",
            "text": "A recent LLM pruning approach, SparseGPT (Frantar & Alistarh, 2023), does not require traditional retraining, but still demands a computationally intensive weight update process. \n\nThe argument concerning the need for retraining and weight update does not fully capture the challenges of pruning LLMs. One might reasonably expect to obtain a fairly high-performing initialization point for retraining using existing popular pruning methods. However, a recent study (Frantar & -3 0 0 2",
            "score": 0.610495974795454,
            "section_title": "INTRODUCTION",
            "char_start_offset": 1778,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 178
                },
                {
                    "start": 181,
                    "end": 301
                },
                {
                    "start": 302,
                    "end": 440
                },
                {
                    "start": 441,
                    "end": 484
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.67578125
        },
        {
            "corpus_id": "269773295",
            "title": "Dynamic Activation Pitfalls in LLaMA Models: An Empirical Study",
            "text": "Large Language Models (LLMs), due to their immense parameter size, require substantial computations during both training and inference phases.Consequently, reducing computation and inference latency while maintaining model performance has emerged as a critical research direction.To address this challenge, various sparsity techniques have been proposed to decrease the resource requirements of models during runtime by reducing the number of parameters or computations involved.\n\nSparsity techniques typically encompass two types: static and dynamic.Static sparsity techniques compress the model in post-training phase by pruning a portion of the weights.The downside of this approach is that once the pruning is complete, the pruned parts cannot be recovered, which may lead to a degradation in model performance.Recent advances in static sparsity have been marked by works such as Wanda [1], SparseGPT [2], and LoRAShear [3].These works have pioneered new pruning metrics, more efficient pruning process, or have targeted different components to achieve higher levels of sparsity in LLMs.By doing so, they have managed to maintain a relatively low performance degradation at a reduced cost, setting a new benchmark for model efficiency in the field.Dynamic sparsity techniques represent a paradigm shift in model efficiency optimization, particularly when contrasted with traditional static counterparts.Where static approaches maintain a fixed set of active parameters or computational units throughout the inference process, dynamic sparsity techniques introduce a level of adaptability that is contingent upon the input data.By dynamically selecting which parameters or units to be activated during model inference, these techniques can tailor the computational load to the unique characteristics of each input, thus achieving heightened computational efficiency.\n\nMirzadeh et al. [4] elucidates the capacity of the ReLU activation function to introduce sparsity and proposes the concept of dynamic activation.Empirical studies in this work have demonstrated that the choice of activation function does not significantly affect accuracy since GeLU, SiLU, and ReLU all perform with comparable precision.However, ReLU can save approximately 30% of computational resources by introducing sparsity, hence the paper advocates for a resurgence in the use of ReLU activation functions in LLMs.",
            "score": 0.6091357558077769,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 142
                },
                {
                    "start": 142,
                    "end": 280
                },
                {
                    "start": 280,
                    "end": 479
                },
                {
                    "start": 481,
                    "end": 551
                },
                {
                    "start": 551,
                    "end": 656
                },
                {
                    "start": 656,
                    "end": 815
                },
                {
                    "start": 815,
                    "end": 928
                },
                {
                    "start": 928,
                    "end": 1091
                },
                {
                    "start": 1091,
                    "end": 1252
                },
                {
                    "start": 1252,
                    "end": 1407
                },
                {
                    "start": 1407,
                    "end": 1631
                },
                {
                    "start": 1631,
                    "end": 1869
                },
                {
                    "start": 1871,
                    "end": 2016
                },
                {
                    "start": 2016,
                    "end": 2208
                },
                {
                    "start": 2208,
                    "end": 2392
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.73193359375
        },
        {
            "corpus_id": "264439471",
            "title": "E-Sparse: Boosting the Large Language Model Inference through Entropy-based N: M Sparsity",
            "text": "To demonstrate the pruning performance of E-Sparse, we conduct a series of experiments to evaluate its efficacy across various model sizes within the LLaMA model family. Similar to Wanda and SparseGPT, we evaluate the perplexity of Wiki-  To assess the generalization of our method, we conduct experiments on OPT model family, which is one of the most representative LLMs prior to the release of the LLaMA. We choose two models of varying sizes, specifically the OPT-6.7B and OPT-30B, for our experiments. According to the result in Table 3, it is evident that the implementation of E-Sparse can lead to a substantial enhancement in WikiText validation. For instance, E-Sparse can achieve a perplexity score of 14.9 at 2:4 sparsity, markedly outperforming Wanda baseline, which  registers at 15.89. \n\nTo provide further evidence of our method's performance, we also present results on several Ze-roShot tasks for LLaMA under 2:4 sparsity. The comprehensive results have been tabulated in Tab 2. It can be observed that our E-Sparse consistently exhibits an edge, particularly evident from the superior average accuracy metrics amassed across the quintet of Zero-Shot tasks when compared with other established baseline methods. E-Sparse outperforms Wanda by a margin of 3% and exceeds SparseGPT by 1% on average accuracy for LLaMA-7B. Despite the 2:4 pruning being the most constrained sparsity pattern, our method achieves enhanced performance for all model size on HellaSwag. Additionally, our approach either matches or surpasses the performance of Wanda and SparseGPT on the other four datasets.",
            "score": 0.6091140030812017,
            "section_title": "Pruning Results on LLMs",
            "char_start_offset": 15575,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 169
                },
                {
                    "start": 170,
                    "end": 406
                },
                {
                    "start": 407,
                    "end": 471
                },
                {
                    "start": 472,
                    "end": 505
                },
                {
                    "start": 506,
                    "end": 653
                },
                {
                    "start": 654,
                    "end": 798
                },
                {
                    "start": 801,
                    "end": 938
                },
                {
                    "start": 939,
                    "end": 1227
                },
                {
                    "start": 1228,
                    "end": 1334
                },
                {
                    "start": 1335,
                    "end": 1477
                },
                {
                    "start": 1478,
                    "end": 1599
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.607421875
        },
        {
            "corpus_id": "276961144",
            "title": "T\u00fdr-the-Pruner: Unlocking Accurate 50% Structural Pruning for LLMs via Global Sparsity Distribution Optimization",
            "text": "Large language models (LLMs) have significantly advanced natural language processing, achieving exceptional performance in tasks such as text understanding, generation, and reasoning (Zhao et al., 2023;Dubey et al., 2024;Brown et al., 2020). However, the computational and storage resources required for model deployment incur high costs and environmental impacts, limiting their accessibility in resource-constrained scenarios. Model compression techniques, such as quantization (Lin et al., 2024;Frantar et al., 2022), pruning (Frantar & Alistarh, 2023;Ma et al., 2023), and low-rank decomposition (Wang et al., 2024), are essential for reducing LLM size and computational demands. This paper focuses on structural pruning, which notably enhances inference efficiency in a hardware-agnostic manner. \n\nExisting structural pruning methods for LLMs are typically classified into local and global techniques. Local pruning methods (Kurtic et al., 2023;Meng et al., 2024), which prune layers individually, enable efficient compression of hundred-billion-scale LLMs on a single GPU via offload approaches. However, they overlook global dependencies in model topology and restrict the sparsity to be uniform across layers. Global pruning methods (Ma et al., 2023;Kwon et al., 2022) alleviate local constraints, facilitating sparsity allocation and the potential for optimal pruning. However, they face resource constraints and rely on memoryintensive gradient backpropagation for saliency calculation. Moreover, by ranking structural saliency uniformly, they neglect inter-structure dependencies, hindering end-to-end optimization. Global pruning also poses a risk of overfitting with limited calibration data. Therefore, a question arises: \n\nHow to achieve efficient global structural pruning with endto-end optimization? \n\nTo address this challenge, we propose T\u00fdr-the-Pruner, an efficient search-based global pruning framework with endto-end optimization. Our framework constructs a supernet by applying local pruning to each layer, producing pruned copies with varying sparsity ratios.",
            "score": 0.6090250772129543,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 241
                },
                {
                    "start": 242,
                    "end": 428
                },
                {
                    "start": 429,
                    "end": 683
                },
                {
                    "start": 684,
                    "end": 800
                },
                {
                    "start": 803,
                    "end": 906
                },
                {
                    "start": 907,
                    "end": 1101
                },
                {
                    "start": 1102,
                    "end": 1217
                },
                {
                    "start": 1218,
                    "end": 1377
                },
                {
                    "start": 1378,
                    "end": 1496
                },
                {
                    "start": 1497,
                    "end": 1626
                },
                {
                    "start": 1627,
                    "end": 1705
                },
                {
                    "start": 1706,
                    "end": 1735
                },
                {
                    "start": 1738,
                    "end": 1817
                },
                {
                    "start": 1820,
                    "end": 1953
                },
                {
                    "start": 1954,
                    "end": 2084
                }
            ],
            "ref_mentions": [
                {
                    "start": 221,
                    "end": 240,
                    "matchedPaperCorpusId": "218971783"
                },
                {
                    "start": 480,
                    "end": 498,
                    "matchedPaperCorpusId": "258999941"
                },
                {
                    "start": 498,
                    "end": 519,
                    "matchedPaperCorpusId": "251765570"
                },
                {
                    "start": 555,
                    "end": 571,
                    "matchedPaperCorpusId": "258823276"
                },
                {
                    "start": 950,
                    "end": 968,
                    "matchedPaperCorpusId": "268536948"
                },
                {
                    "start": 1241,
                    "end": 1258,
                    "matchedPaperCorpusId": "258823276"
                },
                {
                    "start": 1258,
                    "end": 1276,
                    "matchedPaperCorpusId": "248266822"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.658203125
        },
        {
            "corpus_id": "271270022",
            "title": "Reconstruct the Pruned Model without Any Retraining",
            "text": "Large language models (LLMs) have attained significant achievements towards various downstream tasks in recent years [1,2,3,4].However, despite the substantial progress, the deployment of LLMs is constrained by the high parameter counts and considerable computational overhead [5].Retraining-based structured pruning [6,7] is one of the compression techniques to address this issue.By removing a whole group of weights from the original model, such methods can reduce the inference latency and memory storage without requiring any external hardware acceleration support [8].However, such a strategy requires a full dataset to retrain the pruned model, resulting in significant computational overhead (e.g.\u223c33 hours for BERT [6]) and extensive engineering efforts for hyper-parameter tuning and complex deployment [9,10].These requirements render the approach impractical for real-world applications, especially for LLMs.\n\nThe Retraining-free pruning paradigm is proposed to reduce the enormous retraining consumption, which falls into two stages: 1) pruning criteria and 2) distortion reconstruction.For the first stage, each module of the well-trained model is scored based on a specific criterion to identify and prune redundant components [11,12].After that, the distorted output is reconstructed by the subsequent reconstruction stage.Compared to retraining-based approaches, the unique value of such a paradigm is its ability to regain performance without any training and only requires a small calibration dataset.Consequently, it is highly efficient (e.g., several minutes) and well-suited for compressing LLMs.However, there is limited research about retraining-free approaches, and most previous works target either encoder-based or decoder-based models exclusively.Additionally, existing retraining-free methods primarily focus on developing better criteria for determining the pruned architecture, with proposed reconstruction techniques often lacking generalizability.As illustrated in Figure 1a, we applied different algorithms [13,14] to reconstruct models pruned using manifold criteria [13,15,16,17] and compared the accuracy drop.Our results reveal that existing reconstruction approaches exhibit limited and unstable performance, particularly for retraining-based criteria.",
            "score": 0.6088197567442423,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 127
                },
                {
                    "start": 127,
                    "end": 281
                },
                {
                    "start": 281,
                    "end": 382
                },
                {
                    "start": 382,
                    "end": 574
                },
                {
                    "start": 574,
                    "end": 705
                },
                {
                    "start": 705,
                    "end": 820
                },
                {
                    "start": 820,
                    "end": 920
                },
                {
                    "start": 922,
                    "end": 1100
                },
                {
                    "start": 1100,
                    "end": 1250
                },
                {
                    "start": 1250,
                    "end": 1339
                },
                {
                    "start": 1339,
                    "end": 1520
                },
                {
                    "start": 1520,
                    "end": 1618
                },
                {
                    "start": 1618,
                    "end": 1775
                },
                {
                    "start": 1775,
                    "end": 1980
                },
                {
                    "start": 1980,
                    "end": 2147
                },
                {
                    "start": 2147,
                    "end": 2291
                }
            ],
            "ref_mentions": [
                {
                    "start": 122,
                    "end": 124,
                    "matchedPaperCorpusId": "247951931"
                },
                {
                    "start": 277,
                    "end": 280,
                    "matchedPaperCorpusId": "221112343"
                },
                {
                    "start": 320,
                    "end": 322,
                    "matchedPaperCorpusId": "259858812"
                },
                {
                    "start": 1242,
                    "end": 1246,
                    "matchedPaperCorpusId": "7785881"
                },
                {
                    "start": 1246,
                    "end": 1249,
                    "matchedPaperCorpusId": "61815367"
                },
                {
                    "start": 2041,
                    "end": 2045,
                    "matchedPaperCorpusId": "248266822"
                },
                {
                    "start": 2102,
                    "end": 2106,
                    "matchedPaperCorpusId": "248266822"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.5810546875
        },
        {
            "corpus_id": "273374936",
            "title": "DISP-LLM: Dimension-Independent Structural Pruning for Large Language Models",
            "text": "Magnitude-based pruning is the most straightforward approach to reduce model size, where weights with the smallest magnitude are pruned. Han et al. [14] employ this strategy for pruning with L 1 or L 2 norm of weights. Filter pruning [24] extends this setting to structures of the model instead of performing weight-level sparsification. Although magnitude-based pruning methods are very efficient, they result in significant performance drops for LLMs, even for weight pruning [9]. Another line of research, Optimal Brain Damage [23] and Optimal Brain Surgeon [15], utilize second-order information to remove connections. These methods require calculating the inverse of the Hessian matrix, which is computationally intensive for modern neural network architectures like Convolutional Neural Networks (CNNs) [22,16], Transformers [38], or Large Language Models (LLMs) [35]. To reduce the cost of computing the Hessian inverse matrix, Optimal Brain Surgeon can be applied in a layer-wise fashion [7,8], making the computation tractable. However, further scaling up these methods for LLMs remains challenging. Recent methods like SparseGPT [9] or GPTQ [10] aim to minimize the squared error before and after pruning or quantization of a given layer. In this setting, the Hessian inverse matrix becomes easy to compute, as it is simply the multiplication between the feature map and its transpose for a given layer. GPTQ and SparseGPT then quantize or sparsify model weights in a column-by-column manner, and the unpruned or unquantized weights are updated to compensate for the error of pruning and quantization. Wanda [34] further avoids computing the inverse of the Hessian matrix by only considering the diagonal of the Hessian matrix. While SparseGPT and Wanda achieve good results, unstructured sparsity is known to be harder to achieve actual speedup. They also applied their methods on semi-structured settings [31], but the performance becomes much worse. \n\nSeveral researches [28,19,44,13,42,12] apply learnable parameters for specific structures when pruning vision or language models.",
            "score": 0.6086156873659803,
            "section_title": "Related Works",
            "char_start_offset": 4291,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 136
                },
                {
                    "start": 137,
                    "end": 218
                },
                {
                    "start": 219,
                    "end": 337
                },
                {
                    "start": 338,
                    "end": 482
                },
                {
                    "start": 483,
                    "end": 622
                },
                {
                    "start": 623,
                    "end": 874
                },
                {
                    "start": 875,
                    "end": 1036
                },
                {
                    "start": 1037,
                    "end": 1108
                },
                {
                    "start": 1109,
                    "end": 1248
                },
                {
                    "start": 1249,
                    "end": 1413
                },
                {
                    "start": 1414,
                    "end": 1611
                },
                {
                    "start": 1612,
                    "end": 1737
                },
                {
                    "start": 1738,
                    "end": 1856
                },
                {
                    "start": 1857,
                    "end": 1962
                },
                {
                    "start": 1965,
                    "end": 2094
                }
            ],
            "ref_mentions": [
                {
                    "start": 148,
                    "end": 152,
                    "matchedPaperCorpusId": "2238772"
                },
                {
                    "start": 478,
                    "end": 481,
                    "matchedPaperCorpusId": "255372747"
                },
                {
                    "start": 530,
                    "end": 534,
                    "matchedPaperCorpusId": "7785881"
                },
                {
                    "start": 809,
                    "end": 813,
                    "matchedPaperCorpusId": "195908774"
                },
                {
                    "start": 813,
                    "end": 816,
                    "matchedPaperCorpusId": "206594692"
                },
                {
                    "start": 996,
                    "end": 999,
                    "matchedPaperCorpusId": "5750817"
                },
                {
                    "start": 999,
                    "end": 1001,
                    "matchedPaperCorpusId": "251765570"
                },
                {
                    "start": 1139,
                    "end": 1142,
                    "matchedPaperCorpusId": "255372747"
                },
                {
                    "start": 1151,
                    "end": 1155,
                    "matchedPaperCorpusId": "259298689"
                },
                {
                    "start": 1618,
                    "end": 1622,
                    "matchedPaperCorpusId": "259203115"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.62353515625
        },
        {
            "corpus_id": "270380112",
            "title": "Evaluating Zero-Shot Long-Context LLM Compression",
            "text": "Structural pruning involves removing entire filters from the neural network, making it more conducive to hardware implementation. Various methods exist for implementing structural pruning, such as l1-dependent pruning [14,42], first-order importance estimation [24], hessian-based estimation [19,38], \n\nor the optimal brain surgeon [19,21]. \n\nUnstructured methods [13,14,28,35] like magnitude pruning operate at the individual weight level, maintaining performance even at higher sparsity levels. However, existing pruning methods usually require modifications to the training procedure [30], retraining the pruned networks to regain accuracy [23], or a computationally intensive iterative retraining process [3,11]. Yet, scaling these techniques to LLMs with billions of parameters poses a challenge, as the necessary training process demands significant computational resources [15,45]. \n\nWe focus on unstructured pruning methods in this work as they are more fundamental and flexible than structural pruning. Specifically, we choose two representative methods: magnitude pruning and Wanda [35]. Although other model compression methods such as neural architecture search exist [5,46], this study focuses exclusively on pruning and quantization.",
            "score": 0.6068955985540494,
            "section_title": "Related Works",
            "char_start_offset": 4323,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 129
                },
                {
                    "start": 130,
                    "end": 300
                },
                {
                    "start": 303,
                    "end": 340
                },
                {
                    "start": 343,
                    "end": 496
                },
                {
                    "start": 497,
                    "end": 716
                },
                {
                    "start": 717,
                    "end": 888
                },
                {
                    "start": 891,
                    "end": 1011
                },
                {
                    "start": 1012,
                    "end": 1097
                },
                {
                    "start": 1098,
                    "end": 1247
                }
            ],
            "ref_mentions": [
                {
                    "start": 218,
                    "end": 222,
                    "matchedPaperCorpusId": "2238772"
                },
                {
                    "start": 261,
                    "end": 265,
                    "matchedPaperCorpusId": "252383606"
                },
                {
                    "start": 296,
                    "end": 299,
                    "matchedPaperCorpusId": "155089879"
                },
                {
                    "start": 336,
                    "end": 339,
                    "matchedPaperCorpusId": "7785881"
                },
                {
                    "start": 364,
                    "end": 368,
                    "matchedPaperCorpusId": "2134321"
                },
                {
                    "start": 368,
                    "end": 371,
                    "matchedPaperCorpusId": "2238772"
                },
                {
                    "start": 709,
                    "end": 712,
                    "matchedPaperCorpusId": "237940329"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.464111328125
        },
        {
            "corpus_id": "264146174",
            "title": "One-Shot Sensitivity-Aware Mixed Sparsity Pruning for Large Language Models",
            "text": "We adopt a mixed sparsity approach to prune different base models, as shown in Table 2. It can be seen that our method outperforms SparseGPT in three mainstream 13B LLM models. The experiment results demonstrate that our method can further reduce the performance degradation caused by pruning, bringing it closer to the dense model. Since the overall sparsity is set at 50%, our method and SparseGPT achieve an equal model compression ratio and inference acceleration effect. \n\nMoreover, we conduct tests on zero-shot downstream NLP tasks. As shown in Table 3, with an overall sparsity level of 50%, the proposed approach outperforms SparseGPT in terms of accuracy. These  Figure 3 shows that performance degradation expands when the sparsity level increases, which is a common effect for all pruning methods examined (results for LLaMA2-13B model plotted). Nevertheless, the proposed method helps mitigating the degradation even more as the sparsity level becomes higher.",
            "score": 0.606782439557536,
            "section_title": "Evaluation on Mixed Sparsity Pruning",
            "char_start_offset": 13401,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 87
                },
                {
                    "start": 88,
                    "end": 176
                },
                {
                    "start": 177,
                    "end": 332
                },
                {
                    "start": 333,
                    "end": 475
                },
                {
                    "start": 478,
                    "end": 539
                },
                {
                    "start": 540,
                    "end": 665
                },
                {
                    "start": 666,
                    "end": 857
                },
                {
                    "start": 858,
                    "end": 972
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.63623046875
        },
        {
            "corpus_id": "269605373",
            "title": "Enabling High-Sparsity Foundational Llama Models with Efficient Pretraining and Deployment",
            "text": "Large language models (LLMs) have transformed the field of Natural Language Processing (NLP) [1].Their ability to generate text, translate languages, and answer questions in a near-human way has opened up unprecedented applications.However, their massive size creates a significant bottleneck.The computation cost of training and running these models is very high, has significant energy impact, and limits their accessibility [2,3,4].Compression techniques such as quantization have successfully reduced model size and improved inference speed [5,6,7].However, quantization past 4 bits per parameter while preserving accuracy is proving to be a limit that is hard to cross for full recoverability compared to high-quality baseline models [8,9,10].\n\nWeight pruning, that is, setting a fraction of the model parameters to zero, is another promising approach to reach higher compression in the context of Deep Neural Networks (DNNs) [11,12,13,14].Specifically, sparsity reduces the model's storage footprint and can enable faster inference and training through reduced computation and memory movement.However, existing pruning methods often struggle to maintain high accuracy, especially at high sparsity levels and complex tasks [15].These accuracy limitations reduce their potential for creating genuinely efficient and generally usable sparse LLMs.To our knowledge, no techniques currently exist for accurately pruning foundational models to non-trivial sparsities while preserving their abilities on downstream tasks.\n\nTo address this challenge, we consider a new approach that combines accurate pruning and fine-tuning of a foundational model, which we illustrate on the Llama-2 7B architecture [16].Specifically, we investigate the following steps:\n\n\u2022 Sparse Pretraining: We introduce a new approach to creating sparse LLMs that achieves high accuracy for fine-tuned models at up to 70% sparsity.Our approach expands on top of the popular SparseGPT [17] post-training pruning algorithm with further pretraining of the sparse models on subsets of the popular SlimPajama [18] and The Stack [19] datasets.",
            "score": 0.6061306405307934,
            "section_title": "Introduction",
            "char_start_offset": 221,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 97
                },
                {
                    "start": 97,
                    "end": 232
                },
                {
                    "start": 232,
                    "end": 293
                },
                {
                    "start": 293,
                    "end": 435
                },
                {
                    "start": 435,
                    "end": 553
                },
                {
                    "start": 553,
                    "end": 748
                },
                {
                    "start": 750,
                    "end": 945
                },
                {
                    "start": 945,
                    "end": 1099
                },
                {
                    "start": 1099,
                    "end": 1233
                },
                {
                    "start": 1233,
                    "end": 1349
                },
                {
                    "start": 1349,
                    "end": 1519
                },
                {
                    "start": 1521,
                    "end": 1703
                },
                {
                    "start": 1703,
                    "end": 1752
                },
                {
                    "start": 1754,
                    "end": 1900
                },
                {
                    "start": 1900,
                    "end": 2106
                }
            ],
            "ref_mentions": [
                {
                    "start": 93,
                    "end": 96,
                    "matchedPaperCorpusId": "13756489"
                },
                {
                    "start": 548,
                    "end": 550,
                    "matchedPaperCorpusId": "251564521"
                },
                {
                    "start": 938,
                    "end": 941,
                    "matchedPaperCorpusId": "162183964"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.50048828125
        },
        {
            "corpus_id": "276618166",
            "title": "Sparse Brains are Also Adaptive Brains: Cognitive-Load-Aware Dynamic Activation for LLMs",
            "text": "Pruning Methods like SparseGPT(Frantar and Alistarh, 2023) prune weights based on magnitude criteria, achieving up to 50% sparsity without retraining. However, such approaches ignore input-specific sparsity patterns, leading to suboptimal performance on dynamic tasks like dialogue generation. Static methods lack adaptability to varying input contexts and often require retraining to recover performance.",
            "score": 0.6057623164255381,
            "section_title": "Weight",
            "char_start_offset": 17083,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 150
                },
                {
                    "start": 151,
                    "end": 293
                },
                {
                    "start": 294,
                    "end": 405
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.394775390625
        },
        {
            "corpus_id": "273346391",
            "title": "DARE the Extreme: Revisiting Delta-Parameter Pruning For Fine-Tuned Models",
            "text": "Large Language Models (LLMs) like BERT (Devlin et al., 2018), GPT (Floridi & Chiriatti, 2020), and Llama (Touvron et al., 2023) excel in various language-modeling tasks. Finetuning these for specific downstream tasks enhances personalized experiences (Sanh et al., 2021;Labrak et al., 2024) and has become a standard practice in natural language processing (Dodge et al., 2020;Zhao et al., 2023). Indeed, most entries on the OpenLLM Leaderboard involve full-parameter finetunes or their combinations (Liu et al., 2024), underscoring the widespread adoption and availability of fine-tuned models online. Decomposing fine-tuned model weights into the original parameters of the pre-trained model yields delta parameters (DP) (Yu et al., 2023a;Liu et al., 2024;Yao & Klimovic, 2023). Reducing the size of DPs, which are as large as the base model and can number in the hundreds of millions of parameters for LLMs, could significantly enhance communication efficiency in federated learning, minimize task conflicts in model merging, accelerate multi-task serving, and decrease storage needs for new fine-tuned models (see Related Work). Delta-parameter pruning (DPP) drops a fraction p of the DPs towards realizing these benefits. Naturally, DPP can be seen as an instance of generic model-parameter pruning, which compresses neural networks by eliminating weights that contribute minimally, resulting in sparse architectures (LeCun et al., 1989;Han et al., 2015b). Traditional pruning methods typically remove weights post-training based on importance criteria like weight magnitude or activation levels. While these techniques could naturally extend to DPP, their integration into this context remains largely unexplored. Random-based pruning strategies, which provide more flexibility and efficiency in implementation, also offer a competitive alternative. For instance, Random Drop and Rescale (DARE) (Yu et al., 2023a), a recently introduced randomized DPP method, reduces DP size through random pruning followed by rescaling.",
            "score": 0.6047623542653087,
            "section_title": "INTRODUCTION",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 169
                },
                {
                    "start": 170,
                    "end": 396
                },
                {
                    "start": 397,
                    "end": 602
                },
                {
                    "start": 603,
                    "end": 780
                },
                {
                    "start": 781,
                    "end": 1132
                },
                {
                    "start": 1133,
                    "end": 1226
                },
                {
                    "start": 1227,
                    "end": 1461
                },
                {
                    "start": 1462,
                    "end": 1601
                },
                {
                    "start": 1602,
                    "end": 1719
                },
                {
                    "start": 1720,
                    "end": 1855
                },
                {
                    "start": 1856,
                    "end": 2027
                }
            ],
            "ref_mentions": [
                {
                    "start": 1422,
                    "end": 1442,
                    "matchedPaperCorpusId": "7785881"
                },
                {
                    "start": 1442,
                    "end": 1460,
                    "matchedPaperCorpusId": "2238772"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.55029296875
        },
        {
            "corpus_id": "270621063",
            "title": "Dependency-Aware Semi-Structured Sparsity of GLU Variants in Large Language Models",
            "text": "Models. DaSS's performance is evaluated over open LLMs using GLU variants. SwiGLU is the most widely used GLU-based MLP in the recent LLMs, including the LLaMA2 model family (Touvron et al., 2023), which has models with parameters ranging between 7 billion and 70 billion, and also the Mistral-7B model (Jiang et al., 2023). Among them, LLaMA2-70B and Mistral-7B use grouped-query attention (Ainslie et al., 2023), the MLP module parameter accounts for around 80% of the total model parameters. There are only a few open LLMs that use other variants. For GeGLU, we use Gemma-7B. It is worth noting that the MLP intermediate dimension of Gemma-7B is 8\u00d7 model dimension, making the MLP module much larger. For ReGLU, we use ReluLLaMA (Team, 2023), which is fine-tuned using ReGLU variant (Shazeer, 2020;Mirzadeh et al., 2023) based on LLaMA2 with small accuracy loss. The model configuration details are in Appendix A.1. We access the public checkpoints of the involved models provided by HuggingFace Transformers (Wolf et al., 2019). Baseline Approaches. We compare the performance with two LLM-specific one-shot pruning approaches, SparseGPT (Frantar & Alistarh, 2023) and Wanda (Sun et al., 2024). We don't consider structured pruning methods like LLM-Pruner (Ma et al., 2023) and Sheared LLaMA (Xia et al., 2023), as they typically require re-training to recover accuracy, and are less practical for large models like LLaMA2-70B. Those baseline methods utilize uniform layerwise sparsity that can be easily converted into hardware-friendly N:M sparsity pattern. We used the same calibration data set as SparseGPT and Wanda in their model pruning processes, consisting of 128 sequences of 2048 tokens each, randomly selected from the first shard of the C4 dataset (Raffel et al., 2020).",
            "score": 0.6043612848955449,
            "section_title": "Settings",
            "char_start_offset": 13822,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 7
                },
                {
                    "start": 8,
                    "end": 74
                },
                {
                    "start": 75,
                    "end": 324
                },
                {
                    "start": 325,
                    "end": 494
                },
                {
                    "start": 495,
                    "end": 550
                },
                {
                    "start": 551,
                    "end": 578
                },
                {
                    "start": 579,
                    "end": 703
                },
                {
                    "start": 704,
                    "end": 865
                },
                {
                    "start": 866,
                    "end": 918
                },
                {
                    "start": 919,
                    "end": 1032
                },
                {
                    "start": 1033,
                    "end": 1053
                },
                {
                    "start": 1054,
                    "end": 1198
                },
                {
                    "start": 1199,
                    "end": 1431
                },
                {
                    "start": 1432,
                    "end": 1563
                },
                {
                    "start": 1564,
                    "end": 1787
                }
            ],
            "ref_mentions": [
                {
                    "start": 391,
                    "end": 413,
                    "matchedPaperCorpusId": "258833177"
                },
                {
                    "start": 1179,
                    "end": 1197,
                    "matchedPaperCorpusId": "259203115"
                },
                {
                    "start": 1260,
                    "end": 1277,
                    "matchedPaperCorpusId": "258823276"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.259521484375
        },
        {
            "corpus_id": "258865382",
            "title": "Just CHOP: Embarrassingly Simple LLM Compression",
            "text": "This section describes methods for task-agnostic model compression of large language models (LLMs). From previous literature, we adapt pruning strategies that are simple, efficient, and go well with continued pretraining on a large data corpus Table 1: Taxonomy of compression methods. We adapt pruning strategies from BERT-style models to LLMs under the pretrain-then-finetune paradigm. We also provide a list of recent or concurrent works for LLMs which compress them but do not follow it with a finetuning phase. We do not consider an un-or semi-structured form of CHOP as our early experiments validated our structured methods as having sufficient end-task accuracy while maintaining superior inference efficiency. \n\ndescribed in Section 4.1. We prune models in two simple ways: model depth (LayerCHOP; \u00a73.1) and model width (Dim-CHOP; \u00a73.2). It has been observed in the BERT model compression literature that adding distillation in conjunction with pruning amplifies the result of compression (Liang et al., 2023). Hence, we experiment with the same for decoder-only LLMs in the large data regime. A taxonomy of our adapted methods and where they lie within the current compression literature is summarized in Table 1.",
            "score": 0.6039684828056144,
            "section_title": "How to Train Your (Compressed) LLM",
            "char_start_offset": 6954,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 99
                },
                {
                    "start": 100,
                    "end": 285
                },
                {
                    "start": 286,
                    "end": 387
                },
                {
                    "start": 388,
                    "end": 515
                },
                {
                    "start": 516,
                    "end": 718
                },
                {
                    "start": 721,
                    "end": 746
                },
                {
                    "start": 747,
                    "end": 846
                },
                {
                    "start": 847,
                    "end": 1019
                },
                {
                    "start": 1020,
                    "end": 1102
                },
                {
                    "start": 1103,
                    "end": 1223
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.5615234375
        },
        {
            "corpus_id": "258823276",
            "title": "LLM-Pruner: On the Structural Pruning of Large Language Models",
            "text": "This introduces the task-agnostic compression of LLMs, which presents two key challenges: \n\n\u2022 The size of the training corpus of the LLM is enormous. Previous compression methods heavily depend on the training corpus. The LLM has escalated the corpus scale to 1 trillion tokens or more [17,49]. The extensive storage needs and protracted transmission times make the dataset difficult to acquire. Furthermore, if the dataset is proprietary, acquisition of the training corpus verges on impossibility, a situation encountered in [69,37]. \u2022 The unacceptably long duration for the post-training of the pruned LLM. Existing methods require a substantial amount of time for post-training the smaller model [53,28]. For instance, the general distillation in TinyBERT takes around 14 GPU days [20]. Even post-training a task-specific compressed model of BERT demands around 33 hours [59,22]. As the size of both the model and corpus for LLMs increases rapidly, this step will invariably consume an even more extensive time. \n\nTo tackle the aforementioned challenges associated with the task-agnostic compression of LLMs, we introduce a novel approach called LLM-Pruner. Since our goal is to compress LLMs with reduced data dependency and expedited post-training, how to prune model with the minimal disruption to the origin is crucial. To accomplish this, we propose a dependency detection algorithm that identifies all the dependent structures within the model. Once the coupled structure is identified, we employ an efficient importance estimation strategy to select the optimal group for pruning under the task-agnostic setting, where the first-order information and an approximated hessian information is taken into account. Finally, a rapid recovery stage is executed to post-train the pruned model with limited data. \n\nContribution. In this paper, we propose a novel framework, LLM-Pruner, for the task-agnostic compression of the large language model. To the best of our knowledge, LLM-Pruner is the first framework designed for structured pruning of LLMs. We conclude the advantages of the LLM-Pruner as (i) Task-agnostic compression, where the compressed language model retains its ability to serve as a multi-task solver.",
            "score": 0.6036018221163127,
            "section_title": "Introduction",
            "char_start_offset": 2128,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 89
                },
                {
                    "start": 92,
                    "end": 149
                },
                {
                    "start": 150,
                    "end": 217
                },
                {
                    "start": 218,
                    "end": 294
                },
                {
                    "start": 295,
                    "end": 395
                },
                {
                    "start": 396,
                    "end": 535
                },
                {
                    "start": 536,
                    "end": 609
                },
                {
                    "start": 610,
                    "end": 708
                },
                {
                    "start": 709,
                    "end": 790
                },
                {
                    "start": 791,
                    "end": 883
                },
                {
                    "start": 884,
                    "end": 1015
                },
                {
                    "start": 1018,
                    "end": 1161
                },
                {
                    "start": 1162,
                    "end": 1327
                },
                {
                    "start": 1328,
                    "end": 1454
                },
                {
                    "start": 1455,
                    "end": 1720
                },
                {
                    "start": 1721,
                    "end": 1814
                },
                {
                    "start": 1817,
                    "end": 1830
                },
                {
                    "start": 1831,
                    "end": 1950
                },
                {
                    "start": 1951,
                    "end": 2055
                },
                {
                    "start": 2056,
                    "end": 2223
                }
            ],
            "ref_mentions": [
                {
                    "start": 700,
                    "end": 704,
                    "matchedPaperCorpusId": "211296536"
                },
                {
                    "start": 785,
                    "end": 789,
                    "matchedPaperCorpusId": "202719327"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.85595703125
        },
        {
            "corpus_id": "277275922",
            "title": "Efficient self-attention with smart pruning for sustainable large language models",
            "text": "For instance, we observed noticeable differences in performance on tasks like SST-2 (sentiment analysis) and QNLI (question-answering). The pruning method that worked well for SST-2 did not yield the same results for QNLI and other tasks 28 , indicating that different tasks may require different compression strategies tailored to the specific layer or model component. \n\nTable 9 presents a comparison of representative pruning methods applied to large language Transformer models, showcasing metrics such as compression rate and speed-up. For the OPT-175B model, the proposed method achieves a compression rate of 56.7% and a speed-up of 1.61x, surpassing SparseGPT's 54% compression and 1.54 \u00d7 speed-up. On LLaMA2-7B, it delivers a 63% compression rate and 1.47 \u00d7 speed-up, outperforming Sheared LLaMA's 61.4% compression and LLM Pruner's 20% compression with 1.18 \u00d7 speed-up. For TinyLLaMA-1B, it achieves the highest compression rate of 45% and a 1.68 \u00d7 speed-up, exceeding Sheared LLaMA's 34% compression and 1.57 \u00d7 speed-up, as well as LLM Pruner's 12% compression and 1.35 \u00d7 speed-up. The results indicate that the proposed approach consistently outperforms other methods in terms of compression rate and speed improvement across various models. While our proposed pruning method demonstrates significant improvements in compression rates and speed-up across multiple open-source LLMs, there are certain limitations to consider. One primary constraint is the resource-intensive nature of evaluating and retraining large-scale models. Larger LLMs, such as Falcon and others of similar scale, require substantial computational resources, storage, and energy. Additionally, retraining these models on large datasets to assess the full potential The Industrial Revolution marked a turning point in history. With new inventions, such as the spinning jenny and the steam engine, production increased, leading to urban growth Prompt 2 Once upon a time, there was a curious kitten who wanted to explore the wonders of the world \n\nThe Industrial Revolution had a profound impact on society.",
            "score": 0.6027143924749188,
            "section_title": "Results and discussion",
            "char_start_offset": 53143,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 135
                },
                {
                    "start": 136,
                    "end": 370
                },
                {
                    "start": 373,
                    "end": 540
                },
                {
                    "start": 541,
                    "end": 706
                },
                {
                    "start": 707,
                    "end": 879
                },
                {
                    "start": 880,
                    "end": 1092
                },
                {
                    "start": 1093,
                    "end": 1253
                },
                {
                    "start": 1254,
                    "end": 1436
                },
                {
                    "start": 1437,
                    "end": 1541
                },
                {
                    "start": 1542,
                    "end": 1664
                },
                {
                    "start": 1665,
                    "end": 1810
                },
                {
                    "start": 1811,
                    "end": 2027
                },
                {
                    "start": 2030,
                    "end": 2089
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.37109375
        },
        {
            "corpus_id": "274597508",
            "title": "LLM-BIP: Structured Pruning for Large Language Models with Block-Wise Forward Importance Propagation",
            "text": "Large language models (LLMs) have demonstrated remarkable performance across various language tasks, but their widespread deployment is impeded by their large size and high computational costs. Structural pruning is a prevailing technique used to introduce sparsity into pre-trained models and facilitate direct hardware acceleration during inference by removing redundant connections (structurally-grouped parameters), such as channels and attention heads. Existing structural pruning approaches often employ either global or layer-wise pruning criteria; however, they are hindered by ineffectiveness stemming from inaccurate evaluation of connection importance. Global pruning methods typically assess component importance using near-zero and unreliable gradients, while layer-wise pruning approaches encounter significant pruning error accumulation issues. To this end, we propose a more accurate pruning metric based on the block-wise importance score propagation, termed LLM-BIP. Specifically, LLM-BIP precisely evaluates connection importance by gauging its influence on the respective transformer block output, which can be efficiently approximated in a single forward pass through an upper bound derived from the assumption of Lipschitz continuity. We evaluate the proposed method using LLaMA-7B, Vicuna-7B, and LLaMA-13B across common zero-shot tasks. The results demonstrate that our approach achieves an average of 3.26% increase in accuracy for common reasoning tasks compared to previous best baselines. It also reduces perplexity by 14.09 and 68.76 on average for the WikiText2 dataset and PTB dataset, respectively.",
            "score": 0.6017366567209531,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.76953125
        },
        {
            "corpus_id": "276903091",
            "title": "IteRABRe: Iterative Recovery-Aided Block Reduction",
            "text": "We are in the era of the burgeoning of producing Large Language Models (LLMs), which has led to the necessity of making them smaller due to deployment costs. Several approaches focus on model reduction, such as model sparsification, reducing LLM hidden sizes, or removing presumably unimportant blocks. However, preserving performance while compressing models remains challenging. \n\nBlock pruning is a straightforward compression approach for reducing LLM size, motivated by the layer redundancy found in LLM architectures (Men et al., 2024;Dumitru et al., 2024;Chen et al., 2025). While detecting redundant or unimportant blocks can minimize performance degradation from pruning, some loss is inevitable. Although post-finetuning can help recover performance, simultaneous pruning of multiple blocks may still cause unrecoverable damage. One possible solution is to take an iterative approach. Muralidharan et al., 2024 proposes iterative pruning with knowledge distillation to recover from performance loss. This work successfully compresses a 15B LLM into smaller 8B and 4B versions, achieving competitive results compared to other LLMs of similar size. However, this approach may be impractical for those with limited computational resources, as it employs a computedependent method in the pruning process and requires 8T tokens for the recovery process. \n\nThis leads us to ask: Can we develop an exhustive, efficient and effective iterative block pruning method for model compression? We investigate this question by introducing IteRABRe, a straightforward iterative pruning approach. We choose layer pruning for its simplicity and enhanced interpretability in preservation. To test efficiency, we perform recovery using only 2.5M tokens of data. Our method outperforms other baselines by approximately 3% on average for Llama3.1-8B and Qwen2.5-7B models. Furthermore, our approach shows particular strength in preserving linguistic tasks, demonstrating 5% better performance than arXiv:2503.06291v1 [cs.CL] 8 Mar 2025 baselines. Additionally, this approach also exhibits zero-shot cross-lingual capabilities, such as retaining the German language using solely English data recovery.",
            "score": 0.6010100745390894,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 157
                },
                {
                    "start": 158,
                    "end": 302
                },
                {
                    "start": 303,
                    "end": 380
                },
                {
                    "start": 383,
                    "end": 581
                },
                {
                    "start": 582,
                    "end": 705
                },
                {
                    "start": 706,
                    "end": 838
                },
                {
                    "start": 839,
                    "end": 894
                },
                {
                    "start": 895,
                    "end": 1009
                },
                {
                    "start": 1010,
                    "end": 1156
                },
                {
                    "start": 1157,
                    "end": 1358
                },
                {
                    "start": 1361,
                    "end": 1489
                },
                {
                    "start": 1490,
                    "end": 1589
                },
                {
                    "start": 1590,
                    "end": 1679
                },
                {
                    "start": 1680,
                    "end": 1751
                },
                {
                    "start": 1752,
                    "end": 1837
                },
                {
                    "start": 1838,
                    "end": 1852
                },
                {
                    "start": 1853,
                    "end": 1860
                },
                {
                    "start": 1861,
                    "end": 2004
                },
                {
                    "start": 2005,
                    "end": 2034
                },
                {
                    "start": 2035,
                    "end": 2188
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.48681640625
        },
        {
            "corpus_id": "263620656",
            "title": "ECoFLaP: Efficient Coarse-to-Fine Layer-Wise Pruning for Vision-Language Models",
            "text": "the expense of model performance. Unstructured pruning (Sun et al., 2023a;Frantar & Alistarh, 2023), on the other hand, can be advantageous in preserving performance even with high model sparsity given AI acceleration software or sparse matrix computation schemes (Han et al., 2016;Mishra et al., 2021;Das & Ramamoorthy, 2022;NeuralMagic, 2022). Recently, Frantar & Alistarh (2023) suggest a one-shot pruning technique, SparseGPT, for generated pre-trained transformers (GPTs) in an unstructured manner. They newly employ a sparse regression solver that prunes weights at each layer based on row-wise Hessian reconstruction as formulated by a closed-form solution. Wanda (Sun et al., 2023a) proposes a magnitude-based unstructured pruning approach for large language models (LLMs). It promotes layer-wise weight sparsification based on the importance, computed by multiplying weights and input activations. However, these SoTA pruning methods rely on the pre-defined pruning ratio that all layers resort to the same sparsity, restricting the upper bound on model compression. In addition, their methods are tailored to language models without concern for different modalities. On the other hand, our proposed method allows adaptive pruning at each layer without heavy computation of global gradients. Further, to the best of our knowledge, we propose a first unified sparse approximate solver for vision-language multimodal models. \n\nTransformers for vision-language multimodal learning. Vision-language multimodal learning has shown remarkable achievement on various tasks, such as classification (Liu et al., 2018;Liang et al., 2022a), retrieval (Fei et al., 2021), few-shot learning (Tsimpoukelli et al., 2021;Alayrac et al., 2022), visual QA (Kim et al., 2016;Liu et al., 2023), and image/video generation (Zhou et al., 2022b;Singer et al., 2022;Lee et al., 2023).",
            "score": 0.6002169637613131,
            "section_title": "INTRODUCTION",
            "char_start_offset": 5604,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 33
                },
                {
                    "start": 34,
                    "end": 345
                },
                {
                    "start": 346,
                    "end": 503
                },
                {
                    "start": 504,
                    "end": 664
                },
                {
                    "start": 665,
                    "end": 781
                },
                {
                    "start": 782,
                    "end": 906
                },
                {
                    "start": 907,
                    "end": 1075
                },
                {
                    "start": 1076,
                    "end": 1176
                },
                {
                    "start": 1177,
                    "end": 1300
                },
                {
                    "start": 1301,
                    "end": 1431
                },
                {
                    "start": 1434,
                    "end": 1487
                },
                {
                    "start": 1488,
                    "end": 1868
                }
            ],
            "ref_mentions": [
                {
                    "start": 264,
                    "end": 282,
                    "matchedPaperCorpusId": "1663491"
                },
                {
                    "start": 302,
                    "end": 326,
                    "matchedPaperCorpusId": "228372945"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.64599609375
        },
        {
            "corpus_id": "276928323",
            "title": "SEAP: Training-free Sparse Expert Activation Pruning Unlock the Brainpower of Large Language Models",
            "text": "Large Language Models (LLMs) have achieved remarkable success across a wide spectrum of natural language processing (NLP) tasks (Zhao et al., 2024;Zheng et al., 2025), demonstrating their versatility and adaptability in diverse applications. However, their deployment in real-world scenarios remains a significant challenge due to the substantial computational demands during inference. The inference process of LLMs is constrained by memory bandwidth and hardware limitations (Chavan et al., 2024), making efficient deployment particularly difficult, especially in resource-constrained environments such as real-time systems and edge comput-ing. As LLMs continue to scale, these challenges become even more pronounced, necessitating novel approaches to optimize computational efficiency while preserving model performance. \n\nTo mitigate the computational overhead of LLMs, several techniques have been explored. Quantization methods (Bai et al., 2021;Frantar et al., 2023) reduce weight precision, while Mixture of Experts (MoE) architectures (Shazeer et al., 2017;Lepikhin et al., 2020;Fedus et al., 2022) dynamically activate only subsets of the network to improve efficiency. Another widely adopted approach is pruning (Frantar and Alistarh, 2023;Ma et al., 2023;Liu et al., 2024), which removes redundant parameters, neurons, or connections to reduce inference costs and storage requirements. Despite the effectiveness of pruning in reducing model complexity, most existing methods are static, relying on activation distributions collected from general datasets such as WikiText-2 (Merity et al., 2016) and C4 (Raffel et al., 2020a). These methods apply a uniform pruning strategy across all tasks, which may lead to suboptimal efficiency and fail to fully leverage task-specific knowledge requirements. \n\nInspired by cognitive neuroscience, where different brain regions are selectively activated based on task demands, we hypothesize that a similar mechanism exists in LLMs-where different tasks rely on distinct sets of neurons working collaboratively. This suggests that pruning strategies should be adaptive rather than static, dynamically selecting the most relevant parameters for each task.",
            "score": 0.5995081068272525,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 241
                },
                {
                    "start": 242,
                    "end": 386
                },
                {
                    "start": 387,
                    "end": 646
                },
                {
                    "start": 647,
                    "end": 823
                },
                {
                    "start": 826,
                    "end": 912
                },
                {
                    "start": 913,
                    "end": 1179
                },
                {
                    "start": 1180,
                    "end": 1397
                },
                {
                    "start": 1398,
                    "end": 1638
                },
                {
                    "start": 1639,
                    "end": 1808
                },
                {
                    "start": 1811,
                    "end": 2060
                },
                {
                    "start": 2061,
                    "end": 2203
                }
            ],
            "ref_mentions": [
                {
                    "start": 147,
                    "end": 166,
                    "matchedPaperCorpusId": "272423770"
                },
                {
                    "start": 477,
                    "end": 498,
                    "matchedPaperCorpusId": "267412953"
                },
                {
                    "start": 934,
                    "end": 952,
                    "matchedPaperCorpusId": "229923538"
                },
                {
                    "start": 1044,
                    "end": 1066,
                    "matchedPaperCorpusId": "12462234"
                },
                {
                    "start": 1088,
                    "end": 1107,
                    "matchedPaperCorpusId": "231573431"
                },
                {
                    "start": 1251,
                    "end": 1267,
                    "matchedPaperCorpusId": "258823276"
                },
                {
                    "start": 1615,
                    "end": 1637,
                    "matchedPaperCorpusId": "204838007"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.7392578125
        },
        {
            "corpus_id": "264590698",
            "title": "LoRAShear: Efficient Large Language Model Structured Pruning and Knowledge Recovery",
            "text": "While pruning (Han et al., 2015) is well-established in traditional Deep Neural Networks (DNNs), its application to LLMs presents unique challenges. Unlike the smaller, taskspecific DNNs (Ding et al., 2021;2022), LLMs have a large number of parameters and require significant computational resources (Brown et al., 2020). Moreover, it's crucial for them to generalize well across multiple tasks (Xia et al., 2023). Recently, various pruning methods have been developed specifically for LLMs, generally falling into two main categories: unstructured and structured. \n\nUnstructured Pruning. Unstructured pruning methods (Dong et al., 2017;Chen et al., 2020;2021a) focus on setting unimportant individual weights in the model to zero. This fine-grained approach is straightforward and often maintains good performance, even with high compression rates. However, it results in sparse weight matrices that aren't well-suited for hardware accelerators, making them less efficient in real-world deployment. In the realm of LLMs, several new techniques have emerged. \n\nSparseGPT (Frantar & Alistarh, 2023) uses a sophisticated weight update process involving synchronized secondorder Hessian updates, bypassing traditional retraining. In contrast, Wanda (Sun et al., 2023) achieves high sparsity without any retraining, simply by pruning weights with the smallest magnitudes multiplied by their corresponding input activations. PST (Li et al., 2022), however, combines unstructured pruning with efficient fine-tuning, pruning both LoRA and pre-trained model weights. A drawback of this method is the need for a memory-intensive mask that matches the shape of the pre-trained weights. \n\nStructured Pruning. Structured pruning methods (Chen et al., 2021b;2023a;b) focus on removing entire groups of parameters, such as neurons or layers, rather than in-dividual weights. This group-level approach is hardwarefriendly as it maintains dense weight matrices. The main challenge is selecting which structures to remove without compromising model performance.",
            "score": 0.5981957451985652,
            "section_title": "Related Work",
            "char_start_offset": 4789,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 148
                },
                {
                    "start": 149,
                    "end": 321
                },
                {
                    "start": 322,
                    "end": 414
                },
                {
                    "start": 415,
                    "end": 564
                },
                {
                    "start": 567,
                    "end": 588
                },
                {
                    "start": 589,
                    "end": 731
                },
                {
                    "start": 732,
                    "end": 849
                },
                {
                    "start": 850,
                    "end": 999
                },
                {
                    "start": 1000,
                    "end": 1058
                },
                {
                    "start": 1061,
                    "end": 1226
                },
                {
                    "start": 1227,
                    "end": 1419
                },
                {
                    "start": 1420,
                    "end": 1558
                },
                {
                    "start": 1559,
                    "end": 1675
                },
                {
                    "start": 1678,
                    "end": 1697
                },
                {
                    "start": 1698,
                    "end": 1860
                },
                {
                    "start": 1861,
                    "end": 1945
                },
                {
                    "start": 1946,
                    "end": 2044
                }
            ],
            "ref_mentions": [
                {
                    "start": 300,
                    "end": 320,
                    "matchedPaperCorpusId": "218971783"
                },
                {
                    "start": 655,
                    "end": 661,
                    "matchedPaperCorpusId": "215416095"
                },
                {
                    "start": 1071,
                    "end": 1096,
                    "matchedPaperCorpusId": "255372747"
                },
                {
                    "start": 1725,
                    "end": 1745,
                    "matchedPaperCorpusId": "235899080"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.8828125
        },
        {
            "corpus_id": "270370902",
            "title": "VTrans: Accelerating Transformer Compression with Variational Information Bottleneck based Pruning",
            "text": "Datasets.We assess the efficacy of our proposed method across a comprehensive set of linguistic understanding tasks -the General Language Understanding Evaluation (GLUE) benchmark (Wang et al., 2018), Stanford Question Answering Dataset (SQuAD) version 1.1 (Rajpurkar et al., 2016) and WikiText-2 (Merity et al., 2016).\n\nBaseline Models.We compare our method variants with several previous techniques: DynaBERT (Hou et al., 2020), Block Pruning (Lagunas et al., 2021), PostPrune (Kwon et al., 2022), CoFi (Xia et al., 2022), FeatureCorr (Huang et al., 2023), DistilBERT (Sanh et al., 2020a), TinyBERT-GD (Jiao et al., 2019), MiniLM (Wang et al., 2020b), and HomoBERT (Liang et al., 2023a).We also compare our pruned LLaMA-2 models with previous semi-structured pruning techniques-SparseGPT (Frantar & Alistarh, 2023), Wanda (Sun et al., 2023) and structured pruning techniques-LLM-pruner (Ma et al., 2023), Bonsai (Dery et al., 2024).Details are deferred to Appendix A.2.\n\nTraining Details.The primary method utilizes entire datasets for pruning and finetuning, while the Fast and Faster variants operate on a randomly sampled subset, approximately 3% of the data.For Fast and Faster variants, 8000 samples are used for pruning and finetuning on large datasets and 2000 on smaller ones of GLUE and SQuAD.Finetuning uses 16000 samples for large datasets and the entire dataset for smaller ones.We conduct five runs with random seeds for each sparsity constraint, reporting average performance.Experiments conducted to come to the final hyper-parameter settings are provided in Appendix A.5.",
            "score": 0.5976776295770544,
            "section_title": "Implementation Details",
            "char_start_offset": 12449,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 9
                },
                {
                    "start": 9,
                    "end": 319
                },
                {
                    "start": 321,
                    "end": 337
                },
                {
                    "start": 337,
                    "end": 689
                },
                {
                    "start": 689,
                    "end": 934
                },
                {
                    "start": 934,
                    "end": 971
                },
                {
                    "start": 973,
                    "end": 990
                },
                {
                    "start": 990,
                    "end": 1164
                },
                {
                    "start": 1164,
                    "end": 1304
                },
                {
                    "start": 1304,
                    "end": 1393
                },
                {
                    "start": 1393,
                    "end": 1492
                },
                {
                    "start": 1492,
                    "end": 1589
                }
            ],
            "ref_mentions": [
                {
                    "start": 180,
                    "end": 199,
                    "matchedPaperCorpusId": "5034059"
                },
                {
                    "start": 297,
                    "end": 318,
                    "matchedPaperCorpusId": "204838007"
                },
                {
                    "start": 411,
                    "end": 428,
                    "matchedPaperCorpusId": "215415863"
                },
                {
                    "start": 537,
                    "end": 557,
                    "matchedPaperCorpusId": "268030766"
                },
                {
                    "start": 632,
                    "end": 652,
                    "matchedPaperCorpusId": "211296536"
                },
                {
                    "start": 790,
                    "end": 816,
                    "matchedPaperCorpusId": "255372747"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.385498046875
        },
        {
            "corpus_id": "267759551",
            "title": "EBFT: Effective and Block-Wise Fine-Tuning for Sparse LLMs",
            "text": "Fine-tuning for pruned LLMs.For LLMs, specific pruning methods (Ashkboos et al., 2024;An et al., 2023;Syed et al., 2023;Li et al., 2023) have been proposed.LoraPruner (Zhang et al., 2023a), LLM-pruner (Ma et al., 2023), and Compresso (Guo et al., 2023a) aim to remove entire attention heads or FFN units in the transformers (Vaswani et al., 2017), followed by fine-tuning on a large dataset using PEFT (Hu et al., 2021).However, these methods suffer from performance degradation and high retraining costs.SparseGPT (Frantar and Alistarh, 2023) employs OBS (Hassibi et al., 1993) to prune the weights of LLMs and recovers their performance through regression reconstruction.Wanda (Sun et al., 2023) proposes a new importance criterion, which approximates the criterion used in SparseGPT.DSnoT (Zhang et al., 2023d) aims to fine-tune sparse LLMs and designs a criterion to further reduce reconstruction error by reselecting masks.These methods require costly retraining or rely on approximation and heuristic optimization strategies, resulting in significant resource consumption or sub-optimal solutions.To address these challenges, we propose a fine-tuning framework called EBFT, which helps us obtain an optimal and convergent sparse model.EBFT can be integrated with any other pruning methods, requiring only a small number of samples from C4.When the initial mask M l 0 and weight W l 0 are provided, EBFT updates the weight W l t through backpropagation to optimize the reconstruction error L mentioned in Eq.4,ultimately achieving a convergent and optimal solution.Here, W l t represents the weight vector of the l-th block of the LLM in the t-th iteration.\n\n3 Methodology",
            "score": 0.5970216954078191,
            "section_title": "Related Work",
            "char_start_offset": 5882,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 28
                },
                {
                    "start": 28,
                    "end": 156
                },
                {
                    "start": 156,
                    "end": 420
                },
                {
                    "start": 420,
                    "end": 505
                },
                {
                    "start": 505,
                    "end": 673
                },
                {
                    "start": 673,
                    "end": 786
                },
                {
                    "start": 786,
                    "end": 928
                },
                {
                    "start": 928,
                    "end": 1241
                },
                {
                    "start": 1241,
                    "end": 1345
                },
                {
                    "start": 1345,
                    "end": 1515
                },
                {
                    "start": 1515,
                    "end": 1570
                },
                {
                    "start": 1570,
                    "end": 1662
                },
                {
                    "start": 1664,
                    "end": 1677
                }
            ],
            "ref_mentions": [
                {
                    "start": 324,
                    "end": 346,
                    "matchedPaperCorpusId": "13756489"
                },
                {
                    "start": 515,
                    "end": 543,
                    "matchedPaperCorpusId": "253237200"
                },
                {
                    "start": 556,
                    "end": 578,
                    "matchedPaperCorpusId": "61815367"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.259521484375
        },
        {
            "corpus_id": "275789021",
            "title": "The Journey Matters: Average Parameter Count over Pre-training Unifies Sparse and Dense Scaling Laws",
            "text": "As language models grow in size and train on more data, they consistently demonstrate improved performance (Brown et al., 2020;Kaplan et al., 2020;Hoffmann et al., 2024;Nakkiran et al., 2020). However, their enormous size poses an increasingly pressing challenge to their efficient deployment and equitable access. Sparse pre-training integrates neural network pruning (Han et al., 2015;LeCun et al., 1989;Hassibi et al., 1993;He et al., 2017) into pre-training and offers a promising solution to these challenges by activating only a subset of parameters during both training and inference, reducing computational costs; it gained prominence when Lottery Ticket Hypothesis (Frankle & Carbin, 2019) presented compelling evidence for its feasibility. Subsequent work introduced a series of sparse pre-training algorithms (Evci et al., 2020;Peste et al., 2021;Kuznedelev et al., 2024). \n\nWhile a growing body of research investigates pruning large language models (LLMs) post-training (Sun et al., 2024;Frantar & Alistarh, 2023;Xia et al., 2023), our work focuses on sparse pretraining. The combined challenges of LLM pre-training costs and pruning algorithm design present substantial obstacle to this direction of research. For example, Lottery Ticket Hypothesis (Frankle & Carbin, 2019) identifies trainable sparse sub-networks using iterative pruning and retraining, a process that becomes prohibitively expensive at the scale of contemporary LLMs. This enormous expense limits investigation to smaller-scale models, leaving the optimal strategies for sparse pretraining of LLMs largely unknown. Scaling laws -which predict how language modeling loss varies with model and data size -can help us extend insights from small-scale experiments to large models. This leads to a critical question: how does sparsity change the scaling laws, which inform practical LLM training design (Sardana et al., 2024;Grattafiori et al., 2024)?",
            "score": 0.5960776445039158,
            "section_title": "INTRODUCTION",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 192
                },
                {
                    "start": 193,
                    "end": 314
                },
                {
                    "start": 315,
                    "end": 749
                },
                {
                    "start": 750,
                    "end": 883
                },
                {
                    "start": 886,
                    "end": 1084
                },
                {
                    "start": 1085,
                    "end": 1223
                },
                {
                    "start": 1224,
                    "end": 1450
                },
                {
                    "start": 1451,
                    "end": 1597
                },
                {
                    "start": 1598,
                    "end": 1759
                },
                {
                    "start": 1760,
                    "end": 1929
                }
            ],
            "ref_mentions": [
                {
                    "start": 107,
                    "end": 127,
                    "matchedPaperCorpusId": "218971783"
                },
                {
                    "start": 147,
                    "end": 169,
                    "matchedPaperCorpusId": "247778764"
                },
                {
                    "start": 169,
                    "end": 191,
                    "matchedPaperCorpusId": "207808916"
                },
                {
                    "start": 369,
                    "end": 387,
                    "matchedPaperCorpusId": "2238772"
                },
                {
                    "start": 387,
                    "end": 406,
                    "matchedPaperCorpusId": "7785881"
                },
                {
                    "start": 406,
                    "end": 427,
                    "matchedPaperCorpusId": "61815367"
                },
                {
                    "start": 427,
                    "end": 443,
                    "matchedPaperCorpusId": "20157893"
                },
                {
                    "start": 820,
                    "end": 839,
                    "matchedPaperCorpusId": "208267757"
                },
                {
                    "start": 839,
                    "end": 858,
                    "matchedPaperCorpusId": "235606264"
                },
                {
                    "start": 858,
                    "end": 882,
                    "matchedPaperCorpusId": "260611140"
                },
                {
                    "start": 983,
                    "end": 1001,
                    "matchedPaperCorpusId": "259203115"
                },
                {
                    "start": 1881,
                    "end": 1903,
                    "matchedPaperCorpusId": "266693796"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.4248046875
        },
        {
            "corpus_id": "270560879",
            "title": "BlockPruner: Fine-grained Pruning for Large Language Models",
            "text": "With the rapid growth in the size and complexity of large language models (LLMs), the costs associated with their training and inference have escalated significantly. Research indicates that certain layers in LLMs harbor substantial redundancy, and pruning these layers has minimal impact on the overall performance. While various layer pruning methods have been developed based on this insight, they generally overlook the finer-grained redundancies within the layers themselves. In this paper, we delve deeper into the architecture of LLMs and demonstrate that finer-grained pruning can be achieved by targeting redundancies in multi-head attention (MHA) and multi-layer perceptron (MLP) blocks. We propose a novel, training-free structured pruning approach called BlockPruner. Unlike existing layer pruning methods, BlockPruner segments each Transformer layer into MHA and MLP blocks. It then assesses the importance of these blocks using perplexity measures and applies a heuristic search for iterative pruning. We applied BlockPruner to LLMs of various sizes and architectures and validated its performance across a wide range of downstream tasks. Experimental results show that BlockPruner achieves more granular and effective pruning compared to state-of-the-art baselines.",
            "score": 0.594842117166541,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.833984375
        },
        {
            "corpus_id": "271957102",
            "title": "Investigating Language-Specific Calibration For Pruning Multilingual Large Language Models",
            "text": "Recent advances in large language model (LLM) pruning have shown state-of-the-art (SotA) compression results in post-training and retraining-free settings while maintaining high predictive performance. However, previous research mainly considered calibrating based on English text, despite the multilingual nature of modern LLMs and their frequent use in non-English languages. In this paper, we set out to investigate calibrating the pruning of multilingual language models for monolingual applications. We present the first comprehensive empirical study, comparing different calibration languages for pruning multilingual models across diverse languages, tasks, models, and SotA pruning techniques. Our results offer practical suggestions, for example, calibrating in the target language can efficiently retain the language modeling capability but does not necessarily benefit downstream tasks. Through further analysis of latent subspaces, pruning masks, and individual neurons within pruned models, we find that while pruning generally preserves strong language-specific features, it may fail to retain language-specific neuron activation patterns and subtle, language-agnostic features associated with knowledge and reasoning that are needed for complex tasks.",
            "score": 0.5947006350790973,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.461181640625
        },
        {
            "corpus_id": "270063400",
            "title": "SPP: Sparsity-Preserved Parameter-Efficient Fine-Tuning for Large Language Models",
            "text": "During training and weight-merging phases, these learnable parameters are multiplied with the original frozen post-training pruned weights, achieving the effect of exactly maintaining the sparse pattern and ratio throughout all the processes.SPP is easy to implement and can be applied to a wide range of post-training pruning methods and LLMs with various sparsity ratios and patterns (unstructured and N:M sparsity).We evaluate SPP on the LLaMA (Touvron et al., 2023a) and LLaMA-2 (Touvron et al., 2023b) model families with two recent LLM post-training pruning methods, i.e., SparseGPT (Frantar & Alistarh, 2023) and Wanda (Sun et al., 2023).To illustrate the effectiveness of SPP, zeroshot evaluation results of LLaMA 7B/30B models with 75% sparsity ratio are shown in Fig. 1.\n\nThe main contributions of this paper are summarized in three key aspects:\n\n(1) We investigate model pruning methods in the era of LLMs and present a novel parameter-efficient fine-tuning algorithm, SPP, which can maintain model structure and sparsity during both training and weight-merging phases.\n\n(2) Extensive experiments on different post-training pruned LLMs with various sparsity patterns and ratios show the effectiveness of SPP for the efficient training of sparse LLMs.\n\n(3) To the best of our knowledge, this study is the first to systematically explore integrating efficient retraining with advanced post-training pruning methods for LLMs.",
            "score": 0.5946202986306894,
            "section_title": "Introduction",
            "char_start_offset": 3308,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 242
                },
                {
                    "start": 242,
                    "end": 418
                },
                {
                    "start": 418,
                    "end": 645
                },
                {
                    "start": 645,
                    "end": 780
                },
                {
                    "start": 782,
                    "end": 855
                },
                {
                    "start": 857,
                    "end": 1080
                },
                {
                    "start": 1082,
                    "end": 1261
                },
                {
                    "start": 1263,
                    "end": 1433
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.77001953125
        },
        {
            "corpus_id": "273375608",
            "title": "A Review on Edge Large Language Models: Design, Execution, and Applications",
            "text": "Pruning. Unstructured pruning removes individual weights or neurons, resulting in sparse models that are harder to optimize. Movement Pruning [150] adapts pruning decisions based on weight dynamics during fine-tuning, preserving important weights that exhibit significant movement. oBERT [94] introduces a second-order pruning method supporting both unstructured and block pruning. SparseGPT [44] treats pruning as a sparse regression problem, allowing for one-shot pruning without retraining. Plugand-Play [224] integrates activation-based importance to prune weights selectively, further improving pruning robustness in large-scale models. Wanda [160] prunes weights with the smallest magnitudes multiplied by the corresponding input activations, on a per-output basis. BESA [204] targets the overall pruning error with respect to individual transformer blocks and allocates layer-specific sparsity in a differentiable manner. \n\nOverall, pruning techniques for LLMs offer various strategies for balancing size reduction and performance retention. Structured pruning methods like CoFi [197] and LLM-Pruner [124] provide controlled reductions, preserving architecture integrity, while unstructured methods such as Movement Pruning [150] and oBERT [94] offer greater flexibility but may result in irregular, sparse models. In practice, the choice of method depends on the specific deployment scenario, considering the trade-offs between model size, computational efficiency, and task performance.",
            "score": 0.5944875800372429,
            "section_title": "Unstructured",
            "char_start_offset": 22245,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 8
                },
                {
                    "start": 9,
                    "end": 124
                },
                {
                    "start": 125,
                    "end": 281
                },
                {
                    "start": 282,
                    "end": 381
                },
                {
                    "start": 382,
                    "end": 493
                },
                {
                    "start": 494,
                    "end": 641
                },
                {
                    "start": 642,
                    "end": 771
                },
                {
                    "start": 772,
                    "end": 928
                },
                {
                    "start": 931,
                    "end": 1048
                },
                {
                    "start": 1049,
                    "end": 1321
                },
                {
                    "start": 1322,
                    "end": 1495
                }
            ],
            "ref_mentions": [
                {
                    "start": 288,
                    "end": 292,
                    "matchedPaperCorpusId": "247446572"
                },
                {
                    "start": 392,
                    "end": 396,
                    "matchedPaperCorpusId": "255372747"
                },
                {
                    "start": 648,
                    "end": 653,
                    "matchedPaperCorpusId": "259203115"
                },
                {
                    "start": 1086,
                    "end": 1091,
                    "matchedPaperCorpusId": "247922354"
                },
                {
                    "start": 1107,
                    "end": 1112,
                    "matchedPaperCorpusId": "258823276"
                },
                {
                    "start": 1247,
                    "end": 1251,
                    "matchedPaperCorpusId": "247446572"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.8525390625
        },
        {
            "corpus_id": "273501861",
            "title": "FedSpaLLM: Federated Pruning of Large Language Models",
            "text": "Large Language Models (LLMs) such as GPT (OpenAI, 2023) and LlaMA (Touvron et al., 2023) have recently transformed the field of Natural Language Processing (NLP) due to their ability to perform exceptionally well across a variety of complex language benchmarks. However, the increasing scale of these models, which can contain billions of parameters, also brings significant computational and storage costs. The high memory and inference costs make it challenging to deploy LLMs in real-world applications where resources are constrained (Bai et al., 2024a). Consequently, there has been an increasing interest in model compression techniques such as pruning, quantization, and knowledge distillation, which aim to reduce the computational load while maintaining model performance (Zhu et al., 2023). Among these techniques, pruning has emerged as a highly effective approach for reducing the size and complexity of LLMs by introducing sparsity into the models. \n\nDespite the recent success of LLM pruning methods, existing approaches predominantly assume that the calibration data used for pruning is publicly available (Frantar and Alistarh, 2023;Sun et al., 2023). However, in many real-world scenarios, especially when dealing with sensitive applications like medical agents or financial systems, the data used for pruning might be private and cannot be shared openly (Ren et al., 2024). On the other hand, Federated Learning (FL), a distributed machine learning technique that enables multiple clients to collaboratively train models without sharing their private data, has gained significant popularity in traditional machine learning (Zhang et al., 2021). However, most works on LLMs in FL settings have focused on fine-tuning. Due to the intrinsic differences between fine-tuning and pruning, existing FL-based fine-tuning methods cannot handle the problem of pruning LLMs with private data. \n\nTo address the challenges posed by pruning LLMs in federated settings, where private data cannot be shared and heterogeneity exists among clients, we propose a novel method called FedSpaLLM (Federated Sparse LLM). FedSpaLLM is the first framework that allows pruning LLMs under a federated learning setting with resource heterogeneity.",
            "score": 0.5938223709132173,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 261
                },
                {
                    "start": 262,
                    "end": 407
                },
                {
                    "start": 408,
                    "end": 558
                },
                {
                    "start": 559,
                    "end": 800
                },
                {
                    "start": 801,
                    "end": 961
                },
                {
                    "start": 964,
                    "end": 1167
                },
                {
                    "start": 1168,
                    "end": 1391
                },
                {
                    "start": 1392,
                    "end": 1662
                },
                {
                    "start": 1663,
                    "end": 1734
                },
                {
                    "start": 1735,
                    "end": 1899
                },
                {
                    "start": 1902,
                    "end": 2115
                },
                {
                    "start": 2116,
                    "end": 2237
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.53955078125
        },
        {
            "corpus_id": "268875867",
            "title": "Rethinking Pruning for Vision-Language Models: Strategies for Effective Sparsity and Performance Restoration",
            "text": "We consider several pruning techniques, including Global Magnitude Pruning, Gradient-based Pruning, SparseGPT (Frantar and Alistarh, 2023), and Wanda (Sun et al., 2023a).Global Magnitude Pruning prunes are based on weight magnitude, while Gradient-based Pruning prunes use the product of first-order gradient and weight magnitude (Yi-Lin Sung, 2024).SparseGPT is a layer-wise Hessian-based method, and Wanda utilizes weight magnitude and input activation norm for layer-wise pruning.Additionally, we compare against ECoFLaP (Yi-Lin Sung, 2024), which adopts a zero-order gradient-based layer-wise sparsity for vision-language models.We also compare SparseLoRA against DS\u25cbT (Zhang et al., 2024) that updates the masks after pruning.",
            "score": 0.5937126843252378,
            "section_title": "Baselines.",
            "char_start_offset": 13390,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 170
                },
                {
                    "start": 170,
                    "end": 350
                },
                {
                    "start": 350,
                    "end": 483
                },
                {
                    "start": 483,
                    "end": 633
                },
                {
                    "start": 633,
                    "end": 731
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.6279296875
        },
        {
            "corpus_id": "270257857",
            "title": "Pruner-Zero: Evolving Symbolic Pruning Metric from scratch for Large Language Models",
            "text": "In-context learning, a pivotal capability of large language models (LLMs), is crucial for tasks requiring adaptability and reasoning without explicit retraining.To assess this capability within various models, we focused our evaluation on the GSM8K dataset, comprised of diverse and complex grade school math problems that effectively challenge the models' reasoning abilities.Utilizing the LightLLM framework (ModelTC, 2023), a Python-based tool optimized for efficient and scalable inference with LLMs, we tested the LLaMA2 13B model's in-context learning performance using a fixed set of eight demonstrations aligned with the Chain-of-Thought (Wei et al., 2022) approach.The  (Frantar & Alistarh, 2023) 0.1152 Wanda (Sun et al., 2024) 0.1312 Pruner-Zero 0.1403 comparative analysis presented in Table 7 indicates that Pruner-Zero outperforms other pruning approaches such as SparseGPT (Frantar & Alistarh, 2023) and Wanda (Sun et al., 2024) in the in-context learning tasks, despite the general trend of decreased performance post-pruning.This observation, consistent with findings from recent studies (Li et al., 2024c) documenting a reduction in in-context learning capabilities of LLMs following quantization, highlights a significant challenge in the field of model pruning and optimization: preserving the nuanced capabilities of LLMs while reducing their computational overhead.Although Pruner-Zero shows a marked improvement over other methods, the noticeable decline in the performance of the unpruned (Dense) model underscores the delicate balance between model size and functionality, particularly in tasks requiring high cognitive functions such as reasoning and comprehension.",
            "score": 0.5935071323447518,
            "section_title": "Evaluation of In-Context Learning",
            "char_start_offset": 26100,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 161
                },
                {
                    "start": 161,
                    "end": 377
                },
                {
                    "start": 377,
                    "end": 674
                },
                {
                    "start": 674,
                    "end": 1042
                },
                {
                    "start": 1042,
                    "end": 1387
                },
                {
                    "start": 1387,
                    "end": 1691
                }
            ],
            "ref_mentions": [
                {
                    "start": 646,
                    "end": 664,
                    "matchedPaperCorpusId": "246411621"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.607421875
        },
        {
            "corpus_id": "268032346",
            "title": "BESA: Pruning Large Language Models with Blockwise Parameter-Efficient Sparsity Allocation",
            "text": "Methods 1-7B 1-13B Models. Our primary focus for evaluation centers on the LLaMA (Touvron et al., 2023a) family of models, renowned as one of the most prominent series of Large Language Models (LLMs). Specifically, we assessed our methods across various model sizes, including LLaMA-7B/13B/30B/65B, and LLaMA2-7B/13B/70B. Notably, our methodology exhibits consistent improvements and is not bound by the size of the LLaMA model. \n\nBenchmarks. Our initial assessment centers on evaluating the perplexity of pruned LLM models, a widely recognized metric renowned for its reliability and resilience in appraising LLM performance. \n\nIn alignment with prior studies (Frantar & Alistarh, 2023;Sun et al., 2023), we primarily measure model perplexity using the WikiText2 (Merity, 2016), C4 (Raffel et al., 2020), and PTB (Marcus et al., 1994) datasets. In addition to assessing perplexity, we undertake an exhaustive examination of the zero-shot capabilities of pruned models across six standard common-sense benchmark datasets. These benchmarks encompass PIQA (Bisk et al., 2020), BoolQ (Clark et al., 2019), Hel-laSwag (Zellers et al., 2019), WinoGrande (Sakaguchi et al., 2021), as well as the ARC Easy and Challenge (Clark et al., 2018) tasks. \n\nBaselines. We evaluate the following established methods as baselines: (i) SparseGPT, which divides the task of pruning LLM models into a sparse regression problem for a set of transformer blocks, subsequently solving these problems with an approximate sparse regression solver. It is worth noting that SparseGPT updates the values of unpruned weights. (ii) Wanda, a method that leverages the product of weight magnitude and L2 normalization of input activations to determine the importance of LLM model weights, followed by pruning weights with lower importance.",
            "score": 0.5928192290129929,
            "section_title": "Datasets",
            "char_start_offset": 19139,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 26
                },
                {
                    "start": 27,
                    "end": 200
                },
                {
                    "start": 201,
                    "end": 321
                },
                {
                    "start": 322,
                    "end": 428
                },
                {
                    "start": 431,
                    "end": 442
                },
                {
                    "start": 443,
                    "end": 626
                },
                {
                    "start": 629,
                    "end": 845
                },
                {
                    "start": 846,
                    "end": 1021
                },
                {
                    "start": 1022,
                    "end": 1240
                },
                {
                    "start": 1243,
                    "end": 1253
                },
                {
                    "start": 1254,
                    "end": 1521
                },
                {
                    "start": 1522,
                    "end": 1595
                },
                {
                    "start": 1596,
                    "end": 1806
                }
            ],
            "ref_mentions": [
                {
                    "start": 783,
                    "end": 804,
                    "matchedPaperCorpusId": "204838007"
                },
                {
                    "start": 814,
                    "end": 835,
                    "matchedPaperCorpusId": "5151364"
                },
                {
                    "start": 1054,
                    "end": 1073,
                    "matchedPaperCorpusId": "208290939"
                },
                {
                    "start": 1149,
                    "end": 1173,
                    "matchedPaperCorpusId": "199370376"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.55615234375
        },
        {
            "corpus_id": "264590698",
            "title": "LoRAShear: Efficient Large Language Model Structured Pruning and Knowledge Recovery",
            "text": "The advent of Large Language Models (LLMs) (Zhao et al., 2023;Hadi et al., 2023) has marked a significant milestone in evolution of artificial intelligence. These models, distinguished by their extensive parameter sizes, have demonstrated emergent abilities (Wei et al., 2022), catalyzing breakthroughs not only in the realm of natural language processing but also across tasks in various domains (Driess et al., 2023). This has opened up new pos-sibilities for advancing towards Artificial General Intelligence (AGI) (Everitt et al., 2018;Bubeck et al., 2023). However, the enormous size of LLMs, typically ranging from tens to hundreds of billions of parameters (Touvron et al., 2023), incurs substantial computational costs of both processing power and memory requirements. \n\nStructured pruning is an effective way to deliver compact DNNs via identifying and removing redundant structures then recovering the lost knowledge (Han et al., 2015;Chen et al., 2021b). However, its application onto LLMs is facing significant challenges, due to the requirements of massive computational resources and the unavailable training datasets of both pretraining and instructed fine-tuning datasets (Brown et al., 2020). Consequently, the paradigms could be largely categorized as pruning under limited or full resources. For the limited-resource setup, recent pruning works (Ma et al., 2023;Zhang et al., 2023;Sun et al., 2023) uses Low-Rank-Adaptor (LoRA) (Hu et al., 2021) during either pruning and instructed fine-tuning stage to reduce the resource requirements, yet still face significant performance degradation to the full LLMs. For the fullresouce setup, Sheared-LLaMA (Xia et al., 2023) conducts structured pruning on the original LLMs to directly achieve compact counterparts outperforming the equal sizes of LLMs trained from scratch, while requires significant GPU powers that might be not feasible for the public users.",
            "score": 0.5924361228008609,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 156
                },
                {
                    "start": 157,
                    "end": 419
                },
                {
                    "start": 420,
                    "end": 561
                },
                {
                    "start": 562,
                    "end": 776
                },
                {
                    "start": 779,
                    "end": 965
                },
                {
                    "start": 966,
                    "end": 1209
                },
                {
                    "start": 1210,
                    "end": 1310
                },
                {
                    "start": 1311,
                    "end": 1625
                },
                {
                    "start": 1626,
                    "end": 1922
                }
            ],
            "ref_mentions": [
                {
                    "start": 945,
                    "end": 964,
                    "matchedPaperCorpusId": "235899080"
                },
                {
                    "start": 1188,
                    "end": 1208,
                    "matchedPaperCorpusId": "218971783"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.434326171875
        },
        {
            "corpus_id": "268248477",
            "title": "DPPA: Pruning Method for Large Language Model to Model Merging",
            "text": "Traditional pruning techniques are a type of model compression that aim to decrease the number of parameters in a model (Zhu et al., 2023). There have been several studies conducted on this topic, both in the era of pretrained language models and before (Hubara et al., 2021;Mozer and Smolensky, 1988;Han et al., 2015a;Lin et al., 2019). However, progress in these studies has been relatively slow in the era of large language models, as pruning requires a substantial amount of data for fine-tuning, which is not feasible for such models. To tackle this issue, LORA fine-tuning was proposed by Ma et al. (2023) to restore the original performance. Recently, some studies have shifted their focus to pruning methods that do not necessitate fine-tuning. For instance, SparseGPT (Frantar and Alistarh, 2023) utilizes the Hessian matrix for pruning and reduces reconstruction error through subsequent weight updates. Wanda (Sun et al., 2023) combines weight magnitudes with input activations to retain parameters that better align with the current data distribution. DSOT (Zhang et al., 2023c) proposes a parameter adjustment method to minimize the discrepancy between the source model parameters and the pruned model parameters. OWL (Yin et al., 2023) introduces nonuniform layered sparsity, which is advantageous for higher pruning rates.",
            "score": 0.5921582356102105,
            "section_title": "Pruning Technique",
            "char_start_offset": 5296,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 139
                },
                {
                    "start": 140,
                    "end": 337
                },
                {
                    "start": 338,
                    "end": 539
                },
                {
                    "start": 540,
                    "end": 648
                },
                {
                    "start": 649,
                    "end": 752
                },
                {
                    "start": 753,
                    "end": 913
                },
                {
                    "start": 914,
                    "end": 1063
                },
                {
                    "start": 1064,
                    "end": 1226
                },
                {
                    "start": 1227,
                    "end": 1337
                }
            ],
            "ref_mentions": [
                {
                    "start": 254,
                    "end": 275,
                    "matchedPaperCorpusId": "231934142"
                },
                {
                    "start": 275,
                    "end": 301,
                    "matchedPaperCorpusId": "17651092"
                },
                {
                    "start": 301,
                    "end": 319,
                    "matchedPaperCorpusId": "2238772"
                },
                {
                    "start": 319,
                    "end": 336,
                    "matchedPaperCorpusId": "85459412"
                },
                {
                    "start": 595,
                    "end": 611,
                    "matchedPaperCorpusId": "258823276"
                },
                {
                    "start": 777,
                    "end": 805,
                    "matchedPaperCorpusId": "255372747"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.63916015625
        },
        {
            "corpus_id": "276482499",
            "title": "Towards Efficient Automatic Self-Pruning of Large Language Models",
            "text": "Despite exceptional capabilities, Large Language Models (LLMs) still face deployment challenges due to their enormous size. Post-training structured pruning is a promising solution that prunes LLMs without the need for retraining, reducing computational overhead, and it is hardware-deployment friendly. However, the training-free nature of post-training structured pruning leads to significant performance degradation. We argue that the key to mitigating this issue lies in accurately determining the pruning rate for each layer. Meanwhile, we find that LLMs may have prior knowledge about their own redundancy. Based on this insight, we introduce $\\textbf{Self-Pruner}$ an end-to-end automatic self-pruning framework for LLMs, which efficiently search layer-wise pruning rates. Specifically, $\\textbf{Self-Pruner}$ leverages LLMs to autonomously execute the entire evolutionary search process to search for pruning rate configurations. In this process, LLMs are used to generate populations, select parent solutions from the current population, and perform crossover and mutation operations to produce offspring solutions. In this way, LLMs automatically generate and evaluate a large number of candidate solutions, effectively converging to find the pruning rate configurations with minimal human intervention. Extensive experiments demonstrate $\\textbf{Self-Pruner}$'s better performance compared to existing state-of-the-art methods. Notably, $\\textbf{Self-Pruner}$ prunes LLaMA-2-70B to 49B level with only 0.80$\\%$ drop in accuracy across seven commonsense reasoning tasks, achieving a 1.39$\\times$ speedup on NVIDIA A100 80GB GPU. Further pruning to 35B level resulted in only a 3.80$\\%$ decrease in accuracy while obtaining a 1.70$\\times$ speedup.",
            "score": 0.5921028310100698,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.71875
        },
        {
            "corpus_id": "269930256",
            "title": "Your Transformer is Secretly Linear",
            "text": "Research on evaluating and leveraging sparsity for model pruning has become one of the most significant topics within the machine learning community.Molchanov et al. (2016) explored the sparsity of convolutional neural networks through backpropagation and fine-tuning, laying the groundwork for understanding the potential applications of sparsity in resource-efficient inference.The verification approach utilized in a more recent DejaVu (Borse et al., 2023) paper is based on Molchanov's research.\n\nPrevious work (Kurtic et al., 2023) has addressed the challenges associated with naive sparse finetuning in the context of LLMs.Issues such as training instability, poor recovery, and overfitting have prompted an exploration for alternative approaches.The study introduced SquareHead distillation, a method that consistently addresses the challenges in naive sparse fine-tuning, demonstrating accurate recovery even at high sparsity levels.\n\nIn a more recent study WANDA (Sun et al., 2023), the authors present a technique for pruning LLMs to high degrees of sparsity without modifying the remaining weights.Unlike SparseGPT (Frantar and Alistarh, 2023), WANDA seamlessly implements pruning in a single forward pass, leveraging feature norm statistics for efficient pruning.This method achieves noticeable sparsity without the need for a sophisticated iterative weight update procedure, differentiating itself from other pruning techniques.\n\nContextual sparsity introduced by Borse et al. ( 2023) involves sparsifying MLP and attention blocks in LLMs to reduce generation latency.The study identifies essential attention heads and MLP neurons for computation, maintaining performance across in-context learning and language modeling tasks.\n\nRecent work by Ashkboos et al. (2024) shows that LLMs can be sparsified post hoc.Their approach introduces a scheme to replace each weight matrix with a smaller dense matrix, thereby reducing the dimensionality of the networks.Their results show that models of different sizes can be reduced with varying degrees of success.For example, LLAMA-2 70B and OPT 66B can maintain 99% zero-shot accuracy while reducing 25% of the parameters reduced while performing LLM evaluation tasks.",
            "score": 0.5916030827335731,
            "section_title": "Related Work",
            "char_start_offset": 2280,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 149
                },
                {
                    "start": 149,
                    "end": 380
                },
                {
                    "start": 380,
                    "end": 499
                },
                {
                    "start": 501,
                    "end": 629
                },
                {
                    "start": 629,
                    "end": 753
                },
                {
                    "start": 753,
                    "end": 941
                },
                {
                    "start": 943,
                    "end": 1109
                },
                {
                    "start": 1109,
                    "end": 1275
                },
                {
                    "start": 1275,
                    "end": 1441
                },
                {
                    "start": 1443,
                    "end": 1581
                },
                {
                    "start": 1581,
                    "end": 1740
                },
                {
                    "start": 1742,
                    "end": 1823
                },
                {
                    "start": 1823,
                    "end": 1969
                },
                {
                    "start": 1969,
                    "end": 2066
                },
                {
                    "start": 2066,
                    "end": 2222
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.42822265625
        },
        {
            "corpus_id": "270370902",
            "title": "VTrans: Accelerating Transformer Compression with Variational Information Bottleneck based Pruning",
            "text": "Integrating the Variational Information Bottleneck principle, our structured pruning method efficiently prunes redundant units across all structural levels in transformers.This results in upto 70% higher compression compared to prior methods, accompanied by minimal accuracy loss (<1%) on GLUE and SQuAD tasks, along with up to 4x inference speedup.\n\nUnlike previous approaches, we address both parameter and FLOPs constraints with our task-specific distillation approach, requiring significantly lower training resources (upto 60x less) than task-agnostic methods while achieving similar performance.Our efficient compression variants, namely Fast and Faster-VTrans, deliver over 10x pruning-speedup with performance comparable to previous state-of-the-art approaches.Our evaluation encompasses various architectures, including decoder-based GPT models.Demonstrating the effectiveness and scalability of our method, we compress the large LLaMA-2 model by 50% with better performance (7 points) and enhanced inference speed (1.2\u00d7) than other structured pruning methods with lower compression time.Our qualitative analysis supports the competitiveness of our approach, with potential for further quantification in future work.\n\nPaul Ofir Zafrir, Ariel Larey, Guy Boudoukh, Haihao Shen, and Moshe Wasserblat.Prune once for all: Sparse pre-trained language models, 2021.\n\nMichael Zhu and Suyog Gupta.To prune, or not to prune: exploring the efficacy of pruning for model compression, 2017.",
            "score": 0.5915481951942171,
            "section_title": "Conclusion",
            "char_start_offset": 21884,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 172
                },
                {
                    "start": 172,
                    "end": 349
                },
                {
                    "start": 351,
                    "end": 601
                },
                {
                    "start": 601,
                    "end": 769
                },
                {
                    "start": 769,
                    "end": 854
                },
                {
                    "start": 854,
                    "end": 1097
                },
                {
                    "start": 1097,
                    "end": 1225
                },
                {
                    "start": 1227,
                    "end": 1306
                },
                {
                    "start": 1306,
                    "end": 1367
                },
                {
                    "start": 1369,
                    "end": 1397
                },
                {
                    "start": 1397,
                    "end": 1486
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.5947265625
        },
        {
            "corpus_id": "271909421",
            "title": "Enhancing One-shot Pruned Pre-trained Language Models through Sparse-Dense-Sparse Mechanism",
            "text": "Performance on Language Modeling. Table 4 demonstrates the efficacy of the SDS framework on language modeling with SparseGPT / Wanda as baseline pruning methods. On average, SDS improves perplexity by 1.8 points for the 50% sparsity level, 7.5 points for the 2:4 sparsity level, and 3.3 points for the 4:8 sparsity level across all models. For example, at 2:4 sparsity, SDS-Wanda reduces perplexity by 39.61 points for OPT-350M, demonstrating the most substantial improvement. These results highlight SDS's ability to produce more pruning-friendly models, significantly enhancing performance compared to the initial pruned baselines. In summary, our evaluations convincingly demonstrate the robustness and efficacy of the SDS framework across a variety of sparsity configurations. Both language modeling and zero-shot downstream multitask performance metrics affirm the consistent superiority of SDS over baselines. Therefore, SDS is an effective pruning method for PLMs.",
            "score": 0.5912749738882045,
            "section_title": "Results",
            "char_start_offset": 16026,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 33
                },
                {
                    "start": 34,
                    "end": 161
                },
                {
                    "start": 162,
                    "end": 339
                },
                {
                    "start": 340,
                    "end": 476
                },
                {
                    "start": 477,
                    "end": 633
                },
                {
                    "start": 634,
                    "end": 780
                },
                {
                    "start": 781,
                    "end": 915
                },
                {
                    "start": 916,
                    "end": 971
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.445556640625
        },
        {
            "corpus_id": "267413136",
            "title": "Shortened LLaMA: A Simple Depth Pruning for Large Language Models",
            "text": "Structured pruning of modern large language models (LLMs) has emerged as a way of decreasing their high computational needs. Width pruning reduces the size of projection weight matrices (e.g., by removing attention heads) while maintaining the number of layers. Depth pruning, in contrast, removes entire layers or blocks, while keeping the size of the remaining weights unchanged. Most current research focuses on either width-only or a blend of width and depth pruning, with little comparative analysis between the two units (width vs. depth) concerning their impact on LLM inference efficiency. In this work, we show that simple depth pruning can effectively compress LLMs while achieving comparable or superior performance to recent width pruning studies. Our pruning method boosts inference speeds, especially under memory-constrained conditions that require limited batch sizes for running LLMs, where width pruning is ineffective. In retraining pruned models for quality recovery, continued pretraining on a large corpus markedly outperforms LoRA-based tuning, particularly at severe pruning ratios. We hope this work can help build compact yet capable LLMs. Code and models can be found at: https://github.com/Nota-NetsPresso/shortened-llm",
            "score": 0.5909268388449557,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.60498046875
        },
        {
            "corpus_id": "270621063",
            "title": "Dependency-Aware Semi-Structured Sparsity of GLU Variants in Large Language Models",
            "text": "Apart from assessing perplexity, we comprehensively evaluate the performance of pruned LLaMA2-70B models in the widely used commonsense reasoning tasks. \n\nIn Table 2, we present the performance of different sparse LLaMA2-70B models on downstream tasks with prompting. The results show that our method outperforms SparseGPT and Wanda in most tasks at semistructured N:M sparsity pattern. The only exception is that SparseGPT outperforms both Wanda and DaSS in Winogrande task. 50% unstructured SparseGPT pruned model even outperforms the dense model. Such results align with the prevailing LLM compression works (Dettmers & Zettlemoyer, 2023;Frantar & Alistarh, 2023), where single task results can be noisy and the mean accuracy provides stable and reliable results. For unstructured sparsity, our method outperforms Wanda sharing the same complexity level.",
            "score": 0.5907947296208367,
            "section_title": "Downstream Tasks",
            "char_start_offset": 18434,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 152
                },
                {
                    "start": 155,
                    "end": 267
                },
                {
                    "start": 268,
                    "end": 386
                },
                {
                    "start": 387,
                    "end": 475
                },
                {
                    "start": 476,
                    "end": 549
                },
                {
                    "start": 550,
                    "end": 766
                },
                {
                    "start": 767,
                    "end": 857
                }
            ],
            "ref_mentions": [
                {
                    "start": 611,
                    "end": 641,
                    "matchedPaperCorpusId": "254853733"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.41650390625
        },
        {
            "corpus_id": "274233986",
            "title": "Reassessing Layer Pruning in LLMs: New Insights and Methods",
            "text": "In recent years, large language models (LLMs) have achieved unprecedented success in many fields, such as text generation (Achiam et al., 2023;Touvron et al., 2023), semantic analysis (Deng et al., 2023;Zhang et al., 2023b) and machine translation (Zhang et al., 2023a;Wang et al., 2023). However, these achievements come with massive resource consumption, posing significant challenges for deployment on resource-constrained devices. To address these challenges, numerous techniques have been developed to create more efficient LLMs, including pruning (Ma et al., 2023a;Sun et al., 2023), knowledge distillation (Xu et al., 2024;Gu et al., 2024), quantization (Lin et al., 2024;Liu et al., 2023), low-rank factorization (Saha et al., 2023;Zhao et al., 2024a), and system-level inference acceleration (Shah et al., 2024;Lee et al., 2024). Among these methods, pruning has emerged as a promising solution to mitigate the resource demands of LLMs. By selectively removing redundant patterns-such as parameters (Sun et al., 2023), attention heads (Ma et al., 2023a) and layers (Men et al., 2024)-pruning aims to slim down the model while maintaining its original performance as much as possible. Among different types of pruning, layer pruning (Kim et al., 2024;Siddiqui et al., 2024) has garnered particular interest due to its direct impact on pruning the model's depth, thereby decreasing both computational complexity and memory usage. Additionally, thanks to the nice structure of the existing LLMs such as Llama (Dubey et al., 2024), whose transformer blocks have the exactly same dimension of input and output, layer pruning becomes a straightforward and simple solution. Therefore, in this paper, we focus on layer pruning.",
            "score": 0.5902604182867865,
            "section_title": "INTRODUCTION",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 288
                },
                {
                    "start": 289,
                    "end": 434
                },
                {
                    "start": 435,
                    "end": 838
                },
                {
                    "start": 839,
                    "end": 945
                },
                {
                    "start": 946,
                    "end": 1192
                },
                {
                    "start": 1193,
                    "end": 1436
                },
                {
                    "start": 1437,
                    "end": 1675
                },
                {
                    "start": 1676,
                    "end": 1728
                }
            ],
            "ref_mentions": [
                {
                    "start": 184,
                    "end": 203,
                    "matchedPaperCorpusId": "258377669"
                },
                {
                    "start": 203,
                    "end": 223,
                    "matchedPaperCorpusId": "263829356"
                },
                {
                    "start": 248,
                    "end": 269,
                    "matchedPaperCorpusId": "255942578"
                },
                {
                    "start": 553,
                    "end": 571,
                    "matchedPaperCorpusId": "258823276"
                },
                {
                    "start": 661,
                    "end": 679,
                    "matchedPaperCorpusId": "258999941"
                },
                {
                    "start": 721,
                    "end": 740,
                    "matchedPaperCorpusId": "262233736"
                },
                {
                    "start": 1044,
                    "end": 1062,
                    "matchedPaperCorpusId": "258823276"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.56298828125
        },
        {
            "corpus_id": "247794014",
            "title": "TextPruner: A Model Pruning Toolkit for Pre-Trained Language Models",
            "text": "Large pre-trained language models (PLMs) (Devlin et al., 2019;Liu et al., 2019) have achieved great success in a variety of NLP tasks. However, it is difficult to deploy them for real-world applications where computation and memory resources are limited. Reducing the pre-trained model size and speeding up the inference have become a critical issue.\n\nPruning is a common technique for model compression. It identifies and removes redundant or less important neurons from the networks. From the view of the model structure, pruning methods can be categorized into unstructured pruning and structured pruning. In the unstructured pruning, each model parameter is individually removed if it reaches some criteria based on the magnitude or importance score (Han et al., 2015;Zhu and Gupta, 2018;. The unstructured pruning results in sparse matrices and allows for significant model compression, but the inference speed can hardly be improved without specialized devices. While in the structured pruning, rows or columns of the parameters are removed from the weight matrices (McCarley, 2019;Michel et al., 2019;Voita et al., 2019;Lagunas et al., 2021;Hou et al., 2020). Thus, the resulting model speeds up on the common CPU and GPU devices.\n\nPruning methods can also be classified into optimization-free methods (Michel et al., 2019) and the ones that involve optimization (Frankle and Carbin, 2019;Lagunas et al., 2021). The latter usually achieves higher performance, but the former runs faster and is more convenient to use.\n\nPruning PLMs has been of growing interest. Most of the works focus on reducing transformer size while ignoring the vocabulary (Abdaoui et al., 2020). Pruning vocabulary can greatly reduce the model size for multilingual PLMs.\n\nIn this paper, we present TextPruner, a model pruning toolkit for PLMs. It combines both transformer pruning and vocabulary pruning. The purpose of TextPruner is to offer a universal, fast, and easy-to-use tool for model compression. We expect it can be accessible to users with little model training experience. Therefore, we implement the structured optimization-free",
            "score": 0.5901309435441616,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [],
            "ref_mentions": [
                {
                    "start": 41,
                    "end": 62,
                    "matchedPaperCorpusId": "52967399"
                },
                {
                    "start": 772,
                    "end": 792,
                    "matchedPaperCorpusId": "27494814"
                },
                {
                    "start": 1088,
                    "end": 1108,
                    "matchedPaperCorpusId": "166227946"
                },
                {
                    "start": 1108,
                    "end": 1127,
                    "matchedPaperCorpusId": "162183964"
                },
                {
                    "start": 1127,
                    "end": 1148,
                    "matchedPaperCorpusId": "237485472"
                },
                {
                    "start": 1148,
                    "end": 1165,
                    "matchedPaperCorpusId": "215415863"
                },
                {
                    "start": 1309,
                    "end": 1330,
                    "matchedPaperCorpusId": "166227946"
                },
                {
                    "start": 1370,
                    "end": 1396,
                    "matchedPaperCorpusId": "53388625"
                },
                {
                    "start": 1396,
                    "end": 1417,
                    "matchedPaperCorpusId": "237485472"
                },
                {
                    "start": 1652,
                    "end": 1674,
                    "matchedPaperCorpusId": "222291680"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.56298828125
        },
        {
            "corpus_id": "269605957",
            "title": "COPAL: Continual Pruning in Large Language Generative Models",
            "text": "The performance of the dense (no pruning) models sets a fundamental baseline, indicating the optimal performance without any compromise due to pruning.In the LLaMA-7B model, the dense configuration yields an average PPL of 7.714.This benchmark is slightly lower in the LLaMA-30B and LLaMA-65B models, with average PPLs of 6.131 and 6.139, respectively.These variations suggest a nuanced impact of model scale on language processing capabilities, with larger models inherently capable of better performance pre-pruning.\n\nUnstructured Continual Pruning.In Shifting to the LLaMA-30B model with unstructured pruning, COPAL again leads in performance metrics.It achieves a notable 98.2% improvement in average BWT over SparseGPT and a minimal PPL performance drop, for instance, from 8.159 (Dense) to 9.077 on PTB.These results highlight COPAL's scalability and its effectiveness in handling moderately larger models without compromising on the pruning efficiency or language understanding.\n\nIn the LLaMA-65B model, COPAL's efficacy is further amplified.The method achieves nearly negligible average BWT (0.001), a 99.7% improvement over SparseGPT.On the C4 dataset, COPAL significantly achieves PPL performance improvement from 8.878 (Dense) to 8.696, showcasing its exceptional capability in large-scale models.This performance indicates an continual pruning strategy that not only reduces model size but also preserves, and in some aspects, enhances model comprehension abilities.\n\nSemi-structured N:M Continual Pruning.In the realm of semi-structured N:M continual pruning, the COPAL technique stands out for its effectiveness across various pruning configurations, notably within the context of the LLaMA-65B model.When applied to a 2:4 pruning pattern, CO-PAL achieves a 98.2% improvement in average Backward Weight Transfer (BWT), reducing it to 0.038 compared to SparseGPT's 2.128.",
            "score": 0.5897891859071636,
            "section_title": "Results",
            "char_start_offset": 21242,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 151
                },
                {
                    "start": 151,
                    "end": 229
                },
                {
                    "start": 229,
                    "end": 352
                },
                {
                    "start": 352,
                    "end": 518
                },
                {
                    "start": 520,
                    "end": 551
                },
                {
                    "start": 551,
                    "end": 654
                },
                {
                    "start": 654,
                    "end": 809
                },
                {
                    "start": 809,
                    "end": 985
                },
                {
                    "start": 987,
                    "end": 1049
                },
                {
                    "start": 1049,
                    "end": 1143
                },
                {
                    "start": 1143,
                    "end": 1308
                },
                {
                    "start": 1308,
                    "end": 1478
                },
                {
                    "start": 1480,
                    "end": 1518
                },
                {
                    "start": 1518,
                    "end": 1715
                },
                {
                    "start": 1715,
                    "end": 1884
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.390625
        },
        {
            "corpus_id": "270621063",
            "title": "Dependency-Aware Semi-Structured Sparsity of GLU Variants in Large Language Models",
            "text": "Pruning LLM. Neural network pruning in LLM can be broadly categorized into two groups: structured pruning (Ma et al., 2023;Zhang et al., 2023) and unstructured pruning (Frantar & Alistarh, 2023;Sun et al., 2024). Ma et al. (2023) proposes a dependency detection algorithm to detect and prune non-critical grouped structures followed by LoRA fine-tuning. Although structured pruning can usually achieve better hardware efficiency, the accuracy drops a lot even at a low compression rate. Unstructured pruning can yield a higher compression rate and achieve acceleration on Nvidia's GPUs by employing a hardware-friendly N:M sparsity pattern. SparseGPT (Frantar & Alistarh, 2023) leverages the Hessian inverse for pruning and reduces reconstruction error of dense and sparse weights by subsequent weight updates. Wanda (Sun et al., 2024) employs an efficient method that augments input activations into weight magnitudes, and matches the performance of SparseGPT at medium sparsity. Our work incorporates dependency information into unstructured pruning, achieving a novel pruning paradigm. \n\nInherent Sparsity of Transformer MLP. Interestingly, sparsity within the MLP activations of trained Transformer-based models occurs innately even without applying explicit regularizations or constraints (Zhang et al., 2022;Li et al., 2023;Dong et al., 2023). Such a phenomenon is prevalent in learned Transformers, including other zero-saturating functions. Outlier-dependent LLM Compression. Outlier features, defined as features with magnitudes substantially larger than others, are a notable characteristic of LLMs (Dettmers et al., 2022). Despite making up only a small fraction of all feature dimensions, these outliers play a critical role in attention and predictive performance. Such observation has motivated the development of LLM-specific quantization methods (Dettmers et al., 2022;Xiao et al., 2023;Lin et al., 2024;Ashkboos et al., 2023) to handle outliers more effectively.",
            "score": 0.5895348265770062,
            "section_title": "Related Work",
            "char_start_offset": 22888,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 12
                },
                {
                    "start": 13,
                    "end": 212
                },
                {
                    "start": 213,
                    "end": 353
                },
                {
                    "start": 354,
                    "end": 486
                },
                {
                    "start": 487,
                    "end": 640
                },
                {
                    "start": 641,
                    "end": 810
                },
                {
                    "start": 811,
                    "end": 980
                },
                {
                    "start": 981,
                    "end": 1088
                },
                {
                    "start": 1091,
                    "end": 1128
                },
                {
                    "start": 1129,
                    "end": 1349
                },
                {
                    "start": 1350,
                    "end": 1448
                },
                {
                    "start": 1449,
                    "end": 1483
                },
                {
                    "start": 1484,
                    "end": 1633
                },
                {
                    "start": 1634,
                    "end": 1777
                },
                {
                    "start": 1778,
                    "end": 1979
                }
            ],
            "ref_mentions": [
                {
                    "start": 106,
                    "end": 123,
                    "matchedPaperCorpusId": "258823276"
                },
                {
                    "start": 194,
                    "end": 211,
                    "matchedPaperCorpusId": "259203115"
                },
                {
                    "start": 213,
                    "end": 229,
                    "matchedPaperCorpusId": "258823276"
                },
                {
                    "start": 817,
                    "end": 835,
                    "matchedPaperCorpusId": "259203115"
                },
                {
                    "start": 1314,
                    "end": 1330,
                    "matchedPaperCorpusId": "259138847"
                },
                {
                    "start": 1330,
                    "end": 1348,
                    "matchedPaperCorpusId": "268955234"
                },
                {
                    "start": 1609,
                    "end": 1632,
                    "matchedPaperCorpusId": "258509304"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.72705078125
        },
        {
            "corpus_id": "269605957",
            "title": "COPAL: Continual Pruning in Large Language Generative Models",
            "text": "Upon transitioning to the new dataset, COPAL employs a small set of calibration data to guide the pruning process based on the sensitivity analysis we propose.Since the sensitivity measures model's robustness to perturbation caused by the new dataset, this strategy enables the model to seamlessly adapt to new information while preserving previous knowledge.To the best of our knowledge, we are the first to introduce the concept of continual pruning that addresses pruning under a continual model adaptation setting, bypassing the requirement for model re-training.This marks a substantial advancement in the field of LLM optimization.\n\nTo illustrate the effectiveness of our approach, Figure 1 demonstrates the impact of COPAL in both average and worst-case scenarios, showcasing how continual pruning can maintain performance even with increased sparsity.This highlights COPAL's ability to adeptly navigate the balance between model complexity and performance, a crucial factor in real-world applications.\n\nOur contributions are threefold:\n\n\u2022 We explore the inherent challenges in finetuning pretrained LLMs and provide a strategic solution to address both computational inefficiency and limited model adaptability.\n\n\u2022 We propose a mathematical formulation for the concept of continual pruning, utilizing sensitivity analysis to enable the pruning process under a continual model adaptation setting.\n\n\u2022 Through empirical evaluations on large-scale language models, including LLaMA-7B, 13B, 30B, 65B, we show that COPAL outperforms baseline methods, setting a new standard in LLM optimization for both efficiency and adaptability.",
            "score": 0.5891311612075506,
            "section_title": "Introduction",
            "char_start_offset": 3539,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 159
                },
                {
                    "start": 159,
                    "end": 359
                },
                {
                    "start": 359,
                    "end": 567
                },
                {
                    "start": 567,
                    "end": 637
                },
                {
                    "start": 639,
                    "end": 859
                },
                {
                    "start": 859,
                    "end": 1009
                },
                {
                    "start": 1011,
                    "end": 1043
                },
                {
                    "start": 1045,
                    "end": 1219
                },
                {
                    "start": 1221,
                    "end": 1403
                },
                {
                    "start": 1405,
                    "end": 1633
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.849609375
        },
        {
            "corpus_id": "275932211",
            "title": "SwiftPrune: Hessian-Free Weight Pruning for Large Language Models",
            "text": "Efficiency: The SwiftPrune algorithm provides a significant speedup. The performance gains de- Table 5: Zero-shot performance of the pruned LLaMA2-7B, LLaMA2-13B, LLaMA3.1-8B and Pythia-2.8B. \"Latency(s)\" indicates represents the time overhead required for overall model pruning (excluding communication time such as loading to GPU). The 'Avg' denotes the average value calculated across six classification datasets (HS, WG, and OQ represent HellaSwag, WinoGrande, and OpenbookQA respectively). Bold formatting indicates the best performance under equivalent compression ratios. However, note that for Latency(s), it represents the best performance excluding the cost associated with magnitude. The magnitude pruning method is omitted for LLaMA2-13B, LLaMA3.1-8B, and Pythia-2.8B because it causes significant accuracy degradation in these models. \n\nrive primarily from algorithmic innovations. By eliminating computationally intensive Hessian matrix calculations, our O(n) algorithm achieves rapid acceleration in LLM pruning tasks without requiring retraining or weight updates (Table 6). This methodology not only enables efficient assessment of weight significance but also maintains nearconstant time complexity -a critical advantage that prevents substantial increases in computational overhead as model dimensions expand. \n\nOur experiments systematically demonstrate that the proposed method achieves an average speedup of 43.75\u00d7 and 12.29\u00d7 compared to state-of-the-art pruning approaches like SparseGPT and Wanda respectively (detailed in Table 5). This substantial acceleration effectively addresses the temporal overhead inherent in scenarios requiring iterative pruning applications, particularly those involving adaptive sparsity mechanisms and dynamic input pruning techniques. \n\nAccuracy: Zero-shot performance comparison with baselines. We conducted comprehensive fine-grained pruning experiments on the LLaMA2-7B model and rigorously evaluated its average zero-shot learning accuracy across six tasks under three pruning configurations (including 50% sparsity with 2:4 structured pruning) using the lmevaluation-harness framework.",
            "score": 0.5891196368190206,
            "section_title": "Evaluation of SwiftPrune Algorithm",
            "char_start_offset": 16188,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 68
                },
                {
                    "start": 69,
                    "end": 191
                },
                {
                    "start": 192,
                    "end": 333
                },
                {
                    "start": 334,
                    "end": 494
                },
                {
                    "start": 495,
                    "end": 578
                },
                {
                    "start": 579,
                    "end": 694
                },
                {
                    "start": 695,
                    "end": 779
                },
                {
                    "start": 780,
                    "end": 847
                },
                {
                    "start": 850,
                    "end": 894
                },
                {
                    "start": 895,
                    "end": 1090
                },
                {
                    "start": 1091,
                    "end": 1328
                },
                {
                    "start": 1331,
                    "end": 1556
                },
                {
                    "start": 1557,
                    "end": 1790
                },
                {
                    "start": 1793,
                    "end": 1851
                },
                {
                    "start": 1852,
                    "end": 2146
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.609375
        },
        {
            "corpus_id": "267782440",
            "title": "Not All Experts are Equal: Efficient Expert Pruning and Skipping for Mixture-of-Experts Large Language Models",
            "text": "A pivotal advancement in the progress of large language models (LLMs) is the emergence of the Mixture-of-Experts (MoE) LLMs. Compared to traditional LLMs, MoE LLMs can achieve higher performance with fewer parameters, but it is still hard to deploy them due to their immense parameter sizes. Different from previous weight pruning methods that rely on specifically designed hardware, this paper mainly aims to enhance the deployment efficiency of MoE LLMs by introducing plug-and-play expert-level sparsification techniques. Specifically, we propose, for the first time to our best knowledge, post-training approaches for task-agnostic and task-specific expert pruning and skipping of MoE LLMs, tailored to improve deployment efficiency while maintaining model performance across a wide range of tasks. Extensive experiments show that our proposed methods can simultaneously reduce model sizes and increase the inference speed, while maintaining satisfactory performance. Data and code will be available at https://github.com/Lucky-Lance/Expert_Sparsity.",
            "score": 0.588277789773586,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.85546875
        },
        {
            "corpus_id": "273375561",
            "title": "MoE-Pruner: Pruning Mixture-of-Experts Large Language Model using the Hints from Its Router",
            "text": "Models, Datasets, and Evaluation. We conduct pruning experiments on widely adopted opensource MoE models: the base and instruct version of Mixtral-8x7B and Mixtral-8x22B (Jiang et al., 2024). We use samples from the pretraining dataset C4 (Raffel et al., 2020) as calibration data for one-shot pruning since pretraining datasets are often more comprehensive and not dominated by knowledge specific to any particular domain. We use the exact same 128 sequences of calibration data for all one-shot pruning experiments to control this variable factor. We evaluate the perplexity on the WikiText (Merity et al., 2017) validation set. Our expert-wise knowledge distillation method uses a subset of the C4 (Raffel et al., 2020) as the training set. We measure the performance of pruned models on zero-shot tasks and language modeling. For zero-shot evaluation, we use nine popular tasks from EleutherAI LM Harness (Gao et al., 2023). The nine evaluated zero-shot tasks are: ARC-easy, ARC-challenge (Clark et al., 2018), Boolq (Clark et al., 2019), HellaSwag (Zellers et al., 2019), MMLU (Hendrycks et al., 2021), OpenBookQA (OBQA) (Mihaylov et al., 2018), PIQA (Bisk et al., 2020), RTE (Wang et al., 2018), and WinoGrande (Sakaguchi et al., 2021). Baselines and Experiments Setup. We compare MoE-Pruner with prior pruning approaches, including SparseGPT (Frantar & Alistarh, 2023b) and Wanda (Sun et al., 2024). Similarly, our pruning algorithm is implemented in a layer-wise reconstruction manner. All pruning experiments are conducted on a single NVIDIA H100-80GB GPU.",
            "score": 0.5881766708625672,
            "section_title": "EXPERIMENTS",
            "char_start_offset": 16669,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 33
                },
                {
                    "start": 34,
                    "end": 191
                },
                {
                    "start": 192,
                    "end": 423
                },
                {
                    "start": 424,
                    "end": 549
                },
                {
                    "start": 550,
                    "end": 630
                },
                {
                    "start": 631,
                    "end": 743
                },
                {
                    "start": 744,
                    "end": 829
                },
                {
                    "start": 830,
                    "end": 928
                },
                {
                    "start": 929,
                    "end": 1242
                },
                {
                    "start": 1243,
                    "end": 1275
                },
                {
                    "start": 1276,
                    "end": 1406
                },
                {
                    "start": 1407,
                    "end": 1493
                },
                {
                    "start": 1494,
                    "end": 1565
                }
            ],
            "ref_mentions": [
                {
                    "start": 239,
                    "end": 260,
                    "matchedPaperCorpusId": "204838007"
                },
                {
                    "start": 593,
                    "end": 614,
                    "matchedPaperCorpusId": "16299141"
                },
                {
                    "start": 701,
                    "end": 722,
                    "matchedPaperCorpusId": "204838007"
                },
                {
                    "start": 1021,
                    "end": 1041,
                    "matchedPaperCorpusId": "165163607"
                },
                {
                    "start": 1053,
                    "end": 1075,
                    "matchedPaperCorpusId": "159041722"
                },
                {
                    "start": 1082,
                    "end": 1106,
                    "matchedPaperCorpusId": "221516475"
                },
                {
                    "start": 1126,
                    "end": 1149,
                    "matchedPaperCorpusId": "52183757"
                },
                {
                    "start": 1156,
                    "end": 1175,
                    "matchedPaperCorpusId": "208290939"
                },
                {
                    "start": 1181,
                    "end": 1200,
                    "matchedPaperCorpusId": "5034059"
                },
                {
                    "start": 1217,
                    "end": 1241,
                    "matchedPaperCorpusId": "199370376"
                },
                {
                    "start": 1349,
                    "end": 1376,
                    "matchedPaperCorpusId": "255372747"
                },
                {
                    "start": 1387,
                    "end": 1405,
                    "matchedPaperCorpusId": "259203115"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.449951171875
        },
        {
            "corpus_id": "275932211",
            "title": "SwiftPrune: Hessian-Free Weight Pruning for Large Language Models",
            "text": "With existing methods requiring hundreds of seconds per pruning iteration (see Table 1), conventional approaches fail to meet real-time operational demands, making the development of efficient pruning algorithms imperative for practical deployment. \n\nFurthermore, the emergence of advanced GPU architectures underscores the demand for structured hardware-aware pruning methods that achieve genuine acceleration while maintaining computational efficiency (Liu et al., 2017;Lu et al., 2022;Tang et al., 2022;Xia et al., 2024), thereby highlighting the importance of pruning approaches compatible with structured sparse formats. \n\nIn this study, we propose SwiftPrune, a novel pruning method designed to circumvent the high computational complexity associated with Hessian matrix and its inverse calculations by developing an alternative algorithm. First, our observations indicate that identifying weights with minimal loss contribution depends more on their relative importance than on absolute values. To leverage this, SwiftPrune replaces Hessian matrix computations by constructing a numerically preserved sequence as contribution-oriented weight metrics, derived through a series of loss values. Secondly, we introduce the Exponentially Weighted Moving Average (EWMA) method, borrowed from the Transmission Control Protocol (TCP), to replace traditional sorting methods, further reducing computational complexity. Moreover, we extend this approach to support structured sparsity pruning. We conduct comprehensive evaluations of SwiftPrune across three prominent opensource LLM families: Pythia (Biderman et al., 2023), LLaMA2 (Touvron et al., 2023), and LLaMA3 (Dubey et al., 2024). Compared to previous state-of-the-art methods for large language model pruning (Frantar and Alistarh, 2023;Sun et al., 2024), our SwiftPrune framework achieves the pruning process within seconds, delivering an average 12.29\u00d7 speedup (with peak acceleration reaching 56.02\u00d7) while maintaining comparable accuracy retention across standard benchmarks. Experimental results demonstrate that SwiftPrune can finish pruning tasks more rapidly without requiring any retraining or weight updates, thereby addressing application scenarios that necessitate frequent pruning.",
            "score": 0.5879890584062458,
            "section_title": "Introduction",
            "char_start_offset": 1594,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 248
                },
                {
                    "start": 251,
                    "end": 625
                },
                {
                    "start": 628,
                    "end": 845
                },
                {
                    "start": 846,
                    "end": 1001
                },
                {
                    "start": 1002,
                    "end": 1198
                },
                {
                    "start": 1199,
                    "end": 1416
                },
                {
                    "start": 1417,
                    "end": 1490
                },
                {
                    "start": 1491,
                    "end": 1685
                },
                {
                    "start": 1686,
                    "end": 2035
                },
                {
                    "start": 2036,
                    "end": 2250
                }
            ],
            "ref_mentions": [
                {
                    "start": 454,
                    "end": 472,
                    "matchedPaperCorpusId": "207667765"
                },
                {
                    "start": 472,
                    "end": 488,
                    "matchedPaperCorpusId": "249054941"
                },
                {
                    "start": 488,
                    "end": 506,
                    "matchedPaperCorpusId": "255775690"
                },
                {
                    "start": 506,
                    "end": 523,
                    "matchedPaperCorpusId": "263830786"
                },
                {
                    "start": 1597,
                    "end": 1620,
                    "matchedPaperCorpusId": "257921893"
                },
                {
                    "start": 1765,
                    "end": 1793,
                    "matchedPaperCorpusId": "255372747"
                },
                {
                    "start": 1793,
                    "end": 1810,
                    "matchedPaperCorpusId": "259203115"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.6494140625
        },
        {
            "corpus_id": "270621063",
            "title": "Dependency-Aware Semi-Structured Sparsity of GLU Variants in Large Language Models",
            "text": "We used the same calibration data set as SparseGPT and Wanda in their model pruning processes, consisting of 128 sequences of 2048 tokens each, randomly selected from the first shard of the C4 dataset (Raffel et al., 2020). \n\nEvaluation. To comprehensively evaluate the efficacy of our proposed method, two different metrics are utilized to evaluate the performance of the pruned models: (1) perplexity (PPL) of language modeling (2) zero-shot accuracy on 5 commonsense reasoning tasks. Perplexity has been regarded as a consistent and reliable metric for measuring compressed models (Dettmers & Zettlemoyer, 2023;Frantar & Alistarh, 2023), while downstream tasks sometimes have tendency in noisy behavior, but more interpretable. For perplexity evaluation, we use the validation dataset of WikiText2 (Merity et al., 2017). For zero-shot commonsense reasoning tasks, we choose five widely used tasks for accuracy evaluation: ARC (Easy and Challenge) (Clark et al., 2018), HellaSwag (Zellers et al., 2019), PiQA (Bisk et al., 2020), and WinoGrande (Sakaguchi et al., 2021), implemented in the Lm-Evaluation-Harness (Gao et al., 2021). We evaluate the perplexity of all the aforementioned models. To fully demonstrate the task-wise performance in different sparsity patterns, we report the downstream task performance of the largest LLaMA2-70B model. Notably, LLaMA2-70B utilizes grouped-query attention (Ainslie et al., 2023), and the MLP module accounts for more than 80% of the total parameters, representing the most widely adopted architectural design in modern LLMs. \n\nSparsity. In the less interpretable perplexity evaluation, we only prune the MLP layers. In the taskwise evaluation of the LLaMA2-70B model, both attention and MLP modules are pruned, consistent with prior works to accurately assess the performance gap between pruned and original models.",
            "score": 0.5868065056022038,
            "section_title": "Settings",
            "char_start_offset": 15386,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 223
                },
                {
                    "start": 226,
                    "end": 237
                },
                {
                    "start": 238,
                    "end": 486
                },
                {
                    "start": 487,
                    "end": 730
                },
                {
                    "start": 731,
                    "end": 823
                },
                {
                    "start": 824,
                    "end": 1133
                },
                {
                    "start": 1134,
                    "end": 1194
                },
                {
                    "start": 1195,
                    "end": 1348
                },
                {
                    "start": 1349,
                    "end": 1570
                },
                {
                    "start": 1573,
                    "end": 1582
                },
                {
                    "start": 1583,
                    "end": 1661
                },
                {
                    "start": 1662,
                    "end": 1861
                }
            ],
            "ref_mentions": [
                {
                    "start": 201,
                    "end": 222,
                    "matchedPaperCorpusId": "204838007"
                },
                {
                    "start": 584,
                    "end": 614,
                    "matchedPaperCorpusId": "254853733"
                },
                {
                    "start": 801,
                    "end": 822,
                    "matchedPaperCorpusId": "16299141"
                },
                {
                    "start": 1011,
                    "end": 1030,
                    "matchedPaperCorpusId": "208290939"
                },
                {
                    "start": 1047,
                    "end": 1071,
                    "matchedPaperCorpusId": "199370376"
                },
                {
                    "start": 1402,
                    "end": 1424,
                    "matchedPaperCorpusId": "258833177"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.456298828125
        },
        {
            "corpus_id": "271217883",
            "title": "MINI-LLM: Memory-Efficient Structured Pruning for Large Language Models",
            "text": "As Large Language Models (LLMs) grow dramatically in size, there is an increasing trend in compressing and speeding up these models. Previous studies have highlighted the usefulness of gradients for importance scoring in neural network compressing, especially in pruning medium-size networks. However, the substantial memory requirements involved in calculating gradients with backpropagation impede the utilization of gradients in guiding LLM pruning. As a result, most pruning strategies for LLMs rely on gradient-free criteria, such as weight magnitudes or a mix of magnitudes and activations. In this paper, we devise a hybrid pruning criterion, which appropriately integrates magnitude, activation, and gradient to capitalize on feature map sensitivity for pruning LLMs. To overcome memory requirement barriers, we estimate gradients using only forward passes. Based on this, we propose a Memory-effIcieNt structured prunIng procedure for LLMs (MINI-LLM) to remove no-critical channels and multi-attention heads. Experimental results demonstrate the superior performance of MINI-LLM over existing gradient-free methods on three LLMs: LLaMA, BLOOM, and OPT across various downstream tasks (classification, multiple-choice, and generation), while MINI-LLM maintains a GPU memory footprint akin to gradient-free methods.",
            "score": 0.586394114663956,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.6064453125
        },
        {
            "corpus_id": "271533761",
            "title": "Greedy Output Approximation: Towards Efficient Structured Pruning for LLMs Without Retraining",
            "text": "We apply this metric for both attention and feed-forward modules to remove the elements with minimal impact on the model's performance. Finally, we develop an optimization technique that eliminates the need for higher-order information by greedily reducing pruning error through weight reconstruction of the subsequent dense module. Our structured pruning experiments on pre-trained LLMs ranging from millions to billions of parameters demonstrate that our method ensures generalizability, hardware compatibility, and minimal pruning cost. Moreover, it outperforms or achieves comparative performance to other non-retraining methods and even some methods that require retraining.",
            "score": 0.585804039501977,
            "section_title": "Introduction",
            "char_start_offset": 4189,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 135
                },
                {
                    "start": 136,
                    "end": 332
                },
                {
                    "start": 333,
                    "end": 539
                },
                {
                    "start": 540,
                    "end": 679
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.80078125
        },
        {
            "corpus_id": "271891971",
            "title": "Research on Personalized Compression Algorithm for Pre-trained Models Based on Homomorphic Entropy Increase",
            "text": "In order to improve the number of parameters in language Large model (LLM), pruning technique has become a key strategy to optimize llm. Maintain model performance while reducing model size and computational costs. Gromov et al. [20] identify the best layer blocks to prune by considering cross-layer similarities, and make minor tweaks to the model in order to reduce the damage to the model, but the paper does not go into depth on how to make llm more efficient with the parameters in its deepest layers. Zhong L et al. [21] achieved fine-grained pruning by targeting redundancy in multi-head attention (MHA) and multi-layer perceptron (MLP) blocks, splitting each Transformer layer into MHA and MLP blocks, using confusion metrics to assess the importance of these blocks, and iteratively pruning the model based on the importance assessment. \n\nThe QWEN series of models, on the other hand, exhibit excellent performance due to their advanced tool usage and planning capabilities for creating proxy applications. This series includes a range of parameters from 500 to 72 billion, with intensive models and expert hybrid models. In order to explore the pruning effect of our strategy on large language models, we selected the QWEN series of large language models.",
            "score": 0.5835413942132162,
            "section_title": "C. Pruning for LLMs",
            "char_start_offset": 10182,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 136
                },
                {
                    "start": 137,
                    "end": 214
                },
                {
                    "start": 215,
                    "end": 507
                },
                {
                    "start": 508,
                    "end": 846
                },
                {
                    "start": 849,
                    "end": 1016
                },
                {
                    "start": 1017,
                    "end": 1131
                },
                {
                    "start": 1132,
                    "end": 1266
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.430908203125
        },
        {
            "corpus_id": "264128029",
            "title": "Dynamic Sparse No Training: Training-Free Fine-tuning for Sparse LLMs",
            "text": "This fine-tuning step would cause a significant amount of compute and memory footprints due to the colossal model size and massive training data of modern LLMs, which even unnerves large corporations, let alone individual researchers. \n\nTwo previous arts have explored the possibility to scale pruning to billion-level LLMs without any fine-tuning. SparseGPT (Frantar & Alistarh, 2023) formulates LLM pruning as a layer-wise weight reconstruction problem, where the target falls into mitigating the output discrepancy, w.r.t., reconstruction error, between dense and sparse LLMs. To solve the row-Hessian challenge, i.e., the need for calculating the expensive inversion of a huge matrix for each row individually, SparseGPT iteratively applies OBS (Hassibi et al., 1993) to individually prune and updates weights in a column-wise manner, ultimately reaching the same optimal solution as applying the closed-form regression reconstruction. Wanda (Sun et al., 2023) proposes a new pruning metric that takes both weight magnitude and their corresponding input activations into consideration, performing on part with SparseGPT without the need for the expensive second-order information. The intuition behind Wanda lies in the existence of emergent outlier feature dimensions in large-scale LLMs which are significantly larger than typical features and meanwhile are essential for the optimal performance of LLMs (Dettmers et al., 2022). While these two approaches enable LLM pruning without performing fine-tuning, their performance is still far from satisfactory, e.g., starting to lose performance at 20% sparsity with LLaMA-30B. Therefore, it is imperative to enable fine-tuning for sparse LLMs to fully unlock the potential of sparsity to escalate the affordability of LLMs. \n\nIn a parallel vein, Dynamic Sparse Training (DST), as outlined in previous research (Mocanu et al., 2018;Liu et al., 2019;Evci et al., 2020), has garnered considerable attention recently due to its significant saving potentials in the context of neural network training.",
            "score": 0.5831644446972242,
            "section_title": "INTRODUCTION",
            "char_start_offset": 1600,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 234
                },
                {
                    "start": 237,
                    "end": 348
                },
                {
                    "start": 349,
                    "end": 579
                },
                {
                    "start": 580,
                    "end": 939
                },
                {
                    "start": 940,
                    "end": 1184
                },
                {
                    "start": 1185,
                    "end": 1434
                },
                {
                    "start": 1435,
                    "end": 1629
                },
                {
                    "start": 1630,
                    "end": 1776
                },
                {
                    "start": 1779,
                    "end": 2049
                }
            ],
            "ref_mentions": [
                {
                    "start": 749,
                    "end": 771,
                    "matchedPaperCorpusId": "61815367"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.54443359375
        },
        {
            "corpus_id": "273234021",
            "title": "Is C4 Dataset Optimal for Pruning? An Investigation of Calibration Data for LLM Pruning",
            "text": "We evaluate pruning performance using calibration data derived from a range of pre-training datasets including C4, RedPajama, Oscar, and Pile. The results are detailed in Table 2. Our analysis reveals that the average accuracy of Pile consistently outperforms the C4 dataset. Using Wanda with target sparsity 0.5, calibration with the Pile dataset exhibits superior performance in terms of average accuracy across nine downstream tasks, surpassing other pre-training datasets in six out of nine tasks. \n\nSimilarly, for SparseGPT pruning, the Pile dataset achieves the highest average accuracy, although the differences among the four pre-training datasets are small. Notably, when compared with the commonly used C4 dataset, our analysis reveals that Red-Pajama achieves comparable performance, and Pile demonstrates an improvement, outperforming C4 in Wanda pruning across a majority of downstream tasks. Specifically, using the Llama 2-Chat 7b model, Pile leads C4 in seven out of nine tasks when using Wanda. Although when using SparseGPT, Pile outperforms C4 in only four out of nine tasks, Pile still has higher average accuracy across nine tasks. In Table 3, when we target 70% sparsity, we can clearly see that RedPajama and Pile achieve significantly higher average accuracy than C4. These findings underscore that C4 is not the optimal choice of calibration data for LLM pruning. Pile consistently serves as better calibration data in LLM pruning.",
            "score": 0.5829662353257503,
            "section_title": "Pre-training Dataset as Calibration Data",
            "char_start_offset": 13641,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 142
                },
                {
                    "start": 143,
                    "end": 275
                },
                {
                    "start": 276,
                    "end": 501
                },
                {
                    "start": 504,
                    "end": 666
                },
                {
                    "start": 667,
                    "end": 905
                },
                {
                    "start": 906,
                    "end": 1011
                },
                {
                    "start": 1012,
                    "end": 1152
                },
                {
                    "start": 1153,
                    "end": 1291
                },
                {
                    "start": 1292,
                    "end": 1388
                },
                {
                    "start": 1389,
                    "end": 1456
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.363525390625
        },
        {
            "corpus_id": "276421918",
            "title": "DSMoE: Matrix-Partitioned Experts with Dynamic Routing for Computation-Efficient Dense LLMs",
            "text": "Model pruning is an effective approach to achieving sparse LLMs while maintaining model functionality. Pruning methods can be categorized into two main types: unstructured and structured pruning. Unstructured pruning operates at the weight level, allowing for arbitrary weight removal (Lee et al., 2018). In large language models, pruned weights are set to zero (Frantar and Alistarh, 2023;Sun et al., 2023). However, this method requires specialized hardware and software support for acceleration (Han et al., 2015;Wen et al., 2016;Filters'Importance, 2016;Tang et al., 2021). Structured pruning takes a coarser-grained approach by removing complete structural units such as convolution kernels, channels, attention heads, or entire layers (You et al., 2019;Ashkboos et al., 2024;Liu et al., 2021;Ma et al., 2023;Men et al.). Its main advantage is the ability to directly produce regular, narrow model architectures that can achieve acceleration without specialized sparse computation libraries (Luo et al., 2017;Liu et al., 2021;Filters'Importance, 2016;Nonnenmacher et al., 2021). However, both approaches face a fundamental limitation: achieving efficiency through permanent parameter removal may discard valuable knowledge and lose the ability to adapt computation based on input complexity. In recent years, there has been growing interest in exploring sparse computation in large language models. Mixture-of-Experts (MoE) represents a pioneering approach that demonstrates how sparse activation can effectively balance model capacity and computational efficiency. In MoE architectures, only a subset of FFN modules (experts) are activated for each input token (Fedus et al., 2022;Lepikhin et al., 2021;Huang et al., 2024). This foundational idea of conditional computation has inspired various innovations in expert activation strategies. Some works explore heterogeneous expert architectures (Sun et al., 2024) or introduce zero-computation experts (Jin et al., 2024) to further optimize computational efficiency.",
            "score": 0.582867089811619,
            "section_title": "Related Work",
            "char_start_offset": 2804,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 102
                },
                {
                    "start": 103,
                    "end": 195
                },
                {
                    "start": 196,
                    "end": 304
                },
                {
                    "start": 305,
                    "end": 408
                },
                {
                    "start": 409,
                    "end": 577
                },
                {
                    "start": 578,
                    "end": 826
                },
                {
                    "start": 827,
                    "end": 1083
                },
                {
                    "start": 1084,
                    "end": 1296
                },
                {
                    "start": 1297,
                    "end": 1403
                },
                {
                    "start": 1404,
                    "end": 1570
                },
                {
                    "start": 1571,
                    "end": 1729
                },
                {
                    "start": 1730,
                    "end": 1845
                },
                {
                    "start": 1846,
                    "end": 2021
                }
            ],
            "ref_mentions": [
                {
                    "start": 516,
                    "end": 533,
                    "matchedPaperCorpusId": "2056019"
                },
                {
                    "start": 558,
                    "end": 576,
                    "matchedPaperCorpusId": "232170293"
                },
                {
                    "start": 741,
                    "end": 759,
                    "matchedPaperCorpusId": "202660914"
                },
                {
                    "start": 781,
                    "end": 798,
                    "matchedPaperCorpusId": "235825363"
                },
                {
                    "start": 996,
                    "end": 1014,
                    "matchedPaperCorpusId": "11169209"
                },
                {
                    "start": 1014,
                    "end": 1031,
                    "matchedPaperCorpusId": "235825363"
                },
                {
                    "start": 1667,
                    "end": 1687,
                    "matchedPaperCorpusId": "231573431"
                },
                {
                    "start": 1687,
                    "end": 1709,
                    "matchedPaperCorpusId": "220265858"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.49560546875
        },
        {
            "corpus_id": "277043299",
            "title": "Towards Extreme Pruning of LLMs with Plug-and-Play Mixed Sparsity",
            "text": "Once the evolutionary process is complete, the optimal sparsity levels for each layer are determined. Based on the layer-wise sparsity, we can use any metric, which measures the importance of network components, to conduct pruning. In this way, our MSP can work as a plug-and-play module which can easily be integrated into those existing methods that design importance metrics. As will be demonstrated, the performance of existing pruning methods can be elegantly improved by integrating the proposed MSP. Models and Evaluation. We evaluate the pruned models on language modeling and zero-shot tasks. For language modeling, we evaluate MSP on the most widely adopted LLM models: LLaMA 7B/13B/30B/65B [24], LLaMA-2 7B/13B/70B [25], and OPT 125m/350m/1.3B/2.7B/6.7B/13B [26] (Appendix B.1). For zero-shot evaluation, we conduct seven tasks from EleutherAI LM Harness [27]. Following previous works on LLM compression [28], we evaluate the perplexity on the held-out WikiText [29] validation dataset. \n\nBaselines. We conduct experiments on three prior pruning methods including magnitude pruning [30], SparseGPT [6], and Wanda [7], and compare those methods before and after the integration of our MSP. Sparsity. We mainly evaluate four types of target sparsity: structured 2:4, structured 3:4, structured 6:8, and structured 12:16. We adopt N:M sparsity across all linear layers in our pruning settings. However, unlike existing pruning methods, MSP assigns different sparsity levels to different linear layers rather than applying a uniform ratio. To ensure a fair comparison with other pruning methods, in our MSP, we set the same M as the target pruning of the other methods while searching for the optimal N for different layers. \n\nEvolution Settings. The evolutionary algorithm is initialized with 20 individuals and proceeds with 20 iterations. Half of the best-performing individuals in each iteration are selected as the parents for crossover.",
            "score": 0.5828234193353861,
            "section_title": "Plug-and-play Integration",
            "char_start_offset": 14342,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 101
                },
                {
                    "start": 102,
                    "end": 231
                },
                {
                    "start": 232,
                    "end": 378
                },
                {
                    "start": 379,
                    "end": 506
                },
                {
                    "start": 507,
                    "end": 529
                },
                {
                    "start": 530,
                    "end": 601
                },
                {
                    "start": 602,
                    "end": 768
                },
                {
                    "start": 769,
                    "end": 789
                },
                {
                    "start": 790,
                    "end": 871
                },
                {
                    "start": 872,
                    "end": 998
                },
                {
                    "start": 1001,
                    "end": 1011
                },
                {
                    "start": 1012,
                    "end": 1200
                },
                {
                    "start": 1201,
                    "end": 1210
                },
                {
                    "start": 1211,
                    "end": 1330
                },
                {
                    "start": 1331,
                    "end": 1402
                },
                {
                    "start": 1403,
                    "end": 1547
                },
                {
                    "start": 1548,
                    "end": 1732
                },
                {
                    "start": 1735,
                    "end": 1754
                },
                {
                    "start": 1755,
                    "end": 1849
                },
                {
                    "start": 1850,
                    "end": 1950
                }
            ],
            "ref_mentions": [
                {
                    "start": 916,
                    "end": 920,
                    "matchedPaperCorpusId": "253708271"
                },
                {
                    "start": 1110,
                    "end": 1113,
                    "matchedPaperCorpusId": "255372747"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.5615234375
        },
        {
            "corpus_id": "272694272",
            "title": "Evaluating the Impact of Compression Techniques on Task-Specific Performance of Large Language Models",
            "text": "This study evaluates the impact of three popular compression techniques -Magnitude Pruning, SparseGPT, and Wanda-on the LLaMA-2-7B model. The key findings reveal several critical insights into the tradeoffs and effectiveness of these methods in preserving model performance while reducing complexity. \n\nThe results indicate that while SparseGPT and Wanda maintain perplexity levels close to the base model, they exhibit significant degradation in downstream task performance. This disparity underscores the inadequacy of perplexity as a sole evaluation metric for assessing the efficacy of compression techniques. Perplexity measures how confidently a model predicts the next token, but it does not fully capture the nuanced impacts of compression on task-specific outputs. Magnitude Pruning, although effective in inducing sparsity, showed a notable increase in both Loss and Perplexity, suggesting a greater degradation in overall performance. However, it was observed that at low sparsity levels, Magnitude Pruning could improve downstream task performance. This phenomenon is likely due to the removal of redundant parameters, which may improve the model for specific tasks. \n\nAlthough previous studies [7], [14]- [16] have highlighted the problems of using perplexity as the sole evaluation metric for LLM compression, they fail to suggest an alternative. This study addresses this gap by proposing Jensen-Shannon (JS) Divergence as a more comprehensive metric, offering deeper insights into model changes post-compression. Our results reveal that, unlike perplexity, which remains constant up to 50% sparsity, JS Divergence effectively captures the impact of compression on downstream task performance, indicating greater changes in the model's output distribution. SparseGPT exhibited higher JS Divergence compared to Wanda, indicating more generalized changes due to its error correction process. This process, which updates weights using calibration data, acts similarly to fine-tuning and introduces more extensive alterations to the model. In contrast, Wanda's lower JS Divergence suggests closer adherence to the base model but correlates with poorer downstream task performance compared to SparseGPT. \n\nThe integration of GPT-4 as an evaluator in this study serves as an effective proxy for real-world task performance.",
            "score": 0.5825922247970668,
            "section_title": "Discussion",
            "char_start_offset": 19378,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 137
                },
                {
                    "start": 138,
                    "end": 300
                },
                {
                    "start": 303,
                    "end": 475
                },
                {
                    "start": 476,
                    "end": 613
                },
                {
                    "start": 614,
                    "end": 773
                },
                {
                    "start": 774,
                    "end": 945
                },
                {
                    "start": 946,
                    "end": 1060
                },
                {
                    "start": 1061,
                    "end": 1178
                },
                {
                    "start": 1181,
                    "end": 1360
                },
                {
                    "start": 1361,
                    "end": 1528
                },
                {
                    "start": 1529,
                    "end": 1771
                },
                {
                    "start": 1772,
                    "end": 1904
                },
                {
                    "start": 1905,
                    "end": 2050
                },
                {
                    "start": 2051,
                    "end": 2213
                },
                {
                    "start": 2216,
                    "end": 2332
                }
            ],
            "ref_mentions": [
                {
                    "start": 1212,
                    "end": 1216,
                    "matchedPaperCorpusId": "259847758"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.51025390625
        },
        {
            "corpus_id": "263829692",
            "title": "Outlier Weighed Layerwise Sparsity (OWL): A Missing Secret Sauce for Pruning LLMs to High Sparsity",
            "text": "Models and Dataset. We assess OWL's performance across a range of LLMs, encompassing the LLaMA-V1 model family (Touvron et al., 2023b) with parameter counts ranging from 7 billion to 65 billion, as well as OPT-6.7B (Zhang et al., 2022). Our evaluation protocol aligns with established LLM pruning methodologies (Frantar & Alistarh, 2023;Sun et al., 2023), encompassing assessments of language modeling proficiency and zero-shot capabilities of sparse LLMs. Specifically, we measure the Perplexity metric on the Wiki-Text (Merity et al., 2016b) validation dataset for language modeling performance, and employ the Accuracy metric for zero-shot evaluations on seven common sense benchmarks, including BoolQ (Clark et al., 2019), RTE (Wang et al., 2018), HellaSwag (Zellers et al., 2019), WinoGrande (Sakaguchi et al., 2019), ARC Easy and Challenge (Clark et al., 2018), and OpenbookQA (Mihaylov et al., 2018). \n\nBaselines. We choose the three current LLM-pruning baselines, including magnitude (Jaiswal et al., 2023b), SparseGPT (Frantar & Alistarh, 2023), Wanda (Sun et al., 2023). Magnitude pruning serves as a naive baseline for LLMs, with an expected sharp decline in performance at modest sparsity levels, typically ranging from 10% to 30%. SparseGPT and Wanda, on the other hand, are established baselines known for their ability to maintain reasonable performance even at relatively high sparsity levels, typically around 50% to 60%. Notably, in contrast to our approach, all baseline methods employ with uniform layerwise sparsity. We primarily focus on high sparsity levels, not falling below 50%, as regions with low sparsity pose challenges for existing sparse GPU kernels to outperform their dense counterparts (Gale et al., 2020).",
            "score": 0.582441617555111,
            "section_title": "Main Experiments",
            "char_start_offset": 21547,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 19
                },
                {
                    "start": 20,
                    "end": 214
                },
                {
                    "start": 215,
                    "end": 236
                },
                {
                    "start": 237,
                    "end": 456
                },
                {
                    "start": 457,
                    "end": 907
                },
                {
                    "start": 910,
                    "end": 920
                },
                {
                    "start": 921,
                    "end": 1080
                },
                {
                    "start": 1081,
                    "end": 1243
                },
                {
                    "start": 1244,
                    "end": 1438
                },
                {
                    "start": 1439,
                    "end": 1537
                },
                {
                    "start": 1538,
                    "end": 1741
                }
            ],
            "ref_mentions": [
                {
                    "start": 846,
                    "end": 866,
                    "matchedPaperCorpusId": "3922816"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.32275390625
        },
        {
            "corpus_id": "272770793",
            "title": "CFSP: An Efficient Structured Pruning Framework for LLMs with Coarse-to-Fine Activation Information",
            "text": "Although scaling up Large Language Models (LLMs) brings remarkable performance (Brown et al., 2020;OpenAI, 2023;Gemini Team et al., 2023;Meta, 2024;DeepSeek-AI et al., 2024;Yang et al., 2024a), increasing parameters brings more computations and memory consumption, posing a significant challenge of deploying in practical applications. To address this, various model compression methods for LLMs are proposed (Dettmers et al., 2022;Frantar et al., 2022;Lin et al., 2023;Muralidharan et al., 2024). Existing LLM pruning work (Frantar and Alistarh, 2023;Sun et al., 2023;Xu et al., 2024a;Zhang et al., 2024b) focuses mainly on unstructured or semi-structured sparsity. However, these paradigms require specific hardware to achieve practical acceleration. \n\nIn contrast, structured pruning, which imposes structured sparsity by removing groups of consecutive parameters (Louizos et al., 2017;Wang et al., 2020;Xia et al., 2022), is more hardware-friendly on general devices. However, there are some challenges involved in existing structured pruning methods for LLMs: (1) They typically introduce learnable masks to search (Xia et al., 2023;Dery et al., 2024) or utilize gradients to guide pruning (Ma et al., 2023;Zhang et al., 2023a). Unfortunately, they require significant computational overhead, especially for large-scale (e.g., 70B) models. (2) It is also worth noting that they usually assign a uniform sparsity budget per block, which is suboptimal since LLM blocks have different significance in the representation functionality (Gromov et al., 2024a). Moreover, they usually involve a recovery fine-tuning with Low-Rank Adapter (LoRA) (Hu et al., 2022) to enhance pruned models, which also introduce training overhead and overlook the varying importance of blocks.",
            "score": 0.5820872861962003,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 335
                },
                {
                    "start": 336,
                    "end": 497
                },
                {
                    "start": 498,
                    "end": 666
                },
                {
                    "start": 667,
                    "end": 752
                },
                {
                    "start": 755,
                    "end": 971
                },
                {
                    "start": 972,
                    "end": 1233
                },
                {
                    "start": 1234,
                    "end": 1344
                },
                {
                    "start": 1345,
                    "end": 1559
                },
                {
                    "start": 1560,
                    "end": 1772
                }
            ],
            "ref_mentions": [
                {
                    "start": 889,
                    "end": 907,
                    "matchedPaperCorpusId": "204009154"
                },
                {
                    "start": 907,
                    "end": 924,
                    "matchedPaperCorpusId": "247922354"
                },
                {
                    "start": 1195,
                    "end": 1212,
                    "matchedPaperCorpusId": "268253513"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.48974609375
        },
        {
            "corpus_id": "270621063",
            "title": "Dependency-Aware Semi-Structured Sparsity of GLU Variants in Large Language Models",
            "text": "The precise coordination involved in pruning is crucial for upholding the model's structural integrity and its functional capabilities. Although current unstructured pruning methods (Sun et al., 2024;Frantar & Alistarh, 2023) effectively remove a significant number of redundant weights, they operate entirely locally within each linear layer without considering inter-dependencies to other layers. This can lead to a structural mismatch, which is more evident in Wanda as shown in Figure 1b: In Gate and Up projections, the same amount of parameters are pruned for each MLP neuron. However, the intermediate activation norms of GLU are not uniformly distributed and some neurons have much larger norms than others. Based on Wanda pruning metric, more weights connected to neurons with large activation norms are preserved. At high sparsity, this problem gets magnified in Wanda causing significant drops in performance by the broken network. \n\nIn order to overcome the limitations present in current pruning methodologies, we introduce a new paradigm, namely, Dependency-aware Semi-structured Sparsity (DaSS). This approach is specifically designed to navigate the middle ground between the flexibility of unstructured pruning and the structural consistency of dependency-based structured pruning. To emphasize the importance of weights corresponding to large intermediate activations, we present a new MLP pruning metric that assesses each weight's importance based on the product of its magnitude and the norm of the corresponding MLP intermediate activations. Our proposed DaSS method, illustrated in Figure 1c, embodies a semi-structured pattern that retains a degree of the adaptability inherent in unstructured pruning while incorporating the dependency-aware aspect of structured pruning. This balance allows for more precise pruning. DaSS can be easily extended to hardware-friendly N:M sparsity patterns Mishra et al. (2021). \n\nWe perform extensive experiments on LLaMA2 (Touvron et al., 2023), Gemma (Team et al., 2024), andMistral (Jiang et al., 2023) to evaluate DaSS across various tasks from language modeling, 5 commonsense reasoning tasks.",
            "score": 0.5818187523544169,
            "section_title": "Introduction",
            "char_start_offset": 3271,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 135
                },
                {
                    "start": 136,
                    "end": 398
                },
                {
                    "start": 399,
                    "end": 582
                },
                {
                    "start": 583,
                    "end": 715
                },
                {
                    "start": 716,
                    "end": 823
                },
                {
                    "start": 824,
                    "end": 942
                },
                {
                    "start": 945,
                    "end": 1110
                },
                {
                    "start": 1111,
                    "end": 1298
                },
                {
                    "start": 1299,
                    "end": 1563
                },
                {
                    "start": 1564,
                    "end": 1796
                },
                {
                    "start": 1797,
                    "end": 1842
                },
                {
                    "start": 1843,
                    "end": 1935
                },
                {
                    "start": 1938,
                    "end": 2156
                }
            ],
            "ref_mentions": [
                {
                    "start": 182,
                    "end": 200,
                    "matchedPaperCorpusId": "259203115"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.5419921875
        },
        {
            "corpus_id": "278327238",
            "title": "Efficient Shapley Value-based Non-Uniform Pruning of Large Language Models",
            "text": "Notably, while SV-NUP slightly improves the performance of Wanda and SparseGPT, the gains are less pronounced, as the pruned LLMs at 50% sparsity already exhibit robust performance, leaving limited room for further enhancements. Interestingly, SV-NUP's effectiveness is more apparent with larger LLMs, such as the 7-billion-parameter models, whereas its impact diminishes for smaller OPT-2.7B. Table 4 presents the zero-shot performance of various pruning methods applied to LLaMA-7B and LLaMA2-7B across four downstream tasks. The unpruned dense models serve as baselines, achieving 61.36% and 60.27% average accuracy, respectively. Among the pruning strategies, magnitude pruning yields the most significant performance degradation, whereas both Wanda and SparseGPT show stronger resilience, with SparseGPT with SV-NUP achieving up to 57.23% on LLaMA2-7B, closely matching the dense baseline. Notably, our proposed method SV-NUP consistently improves performance across all pruning strategies and model backbones. It is worth noting that the results of ALS occasionally outperform the dense models; we suspect this is due to differences in experimental settings or hyperparameters rather than true pruning benefits. Thus, ALS results are reported for reference only and are excluded from direct comparison. Overall, the results validate the effectiveness and robustness of SV-NUP in mitigating performance loss introduced by non-uniform pruning. Windows Size Selection. Table 5 reports PPL of pruned LLaMA-7B and OPT-6.7B using different pruning strategies with SV-NUP, evaluated under varying window sizes (N =3, 5, 7). \n\nAcross both models and all pruning methods, the PPL remains largely stable as the window size changes, indicating that SV-NUP is robust to the choice of window size. These results suggest that SV-NUP does not require careful tuning of the window size. Different SV Approximation Methods. Table 6 shows our proposed SWSV method with the existing SV approximation method in [41] to demonstrate its efficiency.",
            "score": 0.581812357667225,
            "section_title": "Experimental Evalution",
            "char_start_offset": 20135,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 228
                },
                {
                    "start": 229,
                    "end": 393
                },
                {
                    "start": 394,
                    "end": 527
                },
                {
                    "start": 528,
                    "end": 633
                },
                {
                    "start": 634,
                    "end": 894
                },
                {
                    "start": 895,
                    "end": 1015
                },
                {
                    "start": 1016,
                    "end": 1217
                },
                {
                    "start": 1218,
                    "end": 1308
                },
                {
                    "start": 1309,
                    "end": 1447
                },
                {
                    "start": 1448,
                    "end": 1471
                },
                {
                    "start": 1472,
                    "end": 1523
                },
                {
                    "start": 1524,
                    "end": 1622
                },
                {
                    "start": 1625,
                    "end": 1790
                },
                {
                    "start": 1791,
                    "end": 1876
                },
                {
                    "start": 1877,
                    "end": 1912
                },
                {
                    "start": 1913,
                    "end": 2032
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.53466796875
        }
    ],
    "quotes": {
        "cost": 0.20395200000000002,
        "quotes": [
            {
                "idx": 0,
                "key": "[258823276 | Ma et al. | 2023 | Citations: 440]",
                "snippets": "We propose a novel framework, LLM-Pruner, for the task-agnostic compression of the large language model. To the best of our knowledge, LLM-Pruner is the first framework designed for structured pruning of LLMs...Since our goal is to compress LLMs with reduced data dependency and expedited post-training, how to prune model with the minimal disruption to the origin is crucial. To accomplish this, we propose a dependency detection algorithm that identifies all the dependent structures within the model. Once the coupled structure is identified, we employ an efficient importance estimation strategy to select the optimal group for pruning under the task-agnostic setting, where the first-order information and an approximated hessian information is taken into account. Finally, a rapid recovery stage is executed to post-train the pruned model with limited data.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {},
                "metadata": [
                    {
                        "section_title": "Introduction",
                        "pdf_hash": "",
                        "start": 1846,
                        "end": 2054,
                        "sentence_offsets": [
                            {
                                "start": 1831,
                                "end": 1950
                            },
                            {
                                "start": 1951,
                                "end": 2055
                            }
                        ],
                        "ref_mentions": [],
                        "quote": "We propose a novel framework, LLM-Pruner, for the task-agnostic compression of the large language model. To the best of our knowledge, LLM-Pruner is the first framework designed for structured pruning of LLMs"
                    },
                    {
                        "section_title": "Introduction",
                        "pdf_hash": "",
                        "start": 1162,
                        "end": 1815,
                        "sentence_offsets": [
                            {
                                "start": 1162,
                                "end": 1327
                            },
                            {
                                "start": 1328,
                                "end": 1454
                            },
                            {
                                "start": 1455,
                                "end": 1720
                            },
                            {
                                "start": 1721,
                                "end": 1814
                            }
                        ],
                        "ref_mentions": [],
                        "quote": "Since our goal is to compress LLMs with reduced data dependency and expedited post-training, how to prune model with the minimal disruption to the origin is crucial. To accomplish this, we propose a dependency detection algorithm that identifies all the dependent structures within the model. Once the coupled structure is identified, we employ an efficient importance estimation strategy to select the optimal group for pruning under the task-agnostic setting, where the first-order information and an approximated hessian information is taken into account. Finally, a rapid recovery stage is executed to post-train the pruned model with limited data."
                    }
                ]
            },
            {
                "idx": 1,
                "key": "[259203115 | Sun et al. | 2023 | Citations: 439]",
                "snippets": "In this paper, we introduce a novel, straightforward yet effective pruning method, termed Wanda (Pruning by Weights and activations), designed to induce sparsity in pretrained LLMs. Motivated by the recent observation of emergent large magnitude features in LLMs, our approach prunes weights with the smallest magnitudes multiplied by the corresponding input activations, on a per-output basis. Notably, Wanda requires no retraining or weight update, and the pruned LLM can be used as is. We conduct a thorough evaluation of our method Wanda on LLaMA and LLaMA-2 across various language benchmarks. Wanda significantly outperforms the established baseline of magnitude pruning and performs competitively against recent method involving intensive weight update.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {},
                "metadata": [
                    {
                        "section_title": "abstract",
                        "pdf_hash": "",
                        "start": 426,
                        "end": 1186,
                        "sentence_offsets": [],
                        "ref_mentions": [],
                        "quote": "In this paper, we introduce a novel, straightforward yet effective pruning method, termed Wanda (Pruning by Weights and activations), designed to induce sparsity in pretrained LLMs. Motivated by the recent observation of emergent large magnitude features in LLMs, our approach prunes weights with the smallest magnitudes multiplied by the corresponding input activations, on a per-output basis. Notably, Wanda requires no retraining or weight update, and the pruned LLM can be used as is. We conduct a thorough evaluation of our method Wanda on LLaMA and LLaMA-2 across various language benchmarks. Wanda significantly outperforms the established baseline of magnitude pruning and performs competitively against recent method involving intensive weight update."
                    }
                ]
            },
            {
                "idx": 2,
                "key": "[260900101 | Zhu et al. | 2023 | Citations: 229]",
                "snippets": "An innovative approach in this domain is SparseGPT (Frantar and Alistarh, 2023), which introduces a one-shot pruning strategy without retraining. SparseGPT frames pruning as an extensive sparse regression problem and solves it using an approximate sparse regression solver. SparseGPT achieves significant unstructured sparsity, even up to over 50% on the largest GPT models like OPT-175B and BLOOM-176B, with minimal increase in perplexity. To reduce the cost about the weight update process required by SparseGPT, Wanda (Sun et al., 2023) achieves model sparsity by pruning weights with the smallest magnitudes multiplied by the norm of the corresponding input activations, without the need for retraining or weight updates. To further minimize pruning-induced errors while upholding the desired overall sparsity level, SAMSP (Shao et al., 2023) utilizes the Hessian matrix as a metric for weight matrix sensitivity evaluation, and dynamically adjusts sparsity allocation based on sensitivity. Furthermore, DSnoT (Zhang et al., 2023) minimizes the reconstruction error between dense and sparse models through iterative weight pruning-and-growing on top of sparse LLMs to enhance LLM performance across various sparsity rates, especially at high sparsity levels.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {
                    "[259203115 | Sun et al. | 2023 | Citations: 439]": "As their size increases, Large Languages Models (LLMs) are natural candidates for network pruning methods: approaches that drop a subset of network weights while striving to preserve performance. Existing methods, however, require either retraining, which is rarely affordable for billion-scale LLMs, or solving a weight reconstruction problem reliant on second-order information, which may also be computationally expensive. In this paper, we introduce a novel, straightforward yet effective pruning method, termed Wanda (Pruning by Weights and activations), designed to induce sparsity in pretrained LLMs. Motivated by the recent observation of emergent large magnitude features in LLMs, our approach prunes weights with the smallest magnitudes multiplied by the corresponding input activations, on a per-output basis. Notably, Wanda requires no retraining or weight update, and the pruned LLM can be used as is. We conduct a thorough evaluation of our method Wanda on LLaMA and LLaMA-2 across various language benchmarks. Wanda significantly outperforms the established baseline of magnitude pruning and performs competitively against recent method involving intensive weight update. Code is available at https://github.com/locuslab/wanda.",
                    "[264128029 | Zhang et al. | 2023 | Citations: 43]": "The ever-increasing large language models (LLMs), though opening a potential path for the upcoming artificial general intelligence, sadly drops a daunting obstacle on the way towards their on-device deployment. As one of the most well-established pre-LLMs approaches in reducing model complexity, network pruning appears to lag behind in the era of LLMs, due mostly to its costly fine-tuning (or re-training) necessity under the massive volumes of model parameter and training data. To close this industry-academia gap, we introduce Dynamic Sparse No Training (DSnoT), a training-free fine-tuning approach that slightly updates sparse LLMs without the expensive backpropagation and any weight updates. Inspired by the Dynamic Sparse Training, DSnoT minimizes the reconstruction error between the dense and sparse LLMs, in the fashion of performing iterative weight pruning-and-growing on top of sparse LLMs. To accomplish this purpose, DSnoT particularly takes into account the anticipated reduction in reconstruction error for pruning and growing, as well as the variance w.r.t. different input data for growing each weight. This practice can be executed efficiently in linear time since its obviates the need of backpropagation for fine-tuning LLMs. Extensive experiments on LLaMA-V1/V2, Vicuna, and OPT across various benchmarks demonstrate the effectiveness of DSnoT in enhancing the performance of sparse LLMs, especially at high sparsity levels. For instance, DSnoT is able to outperform the state-of-the-art Wanda by 26.79 perplexity at 70% sparsity with LLaMA-7B. Our paper offers fresh insights into how to fine-tune sparse LLMs in an efficient training-free manner and open new venues to scale the great potential of sparsity to LLMs. Codes are available at https://github.com/zyxxmu/DSnoT.",
                    "[264146174 | Shao et al. | 2023 | Citations: 21]": "Various Large Language Models (LLMs) from the Generative Pretrained Transformer (GPT) family have achieved outstanding performances in a wide range of text generation tasks. However, the enormous model sizes have hindered their practical use in real-world applications due to high inference latency. Therefore, improving the efficiencies of LLMs through quantization, pruning, and other means has been a key issue in LLM studies. In this work, we propose a method based on Hessian sensitivity-aware mixed sparsity pruning to prune LLMs to at least 50% sparsity without the need of any retraining. It allocates sparsity adaptively based on sensitivity, allowing us to reduce pruning-induced error while maintaining the overall sparsity level. The advantages of the proposed method exhibit even more when the sparsity is extremely high. Furthermore, our method is compatible with quantization, enabling further compression of LLMs."
                },
                "metadata": [
                    {
                        "section_title": "Unstructured Pruning",
                        "pdf_hash": "",
                        "start": 331,
                        "end": 1594,
                        "sentence_offsets": [
                            {
                                "start": 331,
                                "end": 476
                            },
                            {
                                "start": 477,
                                "end": 604
                            },
                            {
                                "start": 605,
                                "end": 771
                            },
                            {
                                "start": 772,
                                "end": 1056
                            },
                            {
                                "start": 1057,
                                "end": 1326
                            },
                            {
                                "start": 1327,
                                "end": 1594
                            }
                        ],
                        "ref_mentions": [
                            "259203115",
                            "264146174",
                            "264128029"
                        ],
                        "quote": "An innovative approach in this domain is SparseGPT (Frantar and Alistarh, 2023), which introduces a one-shot pruning strategy without retraining. SparseGPT frames pruning as an extensive sparse regression problem and solves it using an approximate sparse regression solver. SparseGPT achieves significant unstructured sparsity, even up to over 50% on the largest GPT models like OPT-175B and BLOOM-176B, with minimal increase in perplexity. To reduce the cost about the weight update process required by SparseGPT, Wanda (Sun et al., 2023) achieves model sparsity by pruning weights with the smallest magnitudes multiplied by the norm of the corresponding input activations, without the need for retraining or weight updates. To further minimize pruning-induced errors while upholding the desired overall sparsity level, SAMSP (Shao et al., 2023) utilizes the Hessian matrix as a metric for weight matrix sensitivity evaluation, and dynamically adjusts sparsity allocation based on sensitivity. Furthermore, DSnoT (Zhang et al., 2023) minimizes the reconstruction error between dense and sparse models through iterative weight pruning-and-growing on top of sparse LLMs to enhance LLM performance across various sparsity rates, especially at high sparsity levels."
                    }
                ]
            },
            {
                "idx": 3,
                "key": "[264590698 | Chen et al. | 2023 | Citations: 32]",
                "snippets": "SparseGPT (Frantar et al., 2023)) uses a sophisticated weight update process involving synchronized secondorder Hessian updates, bypassing traditional retraining. In contrast, Wanda (Sun et al., 2023) achieves high sparsity without any retraining, simply by pruning weights with the smallest magnitudes multiplied by their corresponding input activations.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {
                    "[255372747 | Frantar et al. | 2023 | Citations: 734]": "We show for the first time that large-scale generative pretrained transformer (GPT) family models can be pruned to at least 50% sparsity in one-shot, without any retraining, at minimal loss of accuracy. This is achieved via a new pruning method called SparseGPT, specifically designed to work efficiently and accurately on massive GPT-family models. We can execute SparseGPT on the largest available open-source models, OPT-175B and BLOOM-176B, in under 4.5 hours, and can reach 60% unstructured sparsity with negligible increase in perplexity: remarkably, more than 100 billion weights from these models can be ignored at inference time. SparseGPT generalizes to semi-structured (2:4 and 4:8) patterns, and is compatible with weight quantization approaches. The code is available at: https://github.com/IST-DASLab/sparsegpt."
                },
                "metadata": [
                    {
                        "section_title": "Related Work",
                        "pdf_hash": "",
                        "start": 1061,
                        "end": 1419,
                        "sentence_offsets": [
                            {
                                "start": 1061,
                                "end": 1226
                            },
                            {
                                "start": 1227,
                                "end": 1419
                            }
                        ],
                        "ref_mentions": [
                            "255372747"
                        ],
                        "quote": "SparseGPT (Frantar et al., 2023)) uses a sophisticated weight update process involving synchronized secondorder Hessian updates, bypassing traditional retraining. In contrast, Wanda (Sun et al., 2023) achieves high sparsity without any retraining, simply by pruning weights with the smallest magnitudes multiplied by their corresponding input activations."
                    }
                ]
            },
            {
                "idx": 4,
                "key": "[265050936 | Das et al. | 2023 | Citations: 19]",
                "snippets": "Large Language Models (LLMs) with billions of parameters are prime targets for network pruning, removing some model weights without hurting performance. Prior approaches such as magnitude pruning, SparseGPT, and Wanda, either concentrated solely on weights or integrated weights with activations for sparsity. However, they overlooked the informative gradients derived from pretrained LLMs. In this paper, we present a novel sparsity-centric pruning method for pretrained LLMs, termed Gradient-based Language Model Pruner (GBLM-Pruner). GBLM-Pruner leverages the first-order term of the Taylor expansion, operating in a training-free manner by harnessing properly normalized gradients from a few calibration samples to determine the pruning metric, and substantially outperforms competitive counterparts like SparseGPT and Wanda in multiple benchmarks.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {},
                "metadata": [
                    {
                        "section_title": "abstract",
                        "pdf_hash": "",
                        "start": 0,
                        "end": 852,
                        "sentence_offsets": [],
                        "ref_mentions": [],
                        "quote": "Large Language Models (LLMs) with billions of parameters are prime targets for network pruning, removing some model weights without hurting performance. Prior approaches such as magnitude pruning, SparseGPT, and Wanda, either concentrated solely on weights or integrated weights with activations for sparsity. However, they overlooked the informative gradients derived from pretrained LLMs. In this paper, we present a novel sparsity-centric pruning method for pretrained LLMs, termed Gradient-based Language Model Pruner (GBLM-Pruner). GBLM-Pruner leverages the first-order term of the Taylor expansion, operating in a training-free manner by harnessing properly normalized gradients from a few calibration samples to determine the pruning metric, and substantially outperforms competitive counterparts like SparseGPT and Wanda in multiple benchmarks."
                    }
                ]
            },
            {
                "idx": 5,
                "key": "[266362404 | An et al. | 2023 | Citations: 61]",
                "snippets": "In this paper, we propose FLAP (FLuctuation-based Adaptive Structured Pruning), a retraining-free structured pruning framework explicitly designed for Large Language Models (LLMs). To address the challenges posed by structured pruning, we introduce a novel structured pruning metric, employ adaptive global model compression strategies, and implement robust compensation mechanisms designed to mitigate potential performance losses. Our empirical results affirm that the structured compression model crafted by FLAP can maintain perplexity and zero-shot performance without any retraining.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {},
                "metadata": [
                    {
                        "quote": "In this paper, we propose FLAP (FLuctuation-based Adaptive Structured Pruning), a retraining-free structured pruning framework explicitly designed for Large Language Models (LLMs). To address the challenges posed by structured pruning, we introduce a novel structured pruning metric, employ adaptive global model compression strategies, and implement robust compensation mechanisms designed to mitigate potential performance losses. Our empirical results affirm that the structured compression model crafted by FLAP can maintain perplexity and zero-shot performance without any retraining.",
                        "pdf_hash": ""
                    }
                ]
            },
            {
                "idx": 6,
                "key": "[267412232 | Wang et al. | 2024 | Citations: 45]",
                "snippets": "Pruning can help optimize the model for deployment and make the model more efficient in terms of computation complexity and memory usage. Accordingly, pruning can be considered as an approach to enable a language model to support longer sequence length, while maintaining the desirable complexity and performance. In general, pruning a model can be categorized into structured and unstructured pruning...Unstructured pruning involves with pruning individual parameters of a model independently based on their magnitudes or importance, resulting in an irregular sparse structure. Due to the irregularity in the structure and in the memory access patterns, unstructured pruning hinders the efficiency gain that might be achieved through structured pruning, and it requires specialized software and/or hardware for efficient deployment. SparseGPT [Frantar and Alistarh, 2023] compresses LLMs with billions of parameter by as much as 60%, almost without affecting the performance of the models. However, SparseGPT heavily relies on weight updates.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {},
                "metadata": [
                    {
                        "section_title": "Model Compression",
                        "pdf_hash": "",
                        "start": 0,
                        "end": 401,
                        "sentence_offsets": [
                            {
                                "start": 0,
                                "end": 137
                            },
                            {
                                "start": 138,
                                "end": 313
                            },
                            {
                                "start": 314,
                                "end": 402
                            }
                        ],
                        "ref_mentions": [],
                        "quote": "Pruning can help optimize the model for deployment and make the model more efficient in terms of computation complexity and memory usage. Accordingly, pruning can be considered as an approach to enable a language model to support longer sequence length, while maintaining the desirable complexity and performance. In general, pruning a model can be categorized into structured and unstructured pruning"
                    },
                    {
                        "section_title": "Model Compression",
                        "pdf_hash": "",
                        "start": 1437,
                        "end": 2077,
                        "sentence_offsets": [
                            {
                                "start": 1437,
                                "end": 1611
                            },
                            {
                                "start": 1612,
                                "end": 1866
                            },
                            {
                                "start": 1867,
                                "end": 2023
                            },
                            {
                                "start": 2024,
                                "end": 2076
                            }
                        ],
                        "ref_mentions": [],
                        "quote": "Unstructured pruning involves with pruning individual parameters of a model independently based on their magnitudes or importance, resulting in an irregular sparse structure. Due to the irregularity in the structure and in the memory access patterns, unstructured pruning hinders the efficiency gain that might be achieved through structured pruning, and it requires specialized software and/or hardware for efficient deployment. SparseGPT [Frantar and Alistarh, 2023] compresses LLMs with billions of parameter by as much as 60%, almost without affecting the performance of the models. However, SparseGPT heavily relies on weight updates."
                    }
                ]
            },
            {
                "idx": 7,
                "key": "[268032346 | Xu et al. | 2024 | Citations: 32]",
                "snippets": "Existing solutions such as SparseGPT and Wanda attempt to alleviate this issue through weight pruning. However, their layer-wise approach results in significant perturbation to the model's output and requires meticulous hyperparameter tuning, such as the pruning rate, which can adversely affect overall model performance.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {},
                "metadata": [
                    {
                        "section_title": "abstract",
                        "pdf_hash": "",
                        "start": 277,
                        "end": 599,
                        "sentence_offsets": [],
                        "ref_mentions": [],
                        "quote": "Existing solutions such as SparseGPT and Wanda attempt to alleviate this issue through weight pruning. However, their layer-wise approach results in significant perturbation to the model's output and requires meticulous hyperparameter tuning, such as the pruning rate, which can adversely affect overall model performance."
                    }
                ]
            },
            {
                "idx": 8,
                "key": "[268041812 | Bai et al. | 2024 | Citations: 11]",
                "snippets": "SparseGPT Frantar and Alistarh [2023], an efficient unstructured pruning method for LLMs with hundreds of billions of parameters, achieves parameter reduction of up to 60% with minimal performance loss. Another approach, Wanda Sun et al. [2023], introduces a novel pruning criterion that evaluates weights by considering both magnitude and related input activations.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {},
                "metadata": [
                    {
                        "quote": "SparseGPT Frantar and Alistarh [2023], an efficient unstructured pruning method for LLMs with hundreds of billions of parameters, achieves parameter reduction of up to 60% with minimal performance loss. Another approach, Wanda Sun et al. [2023], introduces a novel pruning criterion that evaluates weights by considering both magnitude and related input activations.",
                        "pdf_hash": ""
                    }
                ]
            },
            {
                "idx": 9,
                "key": "[269605957 | Malla et al. | 2024 | Citations: 2]",
                "snippets": "To the best of our knowledge, we are the first to introduce the concept of continual pruning that addresses pruning under a continual model adaptation setting, bypassing the requirement for model re-training.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {},
                "metadata": [
                    {
                        "section_title": "Introduction",
                        "pdf_hash": "",
                        "start": 359,
                        "end": 567,
                        "sentence_offsets": [
                            {
                                "start": 359,
                                "end": 567
                            }
                        ],
                        "ref_mentions": [],
                        "quote": "To the best of our knowledge, we are the first to introduce the concept of continual pruning that addresses pruning under a continual model adaptation setting, bypassing the requirement for model re-training."
                    }
                ]
            },
            {
                "idx": 10,
                "key": "[269791108 | Huang et al. | 2024 | Citations: 12]",
                "snippets": "Ma et al. (Ma et al., 2023) propose the LLM-Pruner approach to optimize large language models by selectively removing non-critical coupling structures based on gradient information to achieve efficient compression while preserving functionality and task agnosticism.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {
                    "[258823276 | Ma et al. | 2023 | Citations: 440]": "Large language models (LLMs) have shown remarkable capabilities in language understanding and generation. However, such impressive capability typically comes with a substantial model size, which presents significant challenges in both the deployment, inference, and training stages. With LLM being a general-purpose task solver, we explore its compression in a task-agnostic manner, which aims to preserve the multi-task solving and language generation ability of the original LLM. One challenge to achieving this is the enormous size of the training corpus of LLM, which makes both data transfer and model post-training over-burdensome. Thus, we tackle the compression of LLMs within the bound of two constraints: being task-agnostic and minimizing the reliance on the original training dataset. Our method, named LLM-Pruner, adopts structural pruning that selectively removes non-critical coupled structures based on gradient information, maximally preserving the majority of the LLM's functionality. To this end, the performance of pruned models can be efficiently recovered through tuning techniques, LoRA, in merely 3 hours, requiring only 50K data. We validate the LLM-Pruner on three LLMs, including LLaMA, Vicuna, and ChatGLM, and demonstrate that the compressed models still exhibit satisfactory capabilities in zero-shot classification and generation. The code is available at: https://github.com/horseee/LLM-Pruner"
                },
                "metadata": [
                    {
                        "section_title": "Based on structural pruning",
                        "pdf_hash": "",
                        "start": 800,
                        "end": 1053,
                        "sentence_offsets": [
                            {
                                "start": 800,
                                "end": 1053
                            }
                        ],
                        "ref_mentions": [
                            "258823276"
                        ],
                        "quote": "Ma et al. (Ma et al., 2023) propose the LLM-Pruner approach to optimize large language models by selectively removing non-critical coupling structures based on gradient information to achieve efficient compression while preserving functionality and task agnosticism."
                    }
                ]
            },
            {
                "idx": 11,
                "key": "[270370902 | Dutta et al. | 2024 | Citations: 2]",
                "snippets": "We show the scaling ability of our pruning technique to prune large language models like LLaMA-2 (Touvron et al., 2023).Comparison with previous techniques includes SparseGPT (Frantar et al., 2023) which proposes a second-order layer-wise pruning method that approximates closed form equations thus being able to scale up pruning LLMs.Wanda (Sun et al., 2023) takes into account the norm of weights and input activations for pruning weights in an unstructured/structured manner.Bonsai (Dery et al., 2024) is a gradient-free structured pruning method that estimates module importance perturbatively by generating sub-models and evaluating their performances.LLM-pruner (Ma et al., 2023) is a structured pruning method that uses gradient information to prune large language models in a task-agnostic manner.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {
                    "[255372747 | Frantar et al. | 2023 | Citations: 734]": "We show for the first time that large-scale generative pretrained transformer (GPT) family models can be pruned to at least 50% sparsity in one-shot, without any retraining, at minimal loss of accuracy. This is achieved via a new pruning method called SparseGPT, specifically designed to work efficiently and accurately on massive GPT-family models. We can execute SparseGPT on the largest available open-source models, OPT-175B and BLOOM-176B, in under 4.5 hours, and can reach 60% unstructured sparsity with negligible increase in perplexity: remarkably, more than 100 billion weights from these models can be ignored at inference time. SparseGPT generalizes to semi-structured (2:4 and 4:8) patterns, and is compatible with weight quantization approaches. The code is available at: https://github.com/IST-DASLab/sparsegpt."
                },
                "metadata": [
                    {
                        "section_title": "A.2 Further details about the baseline models",
                        "pdf_hash": "",
                        "start": 227,
                        "end": 1036,
                        "sentence_offsets": [
                            {
                                "start": 227,
                                "end": 347
                            },
                            {
                                "start": 347,
                                "end": 566
                            },
                            {
                                "start": 566,
                                "end": 709
                            },
                            {
                                "start": 709,
                                "end": 888
                            },
                            {
                                "start": 888,
                                "end": 1036
                            }
                        ],
                        "ref_mentions": [
                            "255372747"
                        ],
                        "quote": "We show the scaling ability of our pruning technique to prune large language models like LLaMA-2 (Touvron et al., 2023).Comparison with previous techniques includes SparseGPT (Frantar et al., 2023) which proposes a second-order layer-wise pruning method that approximates closed form equations thus being able to scale up pruning LLMs.Wanda (Sun et al., 2023) takes into account the norm of weights and input activations for pruning weights in an unstructured/structured manner.Bonsai (Dery et al., 2024) is a gradient-free structured pruning method that estimates module importance perturbatively by generating sub-models and evaluating their performances.LLM-pruner (Ma et al., 2023) is a structured pruning method that uses gradient information to prune large language models in a task-agnostic manner."
                    }
                ]
            },
            {
                "idx": 12,
                "key": "[270411995 | Touheed et al. | 2024 | Citations: 1]",
                "snippets": "Ma et al. [29] proposes Large Language Model-Pruner (LLM-Pruner), a task-agnostic compression method, aiming to maintain the multi-task solving and language generation abilities of the original LLM while minimizing reliance on the extensive training dataset.LLM-Pruner adopts structural pruning, removing non-critical coupled structures based on gradient information to preserve most of the LLM's functionality.The pruned models can be efficiently recovered through tuning techniques in a short time and with minimal data.Experimental results on three LLMs, Large Language Model Meta AI (LLaMA), Vicuna, and General Language Model (ChatGLM), demonstrate that the compressed models perform well in zero-shot classification and generation tasks.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {},
                "metadata": [
                    {
                        "section_title": "IV. METHODS",
                        "pdf_hash": "",
                        "start": 1548,
                        "end": 2291,
                        "sentence_offsets": [
                            {
                                "start": 1548,
                                "end": 1806
                            },
                            {
                                "start": 1806,
                                "end": 1959
                            },
                            {
                                "start": 1959,
                                "end": 2070
                            },
                            {
                                "start": 2070,
                                "end": 2291
                            }
                        ],
                        "ref_mentions": [],
                        "quote": "Ma et al. [29] proposes Large Language Model-Pruner (LLM-Pruner), a task-agnostic compression method, aiming to maintain the multi-task solving and language generation abilities of the original LLM while minimizing reliance on the extensive training dataset.LLM-Pruner adopts structural pruning, removing non-critical coupled structures based on gradient information to preserve most of the LLM's functionality.The pruned models can be efficiently recovered through tuning techniques in a short time and with minimal data.Experimental results on three LLMs, Large Language Model Meta AI (LLaMA), Vicuna, and General Language Model (ChatGLM), demonstrate that the compressed models perform well in zero-shot classification and generation tasks."
                    }
                ]
            },
            {
                "idx": 13,
                "key": "[271064490 | Kolbeinsson et al. | 2024 | Citations: 4]",
                "snippets": "\u2022 SparseGPT (Frantar & Alistarh, 2023): an efficient one-shot pruning method tailored for large models. It converts the pruning process into solving large-scale sparse regression problems using an approximate solver. This approach enables rapid pruning on a single GPU with minimal accuracy loss, achieving 50-60% on large models. \n\n\u2022 Wanda (Sun et al., 2023): is another popular method for pruning large language models that relies on a pruning metric that combines a weight's magnitude with the norm of its corresponding input activations, determined from a small calibration dataset. The method focuses on selectively pruning weights within individual outputs of linear layers, aiming for high sparsity levels without modifying unpruned weights. Wanda is computationally efficient, executable in a single forward pass.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {},
                "metadata": [
                    {
                        "section_title": "C.3.2 COMPRESSOR DETAILS",
                        "pdf_hash": "",
                        "start": 82,
                        "end": 903,
                        "sentence_offsets": [
                            {
                                "start": 82,
                                "end": 185
                            },
                            {
                                "start": 186,
                                "end": 298
                            },
                            {
                                "start": 299,
                                "end": 412
                            },
                            {
                                "start": 415,
                                "end": 668
                            },
                            {
                                "start": 669,
                                "end": 830
                            },
                            {
                                "start": 831,
                                "end": 903
                            }
                        ],
                        "ref_mentions": [],
                        "quote": "\u2022 SparseGPT (Frantar & Alistarh, 2023): an efficient one-shot pruning method tailored for large models. It converts the pruning process into solving large-scale sparse regression problems using an approximate solver. This approach enables rapid pruning on a single GPU with minimal accuracy loss, achieving 50-60% on large models. \n\n\u2022 Wanda (Sun et al., 2023): is another popular method for pruning large language models that relies on a pruning metric that combines a weight's magnitude with the norm of its corresponding input activations, determined from a small calibration dataset. The method focuses on selectively pruning weights within individual outputs of linear layers, aiming for high sparsity levels without modifying unpruned weights. Wanda is computationally efficient, executable in a single forward pass."
                    }
                ]
            },
            {
                "idx": 14,
                "key": "[271083368 | Rostam et al. | 2024 | Citations: 4]",
                "snippets": "One-Shot Pruning: SparseGPT implements its pruning strategy through a streamlined process. First, a thorough model analysis is conducted to pinpoint parameters that can be removed without significant impact. This analysis leverages pruning criteria that assess parameter importance without requiring gradient calculations, saving on computational resources. Finally, SparseGPT employs a single step pruning approach, achieving substantial sparsity (at least 50% for massive models) in a single step. This oneshot approach significantly reduces the time and complexity compared to iterative pruning methods.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {},
                "metadata": [
                    {
                        "section_title": "A. OPTIMIZING MODEL TRAINING WITH SPARSEGPT",
                        "pdf_hash": "",
                        "start": 1103,
                        "end": 1709,
                        "sentence_offsets": [
                            {
                                "start": 1103,
                                "end": 1193
                            },
                            {
                                "start": 1194,
                                "end": 1310
                            },
                            {
                                "start": 1311,
                                "end": 1460
                            },
                            {
                                "start": 1461,
                                "end": 1602
                            },
                            {
                                "start": 1603,
                                "end": 1709
                            }
                        ],
                        "ref_mentions": [],
                        "quote": "One-Shot Pruning: SparseGPT implements its pruning strategy through a streamlined process. First, a thorough model analysis is conducted to pinpoint parameters that can be removed without significant impact. This analysis leverages pruning criteria that assess parameter importance without requiring gradient calculations, saving on computational resources. Finally, SparseGPT employs a single step pruning approach, achieving substantial sparsity (at least 50% for massive models) in a single step. This oneshot approach significantly reduces the time and complexity compared to iterative pruning methods."
                    }
                ]
            },
            {
                "idx": 15,
                "key": "[271533761 | Li et al. | 2024 | Citations: 6]",
                "snippets": "This paper introduces a novel approach to pruning large language models (LLMs) by identifying a depth-2 pruning structure and developing two inference-aware pruning criteria. These methods surpass traditional metrics and eliminate the need for computationally expensive retraining. Our two-step reconstruction technique further mitigates pruning errors, ensuring superior performance across various datasets and models. Overall, our approach significantly reduces computational costs and hardware requirements, offering an efficient solution for pruning colossal LLMs.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {},
                "metadata": [
                    {
                        "section_title": "Conclusion:",
                        "pdf_hash": "",
                        "start": 0,
                        "end": 568,
                        "sentence_offsets": [
                            {
                                "start": 0,
                                "end": 174
                            },
                            {
                                "start": 175,
                                "end": 281
                            },
                            {
                                "start": 282,
                                "end": 419
                            },
                            {
                                "start": 420,
                                "end": 568
                            }
                        ],
                        "ref_mentions": [],
                        "quote": "This paper introduces a novel approach to pruning large language models (LLMs) by identifying a depth-2 pruning structure and developing two inference-aware pruning criteria. These methods surpass traditional metrics and eliminate the need for computationally expensive retraining. Our two-step reconstruction technique further mitigates pruning errors, ensuring superior performance across various datasets and models. Overall, our approach significantly reduces computational costs and hardware requirements, offering an efficient solution for pruning colossal LLMs."
                    }
                ]
            },
            {
                "idx": 16,
                "key": "[271909582 | Su et al. | 2024 | Citations: 2]",
                "snippets": "To address these limitations, recent post-training pruning techniques such as SparseGPT and Wanda have emerged.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {},
                "metadata": [
                    {
                        "section_title": "Introduction",
                        "pdf_hash": "",
                        "start": 1158,
                        "end": 1269,
                        "sentence_offsets": [
                            {
                                "start": 1158,
                                "end": 1269
                            }
                        ],
                        "ref_mentions": [],
                        "quote": "To address these limitations, recent post-training pruning techniques such as SparseGPT and Wanda have emerged."
                    }
                ]
            },
            {
                "idx": 17,
                "key": "[271909626 | Dong et al. | 2024 | Citations: 5]",
                "snippets": "In order to reduce the training duration, the model pruning technology can be used to remove unnecessary neurons in the LLMs while retaining the versatility of the pruned LLMs. In this vein, X. Ma et al. propose a task-agnostic pruner (named as, the LLM-Pruner) to preserve the capability to handle various task without requiring original training dataset and long duration  of retraining. Since the LLMs contain redundant parameters that have little/no effects on the performance of models, the LLM-Pruner reduces the scale of LLMs based on the gradient information. More specifically, the LLM-Pruner incorporates three main stages: discovery, estimation, and recovery.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {},
                "metadata": [
                    {
                        "section_title": "B. Compression-Then-Train",
                        "pdf_hash": "",
                        "start": 210,
                        "end": 880,
                        "sentence_offsets": [
                            {
                                "start": 210,
                                "end": 386
                            },
                            {
                                "start": 387,
                                "end": 599
                            },
                            {
                                "start": 600,
                                "end": 777
                            },
                            {
                                "start": 778,
                                "end": 880
                            }
                        ],
                        "ref_mentions": [],
                        "quote": "In order to reduce the training duration, the model pruning technology can be used to remove unnecessary neurons in the LLMs while retaining the versatility of the pruned LLMs. In this vein, X. Ma et al. propose a task-agnostic pruner (named as, the LLM-Pruner) to preserve the capability to handle various task without requiring original training dataset and long duration  of retraining. Since the LLMs contain redundant parameters that have little/no effects on the performance of models, the LLM-Pruner reduces the scale of LLMs based on the gradient information. More specifically, the LLM-Pruner incorporates three main stages: discovery, estimation, and recovery."
                    }
                ]
            },
            {
                "idx": 18,
                "key": "[273350592 | Lu et al. | 2024 | Citations: 10]",
                "snippets": "For example, in the case of LLaMA-7B with a sparsity of 70%, AlphaPruning produces sparse networks with a perplexity of 231.01, significantly outperforming the Magnitude-based pruning baseline of 48419.13. Notably, when applied to Wanda and SparseGPT, two robust LLM pruning methods, AlphaPruning still achieves substantial perplexity reductions, evidenced by a decrease of 61.91 for Wanda and 7.76 for SparseGPT, in the case of LLaMA-7B with a sparsity of 70%.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {},
                "metadata": [
                    {
                        "section_title": "Main results",
                        "pdf_hash": "",
                        "start": 344,
                        "end": 805,
                        "sentence_offsets": [
                            {
                                "start": 344,
                                "end": 549
                            },
                            {
                                "start": 550,
                                "end": 805
                            }
                        ],
                        "ref_mentions": [],
                        "quote": "For example, in the case of LLaMA-7B with a sparsity of 70%, AlphaPruning produces sparse networks with a perplexity of 231.01, significantly outperforming the Magnitude-based pruning baseline of 48419.13. Notably, when applied to Wanda and SparseGPT, two robust LLM pruning methods, AlphaPruning still achieves substantial perplexity reductions, evidenced by a decrease of 61.91 for Wanda and 7.76 for SparseGPT, in the case of LLaMA-7B with a sparsity of 70%."
                    }
                ]
            },
            {
                "idx": 19,
                "key": "[273375608 | Zheng et al. | 2024 | Citations: 22]",
                "snippets": "SparseGPT [44] treats pruning as a sparse regression problem, allowing for one-shot pruning without retraining. Wanda [160] prunes weights with the smallest magnitudes multiplied by the corresponding input activations, on a per-output basis.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {},
                "metadata": [
                    {
                        "quote": "SparseGPT [44] treats pruning as a sparse regression problem, allowing for one-shot pruning without retraining. Wanda [160] prunes weights with the smallest magnitudes multiplied by the corresponding input activations, on a per-output basis.",
                        "pdf_hash": ""
                    }
                ]
            },
            {
                "idx": 20,
                "key": "[273501976 | Zhao et al. | 2024 | Citations: 13]",
                "snippets": "The traditional pruning techniques, which finetune or retrain models (Li et al., 2020) on full datasets for many epochs (i.e., pruning-aware training), are too expensive for LLMs in terms of data and GPU resources. Thus, post-training pruning based on well-pre-trained models with reduced resource requirements represents a more reasonable approach for LLMs. Notably, SparseGPT (Frantar et al., 2023) is the representative post-training pruning work with outstanding performance. It reduces memory cost by sequentially loading transformer blocks, one at a time, instead of loading the whole model. Moreover, it reduces the data cost by using only a small amount of calibration data, eliminating the retraining process on massive data. Besides the optimization based SparseGPT, there are some other heuristic posttraining pruning methods such as (Sun et al., 2023;Zhang et al., 2024), achieving accuracy close to SparseGPT.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {
                    "[221761597 | Li et al. | 2020 | Citations: 65]": "Pretrained large-scale language models have increasingly demonstrated high accuracy on many natural language processing (NLP) tasks. However, the limited weight storage and computational speed on hardware platforms have impeded the popularity of pretrained models, especially in the era of edge computing. In this work, we propose an efficient transformer-based large-scale language representation using hardware-friendly block structure pruning. We incorporate the reweighted group Lasso into block-structured pruning for optimization. Besides the significantly reduced weight storage and computation, the proposed approach achieves high compression rates. Experimental results on different models (BERT, RoBERTa, and DistilBERT) on the General Language Understanding Evaluation (GLUE) benchmark tasks show that we achieve up to 5.0x with zero or minor accuracy degradation on certain task(s). Our proposed method is also orthogonal to existing compact pretrained language models such as DistilBERT using knowledge distillation, since a further 1.79x average compression rate can be achieved on top of DistilBERT with zero or minor accuracy degradation. It is suitable to deploy the final compressed model on resource-constrained edge devices.",
                    "[255372747 | Frantar et al. | 2023 | Citations: 734]": "We show for the first time that large-scale generative pretrained transformer (GPT) family models can be pruned to at least 50% sparsity in one-shot, without any retraining, at minimal loss of accuracy. This is achieved via a new pruning method called SparseGPT, specifically designed to work efficiently and accurately on massive GPT-family models. We can execute SparseGPT on the largest available open-source models, OPT-175B and BLOOM-176B, in under 4.5 hours, and can reach 60% unstructured sparsity with negligible increase in perplexity: remarkably, more than 100 billion weights from these models can be ignored at inference time. SparseGPT generalizes to semi-structured (2:4 and 4:8) patterns, and is compatible with weight quantization approaches. The code is available at: https://github.com/IST-DASLab/sparsegpt."
                },
                "metadata": [
                    {
                        "section_title": "Introduction",
                        "pdf_hash": "",
                        "start": 548,
                        "end": 1476,
                        "sentence_offsets": [
                            {
                                "start": 548,
                                "end": 762
                            },
                            {
                                "start": 763,
                                "end": 906
                            },
                            {
                                "start": 907,
                                "end": 1033
                            },
                            {
                                "start": 1034,
                                "end": 1151
                            },
                            {
                                "start": 1152,
                                "end": 1288
                            },
                            {
                                "start": 1289,
                                "end": 1476
                            }
                        ],
                        "ref_mentions": [
                            "221761597",
                            "255372747"
                        ],
                        "quote": "The traditional pruning techniques, which finetune or retrain models (Li et al., 2020) on full datasets for many epochs (i.e., pruning-aware training), are too expensive for LLMs in terms of data and GPU resources. Thus, post-training pruning based on well-pre-trained models with reduced resource requirements represents a more reasonable approach for LLMs. Notably, SparseGPT (Frantar et al., 2023) is the representative post-training pruning work with outstanding performance. It reduces memory cost by sequentially loading transformer blocks, one at a time, instead of loading the whole model. Moreover, it reduces the data cost by using only a small amount of calibration data, eliminating the retraining process on massive data. Besides the optimization based SparseGPT, there are some other heuristic posttraining pruning methods such as (Sun et al., 2023;Zhang et al., 2024), achieving accuracy close to SparseGPT."
                    }
                ]
            },
            {
                "idx": 21,
                "key": "[273549773 | Bhansali et al. | 2024 | Citations: 0]",
                "snippets": "Recently, more nuanced pruning approaches have been discussed in the literature, improving over more traditional methods like magnitude pruning. Specifically, two state-of-the-art pruning methods are widely discussed in the literature-SparseGPT (Frantar et al., 2023) and Wanda (Sun et al., 2023). Whereas traditional magnitude pruning operates by pruning weights with the largest magnitude, these pruning techniques instead track weight activations, and prune weights with the lowest amount of activation. \n\nSparseGPT creates and solves a layer-wise reconstruction problem to determine the weight activations, whereas Wanda takes the product of a weight's magnitude and the norm of its associated input activations.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {
                    "[255372747 | Frantar et al. | 2023 | Citations: 734]": "We show for the first time that large-scale generative pretrained transformer (GPT) family models can be pruned to at least 50% sparsity in one-shot, without any retraining, at minimal loss of accuracy. This is achieved via a new pruning method called SparseGPT, specifically designed to work efficiently and accurately on massive GPT-family models. We can execute SparseGPT on the largest available open-source models, OPT-175B and BLOOM-176B, in under 4.5 hours, and can reach 60% unstructured sparsity with negligible increase in perplexity: remarkably, more than 100 billion weights from these models can be ignored at inference time. SparseGPT generalizes to semi-structured (2:4 and 4:8) patterns, and is compatible with weight quantization approaches. The code is available at: https://github.com/IST-DASLab/sparsegpt."
                },
                "metadata": [
                    {
                        "section_title": "Model Compression",
                        "pdf_hash": "",
                        "start": 332,
                        "end": 1054,
                        "sentence_offsets": [
                            {
                                "start": 332,
                                "end": 476
                            },
                            {
                                "start": 477,
                                "end": 635
                            },
                            {
                                "start": 636,
                                "end": 844
                            },
                            {
                                "start": 847,
                                "end": 1054
                            }
                        ],
                        "ref_mentions": [
                            "255372747"
                        ],
                        "quote": "Recently, more nuanced pruning approaches have been discussed in the literature, improving over more traditional methods like magnitude pruning. Specifically, two state-of-the-art pruning methods are widely discussed in the literature-SparseGPT (Frantar et al., 2023) and Wanda (Sun et al., 2023). Whereas traditional magnitude pruning operates by pruning weights with the largest magnitude, these pruning techniques instead track weight activations, and prune weights with the lowest amount of activation. \n\nSparseGPT creates and solves a layer-wise reconstruction problem to determine the weight activations, whereas Wanda takes the product of a weight's magnitude and the norm of its associated input activations."
                    }
                ]
            },
            {
                "idx": 22,
                "key": "[273811289 | Yang et al. | 2024 | Citations: 7]",
                "snippets": "Recent advancements in large language models have underscored the need to reduce parameter sizes and latency (Ma et al., 2023). Compression techniques for language models include network pruning (Xu et al., 2021), knowledge distillation (Sun et al., 2019(Sun et al., , 2020)), quantization (Yao et al., 2022), decomposition (Hsu et al., 2022;Yuan et al., 2023;Wang et al., 2024), and methods like early exit (Xin et al., 2020)...Recent studies on LLMs (Sun et al., 2023;(Ma et al., 2023) focus on pruning linear layers, but these methods often fail to reduce computing costs without specialized hardware or libraries.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {
                    "[258823276 | Ma et al. | 2023 | Citations: 440]": "Large language models (LLMs) have shown remarkable capabilities in language understanding and generation. However, such impressive capability typically comes with a substantial model size, which presents significant challenges in both the deployment, inference, and training stages. With LLM being a general-purpose task solver, we explore its compression in a task-agnostic manner, which aims to preserve the multi-task solving and language generation ability of the original LLM. One challenge to achieving this is the enormous size of the training corpus of LLM, which makes both data transfer and model post-training over-burdensome. Thus, we tackle the compression of LLMs within the bound of two constraints: being task-agnostic and minimizing the reliance on the original training dataset. Our method, named LLM-Pruner, adopts structural pruning that selectively removes non-critical coupled structures based on gradient information, maximally preserving the majority of the LLM's functionality. To this end, the performance of pruned models can be efficiently recovered through tuning techniques, LoRA, in merely 3 hours, requiring only 50K data. We validate the LLM-Pruner on three LLMs, including LLaMA, Vicuna, and ChatGLM, and demonstrate that the compressed models still exhibit satisfactory capabilities in zero-shot classification and generation. The code is available at: https://github.com/horseee/LLM-Pruner",
                    "[249395624 | Yao et al. | 2022 | Citations: 479]": "How to efficiently serve ever-larger trained natural language models in practice has become exceptionally challenging even for powerful cloud servers due to their prohibitive memory/computation requirements. In this work, we present an efficient and affordable post-training quantization approach to compress large Transformer-based models, termed as ZeroQuant. ZeroQuant is an end-to-end quantization and inference pipeline with three main components: (1) a fine-grained hardware-friendly quantization scheme for both weight and activations; (2) a novel affordable layer-by-layer knowledge distillation algorithm (LKD) even without the access to the original training data; (3) a highly-optimized quantization system backend support to remove the quantization/dequantization overhead. As such, we are able to show that: (1) ZeroQuant can reduce the precision for weights and activations to INT8 in a cost-free way for both BERT and GPT3-style models with minimal accuracy impact, which leads to up to 5.19x/4.16x speedup on those models compared to FP16 inference; (2) ZeroQuant plus LKD affordably quantize the weights in the fully-connected module to INT4 along with INT8 weights in the attention module and INT8 activations, resulting in 3x memory footprint reduction compared to the FP16 model; (3) ZeroQuant can be directly applied to two of the largest open-sourced language models, including GPT-J6B and GPT-NeoX20, for which our INT8 model achieves similar accuracy as the FP16 model but achieves up to 5.2x better efficiency."
                },
                "metadata": [
                    {
                        "section_title": "Compression on MoE LLMs",
                        "pdf_hash": "",
                        "start": 0,
                        "end": 426,
                        "sentence_offsets": [
                            {
                                "start": 0,
                                "end": 127
                            },
                            {
                                "start": 128,
                                "end": 427
                            }
                        ],
                        "ref_mentions": [
                            "258823276",
                            "249395624"
                        ],
                        "quote": "Recent advancements in large language models have underscored the need to reduce parameter sizes and latency (Ma et al., 2023). Compression techniques for language models include network pruning (Xu et al., 2021), knowledge distillation (Sun et al., 2019(Sun et al., , 2020)), quantization (Yao et al., 2022), decomposition (Hsu et al., 2022;Yuan et al., 2023;Wang et al., 2024), and methods like early exit (Xin et al., 2020)"
                    },
                    {
                        "section_title": "Compression on MoE LLMs",
                        "pdf_hash": "",
                        "start": 1466,
                        "end": 1654,
                        "sentence_offsets": [
                            {
                                "start": 1466,
                                "end": 1653
                            }
                        ],
                        "ref_mentions": [
                            "258823276"
                        ],
                        "quote": "Recent studies on LLMs (Sun et al., 2023;(Ma et al., 2023) focus on pruning linear layers, but these methods often fail to reduce computing costs without specialized hardware or libraries."
                    }
                ]
            },
            {
                "idx": 23,
                "key": "[273901152 | Wei et al. | 2024 | Citations: 4]",
                "snippets": "The massive parameters and computational demands hinder the widespread application of Large Language Models (LLMs). Network pruning provides a practical solution to this problem. However, existing pruning works for LLMs mainly focus on unstructured pruning or necessitate post-pruning fine-tuning. The former relies on special hardware to accelerate computation, while the latter may need substantial computational resources. In this paper, we introduce a retraining-free structured pruning method called SoBP (Structured Optimal Brain Pruning). It leverages global first-order information to select pruning structures, then refines them with a local greedy approach, and finally adopts module-wise reconstruction to mitigate information loss.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {},
                "metadata": [
                    {
                        "section_title": "abstract",
                        "pdf_hash": "",
                        "start": 0,
                        "end": 743,
                        "sentence_offsets": [],
                        "ref_mentions": [],
                        "quote": "The massive parameters and computational demands hinder the widespread application of Large Language Models (LLMs). Network pruning provides a practical solution to this problem. However, existing pruning works for LLMs mainly focus on unstructured pruning or necessitate post-pruning fine-tuning. The former relies on special hardware to accelerate computation, while the latter may need substantial computational resources. In this paper, we introduce a retraining-free structured pruning method called SoBP (Structured Optimal Brain Pruning). It leverages global first-order information to select pruning structures, then refines them with a local greedy approach, and finally adopts module-wise reconstruction to mitigate information loss."
                    }
                ]
            },
            {
                "idx": 24,
                "key": "[273962638 | Cunegatti et al. | 2024 | Citations: 0]",
                "snippets": "In recent times, Large Language Models (LLMs) have shown incredible performance over almost any language task (Wei et al., 2022)(Min et al., 2021)(Chang et al., 2023). However, their performance tends to grow with their sizes (i.e., the number of trainable parameters), which in turn is proportional to the computational burden required to train and then use such models. One way to reduce the computational cost of LLMs is through network pruning, i.e., algorithms that remove parameters while minimizing performance degradation. This approach has been extensively studied on Convolutional Neural Networks (CNNs) [13,25,43,(Evci et al., 2019), but nowadays the focus has shifted towards pre-trained models [41,42,22]. This shift has required a change of paradigm in pruning techniques: in fact, while in CNNs the main paradigm is iterative pruning (with re-training) [13], with pre-trained models (such as LLMs) in most cases it is not possible to fully re-train such models, because (Ashkboos et al., 2024) training data are often not accessible, and (2) full re-training would be anyway too expensive. This calls for \"exploiting\" as much as possible the information contained in a pre-trained model to obtain a performant sparse version of it, using weight's information (Jaiswal et al., 2023), activations (Sun et al., 2023)[39], or reconstruction error (Frantar et al., 2023), without the need for re-training. More recently, a new category of pruning algorithms, which we may call top-up algorithms (i.e., methods that can be applied on top of a given pruning algorithm for LLMs), has emerged, aiming at further improving pruning performance. Such approaches can be divided into two categories: those that minimize the reconstruction error [18](Xu et al., 2024)(Zhang et al., 2023), and those that impose non-uniform sparsity distribution modifying the block-wise sparsity (Yin et al., 2023).",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {
                    "[259203115 | Sun et al. | 2023 | Citations: 439]": "As their size increases, Large Languages Models (LLMs) are natural candidates for network pruning methods: approaches that drop a subset of network weights while striving to preserve performance. Existing methods, however, require either retraining, which is rarely affordable for billion-scale LLMs, or solving a weight reconstruction problem reliant on second-order information, which may also be computationally expensive. In this paper, we introduce a novel, straightforward yet effective pruning method, termed Wanda (Pruning by Weights and activations), designed to induce sparsity in pretrained LLMs. Motivated by the recent observation of emergent large magnitude features in LLMs, our approach prunes weights with the smallest magnitudes multiplied by the corresponding input activations, on a per-output basis. Notably, Wanda requires no retraining or weight update, and the pruned LLM can be used as is. We conduct a thorough evaluation of our method Wanda on LLaMA and LLaMA-2 across various language benchmarks. Wanda significantly outperforms the established baseline of magnitude pruning and performs competitively against recent method involving intensive weight update. Code is available at https://github.com/locuslab/wanda.",
                    "[268032346 | Xu et al. | 2024 | Citations: 32]": "Large language models (LLMs) have demonstrated outstanding performance in various tasks, such as text summarization, text question-answering, and etc. While their performance is impressive, the computational footprint due to their vast number of parameters can be prohibitive. Existing solutions such as SparseGPT and Wanda attempt to alleviate this issue through weight pruning. However, their layer-wise approach results in significant perturbation to the model's output and requires meticulous hyperparameter tuning, such as the pruning rate, which can adversely affect overall model performance. To address this, this paper introduces a novel LLM pruning technique dubbed blockwise parameter-efficient sparsity allocation (BESA) by applying a blockwise reconstruction loss. In contrast to the typical layer-wise pruning techniques, BESA is characterized by two distinctive attributes: i) it targets the overall pruning error with respect to individual transformer blocks, and ii) it allocates layer-specific sparsity in a differentiable manner, both of which ensure reduced performance degradation after pruning. Our experiments show that BESA achieves state-of-the-art performance, efficiently pruning LLMs like LLaMA1, and LLaMA2 with 7B to 70B parameters on a single A100 GPU in just five hours. Code is available at https://github.com/OpenGVLab/LLMPrune-BESA.",
                    "[208267757 | Evci et al. | 2019 | Citations: 608]": "Many applications require sparse neural networks due to space or inference time restrictions. There is a large body of work on training dense networks to yield sparse networks for inference, but this limits the size of the largest trainable sparse model to that of the largest trainable dense model. In this paper we introduce a method to train sparse neural networks with a fixed parameter count and a fixed computational cost throughout training, without sacrificing accuracy relative to existing dense-to-sparse training methods. Our method updates the topology of the sparse network during training by using parameter magnitudes and infrequent gradient calculations. We show that this approach requires fewer floating-point operations (FLOPs) to achieve a given level of accuracy compared to prior techniques. We demonstrate state-of-the-art sparse training results on a variety of networks and datasets, including ResNet-50, MobileNets on Imagenet-2012, and RNNs on WikiText-103. Finally, we provide some insights into why allowing the topology to change during the optimization can overcome local minima encountered when the topology remains static. Code used in our work can be found in this http URL.",
                    "[240420063 | Min et al. | 2021 | Citations: 1083]": "Large, pre-trained language models (PLMs) such as BERT and GPT have drastically changed the Natural Language Processing (NLP) field. For numerous NLP tasks, approaches leveraging PLMs have achieved state-of-the-art performance. The key idea is to learn a generic, latent representation of language from a generic task once, then share it across disparate NLP tasks. Language modeling serves as the generic task, one with abundant self-supervised text available for extensive training. This article presents the key fundamental concepts of PLM architectures and a comprehensive view of the shift to PLM-driven NLP techniques. It surveys work applying the pre-training then fine-tuning, prompting, and text generation approaches. In addition, it discusses PLM limitations and suggested directions for future research.",
                    "[246411621 | Wei et al. | 2022 | Citations: 9683]": "We explore how generating a chain of thought -- a series of intermediate reasoning steps -- significantly improves the ability of large language models to perform complex reasoning. In particular, we show how such reasoning abilities emerge naturally in sufficiently large language models via a simple method called chain of thought prompting, where a few chain of thought demonstrations are provided as exemplars in prompting. Experiments on three large language models show that chain of thought prompting improves performance on a range of arithmetic, commonsense, and symbolic reasoning tasks. The empirical gains can be striking. For instance, prompting a 540B-parameter language model with just eight chain of thought exemplars achieves state of the art accuracy on the GSM8K benchmark of math word problems, surpassing even finetuned GPT-3 with a verifier.",
                    "[255372747 | Frantar et al. | 2023 | Citations: 734]": "We show for the first time that large-scale generative pretrained transformer (GPT) family models can be pruned to at least 50% sparsity in one-shot, without any retraining, at minimal loss of accuracy. This is achieved via a new pruning method called SparseGPT, specifically designed to work efficiently and accurately on massive GPT-family models. We can execute SparseGPT on the largest available open-source models, OPT-175B and BLOOM-176B, in under 4.5 hours, and can reach 60% unstructured sparsity with negligible increase in perplexity: remarkably, more than 100 billion weights from these models can be ignored at inference time. SparseGPT generalizes to semi-structured (2:4 and 4:8) patterns, and is compatible with weight quantization approaches. The code is available at: https://github.com/IST-DASLab/sparsegpt.",
                    "[259088941 | Jaiswal et al. | 2023 | Citations: 34]": "Large pre-trained transformers are show-stealer in modern-day deep learning, and it becomes crucial to comprehend the parsimonious patterns that exist within them as they grow in scale. With exploding parameter counts, Lottery Ticket Hypothesis (LTH) and its variants, have lost their pragmatism in sparsifying them due to high computation and memory bottleneck of repetitive train-prune-retrain routine of iterative magnitude pruning (IMP) which worsens with increasing model size. This paper comprehensively studies induced sparse patterns across multiple large pre-trained vision and language transformers. We propose the existence of -- essential sparsity defined with a sharp dropping point beyond which the performance declines much faster w.r.t the rise of sparsity level, when we directly remove weights with the smallest magnitudes in one-shot without re-training. We also find essential sparsity to hold valid for N:M sparsity patterns as well as on modern-scale large language models (Vicuna-7B). We also present an intriguing emerging phenomenon of abrupt sparsification during the pre-training of BERT, i.e., BERT suddenly becomes heavily sparse in pre-training after certain iterations. Moreover, our observations also indicate a counter-intuitive finding that BERT trained with a larger amount of pre-training data tends to have a better ability to condense knowledge in comparatively relatively fewer parameters. Lastly, we investigate the effect of the pre-training loss on essential sparsity and discover that self-supervised learning (SSL) objectives trigger stronger emergent sparsification properties than supervised learning (SL). Our codes are available at \\url{https://github.com/VITA-Group/essential_sparsity}.",
                    "[259360395 | Chang et al. | 2023 | Citations: 1710]": "Large language models (LLMs) are gaining increasing popularity in both academia and industry, owing to their unprecedented performance in various applications. As LLMs continue to play a vital role in both research and daily use, their evaluation becomes increasingly critical, not only at the task level, but also at the society level for better understanding of their potential risks. Over the past years, significant efforts have been made to examine LLMs from various perspectives. This paper presents a comprehensive review of these evaluation methods for LLMs, focusing on three key dimensions: what to evaluate, where to evaluate, and how to evaluate. Firstly, we provide an overview from the perspective of evaluation tasks, encompassing general natural language processing tasks, reasoning, medical usage, ethics, education, natural and social sciences, agent applications, and other areas. Secondly, we answer the \u2018where\u2019 and \u2018how\u2019 questions by diving into the evaluation methods and benchmarks, which serve as crucial components in assessing the performance of LLMs. Then, we summarize the success and failure cases of LLMs in different tasks. Finally, we shed light on several future challenges that lie ahead in LLMs evaluation. Our aim is to offer invaluable insights to researchers in the realm of LLMs evaluation, thereby aiding the development of more proficient LLMs. Our key point is that evaluation should be treated as an essential discipline to better assist the development of LLMs. We consistently maintain the related open-source materials at: https://github.com/MLGroupJLU/LLM-eval-survey",
                    "[263829692 | Yin et al. | 2023 | Citations: 102]": "Large Language Models (LLMs), renowned for their remarkable performance across diverse domains, present a challenge when it comes to practical deployment due to their colossal model size. In response to this challenge, efforts have been directed toward the application of traditional network pruning techniques to LLMs, uncovering a massive number of parameters that can be pruned in one-shot without hurting performance. Prevailing LLM pruning strategies have consistently adhered to the practice of uniformly pruning all layers at equivalent sparsity, resulting in robust performance. However, this observation stands in contrast to the prevailing trends observed in the field of vision models, where non-uniform layerwise sparsity typically yields stronger results. To understand the underlying reasons for this disparity, we conduct a comprehensive study and discover a strong correlation with the emergence of activation outliers in LLMs. Inspired by this finding, we introduce a novel LLM pruning methodology that incorporates a tailored set of non-uniform layerwise sparsity ratios, termed as Outlier Weighed Layerwise sparsity (OWL). The sparsity ratio of OWL is proportional to the outlier ratio observed within each layer, facilitating a more effective alignment between layerwise weight sparsity and outlier ratios. Our empirical evaluation, conducted across the LLaMA-V1 family and OPT, spanning various benchmarks, demonstrates the distinct advantages offered by OWL over previous methods. For instance, OWL exhibits a remarkable performance gain, surpassing the state-of-the-art Wanda and SparseGPT by 61.22 and 6.80 perplexity at a high sparsity level of 70%, respectively, while delivering 2.6x end-to-end inference speed-up in the DeepSparse inference engine. Codes are available at https://github.com/luuyin/OWL.",
                    "[264128029 | Zhang et al. | 2023 | Citations: 43]": "The ever-increasing large language models (LLMs), though opening a potential path for the upcoming artificial general intelligence, sadly drops a daunting obstacle on the way towards their on-device deployment. As one of the most well-established pre-LLMs approaches in reducing model complexity, network pruning appears to lag behind in the era of LLMs, due mostly to its costly fine-tuning (or re-training) necessity under the massive volumes of model parameter and training data. To close this industry-academia gap, we introduce Dynamic Sparse No Training (DSnoT), a training-free fine-tuning approach that slightly updates sparse LLMs without the expensive backpropagation and any weight updates. Inspired by the Dynamic Sparse Training, DSnoT minimizes the reconstruction error between the dense and sparse LLMs, in the fashion of performing iterative weight pruning-and-growing on top of sparse LLMs. To accomplish this purpose, DSnoT particularly takes into account the anticipated reduction in reconstruction error for pruning and growing, as well as the variance w.r.t. different input data for growing each weight. This practice can be executed efficiently in linear time since its obviates the need of backpropagation for fine-tuning LLMs. Extensive experiments on LLaMA-V1/V2, Vicuna, and OPT across various benchmarks demonstrate the effectiveness of DSnoT in enhancing the performance of sparse LLMs, especially at high sparsity levels. For instance, DSnoT is able to outperform the state-of-the-art Wanda by 26.79 perplexity at 70% sparsity with LLaMA-7B. Our paper offers fresh insights into how to fine-tune sparse LLMs in an efficient training-free manner and open new venues to scale the great potential of sparsity to LLMs. Codes are available at https://github.com/zyxxmu/DSnoT.",
                    "[267301573 | Ashkboos et al. | 2024 | Citations: 184]": "Large language models have become the cornerstone of natural language processing, but their use comes with substantial costs in terms of compute and memory resources. Sparsification provides a solution to alleviate these resource constraints, and recent works have shown that trained models can be sparsified post-hoc. Existing sparsification techniques face challenges as they need additional data structures and offer constrained speedup with current hardware. In this paper we present SliceGPT, a new post-training sparsification scheme which replaces each weight matrix with a smaller (dense) matrix, reducing the embedding dimension of the network. Through extensive experimentation, we show that SliceGPT can remove up to 25% of the model parameters (including embeddings) for LLAMA2-70B, OPT 66B and Phi-2 models while maintaining 99%, 99% and 90% zero-shot task performance of the dense model respectively. Our sliced models run on fewer GPUs and run faster without any additional code optimization: on 24GB consumer GPUs we reduce the total compute for inference on LLAMA2-70B to 64% of that of the dense model; on 40GB A100 GPUs we reduce it to 66%. We offer a new insight, computational invariance in transformer networks, which enables SliceGPT and we hope it will inspire and enable future avenues to reduce memory and computation demands for pre-trained models. Code is available at: https://github.com/microsoft/TransformerCompression"
                },
                "metadata": [
                    {
                        "section_title": "Introduction",
                        "pdf_hash": "",
                        "start": 0,
                        "end": 1719,
                        "sentence_offsets": [
                            {
                                "start": 0,
                                "end": 120
                            },
                            {
                                "start": 121,
                                "end": 324
                            },
                            {
                                "start": 325,
                                "end": 483
                            },
                            {
                                "start": 484,
                                "end": 655
                            },
                            {
                                "start": 656,
                                "end": 1021
                            },
                            {
                                "start": 1022,
                                "end": 1281
                            },
                            {
                                "start": 1282,
                                "end": 1514
                            },
                            {
                                "start": 1515,
                                "end": 1719
                            }
                        ],
                        "ref_mentions": [
                            "246411621",
                            "240420063",
                            "259360395",
                            "208267757",
                            "267301573",
                            "259088941",
                            "259203115",
                            "255372747",
                            "268032346",
                            "264128029",
                            "263829692"
                        ],
                        "quote": "In recent times, Large Language Models (LLMs) have shown incredible performance over almost any language task (Wei et al., 2022)(Min et al., 2021)(Chang et al., 2023). However, their performance tends to grow with their sizes (i.e., the number of trainable parameters), which in turn is proportional to the computational burden required to train and then use such models. One way to reduce the computational cost of LLMs is through network pruning, i.e., algorithms that remove parameters while minimizing performance degradation. This approach has been extensively studied on Convolutional Neural Networks (CNNs) [13,25,43,(Evci et al., 2019), but nowadays the focus has shifted towards pre-trained models [41,42,22]. This shift has required a change of paradigm in pruning techniques: in fact, while in CNNs the main paradigm is iterative pruning (with re-training) [13], with pre-trained models (such as LLMs) in most cases it is not possible to fully re-train such models, because (Ashkboos et al., 2024) training data are often not accessible, and (2) full re-training would be anyway too expensive. This calls for \"exploiting\" as much as possible the information contained in a pre-trained model to obtain a performant sparse version of it, using weight's information (Jaiswal et al., 2023), activations (Sun et al., 2023)[39], or reconstruction error (Frantar et al., 2023), without the need for re-training. More recently, a new category of pruning algorithms, which we may call top-up algorithms (i.e., methods that can be applied on top of a given pruning algorithm for LLMs), has emerged, aiming at further improving pruning performance. Such approaches can be divided into two categories: those that minimize the reconstruction error [18](Xu et al., 2024)(Zhang et al., 2023), and those that impose non-uniform sparsity distribution modifying the block-wise sparsity (Yin et al., 2023)."
                    }
                ]
            },
            {
                "idx": 25,
                "key": "[276079889 | Yi et al. | 2025 | Citations: 0]",
                "snippets": "Wanda (Sun et al., 2023) introduces a pruning metric that incorporates both weight magnitudes and corresponding input activations, achieving perplexity performance comparable to SparseGPT while surpassing simple magnitude-based pruning. The RIA method (Zhang et al., 2024) builds on Wanda by considering relative weight importance, offering performance improvements at minimal additional cost. Moreover, DSnoT (Zhang et al., 2023) proposes pruning and regrowing weights based on statistical properties (e.g., mean and variance) in each pruning row, obviating the need for retraining.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {
                    "[259203115 | Sun et al. | 2023 | Citations: 439]": "As their size increases, Large Languages Models (LLMs) are natural candidates for network pruning methods: approaches that drop a subset of network weights while striving to preserve performance. Existing methods, however, require either retraining, which is rarely affordable for billion-scale LLMs, or solving a weight reconstruction problem reliant on second-order information, which may also be computationally expensive. In this paper, we introduce a novel, straightforward yet effective pruning method, termed Wanda (Pruning by Weights and activations), designed to induce sparsity in pretrained LLMs. Motivated by the recent observation of emergent large magnitude features in LLMs, our approach prunes weights with the smallest magnitudes multiplied by the corresponding input activations, on a per-output basis. Notably, Wanda requires no retraining or weight update, and the pruned LLM can be used as is. We conduct a thorough evaluation of our method Wanda on LLaMA and LLaMA-2 across various language benchmarks. Wanda significantly outperforms the established baseline of magnitude pruning and performs competitively against recent method involving intensive weight update. Code is available at https://github.com/locuslab/wanda."
                },
                "metadata": [
                    {
                        "section_title": "Related Work",
                        "pdf_hash": "",
                        "start": 0,
                        "end": 584,
                        "sentence_offsets": [
                            {
                                "start": 0,
                                "end": 236
                            },
                            {
                                "start": 237,
                                "end": 394
                            },
                            {
                                "start": 395,
                                "end": 584
                            }
                        ],
                        "ref_mentions": [
                            "259203115",
                            "271745835"
                        ],
                        "quote": "Wanda (Sun et al., 2023) introduces a pruning metric that incorporates both weight magnitudes and corresponding input activations, achieving perplexity performance comparable to SparseGPT while surpassing simple magnitude-based pruning. The RIA method (Zhang et al., 2024) builds on Wanda by considering relative weight importance, offering performance improvements at minimal additional cost. Moreover, DSnoT (Zhang et al., 2023) proposes pruning and regrowing weights based on statistical properties (e.g., mean and variance) in each pruning row, obviating the need for retraining."
                    }
                ]
            },
            {
                "idx": 26,
                "key": "[276576138 | Islam et al. | 2025 | Citations: 0]",
                "snippets": "Methods like SparseGPT (Frantar et al., 2023) and Wanda (Sun et al., 2023) use sophisticated weight updates and pruning without retraining, respectively, while PST (Li et al., 2022) combines unstructured pruning with efficient fine-tuning.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {
                    "[255372747 | Frantar et al. | 2023 | Citations: 734]": "We show for the first time that large-scale generative pretrained transformer (GPT) family models can be pruned to at least 50% sparsity in one-shot, without any retraining, at minimal loss of accuracy. This is achieved via a new pruning method called SparseGPT, specifically designed to work efficiently and accurately on massive GPT-family models. We can execute SparseGPT on the largest available open-source models, OPT-175B and BLOOM-176B, in under 4.5 hours, and can reach 60% unstructured sparsity with negligible increase in perplexity: remarkably, more than 100 billion weights from these models can be ignored at inference time. SparseGPT generalizes to semi-structured (2:4 and 4:8) patterns, and is compatible with weight quantization approaches. The code is available at: https://github.com/IST-DASLab/sparsegpt."
                },
                "metadata": [
                    {
                        "section_title": "Related Work",
                        "pdf_hash": "",
                        "start": 0,
                        "end": 245,
                        "sentence_offsets": [
                            {
                                "start": 0,
                                "end": 245
                            }
                        ],
                        "ref_mentions": [
                            "255372747"
                        ],
                        "quote": "Methods like SparseGPT (Frantar et al., 2023) and Wanda (Sun et al., 2023) use sophisticated weight updates and pruning without retraining, respectively, while PST (Li et al., 2022) combines unstructured pruning with efficient fine-tuning."
                    }
                ]
            },
            {
                "idx": 27,
                "key": "[276774084 | Ding et al. | 2025 | Citations: 2]",
                "snippets": "In this study, we employed three state-of-the-art posttraining pruning methods: SparseGPT (Frantar et al., 2023), Wanda (Sun et al., 2023), and RIA (Zhang et al., 2024). These methods are designed to reduce the size of large language models (LLMs) while maintaining their performance across various tasks. Even though all three methods prune the model using a combination of weights W and activations X, they each use different metrics M and strategies to decide which parameters to prune.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {
                    "[255372747 | Frantar et al. | 2023 | Citations: 734]": "We show for the first time that large-scale generative pretrained transformer (GPT) family models can be pruned to at least 50% sparsity in one-shot, without any retraining, at minimal loss of accuracy. This is achieved via a new pruning method called SparseGPT, specifically designed to work efficiently and accurately on massive GPT-family models. We can execute SparseGPT on the largest available open-source models, OPT-175B and BLOOM-176B, in under 4.5 hours, and can reach 60% unstructured sparsity with negligible increase in perplexity: remarkably, more than 100 billion weights from these models can be ignored at inference time. SparseGPT generalizes to semi-structured (2:4 and 4:8) patterns, and is compatible with weight quantization approaches. The code is available at: https://github.com/IST-DASLab/sparsegpt."
                },
                "metadata": [
                    {
                        "section_title": "Pruning Methods",
                        "pdf_hash": "",
                        "start": 0,
                        "end": 493,
                        "sentence_offsets": [
                            {
                                "start": 0,
                                "end": 173
                            },
                            {
                                "start": 174,
                                "end": 309
                            },
                            {
                                "start": 310,
                                "end": 493
                            }
                        ],
                        "ref_mentions": [
                            "255372747",
                            "271745835"
                        ],
                        "quote": "In this study, we employed three state-of-the-art posttraining pruning methods: SparseGPT (Frantar et al., 2023), Wanda (Sun et al., 2023), and RIA (Zhang et al., 2024). These methods are designed to reduce the size of large language models (LLMs) while maintaining their performance across various tasks. Even though all three methods prune the model using a combination of weights W and activations X, they each use different metrics M and strategies to decide which parameters to prune."
                    }
                ]
            },
            {
                "idx": 28,
                "key": "[276928323 | Liang et al. | 2025 | Citations: 0]",
                "snippets": "This paper introduces Sparse Expert Activation Pruning (SEAP), a training-free pruning method that selectively retains task-relevant parameters to reduce inference overhead. Inspired by the clustering patterns of hidden states and activations in LLMs, SEAP identifies task-specific expert activation patterns and prunes the model while preserving task performance and enhancing computational efficiency. Experimental results demonstrate that SEAP significantly reduces computational overhead while maintaining competitive accuracy. Notably, at 50% pruning, SEAP surpasses both WandA and FLAP by over 20%, and at 20% pruning, it incurs only a 2.2% performance drop compared to the dense model.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {},
                "metadata": [
                    {
                        "section_title": "abstract",
                        "pdf_hash": "",
                        "start": 184,
                        "end": 876,
                        "sentence_offsets": [],
                        "ref_mentions": [],
                        "quote": "This paper introduces Sparse Expert Activation Pruning (SEAP), a training-free pruning method that selectively retains task-relevant parameters to reduce inference overhead. Inspired by the clustering patterns of hidden states and activations in LLMs, SEAP identifies task-specific expert activation patterns and prunes the model while preserving task performance and enhancing computational efficiency. Experimental results demonstrate that SEAP significantly reduces computational overhead while maintaining competitive accuracy. Notably, at 50% pruning, SEAP surpasses both WandA and FLAP by over 20%, and at 20% pruning, it incurs only a 2.2% performance drop compared to the dense model."
                    }
                ]
            },
            {
                "idx": 29,
                "key": "[277043299 | Xu et al. | 2025 | Citations: 1]",
                "snippets": "Recent works have introduced innovative approaches in this domain. SparseGPT (Frantar et al., 2023) formalizes the problem of pruning LLMs by solving a local layer-wise reconstruction problem, where their pruning metric and weight update procedure is inspired from Optimal Brain Surgeon (OBS) (Hassibi et al., 1993). Wanda (Kwon et al., 2022) streamlines the process by simplifying SparseGPT's (Frantar et al., 2023) methodology, and explores the weight magnitude and activations as a criterion for pruning, offering a simple yet effective strategy to achieve high sparsity ratios. RIA [8] also focuses on metrics related to weights and activations but introduces channel permutation (Pool et al., 2021) to maximize the retention of important weights under N:M sparsity. Pruner Zero [9] employs evolutionary algorithms to discover optimal pruning metrics, providing a comprehensive framework for metric exploration.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {
                    "[248266822 | Kwon et al. | 2022 | Citations: 154]": "Pruning is an effective way to reduce the huge inference cost of Transformer models. However, prior work on pruning Transformers requires retraining the models. This can add high training cost and high complexity to model deployment, making it difficult to use in many practical situations. To address this, we propose a fast post-training pruning framework for Transformers that does not require any retraining. Given a resource constraint and a sample dataset, our framework automatically prunes the Transformer model using structured sparsity methods. To retain high accuracy without retraining, we introduce three novel techniques: (i) a lightweight mask search algorithm that finds which heads and filters to prune based on the Fisher information; (ii) mask rearrangement that complements the search algorithm; and (iii) mask tuning that reconstructs the output activations for each layer. We apply our method to BERT-base and DistilBERT, and we evaluate its effectiveness on GLUE and SQuAD benchmarks. Our framework achieves up to 2.0x reduction in FLOPs and 1.56x speedup in inference latency, while maintaining<1% loss in accuracy. Importantly, our framework prunes Transformers in less than 3 minutes on a single GPU, which is over two orders of magnitude faster than existing pruning approaches that retrain the models.",
                    "[255372747 | Frantar et al. | 2023 | Citations: 734]": "We show for the first time that large-scale generative pretrained transformer (GPT) family models can be pruned to at least 50% sparsity in one-shot, without any retraining, at minimal loss of accuracy. This is achieved via a new pruning method called SparseGPT, specifically designed to work efficiently and accurately on massive GPT-family models. We can execute SparseGPT on the largest available open-source models, OPT-175B and BLOOM-176B, in under 4.5 hours, and can reach 60% unstructured sparsity with negligible increase in perplexity: remarkably, more than 100 billion weights from these models can be ignored at inference time. SparseGPT generalizes to semi-structured (2:4 and 4:8) patterns, and is compatible with weight quantization approaches. The code is available at: https://github.com/IST-DASLab/sparsegpt."
                },
                "metadata": [
                    {
                        "section_title": "Language Model Pruning",
                        "pdf_hash": "",
                        "start": 107,
                        "end": 935,
                        "sentence_offsets": [
                            {
                                "start": 107,
                                "end": 173
                            },
                            {
                                "start": 174,
                                "end": 385
                            },
                            {
                                "start": 386,
                                "end": 616
                            },
                            {
                                "start": 617,
                                "end": 790
                            },
                            {
                                "start": 791,
                                "end": 935
                            }
                        ],
                        "ref_mentions": [
                            "255372747",
                            "61815367",
                            "248266822",
                            "255372747",
                            "245002847"
                        ],
                        "quote": "Recent works have introduced innovative approaches in this domain. SparseGPT (Frantar et al., 2023) formalizes the problem of pruning LLMs by solving a local layer-wise reconstruction problem, where their pruning metric and weight update procedure is inspired from Optimal Brain Surgeon (OBS) (Hassibi et al., 1993). Wanda (Kwon et al., 2022) streamlines the process by simplifying SparseGPT's (Frantar et al., 2023) methodology, and explores the weight magnitude and activations as a criterion for pruning, offering a simple yet effective strategy to achieve high sparsity ratios. RIA [8] also focuses on metrics related to weights and activations but introduces channel permutation (Pool et al., 2021) to maximize the retention of important weights under N:M sparsity. Pruner Zero [9] employs evolutionary algorithms to discover optimal pruning metrics, providing a comprehensive framework for metric exploration."
                    }
                ]
            },
            {
                "idx": 30,
                "key": "[277435040 | Mecke et al. | 2025 | Citations: 0]",
                "snippets": "Recently, Large Language Models (LLMs) have become very widespread and are used to solve a wide variety of tasks. To successfully handle these tasks, LLMs require longer training times and larger model sizes. This makes LLMs ideal candidates for pruning methods that reduce computational demands while maintaining performance. Previous methods require a retraining phase after pruning to maintain the original model's performance. However, state-of-the-art pruning methods, such as Wanda, prune the model without retraining, making the pruning process faster and more efficient.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {},
                "metadata": [
                    {
                        "section_title": "abstract",
                        "pdf_hash": "",
                        "start": 0,
                        "end": 578,
                        "sentence_offsets": [],
                        "ref_mentions": [],
                        "quote": "Recently, Large Language Models (LLMs) have become very widespread and are used to solve a wide variety of tasks. To successfully handle these tasks, LLMs require longer training times and larger model sizes. This makes LLMs ideal candidates for pruning methods that reduce computational demands while maintaining performance. Previous methods require a retraining phase after pruning to maintain the original model's performance. However, state-of-the-art pruning methods, such as Wanda, prune the model without retraining, making the pruning process faster and more efficient."
                    }
                ]
            },
            {
                "idx": 31,
                "key": "[277452419 | Ma et al. | 2025 | Citations: 1]",
                "snippets": "Unstructured pruning is an optimization technique that achieves model sparsity by evaluating the importance of individual weights. Its flexibility and high compression rates make it a key method for optimizing large language models (LLMs). Unstructured pruning can achieve extremely high compression rates; for instance, Wanda achieves a 60% sparsity rate on LLaMA-7B with minimal performance degradation across multiple downstream tasks (Sun et al., 2023), while Flash-LLM achieves a 70% sparsity rate on OPT-175B, significantly reducing storage requirements with less than 2% performance degradation during inference [28]...SparseGPT (Frantar et al., 2023), on the other hand, introduces a diagonal Hessian approximation to assess the impact of weights on errors, enabling more precise pruning at the cost of high computational complexity and hardware resource requirements. Wanda (Sun et al., 2023) simplifies the SparseGPT algorithm by eliminating the need for Hessian approximations and instead computes pruning metrics by multiplying weights with input activations. This simplification significantly reduces computational complexity while achieving a balance between high accuracy and efficiency. Following this approach, many subsequent methods use SparseGPT and Wanda as baselines or build upon their foundations. RIA (Zhang et al., 2024) introduces a post-training pruning method that re-evaluates the importance of each weight element based on all input and output connections. ADMM (Boza, 2024) builds on SparseGPT by incorporating the Alternating Direction Method of Multipliers (ADMM) to restore model performance after pruning, using a simple iterative mask selection process for pruning. OWL [32] integrates both Wanda and SparseGPT, proposing the OWL metric to allocate varying pruning rates across different layers.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {
                    "[259203115 | Sun et al. | 2023 | Citations: 439]": "As their size increases, Large Languages Models (LLMs) are natural candidates for network pruning methods: approaches that drop a subset of network weights while striving to preserve performance. Existing methods, however, require either retraining, which is rarely affordable for billion-scale LLMs, or solving a weight reconstruction problem reliant on second-order information, which may also be computationally expensive. In this paper, we introduce a novel, straightforward yet effective pruning method, termed Wanda (Pruning by Weights and activations), designed to induce sparsity in pretrained LLMs. Motivated by the recent observation of emergent large magnitude features in LLMs, our approach prunes weights with the smallest magnitudes multiplied by the corresponding input activations, on a per-output basis. Notably, Wanda requires no retraining or weight update, and the pruned LLM can be used as is. We conduct a thorough evaluation of our method Wanda on LLaMA and LLaMA-2 across various language benchmarks. Wanda significantly outperforms the established baseline of magnitude pruning and performs competitively against recent method involving intensive weight update. Code is available at https://github.com/locuslab/wanda.",
                    "[255372747 | Frantar et al. | 2023 | Citations: 734]": "We show for the first time that large-scale generative pretrained transformer (GPT) family models can be pruned to at least 50% sparsity in one-shot, without any retraining, at minimal loss of accuracy. This is achieved via a new pruning method called SparseGPT, specifically designed to work efficiently and accurately on massive GPT-family models. We can execute SparseGPT on the largest available open-source models, OPT-175B and BLOOM-176B, in under 4.5 hours, and can reach 60% unstructured sparsity with negligible increase in perplexity: remarkably, more than 100 billion weights from these models can be ignored at inference time. SparseGPT generalizes to semi-structured (2:4 and 4:8) patterns, and is compatible with weight quantization approaches. The code is available at: https://github.com/IST-DASLab/sparsegpt.",
                    "[266818263 | Boza | 2024 | Citations: 5]": "Pruning large language models (LLMs) is a challenging task due to their enormous size. The primary difficulty is fine-tuning the model after pruning, which is needed to recover the lost performance caused by dropping weights. Recent approaches have either ignored fine-tuning entirely, focusing on efficient pruning criteria, or attempted layer-wise weight updates, preserving the behavior of each layer. However, even layer-wise weight updates can be costly for LLMs, and previous works have resorted to various approximations. In our paper, we propose a fast and effective weight update algorithm for pruned layers based on the Alternating Direction Method of Multipliers (ADMM). We further extend it with a simple gradual pruning mask selection and achieve state-of-the-art pruning performance across a wide range of LLMs. Code is available at https://github.com/fmfi-compbio/admm-pruning."
                },
                "metadata": [
                    {
                        "section_title": "Unstructured Pruning",
                        "pdf_hash": "",
                        "start": 0,
                        "end": 609,
                        "sentence_offsets": [
                            {
                                "start": 0,
                                "end": 130
                            },
                            {
                                "start": 131,
                                "end": 239
                            },
                            {
                                "start": 240,
                                "end": 610
                            }
                        ],
                        "ref_mentions": [
                            "259203115"
                        ],
                        "quote": "Unstructured pruning is an optimization technique that achieves model sparsity by evaluating the importance of individual weights. Its flexibility and high compression rates make it a key method for optimizing large language models (LLMs). Unstructured pruning can achieve extremely high compression rates; for instance, Wanda achieves a 60% sparsity rate on LLaMA-7B with minimal performance degradation across multiple downstream tasks (Sun et al., 2023), while Flash-LLM achieves a 70% sparsity rate on OPT-175B, significantly reducing storage requirements with less than 2% performance degradation during inference [28]"
                    },
                    {
                        "section_title": "Unstructured Pruning",
                        "pdf_hash": "",
                        "start": 1145,
                        "end": 2296,
                        "sentence_offsets": [
                            {
                                "start": 1145,
                                "end": 1377
                            },
                            {
                                "start": 1378,
                                "end": 1558
                            },
                            {
                                "start": 1559,
                                "end": 1689
                            },
                            {
                                "start": 1690,
                                "end": 1808
                            },
                            {
                                "start": 1809,
                                "end": 1958
                            },
                            {
                                "start": 1959,
                                "end": 2165
                            },
                            {
                                "start": 2166,
                                "end": 2295
                            }
                        ],
                        "ref_mentions": [
                            "255372747",
                            "259203115",
                            "271745835",
                            "266818263"
                        ],
                        "quote": "SparseGPT (Frantar et al., 2023), on the other hand, introduces a diagonal Hessian approximation to assess the impact of weights on errors, enabling more precise pruning at the cost of high computational complexity and hardware resource requirements. Wanda (Sun et al., 2023) simplifies the SparseGPT algorithm by eliminating the need for Hessian approximations and instead computes pruning metrics by multiplying weights with input activations. This simplification significantly reduces computational complexity while achieving a balance between high accuracy and efficiency. Following this approach, many subsequent methods use SparseGPT and Wanda as baselines or build upon their foundations. RIA (Zhang et al., 2024) introduces a post-training pruning method that re-evaluates the importance of each weight element based on all input and output connections. ADMM (Boza, 2024) builds on SparseGPT by incorporating the Alternating Direction Method of Multipliers (ADMM) to restore model performance after pruning, using a simple iterative mask selection process for pruning. OWL [32] integrates both Wanda and SparseGPT, proposing the OWL metric to allocate varying pruning rates across different layers."
                    }
                ]
            },
            {
                "idx": 32,
                "key": "[277622258 | Yang et al. | 2025 | Citations: 0]",
                "snippets": "Among structural pruning methods, layer pruning is particularly relevant. Laco (Yang et al., 2024) reduces model depth by merging adjacent layers from the topmost layer downward. ShortGPT (Men et al., 2024) prunes unimportant layers based on a cosine similarity criterion. LLMDrop (He et al., 2024) finds that attention layers are more redundant than MLP layers but also relies on cosine similarity for pruning. Different from these approaches, in this paper, we propose a more effective criterion i.e. En-tropy Increase to identify and remove unimportant layers.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {
                    "[267751181 | Yang et al. | 2024 | Citations: 63]": "Large language models (LLMs) based on transformer are witnessing a notable trend of size expansion, which brings considerable costs to both model training and inference. However, existing methods such as model quantization, knowledge distillation, and model pruning are constrained by various issues, including hardware support limitations, the need for extensive training, and alterations to the model internal structure. In this paper, we propose a concise layer-wise structured pruner called \\textit{Layer Collapse (LaCo)}, in which rear model layers collapse into a prior layer, enabling a rapid reduction in model size while preserving the model structure. Comprehensive experiments show that our method maintains an average task performance of over 80\\% at pruning ratios of 25-30\\%, significantly outperforming existing state-of-the-art structured pruning methods. We also conduct post-training experiments to confirm that the \\textit{LaCo} effectively inherits the parameters of the original model. Additionally, we perform ablation studies on various settings of \\textit{LaCo}. Finally, we discuss our motivation from the perspective of layer-wise similarity and evaluate the performance of the pruned LLMs across various pruning ratios\\footnote{\\url{https://github.com/yangyifei729/LaCo}}."
                },
                "metadata": [
                    {
                        "section_title": "Speedup Test",
                        "pdf_hash": "",
                        "start": 667,
                        "end": 1230,
                        "sentence_offsets": [
                            {
                                "start": 667,
                                "end": 740
                            },
                            {
                                "start": 741,
                                "end": 845
                            },
                            {
                                "start": 846,
                                "end": 939
                            },
                            {
                                "start": 940,
                                "end": 1078
                            },
                            {
                                "start": 1079,
                                "end": 1169
                            },
                            {
                                "start": 1170,
                                "end": 1230
                            }
                        ],
                        "ref_mentions": [
                            "267751181"
                        ],
                        "quote": "Among structural pruning methods, layer pruning is particularly relevant. Laco (Yang et al., 2024) reduces model depth by merging adjacent layers from the topmost layer downward. ShortGPT (Men et al., 2024) prunes unimportant layers based on a cosine similarity criterion. LLMDrop (He et al., 2024) finds that attention layers are more redundant than MLP layers but also relies on cosine similarity for pruning. Different from these approaches, in this paper, we propose a more effective criterion i.e. En-tropy Increase to identify and remove unimportant layers."
                    }
                ]
            },
            {
                "idx": 33,
                "key": "[278327238 | Sun et al. | 2025 | Citations: 3]",
                "snippets": "Uniform Pruning. Traditional pruning requires a round of re-training to restore performance, which poses significant challenges for LLMs. Researchers have developed pruning algorithms specifically tailored for LLM compression. For instance, (Ma et al., 2023) investigated structured sparse LLMs by applying Taylor pruning to remove entire weight rows, followed by LoRA fine-tuning [13]. In recent years, the focus has shifted toward unstructured pruning which eliminates the need for fine-tuning. SparseGPT (Frantar et al., 2023) employs the Hessian inverse for pruning, followed by weight updates to reduce reconstruction errors between dense and sparse weights. Wanda (Sun et al., 2023) introduced a criterion that incorporates weight magnitude and input activations to preserve outlier features.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {
                    "[258823276 | Ma et al. | 2023 | Citations: 440]": "Large language models (LLMs) have shown remarkable capabilities in language understanding and generation. However, such impressive capability typically comes with a substantial model size, which presents significant challenges in both the deployment, inference, and training stages. With LLM being a general-purpose task solver, we explore its compression in a task-agnostic manner, which aims to preserve the multi-task solving and language generation ability of the original LLM. One challenge to achieving this is the enormous size of the training corpus of LLM, which makes both data transfer and model post-training over-burdensome. Thus, we tackle the compression of LLMs within the bound of two constraints: being task-agnostic and minimizing the reliance on the original training dataset. Our method, named LLM-Pruner, adopts structural pruning that selectively removes non-critical coupled structures based on gradient information, maximally preserving the majority of the LLM's functionality. To this end, the performance of pruned models can be efficiently recovered through tuning techniques, LoRA, in merely 3 hours, requiring only 50K data. We validate the LLM-Pruner on three LLMs, including LLaMA, Vicuna, and ChatGLM, and demonstrate that the compressed models still exhibit satisfactory capabilities in zero-shot classification and generation. The code is available at: https://github.com/horseee/LLM-Pruner",
                    "[259203115 | Sun et al. | 2023 | Citations: 439]": "As their size increases, Large Languages Models (LLMs) are natural candidates for network pruning methods: approaches that drop a subset of network weights while striving to preserve performance. Existing methods, however, require either retraining, which is rarely affordable for billion-scale LLMs, or solving a weight reconstruction problem reliant on second-order information, which may also be computationally expensive. In this paper, we introduce a novel, straightforward yet effective pruning method, termed Wanda (Pruning by Weights and activations), designed to induce sparsity in pretrained LLMs. Motivated by the recent observation of emergent large magnitude features in LLMs, our approach prunes weights with the smallest magnitudes multiplied by the corresponding input activations, on a per-output basis. Notably, Wanda requires no retraining or weight update, and the pruned LLM can be used as is. We conduct a thorough evaluation of our method Wanda on LLaMA and LLaMA-2 across various language benchmarks. Wanda significantly outperforms the established baseline of magnitude pruning and performs competitively against recent method involving intensive weight update. Code is available at https://github.com/locuslab/wanda.",
                    "[255372747 | Frantar et al. | 2023 | Citations: 734]": "We show for the first time that large-scale generative pretrained transformer (GPT) family models can be pruned to at least 50% sparsity in one-shot, without any retraining, at minimal loss of accuracy. This is achieved via a new pruning method called SparseGPT, specifically designed to work efficiently and accurately on massive GPT-family models. We can execute SparseGPT on the largest available open-source models, OPT-175B and BLOOM-176B, in under 4.5 hours, and can reach 60% unstructured sparsity with negligible increase in perplexity: remarkably, more than 100 billion weights from these models can be ignored at inference time. SparseGPT generalizes to semi-structured (2:4 and 4:8) patterns, and is compatible with weight quantization approaches. The code is available at: https://github.com/IST-DASLab/sparsegpt."
                },
                "metadata": [
                    {
                        "section_title": "A Extende Related Work",
                        "pdf_hash": "",
                        "start": 0,
                        "end": 751,
                        "sentence_offsets": [
                            {
                                "start": 0,
                                "end": 16
                            },
                            {
                                "start": 17,
                                "end": 137
                            },
                            {
                                "start": 138,
                                "end": 226
                            },
                            {
                                "start": 227,
                                "end": 372
                            },
                            {
                                "start": 373,
                                "end": 482
                            },
                            {
                                "start": 483,
                                "end": 630
                            },
                            {
                                "start": 631,
                                "end": 751
                            }
                        ],
                        "ref_mentions": [
                            "258823276",
                            "255372747",
                            "259203115"
                        ],
                        "quote": "Uniform Pruning. Traditional pruning requires a round of re-training to restore performance, which poses significant challenges for LLMs. Researchers have developed pruning algorithms specifically tailored for LLM compression. For instance, (Ma et al., 2023) investigated structured sparse LLMs by applying Taylor pruning to remove entire weight rows, followed by LoRA fine-tuning [13]. In recent years, the focus has shifted toward unstructured pruning which eliminates the need for fine-tuning. SparseGPT (Frantar et al., 2023) employs the Hessian inverse for pruning, followed by weight updates to reduce reconstruction errors between dense and sparse weights. Wanda (Sun et al., 2023) introduced a criterion that incorporates weight magnitude and input activations to preserve outlier features."
                    }
                ]
            }
        ]
    },
    "cluster": {
        "cost": 0.035514,
        "cot": "To effectively answer the user's query about recent task-agnostic pruning methods for LLMs requiring minimal retraining, I need to organize the quotes into coherent dimensions that build a comprehensive response.\n\nLooking at the user query, I need to cover:\n1. Recent task-agnostic pruning methods for LLMs requiring no/minimal retraining\n2. How these methods achieve efficiency gains\n3. How they maintain performance\n\nFirst, I should provide background on pruning for LLMs to help the user understand the context. Then, I need to discuss the prominent methods mentioned in the quotes (SparseGPT, Wanda, LLM-Pruner, etc.), along with how these methods work and their efficiency/performance characteristics.\n\nHere's my plan for dimensions:\n\n1. \"Introduction to LLM Pruning\" - This will be a synthesis section providing background on what pruning is, why it's needed for LLMs, and the general categories (structured vs. unstructured). This provides necessary context for understanding the specific methods.\n\n2. \"SparseGPT: One-Shot Pruning with Sparse Regression\" - This will be a synthesis section detailing the SparseGPT approach, which is prominently mentioned in the quotes as a leading method.\n\n3. \"Wanda: Weight and Activation-Based Pruning\" - This will be a synthesis section about the Wanda method, which is another major approach mentioned repeatedly in the quotes.\n\n4. \"Other Recent Task-Agnostic Pruning Methods\" - This will be a list section covering additional pruning methods mentioned in the quotes that require no or minimal retraining, such as LLM-Pruner, FLAP, GBLM-Pruner, SoBP, etc.\n\n5. \"Efficiency Gains and Performance Maintenance Techniques\" - This will be a synthesis section explaining how these methods maintain performance while achieving efficiency, covering techniques like reconstruction, compensation mechanisms, etc.\n\n6. \"Comparative Performance and Trade-offs\" - This will be a synthesis section comparing the performance of these methods and discussing their advantages and disadvantages.\n\nThe first section will be primarily background information and may not have direct quotes to support it, but it's necessary for context. The rest of the sections should cover the pruning methods and their characteristics as described in the quotes.",
        "plan": {
            "Introduction to LLM Pruning (synthesis)": [
                6,
                22,
                24,
                30,
                31
            ],
            "SparseGPT: One-Shot Pruning with Sparse Regression (synthesis)": [
                2,
                3,
                8,
                11,
                13,
                14,
                19,
                20,
                21,
                26,
                27,
                29,
                33
            ],
            "Wanda: Weight and Activation-Based Pruning (synthesis)": [
                1,
                2,
                3,
                4,
                7,
                8,
                11,
                13,
                19,
                21,
                25,
                26,
                27,
                29,
                31,
                33
            ],
            "Other Recent Task-Agnostic Pruning Methods (list)": [
                0,
                5,
                9,
                10,
                12,
                15,
                17,
                23,
                28,
                32
            ],
            "Efficiency Gains and Performance Maintenance Techniques (synthesis)": [
                0,
                5,
                15,
                23,
                24,
                25,
                29
            ],
            "Comparative Performance and Trade-offs (synthesis)": [
                4,
                7,
                11,
                16,
                18,
                20,
                25,
                27,
                28,
                31
            ]
        },
        "model": "claude-3-7-sonnet-20250219"
    },
    "summary": {
        "sections": [
            {
                "title": "Introduction to LLM Pruning",
                "tldr": "LLM pruning aims to reduce model size and computational requirements while maintaining performance. Recent task-agnostic pruning methods like SparseGPT and Wanda have emerged that can efficiently sparsify large language models without expensive retraining. (14 sources)",
                "text": "\nLarge Language Models (LLMs) have demonstrated remarkable capabilities across various domains, but their massive size presents significant challenges for deployment, inference, and training <Paper corpusId=\"273962638\" paperTitle=\"(Cunegatti et al., 2024)\" isShortName></Paper> <Paper corpusId=\"273811289\" paperTitle=\"(Yang et al., 2024)\" isShortName></Paper> <Paper corpusId=\"259360395\" paperTitle=\"(Chang et al., 2023)\" isShortName></Paper>. As these models continue to grow in scale, optimizing them becomes increasingly critical for practical applications.\n\nPruning has emerged as a promising approach to address these challenges by reducing the computational complexity and memory usage of LLMs <Paper corpusId=\"267412232\" paperTitle=\"(Wang et al., 2024)\" isShortName></Paper>. It enables models to support longer sequence lengths while maintaining desirable performance characteristics. Pruning techniques for LLMs can be broadly categorized into two types:\n\n1. **Structured pruning**: Removes entire structured units (like neurons or attention heads)\n2. **Unstructured pruning**: Eliminates individual parameters based on their importance, creating irregular sparse structures <Paper corpusId=\"267412232\" paperTitle=\"(Wang et al., 2024)\" isShortName></Paper> <Paper corpusId=\"277452419\" paperTitle=\"(Ma et al., 2025)\" isShortName></Paper>\n\nTraditional pruning approaches for neural networks often required iterative pruning with retraining <Paper corpusId=\"208267757\" paperTitle=\"(Evci et al., 2019)\" isShortName></Paper>, which becomes impractical for billion-parameter LLMs due to the enormous computational costs and limited access to training data <Paper corpusId=\"273962638\" paperTitle=\"(Cunegatti et al., 2024)\" isShortName></Paper> <Paper corpusId=\"267301573\" paperTitle=\"(Ashkboos et al., 2024)\" isShortName></Paper>.\n\nRecent advances in LLM pruning have shifted toward task-agnostic methods that require minimal or no retraining. These approaches aim to preserve the model's multi-task solving capabilities while reducing parameter count <Paper corpusId=\"258823276\" paperTitle=\"(Ma et al., 2023)\" isShortName></Paper>. Pioneering works in this area include:\n\n- **SparseGPT**: Introduced by Frantar et al., this method can compress LLMs by up to 60% with almost no performance degradation through a one-shot pruning technique that uses a diagonal Hessian approximation to assess weight importance <Paper corpusId=\"255372747\" paperTitle=\"(Frantar et al., 2023)\" isShortName></Paper> <Paper corpusId=\"277452419\" paperTitle=\"(Ma et al., 2025)\" isShortName></Paper>\n\n- **Wanda (Weights and Activations)**: Developed by Sun et al., this approach simplifies SparseGPT by eliminating Hessian approximations and instead uses the product of weight magnitudes and input activations to determine pruning importance, achieving competitive results with significantly lower computational requirements <Paper corpusId=\"259203115\" paperTitle=\"(Sun et al., 2023)\" isShortName></Paper> <Paper corpusId=\"277435040\" paperTitle=\"(Mecke et al., 2025)\" isShortName></Paper>\n\nThese innovative pruning methods have opened new possibilities for deploying efficient LLMs in resource-constrained environments, with researchers continuing to build upon these foundations to achieve even better performance-efficiency trade-offs <Paper corpusId=\"263829692\" paperTitle=\"(Yin et al., 2023)\" isShortName></Paper> <Paper corpusId=\"264128029\" paperTitle=\"(Zhang et al., 2023)\" isShortName></Paper> <Paper corpusId=\"268032346\" paperTitle=\"(Xu et al., 2024)\" isShortName></Paper>.",
                "citations": [
                    {
                        "id": "(Cunegatti et al., 2024)",
                        "snippets": [
                            "In recent times, Large Language Models (LLMs) have shown incredible performance over almost any language task (Wei et al., 2022)(Min et al., 2021)(Chang et al., 2023). However, their performance tends to grow with their sizes (i.e., the number of trainable parameters), which in turn is proportional to the computational burden required to train and then use such models. One way to reduce the computational cost of LLMs is through network pruning, i.e., algorithms that remove parameters while minimizing performance degradation. This approach has been extensively studied on Convolutional Neural Networks (CNNs) [13,25,43,(Evci et al., 2019), but nowadays the focus has shifted towards pre-trained models [41,42,22]. This shift has required a change of paradigm in pruning techniques: in fact, while in CNNs the main paradigm is iterative pruning (with re-training) [13], with pre-trained models (such as LLMs) in most cases it is not possible to fully re-train such models, because (Ashkboos et al., 2024) training data are often not accessible, and (2) full re-training would be anyway too expensive. This calls for \"exploiting\" as much as possible the information contained in a pre-trained model to obtain a performant sparse version of it, using weight's information (Jaiswal et al., 2023), activations (Sun et al., 2023)[39], or reconstruction error (Frantar et al., 2023), without the need for re-training. More recently, a new category of pruning algorithms, which we may call top-up algorithms (i.e., methods that can be applied on top of a given pruning algorithm for LLMs), has emerged, aiming at further improving pruning performance. Such approaches can be divided into two categories: those that minimize the reconstruction error [18](Xu et al., 2024)(Zhang et al., 2023), and those that impose non-uniform sparsity distribution modifying the block-wise sparsity (Yin et al., 2023)."
                        ],
                        "paper": {
                            "corpus_id": 273962638,
                            "title": "Zeroth-Order Adaptive Neuron Alignment Based Pruning without Re-Training",
                            "authors": [
                                {
                                    "authorId": "2162450849",
                                    "name": "Elia Cunegatti"
                                },
                                {
                                    "authorId": "2037391480",
                                    "name": "Leonardo Lucio Custode"
                                },
                                {
                                    "authorId": "2295670461",
                                    "name": "Giovanni Iacca"
                                }
                            ],
                            "year": 2024,
                            "venue": "arXiv.org",
                            "n_citations": 0
                        },
                        "score": 0.84375
                    },
                    {
                        "id": "(Yang et al., 2024)",
                        "snippets": [
                            "Recent advancements in large language models have underscored the need to reduce parameter sizes and latency (Ma et al., 2023). Compression techniques for language models include network pruning (Xu et al., 2021), knowledge distillation (Sun et al., 2019(Sun et al., , 2020)), quantization (Yao et al., 2022), decomposition (Hsu et al., 2022;Yuan et al., 2023;Wang et al., 2024), and methods like early exit (Xin et al., 2020)",
                            "Recent studies on LLMs (Sun et al., 2023;(Ma et al., 2023) focus on pruning linear layers, but these methods often fail to reduce computing costs without specialized hardware or libraries."
                        ],
                        "paper": {
                            "corpus_id": 273811289,
                            "title": "MoE-I\u00b2: Compressing Mixture of Experts Models through Inter-Expert Pruning and Intra-Expert Low-Rank Decomposition",
                            "authors": [
                                {
                                    "authorId": "2329224758",
                                    "name": "Cheng Yang"
                                },
                                {
                                    "authorId": "2117517225",
                                    "name": "Yang Sui"
                                },
                                {
                                    "authorId": "2196307128",
                                    "name": "Jinqi Xiao"
                                },
                                {
                                    "authorId": "2152279863",
                                    "name": "Lingyi Huang"
                                },
                                {
                                    "authorId": "2168502148",
                                    "name": "Yu Gong"
                                },
                                {
                                    "authorId": "2329727093",
                                    "name": "Yuanlin Duan"
                                },
                                {
                                    "authorId": "2297818320",
                                    "name": "Wenqi Jia"
                                },
                                {
                                    "authorId": "1471722186",
                                    "name": "Miao Yin"
                                },
                                {
                                    "authorId": "2329746797",
                                    "name": "Yu Cheng"
                                },
                                {
                                    "authorId": "2241581494",
                                    "name": "Bo Yuan"
                                }
                            ],
                            "year": 2024,
                            "venue": "Conference on Empirical Methods in Natural Language Processing",
                            "n_citations": 7
                        },
                        "score": 0.82470703125
                    },
                    {
                        "id": "(Chang et al., 2023)",
                        "snippets": [
                            "Large language models (LLMs) are gaining increasing popularity in both academia and industry, owing to their unprecedented performance in various applications. As LLMs continue to play a vital role in both research and daily use, their evaluation becomes increasingly critical, not only at the task level, but also at the society level for better understanding of their potential risks. Over the past years, significant efforts have been made to examine LLMs from various perspectives. This paper presents a comprehensive review of these evaluation methods for LLMs, focusing on three key dimensions: what to evaluate, where to evaluate, and how to evaluate. Firstly, we provide an overview from the perspective of evaluation tasks, encompassing general natural language processing tasks, reasoning, medical usage, ethics, education, natural and social sciences, agent applications, and other areas. Secondly, we answer the \u2018where\u2019 and \u2018how\u2019 questions by diving into the evaluation methods and benchmarks, which serve as crucial components in assessing the performance of LLMs. Then, we summarize the success and failure cases of LLMs in different tasks. Finally, we shed light on several future challenges that lie ahead in LLMs evaluation. Our aim is to offer invaluable insights to researchers in the realm of LLMs evaluation, thereby aiding the development of more proficient LLMs. Our key point is that evaluation should be treated as an essential discipline to better assist the development of LLMs. We consistently maintain the related open-source materials at: https://github.com/MLGroupJLU/LLM-eval-survey"
                        ],
                        "paper": {
                            "corpus_id": 259360395,
                            "title": "A Survey on Evaluation of Large Language Models",
                            "authors": [
                                {
                                    "authorId": "2140050490",
                                    "name": "Yu-Chu Chang"
                                },
                                {
                                    "authorId": "2206577797",
                                    "name": "Xu Wang"
                                },
                                {
                                    "authorId": "1519290245",
                                    "name": "Jindong Wang"
                                },
                                {
                                    "authorId": "48608007",
                                    "name": "Yuan Wu"
                                },
                                {
                                    "authorId": "2543684",
                                    "name": "Kaijie Zhu"
                                },
                                {
                                    "authorId": "2051536212",
                                    "name": "Hao Chen"
                                },
                                {
                                    "authorId": "2145500840",
                                    "name": "Linyi Yang"
                                },
                                {
                                    "authorId": "3393196",
                                    "name": "Xiaoyuan Yi"
                                },
                                {
                                    "authorId": "35504092",
                                    "name": "Cunxiang Wang"
                                },
                                {
                                    "authorId": "2108024279",
                                    "name": "Yidong Wang"
                                },
                                {
                                    "authorId": "2147205193",
                                    "name": "Weirong Ye"
                                },
                                {
                                    "authorId": "2211964951",
                                    "name": "Yue Zhang"
                                },
                                {
                                    "authorId": "2131636065",
                                    "name": "Yi Chang"
                                },
                                {
                                    "authorId": "2191036692",
                                    "name": "Philip S. Yu"
                                },
                                {
                                    "authorId": "2158406244",
                                    "name": "Qian Yang"
                                },
                                {
                                    "authorId": "1576441343",
                                    "name": "Xingxu Xie"
                                }
                            ],
                            "year": 2023,
                            "venue": "ACM Transactions on Intelligent Systems and Technology",
                            "n_citations": 1710
                        },
                        "score": 0
                    },
                    {
                        "id": "(Wang et al., 2024)",
                        "snippets": [
                            "Pruning can help optimize the model for deployment and make the model more efficient in terms of computation complexity and memory usage. Accordingly, pruning can be considered as an approach to enable a language model to support longer sequence length, while maintaining the desirable complexity and performance. In general, pruning a model can be categorized into structured and unstructured pruning",
                            "Unstructured pruning involves with pruning individual parameters of a model independently based on their magnitudes or importance, resulting in an irregular sparse structure. Due to the irregularity in the structure and in the memory access patterns, unstructured pruning hinders the efficiency gain that might be achieved through structured pruning, and it requires specialized software and/or hardware for efficient deployment. SparseGPT [Frantar and Alistarh, 2023] compresses LLMs with billions of parameter by as much as 60%, almost without affecting the performance of the models. However, SparseGPT heavily relies on weight updates."
                        ],
                        "paper": {
                            "corpus_id": 267412232,
                            "title": "Beyond the Limits: A Survey of Techniques to Extend the Context Length in Large Language Models",
                            "authors": [
                                {
                                    "authorId": "2108048327",
                                    "name": "Xindi Wang"
                                },
                                {
                                    "authorId": "1904419",
                                    "name": "Mahsa Salmani"
                                },
                                {
                                    "authorId": "2282534833",
                                    "name": "Parsa Omidi"
                                },
                                {
                                    "authorId": "2283447900",
                                    "name": "Xiangyu Ren"
                                },
                                {
                                    "authorId": "2066076226",
                                    "name": "Mehdi Rezagholizadeh"
                                },
                                {
                                    "authorId": "50782111",
                                    "name": "A. Eshaghi"
                                }
                            ],
                            "year": 2024,
                            "venue": "International Joint Conference on Artificial Intelligence",
                            "n_citations": 45
                        },
                        "score": 0.80078125
                    },
                    {
                        "id": "(Ma et al., 2025)",
                        "snippets": [
                            "Unstructured pruning is an optimization technique that achieves model sparsity by evaluating the importance of individual weights. Its flexibility and high compression rates make it a key method for optimizing large language models (LLMs). Unstructured pruning can achieve extremely high compression rates; for instance, Wanda achieves a 60% sparsity rate on LLaMA-7B with minimal performance degradation across multiple downstream tasks (Sun et al., 2023), while Flash-LLM achieves a 70% sparsity rate on OPT-175B, significantly reducing storage requirements with less than 2% performance degradation during inference [28]",
                            "SparseGPT (Frantar et al., 2023), on the other hand, introduces a diagonal Hessian approximation to assess the impact of weights on errors, enabling more precise pruning at the cost of high computational complexity and hardware resource requirements. Wanda (Sun et al., 2023) simplifies the SparseGPT algorithm by eliminating the need for Hessian approximations and instead computes pruning metrics by multiplying weights with input activations. This simplification significantly reduces computational complexity while achieving a balance between high accuracy and efficiency. Following this approach, many subsequent methods use SparseGPT and Wanda as baselines or build upon their foundations. RIA (Zhang et al., 2024) introduces a post-training pruning method that re-evaluates the importance of each weight element based on all input and output connections. ADMM (Boza, 2024) builds on SparseGPT by incorporating the Alternating Direction Method of Multipliers (ADMM) to restore model performance after pruning, using a simple iterative mask selection process for pruning. OWL [32] integrates both Wanda and SparseGPT, proposing the OWL metric to allocate varying pruning rates across different layers."
                        ],
                        "paper": {
                            "corpus_id": 277452419,
                            "title": "Model Hemorrhage and the Robustness Limits of Large Language Models",
                            "authors": [
                                {
                                    "authorId": "2352948034",
                                    "name": "Ziyang Ma"
                                },
                                {
                                    "authorId": "2274202084",
                                    "name": "Zuchao Li"
                                },
                                {
                                    "authorId": "2269488794",
                                    "name": "Lefei Zhang"
                                },
                                {
                                    "authorId": "2343636012",
                                    "name": "Gui-Song Xia"
                                },
                                {
                                    "authorId": "2306994733",
                                    "name": "Bo Du"
                                },
                                {
                                    "authorId": "2268745050",
                                    "name": "Liangpei Zhang"
                                },
                                {
                                    "authorId": "2275194788",
                                    "name": "Dacheng Tao"
                                }
                            ],
                            "year": 2025,
                            "venue": "arXiv.org",
                            "n_citations": 1
                        },
                        "score": 0.87548828125
                    },
                    {
                        "id": "(Evci et al., 2019)",
                        "snippets": [
                            "Many applications require sparse neural networks due to space or inference time restrictions. There is a large body of work on training dense networks to yield sparse networks for inference, but this limits the size of the largest trainable sparse model to that of the largest trainable dense model. In this paper we introduce a method to train sparse neural networks with a fixed parameter count and a fixed computational cost throughout training, without sacrificing accuracy relative to existing dense-to-sparse training methods. Our method updates the topology of the sparse network during training by using parameter magnitudes and infrequent gradient calculations. We show that this approach requires fewer floating-point operations (FLOPs) to achieve a given level of accuracy compared to prior techniques. We demonstrate state-of-the-art sparse training results on a variety of networks and datasets, including ResNet-50, MobileNets on Imagenet-2012, and RNNs on WikiText-103. Finally, we provide some insights into why allowing the topology to change during the optimization can overcome local minima encountered when the topology remains static. Code used in our work can be found in this http URL."
                        ],
                        "paper": {
                            "corpus_id": 208267757,
                            "title": "Rigging the Lottery: Making All Tickets Winners",
                            "authors": [
                                {
                                    "authorId": "3399348",
                                    "name": "Utku Evci"
                                },
                                {
                                    "authorId": "2066558041",
                                    "name": "Trevor Gale"
                                },
                                {
                                    "authorId": "10698483",
                                    "name": "Jacob Menick"
                                },
                                {
                                    "authorId": "39163115",
                                    "name": "P. S. Castro"
                                },
                                {
                                    "authorId": "152585800",
                                    "name": "Erich Elsen"
                                }
                            ],
                            "year": 2019,
                            "venue": "International Conference on Machine Learning",
                            "n_citations": 608
                        },
                        "score": 0
                    },
                    {
                        "id": "(Ashkboos et al., 2024)",
                        "snippets": [
                            "Large language models have become the cornerstone of natural language processing, but their use comes with substantial costs in terms of compute and memory resources. Sparsification provides a solution to alleviate these resource constraints, and recent works have shown that trained models can be sparsified post-hoc. Existing sparsification techniques face challenges as they need additional data structures and offer constrained speedup with current hardware. In this paper we present SliceGPT, a new post-training sparsification scheme which replaces each weight matrix with a smaller (dense) matrix, reducing the embedding dimension of the network. Through extensive experimentation, we show that SliceGPT can remove up to 25% of the model parameters (including embeddings) for LLAMA2-70B, OPT 66B and Phi-2 models while maintaining 99%, 99% and 90% zero-shot task performance of the dense model respectively. Our sliced models run on fewer GPUs and run faster without any additional code optimization: on 24GB consumer GPUs we reduce the total compute for inference on LLAMA2-70B to 64% of that of the dense model; on 40GB A100 GPUs we reduce it to 66%. We offer a new insight, computational invariance in transformer networks, which enables SliceGPT and we hope it will inspire and enable future avenues to reduce memory and computation demands for pre-trained models. Code is available at: https://github.com/microsoft/TransformerCompression"
                        ],
                        "paper": {
                            "corpus_id": 267301573,
                            "title": "SliceGPT: Compress Large Language Models by Deleting Rows and Columns",
                            "authors": [
                                {
                                    "authorId": "9543395",
                                    "name": "Saleh Ashkboos"
                                },
                                {
                                    "authorId": "2008063761",
                                    "name": "Maximilian L. Croci"
                                },
                                {
                                    "authorId": "2281641743",
                                    "name": "Marcelo Gennari do Nascimento"
                                },
                                {
                                    "authorId": "2258547286",
                                    "name": "Torsten Hoefler"
                                },
                                {
                                    "authorId": "2266803418",
                                    "name": "James Hensman"
                                }
                            ],
                            "year": 2024,
                            "venue": "International Conference on Learning Representations",
                            "n_citations": 184
                        },
                        "score": 0
                    },
                    {
                        "id": "(Ma et al., 2023)",
                        "snippets": [
                            "We propose a novel framework, LLM-Pruner, for the task-agnostic compression of the large language model. To the best of our knowledge, LLM-Pruner is the first framework designed for structured pruning of LLMs",
                            "Since our goal is to compress LLMs with reduced data dependency and expedited post-training, how to prune model with the minimal disruption to the origin is crucial. To accomplish this, we propose a dependency detection algorithm that identifies all the dependent structures within the model. Once the coupled structure is identified, we employ an efficient importance estimation strategy to select the optimal group for pruning under the task-agnostic setting, where the first-order information and an approximated hessian information is taken into account. Finally, a rapid recovery stage is executed to post-train the pruned model with limited data."
                        ],
                        "paper": {
                            "corpus_id": 258823276,
                            "title": "LLM-Pruner: On the Structural Pruning of Large Language Models",
                            "authors": [
                                {
                                    "authorId": "15532066",
                                    "name": "Xinyin Ma"
                                },
                                {
                                    "authorId": "150110431",
                                    "name": "Gongfan Fang"
                                },
                                {
                                    "authorId": "48631088",
                                    "name": "Xinchao Wang"
                                }
                            ],
                            "year": 2023,
                            "venue": "Neural Information Processing Systems",
                            "n_citations": 440
                        },
                        "score": 0.88037109375
                    },
                    {
                        "id": "(Frantar et al., 2023)",
                        "snippets": [
                            "We show for the first time that large-scale generative pretrained transformer (GPT) family models can be pruned to at least 50% sparsity in one-shot, without any retraining, at minimal loss of accuracy. This is achieved via a new pruning method called SparseGPT, specifically designed to work efficiently and accurately on massive GPT-family models. We can execute SparseGPT on the largest available open-source models, OPT-175B and BLOOM-176B, in under 4.5 hours, and can reach 60% unstructured sparsity with negligible increase in perplexity: remarkably, more than 100 billion weights from these models can be ignored at inference time. SparseGPT generalizes to semi-structured (2:4 and 4:8) patterns, and is compatible with weight quantization approaches. The code is available at: https://github.com/IST-DASLab/sparsegpt."
                        ],
                        "paper": {
                            "corpus_id": 255372747,
                            "title": "SparseGPT: Massive Language Models Can Be Accurately Pruned in One-Shot",
                            "authors": [
                                {
                                    "authorId": "1502248377",
                                    "name": "Elias Frantar"
                                },
                                {
                                    "authorId": "3311387",
                                    "name": "Dan Alistarh"
                                }
                            ],
                            "year": 2023,
                            "venue": "International Conference on Machine Learning",
                            "n_citations": 734
                        },
                        "score": 0
                    },
                    {
                        "id": "(Sun et al., 2023)",
                        "snippets": [
                            "In this paper, we introduce a novel, straightforward yet effective pruning method, termed Wanda (Pruning by Weights and activations), designed to induce sparsity in pretrained LLMs. Motivated by the recent observation of emergent large magnitude features in LLMs, our approach prunes weights with the smallest magnitudes multiplied by the corresponding input activations, on a per-output basis. Notably, Wanda requires no retraining or weight update, and the pruned LLM can be used as is. We conduct a thorough evaluation of our method Wanda on LLaMA and LLaMA-2 across various language benchmarks. Wanda significantly outperforms the established baseline of magnitude pruning and performs competitively against recent method involving intensive weight update."
                        ],
                        "paper": {
                            "corpus_id": 259203115,
                            "title": "A Simple and Effective Pruning Approach for Large Language Models",
                            "authors": [
                                {
                                    "authorId": "2984183",
                                    "name": "Mingjie Sun"
                                },
                                {
                                    "authorId": "2109168016",
                                    "name": "Zhuang Liu"
                                },
                                {
                                    "authorId": "25901845",
                                    "name": "Anna Bair"
                                },
                                {
                                    "authorId": "145116464",
                                    "name": "J. Z. Kolter"
                                }
                            ],
                            "year": 2023,
                            "venue": "International Conference on Learning Representations",
                            "n_citations": 439
                        },
                        "score": 0.88818359375
                    },
                    {
                        "id": "(Mecke et al., 2025)",
                        "snippets": [
                            "Recently, Large Language Models (LLMs) have become very widespread and are used to solve a wide variety of tasks. To successfully handle these tasks, LLMs require longer training times and larger model sizes. This makes LLMs ideal candidates for pruning methods that reduce computational demands while maintaining performance. Previous methods require a retraining phase after pruning to maintain the original model's performance. However, state-of-the-art pruning methods, such as Wanda, prune the model without retraining, making the pruning process faster and more efficient."
                        ],
                        "paper": {
                            "corpus_id": 277435040,
                            "title": "STADE: Standard Deviation as a Pruning Metric",
                            "authors": [
                                {
                                    "authorId": "2352793764",
                                    "name": "Diego Coello de Portugal Mecke"
                                },
                                {
                                    "authorId": "2352792956",
                                    "name": "Haya Alyoussef"
                                },
                                {
                                    "authorId": "2352793615",
                                    "name": "Ilia Koloiarov"
                                },
                                {
                                    "authorId": "2290075048",
                                    "name": "Maximilian Stubbemann"
                                },
                                {
                                    "authorId": "2280660731",
                                    "name": "Lars Schmidt-Thieme"
                                }
                            ],
                            "year": 2025,
                            "venue": "arXiv.org",
                            "n_citations": 0
                        },
                        "score": 0.93017578125
                    },
                    {
                        "id": "(Yin et al., 2023)",
                        "snippets": [
                            "Large Language Models (LLMs), renowned for their remarkable performance across diverse domains, present a challenge when it comes to practical deployment due to their colossal model size. In response to this challenge, efforts have been directed toward the application of traditional network pruning techniques to LLMs, uncovering a massive number of parameters that can be pruned in one-shot without hurting performance. Prevailing LLM pruning strategies have consistently adhered to the practice of uniformly pruning all layers at equivalent sparsity, resulting in robust performance. However, this observation stands in contrast to the prevailing trends observed in the field of vision models, where non-uniform layerwise sparsity typically yields stronger results. To understand the underlying reasons for this disparity, we conduct a comprehensive study and discover a strong correlation with the emergence of activation outliers in LLMs. Inspired by this finding, we introduce a novel LLM pruning methodology that incorporates a tailored set of non-uniform layerwise sparsity ratios, termed as Outlier Weighed Layerwise sparsity (OWL). The sparsity ratio of OWL is proportional to the outlier ratio observed within each layer, facilitating a more effective alignment between layerwise weight sparsity and outlier ratios. Our empirical evaluation, conducted across the LLaMA-V1 family and OPT, spanning various benchmarks, demonstrates the distinct advantages offered by OWL over previous methods. For instance, OWL exhibits a remarkable performance gain, surpassing the state-of-the-art Wanda and SparseGPT by 61.22 and 6.80 perplexity at a high sparsity level of 70%, respectively, while delivering 2.6x end-to-end inference speed-up in the DeepSparse inference engine. Codes are available at https://github.com/luuyin/OWL."
                        ],
                        "paper": {
                            "corpus_id": 263829692,
                            "title": "Outlier Weighed Layerwise Sparsity (OWL): A Missing Secret Sauce for Pruning LLMs to High Sparsity",
                            "authors": [
                                {
                                    "authorId": "2254142682",
                                    "name": "Lu Yin"
                                },
                                {
                                    "authorId": "2325206905",
                                    "name": "You Wu"
                                },
                                {
                                    "authorId": "2109338656",
                                    "name": "Zhenyu (Allen) Zhang"
                                },
                                {
                                    "authorId": "2256992922",
                                    "name": "Cheng-Yu Hsieh"
                                },
                                {
                                    "authorId": "2257105674",
                                    "name": "Yaqing Wang"
                                },
                                {
                                    "authorId": "2257230381",
                                    "name": "Yiling Jia"
                                },
                                {
                                    "authorId": "1691997",
                                    "name": "Mykola Pechenizkiy"
                                },
                                {
                                    "authorId": "2260290217",
                                    "name": "Yi Liang"
                                },
                                {
                                    "authorId": "2254949434",
                                    "name": "Zhangyang Wang"
                                },
                                {
                                    "authorId": "2255081092",
                                    "name": "Shiwei Liu"
                                }
                            ],
                            "year": 2023,
                            "venue": "International Conference on Machine Learning",
                            "n_citations": 102
                        },
                        "score": 0
                    },
                    {
                        "id": "(Zhang et al., 2023)",
                        "snippets": [
                            "The ever-increasing large language models (LLMs), though opening a potential path for the upcoming artificial general intelligence, sadly drops a daunting obstacle on the way towards their on-device deployment. As one of the most well-established pre-LLMs approaches in reducing model complexity, network pruning appears to lag behind in the era of LLMs, due mostly to its costly fine-tuning (or re-training) necessity under the massive volumes of model parameter and training data. To close this industry-academia gap, we introduce Dynamic Sparse No Training (DSnoT), a training-free fine-tuning approach that slightly updates sparse LLMs without the expensive backpropagation and any weight updates. Inspired by the Dynamic Sparse Training, DSnoT minimizes the reconstruction error between the dense and sparse LLMs, in the fashion of performing iterative weight pruning-and-growing on top of sparse LLMs. To accomplish this purpose, DSnoT particularly takes into account the anticipated reduction in reconstruction error for pruning and growing, as well as the variance w.r.t. different input data for growing each weight. This practice can be executed efficiently in linear time since its obviates the need of backpropagation for fine-tuning LLMs. Extensive experiments on LLaMA-V1/V2, Vicuna, and OPT across various benchmarks demonstrate the effectiveness of DSnoT in enhancing the performance of sparse LLMs, especially at high sparsity levels. For instance, DSnoT is able to outperform the state-of-the-art Wanda by 26.79 perplexity at 70% sparsity with LLaMA-7B. Our paper offers fresh insights into how to fine-tune sparse LLMs in an efficient training-free manner and open new venues to scale the great potential of sparsity to LLMs. Codes are available at https://github.com/zyxxmu/DSnoT."
                        ],
                        "paper": {
                            "corpus_id": 264128029,
                            "title": "Dynamic Sparse No Training: Training-Free Fine-tuning for Sparse LLMs",
                            "authors": [
                                {
                                    "authorId": "2108078624",
                                    "name": "Yu-xin Zhang"
                                },
                                {
                                    "authorId": "2258678648",
                                    "name": "Lirui Zhao"
                                },
                                {
                                    "authorId": "49352079",
                                    "name": "Mingbao Lin"
                                },
                                {
                                    "authorId": "2258670567",
                                    "name": "Yunyun Sun"
                                },
                                {
                                    "authorId": "2258671504",
                                    "name": "Yiwu Yao"
                                },
                                {
                                    "authorId": "2258598205",
                                    "name": "Xingjia Han"
                                },
                                {
                                    "authorId": "2258549938",
                                    "name": "Jared Tanner"
                                },
                                {
                                    "authorId": "2258718674",
                                    "name": "Shiwei Liu"
                                },
                                {
                                    "authorId": "2258551942",
                                    "name": "Rongrong Ji"
                                }
                            ],
                            "year": 2023,
                            "venue": "International Conference on Learning Representations",
                            "n_citations": 43
                        },
                        "score": 0
                    },
                    {
                        "id": "(Xu et al., 2024)",
                        "snippets": [
                            "Existing solutions such as SparseGPT and Wanda attempt to alleviate this issue through weight pruning. However, their layer-wise approach results in significant perturbation to the model's output and requires meticulous hyperparameter tuning, such as the pruning rate, which can adversely affect overall model performance."
                        ],
                        "paper": {
                            "corpus_id": 268032346,
                            "title": "BESA: Pruning Large Language Models with Blockwise Parameter-Efficient Sparsity Allocation",
                            "authors": [
                                {
                                    "authorId": "2153917002",
                                    "name": "Peng Xu"
                                },
                                {
                                    "authorId": "2283133523",
                                    "name": "Wenqi Shao"
                                },
                                {
                                    "authorId": "2287768783",
                                    "name": "Mengzhao Chen"
                                },
                                {
                                    "authorId": "2287949030",
                                    "name": "Shitao Tang"
                                },
                                {
                                    "authorId": "2273778831",
                                    "name": "Kai-Chuang Zhang"
                                },
                                {
                                    "authorId": "2269823523",
                                    "name": "Peng Gao"
                                },
                                {
                                    "authorId": "2287838451",
                                    "name": "Fengwei An"
                                },
                                {
                                    "authorId": "2256992387",
                                    "name": "Yu Qiao"
                                },
                                {
                                    "authorId": "2253674868",
                                    "name": "Ping Luo"
                                }
                            ],
                            "year": 2024,
                            "venue": "International Conference on Learning Representations",
                            "n_citations": 32
                        },
                        "score": 0.84326171875
                    }
                ],
                "format": "synthesis",
                "table": null,
                "model": "claude-3-7-sonnet-20250219"
            },
            {
                "title": "SparseGPT: One-Shot Pruning with Sparse Regression",
                "tldr": "SparseGPT revolutionized LLM pruning by introducing a one-shot approach that treats pruning as a sparse regression problem, enabling up to 60% parameter reduction without retraining. It uses Hessian-based weight importance assessment and optimized weight updates to maintain model performance while being computationally efficient enough to prune even the largest available LLMs. (8 sources)",
                "text": "\nSparseGPT, introduced by Frantar and Alistarh in 2023, represents a groundbreaking advancement in LLM pruning techniques that eliminates the need for costly retraining <Paper corpusId=\"255372747\" paperTitle=\"(Frantar et al., 2023)\" isShortName></Paper>. This method frames pruning as an extensive sparse regression problem and employs an approximate sparse regression solver to identify unimportant weights <Paper corpusId=\"260900101\" paperTitle=\"(Zhu et al., 2023)\" isShortName></Paper>. The key innovation lies in its one-shot approach, which allows for significant parameter reduction in a single step rather than through iterative pruning cycles <Paper corpusId=\"271083368\" paperTitle=\"(Rostam et al., 2024)\" isShortName></Paper>.\n\nThe pruning process in SparseGPT follows a sophisticated workflow:\n1. It conducts a thorough model analysis to identify parameters that can be removed without significant impact <Paper corpusId=\"271083368\" paperTitle=\"(Rostam et al., 2024)\" isShortName></Paper>\n2. It uses synchronized second-order Hessian updates to assess weight importance <Paper corpusId=\"264590698\" paperTitle=\"(Chen et al., 2023)\" isShortName></Paper>\n3. It solves a layer-wise reconstruction problem to determine weight activations <Paper corpusId=\"273549773\" paperTitle=\"(Bhansali et al., 2024)\" isShortName></Paper>\n4. It performs weight updates to minimize the reconstruction error between dense and sparse weights <Paper corpusId=\"278327238\" paperTitle=\"(Sun et al., 2025)\" isShortName></Paper>\n\nWhat makes SparseGPT particularly remarkable is its ability to scale to enormous models while maintaining efficiency. It can be executed on the largest available open-source models like OPT-175B and BLOOM-176B in under 4.5 hours <Paper corpusId=\"255372747\" paperTitle=\"(Frantar et al., 2023)\" isShortName></Paper>. The approach achieves up to 60% unstructured sparsity with only negligible increases in perplexity, effectively allowing more than 100 billion weights to be ignored during inference <Paper corpusId=\"255372747\" paperTitle=\"(Frantar et al., 2023)\" isShortName></Paper> <Paper corpusId=\"268041812\" paperTitle=\"(Bai et al., 2024)\" isShortName></Paper>.\n\nAdditionally, SparseGPT demonstrates versatility across different sparsity patterns. It generalizes to semi-structured patterns (2:4 and 4:8) and is compatible with weight quantization approaches, enabling further compression beyond pruning alone <Paper corpusId=\"255372747\" paperTitle=\"(Frantar et al., 2023)\" isShortName></Paper>. This flexibility makes it adaptable to different hardware acceleration requirements.\n\nThe computational efficiency of SparseGPT comes from its design to handle memory constraints by sequentially loading transformer blocks one at a time instead of loading the entire model <Paper corpusId=\"273501976\" paperTitle=\"(Zhao et al., 2024)\" isShortName></Paper>. It also reduces data requirements by using only a small amount of calibration data, eliminating the need for retraining on massive datasets <Paper corpusId=\"273501976\" paperTitle=\"(Zhao et al., 2024)\" isShortName></Paper>.\n\nWhile SparseGPT represents a significant advancement in LLM pruning, it has inspired further refinements such as Wanda, which simplifies the approach by eliminating the computationally expensive Hessian approximations while achieving competitive results <Paper corpusId=\"260900101\" paperTitle=\"(Zhu et al., 2023)\" isShortName></Paper> <Paper corpusId=\"264590698\" paperTitle=\"(Chen et al., 2023)\" isShortName></Paper>.",
                "citations": [
                    {
                        "id": "(Frantar et al., 2023)",
                        "snippets": [
                            "We show for the first time that large-scale generative pretrained transformer (GPT) family models can be pruned to at least 50% sparsity in one-shot, without any retraining, at minimal loss of accuracy. This is achieved via a new pruning method called SparseGPT, specifically designed to work efficiently and accurately on massive GPT-family models. We can execute SparseGPT on the largest available open-source models, OPT-175B and BLOOM-176B, in under 4.5 hours, and can reach 60% unstructured sparsity with negligible increase in perplexity: remarkably, more than 100 billion weights from these models can be ignored at inference time. SparseGPT generalizes to semi-structured (2:4 and 4:8) patterns, and is compatible with weight quantization approaches. The code is available at: https://github.com/IST-DASLab/sparsegpt."
                        ],
                        "paper": {
                            "corpus_id": 255372747,
                            "title": "SparseGPT: Massive Language Models Can Be Accurately Pruned in One-Shot",
                            "authors": [
                                {
                                    "authorId": "1502248377",
                                    "name": "Elias Frantar"
                                },
                                {
                                    "authorId": "3311387",
                                    "name": "Dan Alistarh"
                                }
                            ],
                            "year": 2023,
                            "venue": "International Conference on Machine Learning",
                            "n_citations": 734
                        },
                        "score": 0
                    },
                    {
                        "id": "(Zhu et al., 2023)",
                        "snippets": [
                            "An innovative approach in this domain is SparseGPT (Frantar and Alistarh, 2023), which introduces a one-shot pruning strategy without retraining. SparseGPT frames pruning as an extensive sparse regression problem and solves it using an approximate sparse regression solver. SparseGPT achieves significant unstructured sparsity, even up to over 50% on the largest GPT models like OPT-175B and BLOOM-176B, with minimal increase in perplexity. To reduce the cost about the weight update process required by SparseGPT, Wanda (Sun et al., 2023) achieves model sparsity by pruning weights with the smallest magnitudes multiplied by the norm of the corresponding input activations, without the need for retraining or weight updates. To further minimize pruning-induced errors while upholding the desired overall sparsity level, SAMSP (Shao et al., 2023) utilizes the Hessian matrix as a metric for weight matrix sensitivity evaluation, and dynamically adjusts sparsity allocation based on sensitivity. Furthermore, DSnoT (Zhang et al., 2023) minimizes the reconstruction error between dense and sparse models through iterative weight pruning-and-growing on top of sparse LLMs to enhance LLM performance across various sparsity rates, especially at high sparsity levels."
                        ],
                        "paper": {
                            "corpus_id": 260900101,
                            "title": "A Survey on Model Compression for Large Language Models",
                            "authors": [
                                {
                                    "authorId": "2205622982",
                                    "name": "Xunyu Zhu"
                                },
                                {
                                    "authorId": "153154515",
                                    "name": "Jian Li"
                                },
                                {
                                    "authorId": "2144384857",
                                    "name": "Yong Liu"
                                },
                                {
                                    "authorId": "2112563365",
                                    "name": "Can Ma"
                                },
                                {
                                    "authorId": "2154491572",
                                    "name": "Weiping Wang"
                                }
                            ],
                            "year": 2023,
                            "venue": "Transactions of the Association for Computational Linguistics",
                            "n_citations": 229
                        },
                        "score": 0.8232421875
                    },
                    {
                        "id": "(Rostam et al., 2024)",
                        "snippets": [
                            "One-Shot Pruning: SparseGPT implements its pruning strategy through a streamlined process. First, a thorough model analysis is conducted to pinpoint parameters that can be removed without significant impact. This analysis leverages pruning criteria that assess parameter importance without requiring gradient calculations, saving on computational resources. Finally, SparseGPT employs a single step pruning approach, achieving substantial sparsity (at least 50% for massive models) in a single step. This oneshot approach significantly reduces the time and complexity compared to iterative pruning methods."
                        ],
                        "paper": {
                            "corpus_id": 271083368,
                            "title": "Achieving Peak Performance for Large Language Models: A Systematic Review",
                            "authors": [
                                {
                                    "authorId": "1470666105",
                                    "name": "Z. R. K. Rostam"
                                },
                                {
                                    "authorId": "3208184",
                                    "name": "S. Sz\u00e9n\u00e1si"
                                },
                                {
                                    "authorId": "9717627",
                                    "name": "G\u00e1bor Kert\u00e9sz"
                                }
                            ],
                            "year": 2024,
                            "venue": "IEEE Access",
                            "n_citations": 4
                        },
                        "score": 0.8583984375
                    },
                    {
                        "id": "(Chen et al., 2023)",
                        "snippets": [
                            "SparseGPT (Frantar et al., 2023)) uses a sophisticated weight update process involving synchronized secondorder Hessian updates, bypassing traditional retraining. In contrast, Wanda (Sun et al., 2023) achieves high sparsity without any retraining, simply by pruning weights with the smallest magnitudes multiplied by their corresponding input activations."
                        ],
                        "paper": {
                            "corpus_id": 264590698,
                            "title": "LoRAShear: Efficient Large Language Model Structured Pruning and Knowledge Recovery",
                            "authors": [
                                {
                                    "authorId": "2257252148",
                                    "name": "Tianyi Chen"
                                },
                                {
                                    "authorId": "2257191230",
                                    "name": "Tianyu Ding"
                                },
                                {
                                    "authorId": "2262446441",
                                    "name": "Badal Yadav"
                                },
                                {
                                    "authorId": "15623770",
                                    "name": "Ilya Zharkov"
                                },
                                {
                                    "authorId": "46225943",
                                    "name": "Luming Liang"
                                }
                            ],
                            "year": 2023,
                            "venue": "arXiv.org",
                            "n_citations": 32
                        },
                        "score": 0.8828125
                    },
                    {
                        "id": "(Bhansali et al., 2024)",
                        "snippets": [
                            "Recently, more nuanced pruning approaches have been discussed in the literature, improving over more traditional methods like magnitude pruning. Specifically, two state-of-the-art pruning methods are widely discussed in the literature-SparseGPT (Frantar et al., 2023) and Wanda (Sun et al., 2023). Whereas traditional magnitude pruning operates by pruning weights with the largest magnitude, these pruning techniques instead track weight activations, and prune weights with the lowest amount of activation. \n\nSparseGPT creates and solves a layer-wise reconstruction problem to determine the weight activations, whereas Wanda takes the product of a weight's magnitude and the norm of its associated input activations."
                        ],
                        "paper": {
                            "corpus_id": 273549773,
                            "title": "LEGO: Language Model Building Blocks",
                            "authors": [
                                {
                                    "authorId": "2164257071",
                                    "name": "Shrenik Bhansali"
                                },
                                {
                                    "authorId": "2327337534",
                                    "name": "Alwin Jin"
                                },
                                {
                                    "authorId": "2315304043",
                                    "name": "Tyler Lizzo"
                                },
                                {
                                    "authorId": "2315302093",
                                    "name": "Larry Heck"
                                }
                            ],
                            "year": 2024,
                            "venue": "arXiv.org",
                            "n_citations": 0
                        },
                        "score": 0.86962890625
                    },
                    {
                        "id": "(Sun et al., 2025)",
                        "snippets": [
                            "Uniform Pruning. Traditional pruning requires a round of re-training to restore performance, which poses significant challenges for LLMs. Researchers have developed pruning algorithms specifically tailored for LLM compression. For instance, (Ma et al., 2023) investigated structured sparse LLMs by applying Taylor pruning to remove entire weight rows, followed by LoRA fine-tuning [13]. In recent years, the focus has shifted toward unstructured pruning which eliminates the need for fine-tuning. SparseGPT (Frantar et al., 2023) employs the Hessian inverse for pruning, followed by weight updates to reduce reconstruction errors between dense and sparse weights. Wanda (Sun et al., 2023) introduced a criterion that incorporates weight magnitude and input activations to preserve outlier features."
                        ],
                        "paper": {
                            "corpus_id": 278327238,
                            "title": "Efficient Shapley Value-based Non-Uniform Pruning of Large Language Models",
                            "authors": [
                                {
                                    "authorId": "2359207803",
                                    "name": "Chuan Sun"
                                },
                                {
                                    "authorId": "2148706587",
                                    "name": "Han Yu"
                                },
                                {
                                    "authorId": "2313694394",
                                    "name": "Li-zhen Cui"
                                },
                                {
                                    "authorId": "2283747425",
                                    "name": "Xiaoxiao Li"
                                }
                            ],
                            "year": 2025,
                            "venue": "arXiv.org",
                            "n_citations": 3
                        },
                        "score": 0.82666015625
                    },
                    {
                        "id": "(Bai et al., 2024)",
                        "snippets": [
                            "SparseGPT Frantar and Alistarh [2023], an efficient unstructured pruning method for LLMs with hundreds of billions of parameters, achieves parameter reduction of up to 60% with minimal performance loss. Another approach, Wanda Sun et al. [2023], introduces a novel pruning criterion that evaluates weights by considering both magnitude and related input activations."
                        ],
                        "paper": {
                            "corpus_id": 268041812,
                            "title": "SparseLLM: Towards Global Pruning of Pre-trained Language Models",
                            "authors": [
                                {
                                    "authorId": "7583867",
                                    "name": "Guangji Bai"
                                },
                                {
                                    "authorId": "2288037157",
                                    "name": "Yijiang Li"
                                },
                                {
                                    "authorId": "2284591355",
                                    "name": "Chen Ling"
                                },
                                {
                                    "authorId": "2288023827",
                                    "name": "Kibaek Kim"
                                },
                                {
                                    "authorId": "2284637383",
                                    "name": "Liang Zhao"
                                }
                            ],
                            "year": 2024,
                            "venue": "Neural Information Processing Systems",
                            "n_citations": 11
                        },
                        "score": 0.826171875
                    },
                    {
                        "id": "(Zhao et al., 2024)",
                        "snippets": [
                            "The traditional pruning techniques, which finetune or retrain models (Li et al., 2020) on full datasets for many epochs (i.e., pruning-aware training), are too expensive for LLMs in terms of data and GPU resources. Thus, post-training pruning based on well-pre-trained models with reduced resource requirements represents a more reasonable approach for LLMs. Notably, SparseGPT (Frantar et al., 2023) is the representative post-training pruning work with outstanding performance. It reduces memory cost by sequentially loading transformer blocks, one at a time, instead of loading the whole model. Moreover, it reduces the data cost by using only a small amount of calibration data, eliminating the retraining process on massive data. Besides the optimization based SparseGPT, there are some other heuristic posttraining pruning methods such as (Sun et al., 2023;Zhang et al., 2024), achieving accuracy close to SparseGPT."
                        ],
                        "paper": {
                            "corpus_id": 273501976,
                            "title": "Pruning Foundation Models for High Accuracy without Retraining",
                            "authors": [
                                {
                                    "authorId": "2241612245",
                                    "name": "Pu Zhao"
                                },
                                {
                                    "authorId": "2327046378",
                                    "name": "Fei Sun"
                                },
                                {
                                    "authorId": "2007668856",
                                    "name": "Xuan Shen"
                                },
                                {
                                    "authorId": "2241698013",
                                    "name": "Pinrui Yu"
                                },
                                {
                                    "authorId": "32409528",
                                    "name": "Zhenglun Kong"
                                },
                                {
                                    "authorId": "2290628977",
                                    "name": "Yanzhi Wang"
                                },
                                {
                                    "authorId": "2322988586",
                                    "name": "Xue Lin"
                                }
                            ],
                            "year": 2024,
                            "venue": "Conference on Empirical Methods in Natural Language Processing",
                            "n_citations": 13
                        },
                        "score": 0.826171875
                    }
                ],
                "format": "synthesis",
                "table": null,
                "model": "claude-3-7-sonnet-20250219"
            },
            {
                "title": "Wanda: Weight and Activation-Based Pruning",
                "tldr": "Wanda simplifies SparseGPT's approach by using a straightforward metric that multiplies weight magnitudes with input activations to determine pruning importance, requiring no retraining or weight updates. It achieves competitive performance while being computationally more efficient than methods relying on Hessian approximations. (9 sources)",
                "text": "\nIntroduced by Sun et al. in 2023, Wanda (Pruning by Weights and Activations) represents a significant advancement in task-agnostic LLM pruning by offering a simpler yet effective alternative to SparseGPT <Paper corpusId=\"259203115\" paperTitle=\"(Sun et al., 2023)\" isShortName></Paper>. While SparseGPT uses computationally expensive Hessian approximations and weight updates, Wanda achieves comparable results through a more straightforward approach that requires no retraining or weight modification <Paper corpusId=\"260900101\" paperTitle=\"(Zhu et al., 2023)\" isShortName></Paper> <Paper corpusId=\"264590698\" paperTitle=\"(Chen et al., 2023)\" isShortName></Paper>.\n\nThe core innovation of Wanda lies in its pruning criterion, which combines two key factors:\n1. The magnitude of the weight itself\n2. The norm of the corresponding input activations\n\nBy identifying and pruning weights with the smallest product of these two factors on a per-output basis, Wanda effectively preserves the most important connections in the network <Paper corpusId=\"259203115\" paperTitle=\"(Sun et al., 2023)\" isShortName></Paper>. This approach is motivated by recent observations of emergent large magnitude features in LLMs, which Wanda specifically aims to preserve <Paper corpusId=\"277452419\" paperTitle=\"(Ma et al., 2025)\" isShortName></Paper>.\n\nOne of Wanda's most significant advantages is its computational efficiency. Unlike SparseGPT, which requires solving a layer-wise reconstruction problem and performing weight updates, Wanda can be executed in a single forward pass through the model <Paper corpusId=\"271064490\" paperTitle=\"(Kolbeinsson et al., 2024)\" isShortName></Paper>. This efficiency makes it particularly suitable for extremely large models where computational resources are limited <Paper corpusId=\"276774084\" paperTitle=\"(Ding et al., 2025)\" isShortName></Paper>.\n\nWanda has been extensively evaluated on models like LLaMA and LLaMA-2 across various language benchmarks, where it significantly outperforms traditional magnitude pruning while performing competitively against methods involving intensive weight updates <Paper corpusId=\"259203115\" paperTitle=\"(Sun et al., 2023)\" isShortName></Paper>. It can achieve up to 60% sparsity on LLaMA-7B with minimal performance degradation across multiple downstream tasks <Paper corpusId=\"277452419\" paperTitle=\"(Ma et al., 2025)\" isShortName></Paper>.\n\nThe method's success has established it as a baseline for subsequent pruning approaches, with numerous researchers building upon its foundations <Paper corpusId=\"277452419\" paperTitle=\"(Ma et al., 2025)\" isShortName></Paper>. For instance, newer methods like RIA (Zhang et al., 2024) extend Wanda by re-evaluating weight importance based on all input and output connections <Paper corpusId=\"276079889\" paperTitle=\"(Yi et al., 2025)\" isShortName></Paper>, while DSnoT (Zhang et al., 2023) proposes pruning and regrowing weights based on statistical properties to further enhance performance without retraining <Paper corpusId=\"276079889\" paperTitle=\"(Yi et al., 2025)\" isShortName></Paper>.\n\nDespite its simplicity, Wanda has certain limitations. Some researchers note that its layer-wise approach can result in significant perturbation to the model's output and requires careful hyperparameter tuning of pruning rates, which may adversely affect overall model performance <Paper corpusId=\"268032346\" paperTitle=\"(Xu et al., 2024)\" isShortName></Paper>. This has motivated the development of methods like GBLM-Pruner, which leverages gradients from pretrained LLMs to determine pruning importance <Paper corpusId=\"265050936\" paperTitle=\"(Das et al., 2023)\" isShortName></Paper>.",
                "citations": [
                    {
                        "id": "(Sun et al., 2023)",
                        "snippets": [
                            "In this paper, we introduce a novel, straightforward yet effective pruning method, termed Wanda (Pruning by Weights and activations), designed to induce sparsity in pretrained LLMs. Motivated by the recent observation of emergent large magnitude features in LLMs, our approach prunes weights with the smallest magnitudes multiplied by the corresponding input activations, on a per-output basis. Notably, Wanda requires no retraining or weight update, and the pruned LLM can be used as is. We conduct a thorough evaluation of our method Wanda on LLaMA and LLaMA-2 across various language benchmarks. Wanda significantly outperforms the established baseline of magnitude pruning and performs competitively against recent method involving intensive weight update."
                        ],
                        "paper": {
                            "corpus_id": 259203115,
                            "title": "A Simple and Effective Pruning Approach for Large Language Models",
                            "authors": [
                                {
                                    "authorId": "2984183",
                                    "name": "Mingjie Sun"
                                },
                                {
                                    "authorId": "2109168016",
                                    "name": "Zhuang Liu"
                                },
                                {
                                    "authorId": "25901845",
                                    "name": "Anna Bair"
                                },
                                {
                                    "authorId": "145116464",
                                    "name": "J. Z. Kolter"
                                }
                            ],
                            "year": 2023,
                            "venue": "International Conference on Learning Representations",
                            "n_citations": 439
                        },
                        "score": 0.88818359375
                    },
                    {
                        "id": "(Zhu et al., 2023)",
                        "snippets": [
                            "An innovative approach in this domain is SparseGPT (Frantar and Alistarh, 2023), which introduces a one-shot pruning strategy without retraining. SparseGPT frames pruning as an extensive sparse regression problem and solves it using an approximate sparse regression solver. SparseGPT achieves significant unstructured sparsity, even up to over 50% on the largest GPT models like OPT-175B and BLOOM-176B, with minimal increase in perplexity. To reduce the cost about the weight update process required by SparseGPT, Wanda (Sun et al., 2023) achieves model sparsity by pruning weights with the smallest magnitudes multiplied by the norm of the corresponding input activations, without the need for retraining or weight updates. To further minimize pruning-induced errors while upholding the desired overall sparsity level, SAMSP (Shao et al., 2023) utilizes the Hessian matrix as a metric for weight matrix sensitivity evaluation, and dynamically adjusts sparsity allocation based on sensitivity. Furthermore, DSnoT (Zhang et al., 2023) minimizes the reconstruction error between dense and sparse models through iterative weight pruning-and-growing on top of sparse LLMs to enhance LLM performance across various sparsity rates, especially at high sparsity levels."
                        ],
                        "paper": {
                            "corpus_id": 260900101,
                            "title": "A Survey on Model Compression for Large Language Models",
                            "authors": [
                                {
                                    "authorId": "2205622982",
                                    "name": "Xunyu Zhu"
                                },
                                {
                                    "authorId": "153154515",
                                    "name": "Jian Li"
                                },
                                {
                                    "authorId": "2144384857",
                                    "name": "Yong Liu"
                                },
                                {
                                    "authorId": "2112563365",
                                    "name": "Can Ma"
                                },
                                {
                                    "authorId": "2154491572",
                                    "name": "Weiping Wang"
                                }
                            ],
                            "year": 2023,
                            "venue": "Transactions of the Association for Computational Linguistics",
                            "n_citations": 229
                        },
                        "score": 0.8232421875
                    },
                    {
                        "id": "(Chen et al., 2023)",
                        "snippets": [
                            "SparseGPT (Frantar et al., 2023)) uses a sophisticated weight update process involving synchronized secondorder Hessian updates, bypassing traditional retraining. In contrast, Wanda (Sun et al., 2023) achieves high sparsity without any retraining, simply by pruning weights with the smallest magnitudes multiplied by their corresponding input activations."
                        ],
                        "paper": {
                            "corpus_id": 264590698,
                            "title": "LoRAShear: Efficient Large Language Model Structured Pruning and Knowledge Recovery",
                            "authors": [
                                {
                                    "authorId": "2257252148",
                                    "name": "Tianyi Chen"
                                },
                                {
                                    "authorId": "2257191230",
                                    "name": "Tianyu Ding"
                                },
                                {
                                    "authorId": "2262446441",
                                    "name": "Badal Yadav"
                                },
                                {
                                    "authorId": "15623770",
                                    "name": "Ilya Zharkov"
                                },
                                {
                                    "authorId": "46225943",
                                    "name": "Luming Liang"
                                }
                            ],
                            "year": 2023,
                            "venue": "arXiv.org",
                            "n_citations": 32
                        },
                        "score": 0.8828125
                    },
                    {
                        "id": "(Ma et al., 2025)",
                        "snippets": [
                            "Unstructured pruning is an optimization technique that achieves model sparsity by evaluating the importance of individual weights. Its flexibility and high compression rates make it a key method for optimizing large language models (LLMs). Unstructured pruning can achieve extremely high compression rates; for instance, Wanda achieves a 60% sparsity rate on LLaMA-7B with minimal performance degradation across multiple downstream tasks (Sun et al., 2023), while Flash-LLM achieves a 70% sparsity rate on OPT-175B, significantly reducing storage requirements with less than 2% performance degradation during inference [28]",
                            "SparseGPT (Frantar et al., 2023), on the other hand, introduces a diagonal Hessian approximation to assess the impact of weights on errors, enabling more precise pruning at the cost of high computational complexity and hardware resource requirements. Wanda (Sun et al., 2023) simplifies the SparseGPT algorithm by eliminating the need for Hessian approximations and instead computes pruning metrics by multiplying weights with input activations. This simplification significantly reduces computational complexity while achieving a balance between high accuracy and efficiency. Following this approach, many subsequent methods use SparseGPT and Wanda as baselines or build upon their foundations. RIA (Zhang et al., 2024) introduces a post-training pruning method that re-evaluates the importance of each weight element based on all input and output connections. ADMM (Boza, 2024) builds on SparseGPT by incorporating the Alternating Direction Method of Multipliers (ADMM) to restore model performance after pruning, using a simple iterative mask selection process for pruning. OWL [32] integrates both Wanda and SparseGPT, proposing the OWL metric to allocate varying pruning rates across different layers."
                        ],
                        "paper": {
                            "corpus_id": 277452419,
                            "title": "Model Hemorrhage and the Robustness Limits of Large Language Models",
                            "authors": [
                                {
                                    "authorId": "2352948034",
                                    "name": "Ziyang Ma"
                                },
                                {
                                    "authorId": "2274202084",
                                    "name": "Zuchao Li"
                                },
                                {
                                    "authorId": "2269488794",
                                    "name": "Lefei Zhang"
                                },
                                {
                                    "authorId": "2343636012",
                                    "name": "Gui-Song Xia"
                                },
                                {
                                    "authorId": "2306994733",
                                    "name": "Bo Du"
                                },
                                {
                                    "authorId": "2268745050",
                                    "name": "Liangpei Zhang"
                                },
                                {
                                    "authorId": "2275194788",
                                    "name": "Dacheng Tao"
                                }
                            ],
                            "year": 2025,
                            "venue": "arXiv.org",
                            "n_citations": 1
                        },
                        "score": 0.87548828125
                    },
                    {
                        "id": "(Kolbeinsson et al., 2024)",
                        "snippets": [
                            "\u2022 SparseGPT (Frantar & Alistarh, 2023): an efficient one-shot pruning method tailored for large models. It converts the pruning process into solving large-scale sparse regression problems using an approximate solver. This approach enables rapid pruning on a single GPU with minimal accuracy loss, achieving 50-60% on large models. \n\n\u2022 Wanda (Sun et al., 2023): is another popular method for pruning large language models that relies on a pruning metric that combines a weight's magnitude with the norm of its corresponding input activations, determined from a small calibration dataset. The method focuses on selectively pruning weights within individual outputs of linear layers, aiming for high sparsity levels without modifying unpruned weights. Wanda is computationally efficient, executable in a single forward pass."
                        ],
                        "paper": {
                            "corpus_id": 271064490,
                            "title": "Composable Interventions for Language Models",
                            "authors": [
                                {
                                    "authorId": "68978244",
                                    "name": "Arinbj\u00f6rn Kolbeinsson"
                                },
                                {
                                    "authorId": "2212970046",
                                    "name": "Kyle O'Brien"
                                },
                                {
                                    "authorId": "2310637497",
                                    "name": "Tianjin Huang"
                                },
                                {
                                    "authorId": "2269765109",
                                    "name": "Shanghua Gao"
                                },
                                {
                                    "authorId": "2310512874",
                                    "name": "Shiwei Liu"
                                },
                                {
                                    "authorId": "2290185444",
                                    "name": "Jonathan Richard Schwarz"
                                },
                                {
                                    "authorId": "2187496179",
                                    "name": "Anurag Vaidya"
                                },
                                {
                                    "authorId": "2310436669",
                                    "name": "Faisal Mahmood"
                                },
                                {
                                    "authorId": "2095762",
                                    "name": "M. Zitnik"
                                },
                                {
                                    "authorId": "2295593785",
                                    "name": "Tianlong Chen"
                                },
                                {
                                    "authorId": "32452740",
                                    "name": "Thomas Hartvigsen"
                                }
                            ],
                            "year": 2024,
                            "venue": "International Conference on Learning Representations",
                            "n_citations": 4
                        },
                        "score": 0.87060546875
                    },
                    {
                        "id": "(Ding et al., 2025)",
                        "snippets": [
                            "In this study, we employed three state-of-the-art posttraining pruning methods: SparseGPT (Frantar et al., 2023), Wanda (Sun et al., 2023), and RIA (Zhang et al., 2024). These methods are designed to reduce the size of large language models (LLMs) while maintaining their performance across various tasks. Even though all three methods prune the model using a combination of weights W and activations X, they each use different metrics M and strategies to decide which parameters to prune."
                        ],
                        "paper": {
                            "corpus_id": 276774084,
                            "title": "Revisiting Large Language Model Pruning using Neuron Semantic Attribution",
                            "authors": [
                                {
                                    "authorId": "2237598833",
                                    "name": "Yizhuo Ding"
                                },
                                {
                                    "authorId": "2244778967",
                                    "name": "Xinwei Sun"
                                },
                                {
                                    "authorId": "2244698019",
                                    "name": "Yanwei Fu"
                                },
                                {
                                    "authorId": "2349205822",
                                    "name": "Guosheng Hu"
                                }
                            ],
                            "year": 2025,
                            "venue": "arXiv.org",
                            "n_citations": 2
                        },
                        "score": 0.9404296875
                    },
                    {
                        "id": "(Yi et al., 2025)",
                        "snippets": [
                            "Wanda (Sun et al., 2023) introduces a pruning metric that incorporates both weight magnitudes and corresponding input activations, achieving perplexity performance comparable to SparseGPT while surpassing simple magnitude-based pruning. The RIA method (Zhang et al., 2024) builds on Wanda by considering relative weight importance, offering performance improvements at minimal additional cost. Moreover, DSnoT (Zhang et al., 2023) proposes pruning and regrowing weights based on statistical properties (e.g., mean and variance) in each pruning row, obviating the need for retraining."
                        ],
                        "paper": {
                            "corpus_id": 276079889,
                            "title": "Symmetric Pruning of Large Language Models",
                            "authors": [
                                {
                                    "authorId": "2292143960",
                                    "name": "Kai Yi"
                                },
                                {
                                    "authorId": "2342412598",
                                    "name": "Peter Richt'arik"
                                }
                            ],
                            "year": 2025,
                            "venue": "arXiv.org",
                            "n_citations": 0
                        },
                        "score": 0.85400390625
                    },
                    {
                        "id": "(Xu et al., 2024)",
                        "snippets": [
                            "Existing solutions such as SparseGPT and Wanda attempt to alleviate this issue through weight pruning. However, their layer-wise approach results in significant perturbation to the model's output and requires meticulous hyperparameter tuning, such as the pruning rate, which can adversely affect overall model performance."
                        ],
                        "paper": {
                            "corpus_id": 268032346,
                            "title": "BESA: Pruning Large Language Models with Blockwise Parameter-Efficient Sparsity Allocation",
                            "authors": [
                                {
                                    "authorId": "2153917002",
                                    "name": "Peng Xu"
                                },
                                {
                                    "authorId": "2283133523",
                                    "name": "Wenqi Shao"
                                },
                                {
                                    "authorId": "2287768783",
                                    "name": "Mengzhao Chen"
                                },
                                {
                                    "authorId": "2287949030",
                                    "name": "Shitao Tang"
                                },
                                {
                                    "authorId": "2273778831",
                                    "name": "Kai-Chuang Zhang"
                                },
                                {
                                    "authorId": "2269823523",
                                    "name": "Peng Gao"
                                },
                                {
                                    "authorId": "2287838451",
                                    "name": "Fengwei An"
                                },
                                {
                                    "authorId": "2256992387",
                                    "name": "Yu Qiao"
                                },
                                {
                                    "authorId": "2253674868",
                                    "name": "Ping Luo"
                                }
                            ],
                            "year": 2024,
                            "venue": "International Conference on Learning Representations",
                            "n_citations": 32
                        },
                        "score": 0.84326171875
                    },
                    {
                        "id": "(Das et al., 2023)",
                        "snippets": [
                            "Large Language Models (LLMs) with billions of parameters are prime targets for network pruning, removing some model weights without hurting performance. Prior approaches such as magnitude pruning, SparseGPT, and Wanda, either concentrated solely on weights or integrated weights with activations for sparsity. However, they overlooked the informative gradients derived from pretrained LLMs. In this paper, we present a novel sparsity-centric pruning method for pretrained LLMs, termed Gradient-based Language Model Pruner (GBLM-Pruner). GBLM-Pruner leverages the first-order term of the Taylor expansion, operating in a training-free manner by harnessing properly normalized gradients from a few calibration samples to determine the pruning metric, and substantially outperforms competitive counterparts like SparseGPT and Wanda in multiple benchmarks."
                        ],
                        "paper": {
                            "corpus_id": 265050936,
                            "title": "Beyond Size: How Gradients Shape Pruning Decisions in Large Language Models",
                            "authors": [
                                {
                                    "authorId": "2211732585",
                                    "name": "Rocktim Jyoti Das"
                                },
                                {
                                    "authorId": "2243392466",
                                    "name": "Liqun Ma"
                                },
                                {
                                    "authorId": "2243374493",
                                    "name": "Zhiqiang Shen"
                                }
                            ],
                            "year": 2023,
                            "venue": "arXiv.org",
                            "n_citations": 19
                        },
                        "score": 0.86962890625
                    }
                ],
                "format": "synthesis",
                "table": null,
                "model": "claude-3-7-sonnet-20250219"
            },
            {
                "title": "Other Recent Task-Agnostic Pruning Methods",
                "tldr": "Beyond SparseGPT and Wanda, several innovative pruning methods have emerged for LLMs that require no or minimal retraining. These approaches include structured pruning techniques like LLM-Pruner and FLAP, as well as specialized methods targeting layer pruning, expert activation patterns, and depth reduction. (11 sources)",
                "text": "\n## LLM-Pruner\nLLM-Pruner was the first framework specifically designed for structured pruning of LLMs in a task-agnostic manner. It identifies and removes non-critical coupled structures based on gradient information to preserve the model's functionality while minimizing reliance on the original training dataset. The approach follows three main stages: dependency detection to identify dependent structures, importance estimation using first-order and approximated Hessian information, and a rapid recovery stage to fine-tune the pruned model with limited data. <Paper corpusId=\"258823276\" paperTitle=\"(Ma et al., 2023)\" isShortName></Paper> <Paper corpusId=\"269791108\" paperTitle=\"(Huang et al., 2024)\" isShortName></Paper> <Paper corpusId=\"270411995\" paperTitle=\"(Touheed et al., 2024)\" isShortName></Paper> <Paper corpusId=\"271909626\" paperTitle=\"(Dong et al., 2024)\" isShortName></Paper>\n\n## FLAP (FLuctuation-based Adaptive Structured Pruning)\nFLAP addresses the challenges of structured pruning for LLMs by introducing a novel pruning metric, adaptive global model compression strategies, and robust compensation mechanisms. This retraining-free framework maintains perplexity and zero-shot performance while providing the benefits of structured compression. <Paper corpusId=\"266362404\" paperTitle=\"(An et al., 2023)\" isShortName></Paper>\n\n## SoBP (Structured Optimal Brain Pruning)\nSoBP offers a retraining-free structured pruning method that leverages global first-order information to select pruning structures. It then refines these structures with a local greedy approach and adopts module-wise reconstruction to minimize information loss, making it suitable for practical applications without relying on specialized hardware or extensive computational resources for post-pruning fine-tuning. <Paper corpusId=\"273901152\" paperTitle=\"(Wei et al., 2024)\" isShortName></Paper>\n\n## SEAP (Sparse Expert Activation Pruning)\nInspired by clustering patterns of hidden states and activations in LLMs, SEAP identifies task-specific expert activation patterns and selectively retains task-relevant parameters. This training-free method reduces inference overhead while preserving task performance. At 50% pruning, SEAP outperforms both Wanda and FLAP by over 20%, and at 20% pruning, it experiences only a 2.2% performance drop compared to the dense model. <Paper corpusId=\"276928323\" paperTitle=\"(Liang et al., 2025)\" isShortName></Paper>\n\n## Layer Collapse (LaCo)\nLaCo focuses on reducing model depth by merging adjacent layers from the topmost layer downward. This approach enables rapid reduction in model size while preserving the model structure. LaCo maintains an average task performance of over 80% at pruning ratios of 25-30%, outperforming other structured pruning methods. <Paper corpusId=\"277622258\" paperTitle=\"(Yang et al., 2025)\" isShortName></Paper> <Paper corpusId=\"267751181\" paperTitle=\"(Yang et al._1, 2024)\" isShortName></Paper>\n\n## Depth-2 Pruning with Inference-Aware Criteria\nThis approach identifies a depth-2 pruning structure and develops inference-aware pruning criteria that outperform traditional metrics while eliminating the need for computationally expensive retraining. A two-step reconstruction technique mitigates pruning errors, ensuring superior performance across various datasets and models while significantly reducing computational costs and hardware requirements. <Paper corpusId=\"271533761\" paperTitle=\"(Li et al., 2024)\" isShortName></Paper>\n\n## Continual Pruning\nThis novel concept addresses pruning under a continual model adaptation setting, bypassing the requirement for model re-training entirely. It represents a shift toward more dynamic and adaptive pruning approaches for LLMs. <Paper corpusId=\"269605957\" paperTitle=\"(Malla et al., 2024)\" isShortName></Paper>",
                "citations": [
                    {
                        "id": "(Ma et al., 2023)",
                        "snippets": [
                            "We propose a novel framework, LLM-Pruner, for the task-agnostic compression of the large language model. To the best of our knowledge, LLM-Pruner is the first framework designed for structured pruning of LLMs",
                            "Since our goal is to compress LLMs with reduced data dependency and expedited post-training, how to prune model with the minimal disruption to the origin is crucial. To accomplish this, we propose a dependency detection algorithm that identifies all the dependent structures within the model. Once the coupled structure is identified, we employ an efficient importance estimation strategy to select the optimal group for pruning under the task-agnostic setting, where the first-order information and an approximated hessian information is taken into account. Finally, a rapid recovery stage is executed to post-train the pruned model with limited data."
                        ],
                        "paper": {
                            "corpus_id": 258823276,
                            "title": "LLM-Pruner: On the Structural Pruning of Large Language Models",
                            "authors": [
                                {
                                    "authorId": "15532066",
                                    "name": "Xinyin Ma"
                                },
                                {
                                    "authorId": "150110431",
                                    "name": "Gongfan Fang"
                                },
                                {
                                    "authorId": "48631088",
                                    "name": "Xinchao Wang"
                                }
                            ],
                            "year": 2023,
                            "venue": "Neural Information Processing Systems",
                            "n_citations": 440
                        },
                        "score": 0.88037109375
                    },
                    {
                        "id": "(Huang et al., 2024)",
                        "snippets": [
                            "Ma et al. (Ma et al., 2023) propose the LLM-Pruner approach to optimize large language models by selectively removing non-critical coupling structures based on gradient information to achieve efficient compression while preserving functionality and task agnosticism."
                        ],
                        "paper": {
                            "corpus_id": 269791108,
                            "title": "When Large Language Model Meets Optimization",
                            "authors": [
                                {
                                    "authorId": "2301544444",
                                    "name": "Sen Huang"
                                },
                                {
                                    "authorId": "2301490921",
                                    "name": "Kaixiang Yang"
                                },
                                {
                                    "authorId": "2301455526",
                                    "name": "Sheng Qi"
                                },
                                {
                                    "authorId": "2268020758",
                                    "name": "Rui Wang"
                                }
                            ],
                            "year": 2024,
                            "venue": "Swarm and Evolutionary Computation",
                            "n_citations": 12
                        },
                        "score": 0.8603515625
                    },
                    {
                        "id": "(Touheed et al., 2024)",
                        "snippets": [
                            "Ma et al. [29] proposes Large Language Model-Pruner (LLM-Pruner), a task-agnostic compression method, aiming to maintain the multi-task solving and language generation abilities of the original LLM while minimizing reliance on the extensive training dataset.LLM-Pruner adopts structural pruning, removing non-critical coupled structures based on gradient information to preserve most of the LLM's functionality.The pruned models can be efficiently recovered through tuning techniques in a short time and with minimal data.Experimental results on three LLMs, Large Language Model Meta AI (LLaMA), Vicuna, and General Language Model (ChatGLM), demonstrate that the compressed models perform well in zero-shot classification and generation tasks."
                        ],
                        "paper": {
                            "corpus_id": 270411995,
                            "title": "Applications of Pruning Methods in Natural Language Processing",
                            "authors": [
                                {
                                    "authorId": "2305959319",
                                    "name": "Marva Touheed"
                                },
                                {
                                    "authorId": "2305868456",
                                    "name": "Urooj Zubair"
                                },
                                {
                                    "authorId": "17492832",
                                    "name": "Dilshad Sabir"
                                },
                                {
                                    "authorId": "2293111925",
                                    "name": "Ali Hassan"
                                },
                                {
                                    "authorId": "2305969817",
                                    "name": "Muhammad Fasih Uddin Butt"
                                },
                                {
                                    "authorId": "1713703",
                                    "name": "Farhan Riaz"
                                },
                                {
                                    "authorId": "2305963536",
                                    "name": "Wadood Abdul"
                                },
                                {
                                    "authorId": "119778535",
                                    "name": "R. Ayub"
                                }
                            ],
                            "year": 2024,
                            "venue": "IEEE Access",
                            "n_citations": 1
                        },
                        "score": 0.8623046875
                    },
                    {
                        "id": "(Dong et al., 2024)",
                        "snippets": [
                            "In order to reduce the training duration, the model pruning technology can be used to remove unnecessary neurons in the LLMs while retaining the versatility of the pruned LLMs. In this vein, X. Ma et al. propose a task-agnostic pruner (named as, the LLM-Pruner) to preserve the capability to handle various task without requiring original training dataset and long duration  of retraining. Since the LLMs contain redundant parameters that have little/no effects on the performance of models, the LLM-Pruner reduces the scale of LLMs based on the gradient information. More specifically, the LLM-Pruner incorporates three main stages: discovery, estimation, and recovery."
                        ],
                        "paper": {
                            "corpus_id": 271909626,
                            "title": "Fine-Tuning and Deploying Large Language Models Over Edges: Issues and Approaches",
                            "authors": [
                                {
                                    "authorId": "2278396109",
                                    "name": "Yanjie Dong"
                                },
                                {
                                    "authorId": "2316525665",
                                    "name": "Xiaoyi Fan"
                                },
                                {
                                    "authorId": "2266804662",
                                    "name": "Fangxin Wang"
                                },
                                {
                                    "authorId": "2278907861",
                                    "name": "Chengming Li"
                                },
                                {
                                    "authorId": "2264958744",
                                    "name": "Victor C. M. Leung"
                                },
                                {
                                    "authorId": "1718919",
                                    "name": "Xiping Hu"
                                }
                            ],
                            "year": 2024,
                            "venue": "arXiv.org",
                            "n_citations": 5
                        },
                        "score": 0.9091796875
                    },
                    {
                        "id": "(An et al., 2023)",
                        "snippets": [
                            "In this paper, we propose FLAP (FLuctuation-based Adaptive Structured Pruning), a retraining-free structured pruning framework explicitly designed for Large Language Models (LLMs). To address the challenges posed by structured pruning, we introduce a novel structured pruning metric, employ adaptive global model compression strategies, and implement robust compensation mechanisms designed to mitigate potential performance losses. Our empirical results affirm that the structured compression model crafted by FLAP can maintain perplexity and zero-shot performance without any retraining."
                        ],
                        "paper": {
                            "corpus_id": 266362404,
                            "title": "Fluctuation-based Adaptive Structured Pruning for Large Language Models",
                            "authors": [
                                {
                                    "authorId": "2167834971",
                                    "name": "Yongqi An"
                                },
                                {
                                    "authorId": "2118489444",
                                    "name": "Xu Zhao"
                                },
                                {
                                    "authorId": "40418746",
                                    "name": "Tao Yu"
                                },
                                {
                                    "authorId": "2113727378",
                                    "name": "Ming Tang"
                                },
                                {
                                    "authorId": "2241943585",
                                    "name": "Jinqiao Wang"
                                }
                            ],
                            "year": 2023,
                            "venue": "AAAI Conference on Artificial Intelligence",
                            "n_citations": 61
                        },
                        "score": 0.88671875
                    },
                    {
                        "id": "(Wei et al., 2024)",
                        "snippets": [
                            "The massive parameters and computational demands hinder the widespread application of Large Language Models (LLMs). Network pruning provides a practical solution to this problem. However, existing pruning works for LLMs mainly focus on unstructured pruning or necessitate post-pruning fine-tuning. The former relies on special hardware to accelerate computation, while the latter may need substantial computational resources. In this paper, we introduce a retraining-free structured pruning method called SoBP (Structured Optimal Brain Pruning). It leverages global first-order information to select pruning structures, then refines them with a local greedy approach, and finally adopts module-wise reconstruction to mitigate information loss."
                        ],
                        "paper": {
                            "corpus_id": 273901152,
                            "title": "Structured Optimal Brain Pruning for Large Language Models",
                            "authors": [
                                {
                                    "authorId": "2330242586",
                                    "name": "Jiateng Wei"
                                },
                                {
                                    "authorId": "2329898504",
                                    "name": "Quan Lu"
                                },
                                {
                                    "authorId": "2329738680",
                                    "name": "Ning Jiang"
                                },
                                {
                                    "authorId": "2258340244",
                                    "name": "Siqi Li"
                                },
                                {
                                    "authorId": "2256984205",
                                    "name": "Jingyang Xiang"
                                },
                                {
                                    "authorId": "2257200295",
                                    "name": "Jun Chen"
                                },
                                {
                                    "authorId": "2257376000",
                                    "name": "Yong Liu"
                                }
                            ],
                            "year": 2024,
                            "venue": "Conference on Empirical Methods in Natural Language Processing",
                            "n_citations": 4
                        },
                        "score": 0.89794921875
                    },
                    {
                        "id": "(Liang et al., 2025)",
                        "snippets": [
                            "This paper introduces Sparse Expert Activation Pruning (SEAP), a training-free pruning method that selectively retains task-relevant parameters to reduce inference overhead. Inspired by the clustering patterns of hidden states and activations in LLMs, SEAP identifies task-specific expert activation patterns and prunes the model while preserving task performance and enhancing computational efficiency. Experimental results demonstrate that SEAP significantly reduces computational overhead while maintaining competitive accuracy. Notably, at 50% pruning, SEAP surpasses both WandA and FLAP by over 20%, and at 20% pruning, it incurs only a 2.2% performance drop compared to the dense model."
                        ],
                        "paper": {
                            "corpus_id": 276928323,
                            "title": "SEAP: Training-free Sparse Expert Activation Pruning Unlock the Brainpower of Large Language Models",
                            "authors": [
                                {
                                    "authorId": "2268495952",
                                    "name": "Xun Liang"
                                },
                                {
                                    "authorId": "2284861141",
                                    "name": "Hanyu Wang"
                                },
                                {
                                    "authorId": "2349573796",
                                    "name": "Huayi Lai"
                                },
                                {
                                    "authorId": "2268393907",
                                    "name": "Simin Niu"
                                },
                                {
                                    "authorId": "2268434524",
                                    "name": "Shichao Song"
                                },
                                {
                                    "authorId": "2303425635",
                                    "name": "Jiawei Yang"
                                },
                                {
                                    "authorId": "2326243408",
                                    "name": "Jihao Zhao"
                                },
                                {
                                    "authorId": "2268399953",
                                    "name": "Feiyu Xiong"
                                },
                                {
                                    "authorId": "2268400606",
                                    "name": "Bo Tang"
                                },
                                {
                                    "authorId": "2268429641",
                                    "name": "Zhiyu Li"
                                }
                            ],
                            "year": 2025,
                            "venue": "arXiv.org",
                            "n_citations": 0
                        },
                        "score": 0.853515625
                    },
                    {
                        "id": "(Yang et al., 2025)",
                        "snippets": [
                            "Among structural pruning methods, layer pruning is particularly relevant. Laco (Yang et al., 2024) reduces model depth by merging adjacent layers from the topmost layer downward. ShortGPT (Men et al., 2024) prunes unimportant layers based on a cosine similarity criterion. LLMDrop (He et al., 2024) finds that attention layers are more redundant than MLP layers but also relies on cosine similarity for pruning. Different from these approaches, in this paper, we propose a more effective criterion i.e. En-tropy Increase to identify and remove unimportant layers."
                        ],
                        "paper": {
                            "corpus_id": 277622258,
                            "title": "Entropy-Based Block Pruning for Efficient Large Language Models",
                            "authors": [
                                {
                                    "authorId": "2354128342",
                                    "name": "Liangwei Yang"
                                },
                                {
                                    "authorId": "48615640",
                                    "name": "Yuhui Xu"
                                },
                                {
                                    "authorId": "2286700513",
                                    "name": "Juntao Tan"
                                },
                                {
                                    "authorId": "36187119",
                                    "name": "Doyen Sahoo"
                                },
                                {
                                    "authorId": "2238207181",
                                    "name": "Silvio Savarese"
                                },
                                {
                                    "authorId": "2256976968",
                                    "name": "Caiming Xiong"
                                },
                                {
                                    "authorId": "2258793468",
                                    "name": "Huan Wang"
                                },
                                {
                                    "authorId": "71926704",
                                    "name": "Shelby Heinecke"
                                }
                            ],
                            "year": 2025,
                            "venue": "arXiv.org",
                            "n_citations": 0
                        },
                        "score": 0.810546875
                    },
                    {
                        "id": "(Yang et al._1, 2024)",
                        "snippets": [
                            "Large language models (LLMs) based on transformer are witnessing a notable trend of size expansion, which brings considerable costs to both model training and inference. However, existing methods such as model quantization, knowledge distillation, and model pruning are constrained by various issues, including hardware support limitations, the need for extensive training, and alterations to the model internal structure. In this paper, we propose a concise layer-wise structured pruner called \\textit{Layer Collapse (LaCo)}, in which rear model layers collapse into a prior layer, enabling a rapid reduction in model size while preserving the model structure. Comprehensive experiments show that our method maintains an average task performance of over 80\\% at pruning ratios of 25-30\\%, significantly outperforming existing state-of-the-art structured pruning methods. We also conduct post-training experiments to confirm that the \\textit{LaCo} effectively inherits the parameters of the original model. Additionally, we perform ablation studies on various settings of \\textit{LaCo}. Finally, we discuss our motivation from the perspective of layer-wise similarity and evaluate the performance of the pruned LLMs across various pruning ratios\\footnote{\\url{https://github.com/yangyifei729/LaCo}}."
                        ],
                        "paper": {
                            "corpus_id": 267751181,
                            "title": "LaCo: Large Language Model Pruning via Layer Collapse",
                            "authors": [
                                {
                                    "authorId": "2108989265",
                                    "name": "Yifei Yang"
                                },
                                {
                                    "authorId": "2253004027",
                                    "name": "Zouying Cao"
                                },
                                {
                                    "authorId": "2251173128",
                                    "name": "Hai Zhao"
                                }
                            ],
                            "year": 2024,
                            "venue": "Conference on Empirical Methods in Natural Language Processing",
                            "n_citations": 63
                        },
                        "score": 0
                    },
                    {
                        "id": "(Li et al., 2024)",
                        "snippets": [
                            "This paper introduces a novel approach to pruning large language models (LLMs) by identifying a depth-2 pruning structure and developing two inference-aware pruning criteria. These methods surpass traditional metrics and eliminate the need for computationally expensive retraining. Our two-step reconstruction technique further mitigates pruning errors, ensuring superior performance across various datasets and models. Overall, our approach significantly reduces computational costs and hardware requirements, offering an efficient solution for pruning colossal LLMs."
                        ],
                        "paper": {
                            "corpus_id": 271533761,
                            "title": "Greedy Output Approximation: Towards Efficient Structured Pruning for LLMs Without Retraining",
                            "authors": [
                                {
                                    "authorId": "2261150750",
                                    "name": "Jianwei Li"
                                },
                                {
                                    "authorId": "2310390224",
                                    "name": "Yijun Dong"
                                },
                                {
                                    "authorId": "2261081394",
                                    "name": "Qi Lei"
                                }
                            ],
                            "year": 2024,
                            "venue": "arXiv.org",
                            "n_citations": 6
                        },
                        "score": 0.8359375
                    },
                    {
                        "id": "(Malla et al., 2024)",
                        "snippets": [
                            "To the best of our knowledge, we are the first to introduce the concept of continual pruning that addresses pruning under a continual model adaptation setting, bypassing the requirement for model re-training."
                        ],
                        "paper": {
                            "corpus_id": 269605957,
                            "title": "COPAL: Continual Pruning in Large Language Generative Models",
                            "authors": [
                                {
                                    "authorId": "51128743",
                                    "name": "Srikanth Malla"
                                },
                                {
                                    "authorId": "2300246153",
                                    "name": "Joon Hee Choi"
                                },
                                {
                                    "authorId": "2301128189",
                                    "name": "Chiho Choi"
                                }
                            ],
                            "year": 2024,
                            "venue": "International Conference on Machine Learning",
                            "n_citations": 2
                        },
                        "score": 0.849609375
                    }
                ],
                "format": "list",
                "table": null,
                "model": "claude-3-7-sonnet-20250219"
            },
            {
                "title": "Efficiency Gains and Performance Maintenance Techniques",
                "tldr": "Recent task-agnostic pruning methods achieve efficiency gains through diverse techniques that minimize performance degradation. These approaches balance computational efficiency and accuracy maintenance through specialized pruning criteria, module-wise reconstruction, and adaptive sparsity allocation. (11 sources)",
                "text": "\nPruning LLMs without retraining presents a fundamental challenge: achieving significant efficiency gains while preserving the model's performance capabilities. Modern pruning methods address this challenge through several key techniques:\n\n## Minimizing Model Disruption\n\nEffective pruning methods aim to identify and preserve the most important connections in the network while removing redundant ones. LLM-Pruner accomplishes this through a three-stage approach that detects dependencies between model structures, estimates their importance using both first-order and approximated Hessian information, and implements a rapid recovery stage with limited data <Paper corpusId=\"258823276\" paperTitle=\"(Ma et al., 2023)\" isShortName></Paper>. This careful approach to structural preservation helps maintain model functionality despite significant parameter reduction.\n\n## Reconstruction Techniques\n\nMany pruning methods implement reconstruction strategies to minimize the output difference between the dense and sparse models. SoBP (Structured Optimal Brain Pruning) employs module-wise reconstruction to mitigate information loss, which enables effective pruning without reliance on specialized hardware or extensive post-pruning computations <Paper corpusId=\"273901152\" paperTitle=\"(Wei et al., 2024)\" isShortName></Paper>. Similarly, newer approaches like DSnoT minimize reconstruction error between dense and sparse LLMs by iteratively pruning and regrowing weights based on statistical properties, avoiding traditional backpropagation-based fine-tuning <Paper corpusId=\"276079889\" paperTitle=\"(Yi et al., 2025)\" isShortName></Paper> <Paper corpusId=\"264128029\" paperTitle=\"(Zhang et al., 2023)\" isShortName></Paper>.\n\n## Adaptive Sparsity Allocation\n\nRather than applying uniform sparsity across all layers, more sophisticated approaches implement adaptive sparsity allocation. FLAP introduces novel pruning metrics, adaptive global model compression strategies, and robust compensation mechanisms to maintain perplexity and zero-shot performance without retraining <Paper corpusId=\"266362404\" paperTitle=\"(An et al., 2023)\" isShortName></Paper>. BESA (Blockwise Parameter-Efficient Sparsity Allocation) allocates layer-specific sparsity in a differentiable manner, reducing performance degradation after pruning <Paper corpusId=\"273962638\" paperTitle=\"(Cunegatti et al., 2024)\" isShortName></Paper> <Paper corpusId=\"268032346\" paperTitle=\"(Xu et al., 2024)\" isShortName></Paper>.\n\n## Inference-Aware Criteria\n\nA critical advancement in recent pruning approaches is the development of inference-aware pruning criteria. These methods explicitly consider how pruning affects the model's inference capabilities rather than focusing solely on training-time metrics. For example, the depth-2 pruning approach with inference-aware criteria and two-step reconstruction significantly reduces computational costs and hardware requirements while maintaining superior performance across various datasets and models <Paper corpusId=\"271533761\" paperTitle=\"(Li et al., 2024)\" isShortName></Paper>.\n\n## Leveraging Activation Patterns\n\nUnderstanding emergent activation patterns in LLMs has led to improved pruning strategies. Wanda's success in using the product of weight magnitudes and input activations has inspired methods like RIA (Relative Importance Assessment), which re-evaluates weight importance based on input and output connections <Paper corpusId=\"277043299\" paperTitle=\"(Xu et al., 2025)\" isShortName></Paper>. OWL (Outlier Weighed Layerwise sparsity) incorporates non-uniform layerwise sparsity ratios proportional to the outlier ratio observed within each layer, creating more effective alignment between weight sparsity and activation patterns <Paper corpusId=\"273962638\" paperTitle=\"(Cunegatti et al., 2024)\" isShortName></Paper> <Paper corpusId=\"263829692\" paperTitle=\"(Yin et al., 2023)\" isShortName></Paper>.\n\n## Hardware Compatibility\n\nEfficient pruning methods must also consider hardware compatibility to translate parameter reduction into actual computational speedups. Some recent methods focus on creating sparse patterns that align with specific hardware acceleration capabilities, while others like SliceGPT replace weight matrices with smaller dense matrices to reduce the embedding dimension, enabling models to run on fewer GPUs without additional code optimization <Paper corpusId=\"273962638\" paperTitle=\"(Cunegatti et al., 2024)\" isShortName></Paper> <Paper corpusId=\"267301573\" paperTitle=\"(Ashkboos et al., 2024)\" isShortName></Paper>.\n\nThrough these diverse approaches, modern task-agnostic pruning methods have achieved remarkable efficiency gains while maintaining model performance, making LLMs more accessible for deployment in resource-constrained environments.",
                "citations": [
                    {
                        "id": "(Ma et al., 2023)",
                        "snippets": [
                            "We propose a novel framework, LLM-Pruner, for the task-agnostic compression of the large language model. To the best of our knowledge, LLM-Pruner is the first framework designed for structured pruning of LLMs",
                            "Since our goal is to compress LLMs with reduced data dependency and expedited post-training, how to prune model with the minimal disruption to the origin is crucial. To accomplish this, we propose a dependency detection algorithm that identifies all the dependent structures within the model. Once the coupled structure is identified, we employ an efficient importance estimation strategy to select the optimal group for pruning under the task-agnostic setting, where the first-order information and an approximated hessian information is taken into account. Finally, a rapid recovery stage is executed to post-train the pruned model with limited data."
                        ],
                        "paper": {
                            "corpus_id": 258823276,
                            "title": "LLM-Pruner: On the Structural Pruning of Large Language Models",
                            "authors": [
                                {
                                    "authorId": "15532066",
                                    "name": "Xinyin Ma"
                                },
                                {
                                    "authorId": "150110431",
                                    "name": "Gongfan Fang"
                                },
                                {
                                    "authorId": "48631088",
                                    "name": "Xinchao Wang"
                                }
                            ],
                            "year": 2023,
                            "venue": "Neural Information Processing Systems",
                            "n_citations": 440
                        },
                        "score": 0.88037109375
                    },
                    {
                        "id": "(Wei et al., 2024)",
                        "snippets": [
                            "The massive parameters and computational demands hinder the widespread application of Large Language Models (LLMs). Network pruning provides a practical solution to this problem. However, existing pruning works for LLMs mainly focus on unstructured pruning or necessitate post-pruning fine-tuning. The former relies on special hardware to accelerate computation, while the latter may need substantial computational resources. In this paper, we introduce a retraining-free structured pruning method called SoBP (Structured Optimal Brain Pruning). It leverages global first-order information to select pruning structures, then refines them with a local greedy approach, and finally adopts module-wise reconstruction to mitigate information loss."
                        ],
                        "paper": {
                            "corpus_id": 273901152,
                            "title": "Structured Optimal Brain Pruning for Large Language Models",
                            "authors": [
                                {
                                    "authorId": "2330242586",
                                    "name": "Jiateng Wei"
                                },
                                {
                                    "authorId": "2329898504",
                                    "name": "Quan Lu"
                                },
                                {
                                    "authorId": "2329738680",
                                    "name": "Ning Jiang"
                                },
                                {
                                    "authorId": "2258340244",
                                    "name": "Siqi Li"
                                },
                                {
                                    "authorId": "2256984205",
                                    "name": "Jingyang Xiang"
                                },
                                {
                                    "authorId": "2257200295",
                                    "name": "Jun Chen"
                                },
                                {
                                    "authorId": "2257376000",
                                    "name": "Yong Liu"
                                }
                            ],
                            "year": 2024,
                            "venue": "Conference on Empirical Methods in Natural Language Processing",
                            "n_citations": 4
                        },
                        "score": 0.89794921875
                    },
                    {
                        "id": "(Yi et al., 2025)",
                        "snippets": [
                            "Wanda (Sun et al., 2023) introduces a pruning metric that incorporates both weight magnitudes and corresponding input activations, achieving perplexity performance comparable to SparseGPT while surpassing simple magnitude-based pruning. The RIA method (Zhang et al., 2024) builds on Wanda by considering relative weight importance, offering performance improvements at minimal additional cost. Moreover, DSnoT (Zhang et al., 2023) proposes pruning and regrowing weights based on statistical properties (e.g., mean and variance) in each pruning row, obviating the need for retraining."
                        ],
                        "paper": {
                            "corpus_id": 276079889,
                            "title": "Symmetric Pruning of Large Language Models",
                            "authors": [
                                {
                                    "authorId": "2292143960",
                                    "name": "Kai Yi"
                                },
                                {
                                    "authorId": "2342412598",
                                    "name": "Peter Richt'arik"
                                }
                            ],
                            "year": 2025,
                            "venue": "arXiv.org",
                            "n_citations": 0
                        },
                        "score": 0.85400390625
                    },
                    {
                        "id": "(Zhang et al., 2023)",
                        "snippets": [
                            "The ever-increasing large language models (LLMs), though opening a potential path for the upcoming artificial general intelligence, sadly drops a daunting obstacle on the way towards their on-device deployment. As one of the most well-established pre-LLMs approaches in reducing model complexity, network pruning appears to lag behind in the era of LLMs, due mostly to its costly fine-tuning (or re-training) necessity under the massive volumes of model parameter and training data. To close this industry-academia gap, we introduce Dynamic Sparse No Training (DSnoT), a training-free fine-tuning approach that slightly updates sparse LLMs without the expensive backpropagation and any weight updates. Inspired by the Dynamic Sparse Training, DSnoT minimizes the reconstruction error between the dense and sparse LLMs, in the fashion of performing iterative weight pruning-and-growing on top of sparse LLMs. To accomplish this purpose, DSnoT particularly takes into account the anticipated reduction in reconstruction error for pruning and growing, as well as the variance w.r.t. different input data for growing each weight. This practice can be executed efficiently in linear time since its obviates the need of backpropagation for fine-tuning LLMs. Extensive experiments on LLaMA-V1/V2, Vicuna, and OPT across various benchmarks demonstrate the effectiveness of DSnoT in enhancing the performance of sparse LLMs, especially at high sparsity levels. For instance, DSnoT is able to outperform the state-of-the-art Wanda by 26.79 perplexity at 70% sparsity with LLaMA-7B. Our paper offers fresh insights into how to fine-tune sparse LLMs in an efficient training-free manner and open new venues to scale the great potential of sparsity to LLMs. Codes are available at https://github.com/zyxxmu/DSnoT."
                        ],
                        "paper": {
                            "corpus_id": 264128029,
                            "title": "Dynamic Sparse No Training: Training-Free Fine-tuning for Sparse LLMs",
                            "authors": [
                                {
                                    "authorId": "2108078624",
                                    "name": "Yu-xin Zhang"
                                },
                                {
                                    "authorId": "2258678648",
                                    "name": "Lirui Zhao"
                                },
                                {
                                    "authorId": "49352079",
                                    "name": "Mingbao Lin"
                                },
                                {
                                    "authorId": "2258670567",
                                    "name": "Yunyun Sun"
                                },
                                {
                                    "authorId": "2258671504",
                                    "name": "Yiwu Yao"
                                },
                                {
                                    "authorId": "2258598205",
                                    "name": "Xingjia Han"
                                },
                                {
                                    "authorId": "2258549938",
                                    "name": "Jared Tanner"
                                },
                                {
                                    "authorId": "2258718674",
                                    "name": "Shiwei Liu"
                                },
                                {
                                    "authorId": "2258551942",
                                    "name": "Rongrong Ji"
                                }
                            ],
                            "year": 2023,
                            "venue": "International Conference on Learning Representations",
                            "n_citations": 43
                        },
                        "score": 0
                    },
                    {
                        "id": "(An et al., 2023)",
                        "snippets": [
                            "In this paper, we propose FLAP (FLuctuation-based Adaptive Structured Pruning), a retraining-free structured pruning framework explicitly designed for Large Language Models (LLMs). To address the challenges posed by structured pruning, we introduce a novel structured pruning metric, employ adaptive global model compression strategies, and implement robust compensation mechanisms designed to mitigate potential performance losses. Our empirical results affirm that the structured compression model crafted by FLAP can maintain perplexity and zero-shot performance without any retraining."
                        ],
                        "paper": {
                            "corpus_id": 266362404,
                            "title": "Fluctuation-based Adaptive Structured Pruning for Large Language Models",
                            "authors": [
                                {
                                    "authorId": "2167834971",
                                    "name": "Yongqi An"
                                },
                                {
                                    "authorId": "2118489444",
                                    "name": "Xu Zhao"
                                },
                                {
                                    "authorId": "40418746",
                                    "name": "Tao Yu"
                                },
                                {
                                    "authorId": "2113727378",
                                    "name": "Ming Tang"
                                },
                                {
                                    "authorId": "2241943585",
                                    "name": "Jinqiao Wang"
                                }
                            ],
                            "year": 2023,
                            "venue": "AAAI Conference on Artificial Intelligence",
                            "n_citations": 61
                        },
                        "score": 0.88671875
                    },
                    {
                        "id": "(Cunegatti et al., 2024)",
                        "snippets": [
                            "In recent times, Large Language Models (LLMs) have shown incredible performance over almost any language task (Wei et al., 2022)(Min et al., 2021)(Chang et al., 2023). However, their performance tends to grow with their sizes (i.e., the number of trainable parameters), which in turn is proportional to the computational burden required to train and then use such models. One way to reduce the computational cost of LLMs is through network pruning, i.e., algorithms that remove parameters while minimizing performance degradation. This approach has been extensively studied on Convolutional Neural Networks (CNNs) [13,25,43,(Evci et al., 2019), but nowadays the focus has shifted towards pre-trained models [41,42,22]. This shift has required a change of paradigm in pruning techniques: in fact, while in CNNs the main paradigm is iterative pruning (with re-training) [13], with pre-trained models (such as LLMs) in most cases it is not possible to fully re-train such models, because (Ashkboos et al., 2024) training data are often not accessible, and (2) full re-training would be anyway too expensive. This calls for \"exploiting\" as much as possible the information contained in a pre-trained model to obtain a performant sparse version of it, using weight's information (Jaiswal et al., 2023), activations (Sun et al., 2023)[39], or reconstruction error (Frantar et al., 2023), without the need for re-training. More recently, a new category of pruning algorithms, which we may call top-up algorithms (i.e., methods that can be applied on top of a given pruning algorithm for LLMs), has emerged, aiming at further improving pruning performance. Such approaches can be divided into two categories: those that minimize the reconstruction error [18](Xu et al., 2024)(Zhang et al., 2023), and those that impose non-uniform sparsity distribution modifying the block-wise sparsity (Yin et al., 2023)."
                        ],
                        "paper": {
                            "corpus_id": 273962638,
                            "title": "Zeroth-Order Adaptive Neuron Alignment Based Pruning without Re-Training",
                            "authors": [
                                {
                                    "authorId": "2162450849",
                                    "name": "Elia Cunegatti"
                                },
                                {
                                    "authorId": "2037391480",
                                    "name": "Leonardo Lucio Custode"
                                },
                                {
                                    "authorId": "2295670461",
                                    "name": "Giovanni Iacca"
                                }
                            ],
                            "year": 2024,
                            "venue": "arXiv.org",
                            "n_citations": 0
                        },
                        "score": 0.84375
                    },
                    {
                        "id": "(Xu et al., 2024)",
                        "snippets": [
                            "Existing solutions such as SparseGPT and Wanda attempt to alleviate this issue through weight pruning. However, their layer-wise approach results in significant perturbation to the model's output and requires meticulous hyperparameter tuning, such as the pruning rate, which can adversely affect overall model performance."
                        ],
                        "paper": {
                            "corpus_id": 268032346,
                            "title": "BESA: Pruning Large Language Models with Blockwise Parameter-Efficient Sparsity Allocation",
                            "authors": [
                                {
                                    "authorId": "2153917002",
                                    "name": "Peng Xu"
                                },
                                {
                                    "authorId": "2283133523",
                                    "name": "Wenqi Shao"
                                },
                                {
                                    "authorId": "2287768783",
                                    "name": "Mengzhao Chen"
                                },
                                {
                                    "authorId": "2287949030",
                                    "name": "Shitao Tang"
                                },
                                {
                                    "authorId": "2273778831",
                                    "name": "Kai-Chuang Zhang"
                                },
                                {
                                    "authorId": "2269823523",
                                    "name": "Peng Gao"
                                },
                                {
                                    "authorId": "2287838451",
                                    "name": "Fengwei An"
                                },
                                {
                                    "authorId": "2256992387",
                                    "name": "Yu Qiao"
                                },
                                {
                                    "authorId": "2253674868",
                                    "name": "Ping Luo"
                                }
                            ],
                            "year": 2024,
                            "venue": "International Conference on Learning Representations",
                            "n_citations": 32
                        },
                        "score": 0.84326171875
                    },
                    {
                        "id": "(Li et al., 2024)",
                        "snippets": [
                            "This paper introduces a novel approach to pruning large language models (LLMs) by identifying a depth-2 pruning structure and developing two inference-aware pruning criteria. These methods surpass traditional metrics and eliminate the need for computationally expensive retraining. Our two-step reconstruction technique further mitigates pruning errors, ensuring superior performance across various datasets and models. Overall, our approach significantly reduces computational costs and hardware requirements, offering an efficient solution for pruning colossal LLMs."
                        ],
                        "paper": {
                            "corpus_id": 271533761,
                            "title": "Greedy Output Approximation: Towards Efficient Structured Pruning for LLMs Without Retraining",
                            "authors": [
                                {
                                    "authorId": "2261150750",
                                    "name": "Jianwei Li"
                                },
                                {
                                    "authorId": "2310390224",
                                    "name": "Yijun Dong"
                                },
                                {
                                    "authorId": "2261081394",
                                    "name": "Qi Lei"
                                }
                            ],
                            "year": 2024,
                            "venue": "arXiv.org",
                            "n_citations": 6
                        },
                        "score": 0.8359375
                    },
                    {
                        "id": "(Xu et al., 2025)",
                        "snippets": [
                            "Recent works have introduced innovative approaches in this domain. SparseGPT (Frantar et al., 2023) formalizes the problem of pruning LLMs by solving a local layer-wise reconstruction problem, where their pruning metric and weight update procedure is inspired from Optimal Brain Surgeon (OBS) (Hassibi et al., 1993). Wanda (Kwon et al., 2022) streamlines the process by simplifying SparseGPT's (Frantar et al., 2023) methodology, and explores the weight magnitude and activations as a criterion for pruning, offering a simple yet effective strategy to achieve high sparsity ratios. RIA [8] also focuses on metrics related to weights and activations but introduces channel permutation (Pool et al., 2021) to maximize the retention of important weights under N:M sparsity. Pruner Zero [9] employs evolutionary algorithms to discover optimal pruning metrics, providing a comprehensive framework for metric exploration."
                        ],
                        "paper": {
                            "corpus_id": 277043299,
                            "title": "Towards Extreme Pruning of LLMs with Plug-and-Play Mixed Sparsity",
                            "authors": [
                                {
                                    "authorId": "2350425334",
                                    "name": "Chi Xu"
                                },
                                {
                                    "authorId": "2350430287",
                                    "name": "Gefei Zhang"
                                },
                                {
                                    "authorId": "2350478667",
                                    "name": "Yantong Zhu"
                                },
                                {
                                    "authorId": "2323368873",
                                    "name": "Luca Benini"
                                },
                                {
                                    "authorId": "2282196819",
                                    "name": "Guosheng Hu"
                                },
                                {
                                    "authorId": "2323432053",
                                    "name": "Yawei Li"
                                },
                                {
                                    "authorId": "2350803789",
                                    "name": "Zhihong Zhang"
                                }
                            ],
                            "year": 2025,
                            "venue": "arXiv.org",
                            "n_citations": 1
                        },
                        "score": 0.81787109375
                    },
                    {
                        "id": "(Yin et al., 2023)",
                        "snippets": [
                            "Large Language Models (LLMs), renowned for their remarkable performance across diverse domains, present a challenge when it comes to practical deployment due to their colossal model size. In response to this challenge, efforts have been directed toward the application of traditional network pruning techniques to LLMs, uncovering a massive number of parameters that can be pruned in one-shot without hurting performance. Prevailing LLM pruning strategies have consistently adhered to the practice of uniformly pruning all layers at equivalent sparsity, resulting in robust performance. However, this observation stands in contrast to the prevailing trends observed in the field of vision models, where non-uniform layerwise sparsity typically yields stronger results. To understand the underlying reasons for this disparity, we conduct a comprehensive study and discover a strong correlation with the emergence of activation outliers in LLMs. Inspired by this finding, we introduce a novel LLM pruning methodology that incorporates a tailored set of non-uniform layerwise sparsity ratios, termed as Outlier Weighed Layerwise sparsity (OWL). The sparsity ratio of OWL is proportional to the outlier ratio observed within each layer, facilitating a more effective alignment between layerwise weight sparsity and outlier ratios. Our empirical evaluation, conducted across the LLaMA-V1 family and OPT, spanning various benchmarks, demonstrates the distinct advantages offered by OWL over previous methods. For instance, OWL exhibits a remarkable performance gain, surpassing the state-of-the-art Wanda and SparseGPT by 61.22 and 6.80 perplexity at a high sparsity level of 70%, respectively, while delivering 2.6x end-to-end inference speed-up in the DeepSparse inference engine. Codes are available at https://github.com/luuyin/OWL."
                        ],
                        "paper": {
                            "corpus_id": 263829692,
                            "title": "Outlier Weighed Layerwise Sparsity (OWL): A Missing Secret Sauce for Pruning LLMs to High Sparsity",
                            "authors": [
                                {
                                    "authorId": "2254142682",
                                    "name": "Lu Yin"
                                },
                                {
                                    "authorId": "2325206905",
                                    "name": "You Wu"
                                },
                                {
                                    "authorId": "2109338656",
                                    "name": "Zhenyu (Allen) Zhang"
                                },
                                {
                                    "authorId": "2256992922",
                                    "name": "Cheng-Yu Hsieh"
                                },
                                {
                                    "authorId": "2257105674",
                                    "name": "Yaqing Wang"
                                },
                                {
                                    "authorId": "2257230381",
                                    "name": "Yiling Jia"
                                },
                                {
                                    "authorId": "1691997",
                                    "name": "Mykola Pechenizkiy"
                                },
                                {
                                    "authorId": "2260290217",
                                    "name": "Yi Liang"
                                },
                                {
                                    "authorId": "2254949434",
                                    "name": "Zhangyang Wang"
                                },
                                {
                                    "authorId": "2255081092",
                                    "name": "Shiwei Liu"
                                }
                            ],
                            "year": 2023,
                            "venue": "International Conference on Machine Learning",
                            "n_citations": 102
                        },
                        "score": 0
                    },
                    {
                        "id": "(Ashkboos et al., 2024)",
                        "snippets": [
                            "Large language models have become the cornerstone of natural language processing, but their use comes with substantial costs in terms of compute and memory resources. Sparsification provides a solution to alleviate these resource constraints, and recent works have shown that trained models can be sparsified post-hoc. Existing sparsification techniques face challenges as they need additional data structures and offer constrained speedup with current hardware. In this paper we present SliceGPT, a new post-training sparsification scheme which replaces each weight matrix with a smaller (dense) matrix, reducing the embedding dimension of the network. Through extensive experimentation, we show that SliceGPT can remove up to 25% of the model parameters (including embeddings) for LLAMA2-70B, OPT 66B and Phi-2 models while maintaining 99%, 99% and 90% zero-shot task performance of the dense model respectively. Our sliced models run on fewer GPUs and run faster without any additional code optimization: on 24GB consumer GPUs we reduce the total compute for inference on LLAMA2-70B to 64% of that of the dense model; on 40GB A100 GPUs we reduce it to 66%. We offer a new insight, computational invariance in transformer networks, which enables SliceGPT and we hope it will inspire and enable future avenues to reduce memory and computation demands for pre-trained models. Code is available at: https://github.com/microsoft/TransformerCompression"
                        ],
                        "paper": {
                            "corpus_id": 267301573,
                            "title": "SliceGPT: Compress Large Language Models by Deleting Rows and Columns",
                            "authors": [
                                {
                                    "authorId": "9543395",
                                    "name": "Saleh Ashkboos"
                                },
                                {
                                    "authorId": "2008063761",
                                    "name": "Maximilian L. Croci"
                                },
                                {
                                    "authorId": "2281641743",
                                    "name": "Marcelo Gennari do Nascimento"
                                },
                                {
                                    "authorId": "2258547286",
                                    "name": "Torsten Hoefler"
                                },
                                {
                                    "authorId": "2266803418",
                                    "name": "James Hensman"
                                }
                            ],
                            "year": 2024,
                            "venue": "International Conference on Learning Representations",
                            "n_citations": 184
                        },
                        "score": 0
                    }
                ],
                "format": "synthesis",
                "table": null,
                "model": "claude-3-7-sonnet-20250219"
            },
            {
                "title": "Comparative Performance and Trade-offs",
                "tldr": "Recent task-agnostic pruning methods exhibit varying performance across different models and sparsity levels, with methods like SparseGPT generally achieving better results at higher computational costs, while Wanda offers a favorable balance of efficiency and performance with simpler implementation. (10 sources)",
                "text": "\nWhen comparing recent task-agnostic pruning methods for LLMs, clear trade-offs emerge between computational complexity, pruning performance, and implementation difficulty. These trade-offs are critical considerations when selecting appropriate pruning approaches for specific deployment scenarios.\n\n## Performance Across Sparsity Levels\n\nSparseGPT remains a performance leader at high sparsity levels, allowing models to reach up to 60% unstructured sparsity with negligible increases in perplexity <Paper corpusId=\"255372747\" paperTitle=\"(Frantar et al., 2023)\" isShortName></Paper>. However, newer methods like AlphaPruning have demonstrated substantial improvements when applied to existing pruning techniques. For instance, when applied to LLaMA-7B at 70% sparsity, AlphaPruning reduced perplexity by 61.91 for Wanda and 7.76 for SparseGPT, indicating that SparseGPT's initial performance was already stronger <Paper corpusId=\"273350592\" paperTitle=\"(Lu et al., 2024)\" isShortName></Paper>.\n\nWanda has proven particularly effective at moderate sparsity levels, achieving 60% sparsity on LLaMA-7B with minimal performance degradation across multiple downstream tasks <Paper corpusId=\"277452419\" paperTitle=\"(Ma et al., 2025)\" isShortName></Paper>. However, specialized methods like SEAP (Sparse Expert Activation Pruning) have surpassed both Wanda and FLAP by over 20% at 50% pruning ratios, experiencing only a 2.2% performance drop compared to the dense model at 20% pruning <Paper corpusId=\"276928323\" paperTitle=\"(Liang et al., 2025)\" isShortName></Paper>.\n\n## Computational Complexity\n\nA significant trade-off exists between pruning performance and computational requirements. SparseGPT's use of diagonal Hessian approximations allows for precise pruning but comes with high computational complexity and substantial hardware resource demands <Paper corpusId=\"277452419\" paperTitle=\"(Ma et al., 2025)\" isShortName></Paper>. In contrast, Wanda eliminates the need for Hessian approximations by simply multiplying weights with input activations, significantly reducing computational requirements while maintaining competitive performance <Paper corpusId=\"277452419\" paperTitle=\"(Ma et al., 2025)\" isShortName></Paper> <Paper corpusId=\"276079889\" paperTitle=\"(Yi et al., 2025)\" isShortName></Paper>.\n\nThis computational efficiency has made Wanda a popular baseline for subsequent pruning methods. Many newer approaches either use SparseGPT and Wanda as baselines or build upon their foundations <Paper corpusId=\"277452419\" paperTitle=\"(Ma et al., 2025)\" isShortName></Paper>. For example, RIA extends Wanda by re-evaluating weight importance based on all input and output connections, offering performance improvements at minimal additional computational cost <Paper corpusId=\"276079889\" paperTitle=\"(Yi et al., 2025)\" isShortName></Paper>.\n\n## Implementation Challenges and Hyperparameter Sensitivity\n\nImplementation complexity varies significantly across pruning methods. While SparseGPT's optimization-based approach delivers outstanding performance, it requires sophisticated implementation and careful management of memory constraints <Paper corpusId=\"273501976\" paperTitle=\"(Zhao et al., 2024)\" isShortName></Paper>. Methods like ADMM build on SparseGPT by incorporating the Alternating Direction Method of Multipliers to restore model performance after pruning through an iterative mask selection process <Paper corpusId=\"277452419\" paperTitle=\"(Ma et al., 2025)\" isShortName></Paper> <Paper corpusId=\"266818263\" paperTitle=\"(Boza, 2024)\" isShortName></Paper>.\n\nHyperparameter sensitivity represents another critical trade-off. Layer-wise pruning approaches like Wanda can result in significant perturbation to the model's output and require meticulous tuning of pruning rates, which may adversely affect overall model performance <Paper corpusId=\"268032346\" paperTitle=\"(Xu et al., 2024)\" isShortName></Paper>. This sensitivity has motivated the development of methods like GBLM-Pruner, which leverages gradients from pretrained LLMs to determine pruning importance <Paper corpusId=\"265050936\" paperTitle=\"(Das et al., 2023)\" isShortName></Paper>.\n\n## Model and Task Generalization\n\nThe generalization capability of pruning methods across different model architectures and tasks represents another important consideration. SparseGPT has demonstrated broad applicability, working effectively on models ranging from OPT-175B to BLOOM-176B <Paper corpusId=\"255372747\" paperTitle=\"(Frantar et al., 2023)\" isShortName></Paper>. Similarly, Wanda has been extensively evaluated on LLaMA and LLaMA-2 across various language benchmarks <Paper corpusId=\"276774084\" paperTitle=\"(Ding et al., 2025)\" isShortName></Paper>.\n\nHowever, performance can vary significantly depending on the specific model architecture and task requirements. Recent research has found that while SparseGPT, Wanda, and RIA all prune models using a combination of weights and activations, their different metrics and strategies result in varying effectiveness across different scenarios <Paper corpusId=\"276774084\" paperTitle=\"(Ding et al., 2025)\" isShortName></Paper>.\n\nAs the field continues to evolve, newer methods increasingly aim to combine the strengths of earlier approaches while addressing their limitations. For instance, OWL integrates both Wanda and SparseGPT, proposing a metric to allocate varying pruning rates across different layers <Paper corpusId=\"277452419\" paperTitle=\"(Ma et al., 2025)\" isShortName></Paper>. This trend toward hybrid approaches suggests that the future of task-agnostic LLM pruning likely lies in methods that balance computational efficiency, implementation simplicity, and robust performance across diverse models and tasks.",
                "citations": [
                    {
                        "id": "(Frantar et al., 2023)",
                        "snippets": [
                            "We show for the first time that large-scale generative pretrained transformer (GPT) family models can be pruned to at least 50% sparsity in one-shot, without any retraining, at minimal loss of accuracy. This is achieved via a new pruning method called SparseGPT, specifically designed to work efficiently and accurately on massive GPT-family models. We can execute SparseGPT on the largest available open-source models, OPT-175B and BLOOM-176B, in under 4.5 hours, and can reach 60% unstructured sparsity with negligible increase in perplexity: remarkably, more than 100 billion weights from these models can be ignored at inference time. SparseGPT generalizes to semi-structured (2:4 and 4:8) patterns, and is compatible with weight quantization approaches. The code is available at: https://github.com/IST-DASLab/sparsegpt."
                        ],
                        "paper": {
                            "corpus_id": 255372747,
                            "title": "SparseGPT: Massive Language Models Can Be Accurately Pruned in One-Shot",
                            "authors": [
                                {
                                    "authorId": "1502248377",
                                    "name": "Elias Frantar"
                                },
                                {
                                    "authorId": "3311387",
                                    "name": "Dan Alistarh"
                                }
                            ],
                            "year": 2023,
                            "venue": "International Conference on Machine Learning",
                            "n_citations": 734
                        },
                        "score": 0
                    },
                    {
                        "id": "(Lu et al., 2024)",
                        "snippets": [
                            "For example, in the case of LLaMA-7B with a sparsity of 70%, AlphaPruning produces sparse networks with a perplexity of 231.01, significantly outperforming the Magnitude-based pruning baseline of 48419.13. Notably, when applied to Wanda and SparseGPT, two robust LLM pruning methods, AlphaPruning still achieves substantial perplexity reductions, evidenced by a decrease of 61.91 for Wanda and 7.76 for SparseGPT, in the case of LLaMA-7B with a sparsity of 70%."
                        ],
                        "paper": {
                            "corpus_id": 273350592,
                            "title": "AlphaPruning: Using Heavy-Tailed Self Regularization Theory for Improved Layer-wise Pruning of Large Language Models",
                            "authors": [
                                {
                                    "authorId": "2311995497",
                                    "name": "Haiquan Lu"
                                },
                                {
                                    "authorId": "2111405998",
                                    "name": "Yefan Zhou"
                                },
                                {
                                    "authorId": "2255081092",
                                    "name": "Shiwei Liu"
                                },
                                {
                                    "authorId": "2284563898",
                                    "name": "Zhangyang Wang"
                                },
                                {
                                    "authorId": "2249392052",
                                    "name": "Michael W. Mahoney"
                                },
                                {
                                    "authorId": "2249529142",
                                    "name": "Yaoqing Yang"
                                }
                            ],
                            "year": 2024,
                            "venue": "Neural Information Processing Systems",
                            "n_citations": 10
                        },
                        "score": 0.88037109375
                    },
                    {
                        "id": "(Ma et al., 2025)",
                        "snippets": [
                            "Unstructured pruning is an optimization technique that achieves model sparsity by evaluating the importance of individual weights. Its flexibility and high compression rates make it a key method for optimizing large language models (LLMs). Unstructured pruning can achieve extremely high compression rates; for instance, Wanda achieves a 60% sparsity rate on LLaMA-7B with minimal performance degradation across multiple downstream tasks (Sun et al., 2023), while Flash-LLM achieves a 70% sparsity rate on OPT-175B, significantly reducing storage requirements with less than 2% performance degradation during inference [28]",
                            "SparseGPT (Frantar et al., 2023), on the other hand, introduces a diagonal Hessian approximation to assess the impact of weights on errors, enabling more precise pruning at the cost of high computational complexity and hardware resource requirements. Wanda (Sun et al., 2023) simplifies the SparseGPT algorithm by eliminating the need for Hessian approximations and instead computes pruning metrics by multiplying weights with input activations. This simplification significantly reduces computational complexity while achieving a balance between high accuracy and efficiency. Following this approach, many subsequent methods use SparseGPT and Wanda as baselines or build upon their foundations. RIA (Zhang et al., 2024) introduces a post-training pruning method that re-evaluates the importance of each weight element based on all input and output connections. ADMM (Boza, 2024) builds on SparseGPT by incorporating the Alternating Direction Method of Multipliers (ADMM) to restore model performance after pruning, using a simple iterative mask selection process for pruning. OWL [32] integrates both Wanda and SparseGPT, proposing the OWL metric to allocate varying pruning rates across different layers."
                        ],
                        "paper": {
                            "corpus_id": 277452419,
                            "title": "Model Hemorrhage and the Robustness Limits of Large Language Models",
                            "authors": [
                                {
                                    "authorId": "2352948034",
                                    "name": "Ziyang Ma"
                                },
                                {
                                    "authorId": "2274202084",
                                    "name": "Zuchao Li"
                                },
                                {
                                    "authorId": "2269488794",
                                    "name": "Lefei Zhang"
                                },
                                {
                                    "authorId": "2343636012",
                                    "name": "Gui-Song Xia"
                                },
                                {
                                    "authorId": "2306994733",
                                    "name": "Bo Du"
                                },
                                {
                                    "authorId": "2268745050",
                                    "name": "Liangpei Zhang"
                                },
                                {
                                    "authorId": "2275194788",
                                    "name": "Dacheng Tao"
                                }
                            ],
                            "year": 2025,
                            "venue": "arXiv.org",
                            "n_citations": 1
                        },
                        "score": 0.87548828125
                    },
                    {
                        "id": "(Liang et al., 2025)",
                        "snippets": [
                            "This paper introduces Sparse Expert Activation Pruning (SEAP), a training-free pruning method that selectively retains task-relevant parameters to reduce inference overhead. Inspired by the clustering patterns of hidden states and activations in LLMs, SEAP identifies task-specific expert activation patterns and prunes the model while preserving task performance and enhancing computational efficiency. Experimental results demonstrate that SEAP significantly reduces computational overhead while maintaining competitive accuracy. Notably, at 50% pruning, SEAP surpasses both WandA and FLAP by over 20%, and at 20% pruning, it incurs only a 2.2% performance drop compared to the dense model."
                        ],
                        "paper": {
                            "corpus_id": 276928323,
                            "title": "SEAP: Training-free Sparse Expert Activation Pruning Unlock the Brainpower of Large Language Models",
                            "authors": [
                                {
                                    "authorId": "2268495952",
                                    "name": "Xun Liang"
                                },
                                {
                                    "authorId": "2284861141",
                                    "name": "Hanyu Wang"
                                },
                                {
                                    "authorId": "2349573796",
                                    "name": "Huayi Lai"
                                },
                                {
                                    "authorId": "2268393907",
                                    "name": "Simin Niu"
                                },
                                {
                                    "authorId": "2268434524",
                                    "name": "Shichao Song"
                                },
                                {
                                    "authorId": "2303425635",
                                    "name": "Jiawei Yang"
                                },
                                {
                                    "authorId": "2326243408",
                                    "name": "Jihao Zhao"
                                },
                                {
                                    "authorId": "2268399953",
                                    "name": "Feiyu Xiong"
                                },
                                {
                                    "authorId": "2268400606",
                                    "name": "Bo Tang"
                                },
                                {
                                    "authorId": "2268429641",
                                    "name": "Zhiyu Li"
                                }
                            ],
                            "year": 2025,
                            "venue": "arXiv.org",
                            "n_citations": 0
                        },
                        "score": 0.853515625
                    },
                    {
                        "id": "(Yi et al., 2025)",
                        "snippets": [
                            "Wanda (Sun et al., 2023) introduces a pruning metric that incorporates both weight magnitudes and corresponding input activations, achieving perplexity performance comparable to SparseGPT while surpassing simple magnitude-based pruning. The RIA method (Zhang et al., 2024) builds on Wanda by considering relative weight importance, offering performance improvements at minimal additional cost. Moreover, DSnoT (Zhang et al., 2023) proposes pruning and regrowing weights based on statistical properties (e.g., mean and variance) in each pruning row, obviating the need for retraining."
                        ],
                        "paper": {
                            "corpus_id": 276079889,
                            "title": "Symmetric Pruning of Large Language Models",
                            "authors": [
                                {
                                    "authorId": "2292143960",
                                    "name": "Kai Yi"
                                },
                                {
                                    "authorId": "2342412598",
                                    "name": "Peter Richt'arik"
                                }
                            ],
                            "year": 2025,
                            "venue": "arXiv.org",
                            "n_citations": 0
                        },
                        "score": 0.85400390625
                    },
                    {
                        "id": "(Zhao et al., 2024)",
                        "snippets": [
                            "The traditional pruning techniques, which finetune or retrain models (Li et al., 2020) on full datasets for many epochs (i.e., pruning-aware training), are too expensive for LLMs in terms of data and GPU resources. Thus, post-training pruning based on well-pre-trained models with reduced resource requirements represents a more reasonable approach for LLMs. Notably, SparseGPT (Frantar et al., 2023) is the representative post-training pruning work with outstanding performance. It reduces memory cost by sequentially loading transformer blocks, one at a time, instead of loading the whole model. Moreover, it reduces the data cost by using only a small amount of calibration data, eliminating the retraining process on massive data. Besides the optimization based SparseGPT, there are some other heuristic posttraining pruning methods such as (Sun et al., 2023;Zhang et al., 2024), achieving accuracy close to SparseGPT."
                        ],
                        "paper": {
                            "corpus_id": 273501976,
                            "title": "Pruning Foundation Models for High Accuracy without Retraining",
                            "authors": [
                                {
                                    "authorId": "2241612245",
                                    "name": "Pu Zhao"
                                },
                                {
                                    "authorId": "2327046378",
                                    "name": "Fei Sun"
                                },
                                {
                                    "authorId": "2007668856",
                                    "name": "Xuan Shen"
                                },
                                {
                                    "authorId": "2241698013",
                                    "name": "Pinrui Yu"
                                },
                                {
                                    "authorId": "32409528",
                                    "name": "Zhenglun Kong"
                                },
                                {
                                    "authorId": "2290628977",
                                    "name": "Yanzhi Wang"
                                },
                                {
                                    "authorId": "2322988586",
                                    "name": "Xue Lin"
                                }
                            ],
                            "year": 2024,
                            "venue": "Conference on Empirical Methods in Natural Language Processing",
                            "n_citations": 13
                        },
                        "score": 0.826171875
                    },
                    {
                        "id": "(Boza, 2024)",
                        "snippets": [
                            "Pruning large language models (LLMs) is a challenging task due to their enormous size. The primary difficulty is fine-tuning the model after pruning, which is needed to recover the lost performance caused by dropping weights. Recent approaches have either ignored fine-tuning entirely, focusing on efficient pruning criteria, or attempted layer-wise weight updates, preserving the behavior of each layer. However, even layer-wise weight updates can be costly for LLMs, and previous works have resorted to various approximations. In our paper, we propose a fast and effective weight update algorithm for pruned layers based on the Alternating Direction Method of Multipliers (ADMM). We further extend it with a simple gradual pruning mask selection and achieve state-of-the-art pruning performance across a wide range of LLMs. Code is available at https://github.com/fmfi-compbio/admm-pruning."
                        ],
                        "paper": {
                            "corpus_id": 266818263,
                            "title": "Fast and Effective Weight Update for Pruned Large Language Models",
                            "authors": [
                                {
                                    "authorId": "2315325819",
                                    "name": "Vladim\u00edr Boza"
                                }
                            ],
                            "year": 2024,
                            "venue": "Trans. Mach. Learn. Res.",
                            "n_citations": 5
                        },
                        "score": 0
                    },
                    {
                        "id": "(Xu et al., 2024)",
                        "snippets": [
                            "Existing solutions such as SparseGPT and Wanda attempt to alleviate this issue through weight pruning. However, their layer-wise approach results in significant perturbation to the model's output and requires meticulous hyperparameter tuning, such as the pruning rate, which can adversely affect overall model performance."
                        ],
                        "paper": {
                            "corpus_id": 268032346,
                            "title": "BESA: Pruning Large Language Models with Blockwise Parameter-Efficient Sparsity Allocation",
                            "authors": [
                                {
                                    "authorId": "2153917002",
                                    "name": "Peng Xu"
                                },
                                {
                                    "authorId": "2283133523",
                                    "name": "Wenqi Shao"
                                },
                                {
                                    "authorId": "2287768783",
                                    "name": "Mengzhao Chen"
                                },
                                {
                                    "authorId": "2287949030",
                                    "name": "Shitao Tang"
                                },
                                {
                                    "authorId": "2273778831",
                                    "name": "Kai-Chuang Zhang"
                                },
                                {
                                    "authorId": "2269823523",
                                    "name": "Peng Gao"
                                },
                                {
                                    "authorId": "2287838451",
                                    "name": "Fengwei An"
                                },
                                {
                                    "authorId": "2256992387",
                                    "name": "Yu Qiao"
                                },
                                {
                                    "authorId": "2253674868",
                                    "name": "Ping Luo"
                                }
                            ],
                            "year": 2024,
                            "venue": "International Conference on Learning Representations",
                            "n_citations": 32
                        },
                        "score": 0.84326171875
                    },
                    {
                        "id": "(Das et al., 2023)",
                        "snippets": [
                            "Large Language Models (LLMs) with billions of parameters are prime targets for network pruning, removing some model weights without hurting performance. Prior approaches such as magnitude pruning, SparseGPT, and Wanda, either concentrated solely on weights or integrated weights with activations for sparsity. However, they overlooked the informative gradients derived from pretrained LLMs. In this paper, we present a novel sparsity-centric pruning method for pretrained LLMs, termed Gradient-based Language Model Pruner (GBLM-Pruner). GBLM-Pruner leverages the first-order term of the Taylor expansion, operating in a training-free manner by harnessing properly normalized gradients from a few calibration samples to determine the pruning metric, and substantially outperforms competitive counterparts like SparseGPT and Wanda in multiple benchmarks."
                        ],
                        "paper": {
                            "corpus_id": 265050936,
                            "title": "Beyond Size: How Gradients Shape Pruning Decisions in Large Language Models",
                            "authors": [
                                {
                                    "authorId": "2211732585",
                                    "name": "Rocktim Jyoti Das"
                                },
                                {
                                    "authorId": "2243392466",
                                    "name": "Liqun Ma"
                                },
                                {
                                    "authorId": "2243374493",
                                    "name": "Zhiqiang Shen"
                                }
                            ],
                            "year": 2023,
                            "venue": "arXiv.org",
                            "n_citations": 19
                        },
                        "score": 0.86962890625
                    },
                    {
                        "id": "(Ding et al., 2025)",
                        "snippets": [
                            "In this study, we employed three state-of-the-art posttraining pruning methods: SparseGPT (Frantar et al., 2023), Wanda (Sun et al., 2023), and RIA (Zhang et al., 2024). These methods are designed to reduce the size of large language models (LLMs) while maintaining their performance across various tasks. Even though all three methods prune the model using a combination of weights W and activations X, they each use different metrics M and strategies to decide which parameters to prune."
                        ],
                        "paper": {
                            "corpus_id": 276774084,
                            "title": "Revisiting Large Language Model Pruning using Neuron Semantic Attribution",
                            "authors": [
                                {
                                    "authorId": "2237598833",
                                    "name": "Yizhuo Ding"
                                },
                                {
                                    "authorId": "2244778967",
                                    "name": "Xinwei Sun"
                                },
                                {
                                    "authorId": "2244698019",
                                    "name": "Yanwei Fu"
                                },
                                {
                                    "authorId": "2349205822",
                                    "name": "Guosheng Hu"
                                }
                            ],
                            "year": 2025,
                            "venue": "arXiv.org",
                            "n_citations": 2
                        },
                        "score": 0.9404296875
                    }
                ],
                "format": "synthesis",
                "table": null,
                "model": "claude-3-7-sonnet-20250219"
            }
        ],
        "cost": 0.24388500000000002
    }
}